---
title: Feministische Netzpolitik und Künstliche Intelligenz in der politischen Bildung [Feminist network politics and artificial intelligence in political education]
authors:
  - K. Mosene
year: 2023
type: webpage
doi: 
url: "https://www.politische-medienkompetenz.de/debatte/ki-und-intersektionalitaet/"
tags:
  - paper
llm_decision: Include
llm_confidence: 0.92
llm_categories:
  - AI_Literacies
  - KI_Sonstige
  - Bias_Ungleichheit
  - Gender
  - Feministisch
  - Fairness
human_decision: Exclude
human_categories:
  - KI_Sonstige
  - Bias_Ungleichheit
  - Gender
  - Diversitaet
  - Feministisch
  - Fairness
agreement: disagree
---

# Feministische Netzpolitik und Künstliche Intelligenz in der politischen Bildung [Feminist network politics and artificial intelligence in political education]

## Transformation Trail

### Stufe 1: Extraktion & Klassifikation (LLM)

**Extrahierte Kategorien:** AI_Literacies, KI_Sonstige, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness
**Argumente:** 3 extrahiert

### Stufe 3: Verifikation (LLM)

| Metrik | Score |
|--------|-------|
| Completeness | 88 |
| Correctness | 92 |
| Category Validation | 87 |
| **Overall Confidence** | **89** |

### Stufe 4: Assessment

**LLM:** Include (Confidence: 0.92)
**Human:** Exclude

**Kategorie-Vergleich (bei Divergenz):**

| Kategorie | Human | LLM | Divergent |
|-----------|-------|-----|----------|
| AI_Literacies | Nein | Ja | X |
| Generative_KI | Nein | Nein |  |
| Prompting | Nein | Nein |  |
| KI_Sonstige | Ja | Ja |  |
| Soziale_Arbeit | Nein | Nein |  |
| Bias_Ungleichheit | Ja | Ja |  |
| Gender | Ja | Ja |  |
| Diversitaet | Ja | Nein | X |
| Feministisch | Ja | Ja |  |
| Fairness | Ja | Ja |  |

> Siehe [[Divergenz Mosene_2023_Feministische_Netzpolitik_und_Künstliche]] fuer detaillierte Analyse


## Key Concepts

- [[Algorithmic Bias]]
- [[Critical AI Literacy]]
- [[Intersectionality]]
- [[Training Data Bias]]

## Wissensdokument

# Feministische Netzpolitik und Künstliche Intelligenz in der Politischen Bildung

## Kernbefund

Diskriminierende Stereotype und koloniale Strukturen sind bereits in der technischen Infrastruktur und den Trainingsdaten von KI-Systemen eingeschrieben; feministische Netzpolitik und kritische politische Bildung sind notwendig, um diese Mechanismen aufzudecken und emanzipatorische Alternativen zu schaffen.

## Forschungsfrage

Wie können feministische und intersektionale Perspektiven helfen, diskriminierende Strukturen in künstlichen Intelligenzsystemen zu identifizieren und in der politischen Bildung zu adressieren?

## Methodik

Theoretisch/Review - kritische Analyse von KI-Systemen und deren gesellschaftlichen Auswirkungen aus feministischer und intersektionaler Perspektive
**Datenbasis:** nicht empirisch - konzeptionelle und diskursanalytische Analyse

## Hauptargumente

- Technologie ist nicht neutral (Kranzberg's First Law): Sprachassistenten wie Siri und Alexa verkörpern stereotype weibliche Rollenvorstellungen (Sekretärin, Hausfrau, Mutter), was zeigt, dass normative Entscheidungen bereits in der Gestaltung technischer Systeme eingebettet sind.
- Künstliche Intelligenz perpetuiert strukturelle Diskriminierung durch verzerrte Trainingsdaten: Biometrische Gesichtserkennung wurde überwiegend mit Daten weißer Personen trainiert und kann daher Schwarze Menschen und People of Colour mit deutlich höherer Fehlerquote nicht identifizieren, was zu Racial Profiling in der Strafverfolgung führt.
- Digitale Infrastruktur folgt kolonialen Geografien: Die Routen von Glasfaserkabeln folgen den historischen Linien von Telegraphenkabeln und Sklavenschiffrouten; proprietäre Tech-Konzerne aus dem Silicon Valley dominieren das Internet und tragen ihre Interessen in den globalen Süden, statt lokale, bedarfsgerechte Initiativen zu unterstützen.

## Kategorie-Evidenz

### Evidenz 1

Es ist deshalb nicht nur wichtig, den Nutzer_innen von Technologien zu vermitteln, wie diese funktionieren, sondern vor allem wie sie entstanden sind, welche gesellschaftlichen Ideen und Realitäten sie widerspiegeln, welche soziopolitischen Potenziale sich in ihnen verbergen.

### Evidenz 2

Künstliche Intelligenzen denken also nicht wie Menschen. Sie erkennen Hunde und Katzen nicht einfach, sondern lernen, dass beispielsweise Hundefotos statistisch häufiger mit einem Copyrightzeichen versehen sind als Katzenfotos. Künstliche Intelligenz ist damit stark von Trainingsdaten abhängig.

### Evidenz 3

Es ist hinlänglich bekannt, dass biometrische Gesichtserkennung, die vorwiegend in einem männlich dominierten Raum im weißen globalen Norden entwickelt wird, lange nicht in der Lage war, Schwarze Menschen und People of Colour zu identifizieren, da sie sich überwiegend auf Trainingsdatensätze weißer Personen stützte.

### Evidenz 4

Unsere Alltagsbegleiter heißen heute Siri und Alexa: eine Frau in der Hosentasche oder im Haushalt, die uns stets gefügig zur Seite steht. Dahinter verbergen sich stereotype weibliche Rollenvorstellungen: die Frau als Sekretärin, Hausfrau und Mutter.

### Evidenz 5

Intersektionale Perspektiven nehmen ineinandergreifende, sich teils verstärkende Strukturen von Ungleichheit, Macht und Herrschaft in den Blick, und ein besonderer Fokus liegt dabei auf der wechselseitigen Konstitution von Rassismus und Sexismus in ihren gesellschaftsstrukturierenden Formen.

### Evidenz 6

Feministische Netzpolitik übt Kritik an bestehenden Ungleichheitsverhältnissen im digitalen Raum. Donna Haraway beschrieb in ihrem 'Cyborg Manifesto' das potenzielle Aufbrechen von Geschlechterzuschreibungen. Feministische Netzpolitik besteht darin, KI-Forscher_innen und Aktivist_innen zu unterstützen, die im Techniksektor die Bias aufzuheben suchen.

### Evidenz 7

Machtstrukturen sind schon in der technischen Infrastruktur fest angelegt. Sie führen die Geschichte des Kolonialismus auch in der virtuellen Sphäre fort: in Gestalt des 'digitalen' oder 'elektronischen Kolonialismus'. Feministische Datensätze könnten die Utopie eines non-biased Internet bilden.

## Assessment-Relevanz

**Domain Fit:** Das Paper hat hohe Relevanz für die Schnittstelle zwischen KI und feministischen/intersektionalen Perspektiven. Es adressiert jedoch nicht direkt Soziale Arbeit, sondern konzentriert sich auf politische Bildung und netzpolitische Kritik.

**Unique Contribution:** Das Paper bietet eine deutschsprachige, intersektional-feministische Analyse von KI-Systemen, die sowohl algorithmische Diskriminierung als auch koloniale Strukturen der digitalen Infrastruktur kritisiert und Emanzipationsperspektiven für die politische Bildung aufzeigt.

**Limitations:** Das Paper ist konzeptionell-theoretisch und enthält keine empirischen Daten; es fokussiert auf Problembeschreibung mehr als auf lösungsorientierte Interventionen oder Implementierungsstrategien.

**Target Group:** Politische Bildner_innen, Multiplikator_innen in der Erwachsenenbildung, KI-Kritiker_innen, Aktivist_innen im Bereich feministische Netzpolitik, Journalist_innen, Policy-Maker im Bildungsbereich, sowie allgemein an kritischer Medienkompetenzvermittlung Interessierte

## Schlüsselreferenzen

- [[Haraway_Donna_1985]] - Manifesto for Cyborgs: Science, Technology, and Socialist Feminism in the 1980's
- [[Crenshaw_Kimberlé_1989]] - Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine
- [[Kranzberg_Melvin_1986]] - Technology and History: Kranzberg's Laws
- [[Shepard_Nicole_2017]] - Was hat Überwachung mit Sex und Gender zu tun?
- [[Sinders_Caroline_None]] - Feminist Data Set
- [[Holev_Ina_2020]] - 2020: Digitaler Kolonialismus
- [[Köppert_Katrin_2019]] - 'Internet is not in the Cloud.' Digitaler Kolonialismus
- [[NIST_2019]] - NIST Study Evaluates Effects of Race, Age, Sex on Face Recognition Software
