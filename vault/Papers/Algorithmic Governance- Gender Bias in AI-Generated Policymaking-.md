---
title: "Algorithmic Governance: Gender Bias in AI-Generated Policymaking?"
authors:
  - D. A. Voutyrakou
  - C. Skordoulis
year: 2025
type: journalArticle
doi: 10.1007/s44230-025-00109-2
url: "https://doi.org/10.1007/s44230-025-00109-2"
tags:
  - paper
llm_decision: Include
llm_confidence: 0.92
llm_categories:
  - Generative_KI
  - Prompting
  - Bias_Ungleichheit
  - Gender
  - Feministisch
  - Fairness
human_decision: Include
human_categories:
  - Generative_KI
  - Prompting
  - KI_Sonstige
  - Bias_Ungleichheit
  - Gender
  - Diversitaet
  - Feministisch
  - Fairness
agreement: agree
---

# Algorithmic Governance: Gender Bias in AI-Generated Policymaking?

## Transformation Trail

### Stufe 1: Extraktion & Klassifikation (LLM)

**Extrahierte Kategorien:** AI_Literacies, Generative_KI, Prompting, KI_Sonstige, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness
**Argumente:** 3 extrahiert

### Stufe 3: Verifikation (LLM)

| Metrik | Score |
|--------|-------|
| Completeness | 92 |
| Correctness | 98 |
| Category Validation | 96 |
| **Overall Confidence** | **95** |

### Stufe 4: Assessment

**LLM:** Include (Confidence: 0.92)
**Human:** Include

## Key Concepts

- [[Algorithmic Gender Bias]]
- [[Algorithmic Governance]]
- [[Intersectional AI Fairness]]

## Wissensdokument

# Algorithmic Governance: Gender Bias in AI-Generated Policymaking?

## Kernbefund

KI-Tools integrieren genderspezifische Bedürfnisse (Menstruationshygiene, Sicherheit, Kinderbetreuung, thermischer Komfort) nur dann in Politikempfehlungen, wenn Gender explizit im Prompt erwähnt wird; ansonsten wird eine androzentristische Standardperspektive angewandt.

## Forschungsfrage

Berücksichtigen populäre KI-Tools wie ChatGPT und Microsoft Copilot Geschlechteraspekte bei der Erstellung von Politikempfehlungen, sowohl wenn Gender explizit erwähnt wird als auch wenn nicht?

## Methodik

Mixed-Methods: Vier experimentelle Szenarien mit wiederholten Prompts in zwei KI-Systemen (ChatGPT GPT-4 und Microsoft Copilot), Häufigkeitsanalyse und qualitative Kodierung der Ergebnisse.
**Datenbasis:** Vier experimentelle Szenarien: Restroom Design, Schneeräumpolitik, Bürotemperaturregelung, Business-Travel-Ausgaben; jeweils mit Varianten (unspezifisch, männlich, weiblich, nicht-binär); Tests in ChatGPT und Microsoft Copilot

## Hauptargumente

- Gender-Bias in KI-Tools ist nicht primär ein Trainingsdaten-Problem, sondern ein strukturelles Designproblem, das eine androzentristische Perspektive als 'neutral' kodifiziert und genderspezifische Bedürfnisse nur als Sonderfälle behandelt.
- Die angenommene Neutralität von KI-Systemen verschleiert tatsächlich eine male-default-Logik, die körperliche Unterschiede (Stoffwechselrate, Menstruation), Sorgearbeit und Sicherheitsbedenken systematisch ausblendet, wenn nicht explizit aufgefordert.
- Faire und genderbewusste Algorithmen-Governance erfordert eine proaktive Integration von feministischer Ethik (Care-Ethik, Capabilities Approach) und intersektionalen Perspektiven in Algorithmen-Design, nicht nur nachträgliche Korrektionen.

## Kategorie-Evidenz

### Evidenz 1

Diskussion darüber, dass Nutzer als 'aktive Teilnehmer' in den Bias-Feedback-Loop beitragen und dass 'Prompt Literacy und Public Education' notwendig sind: 'users themselves contribute to the bias feedback loop, making them active participants rather than passive recipients'

### Evidenz 2

Fokus auf zwei generative KI-Systeme: 'We tested these experiments in two different AI tools, namely ChatGPT (GPT-4) and Microsoft Copilot'

### Evidenz 3

Explizite Variation der Prompt-Struktur mit/ohne Gender-Erwähnungen: 'Each scenario includes a gender-neutral prompt, a male-specific version, and a female-specific version and a non-binary-specific version'

### Evidenz 4

Analyse von LLMs und NLP-Systemen als Teil von algorithmischen Entscheidungssystemen: 'Large language models such as ChatGPT or Microsoft Copilot add layers of complexity to understanding gender bias in AI'

### Evidenz 5

Zentrale These der strukturellen Benachteiligung durch androzentrische Algorithmen-Designs: 'the supposed neutrality of AI tools actually reproduces an androcentric norm' und 'a technological design that equates neutral with male'

### Evidenz 6

Expliziter Fokus auf Gender-Bias in vier verschiedenen Policy-Kontexten und auf geschlechtsspezifische Bedürfnisse: 'gender bias...arises when systems consistently advantage or disadvantage individuals based on gender'

### Evidenz 7

Intersektionale Perspektive und Analyse nicht-binärer Identitäten: 'The non-binary scenario further illustrates this pattern: inclusivity features such as gender-neutral policies and identity-affirming practices were activated only upon explicit mention'

### Evidenz 8

Explizite Verwendung feministischer Theorien (Tronto's Care-Ethik, Grosz' embodied subjectivity, Bobel on menstrual stigma, Koskela on women's spatial confidence): 'Drawing on Joan Tronto's ethics of care and Iris Marion Young's concept of justice as inclusion, this perspective argues that fair governance must recognize and respond to differentiated social positions'

### Evidenz 9

Kritik an oberflächlichen Fairness-Ansätzen und Forderung nach kontextualisierten Fairness-Konzepten: 'fairness interventions must align with real-world harm mitigation and not merely with metric optimization'

## Assessment-Relevanz

**Domain Fit:** Hoch relevant für die Schnittstelle KI und Gender-Gerechtigkeit; untersucht konkret, wie KI-Systeme in Governance und Policy-Making androzentristische Bias perpetuieren. Trägt zu dringend benötigten empirischen Erkenntnissen über generative KI im öffentlichen Sektor bei.

**Unique Contribution:** Erste systematische experimentelle Studie, die nachweist, dass KI-Tools Gender-Berücksichtigung nicht von sich aus vornehmen, sondern diese als 'Sonderfälle' behandeln, was eine fundamentale Designfrage aufwirft.

**Limitations:** Limitierte Stichprobe (nur zwei KI-Tools); keine Analyse der Unterschiede zwischen GPT-4 und Copilot im Detail; keine Nutzerstudie zur Wahrnehmung oder Konsequenzen dieser Biases.

**Target Group:** Policy-Maker und Governance-Experten, KI-Ethik-Forscher, feministische Technologiekritiker, KI-Entwickler und -Designer, Sozialwissenschaftler, Organisationen der Gleichstellungsarbeit

## Schlüsselreferenzen

- [[Tronto_Joan_1993]] - Moral Boundaries: A Political Argument for an Ethics of Care
- [[DIgnazio_Catherine_Klein_Lauren_F_2020]] - Data Feminism
- [[CostanzaChock_Sasha_2020]] - Design Justice: Community-Led Practices to Build the Worlds We Need
- [[West_Sarah_M_Whittaker_Meredith_Crawford_Kate_2019]] - Discriminating Systems: Gender, Race and Power in AI
- [[Buolamwini_Joy_Gebru_Timnit_2018]] - Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification
- [[Crenshaw_Kimberlé_1991]] - Mapping the Margins: Intersectionality, Identity Politics, and Violence Against Women of Color
- [[Sen_Amartya_1999]] - Development as Freedom
- [[Nussbaum_Martha_C_2011]] - Creating Capabilities: The Human Development Approach
- [[Grosz_Elizabeth_A_1994]] - Volatile Bodies
- [[Bobel_Chris_Ed_2019]] - The Managed Body: Developing Girls and Menstrual Health in the Global South
