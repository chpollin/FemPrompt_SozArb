---
title: "AI & Intersectionality: A Toolkit For Fairness & Inclusion"
authors:
  - Unknown Author
year: 2024
type: report
doi: 
url: "https://diversifair-project.eu/wp-content/uploads/2025/01/Copie-de-INDUSTRY-Educ-Kits-A4-V2.pdf"
tags:
  - paper
llm_decision: Include
llm_confidence: 0.95
llm_categories:
  - AI_Literacies
  - Prompting
  - KI_Sonstige
  - Bias_Ungleichheit
  - Gender
  - Diversitaet
  - Feministisch
  - Fairness
---

# AI & Intersectionality: A Toolkit For Fairness & Inclusion

## Transformation Trail

### Stufe 1: Extraktion & Klassifikation (LLM)

**Extrahierte Kategorien:** AI_Literacies, KI_Sonstige, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness
**Argumente:** 3 extrahiert

### Stufe 3: Verifikation (LLM)

| Metrik | Score |
|--------|-------|
| Completeness | 92 |
| Correctness | 98 |
| Category Validation | 95 |
| **Overall Confidence** | **95** |

### Stufe 4: Assessment

**LLM:** Include (Confidence: 0.95)

## Key Concepts

- [[AI Literacy]]
- [[Data Feminism]]
- [[Intersectional Algorithmic Bias]]

## Wissensdokument

# AI & Intersectionality: A Toolkit for Fairness & Inclusion for the Industry Sector

## Kernbefund

Intersektionale Vorurteile in KI entstehen durch Überlagerung mehrerer Diskriminierungsformen und erfordern ganzheitliche, disziplinübergreifende Strategien zur Mitigation, die über isolierte Bias-Ansätze hinausgehen.

## Forschungsfrage

Wie können Organisationen intersektionale Vorurteile in KI-Systemen identifizieren und abbauen, um faire und inklusive Technologien zu entwickeln?

## Methodik

Mixed-Methods Review: Literaturanalyse, Stakeholder-Interviews (KI-Community und Policy-Sektor), Fokusgruppen, Best-Practice-Sammlung, Fallstudienanalyse, Toolkit-Entwicklung
**Datenbasis:** Nicht spezifiziert; basierend auf Interviews und Fokusgruppen mit AI-Community und Policy-Experten, Literaturanalyse von Fallstudien und Forschungsergebnissen

## Hauptargumente

- Intersektionale Vorurteile in KI sind komplexer als eindimensionale Bias-Ansätze: Sie entstehen durch Überlagerung von Rassismus, Sexismus, Ableismus und Kolonialismus und beeinflussen Menschen mit multiplen marginalisierten Identitäten in spezifischen Weisen (z.B. Black women in Gesichtserkennung oder immigrant families in Wohlfahrtssystemen).
- Technische Lösungen allein sind unzureichend; notwendig sind ganzheitliche Strategien, die Datenpraxis, Teamdiversität, Transparenz, Disziplinübergreifende Zusammenarbeit und kritische Reflexion der Systemexistenz einbeziehen.
- Organisationen (Industrie, Public Sector, Governance) müssen konkrete, rollenspezifische Maßnahmen implementieren: Entwickler müssen intersektional designen, Führungskräfte müssen Fairness budgetieren, Governance-Teams müssen Accountability etablieren, HR muss diverse Teams aufbauen und AI-Literacy fördern.

## Kategorie-Evidenz

### Evidenz 1

Build awareness and AI literacy: 'Provide AI literacy training across teams, focusing on ethics, intersectionality, and societal impact. Debunk AI myths, such as the notion of AI neutrality, through internal workshops and communication.' (S. 18-19)

### Evidenz 2

Fokus auf klassische ML-Systeme und algorithmische Entscheidungssysteme: Predictive Policing, Healthcare Algorithms, Facial Recognition, Hiring Algorithms, Credit Scoring, Welfare Fraud Detection

### Evidenz 3

Intersectional bias in AI describes 'the AI harms as experienced by people due to multiple intersecting and often marginalised parts of their identity.' Beispiele: Dutch childcare scandal (immigrant families), predictive policing (low-income communities of colour), healthcare disparities (uninsured/underinsured populations)

### Evidenz 4

Gender bias dokumentiert in Amazon Recruiting Tool, Apple Credit Card, Austrian Unemployment Agency; Frauen in Tech-Rollen unterrepräsentiert: '21% of leaders are women, 4% are women of colour, 1% are Black women' (S. 9)

### Evidenz 5

Zentral: 'Embed inclusivity and cultural sensitivity: Prioritise localised solutions tailored to specific cultural or regional needs. Plan for marginal use cases, allocating resources to support the most vulnerable groups.' Fallstudien zu marginalisierten Gruppen: Migranten, Racial minorities, disabled persons, single-parent households

### Evidenz 6

Explizit feministische Perspektive: Referenzen auf Kimberlé Crenshaw (Intersectionality Begriffsschöpferin), Suresh et al. 'Towards Intersectional Feminist and Participatory ML' (2022), D'Ignazio & Bhargava zu Data Justice, Partizipatorischer ML-Ansatz, Community-basierte Ansätze zur Gegendatensammlung (Feminicide Counterdata Collection)

### Evidenz 7

Fairness definiert als: 'designing systems that promote equitable outcomes for all individuals, regardless of identity.' Kritik an single-axis Fairness-Ansätzen: 'Many approaches to AI fairness focus on addressing just one type of bias at a time, such as gender or race. However, this approach ignores the complex ways biases overlap' (S. 8)

## Assessment-Relevanz

**Domain Fit:** Hochgradig relevant für Schnittstelle KI/Fairness/Intersektionalität: Das Toolkit adressiert explizit, wie algorithmische Systeme marginalisierte Gruppen in ihrer Mehrfachvulnerabilität treffen. Während es nicht direkt auf Soziale Arbeit fokussiert, sind deren Zielgruppen (vulnerable Populationen, Migrant:innen, Familien in Wohlfahrtssystemen) zentrale Fallbeispiele.

**Unique Contribution:** Das Toolkit verbindet intersektionale Theorie (Crenshaw) mit KI-Praxis durch konkrete, rollenspezifische Strategien für verschiedene Organisationstypen (Entwickler, Führung, Governance, HR) und illustriert Konzepte mit EU-basierten Fallstudien.

**Limitations:** Nicht angegeben; Document ist Toolkit und nicht Forschungspaper mit expliziten Methodenlimitationen. Datengrundlage der Interviews/Fokusgruppen (n, Sampling, Analyse) ist nicht dokumentiert.

**Target Group:** Primär: Industry Professionals, AI Developers, Executives, Governance Teams, HR Professionals in Unternehmen. Sekundär: Policymakers, Public Sector Leaders, Civil Society Organisations. Tertär: Alle Stakeholder mit Interesse an fairer KI und Intersektionalität (Lehrende, Nutzer:innen von KI-Systemen)

## Schlüsselreferenzen

- [[Crenshaw_Kimberlé_1989]] - Demarginalizing the Intersection of Race and Sex (Foundational intersectionality concept)
- [[Buolamwini_Gebru_2018]] - Gender Shades: Intersectional Accuracy Disparities in AI
- [[Suresh_et_al_2022]] - Towards Intersectional Feminist and Participatory ML: Case Study in Feminicide Counterdata Collection
- [[DIgnazio_Klein_2020]] - Data Feminism
- [[Howard_Ayanna_2021]] - Real Talk: Intersectionality and AI
- [[Ulnicane_Inga_2024]] - Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action
- [[Amnesty_International_2021]] - Xenophobic Machines: The Dutch Child Benefit Scandal
- [[Eticas_Foundation_2024]] - Automating (In)Justice: An Adversarial Audit of RisCanvi
- [[Dastin_Jeffrey_2018]] - Amazon scraps secret AI recruiting tool that showed bias against women
- [[UNESCO_2020]] - Artificial Intelligence and Gender Equality
