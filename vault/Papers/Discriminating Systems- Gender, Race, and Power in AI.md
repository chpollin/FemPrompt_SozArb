---
title: "Discriminating Systems: Gender, Race, and Power in AI"
authors:
  - S.M. West
  - M. Whittaker
  - K. Crawford
year: 2023
type: report
doi: 
url: "https://ainowinstitute.org/wp-content/uploads/2023/04/discriminatingsystems.pdf"
tags:
  - paper
llm_decision: Include
llm_confidence: 0.95
llm_categories:
  - KI_Sonstige
  - Bias_Ungleichheit
  - Gender
  - Diversitaet
  - Feministisch
  - Fairness
human_decision: Exclude
human_categories:
  - KI_Sonstige
  - Bias_Ungleichheit
  - Gender
  - Diversitaet
agreement: disagree
---

# Discriminating Systems: Gender, Race, and Power in AI

## Transformation Trail

### Stufe 1: Extraktion & Klassifikation (LLM)

**Extrahierte Kategorien:** KI_Sonstige, Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness
**Argumente:** 5 extrahiert

### Stufe 3: Verifikation (LLM)

| Metrik | Score |
|--------|-------|
| Completeness | 92 |
| Correctness | 98 |
| Category Validation | 95 |
| **Overall Confidence** | **95** |

### Stufe 4: Assessment

**LLM:** Include (Confidence: 0.95)
**Human:** Exclude

**Kategorie-Vergleich (bei Divergenz):**

| Kategorie | Human | LLM | Divergent |
|-----------|-------|-----|----------|
| AI_Literacies | Nein | Nein |  |
| Generative_KI | Nein | Nein |  |
| Prompting | Nein | Nein |  |
| KI_Sonstige | Ja | Ja |  |
| Soziale_Arbeit | Nein | Nein |  |
| Bias_Ungleichheit | Ja | Ja |  |
| Gender | Ja | Ja |  |
| Diversitaet | Ja | Ja |  |
| Feministisch | Nein | Ja | X |
| Fairness | Nein | Ja | X |

> Siehe [[Divergenz West_2023_Discriminating_Systems_Gender,_Race,_and_Power_in]] fuer detaillierte Analyse


## Key Concepts

- [[Algorithmic Bias]]
- [[Algorithmic Fairness]]
- [[Intersectional Feminism in AI]]

## Wissensdokument

# Discriminating Systems: Gender, Race, and Power in AI

## Kernbefund

Die Diversity-Krise in der AI-Industrie und der Bias in AI-Systemen sind zwei Manifestationen desselben Problems struktureller Ungleichheit. Die herrschende Pipeline-Fokus-Strategie hat nach Jahrzehnten kein substantielles Fortschritt gebracht, weil sie tiefere Probleme wie Workplace-Kultur, Machtasymmetrien und Ausschlusslogiken ignoriert.

## Forschungsfrage

Wie sind die Diversity-Krisen in der AI-Industrie und die Probleme von Bias in AI-Systemen miteinander verflochten, und welche strukturellen Faktoren erklären diese Ungleichheiten?

## Methodik

Theoretisch/Review – umfassende Literaturanalyse über ein Jahr mit Perspektiven aus Informatik, Sozialwissenschaften und Geisteswissenschaften; systematische Analyse von Industrie-Statistiken, Fallstudien zu AI-Systemen und Dokumentation von Workplace-Bewegungen
**Datenbasis:** Sekundäranalyse: 18% Frauen an führenden AI-Konferenzen, >80% männliche AI-Professoren, 10-15% Frauen in AI-Research in Top-Tech-Unternehmen, 2.5-4% schwarze Arbeitnehmer bei Google/Facebook/Microsoft; keine Daten zu trans Arbeitern

## Hauptargumente

- Die Diversity-Krise ist nicht nur ein Geschlechter-, sondern ein umfassendes Macht- und Machtverhältnis-Problem, das durch intersektionale Perspektiven verstanden werden muss, da ausschließliche 'Frauen-in-Tech'-Fokussierung weiße Frauen bevorzugt und andere marginalisierte Gruppen ignoriert.
- AI-Systeme werden in extrem homogenen Räumen (weiß, wohlhabend, männlich, technisch orientiert) entwickelt, was dazu führt, dass bestehende gesellschaftliche Diskriminierungsmuster in den Systemen reproduziert und verstärkt werden, was zwei eng verflochtene Probleme bildet.
- Der bisherige Fokus auf technische Bias-Lösungen ist unzureichend und kann aktiv schaden – besonders wenn Systeme wie Gesichtserkennung in polizeilicher Überwachung missbraucht werden und marginalisierte Communities ohne Zustimmung für Datenbeschaffung ausgebeutet werden.
- Biologischer Determinismus re-emergt sowohl als Gegenbewegung gegen Diversity-Forderungen als auch in AI-Systemdesign selbst, was historische Rechtfertigungslogiken für Ungleichheit revitalisiert und automatisiert.
- Einige AI-Systeme (z.B. Geschlechtserkennung, Sexualitätserkennung) sollten möglicherweise gar nicht entwickelt werden; 'Diversifizierung' von Datensätzen kann harmful systems nur legitimieren statt sie in Frage zu stellen.

## Kategorie-Evidenz

### Evidenz 1

Fokus auf algorithmische Klassifikationssysteme, Gesichtserkennung, Predictive Policing, Chatbots und kommerzialisierte AI-Systeme im Allgemeinen, nicht generativ oder spezifisch auf LLMs fokussiert.

### Evidenz 2

Direkter Bezug zu marginalisieren Communities, Gerechtigkeit, Überwachung von Armen und Farbigen, polizeiliche Systeme, die Soziale Arbeit und Care-Kontexte betreffen: 'AI systems are increasingly tasked with sorting those who are worthy from those who are not - be it for school admission, release from prison, or job interviews.'

### Evidenz 3

Zentrale These: 'Image recognition technologies miscategorize black faces, sentencing algorithms discriminate against black defendants, chatbots easily adopt racist and misogynistic language... such bias mirrors and replicates existing structures of inequality in society.'

### Evidenz 4

Explizit Geschlechterperspektive durchgehend: 'women comprise only 15% of AI research staff at Facebook and just 10% at Google'; kritische Analyse von Geschlechts-Klassifikationssystemen: 'such systems functionally understand gender as an essential, biological, and binary identity that can be 'detected' and affirmed through the lens of a commercialized technical system.'

### Evidenz 5

Intersektionale Analyse zentral: 'The overwhelming focus on 'women in tech' is too narrow and likely to privilege white women over others. We need to acknowledge how the intersections of race, gender, and other identities and attributes shape people's experiences with AI.' Kritik an 'colorless diversity' ohne Berücksichtigung von race.

### Evidenz 6

Explizite Nutzung intersektionaler feministischer Perspektiven (Erica Joy Baker's 'colorless diversity', Gender Shades von Buolamwini & Gebru als Beispiel intersektionaler Forschung); Referenzen zu Power, Representation und struktureller Diskriminierung aus feministischer Perspektive; Autoren-Positionierung: 'As white women, we don't experience the intersections of oppression in the same way that people of color and gender minorities... the silence of those who experience privilege in this space is the problem.'

### Evidenz 7

Kritik an und Diskussion von Fairness-Ansätzen: 'the AI research community has begun addressing the problem of bias by building on a body of work on fairness, accountability, and transparency... focused on adjusting AI systems in ways that produce a result deemed 'fair' by one of various mathematical definitions'; Argument, dass technische Fairness-Lösungen allein unzureichend sind.

## Assessment-Relevanz

**Domain Fit:** Hochgradig relevant für die Schnittstelle AI/Soziale Arbeit/Gender: Das Paper verbindet strukturelle Ungleichheiten in AI-Entwicklung direkt mit gesellschaftlichen Auswirkungen auf marginalisierte Communities und zeigt, wie algorithmische Systeme Soziale Arbeit-Kontexte (Polizei, Gesundheit, Bildung) durchdringen und Ungleichheiten verstärken.

**Unique Contribution:** Pionierbeitrag der integrierten Analyse von Workforce-Diversity und System-Bias als zwei Seiten desselben Machtproblems, kombiniert mit intersektionaler, feministischer Kritik an bisherigen technizistischen und Pipeline-fokussierten Lösungsansätzen.

**Limitations:** Keine primären empirischen Daten erhoben; Fokus liegt auf USA/Westliche Tech-Industrie; begrenzte Analyse konkreter Interventionseffekte bereits implementierter Diversity-Maßnahmen; Sekundäranalyse aktueller zu 2019.

**Target Group:** AI-Entwickler und -Forscher, Tech-Industrie-Leadership, Policymaker, Sozialarbeiter in Kontexten algorithimischer Systeme, Geschlechter- und Diversity-Fachleute, Aktivisten und Worker-organisationen in Tech, Akademiker in AI/HCI/Gender Studies

## Schlüsselreferenzen

- [[Buolamwini_Gebru_2018]] - Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification
- [[Gebru_Morgenstern_et_al_2018]] - Datasheets for Datasets
- [[Baker_Erica_Joy_2015]] - #FFFFFF Diversity
- [[Gould_Stephen_Jay_1981]] - The Mismeasure of Man
- [[Crawford_Calo_2016]] - There is a Blind Spot in AI Research
- [[Obermeyer_Mullainathan_2019]] - Dissecting Racial Bias in an Algorithm that Guides Health Decisions for 70 million people
- [[Keyes_Os_2018]] - The Misgendering Machines: Trans/HCI Implications of Automatic Gender Recognition
- [[Richardson_Schultz_Crawford_2019]] - Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice
- [[Samudzi_Zoé_2019]] - Bots Are Terrible at Recognizing Black Faces. Let's Keep it That Way.
- [[DIgnazio_Klein_2020]] - Data Feminism (referenced implicitly through intersectional framework)
