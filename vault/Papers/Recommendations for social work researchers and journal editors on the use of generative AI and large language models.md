---
title: Recommendations for social work researchers and journal editors on the use of generative AI and large language models
authors:
  - B. G. Victor
  - R. L. Sokol
  - L. Goldkind
  - B. E. Perron
year: 2023
type: journalArticle
doi: 10.1086/726021
url: "https://doi.org/10.1086/726021"
tags:
  - paper
llm_decision: Include
llm_confidence: 0.95
llm_categories:
  - AI_Literacies
  - Generative_KI
  - Prompting
  - Soziale_Arbeit
human_decision: Include
human_categories:
  - Generative_KI
  - Prompting
  - KI_Sonstige
  - Soziale_Arbeit
  - Bias_Ungleichheit
  - Gender
agreement: agree
---

# Recommendations for social work researchers and journal editors on the use of generative AI and large language models

## Transformation Trail

### Stufe 1: Extraktion & Klassifikation (LLM)

**Extrahierte Kategorien:** AI_Literacies, Generative_KI, KI_Sonstige, Soziale_Arbeit, Bias_Ungleichheit, Diversitaet, Fairness
**Argumente:** 3 extrahiert

### Stufe 3: Verifikation (LLM)

| Metrik | Score |
|--------|-------|
| Completeness | 92 |
| Correctness | 98 |
| Category Validation | 95 |
| **Overall Confidence** | **94** |

### Stufe 4: Assessment

**LLM:** Include (Confidence: 0.95)
**Human:** Include

## Wissensdokument

# Recommendations for Social Work Researchers and Journal Editors on the Use of Generative AI and Large Language Models

## Kernbefund

Generative KI und LLMs bieten sowohl transformatives Potenzial als auch erhebliche Risiken für die Sozialarbeitsforschung; es werden spezifische Empfehlungen für Forscher und Herausgeber entwickelt, die Chancen maximieren und Risiken (Bias, Datenschutz, Qualität) minimieren.

## Forschungsfrage

Wie sollten Forscher und Zeitschriftenredakteure in der Sozialen Arbeit generative KI und Large Language Models verantwortungsvoll und ethisch in der Forschung einsetzen?

## Methodik

Theoretisch/Review - Disruptive-Disrupting Framework kombiniert mit systematischer Analyse bestehender Richtlinien von Nature, Science, JAMA, Elsevier, COPE und PNAS.
**Datenbasis:** nicht empirisch - konzeptionelle und guideline-basierte Analyse mit praktischen Fallbeispielen

## Hauptargumente

- Generative KI und LLMs haben disruptives Potenzial zur Verbesserung von Literaturrecherchen, qualitativer und quantitativer Datenanalyse sowie Manuskriptwriting in der Sozialarbeitsforschung, können aber durch Automatisierung wesentlicher Forschungsschritte die Qualität gefährden.
- Diese Technologien sind gleichzeitig 'disrupting' in ihrer Fähigkeit, Bias zu reproduzieren, Datenschutzverletzungen zu verursachen, Falschinformationen zu generieren und die Reproduzierbarkeit von Ergebnissen zu gefährden, was spezifische ethische Kontrollen erfordert.
- Forscher müssen vor Einsatz dieser Technologien tiefes Verständnis ihrer Konstruktion, ethischen Implikationen und Limitationen entwickeln und volle Verantwortung für KI-generierte Inhalte übernehmen; Herausgeber müssen robuste Quality-Control- und Transparenzverfahren implementieren.

## Kategorie-Evidenz

### Evidenz 1

Requirement for researchers to 'understand how AI tools are constructed, including a sense of their underlying algorithms, data sources, and training processes' and emphasis on 'continuous learning and development' to stay current with AI tools and techniques.

### Evidenz 2

Extensive focus on ChatGPT, LLMs, and generative AI capabilities: 'Generative AI uses deep learning and natural language processing to understand and respond to human conversation' and detailed discussion of applications in literature reviews, text generation, and data analysis.

### Evidenz 3

Discussion of broader AI applications including sentiment analysis, data mining, image recognition, and algorithmic decision-making tools beyond pure generative models.

### Evidenz 4

Entire paper addresses social work research specifically: 'the social work profession will need to iteratively adapt and revise guidelines' and explicit focus on 'vulnerable and marginalized groups' in social work research contexts.

### Evidenz 5

Critical discussion of how 'LLMs are trained on large amounts of data, the models will reproduce biases inherent in the data, such as biases that result from the use of texts with racial and gender biases' and concern about 'exacerbate existing inequalities and biases in academic research.'

### Evidenz 6

Recognition that guidelines 'may not adequately address the unique ethical issues that require careful consideration within social work research contexts, such as research involving vulnerable and marginalized groups' and discussion of English-language barriers for non-English speaking researchers.

### Evidenz 7

Emphasis on 'fair and rigorous peer-review process' and requirements that research 'minimize them to ensure nondiscriminatory outcomes and reporting' with attention to algorithmic fairness in model design and application.

## Assessment-Relevanz

**Domain Fit:** Hochgradig relevant an der Schnittstelle KI/Soziale Arbeit: Das Paper adressiert direkt die Anwendung generativer KI in Sozialarbeitsforschung und -lehre und berücksichtigt spezifische ethische Anforderungen der Disziplin (vulnerable Gruppen, Equity). Es fehlt jedoch eine explizit intersektionale oder feministische Perspektive.

**Unique Contribution:** Erste spezialisierte Richtlinien für den Einsatz von generativer KI in der Sozialarbeitsforschung, die das disruptive-disrupting Framework kombiniert und sowohl Chancen als auch Risiken systematisch analysiert.

**Limitations:** Rein konzeptioneller Ansatz ohne empirische Validierung der Empfehlungen; begrenzte Diskussion geschlechterspezifischer oder intersektionaler Auswirkungen; Fokus primär auf anglophone Kontexte und High-Impact-Journals.

**Target Group:** Sozialarbeitsforschende, Zeitschriftenherausgeber und Peer Reviewer in der Sozialen Arbeit; Lehrende in Sozialarbeitsausbildung; Policy-Maker und Fachverbände der Sozialen Arbeit; sekundär relevant für KI-Entwickler mit Interesse an Anwendungen im sozialen Sektor

## Schlüsselreferenzen

- [[Alkaissi_McFarlane_2023]] - Artificial hallucinations in ChatGPT
- [[Gordon_2023]] - Large language models are biased
- [[Felten_Raj_Seamans_2023]] - How will language modelers like ChatGPT affect occupations and industries
- [[Danneels_2004]] - Disruptive technology reconsidered
- [[Committee_on_Publication_Ethics_COPE_2023]] - Authorship and AI tools
- [[Nature_2023]] - Tools such as ChatGPT threaten transparent science
- [[Flanagin_et_al_2023]] - Nonhuman 'authors' and implications for scientific publication
- [[Singer_Báez_Rios_2023]] - AI creates the message: Integrating AI language learning models into social work education
- [[Duracinsky_et_al_2017]] - Barriers to publishing in biomedical journals perceived by French researchers
