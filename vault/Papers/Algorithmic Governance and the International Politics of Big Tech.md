---
title: Algorithmic Governance and the International Politics of Big Tech
authors:
  - S. Srivastava
year: 2024
type: journalArticle
url: https://www.cambridge.org/core/journals/perspectives-on-politics/article/algorithmic-governance-and-the-international-politics-of-big-tech/3C04908735A5F2EE8A70AFED647741FB
tags:
  - paper
  - feminist-ai
  - bias-research
date_added: 2026-02-22
date_modified: 2026-02-22
bias_types:
  - Intersectional Accuracy
  - Intersectionality
mitigation_strategies:
  - Intersectional Accuracy
llm_decision: Exclude
llm_confidence: 0.85
llm_categories:
  - KI_Sonstige
  - Bias_Ungleichheit
human_decision: Unclear
human_categories:
  - KI_Sonstige
  - Bias_Ungleichheit
  - Diversitaet
  - Fairness
agreement: disagree
---

# Algorithmic Governance and the International Politics of Big Tech

## Abstract

Structural analysis examines how Big Tech corporations exercise "entrepreneurial private authority" through algorithmic governance systems challenging state sovereignty. Moves beyond individual-focused approaches to analyze how technology firms exercise instrumental, structural, and discursive power globally. Critiques approaches focusing on individual user agency or technical fixes, examining instead how "Big Tech's algorithmic governance incentivizes 'information pollution'" and creates systemic power imbalances.

## Assessment

**LLM Decision:** Exclude (Confidence: 0.85)
**LLM Categories:** KI_Sonstige, Bias_Ungleichheit
**Human Decision:** Unclear
**Human Categories:** KI_Sonstige, Bias_Ungleichheit, Diversitaet, Fairness
**Agreement:** Disagree

## Key Concepts

### Bias Types
- [[Intersectional Accuracy]]
- [[Intersectionality]]

### Mitigation Strategies
- [[Intersectional Accuracy]]

## Full Text

---
title: "Algorithmic Governance: Gender Bias in AI-Generated Policymaking?"
authors: ["Dialekti Athina Voutyrakou", "Constantine Skordoulis"]
year: 2025
type: journalArticle
language: en
processed: 2026-02-05
source_file: Voutyrakou_2025_Algorithmic_Governance_Gender_Bias_in.md
confidence: 95
---

# Algorithmic Governance: Gender Bias in AI-Generated Policymaking?

## Kernbefund

KI-Tools integrieren genderspezifische Bedürfnisse (Menstruationshygiene, Sicherheit, Kinderbetreuung, thermischer Komfort) nur dann in Politikempfehlungen, wenn Gender explizit im Prompt erwähnt wird; ansonsten wird eine androzentristische Standardperspektive angewandt.

## Forschungsfrage

Berücksichtigen populäre KI-Tools wie ChatGPT und Microsoft Copilot Geschlechteraspekte bei der Erstellung von Politikempfehlungen, sowohl wenn Gender explizit erwähnt wird als auch wenn nicht?

## Methodik

Mixed-Methods: Vier experimentelle Szenarien mit wiederholten Prompts in zwei KI-Systemen (ChatGPT GPT-4 und Microsoft Copilot), Häufigkeitsanalyse und qualitative Kodierung der Ergebnisse.
**Datenbasis:** Vier experimentelle Szenarien: Restroom Design, Schneeräumpolitik, Bürotemperaturregelung, Business-Travel-Ausgaben; jeweils mit Varianten (unspezifisch, männlich, weiblich, nicht-binär); Tests in ChatGPT und Microsoft Copilot

## Hauptargumente

- Gender-Bias in KI-Tools ist nicht primär ein Trainingsdaten-Problem, sondern ein strukturelles Designproblem, das eine androzentristische Perspektive als 'neutral' kodifiziert und genderspezifische Bedürfnisse nur als Sonderfälle behandelt.
- Die angenommene Neutralität von KI-Systemen verschleiert tatsächlich eine male-default-Logik, die körperliche Unterschiede (Stoffwechselrate, Menstruation), Sorgearbeit und Sicherheitsbedenken systematisch ausblendet, wenn nicht explizit aufgefordert.
- Faire und genderbewusste Algorithmen-Governance erfordert eine proaktive Integration von feministischer Ethik (Care-Ethik, Capabilities Approach) und intersektionalen Perspektiven in Algorithmen-Design, nicht nur nachträgliche Korrektionen.

## Kategorie-Evidenz

### Evidenz 1

Diskussion darüber, dass Nutzer als 'aktive Teilnehmer' in den Bias-Feedback-Loop beitragen und dass 'Prompt Literacy und Public Education' notwendig sind: 'users themselves contribute to the bias feedback loop, making them active participants rather than passive recipients'

### Evidenz 2

Fokus auf zwei generative KI-Systeme: 'We tested these experiments in two different AI tools, namely ChatGPT (GPT-4) and Microsoft Copilot'

### Evidenz 3

Explizite Variation der Prompt-Struktur mit/ohne Gender-Erwähnungen: 'Each scenario includes a gender-neutral prompt, a male-specific version, and a female-specific version and a non-binary-specific version'

### Evidenz 4

Analyse von LLMs und NLP-Systemen als Teil von algorithmischen Entscheidungssystemen: 'Large language models such as ChatGPT or Microsoft Copilot add layers of complexity to understanding gender bias in AI'

### Evidenz 5

Zentrale These der strukturellen Benachteiligung durch androzentrische Algorithmen-Designs: 'the supposed neutrality of AI tools actually reproduces an androcentric norm' und 'a technological design that equates neutral with male'

### Evidenz 6

Expliziter Fokus auf Gender-Bias in vier verschiedenen Policy-Kontexten und auf geschlechtsspezifische Bedürfnisse: 'gender bias...arises when systems consistently advantage or disadvantage individuals based on gender'

### Evidenz 7

Intersektionale Perspektive und Analyse nicht-binärer Identitäten: 'The non-binary scenario further illustrates this pattern: inclusivity features such as gender-neutral policies and identity-affirming practices were activated only upon explicit mention'

### Evidenz 8

Explizite Verwendung feministischer Theorien (Tronto's Care-Ethik, Grosz' embodied subjectivity, Bobel on menstrual stigma, Koskela on women's spatial confidence): 'Drawing on Joan Tronto's ethics of care and Iris Marion Young's concept of justice as inclusion, this perspective argues that fair governance must recognize and respond to differentiated social positions'

### Evidenz 9

Kritik an oberflächlichen Fairness-Ansätzen und Forderung nach kontextualisierten Fairness-Konzepten: 'fairness interventions must align with real-world harm mitigation and not merely with metric optimization'

## Assessment-Relevanz

**Domain Fit:** Hoch relevant für die Schnittstelle KI und Gender-Gerechtigkeit; untersucht konkret, wie KI-Systeme in Governance und Policy-Making androzentristische Bias perpetuieren. Trägt zu dringend benötigten empirischen Erkenntnissen über generative KI im öffentlichen Sektor bei.

**Unique Contribution:** Erste systematische experimentelle Studie, die nachweist, dass KI-Tools Gender-Berücksichtigung nicht von sich aus vornehmen, sondern diese als 'Sonderfälle' behandeln, was eine fundamentale Designfrage aufwirft.

**Limitations:** Limitierte Stichprobe (nur zwei KI-Tools); keine Analyse der Unterschiede zwischen GPT-4 und Copilot im Detail; keine Nutzerstudie zur Wahrnehmung oder Konsequenzen dieser Biases.

**Target Group:** Policy-Maker und Governance-Experten, KI-Ethik-Forscher, feministische Technologiekritiker, KI-Entwickler und -Designer, Sozialwissenschaftler, Organisationen der Gleichstellungsarbeit

## Schlüsselreferenzen

- [[Tronto_Joan_1993]] - Moral Boundaries: A Political Argument for an Ethics of Care
- [[DIgnazio_Catherine_Klein_Lauren_F_2020]] - Data Feminism
- [[CostanzaChock_Sasha_2020]] - Design Justice: Community-Led Practices to Build the Worlds We Need
- [[West_Sarah_M_Whittaker_Meredith_Crawford_Kate_2019]] - Discriminating Systems: Gender, Race and Power in AI
- [[Buolamwini_Joy_Gebru_Timnit_2018]] - Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification
- [[Crenshaw_Kimberlé_1991]] - Mapping the Margins: Intersectionality, Identity Politics, and Violence Against Women of Color
- [[Sen_Amartya_1999]] - Development as Freedom
- [[Nussbaum_Martha_C_2011]] - Creating Capabilities: The Human Development Approach
- [[Grosz_Elizabeth_A_1994]] - Volatile Bodies
- [[Bobel_Chris_Ed_2019]] - The Managed Body: Developing Girls and Menstrual Health in the Global South
