---
title: Algorithms, artificial intelligence and discrimination
authors:
  - O. Lund
year: 2025
type: report
doi: 
url: "https://ldo.no/content/uploads/2025/05/Algorithms-artificial-intelligence-and-discrimination-report.pdf"
tags:
  - paper
llm_decision: Include
llm_confidence: 0.85
llm_categories:
  - KI_Sonstige
  - Bias_Ungleichheit
  - Fairness
human_decision: Exclude
human_categories:
  - KI_Sonstige
  - Fairness
agreement: disagree
---

# Algorithms, artificial intelligence and discrimination

## Transformation Trail

### Stufe 1: Extraktion & Klassifikation (LLM)

**Extrahierte Kategorien:** KI_Sonstige, Soziale_Arbeit, Bias_Ungleichheit, Diversitaet, Fairness
**Argumente:** 3 extrahiert

### Stufe 3: Verifikation (LLM)

| Metrik | Score |
|--------|-------|
| Completeness | 92 |
| Correctness | 98 |
| Category Validation | 95 |
| **Overall Confidence** | **93** |

### Stufe 4: Assessment

**LLM:** Include (Confidence: 0.85)
**Human:** Exclude

**Kategorie-Vergleich (bei Divergenz):**

| Kategorie | Human | LLM | Divergent |
|-----------|-------|-----|----------|
| AI_Literacies | Nein | Nein |  |
| Generative_KI | Nein | Nein |  |
| Prompting | Nein | Nein |  |
| KI_Sonstige | Ja | Ja |  |
| Soziale_Arbeit | Nein | Nein |  |
| Bias_Ungleichheit | Nein | Ja | X |
| Gender | Nein | Nein |  |
| Diversitaet | Nein | Nein |  |
| Feministisch | Nein | Nein |  |
| Fairness | Ja | Ja |  |

> Siehe [[Divergenz Lund_2025_Algorithms,_artificial_intelligence_and]] fuer detaillierte Analyse


## Key Concepts

- [[Algorithmic Discrimination]]
- [[Algorithmic Fairness]]
- [[Algorithmic Transparency and Explainability]]
- [[Intersectional Discrimination]]
- [[Proxy Discrimination]]

## Wissensdokument

# Algorithms, Artificial Intelligence and Discrimination: An Analysis of the Equality and Anti-Discrimination Act's Possibilities and Limitations

## Kernbefund

Das EAD Act ist zwar auf algorithmische Diskriminierung anwendbar, weist aber erhebliche Lücken auf. Eine spezifische Gesetzesbestimmung zu algorithmischer Differenzialbehandlung ist erforderlich, um präzisere Schutzstandards etablieren zu können und rechtliche Sicherheit für alle beteiligten Akteure zu schaffen.

## Forschungsfrage

Wie kann das norwegische Gleichstellungs- und Antidiskriminierungsgesetz (EAD Act) algorithmische Diskriminierung effektiv adressieren, und welche legislativen Anpassungen sind notwendig?

## Methodik

Theoretisch-rechtliche Analyse. Systematische Untersuchung der Anwendbarkeit und Grenzen des geltenden Antidiskriminierungsrechts auf algorithmische Systeme unter Berücksichtigung von EU/EEA-Richtlinien, dem AI Act und der GDPR.
**Datenbasis:** nicht empirisch; basiert auf Rechtsanalyse, Fallrecht (EuGH, norwegisches Anti-Diskriminierungstribunal) und legislativen Dokumenten

## Hauptargumente

- Algorithmische und KI-Systeme werden zunehmend in gesellschaftskritischen Bereichen (Gesundheit, Wohnen, Kreditvergabe, Polizeiarbeit) eingesetzt und können individuelle Positionen erheblich beeinflussen, daher sollte der Schutz vor Diskriminierung nicht primär auf den Arbeitsmarkt beschränkt bleiben, sondern alle Sektoren umfassen.
- Proxy-Diskriminierung und die Verwendung von Faktoren, die eng mit geschützten Charakteristiken verknüpft sind, erschweren die klassische rechtliche Unterscheidung zwischen direkter und indirekter Diskriminierung und erfordern spezifische algorithmische Kriterien für Zulässigkeit.
- Die Beweislastregeln im Diskriminierungsrecht sind zentral für die praktische Durchsetzung; Inspiration kann aus der Lohngleichheitsrichtlinie (Pay Transparency Directive) gewonnen werden, die Transparenzverpflichtungen mit Beweislastregeln verknüpft und das 'Black-Box'-Problem adressiert.

## Kategorie-Evidenz

### Evidenz 1

Der Report analysiert systematisch algorithmische Systeme und KI-Systeme in regulatorischem Kontext: 'The AI Act specifies that existing EU regulation on discrimination is not impacted by the Act.' Fokus auf Algorithmen, maschinelles Lernen und Entscheidungsunterstützungssysteme.

### Evidenz 2

Der Report adressiert Wohlfahrtsverwaltung und soziale Dienste explizit: 'algorithmic systems may significantly impact the position of individuals, as they can affect individuals' access to essential services and rights, such as healthcare, housing or the ability to obtain insurance.' Dies betrifft Soziale Arbeit in Kontext von Wohlfahrtsverwaltung und Zugang zu sozialen Dienstleistungen.

### Evidenz 3

Zentrales Thema des gesamten Reports: 'Discrimination may occur through algorithmic systems' emphasis on proxy factors ('proxy discrimination'). The use of proxy factors triggers new issues, particularly in relation to the prohibition against indirect discrimination.' Analyse struktureller Diskriminierungsmechanismen durch Algorithmen.

### Evidenz 4

Report thematisiert intersektionale Diskriminierung explizit: 'differential treatment also can be found to have occurred against a group within a larger protected group' und 'Section 6 of the Equality and Anti-Discrimination Act's inclusion of a prohibition against multiple forms of discrimination.' Schutz marginalisierter Gruppen und Minderheiten.

### Evidenz 5

Report analysiert Fairness-Aspekte algorithmischer Systeme: 'a requirement might be established that the differential treatment be suitable and necessary for the realization of the data system's purpose' und diskutiert Fairness-Kriterien für algorithmische Systeme systematisch in Kapitel 4.6.

## Assessment-Relevanz

**Domain Fit:** Hochrelevant für die Schnittstelle KI und Soziale Arbeit: Der Report zeigt, wie algorithmische Systeme in wohlfahrtsverwaltunglichen Kontexten zu Diskriminierung führen können und analysiert regulatorische Mechanismen zur Prävention. Dies ist zentral für sozialarbeiterische Ethik und Praxis.

**Unique Contribution:** Der Report leistet erstmalig eine umfassende rechtliche Analyse der Anwendbarkeit norwegischen Antidiskriminierungsrechts auf Algorithmen und KI-Systeme unter EU/EEA-Perspektive und entwickelt konkrete legislative Reformvorschläge (insb. zu algorithmischer Differenzialbehandlung und Beweislastregeln).

**Limitations:** Rein rechtlich-theoretische Analyse ohne empirische Fallstudien oder Stakeholder-Perspektiven von Betroffenen; Fokus auf Norwegen/EU-Recht, daher begrenzte internationale Generalisierbarkeit.

**Target Group:** Policymaker und Legislatoren (Schwerpunkt), Rechtsanwälte und Richter im Antidiskriminierungsrecht, KI-Regulatoren und Compliance-Officer, Interessenorganisationen und NGOs, Wohlfahrtsverwaltungen und öffentliche Behörden, sekundär Sozialarbeiter und Social-Work-Fachkräfte in regulatorischen Kontexten

## Schlüsselreferenzen

- [[Gerards_Janneke_Xenidis_Raphaële_2020]] - Algorithmic Discrimination in Europe: Challenges and Opportunities for Gender Equality and Non-Discrimination Law
- [[Hellum_Anne_Strand_Vibeke_Blaker_2022]] - Likestillings- og diskrimineringsrett (Equality and Non-Discrimination Law)
- [[Wachter_Sandra_Mittelstadt_Brent_Russell_Chris_2021]] - Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI
- [[Borgesius_Frederik_Zuiderveen_2018]] - Discrimination, Artificial Intelligence, and Algorithmic Decision-Making
- [[Datta_Anupam_et_al_2017]] - Proxy Discrimination in Data-Driven Systems
- [[Xenidis_Raphaële_2020]] - Tuning EU Equality Law to Algorithmic Discrimination: Three Pathways to Resilience
- [[Hauglid_Mathias_Karlsen_2024]] - Bias and Discrimination in Clinical Decision Support Systems Based on Artificial Intelligence
- [[Schartum_Dag_Wiese_2019]] - Digitalisierung of Public Administration: From Legal Text to Program Code
- [[Strümke_Inga_Slavkovik_Marija_Stachl_Clemens_2023]] - Against Algorithmic Exploitation of Human Vulnerabilities
