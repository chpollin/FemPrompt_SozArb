---
title: "Exploring opportunities and risks in decision support technologies for social workers: An empirical study in the field of disabled people's services"
authors:
  - D. Schneider
  - A. Maier
  - P. Cimiano
  - U. Seelmeyer
year: 2022
type: journalArticle
doi: 10.1093/bjsw/bcab262
url: 
tags:
  - paper
llm_decision: Include
llm_confidence: 0.95
llm_categories:
  - KI_Sonstige
  - Soziale_Arbeit
  - Bias_Ungleichheit
  - Fairness
human_decision: Exclude
human_categories:
  - KI_Sonstige
  - Soziale_Arbeit
agreement: disagree
---

# Exploring opportunities and risks in decision support technologies for social workers: An empirical study in the field of disabled people's services

## Transformation Trail

### Stufe 1: Extraktion & Klassifikation (LLM)

**Extrahierte Kategorien:** AI_Literacies, KI_Sonstige, Soziale_Arbeit, Bias_Ungleichheit, Diversitaet, Fairness
**Argumente:** 3 extrahiert

### Stufe 3: Verifikation (LLM)

| Metrik | Score |
|--------|-------|
| Completeness | 88 |
| Correctness | 92 |
| Category Validation | 87 |
| **Overall Confidence** | **89** |

### Stufe 4: Assessment

**LLM:** Include (Confidence: 0.95)
**Human:** Exclude

**Kategorie-Vergleich (bei Divergenz):**

| Kategorie | Human | LLM | Divergent |
|-----------|-------|-----|----------|
| AI_Literacies | Nein | Nein |  |
| Generative_KI | Nein | Nein |  |
| Prompting | Nein | Nein |  |
| KI_Sonstige | Ja | Ja |  |
| Soziale_Arbeit | Ja | Ja |  |
| Bias_Ungleichheit | Nein | Ja | X |
| Gender | Nein | Nein |  |
| Diversitaet | Nein | Nein |  |
| Feministisch | Nein | Nein |  |
| Fairness | Nein | Ja | X |

> Siehe [[Divergenz Schneider_2022_Exploring_opportunities_and_risks_in_decision]] fuer detaillierte Analyse


## Key Concepts

- [[Algorithmic Fairness in Vulnerable Populations]]
- [[Data Literacy in Professional Practice]]

## Wissensdokument

# Exploring Opportunities and Risks in Decision Support Technologies for Social Workers: An Empirical Study in the Field of Disabled People's Services

## Kernbefund

DSSs mit Visualisierungen der Klient*innen-Entwicklung werden als unterstützend wahrgenommen; es besteht Bedarf für partizipative Entscheidungsfindung; technische und professionelle Zuverlässigkeit dürfen nicht verwechselt werden.

## Forschungsfrage

Wie können Fachkräfte der Sozialen Arbeit in der Teilhabeplanung für Menschen mit Behinderung durch Entscheidungsunterstützungssysteme (DSSs) unterstützt werden, und welche Erwartungen, Befürchtungen und ethischen Implikationen sind damit verbunden?

## Methodik

Mixed Methods: Empirisch qualitativ mit zwei Teilen - ESI Study (Interviews zu Erwartungen und Befürchtungen, n nicht spezifiziert) und User Study (Prototyp-Testing mit 5 Fachkräften). Prospektive Technologiebewertung mit Antizipationsmethoden.
**Datenbasis:** 22 Klient*innen-Dateien mit 295.812 Datensätzen von 2 Wohneinrichtungen; Interviews mit Fachkräften von Leistungserbringern und Teilhabebehörden; Prototyp-Teststudie mit 5 Sozialarbeiter*innen

## Hauptargumente

- Visuelle Darstellungen von Klient*innen-Entwicklungen durch KI-basierte DSSs können berufliche Reflexion fördern und einen Mehrwert bieten, insofern sie subjektive Perspektiven transparenter machen und die professionelle Urteilsbildung unterstützen.
- Gegenwärtige Vorstellungen von DSSs fokussieren primär auf Professional-Algorithmus-Interaktion und ignorieren die Notwendigkeit partizipativer Entscheidungsfindung mit Service-Nutzer*innen, was ein kritisches Defizit in der Konzeptentwicklung darstellt.
- Die in professionelle Dokumentation eingeflossenen Biases, Subjektivitäten und Datenqualitätsprobleme stellen grundsätzliche Herausforderungen dar und erfordern Datenkompetenz und kritisches Verständnis der Unterschiede zwischen technischer und professioneller Zuverlässigkeit.

## Kategorie-Evidenz

### Evidenz 1

Data literacy und Verständnis technischer Prozesse werden als erforderlich benannt: 'Keeping this crucial distinction in mind and accounting for it in daily work with algorithms requires data literacy and an understanding of the technical processes'

### Evidenz 2

Fokus auf AI-basierte Vorhersagesysteme (LONA-Scoring), algorithmische Entscheidungssysteme und natürliche Sprachverarbeitung: 'The system relies on an artificial intelligence (AI) based system trained to predict levels of need for assistance (LONA) from textual documentations'

### Evidenz 3

Expliziter Fokus auf Soziale Arbeit in Teilhabeplanung für Menschen mit Behinderung, professionelle Urteilsbildung und Fachkräfte-Kompetenzen: 'MAEWIN project, therefore, addresses the question of how professionals of social care providers could be supported in the context of SSP by DSSs'

### Evidenz 4

Kritische Analyse von Biases in Dokumentation und Datenbasis: 'documentation may contain hidden biases, biased perspectives, or prejudices' und Diskriminierungsrisiken von Algorithmen: 'systemic discrimination'

### Evidenz 5

Fokus auf Menschen mit Behinderung als marginalisierte Gruppe und deren Partizipation in Entscheidungsprozessen: 'shared decision-making processes with the persons entitled to benefits'

### Evidenz 6

Fairness-Konzepte in algorithmischen Entscheidungssystemen und Anforderung fairer Darstellung: 'data basis used by algorithms is quality controlled and free of biases caused by data reflecting the perceptions of specific stakeholders'

## Assessment-Relevanz

**Domain Fit:** Hochgradig relevant für die Schnittstelle KI/Soziale Arbeit. Das Paper adressiert zentrale Fragen der Implementierung von KI-Systemen in einer kritischen Profession und untersucht Auswirkungen auf vulnerable Zielgruppen (Menschen mit Behinderung) und professionelle Praxis mit Fokus auf Partizipation.

**Unique Contribution:** Einzigartig ist die Beteiligung von Sozialarbeiter*innen als Antizipant*innen in frühen Entwicklungsphasen von DSSs kombiniert mit kritischer Analyse von Subjektivität und Bias in professioneller Dokumentation sowie die Forderung nach partizipativen (statt nur technokratischen) Entscheidungsmodellen.

**Limitations:** Kleine Stichprobe in User Study (n=5); fehlende explizite Perspektive von Service-Nutzer*innen (Menschen mit Behinderung) selbst; Geschlechter- und intersektionale Dimensionen werden nicht systematisch analysiert; Fokus auf Deutschland begrenzt Generalisierbarkeit.

**Target Group:** Sozialarbeiter*innen, Fachkräfte der Behindertenhilfe, KI-Entwickler*innen mit Anwendungsfokus Soziale Arbeit, Policymaker im Sozialsektor, Forscher*innen zu Technology Assessment und Verantwortungsvoller Innovation, Vertreter*innen von Behindertenorganisationen

## Schlüsselreferenzen

- [[Gillingham_2019]] - Decision support systems, social justice and algorithmic accountability in social work
- [[Crawford_2013]] - The hidden biases in big data
- [[Raji_2020]] - How our data encodes systematic racism
- [[Wachter_Mittelstadt_Floridi_2017]] - Transparent, explainable, and accountable AI
- [[Collingridge_1980]] - The social control of technology
- [[Braun_et_al_2020]] - Primer on an ethics of AI-based decision support systems in the clinic
- [[Schneider_Seelmeyer_2019]] - Challenges in using big data to develop decision support systems for social work in Germany
- [[Chiusi_et_al_2020]] - Automating society report 2020
