---
title: "Artificial intelligence in social work: An EPIC model for practice"
authors:
  - L. Goldkind
  - Y. Hamama-Raz
  - Z. Levitats
  - L. Levin
  - M. Ben-Ezra
year: 2024
type: journalArticle
doi: 10.1080/0312407X.2025.2488345
url: "https://doi.org/10.1080/0312407X.2025.2488345"
tags:
  - paper
llm_decision: Include
llm_confidence: 0.92
llm_categories:
  - AI_Literacies
  - KI_Sonstige
  - Soziale_Arbeit
  - Bias_Ungleichheit
  - Fairness
human_decision: Exclude
human_categories: []
agreement: disagree
---

# Artificial intelligence in social work: An EPIC model for practice

## Transformation Trail

### Stufe 1: Extraktion & Klassifikation (LLM)

**Extrahierte Kategorien:** AI_Literacies, Generative_KI, KI_Sonstige, Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Fairness
**Argumente:** 3 extrahiert

### Stufe 3: Verifikation (LLM)

| Metrik | Score |
|--------|-------|
| Completeness | 92 |
| Correctness | 98 |
| Category Validation | 95 |
| **Overall Confidence** | **95** |

### Stufe 4: Assessment

**LLM:** Include (Confidence: 0.92)
**Human:** Exclude

**Kategorie-Vergleich (bei Divergenz):**

| Kategorie | Human | LLM | Divergent |
|-----------|-------|-----|----------|
| AI_Literacies | Nein | Ja | X |
| Generative_KI | Nein | Nein |  |
| Prompting | Nein | Nein |  |
| KI_Sonstige | Nein | Ja | X |
| Soziale_Arbeit | Nein | Ja | X |
| Bias_Ungleichheit | Nein | Ja | X |
| Gender | Nein | Nein |  |
| Diversitaet | Nein | Nein |  |
| Feministisch | Nein | Nein |  |
| Fairness | Nein | Ja | X |

> Siehe [[Divergenz Boetto_2025_Artificial_Intelligence_in_Social_Work_An_EPIC]] fuer detaillierte Analyse


## Key Concepts

- [[AI Literacy in Social Work Education]]
- [[Algorithmic Bias in Social Work]]
- [[EPIC Model for AI Integration]]
- [[Human-in-the-Loop Decision Support]]

## Wissensdokument

# Artificial Intelligence in Social Work: An EPIC Model for Practice

## Kernbefund

Das EPIC-Modell (Ethics and Justice, Policy Development and Advocacy, Intersectoral Collaboration, Community Engagement and Empowerment) bietet einen strukturierten Rahmen für die ethische Integration von KI in die Soziale Arbeit unter Berücksichtigung von Bias, Gerechtigkeit und Empowerment marginalisierter Gruppen.

## Forschungsfrage

Wie können KI-Systeme ethisch und gerecht in die Soziale Arbeit integriert werden, um sowohl Chancen zu nutzen als auch Risiken für vulnerable Gruppen zu minimieren?

## Methodik

Theoretisch: Umfassende Literaturübersicht zur Schnittstelle von KI und Sozialer Arbeit; Entwicklung eines konzeptionellen Modells basierend auf Literaturanalyse
**Datenbasis:** Qualitative Literaturanalyse; keine empirischen Daten (theoretisches Paper)

## Hauptargumente

- KI-Systeme sind nicht neutral und reproduzieren historische Diskriminierungen: Algorithmen sind in kolonialen Wissensstrukturen verankert, zeigen Geschlechts- und Rasse-Bias (z.B. Fehlerrate von 34,7% bei Frauen mit dunklerer Hautfarbe) und exkludieren marginalisierte Gruppen von Anfang an.
- Ein Dual-Human-Technology-Ansatz ist notwendig: KI sollte als Entscheidungsunterstützung dienen, nicht als Ersatz für professionelle menschliche Urteilskraft, um die therapeutische Beziehung und Empathie zu bewahren sowie Vertrauen zwischen Klienten und Sozialarbeitern zu schützen.
- Dekolonisierung und Gemeinschaftsbeteiligung sind zentral: First Nations-Datensouveränität, Beteiligung von Service-Usern in allen Phasen des KI-Lebenszyklus und Aufbau von Community-Bewusstsein über KI-Risiken sind essentiell für gerechte Implementierung.

## Kategorie-Evidenz

### Evidenz 1

Emphasis on need for 'inclusion of AI content in course accreditation requirements and practice standards' and 'develop education opportunities for increasing community AI knowledge and skills' for social workers and communities.

### Evidenz 2

Discussion of generative AI as a subcategory with 'capacity to autonomously augment, synthesise, and innovate new data'; specific examples of ChatGPT use in child protection settings with privacy breaches and inaccurate outputs.

### Evidenz 3

Extensive discussion of machine learning, predictive risk modeling (suicide, domestic violence, child protection), AI-powered chatbots, and decision-support systems in healthcare and social work contexts.

### Evidenz 4

Central focus throughout: 'AI provides opportunities to address social problems and advance socially just outcomes'; applications in child welfare, counselling, mental health, and domestic violence; implications for service user-practitioner relationships.

### Evidenz 5

Critical analysis of algorithmic bias: 'algorithms supporting AI rarely consider diverse perspectives'; research showing 'gender classification systems produced an error rate up to 34.7% for darker-skinned females'; concerns about 'digital divide' and 'information poverty for marginalised groups'; reinforcement of 'colonial knowledges in AI algorithms'.

### Evidenz 6

Explicit reference to gender bias: 'considerations of gender, race, ethnicity, and sexual orientation may be nonexistent or significantly prejudiced and discriminative'; citation of Buolamwini & Gebru (2018) Gender Shades study on gender classification disparities.

### Evidenz 7

Emphasis on representation and inclusion: 'lack of community involvement in AI development' means diverse perspectives missing; advocacy for 'First Nations' and ethnic data sovereignty and control'; need for 'underrepresented groups in AI design and development'; focus on 'marginalised communities' and 'vulnerable populations'.

### Evidenz 8

Discussion of ethical AI frameworks addressing fairness: 'structured, ethical approach to integrating AI emphasises the importance of addressing biases and promoting justice'; strategies include 'decolonisation processes' and 'creation of AI employment pathways for underrepresented groups'; emphasis on 'secure, safe, transparent, and socially just' systems.

## Assessment-Relevanz

**Domain Fit:** Hochgradig relevant für die Schnittstelle AI-Soziale Arbeit-Gerechtigkeit. Das Paper adressiert direkt, wie KI die Kernwerte der Sozialen Arbeit (Empowerment, Gerechtigkeit, ethische Praxis) beeinflusst und bietet einen strukturierten Rahmen für gerechte KI-Integration in marginalisierte Communities.

**Unique Contribution:** Das EPIC-Modell ist ein originärer konzeptioneller Beitrag, der speziell für Soziale Arbeit entwickelt wurde und Ethik, Policy, Interdisziplinarität und Community-Empowerment systematisch integriert—nicht nur technische oder ethische Fragen isoliert behandelt.

**Limitations:** Das Paper ist rein theoretisch-konzeptionell ohne empirische Validierung des EPIC-Modells; begrenzte Diskussion der Implementierungsbarrieren; Annahme, dass Communities Ressourcen für Partizipation haben, wird als Limitation selbst erkannt.

**Target Group:** Sozialarbeiter:innen, Fachkräfte in Sozialen Diensten, Policy-Maker im Sozialsektor, KI-Entwickler:innen in sozialen Anwendungsbereichen, Sozialarbeitslehrende, Organisationen im Sozialwesen, marginalisierte Communities und ihre Advocate:innen

## Schlüsselreferenzen

- [[Buolamwini_Gebru_2018]] - Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification
- [[DankwaMullan_et_al_2021]] - Framework for Integrating Health Equity and Racial Justice into AI Development Lifecycle
- [[Cave_Dihal_2020]] - The Whiteness of AI
- [[Meilvang_Dahler_2024]] - Decision Support and Algorithmic Support: The Construction of Algorithms and Professional Discretion in Social Work
- [[Reamer_2023]] - Artificial Intelligence in Social Work: Emerging Ethical Issues
- [[Rice_Tambe_2018]] - Merging Social Work Science and Computer Science for Social Good
- [[Khawaja_BélislePipon_2023]] - Your Robot Therapist is Not Your Therapist: Understanding the Role of AI-Powered Mental Health Chatbots
- [[Gough_Spencer_2019]] - Ethical Social Work Practice in the Technological Era
