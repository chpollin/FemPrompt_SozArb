---
title: "AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?"
authors:
  - D. Schneider
  - K. Weber
year: 2024
type: journalArticle
doi: 10.14512/tatup.33.1.08
url: 
tags:
  - paper
llm_decision: Include
llm_confidence: 0.85
llm_categories:
  - KI_Sonstige
  - Bias_Ungleichheit
  - Fairness
human_decision: Exclude
human_categories: []
agreement: disagree
---

# AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?

## Transformation Trail

### Stufe 1: Extraktion & Klassifikation (LLM)

**Extrahierte Kategorien:** AI_Literacies, KI_Sonstige, Bias_Ungleichheit, Diversitaet, Fairness
**Argumente:** 3 extrahiert

### Stufe 3: Verifikation (LLM)

| Metrik | Score |
|--------|-------|
| Completeness | 78 |
| Correctness | 92 |
| Category Validation | 88 |
| **Overall Confidence** | **86** |

### Stufe 4: Assessment

**LLM:** Include (Confidence: 0.85)
**Human:** Exclude

**Kategorie-Vergleich (bei Divergenz):**

| Kategorie | Human | LLM | Divergent |
|-----------|-------|-----|----------|
| AI_Literacies | Nein | Nein |  |
| Generative_KI | Nein | Nein |  |
| Prompting | Nein | Nein |  |
| KI_Sonstige | Nein | Ja | X |
| Soziale_Arbeit | Nein | Nein |  |
| Bias_Ungleichheit | Nein | Ja | X |
| Gender | Nein | Nein |  |
| Diversitaet | Nein | Nein |  |
| Feministisch | Nein | Nein |  |
| Fairness | Nein | Ja | X |

> Siehe [[Divergenz Schneider_2024_AI_for_decision_support_What_are_possible]] fuer detaillierte Analyse


## Key Concepts

- [[Algorithmic Fairness]]
- [[Automated Decision-Making Systems]]

## Wissensdokument

# AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?

## Kernbefund

Die Erklärbarkeit von KI-Systemen ist zentral für deren legitime Verwendung, muss aber situativ ausgestaltet werden und darf professionelle Rollen nicht infrage stellen. Vertrauen wird durch funktionale Legitimation als Zweitmeinung aufgebaut, nicht primär durch technische Transparenz.

## Forschungsfrage

Welche sozialen Auswirkungen, ethischen Herausforderungen und Regulierungsoptionen entstehen durch den Einsatz von KI-Systemen zur Unterstützung von Entscheidungen in verschiedenen gesellschaftlichen Bereichen?

## Methodik

Mixed Methods: Qualitative Interviewstudien mit szenariobasierten Ansätzen, Literaturanalyse, Technikfolgenabschätzung (TA), transdisziplinäre Perspektive
**Datenbasis:** Szenariobasierte Interviews mit Akteuren des Gesundheitswesens (n nicht explizit angegeben); Analyse von Gerichtsentscheidungen und Grenzpolizeiverfahren; qualitative Auswertung zu medizinischen Szenarien (Diagnostik und Dokumentation)

## Hauptargumente

- KI-Systeme zur Entscheidungsunterstützung erfordern spezifische Formen der Erklärbarkeit, die kontextabhängig und adressatinnengerecht gestaltet werden müssen, nicht nur technisch transparent sein müssen.
- Automatisierte Entscheidungssysteme an den EU-Grenzen (ADDS) perpetuieren und verstärken bereits bestehende Diskriminierungsdynamiken, insbesondere gegenüber Migrant*innen, durch die Kodifikation von Bias in Algorithmen.
- Vertrauen in KI-Systeme wird durch ihre funktionale Rolle als Unterstützungsinstrument (Zweitmeinung) legitimiert; virtuelle Verkörperung kann sprachbasierte Erklärungen verbessern und die Akzeptanz erhöhen.

## Kategorie-Evidenz

### Evidenz 1

Zentrale Anforderung ist 'Erklärbarkeit' von KI-Systemen als 'Bereitstellung von adressat*innengerechten Informationen über ihre Funktionsweise' und der Aufbau von Vertrauen durch Transparenz in Entscheidungsprozessen.

### Evidenz 2

Fokus auf AI-based decision support systems in Medizin, Justiz und Grenzpolizei; algorithmische Entscheidungssysteme, Machine Learning, Gesichtserkennung, Deception Detection Systeme.

### Evidenz 3

ADDS-Systeme an EU-Grenzen 'continue inequalities and discriminatory dynamics but, by automating them, embed them further into the social fabric'; Diskriminierung in algorithmischen Systemen wird analysiert als strukturelle Benachteiligung von Migrant*innen und vulnerablen Gruppen.

### Evidenz 4

Analyse verschiedener Stakeholder-Perspektiven (Richter*innen, Patient*innen, Grenzbeamte, Ärzt*innen); marginalisierte Communities (Migrant*innen) als besonders von automatisierten Entscheidungssystemen betroffen.

### Evidenz 5

Analyse von Fairness in justiziellen Entscheidungssystemen, Debiasing-Potential von KI, Algorithmic Fairness in Policing und Grenzkontrollen; kritische Reflexion über Fairness-Metriken und deren Grenzen in real-world Anwendungen.

## Assessment-Relevanz

**Domain Fit:** Das Special Topic hat hohe Relevanz für die Schnittstelle KI und Gesellschaft, adressiert aber primär Technikfolgenabschätzung und ethische Fragen von Entscheidungsunterstützungssystemen, nicht spezifisch für Soziale Arbeit. Allerdings relevant für soziale Auswirkungen algorithmischer Systeme auf vulnerable Gruppen.

**Unique Contribution:** Die transdisziplinäre und szenariobasierte Herangehensweise kombiniert technische, ethische und gesellschaftliche Perspektiven auf KI-Entscheidungssysteme und betont die Bedeutung von Kontextualität und Situativität gegenüber universalen Lösungsansätzen.

**Limitations:** Geringe direkte Verbindung zu Sozialer Arbeit als Disziplin; Fallstudien konzentrieren sich auf Medizin, Justiz und Grenzkontrolle; Geschlechterperspektive wird nicht explizit behandelt; Gender-spezifische Auswirkungen algorithmischer Systeme nicht analysiert.

**Target Group:** Technikfolgenabschätzung-Forschende, Policy-Maker, Ethiker*innen im KI-Kontext, Praktiker*innen in Justiz und Medizin, Organisationen der Grenzpolizei, allgemein interessierte transdisziplinäre Wissenschaftler*innen und Stakeholder in KI-Governance.

## Schlüsselreferenzen

- [[Ammicht_Quinn_Regina_2015]] - Trust generating security generating trust
- [[Bacchini_Fabio_Lorusso_Ludovica_2019]] - Race, again. How face recognition technology reinforces racial discrimination
- [[Feldman_Barrett_Lisa_et_al_2019]] - Emotional expressions reconsidered. Challenges to inferring emotion from human facial movements
- [[Gillespie_Nicole_et_al_2023]] - Trust in artificial intelligence. A global study
- [[Hobson_Zoë_et_al_2023]] - Artificial fairness? Trust in algorithmic police decision-making
- [[SánchezMonedero_Javier_Dencik_Lina_2022]] - The politics of deceptive borders. 'Biomarkers of deceit' and the case of iBorderCtrl
- [[Selbst_Andrew_2017]] - Disparate impact in big data policing
- [[Starke_Christoph_et_al_2022]] - Fairness perceptions of algorithmic decision-making. A systematic review
