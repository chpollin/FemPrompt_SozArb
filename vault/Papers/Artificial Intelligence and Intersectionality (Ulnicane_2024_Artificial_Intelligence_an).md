---
title: Artificial Intelligence and Intersectionality
authors:
  - I. Ulnicane
year: 2024
type: journalArticle
doi: 
url: "https://ecpr.eu/news/news/details/749"
tags:
  - paper
llm_decision: Include
llm_confidence: 0.95
llm_categories:
  - KI_Sonstige
  - Bias_Ungleichheit
  - Gender
  - Diversitaet
  - Feministisch
  - Fairness
human_decision: Exclude
human_categories:
  - KI_Sonstige
  - Bias_Ungleichheit
  - Gender
  - Diversitaet
  - Feministisch
agreement: disagree
---

# Artificial Intelligence and Intersectionality

## Transformation Trail

### Stufe 1: Extraktion & Klassifikation (LLM)

**Extrahierte Kategorien:** KI_Sonstige, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness
**Argumente:** 3 extrahiert

### Stufe 3: Verifikation (LLM)

| Metrik | Score |
|--------|-------|
| Completeness | 92 |
| Correctness | 98 |
| Category Validation | 88 |
| **Overall Confidence** | **92** |

### Stufe 4: Assessment

**LLM:** Include (Confidence: 0.95)
**Human:** Exclude

**Kategorie-Vergleich (bei Divergenz):**

| Kategorie | Human | LLM | Divergent |
|-----------|-------|-----|----------|
| AI_Literacies | Nein | Nein |  |
| Generative_KI | Nein | Nein |  |
| Prompting | Nein | Nein |  |
| KI_Sonstige | Ja | Ja |  |
| Soziale_Arbeit | Nein | Nein |  |
| Bias_Ungleichheit | Ja | Ja |  |
| Gender | Ja | Ja |  |
| Diversitaet | Ja | Ja |  |
| Feministisch | Ja | Ja |  |
| Fairness | Nein | Ja | X |

> Siehe [[Divergenz Ulnicane_2024_Artificial_Intelligence_and_Intersectionality]] fuer detaillierte Analyse


## Key Concepts

- [[Algorithmic Bias]]
- [[Gender Bias in AI Systems]]
- [[Intersectionality in AI]]

## Wissensdokument

# Artificial Intelligence and Intersectionality

## Kernbefund

AI amplifiziert und verstärkt menschliche Vorurteile und reflektiert tiefgreifende historische und systemische Ungleichheiten; ein breiter intersektionaler Ansatz ist notwendig, der über reine Diversitätszahlen hinausgeht und strukturelle sowie kulturelle Probleme in der Tech-Branche adressiert.

## Forschungsfrage

Wie rahmen AI-Dokumente Bedenken bezüglich Bias und Ungleichheit in AI ein und welche Empfehlungen zur Bekämpfung dieser Probleme werden aus einer intersektionalen Perspektive gegeben?

## Methodik

Theoretisch/Review - Analyse von AI-Policy-Dokumenten mit intersektionaler Linse, Unterscheidung zwischen technischem und soziotechnischem Framing
**Datenbasis:** Vier hochprofilierte Berichte zu AI und Geschlecht; Analyse von AI-Policy-Dokumenten

## Hauptargumente

- Bias in AI wird oft technisch gerahmt als behebbar durch technische Maßnahmen, doch dies ignoriert die tief verwurzelten sozialen, politischen, kulturellen und historischen Dimensionen des Problems.
- Der Mangel an Frauen und Minderheiten in der Informatik und AI-Entwicklung führt zu einer negativen Rückkopplungsschleife: homogene Entwicklerteams bauen voreingenommene Systeme, was Ungleichheit perpetuiert und bisherige Fortschritte gefährdet.
- Effektive Lösungen erfordern einen systemischen, intersektionalen Ansatz, der Kultur, Macht und Einflussmöglichkeiten transformiert, statt nur Zahlen zu erhöhen, und der Perspektiven mehrerer Disziplinen, Sektoren und marginalisierter Gruppen einbezieht.

## Kategorie-Evidenz

### Evidenz 1

Analyse von AI-Systemen wie Einstellungsalgorithmen, Gesichtserkennung und digitalen Sprachassistenten als Fallbeispiele für algorithmische Bias

### Evidenz 2

Growing evidence suggests that AI is amplifying and exacerbating gender, racial, ethnic and other stereotypes; AI amplifies and exacerbates human biases and reflects deep rooted historical and systemic inequalities and power asymmetries

### Evidenz 3

focus on framing concerns and recommendations related to gender; biased AI applications include hiring algorithms that discriminate against female candidates, facial recognition that performs poorly on black and female faces as well as obedient and subservient digital female voice assistants

### Evidenz 4

The diversity crisis among AI developers and founders; Lack of women and minorities in computing; intersectional lens to highlight the interaction of multiple identities - gender, race, class and others - leading to the marginalization, exclusion and discrimination of certain social groups

### Evidenz 5

use an intersectional lens; intersectionality provides an illuminating perspective; use intersectionality to reimagine AI in more inclusive and participatory ways - referenziert implizit intersektionale feministische Theorie (Crenshaw-Tradition)

### Evidenz 6

concerns about bias and inequality in AI; issues such as justice, fairness and equality; focus on shaping culture, power and opportunities

## Assessment-Relevanz

**Domain Fit:** Das Paper ist hochrelevant für die Schnittstelle AI/Gender/Diversität, bietet eine kritische intersektionale Perspektive auf Bias in AI-Systemen, hat aber einen geringen direkten Bezug zur Sozialen Arbeit.

**Unique Contribution:** Die systematische Anwendung einer intersektionalen Linse auf die Analyse von AI-Policy-Dokumenten und die Unterscheidung zwischen technischem und soziotechnischem Framing bietet einen innovativen Zugang zur Critique von AI-Governance.

**Limitations:** Das Paper basiert auf Dokumentenanalyse von vier hochprofiligen Berichten; empirische Daten zur Wirksamkeit von intersektionalen Interventionen fehlen; direkter Bezug zu Sozialer Arbeit wird nicht hergestellt.

**Target Group:** AI-Policymaker, Wissenschaftler:innen in Gender Studies und Critical AI Studies, Tech-Führungskräfte und Diversitätsmanager:innen, Aktivist:innen für soziale Gerechtigkeit in Tech, jedoch weniger relevant für praktiker:innen der Sozialen Arbeit

## Schlüsselreferenzen

- [[Ulnicane_2024]] - Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action
- [[Ulnicane_Aden_2023]] - Power and politics in framing bias in artificial intelligence policy
