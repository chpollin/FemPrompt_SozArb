---
title: Explainable Artificial Intelligence
authors:
  - Unknown Author
year: 2023
type: report
doi: 
url: "https://www.edps.europa.eu/system/files/2023-11/23-11-16_techdispatch_xai_en.pdf"
tags:
  - paper
llm_decision: Include
llm_confidence: 0.85
llm_categories:
  - AI_Literacies
  - KI_Sonstige
  - Bias_Ungleichheit
  - Diversitaet
  - Fairness
---

# Explainable Artificial Intelligence

## Transformation Trail

### Stufe 1: Extraktion & Klassifikation (LLM)

**Extrahierte Kategorien:** AI_Literacies, KI_Sonstige, Bias_Ungleichheit, Fairness
**Argumente:** 3 extrahiert

### Stufe 3: Verifikation (LLM)

| Metrik | Score |
|--------|-------|
| Completeness | 88 |
| Correctness | 92 |
| Category Validation | 85 |
| **Overall Confidence** | **88** |

### Stufe 4: Assessment

**LLM:** Include (Confidence: 0.85)

## Key Concepts

- [[Algorithmic Bias]]
- [[Algorithmic Fairness]]
- [[Explainable Artificial Intelligence (XAI)]]

## Wissensdokument

# EDPS TechDispatch on Explainable Artificial Intelligence

## Kernbefund

XAI ist ein essentielles Mittel zur Förderung von Transparenz, Rechenschaftspflicht und Fairness bei KI-Systemen, kann aber selbst neue Risiken schaffen und muss durch menschenzentrierte Design-Ansätze und kritische Reflexion flankiert werden.

## Forschungsfrage

Wie können KI-Systeme transparent und erklärbar gestaltet werden, um Datenschutz, Fairness und Vertrauen zu gewährleisten?

## Methodik

Theoretisch/Review - Analyse und Darstellung von Konzepten, Risiken und Best Practices der Explainable AI (XAI) im Kontext des Datenschutzes
**Datenbasis:** nicht empirisch - konzeptionelle und normative Analyse basierend auf Literatur, Rechtsprechung und Best Practice

## Hauptargumente

- Der 'Black-Box-Effekt' bei komplexen KI-Systemen führt zu Risiken für Individuen durch verborgene Diskriminierung, Bias und mangelnde Rechenschaftspflicht, besonders bei automatisierten Entscheidungen durch öffentliche Behörden.
- XAI kann durch Transparency, Interpretability und Explainability-Mechanismen Datenschutzprinzipien unterstützen und Compliance mit GDPR gewährleisten, muss aber sorgfältig umgesetzt werden.
- Die Implementierung von XAI birgt eigene Risiken: Misinterpretation, Sicherheitslücken, Disclosure von Geschäftsgeheimnissen und Überreliance auf Systeme - daher ist ein menschenzentrierter, kontextsensitiver Ansatz erforderlich.

## Kategorie-Evidenz

### Evidenz 1

Fokus auf Verständnis und kritische Reflexion von KI-Systemen durch verschiedene Stakeholder: 'XAI empowers individuals with understandable insights into how their personal data is being handled'

### Evidenz 2

Breite Behandlung von ML, Deep Learning, neuronalen Netzwerken und algorithmischen Entscheidungssystemen: 'AI systems such as machine learning (ML) or deep learning (DL) use algorithms learned by their own process of training'

### Evidenz 3

Explizite Analyse von Diskriminierungsrisiken und Bias in KI: 'when AI is used to select job applicants, systems might inadvertently favour candidates from certain demographics or backgrounds due to biased training data'

### Evidenz 4

Zentrale Behandlung von Fairness-Anforderungen und Fairness-Implementierung: 'the limitations of black box approaches should be considered when trying to assess the fairness of the models'

## Assessment-Relevanz

**Domain Fit:** Das Paper ist primär für Datenschutz-, KI-Governance und Tech-Policy relevant, nicht direkt für Soziale Arbeit. Es bietet jedoch wichtige Erkenntnisse für Sozialarbeiter:innen, die mit algorithmischen Systemen in Bedarfserkennung, Ressourcenallokation oder Fallmanagement arbeiten.

**Unique Contribution:** Die systematische Integration von technischen XAI-Ansätzen mit Datenschutzrecht (GDPR), Fairness-Anforderungen und menschenzentrierten Designprinzipien unter Berücksichtigung von Risiken der XAI-Implementierung selbst.

**Limitations:** Begrenzte Analyse spezifischer Sektoren (erwähnt Gesundheit, Finanzen, keine detaillierte Behandlung von Soziale-Arbeit-Kontexten); keine empirischen Fallstudien oder Evaluationen von XAI-Implementierungen

**Target Group:** Datenschutzbeauftragte, KI-Entwickler:innen, Policy-Maker, Regulatorische Behörden, Organisationen die KI-Systeme einsetzen, sekundär: Sozialarbeiter:innen die mit automatisierten Entscheidungssystemen arbeiten

## Schlüsselreferenzen

- [[Miller_T_H_2017]] - Explainable AI: Beware of inmates running the asylum
- [[Ribeiro_M_T_2016]] - Why should I trust you? Explaining predictions of any classifier (LIME)
- [[Gunning_D_S_2019]] - XAI-Explainable Artificial Intelligence
- [[Burrell_J_2016]] - How the machine 'thinks': Understanding opacity in machine learning algorithms
- [[Lepri_B_O_2018]] - Fair, transparent, and accountable algorithmic decision-making processes
- [[Lipton_Z_C_2018]] - The mythos of model interpretability
- [[Mittelstadt_B_R_2019]] - Explaining explanations in AI
- [[Kuppa_A_2021]] - Adversarial XAI methods in cybersecurity
- [[Peters_U_2023]] - Explainable AI lacks regulative reasons: why AI and human decision-making are not equally opaque
