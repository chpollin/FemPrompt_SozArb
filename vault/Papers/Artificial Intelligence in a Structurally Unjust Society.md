---
title: Artificial Intelligence in a Structurally Unjust Society
authors:
  - T.-A. Lin
  - P.-H. C. Chen
year: 2022
type: journalArticle
doi: 
url: "https://ojs.lib.uwo.ca/index.php/fpq/article/download/14191/12139/38443"
tags:
  - paper
llm_decision: Include
llm_confidence: 0.85
llm_categories:
  - KI_Sonstige
  - Bias_Ungleichheit
  - Fairness
human_decision: Unclear
human_categories:
  - KI_Sonstige
  - Bias_Ungleichheit
  - Gender
  - Diversitaet
  - Feministisch
  - Fairness
agreement: disagree
---

# Artificial Intelligence in a Structurally Unjust Society

## Transformation Trail

### Stufe 1: Extraktion & Klassifikation (LLM)

**Extrahierte Kategorien:** KI_Sonstige, Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Fairness
**Argumente:** 3 extrahiert

### Stufe 3: Verifikation (LLM)

| Metrik | Score |
|--------|-------|
| Completeness | 92 |
| Correctness | 98 |
| Category Validation | 95 |
| **Overall Confidence** | **94** |

### Stufe 4: Assessment

**LLM:** Include (Confidence: 0.85)
**Human:** Unclear

**Kategorie-Vergleich (bei Divergenz):**

| Kategorie | Human | LLM | Divergent |
|-----------|-------|-----|----------|
| AI_Literacies | Nein | Nein |  |
| Generative_KI | Nein | Nein |  |
| Prompting | Nein | Nein |  |
| KI_Sonstige | Ja | Ja |  |
| Soziale_Arbeit | Nein | Nein |  |
| Bias_Ungleichheit | Ja | Ja |  |
| Gender | Ja | Nein | X |
| Diversitaet | Ja | Nein | X |
| Feministisch | Ja | Nein | X |
| Fairness | Ja | Ja |  |

> Siehe [[Divergenz Lin_2022_Artificial_Intelligence_in_a_Structurally_Unjust]] fuer detaillierte Analyse


## Key Concepts

- [[Algorithmic Bias]]
- [[Algorithmic Fairness]]
- [[Gender Bias in AI]]
- [[Structural Injustice]]

## Wissensdokument

# Artificial Intelligence in a Structurally Unjust Society

## Kernbefund

KI-Bias sollte als Form struktureller Ungerechtigkeit verstanden werden, die auftritt, wenn KI-Systeme mit anderen sozialen Faktoren interagieren und bestehende Ungleichheiten verschärfen. Dies erfordert kollektive Verantwortung aller Beteiligten, nicht nur von Technik-Entwickler:innen.

## Forschungsfrage

Wie kann der Rahmen der strukturellen Ungerechtigkeit helfen, die ethischen Bedenken rund um KI-Bias zu klären und neue Ansätze zur Verfolgung von KI-Fairness zu konzeptualisieren?

## Methodik

Theoretisch-konzeptionell: Anwendung von Iris Marion Youngs Strukturelle-Ungerechtigkeit-Framework und Social Connection Model auf KI-Bias, mit Fallstudie in der medizinischen KI
**Datenbasis:** nicht angegeben (konzeptionelle Analyse, illustrative Beispiele aus Literatur)

## Hauptargumente

- AI-Bias ist nicht primär ein technisches Problem einzelner Algorithmen, sondern eine strukturelle Ungerechtigkeit, die aus Interaktionen zwischen KI-Systemen und bestehenden sozialen Strukturen entsteht, die Ungleichheiten reproduzieren und verstärken.
- Der dominante Ansatz zur AI-Fairness, der sich auf statistische Parität konzentriert und Algorithmus-Debiasing betont, ist unzureichend und ignoriert die sozialen Kontexte und Machtungleichgewichte, die KI-Bias produzieren.
- Youngs Social Connection Model begründet eine politische Verantwortung aller Beteiligten in der unjusten Sozialstruktur - von Tech-Führungskräften bis zu Nutzer:innen - für kollektive Maßnahmen zur Umgestaltung dieser Strukturen.

## Kategorie-Evidenz

### Evidenz 1

Fokus auf Machine Learning, algorithmische Entscheidungssysteme in verschiedenen Domänen: 'Several studies have revealed that AI may reproduce existing social injustices, such as sexism and racism' und spezifische Beispiele wie COMPAS, Amazon Recruiting Tool, Google Search Engine.

### Evidenz 2

Thematisiert strukturelle Ungerechtigkeit und Verantwortung in Systemen, die vulnerable Gruppen treffen; Bezug zu Domänen der Sozialen Arbeit wie Gesundheitswesen und strukturelle Gerechtigkeit: 'AI bias exists when AI systems interact with other social factors to exacerbate existing social inequalities, making some groups of people more vulnerable to undeserved burdens'.

### Evidenz 3

Zentrale Thematik durchgehend. 'AI bias is a form of structural injustice that exists when AI systems interact with other social factors to exacerbate existing social inequalities.' Detaillierte Analyse von Rassismus- und Geschlechterdiskriminierung in KI-Systemen.

### Evidenz 4

Explizite Beispiele von Geschlechterdiskriminierung: 'one AI recruiting tool downgrades resumes containing the keyword 'women's,' such as 'women's college' and 'women's chess club,' resulting in a preference for male candidates' und Analyse von Geschlechterdisparitäten im Gesundheitswesen.

### Evidenz 5

Intersektionale Perspektive auf strukturelle Ungleichheit: 'algorithms behind one global search engine tend to represent women of color with degrading stereotypes' und Fokus auf 'Racial, gender, and class disparities in health care'.

### Evidenz 6

Zentrale Kritik und Rekonzeptualisierung: 'the dominant approach sees the problem of AI bias as being primarily located within algorithms, understands the goal of AI fairness as ensuring some parity of some statistical measures between different groups of people' wird kritisiert zugunsten eines strukturellen Fairness-Ansatzes.

## Assessment-Relevanz

**Domain Fit:** Hochgradig relevant für KI-Ethik und strukturelle Gerechtigkeit. Das Paper verbindet KI-Bias direkt mit Sozialarbeit-relevanten Themen wie Gesundheitsgerechtigkeit, strukturelle Ungleichheit und kollektive Verantwortung. Der Fokus auf strukturelle Ungerechtigkeit und Machtungleichgewichte ist zentral für kritische Soziale Arbeit.

**Unique Contribution:** Der Hauptbeitrag besteht darin, Youngs theoretischen Rahmen der strukturellen Ungerechtigkeit und des Social Connection Model auf KI-Systeme anzuwenden und damit über rein technische Fairness-Ansätze hinauszugehen, um eine ganzheitlichere, gesellschaftlich kontextualisierte Perspektive auf KI-Bias zu etablieren.

**Limitations:** Keine empirische Validierung; konzeptionell-theoretischer Ansatz; Fallstudie auf Gesundheitswesen beschränkt; wenig praktische Operationalisierung der Empfehlungen für verschiedene Akteure; keine Berücksichtigung von Kritiken an Youngs SCM-Modell im Detail.

**Target Group:** KI-Ethiker:innen, Sozialarbeiter:innen und Sozialwissenschaftler:innen, Policy-Maker im Tech- und Gesundheitssektor, Tech-Führungskräfte und Ingenieur:innen, Aktivist:innen für algorithmische Gerechtigkeit, akademische Forscher:innen in Schnittstelle KI/Ethik/Gerechtigkeit

## Schlüsselreferenzen

- [[Young_Iris_Marion_2011]] - Responsibility for Justice
- [[Young_Iris_Marion_2006]] - Responsibility and Global Justice: A Social Connection Model
- [[Noble_Safiya_Umoja_2018]] - Algorithms of Oppression: How Search Engines Reinforce Racism
- [[Buolamwini_Gebru_2018]] - Gender Shades (implied from context on algorithmic bias)
- [[Angwin_et_al_2016]] - COMPAS Recidivism Algorithm Study
- [[Dastin_2018]] - Amazon AI Recruiting Tool Gender Bias
- [[Barocas_Selbst_2016]] - Big Data's Disparate Impact
- [[Haslanger_Sally_2000]] - Gender and Race: (What) Are They?
- [[Hoffmann_Anna_Lauren_2019]] - Where Fairness Fails: Data, Algorithms, and the Limits of Antidiscrimination Discourse
- [[Le_Bui_Noble_2020]] - We're Missing a Moral Framework of Justice in Artificial Intelligence
