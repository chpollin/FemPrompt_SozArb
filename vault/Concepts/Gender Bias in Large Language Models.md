---
title: Gender Bias in Large Language Models
type: concept
frequency: 9
papers_count: 9
related_concepts:
  - Algorithmic Fairness
  - Chain-of-Thought Prompting
  - Reinforcement Learning from Human Feedback (RLHF)
  - Intersectional Algorithmic Bias
  - AI Literacy in Social Work
tags:
  - concept
---
# Gender Bias in Large Language Models

## Definition

Systematische Verzerrungen in LLM-Ausgaben, die geschlechtsspezifische Stereotypen reproduzieren und bestimmte Geschlechtsidentitäten (insbesondere non-binäre Personen) durch herablassende oder reduktionistische Darstellungen benachteiligen.

## Co-occurrence

| Konzept | Gemeinsame Papers |
|---------|------------------|
| [[Algorithmic Fairness]] | 4 |
| [[Chain-of-Thought Prompting]] | 2 |
| [[Reinforcement Learning from Human Feedback (RLHF)]] | 2 |
| [[Intersectional Algorithmic Bias]] | 1 |
| [[AI Literacy in Social Work]] | 1 |
| [[Algorithmic Bias in Social Services]] | 1 |
| [[Algorithmic Fairness Evaluation]] | 1 |
| [[Responsible AI Development]] | 1 |

## Papers

- [[Measuring gender and racial biases in large language models- Intersectional evidence from automated resume evaluation]]
- [[Debnath_2024_LLMs]]
- [[Bias, accuracy, and trust- Gender-diverse perspectives on large language models]]
- [[Evaluating gender bias in large language models via chain-of-thought prompting]]
- [[Evaluating gender bias in large language models via chain-of-thought prompting (Kaneko_2024_Evaluating_gender_bias_in_la)]]
- [[DR.GAP- Mitigating bias in large language models using gender-aware prompting with demonstration and reasoning]]
- [[Tang_2024_GenderCARE_A_Comprehensive_Framework_for]]
- [[Challenging systematic prejudices- an Investigation into Gender Bias in Large Language Models]]
- [[Debiasing prompts for gender bias in large language models]]

## Assessment-Divergenz

Von 5 bewerteten Papers: 2 Divergenzen (40%)

