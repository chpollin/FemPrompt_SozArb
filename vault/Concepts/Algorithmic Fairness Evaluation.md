---
title: Algorithmic Fairness Evaluation
type: concept
frequency: 7
papers_count: 7
related_concepts:
  - AI Literacy
  - Chain-of-Thought Reasoning
  - Algorithmic Bias in Child Welfare
  - Disparate Impact Assessment
  - Chain-of-Thought Prompting
tags:
  - concept
---
# Algorithmic Fairness Evaluation

## Definition

Systematische Bewertung von KI-Systemen auf Diskriminierung und Bias über demografische Gruppen hinweg durch Benchmarking-Frameworks, die Fairness-Implikationen von Prompt-Designs und Modelltendenz zu verzerrten Outputs berücksichtigen.

## Co-occurrence

| Konzept | Gemeinsame Papers |
|---------|------------------|
| [[AI Literacy]] | 2 |
| [[Chain-of-Thought Reasoning]] | 2 |
| [[Algorithmic Bias in Child Welfare]] | 1 |
| [[Disparate Impact Assessment]] | 1 |
| [[Chain-of-Thought Prompting]] | 1 |
| [[Gender Bias in Large Language Models]] | 1 |
| [[Algorithmic Bias]] | 1 |
| [[Computational Thinking]] | 1 |

## Papers

- [[Reflexive prompt engineering- A framework for responsible prompt engineering and AI interaction design]]
- [[A systematic review of sophisticated predictive and prescriptive analytics in child welfare- Accuracy, equity, and bias]]
- [[Evaluating gender bias in large language models via chain-of-thought prompting (Kaneko_2024_Evaluating_gender_bias_in_la)]]
- [[Intersectional Stereotypes in Large Language Models- Dataset and Analysis]]
- [[AI Literacy- A Framework to Understand, Evaluate, and Use Emerging Technology]]
- [[The power of prompts- Evaluating and mitigating gender bias in MT with LLMs]]
- [[Gender, race, and intersectional bias in AI resume screening via language model retrieval]]

## Assessment-Divergenz

Von 5 bewerteten Papers: 2 Divergenzen (40%)

