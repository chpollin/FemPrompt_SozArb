---
title: Reinforcement Learning from Human Feedback (RLHF)
type: concept
frequency: 2
papers_count: 2
related_concepts:
  - Gender Bias in Large Language Models
  - Algorithmic Fairness
  - Responsible AI Development
tags:
  - concept
---
# Reinforcement Learning from Human Feedback (RLHF)

## Definition

Trainingsmethode zur Anpassung von LLM-Ausgaben basierend auf menschlichem Feedback, die nachweislich Gender Bias und sexistische Inhalte substanziell reduzieren kann.

## Co-occurrence

| Konzept | Gemeinsame Papers |
|---------|------------------|
| [[Gender Bias in Large Language Models]] | 2 |
| [[Algorithmic Fairness]] | 1 |
| [[Responsible AI Development]] | 1 |

## Papers

- [[Challenging systematic prejudices- an Investigation into Gender Bias in Large Language Models]]
- [[Debiasing prompts for gender bias in large language models]]

## Assessment-Divergenz

Von 1 bewerteten Papers: 0 Divergenzen (0%)

