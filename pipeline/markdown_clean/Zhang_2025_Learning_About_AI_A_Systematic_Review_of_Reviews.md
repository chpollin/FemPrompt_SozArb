---
source_file: Zhang_2025_Learning_About_AI_A_Systematic_Review_of_Reviews.pdf
conversion_date: 2026-02-03T09:34:38.243858
converter: docling
quality_score: 95
---

<!-- image -->

## Learning About AI: A Systematic Review of Reviews on AI Literacy

Journal of Educational Computing Research 2025, Vol. 63(5) 1292 -1322 © The Author(s) 2025 Article reuse guidelines: sagepub.com/journals-permissions DOI: 10.1177/07356331251342081 journals.sagepub.com/home/jec

<!-- image -->

Shan Zhang 1  , Priyadharshini Ganapathy Prasad 1  , and Noah L. Schroeder 1 

## Abstract

Given the ubiquity of arti /uniFB01 cial intelligence (AI), it is essential to empower students to become creators, designers, and producers of AI technologies, rather than limiting them to the role of informed consumers. To achieve this, learners need to be equipped with AI knowledge and concepts and develop AI literacy. Paradoxically, it is largely unclear what AI literacy is and how weshould learn and teach it. We address both of these questions through a systematic review of systematic reviews, also known as an umbrella review, to gain a comprehensive understanding of AI literacy. After searching the literature, we critically examine the results of 17 reviews focusing on AI literacy and the teaching and learning of AI concepts. Our analysis revealed several encouraging developments: a general consensus on the de /uniFB01 nition of AI literacy, the availability of teaching tools and materials that support AI learning without prior programming experience, and effective pedagogical approaches that have shown positive effects on students ' understanding and engagement. In addition, we identi /uniFB01 ed several areas needing attention in the /uniFB01 eld: an interdisciplinary pedagogical approach, integration of ethical considerations in AI education, discussions on AI policy, and standardized, content-validated, reliable assessments across educational levels and cultures.

## Keywords

arti /uniFB01 cial intelligence education, AI literacy, teaching and learning AI, AI education, umbrella review, systematic review

1 University of Florida, Gainesville, FL, USA

## Corresponding Author:

Shan Zhang, School of Teaching and Learning, College of Education, University of Florida, Gainesville, FL 32611-6120, USA.

Email: zhangshan@u /uniFB02 .com

## Introduction

With the rapid advancement of arti /uniFB01 cial intelligence (AI), the importance of AI in daily life has become increasingly recognized, leading to a growing emphasis on AI education. AI is now considered as essential as traditional literacy skills, such as reading and writing (Kandlhofer et al., 2016). The concept of literacy has expanded to encompass multiple domains, including digital, scienti /uniFB01 c, computing, media, and data literacies (Long &amp; Magerko, 2020; Park et al., 2021; Pinski &amp; Benlian, 2024), each aimed at identifying critical competencies within speci /uniFB01 c /uniFB01 elds that have the ability to facilitate expression, communication, and access to knowledge.

In response, researchers have introduced the notion of AI literacy, de /uniFB01 ned as ' a set of competencies that enables individuals to critically evaluate AI technologies; communicate and collaborate effectively with AI; and use AI as a tool online, at home, and in the workplace ' (Long &amp; Magerko, 2020, p. 2). This emphasizes the need for individuals to move beyond passive use, empowering them to become problem solvers and critical thinkers who can apply AI to real-world challenges while considering its ethical implications (Chiu et al., 2024; Ghallab, 2019).

Efforts to improve AI literacy have long been a topic of interest in computer science education (Torrey, 2021). Recently, there has been a shift towards integrating AI education into curricula for non-computer science majors (Kong et al., 2021) and K-12 students (Steinbauer et al., 2021; Tedre et al., 2021), re /uniFB02 ecting the understanding of AI ' s role in workforce preparation (Kim et al., 2023). Several countries, including the United States, Germany, China, and India, have developed policies and curricula for AI education at both K-12 and higher education levels (UNESCO, 2022). In addition, researchers and educators have been addressing the limitations of traditional resources such as textbooks and simulations by implementing hands-on pedagogical methods, including inquiry-based learning, project-based learning, makerspaces, and design experiences, to encourage students to transcend the role of end-users of AI (Ng et al., 2023).

Despite the growing body of research on AI literacy, the existing literature re /uniFB02 ects a diverse range of perspectives, with reviews focusing on different aspects of teaching and learning (Fleger et al., 2023; Gennari et al., 2023). This diversity enriches the /uniFB01 eld but also makes it challenging to achieve a cohesive understanding of AI literacy and its broader implications (Pinski &amp; Benlian, 2024). Therefore, this paper aims to provide an umbrella review of the systematic review literature on AI literacy, consolidating current knowledge, identifying gaps, and proposing directions for future research.

## Background

## AI Literacy De /uniFB01 nitions and Frameworks

Recent scholarship has increasingly focused on developing AI literacy frameworks for future generations, though there remains little consensus on what exactly AI literacy consists of. Early work by Kandlhofer et al. (2016) introduced AI literacy as the ability

to understand the core concepts and functions of AI, primarily emphasizing technical knowledge. Subsequent studies, however, broadened this de /uniFB01 nition to include competencies such as communication and collaboration with AI systems. For example, Long and Magerko (2020) offered a comprehensive framework addressing technical skills along with the abilities necessary to cooperate, communicate, and coexist with AI technologies including /uniFB01 ve core AI topics -concept, functionality, capacity, applications, and perceptions -alongside 17 competencies and 15 design considerations aimed at guiding educational integration.

Similarly, Ng et al. (2021b) extended AI literacy to K-12 and university education, identifying it as a critical 21st-century skill. Their framework, grounded in Bloom ' s Taxonomy and technological pedagogical content knowledge (TPACK) (Koehler &amp; Mishra, 2009), focused on educational models and the challenges of teaching AI concepts, offering strategies that accommodate diverse learning needs. More recently, Kong et al. (2024) advanced the conversation by proposing a multidimensional AI literacy framework, categorizing it into cognitive, metacognitive, emotional, and social dimensions. This approach emphasized not only understanding and using AI but also addressing its ethical and societal implications, re /uniFB02 ecting a holistic view of AI literacy.

Furthermore, Pinski and Benlian (2024) proposed a holistic AI literacy framework, categorizing it into two main areas: Pro /uniFB01 ciency Dimensions and Subject Areas. Pro /uniFB01 ciency Dimensions include knowledge, awareness, skills, competencies, and experience, re /uniFB02 ecting the various aspects of human capability to engage with AI. Subject Areas cover AI-related content such as models, data, tools, interfaces, and societal impact. Their framework emphasizes the complex and interconnected relationship between these dimensions and areas, noting that different user groups -ranging from students to professionals -with each having distinct AI literacy needs. Tailoring AI literacy initiatives to accommodate these varying levels of expertise is essential for fostering both basic functional knowledge and advanced skills.

Beyond these broader frameworks, Chiu et al. (2024) further contributed by developing a model speci /uniFB01 cally for K-12 AI education, which focused on /uniFB01 ve key components: technology, impact, ethics, collaboration, and self-re /uniFB02 ection. This model aimed to foster both technical pro /uniFB01 ciency and social-emotional competencies, promoting a more inclusive and personalized approach to AI education.

Overall, the evolving /uniFB01 eld of AI literacy has generated a range of frameworks, each emphasizing different aspects, from technical knowledge to social and ethical competencies. This diversity highlights the need for continued dialogue among educators, researchers, and policymakers to establish a uni /uniFB01 ed understanding of AI literacy that adequately prepares individuals to navigate the challenges and opportunities of an AIdriven world.

## AI Literacy is Becoming More Critical at All Levels of Schooling

In recent years, AI education has increasingly focused on enhancing students ' understanding of AI literacy across different educational levels -from preschool (Su, Ng,

&amp; Chu, 2023), K-12 (Casal-Otero et al., 2023; Su, Guo, et al., 2023; Wang &amp; Lester, 2023), and through higher education (Laupichler et al., 2022) -by developing AI curricula, addressing ethical implications, shaping attitudes toward AI, and tailoring perceived knowledge and skills appropriate for each stage of learning.

At the preschool level, AI education introduces basic machine learning concepts through unplugged and interactive activities, and games that encourage curiosity and exploration, aiming to develop foundational skills such as pattern recognition and problem-solving (Su, Ng, &amp; Chu, 2023). At the K-12 level, programs have emphasized developing fundamental AI skills, understanding AI applications, and exploring ethical considerations appropriate for each grade, often incorporating hands-on projects and real-world examples to engage students (Casal-Otero et al., 2023; Touretzky et al., 2019; Wang &amp; Lester, 2023; Zhang et al., 2024). In higher education, AI literacy becomes more specialized, with institutions creating comprehensive frameworks to guide the integration of AI literacy into curricula, helping faculty and students understand advanced AI concepts, focusing on critical analysis of AI technologies and practical implications within speci /uniFB01 c /uniFB01 elds of study (Laupichler et al., 2022).

Perhaps it is not surprising then that research indicates children have more misconceptions about AI when they are not taught about AI early in their schooling (Kim et al., 2023; Mertala &amp; Fagerlund, 2024). Furthermore, not teaching children about AI when they are young may even obstruct their ability to develop future AI technologies (Ghallab, 2019). But the potential problems with not teaching children about AI when they are young do not stop here. Students may not be interested in AI-related careers if they develop an excessive dread of AI (Bewersdorff et al., 2023; Cave et al., 2019) or alternatively view AI as a universal solution (Kim et al., 2023). In other words, students must learn a balanced view of AI, highlighting the importance of AI literacy. This issue is further exacerbated by the absence of structured AI literacy curricula in K12 education, underscoring the necessity of educational reforms that prioritize AI literacy from a young age (Ng et al., 2021b; Voulgari et al., 2021).

The Landscape of AI Curricula and Policy. With the growing recognition of AI literacy as essential for all learners (Author et al., 2024), AI curricula are rapidly evolving across educational levels, with various frameworks focusing on building students ' knowledge, skills, and attitudes toward AI. For example, frameworks or curricula have been developed for various educational levels, including early childhood education (Su &amp; Zhong, 2022), elementary school (Kim et al., 2021), middle school (Song et al., 2024), high school (Bellas et al., 2023), and K-12 as a whole (Chiu, 2021).

Meanwhile, governments worldwide have adopted AI curricula to prepare students for a future shaped by AI technologies, including China (Song et al., 2022; Wu et al., 2021) the Ministries of Education in Korea, the United Arab Emirates, and India (UNESCO, 2022). The importance of AI literacy is re /uniFB02 ected in policy as

well. More than 30 countries have released national AI policy strategies since 2021, and one of the major themes of the national AI policy strategies is public AI literacy (Schiff, 2022). For example, the European Union published a series of policy documents for its Arti /uniFB01 cial Intelligence Strategy that emphasized the need for ethical considerations and interdisciplinary approaches in teaching AI concepts (European Commission, High-Level Expert Group on AI, 2019; Xiong et al., 2023). These policies re /uniFB02 ect a broader recognition of the necessity to equip students with the skills to navigate the complexities of AI technologies and their societal implications. Similarly, the South Korean government established the ' Arti /uniFB01 cial Intelligence National Strategy, ' which emphasized the importance of AI literacy. This policy integrates AI literacy into various subjects to foster innovation and technological pro /uniFB01 ciency among learners to prepare them for the AI-driven future (Lee &amp; Jeong, 2023). In the United States, the AI4K12 initiative (AI4K12.org) was established to set national curriculum standards for AI education in K-12 and proposed the Five Big Ideas framework (Touretzky et al., 2019), offering a curated directory of instructional resources and fostering a community dedicated to AI education for K-12 learners. The Of /uniFB01 ce of Educational Technology has just launched a new toolkit with guidelines for safe, ethical, and equitable AI integration, which includes AI literacy as an important module (U.S. Department of Education, Of /uniFB01 ce of Educational Technology, 2024). Clearly, AI literacy is of the utmost importance.

## The Present Study

Despite the awareness of the positive effects of AI literacy on students ' development, efforts to design curricula that teach aspects of AI literacy, and initiatives from different countries to promote AI literacy, there remains a lack of a comprehensive understanding of what AI literacy truly encompasses (Laupichler et al., 2022; Pinski &amp; Benlian, 2024). This gap creates a challenge in identifying which facets of AI literacy are being adequately addressed and which are being overlooked.

Meanwhile, several reviews have been conducted to explore AI knowledge and concepts, available teaching resources, common instructional strategies, and the experiences of both learners and teachers in AI education (Ng et al., 2023; Rizvi et al., 2023; Yue et al., 2022). Each review presents a distinct focus, making it unclear what commonalities and differences have been identi /uniFB01 ed in the AI literacy /uniFB01 eld, particularly given that AI literacy itself lacks a uni /uniFB01 ed framework (Pinski &amp; Benlian, 2024). Moreover, the abundance of reviews published in recent years raises concerns about the potential for ' research waste ' , where insuf /uniFB01 cient consideration is given to previously published work (Grainger et al., 2020; Robinson et al., 2021). In an effort to address these issues, we conduct a systematic review of systematic reviews (Sutton et al., 2019), also known as an umbrella review (Aromataris et al., 2015), to holistically understand the AI literacy research landscape. Speci /uniFB01 cally, we posed the following research questions:

RQ1: What is AI literacy?

RQ2: What are the key /uniFB01 ndings and implications for research and practice on AI literacy?

RQ3: What are promising future research directions?

## Methods

We conducted an umbrella review (Aromataris et al., 2015) and we report the process following the PRISMA guidelines (Page et al., 2021). We outline the methods used below.

## Literature Search

This umbrella review is a part of a large-scale, multifaceted review investigating AI in Education. On January 27, 2024, we searched nine databases covering various disciplines, including education, psychology, social sciences, computing, and health professions. To capture relevant reviews, we applied the following search string, ( ' AI ' OR ' arti /uniFB01 cial intelligence ' OR ' AIED ' OR ' AI ED ' ) AND (edu*) AND ( ' mapping review ' OR ' systematic review ' OR ' scoping review ' OR ' rapid review ' OR ' umbrella review ' OR meta-ana*). Beyond these databases, we included 12 studies identi /uniFB01 ed through an informal search of Computers and Education: Arti /uniFB01 cial Intelligence and Google Scholar. After excluding duplicates ( n = 565), 2336 abstracts were selected for initial screening. The details of the database search and database building process can be found below in Table 1.

## Inclusion and Exclusion Criteria

We had two phases of inclusion criteria to help narrow our sample. To initially be included in the pool of studies for further consideration, studies must have met the following criteria:

- (1) The study had to be systematically conducted with keywords, databases searched, and inclusion or exclusion criteria reported clearly.
- (2) The study had to focus on AI in education.
- (3) The study had to be published in English.

## Study Screening

The study screening was broken into three phases.

Phase I Screening. We /uniFB01 rst sought to identify reviews in the /uniFB01 eld of AI education. As such, in the /uniFB01 rst phase of study screening the titles and abstracts of studies were /uniFB01 rst

Table 1. Overview of the Database Search and Duplicate Removal Process.

| Database                       |   Records |
|--------------------------------|-----------|
| Academic search complete       |       161 |
| ACM digital library            |       808 |
| APA PsycInfo                   |       109 |
| CINAHL plus with full text     |       103 |
| Education research complete    |       137 |
| ERIC                           |        81 |
| Web of science (all databases) |       563 |
| Scopus                         |       710 |
| IEEE                           |       217 |
| Informal searches              |        12 |
| Total                          |      2901 |
| Duplicates removed             |       565 |
| Abstracts screened in phase 1  |      2336 |

examined to see if they met the inclusion criteria 1 -3. We used Covidence (https://app. covidence.org), a web-based platform designed for systematic review screening, to facilitate this process. After applying inclusion criteria during abstract screening ( n = 2336), 307 studies were selected for full-text screening.

Phase II Screening. We reviewed the full text of 307 studies to ensure they all met the inclusion and exclusion criteria 1 -3. Of these, 224 studies were included and met the criteria in the Phase II screening. Please refer to the PRISMA below for details (Figure 1).

Phase III Screening. To further narrow down the studies that focused on AI literacy, the full text of 224 studies were examined to see if they met the inclusion and exclusion criteria. It wasat this stage that we added more narrowed inclusion criteria. Speci /uniFB01 cally, to be included in the analysis, the study must have focused on teaching or learning about AI (sometimes referred to as AI training), or the study must have focused on resources for teaching or learning about AI. In the end, 17 studies were included in the analyses.

## Data Extraction

One author extracted data from all of the studies.

Variables Extracted. To gain a comprehensive understanding of the /uniFB01 ndings around AI literacy, we extracted various types of data such as context and dataset information, targeted participants, research focus, AI literacy de /uniFB01 nition, age or developmental AI literacy de /uniFB01 nition, implications for research and practice, future directions, and study

Figure 1. PRISMA /uniFB02 owchart adapted from Page et al. (2021).

<!-- image -->

quality (adapted from an existing metric, Aromataris et al., 2015). The details of the coding scheme are described in Table 2.

Inter-Rater Agreement. To calculate inter-rater agreement, two authors coded 23.5% of all studies ( n = 4) independently, resulting in an inter-rater agreement of 94%. Any disagreements were resolved by referring back to the full text of the study. In addition, to increase the reliability and validity of the data extraction for the research implications, implications for practice, and future research directions, two authors collaboratively coded these sections. Speci /uniFB01 cally, one author extracted data from all studies, and then another author synthesized the data into the primary, synthesized /uniFB01 ndings for presentation in our coding form. The authors discussed and /uniFB01 nalized these main points to ensure they were in agreement. This collaborative approach ensured a thorough and consistent analysis, enhancing the overall accuracy and dependability of the data extraction process for these critical areas.

## Data Availability

The full coding forms with the data extracted from each study are included in Supplemental Materials 1.

## Results

We identi /uniFB01 ed 17 reviews focusing on AI literacy, the teaching and learning of AI, and resources for teaching or learning about AI. All were published between 2021 and 2024, with the majority ( n = 10) published in 2023. The literature searches used in these reviews were conducted from 2020 to 2023. The studies covered a range of databases, from one to eight, with the number of included studies analyzed varying from 10 to 179 ( median = 29). Most of these reviews were identi /uniFB01 ed as systematic reviews, while two were called scoping reviews.

In terms of context, eleven reviews focused on K-12 education, one on early childhood education (ECE), one on higher and adult education, three on health professions education, and one on K-16 education. Additionally, we assessed the quality indicators in each of these studies and found that /uniFB01 ve did not clearly describe the variables and subcategories/options being extracted, seven did not provide a full coding form with all data extracted, and only three studies evaluated study quality using a metric -two of these three were from health professions education reviews. Further details are provided in Supplemental Materials 1.

## RQ1: What is AI Literacy?

We questioned if any of the reviews we examined would use an age-speci /uniFB01 c de /uniFB01 nition of AI literacy. However, none of the 17 reviews did so. Interestingly, nine out of 17 studies did not include a de /uniFB01 nition for AI literacy at all (Supplemental Materials 1).

Table 2. The Variables and Coding Scheme Used During Data Extraction.

| Variable                                                                                              | Description and Coding Scheme                                                                                                                        |
|-------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| Title                                                                                                 | We coded the study title.                                                                                                                            |
| Author                                                                                                | We coded the author(s) of the study.                                                                                                                 |
| Type of review                                                                                        | We coded the type of review. • Systematic review • Meta-analysis • Scoping review • Rapid review                                                     |
| Literature search date range Number of databases                                                      | We coded the literature search date range. We coded the number of databases that were searched.                                                      |
| Number of studies included                                                                            | We coded the number of studies included that met the inclusion criteria.                                                                             |
| Grade of participants                                                                                 | We coded the grade level of the participants                                                                                                         |
| Research questions                                                                                    | We coded the research questions in the study                                                                                                         |
| De /uniFB01 nition AI literacy                                                                        | Wecodedthe de /uniFB01 nition of AI literacy used in each study if they provided one.                                                                |
| Implications for research Implications for practice                                                   | We coded the main /uniFB01 ndings from the discussion for research implications                                                                      |
|                                                                                                       | Wecoded the main claims from the suggestions for educators & teachers.                                                                               |
| Future research directions We coded the future The following criteria are coded for Research question | research directions the study quality examination, adapted from Aromataris et al. (2015) Is the review question clearly and explicitly stated? • Yes |
| Inclusion criteria                                                                                    | • No Were clear inclusion criteria explicitly stated? • Yes                                                                                          |
|                                                                                                       | • No                                                                                                                                                 |
|                                                                                                       | • No Did the authors clearly report the date the search was conducted (at least the year)? • Yes                                                     |
| Date of the search                                                                                    | • No                                                                                                                                                 |

(continued)

Table 2. (continued)

| Variable                                  | Description and Coding Scheme                                                                                                              |
|-------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|
| Speci /uniFB01 c databases                | Did the authors clearly report what databases were searched? • Yes • No                                                                    |
| Coding form clearly described             | Was the coding form for extracting data from studies clearly described? • Yes • No                                                         |
| Full coding form available                | Was the full, complete coding form with all data extracted from studies provided? • Yes • No                                               |
| IRR for data extraction reported          | Was inter-rater reliability or inter-rater agreement reported for the data extraction? • Yes • No                                          |
| Publication bias assessed                 | Was the full, complete coding form with all data extracted from studies provided? • Yes • No                                               |
| Study quality reported in the coding form | Did the reviewers assess the quality/validity of the included studies beyond including only articles of speci /uniFB01 c types? • Yes • No |

Among the articles that included a de /uniFB01 nition ( n = 8), three reviews (Laupichler et al., 2022; Su, Guo, et al., 2023; Yue et al., 2022) directly quoted the AI literacy de /uniFB01 nition from Long and Magerko ' s study (2020), which described AI literacy as ' a set of competencies that enables individuals to critically evaluate AI technologies; communicate and collaborate effectively with AI; and use AI as a tool online, at home, and in the workplace ' (p. 2), and four reviews paraphrased this de /uniFB01 nition in their own words (Casal-Otero et al., 2023; Fleger et al., 2023; Ng et al., 2023; Su, Ng, &amp; Chu, 2023). One review provided its own de /uniFB01 nition of what AI literacy is and the authors refer to AI literacy as the design and implementation of AI learning activities, learning tools, applications, and teaching models that equip students with the knowledge, skills, and attitudes needed to thrive in an AI-drive future (Yim &amp; Su, 2024).

## RQ2: What are the Findings in the Field?

We describe the /uniFB01 ndings of these 17 reviews by categorizing them into different educational grade levels and major /uniFB01 elds of study: ECE, K-12 education, higher and adult education, and health professional education.

Early Childhood Education. Only one study examined AI literacy at the ECE level. Su, Ng, and Chu (2023) reviewed AI curriculum designs for young children and found that most studies focused on young children ' s learning of AI or machine learning through intervention studies. They recommended designing a curriculum with six key components: AI knowledge, AI processes, the impact of AI, student relevance, teacherstudent communication, and /uniFB02 exibility, and proposed TPACK may provide a good framework for AI integration. While three methods were used to evaluate children ' s AI literacy: assessments of knowledge and theory of mind skills, questionnaires, and observations, future research should adapt performance-based measurements to better capture children ' s understanding and abilities related to AI. In addition, they also highlighted the lack of standard questionnaires, surveys, or performance-based measurements for assessing young children ' s knowledge and skills.

K-12 Education. Twelve reviews in our sample examined K-12 education, however one (Ng et al., 2023) also examined other age groups. Speci /uniFB01 cally, we found six reviews that focused primarily on curricular and/or teaching approaches (Casal-Otero et al., 2023; Ng et al., 2023; Rizvi et al., 2023; Sanusi et al., 2023; Su, Guo, et al., 2023; Yue et al., 2022), while /uniFB01 ve reviews focused primarily on instructional tools (Fleger et al., 2023; Gennari et al., 2023; Ng et al., 2023; Sanusi et al., 2021; Yim &amp; Su, 2024). Additionally, two reviews were speci /uniFB01 c to locales; one examined the educational approaches for teaching AI at the K-12 levels in the Asia-Paci /uniFB01 c region (Su et al., 2022), while the other looked at pedagogical approaches and instructional materials and tools used to teach AI in Africa (Oyelere et al., 2022).

Synthesizing the results of the 12 reviews, we identi /uniFB01 ed common /uniFB01 ndings related to three major themes, which are (1) theoretical frameworks, (2) teaching strategies, tools, learning materials, and assessments, and (3) teacher professional development in the /uniFB01 eld.

Related to Theoretical Frameworks. Findings revealed that few tools or curricular design approaches in the reviews were based on theoretical or conceptual frameworks (Lee et al., 2021; Yim &amp; Su, 2024). However, when frameworks were cited, constructionism and social constructivism were the most popular frameworks cited (Yue et al., 2022).

Regarding Teaching Strategies, Tools, Learning Materials, and Assessments. Popular approaches included project-based learning, problem-based learning, collaborative projects, gami /uniFB01 cation, and interactions with intelligent agents to help students understand AI (Ng et al., 2023; Yim &amp; Su, 2024). Additionally, a variety of tools are now available that help students learn AI without prior programming experience (Yim &amp; Su, 2024). Many tools are free and digitally accessible online for teaching K-12 students machine learning, such as recognition, object detection, and speech synthesis, using a variety of data types (Oyelere et al., 2022; Sanusi et al., 2021). In fact, these areas have been the primary focus of most AI education research (Gennari et al., 2023). Among

machine learning content, classi /uniFB01 cation tasks are the most commonly taught, compared to other topics like decision trees which have not been investigated in this context (Fleger et al., 2023). In addition, Sanusi et al. (2023) also suggested teachers consider embedding machine learning concepts into both STEM and non-STEM content areas as it does not need to be taught in isolation.

To examine the impact of tools used to teach AI, researchers have focused on learning and engagement. These aspects have been measured in different ways, including surveys, observations, and users ' re /uniFB02 ections. Moreover, reviews also showed that collaboration and creativity dimensions of learning and engagement (Gennari et al., 2023), as well as learner ' s backgrounds (Gennari et al., 2023; Su et al., 2022), are relevant to the learning of AI. Thus, it ' s important to ensure the learning materials are appropriate for the learners ' educational levels (Su, Guo, et al., 2023), experience (Yim &amp; Su, 2024), and are culturally relevant to the learners (Su et al., 2022). In addition, while no uni /uniFB01 ed curriculum exists (Ng et al., 2023; Yue et al., 2022), common educational standards should specify what types of AI knowledge are appropriate at different levels of K-12 education (Su et al., 2022). Moreover, studies included in another review found positive effects of most AI teaching units on students ' understanding of AI concepts, their attitudes, and their interest in AI. However, most interventions were synchronous, less than 3 hours, and focused on only one topic (Yue et al., 2022), highlighting the need for longitudinal studies (Rizvi et al., 2023). In addition, reviews also found that students ' programming experience positively correlates with their engagement in learning AI concepts. Students already familiar with programming showed a more positive change in their perception of AI (Rizvi et al., 2023), whereas students without prior programming experience struggled more with learning AI concepts (Su et al., 2022). Lastly, the need for validated assessment methods was also highlighted (Su et al., 2022).

Regarding Teacher Professional Development. Several challenges have been identi /uniFB01 ed that educators face while teaching AI concepts and skills in their classroom, such as providing cognitive scaffolding, understanding programming syntax for novice programmers, and building teachers ' con /uniFB01 dence and competence with AI (Ng et al., 2023). Consequently, a lot of upskilling may be required through professional development programs and school support (Ng et al., 2023; Su et al., 2022). However, few studies focused on training teachers (Sanusi et al., 2023). Therefore, it ' s crucial for teachers to be actively involved in co-designing AI literacy education to increase their AI literacy and AI literacy training should be integrated into both teacher education and professional development programs (Ng et al., 2023; Oyelere et al., 2022; Rizvi et al., 2023; Yim &amp; Su, 2024).

Higher Education and Beyond. Two reviews focused on AI literacy in higher education and beyond (Laupichler et al., 2022; Ng et al., 2023). Laupichler et al. (2022) mentioned the term AI literacy is not yet precisely de /uniFB01 ned, although Long and

Magerko ' s de /uniFB01 nition (2020) is the most encompassing and best /uniFB01 t for higher and adult education contexts in their opinion. Similar to the /uniFB01 ndings around K-12 education, teacher professional development around AI literacy is necessary (Ng et al., 2023). Most research on AI literacy in higher and adult education has been recent, with the USA and Asian countries leading in publications (Laupichler et al., 2022). However, the quality of these studies varied widely.

Health Professions Education. Three reviews focused on AI literacy in the education of healthcare professionals (Kimiafar et al., 2023; Lee et al., 2021; Pupic et al., 2023). Lee et al. (2021) found that all included studies were from 2017 onward, indicating that AI literacy is a growing area of interest in medical education. They identi /uniFB01 ed /uniFB01 ve key themes for developing AI curricula: managing AI systems, ethical and legal implications, biomedical knowledge, critical appraisal of AI systems, and working with electronic health records. However, there was no clear consensus on what should be taught about AI during medical school, and none of the studies were grounded in educational theory. This lack of clarity was also echoed in Pupic et al. ' s review (2023), which emphasized the need for a theoretical foundation for AI practices and understanding AI ethics. They suggested that machine learning, deep learning, and natural language processing should be incorporated into medical education. Meanwhile, Kimiafar et al. (2023) found mixed levels of AI literacy among participants, with about half of the studies reporting acceptable literacy levels and the other half reporting very low levels. Despite this, most healthcare professionals and students were motivated to use AI to improve healthcare, though few had received formal training in its use.

## RQ3: What are Promising Future Research Directions?

Several reviews have identi /uniFB01 ed that many studies on AI literacy lack strong theoretical foundations (Lee et al., 2021; Yim &amp; Su, 2024). Furthermore, these reviews emphasized the necessity for providing reliable and validated evidence of evaluation and standardized assessments (especially performance-based measures) or instrumentations of both AI literacy and AI concepts (Gennari et al., 2023; Laupichler et al., 2022; Su et al., 2022, 2023a; Yim &amp; Su, 2024; Yue et al., 2022). Additionally, teaching machine learning (ML) and AI in non-STEM contexts remains underexplored.

We also found that research evidence is needed regarding the differentiation of existing tools that are most appropriate for speci /uniFB01 c educational levels (Sanusi et al., 2021), how to optimize the tools used to teach AI concepts in K-12 education, what skills are required for one to be considered as having AI literacy (Yue et al., 2022), and context-speci /uniFB01 c resources for teaching AI are needed (Oyelere et al., 2022). Meanwhile, more physical activities and strategies for teaching AI concepts are needed for young kids (Gennari et al., 2023). The importance of co-designing AI curriculum and AI professional development for teachers was also noted (Casal-Otero et al., 2023; Ng et al., 2023; Rizvi et al., 2023; Sanusi et al., 2023; Su, Ng, &amp; Chu, 2023; Yim &amp; Su, 2024),

alongside a notable absence of discussion and focus on AI policy (Rizvi et al., 2023). Overall, the need for more empirical research in different settings (informal vs. formal) and different contexts is highlighted across all studies we reviewed.

## Discussion

## What is AI Literacy?

While none of the reviews provide an age-speci /uniFB01 c de /uniFB01 nition of AI literacy, authors in the /uniFB01 eld seem to have reached a general consensus on the de /uniFB01 nition of AI literacy and framework of Long and Magerko (2020), which emphasizes the knowledge and techniques necessary for effective, ethical, and critical use of AI or human-AI collaboration (Casal-Otero et al., 2023; Fleger et al., 2023; Laupichler et al., 2022; Ng et al., 2023; Su et al., 2023a, 2023b; Yue et al., 2022).

Building upon the AI literacy de /uniFB01 nition and 17 competencies introduced in Long and Magerko ' s study (2020), as more and various AI applications are available, recent concerns have emerged regarding the negative consequences of AI interactions from both technical and student-centered perspectives, again highlighting the importance of teaching and learning AI literacy. Consider large language models (LLMs) as an example: when two students with different AI literacy levels receive the same response containing hallucinations, which is when the LLM makes up information that is not true (Ahmad et al., 2023; Salamin et al., 2023), their reactions may differ signi /uniFB01 cantly. Students with low AI literacy may be likely to accept the answer without question, while those with high AI literacy may tend to critically evaluate the response by posing follow-up questions. Evidently, students with strong AI literacy do not blindly accept AI-generated responses but maintain a critical mindset, recognizing that AI can produce incorrect or misleading information (Oelschlager, 2024). This observation leads to another concern: misuse (Anderljung &amp; Hazell, 2023; Veluru, 2024) or over-reliance on AI (Zhai et al., 2024). Students with low AI literacy may depend excessively on AI without critical engagement or suf /uniFB01 cient mental effort. This behavior risks diminishing their critical learning mindset and may hinder their long-term learning progress, despite potential short-term gains (Bastani et al., 2024; Lehmann et al., 2024).

To summarize the de /uniFB01 nition from Long and Magerko ' s (2020) paper with our /uniFB01 ndings, we refer to AI literacy as encompassing a range of skills that foster a comprehensive understanding of AI and enable individuals to purposefully explore AI technologies and thoughtfully assess their implications for both personal learning gains and social impact. Speci /uniFB01 cally, this process involves several steps: /uniFB01 rst, acquiring knowledge and skills about AI concepts and methods to identify AI-powered tools and platforms; second, gaining insight into the functionality of AI to interact with it intentionality and ef /uniFB01 ciently; and third, critically evaluating the in /uniFB02 uence of AI tools on everyday life, personal learning gains, and social impact. This de /uniFB01 nition also aligned well with the existing six constructs of AI literacy, which are: recognize,

know &amp; understand, use &amp; apply, evaluate, create, and navigate ethically (Almatra /uniFB01 et al., 2024; Ng et al., 2021a), which can be applied in different contexts and populations. It is also worth noting that while our analysis revealed a general consensus on the de /uniFB01 nition of AI literacy, we anticipate that researchers will continue to re /uniFB01 ne this de /uniFB01 nition (Chiu et al., 2024; Pinski &amp; Benlian, 2023), not just based on the ideas of ' literacy ' , such as digital literacy (Wang, Rau, &amp; Yuan, 2023), information literacy (Wang, Li, &amp; Huang, 2023), and other literacy concepts (Kandlhofer et al., 2016; Wienrich &amp; Carolus, 2021), but also for a speci /uniFB01 c age group or user groups, such as workplaces and organizations (Cetindamar et al., 2024).

## Key Findings and Opportunities for Future Research

Key Findings. Overall, as listed in Table 3, the themes we found across different /uniFB01 elds and educational levels show notable similarities, encouraging developments, and signi /uniFB01 cant progress in AI literacy education. Researchers have identi /uniFB01 ed a variety of freely accessible online tools that help students learn AI concepts without prior programming experience, particularly in areas such as machine learning, recognition, object detection, and speech synthesis (Oyelere et al., 2022; Sanusi et al., 2021). Several effective teaching approaches have also been identi /uniFB01 ed, including projectbased learning, problem-based learning, collaborative projects, and gami /uniFB01 cation (Ng et al., 2023; Yim &amp; Su, 2024). Studies have shown positive effects of AI teaching units on students ' understanding of AI concepts, attitudes, and interest in AI (Yue et al., 2022), with students who have programming experience showing particularly positive engagement (Rizvi et al., 2023).

The reviews also revealed growing momentum in integrating AI literacy across different educational contexts, from early childhood to professional education, with healthcare professionals showing strong motivation to incorporate AI in their practice (Kimiafar et al., 2023). It is interesting to note that most of the reviews in our study focus on K-12 education. While there is an emerging trend and increasing attention to AI education in PreK-12 -partly due to research highlighting the importance of introducing AI concepts early -there are relatively few studies on how to teach AI literacy/AI literacy curriculum to undergraduate and graduate students with diverse academic backgrounds (Kong et al., 2021). Although there is a general consensus on the importance of equipping these populations with AI literacy, particularly in computer science education, direct research on instructional approaches for these groups remains limited. Additionally, researchers have successfully identi /uniFB01 ed key components for curriculum design, such as AI knowledge, processes, impact assessment, and student relevance (Su, Ng, &amp; Chu, 2023), providing a strong foundation for future educational initiatives. The emergence of tools that allow students to learn AI without prior programming experience (Yim &amp; Su, 2024) has democratized access to AI education, while the integration of machine learning concepts into both STEM and nonSTEM areas (Sanusi et al., 2023) demonstrates the /uniFB01 eld ' s adaptability and broad applicability.

Table 3. Summary of Key Findings ( ' What We Know ' ) and Gaps ( ' What We Do Not Know ' ) in AI Literacy.

| What We Know                                                                                                                                                                                                                                                  | What We Do Not Know                                                                                                                                                                     |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| There is a general consensus on the de /uniFB01 nition of AI literacy (Casal-Otero et al., 2023; Fleger et al., 2023; Laupichler et al., 2022;Ng et al., 2023; Su et al., 2023a, 2023b; Yue et al., 2022).                                                    | AI competencies have not been mapped onto different teaching resources (e.g., lesson plans) for various educational levels.                                                             |
| Effective teaching strategies have been identi /uniFB01 ed (Ng et al., 2023; Yim & Su, 2024).                                                                                                                                                                 | AI has not been broadly integrated in non-STEM contexts.                                                                                                                                |
| Many tools and curricula are | Teachers face many challenges while teaching AI concepts and skills in their classroom (Ng et al., 2023; Su et al., 2022), and there are increasing efforts in co-designing professional development programs (Lin & Van Brummelen, 2021; Tatar et al., 2024; | More hands-on activities are needed to help teachers integrate AI in the classroom; the impact of co-designed professional development needs to be evaluated.                           |
| There are performance-based AI literacy measurements with validity evidence for different user groups (Lintner, 2024).                                                                                                                                        | The development of standardized, theoretically-based, content-validated, and reliable assessments across different educational levels and cultures is needed.                           |
| Policy is a signi /uniFB01 cant gap in discussions around AI in education (Rizvi et al., 2023).                                                                                                                                                               | New policies should consider both technical aspects and student-centered perspectives                                                                                                   |
| Teachers need clearer policies at the school and district levels regarding the use of AI tools and teaching AI concepts.                                                                                                                                      | on AI usage. Clear guidelines on integrating AI into teaching and learning are needed, including considerations for student privacy and algorithmic fairness (Ghimire & Edwards, 2024). |
| Several factors impact AI learning, including culture, prior background (Su et al., 2022), collaboration and creativity in learning and engagement (Gennari et al., 2023), and programming experience (Rizvi et al., 2023) that impact AI ' s learning        | We need a better understanding the relationship between AI literacy and other constructs (e.g., learning outcomes).                                                                     |

Future Research Directions. Despite these encouraging developments, several clear gaps have been identi /uniFB01 ed that warrant attention in future research.

First, reviews have shown that many learning tools or curricular designs lack a solid foundation in theoretical or conceptual frameworks (Lee et al., 2021; Pupic et al., 2023;

Yim &amp; Su, 2024). This gap is also re /uniFB02 ected in the literature about the disconnect between system design and practical classroom integrations. Research has primarily concentrated on creating ef /uniFB01 cient AI systems, placing less attention on their implementation in educational contexts and the practical challenges associated with their use (Heeg &amp; Avraamidou, 2023; Jia et al., 2024, Zhai et al., 2024, Author et al., under review). It is clear that while the /uniFB01 eld is still iterating AI teaching tools and growing as a whole, there is also the need for sound theoretical grounding in these tools ' design and development. Researchers should clearly articulate what theories are guiding the design of their AI teaching tools so that it is clear that pedagogical best practices are being followed. Co-design with teachers may also be essential for this curriculum (Lin &amp; Van Brummelen, 2021; Tatar et al., 2024; Yau et al., 2022) and tool development process (Lawrence et al., 2024).

Similarly, reviews have pointed to a lack of standardized and valid measurement of AI literacy (Su et al., 2022, 2023b). Educators and researchers have given limited attention to the development and comprehension of AI literacy assessment instruments. While this gap remains signi /uniFB01 cant, recent research has shown increasing efforts to develop AI literacy assessments. Researchers synthesized 22 studies validating 16 different AI literacy scales targeting various populations, such as the general public, college students, secondary school students, medical students, and teachers (Lintner, 2024). Among the 16 scales identi /uniFB01 ed, 13 of these scales were self-reported, replying on Likert-scale items (Carolus et al., 2023; Çelebi et al., 2023; Celik, 2023; Chan &amp; Zhou, 2023; Hwang et al., 2023; Karaca et al., 2021; Kim &amp; Lee, 2022; Laupichler et al., 2023a, 2023b, 2024; Lee &amp; Park, 2024; Moodi Ghalibaf et al., 2023; MoralesGarc´ ı a et al., 2024; Ng et al., 2023; Pinski &amp; Benlian, 2023; Polatgil &amp; Güler, 2023; Wang &amp; Chuang, 2024; Wang, Rau, &amp; Yuan, 2023; Yilmaz &amp; Yilmaz, 2023; Çelebi et al., 2023), while only 3 were performance-based scales (Hornberger et al., 2023; Soto-San /uniFB01 el et al., 2024; Zhang et al., 2024). Although these scales demonstrated good structural validity and internal consistency, the authors of this review concluded they lacked comprehensive testing for content validity, reliability, and cross-cultural validity. It ' s worth noting that one of the scales actually took a theory-based approach to develop an AI literacy test which is aligned with Long and Magerko ' s (2020) AI literacy conceptualization (Hornberger et al., 2023). In addition, another recent review focuses on K-12 AI literacy curricula, analyzing the various methods used to assess students ' AI knowledge and perspectives. The assessments are categorized into formative and summative approaches, and the content they assess includes AI concepts, practices, and perspectives. The study revealed that most assessments focus on summative evaluation of AI concepts and psychological beliefs about AI, while formative, activity-based assessments and evaluations of critical AI skills such as communication and ethical understanding are lacking (Williams, 2023). So, while the reviews we examined largely stated that more work around assessment is needed, we can clearly see that the /uniFB01 eld is heading in that direction. However, it is likely that more work will be needed to create psychometrically and theoretically sound instruments for measuring AI literacy (or its components).

Similarly, teaching materials should not be con /uniFB01 ned solely to machine-learning classi /uniFB01 cation tasks (Fleger et al., 2023); they should also be aligned with AI competencies. While no uni /uniFB01 ed curriculum for AI education currently exists (Ng et al., 2023; Yue et al., 2022), it is important to establish common educational standards specifying which AI knowledge is appropriate at different K-12 education levels (Su et al., 2022). Building on recent guidance released by the Education Testing Service on accessing existing AI literacies (Sparks et al., 2024), future studies should explore how to design teaching materials and assessment tools that encompass all dimensions of AI competencies. These should also align with the common core state standards (Council Of Chief State School Of /uniFB01 cers &amp; National Governors ' Association, 2009) (https:// corestandards.org/), and be adapted to various educational levels. Overall, AI literacy requires an interdisciplinary approach that integrates holistic, active, and collaborative learning strategies centered around authentic problem-solving. While efforts, particularly in the US and China, are underway to establish a framework for AI literacy, clear guidelines are still needed to de /uniFB01 ne what K-12 students should learn about AI, including ethical considerations, and how curriculum goals match with AI literacy competencies and state standards.

Researchers have also found there are a variety of tools available to help students learn AI without prior programming experience (Yim &amp; Su, 2024). This /uniFB01 nding aligns well with Long and Magerko ' s argument (2020) that, while understanding programming can aid in comprehension and would be needed in developing AI, most individuals interacting with AI in their daily lives do not need to know how to program. Instead, AI competencies can be developed without a programming background or computational literacy. Furthermore, several factors can in /uniFB02 uence students ' AI learning, including their culture, prior background (Su et al., 2022), collaboration and creativity in learning and engagement (Gennari et al., 2023), and programming experience (Rizvi et al., 2023). However, there is limited research on teaching AI at the K-12 level and few studies have analyzed students ' learning outcomes (Casal-Otero et al., 2023; Rizvi et al., 2023). Therefore, future research should consider AI literacy as a covariate to further explore other factors that impact it, and how that is associated with learning outcomes.

Another issue noted in many reviews we examined was that teachers face many challenges in integrating AI into their teaching. These challenges are consistent, including a lack of school support, children ' s limited comprehensive abilities, insuf /uniFB01 cient teacher knowledge of AI, and a lack of curriculum guidelines (Su, 2024). Thus, codesigning professional development with teachers is crucial, not only to equip them with AI knowledge for discussing AI with students (Ng et al., 2023) but also to help them learn to identify effective tools for classroom use (Oyelere et al., 2022; Sanusi et al., 2021). To better assist teachers in learning the purpose of AI applications and effectively integrate them in their classrooms, AI tools can be conceptualized into two categories: teaching-facing tools, which assist teachers in completing administrative tasks and classroom needs (e.g., generating grade reports, IEPs, materials/worksheets, email creation), and student-facing tools, where students directly interact with AI in the classroom (e.g., intelligent tutors, automated feedback/scoring generators, quizzes/

games, content creation with generative AI). In addition, integration of AI should not be limited to STEM /uniFB01 elds but should also extend to non-STEM subjects such as language arts and social science education (Sanusi et al., 2023; Wang &amp; Lester, 2023).

While reviews have highlighted the need for AI-related professional development, researchers have made signi /uniFB01 cant progress in two key areas. First, there is growing research examining teachers ' perspectives, attitudes, perceptions, and readiness to learn and teach AI (Ayanwale et al., 2022; Polak et al., 2022; Yue et al., 2024). Second, more researchers are working directly with teachers to co-develop AI curricula and AI professional development programs (Lin &amp; Van Brummelen, 2021; Tatar et al., 2024; Yau et al., 2022). Observing how effectively teachers implement their AI knowledge and pedagogical approaches in the classroom also provides a way to evaluate these codesigned professional development programs.

Meanwhile, we also identi /uniFB01 ed a signi /uniFB01 cant gap in discussions around AI policy within the reviewed literature. In a participatory design session with the authors, K-12 teachers emphasized the need for clearer policies, particularly at the school-district level. One teacher noted a lack of clarity about what is permitted in the classroom, not only regarding the integration of AI tools into teaching and learning but also in teaching AI concepts to students. This challenge mirrors /uniFB01 ndings from a recent survey involving 102 high school principals and higher education provosts, which further highlights the existing policy gap. Speci /uniFB01 cally, the survey revealed that most institutions lack guidelines for the ethical use of AI tools. Moreover, high schools tend to be less proactive in policy development compared to higher education institutions. Even where policies exist, they often fail to address crucial issues, such as student privacy and algorithmic transparency (Ghimire &amp; Edwards, 2024).

Overall, our /uniFB01 ndings indicate that most of the reviews emphasize the need for further research, particularly experimental studies or longitudinal research across different grade levels (Laupichler et al., 2022; Lee et al., 2021; Rizvi et al., 2023; Sanusi et al., 2023; Su, Guo, et al., 2023; Yim &amp; Su, 2024; Yue et al., 2022). Some of the gaps identi /uniFB01 ed in this study include the need for theory-driven, effective frameworks and methods for learning and teaching AI in K-12 schools, interdisciplinary AI education integration (beyond STEM), K-12 AI professional development for both pre-service and in-service teachers focusing on AI content and pedagogical knowledge, and robust AI standardized assessment methods that aligned with competencies and state standards. Some of these gaps have also been highlighted in a recent call to action for K-12 AI literacy (Wang &amp; Lester, 2023).

## Implications

Implications for Theory. We found a general consensus on the de /uniFB01 nition of AI literacy, aligning with the framework proposed by Long and Magerko (2020). Their framework highlights the knowledge and skills required for the effective, ethical, and critical use of AI, as well as fostering collaboration between humans and AI. Building on this, we want to draw attention to the comprehensive de /uniFB01 nition of AI literacy that includes a range of skills aimed at fostering a deep understanding of AI, empowering individuals

to purposefully engage with AI technologies, and critically assessing their implications for both personal learning gains and societal impact.

Moreover, our /uniFB01 ndings suggest that AI literacy is a multifaceted construct shaped by learners ' prior AI experience, cultural background, and engagement levels. While demographic and experiential factors may in /uniFB02 uence learners ' interaction with AI (Kim &amp; Lee, 2022; Moodi Ghalibaf et al., 2023; Rizvi et al., 2023), there is still limited evidence connecting these factors directly to learning outcomes. Few studies have explored these relationships in depth, particularly regarding how learners ' characteristics, non-cognitive dimensions (behavior or affective), and their AI literacy might moderate or mediate learning outcomes. Thus, future research should further explore the intersection to develop a more nuanced understanding of AI literacy. Systematic, longitudinal research, rather than one-off studies, is necessary to uncover the complexities involved in AI literacy development.

Implications for Practice. Akey implication for practice is to continue with co-designing AI-related curricula, tool development, and professional development programs with both pre-and in-service teachers. By involving teachers in the development process, the content can be tailored to their speci /uniFB01 c classroom needs while ensuring that AI is integrated seamlessly into their existing instructional practices without increasing their workload. Importantly, this collaboration should also involve open discussions with teachers about not only the bene /uniFB01 ts but also the potential downsides of AI, equipping them to critically evaluate AI ' s ethical implications and limitations.

Secondly, it is crucial to provide teachers who teach AI as a new subject and integrate AI into existing disciplines with practical, hands-on activities, detailed lesson plans, and clear guidance on integrating AI into their teaching. These resources should focus on pedagogical strategies that make AI a natural part of instruction. Moreover, these strategies for teaching AI should not be limited to STEM curricula but should also extend to non-STEM + C subjects, ensuring that AI literacy is incorporated across disciplines such as the language and arts, and social sciences.

Lastly, there is a need for more attention to AI policy in education. Teachers should be informed about current AI policies and how they affect classroom practices, particularly in areas such as data privacy, ethical use of AI, and equity in AI access. Understanding these policies will empower teachers to implement AI tools responsibly, ensuring that students are protected while learning to navigate AI technologies. Moving forward, clearer guidelines and policies are essential to support teachers in adopting AI in a safe and ethical manner.

## Limitations

The primary limitation of this umbrella review is the methodology itself. While umbrella reviews are essential for synthesizing existing reviews and providing a more cohesive understanding of a /uniFB01 eld, they are inherently dependent on existing systematic reviews and meta-analyses. Given the rapid pace of advancements in AI technologies, particularly in educational contexts, this reliance can present challenges. An umbrella

review depends not only on the publication of primary studies but also on the subsequent publication of systematic reviews or meta-analyses, which means it may not capture the most recent, cutting-edge developments in the /uniFB01 eld. However, we believe this limitation has less impact in the context of our review. Our focus is on AI literacy -its de /uniFB01 nition, key /uniFB01 ndings, and future research directions. While rapid innovations in AI tools and applications are ongoing, these changes are unlikely to fundamentally alter the core components of AI literacy in the near term. Instead, what is more likely to evolve are the tools and curricula used to teach AI literacy. Therefore, we position this review as a foundation for understanding current efforts and initiatives related to AI literacy curricula and policy. We encourage future researchers exploring these spaces to consider more recent primary studies published since the reviews included in this analysis were conducted.

## Conclusion

Previous research has shown that AI literacy is becoming increasingly crucial for individuals across various educational levels and professions. Many countries and institutions have taken initiatives to promote AI literacy (Long &amp; Magerko, 2020; Ng et al., 2023). However, the understanding of what constitutes AI literacy and how it should be taught has remained imprecise. Our systematic umbrella review aimed to consolidate state-of-the-art knowledge on AI literacy by examining 17 systematic reviews across diverse contexts and educational levels. To summarize our /uniFB01 ndings concisely, as a /uniFB01 eld there seems to be consensus on what AI literacy is and why it is important. This opens the door for the critical work that must be done around AI ethics. However, we must now establish how to most effectively measure AI literacy so that effective policy and curricula can be created.

## Authors Contribution

Shan Zhang: Conceptualization, Methodology, Investigation, Validation, Formal Analysis, Data Curation, Writing -Original Draft, Writing - Review &amp; Editing. Priyadharshini Ganapathy Prasad: Writing - Original Draft, Writing - Review &amp; Editing. Noah L. Schroeder: Conceptualization, Methodology, Investigation, Validation, Formal Analysis, Data Curation, Project Administration, Writing - Original Draft, Writing - Review &amp; Editing, Supervision.

## Declaration of Con /uniFB02 icting Interests

The author(s) declared no potential con /uniFB02 icts of interest with respect to the research, authorship, and/or publication of this article.

## Funding

The author(s) received no /uniFB01 nancial support for the research, authorship, and/or publication of this article.

## Ethical Statement

This study is a review of existing literature and did not involve any new studies with human participants or animals conducted by the authors. As such, no ethical approval was required.

## ORCID iDs

Shan Zhang 

```
https://orcid.org/0009-0003-3532-0661 https://orcid.org/0000-0002-3281-2594
```

Priyadharshini Ganapathy Prasad  https://orcid.org/0009-0005-1826-0489

Noah L. Schroeder 

## Data Availability Statement

All data extracted from the studies is available in the Supplemental Materials.

## Supplemental Material

Supplemental material for this article is 
## References

* indicates study included in the Umbrella Review Analysis
- Ahmad, Z., Kaiser, W., &amp; Rahim, S. (2023). Hallucinations in ChatGPT: An unreliable tool for learning. Rupkatha Journal on Interdisciplinary Studies in Humanities , 15 (4), 12. https:// doi.org/10.21659/rupkatha.v15n4.17
- Almatra /uniFB01 , O., Johri, A., &amp; Lee, H. (2024). A systematic review of AI literacy conceptualization, constructs, and implementation and assessment efforts (2019 -2023). Computers and Education Open , 6 (1), Article 100173. - Anderljung, M., &amp; Hazell, J. (2023). Protecting society from AI misuse: When are restrictions on capabilities warranted? (No. arXiv:2303.09377). arXiv. https://arxiv.org/abs/2303.09377
- Aromataris, E., Fernandez, R., Godfrey, C. M., Holly, C., Khalil, H., &amp; Tungpunkom, P. (2015). Summarizing systematic reviews: Methodological development, conduct and reporting of an umbrella review approach. International Journal of Evidence-Based Healthcare , 13 (3), 132 -140. - Author et al. (2024). Anonymized for blind review.
- Author et al. (under review). Anonymized for blind review.
- Ayanwale, M. A., Sanusi, I. T., Adelana, O. P., Aruleba, K. D., &amp; Oyelere, S. S. (2022). Teachers ' readiness and intention to teach arti /uniFB01 cial intelligence in schools. Computers and Education: Arti /uniFB01 cial Intelligence , 3 (1), Article 100099. - Bastani, H., Bastani, O., Sungu, A., Ge, H., Kabakc ı , ¨ O., &amp; Mariman, R. (2024). Generative AI can harm learning (SSRN Scholarly Paper No. 4895486). Social Science Research Network. - Bellas, F., Guerreiro-Santalla, S., Naya, M., &amp; Duro, R. J. (2023). AI curriculum for European high schools: An embedded intelligence approach. International Journal of Arti /uniFB01 cial Intelligence in Education , 33 (2), 399 -426. - Bewersdorff, A., Zhai, X., Roberts, J., &amp; Nerdel, C. (2023). Myths, mis- and preconceptions of arti /uniFB01 cial intelligence: A review of the literature. Computers and Education: Arti /uniFB01 cial Intelligence , 4 (3), Article 100143. - Carolus, A., Koch, M. J., Straka, S., Latoschik, M. E., &amp; Wienrich, C. (2023). MAILS - meta AI literacy scale: Development and testing of an AI literacy questionnaire based on wellfounded competency models and psychological change- and meta-competencies. Computers in Human Behavior: Arti /uniFB01 cial Humans , 1 (2), Article 100014. https://doi.org/10. 1016/j.chbah.2023.100014
* Casal-Otero, L., Catala, A., Fern´ andez-Morante, C., Taboada, M., Cebreiro, B., &amp; Barro, S. (2023). Ai literacy in k-12: A systematic literature review. International Journal of STEM Education , 10 (1), 29. - Cave, S., Coughlan, K., &amp; Dihal, K. (2019). ' scary robots ' : Examining public responses to AI. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, Honolulu, HI, USA, 27 -28 January, 2019, pp. 331 -337. https://doi.org/10.1145/ 3306618.3314232
- Çelebi, C., Yilmaz, F., DemiR, U., &amp; Karakus, F. (2023). Arti /uniFB01 cial intelligence, AI literacy, digital literacy, AI literacy scale. Ogretim Teknolojisi ve Hayat Boyu Ogrenme Dergisi Instructional Technology and Lifelong Learning . - Celik, I. (2023). Towards intelligent-TPACK: An empirical study on teachers ' professional knowledge to ethically integrate arti /uniFB01 cial intelligence (AI)-based tools into education. Computers in Human Behavior , 138 , Article 107468. https://doi.org/10.1016/j.chb.2022. 107468
- Cetindamar, D., Kitto, K., Wu, M., Zhang, Y., Abedin, B., &amp; Knight, S. (2024). Explicating AI literacy of employees at digital workplaces. IEEE Transactions on Engineering Management , 71 , 810 -823. - Chan, C. K. Y., &amp; Zhou, W. (2023). An expectancy value theory (EVT) based instrument for measuring student perceptions of generative AI. Smart Learning Environments , 10 (1), 64. - Chiu, T. K. F. (2021). A holistic approach to the design of arti /uniFB01 cial intelligence (AI) education for K-12 schools. TechTrends , 65 (5), 796 -807. - Chiu, T. K. F., Ahmad, Z., Ismailov, M., &amp; Sanusi, I. T. (2024). What are arti /uniFB01 cial intelligence literacy and competency? A comprehensive framework to support them. Computers and Education Open , 6 (6), Article 100171. - Council Of Chief State School Of /uniFB01 cers &amp; National Governors ' Association. (2009). Common core state standards initiative . https://www.corestandards.org
- European Commission, High-Level Expert Group on AI. (2019, June 26). Policy and investment recommendations for trustworthy arti /uniFB01 cial intelligence . European Commission. https:// digital-strategy.ec.europa.eu/en/library/policy-and-investment-recommendationstrustworthy-arti /uniFB01 cial-intelligence
* Fleger, C.-B., Amanuel, Y., &amp; Krugel, J. (2023). Learning tools using block-based programming for AI education. In 2023 IEEE Global Engineering Education Conference (EDUCON), Kuwait, 01 -04 May 2023, pp. 1 -5. https://doi.org/10.1109/EDUCON54358. 2023.10125154

* Gennari, R., Melonio, A., Pellegrino, M. A., &amp; D ' Angelo, M. (2023). How to playfully teach ai to young learners: A systematic literature review. In Proceedings of the 15th Biannual Conference of the Italian SIGCHI Chapter, Torino Italy, 20 -22 September, 2023, pp. 1 -9. - Ghallab, M. (2019). Responsible AI: Requirements and challenges. AI Perspectives , 1 (1), 3. - Ghimire, A., &amp; Edwards, J. (2024). From guidelines to governance: A study of AI policies in education. In A. M. Olney, I.-A. Chounta, Z. Liu, O. C. Santos, &amp; I. I. Bittencourt (Eds.), Arti /uniFB01 cial intelligence in education. Posters and late breaking results, workshops and tutorials, industry and innovation tracks, practitioners, doctoral consortium and blue sky (pp. 299 -307). Springer Nature Switzerland. - Grainger, M. J., Bolam, F. C., Stewart, G. B., &amp; Nilsen, E. B. (2020). Evidence synthesis for tackling research waste. Nature Ecology &amp; Evolution , 4 (4), 495 -497. https://doi.org/10. 1038/s41559-020-1141-6
- Heeg, D. M., &amp; Avraamidou, L. (2023). The use of arti /uniFB01 cial intelligence in school science: A systematic literature review. Educational Media International , 60 (2), 125 -150. https://doi. org/10.1080/09523987.2023.2264990
- Hornberger, M., Bewersdorff, A., &amp; Nerdel, C. (2023). What do university students know about arti /uniFB01 cial intelligence? Development and validation of an AI literacy test. Computers and Education: Arti /uniFB01 cial Intelligence , 5 (1), Article 100165. - Hwang, Y., Lee, J. H., &amp; Shin, D. (2023). What is prompt literacy? An exploratory study of language learners ' development of new literacy skill using generative AI (No. arXiv: 2311.05373). arXiv. - Jia, F., Sun, D., &amp; Looi, C. (2024). Arti /uniFB01 cial intelligence in science education (2013 -2023): Research trends in ten years. Journal of Science Education and Technology , 33 (1), 94 -117. - Kandlhofer, M., Steinbauer, G., Hirschmugl-Gaisch, S., &amp; Huber, P. (2016). Arti /uniFB01 cial intelligence and computer science in education: From kindergarten to university. In 2016 IEEE Frontiers in Education Conference (FIE), Erie, PA, USA, 12 -15 October 2016, pp. 1 -9. - Karaca, O., Çal ı s ¸kan, S. A., &amp; Demir, K. (2021). Medical arti /uniFB01 cial intelligence readiness scale for medical students (MAIRS-MS) -development, validity and reliability study. BMCMedical Education , 21 (1), 112. - Kim, K., Kwon, K., Ottenbreit-Leftwich, A., Bae, H., &amp; Glazewski, K. (2023). Exploring middle school students ' common naive conceptions of arti /uniFB01 cial intelligence concepts, and the evolution of these ideas. Education and Information Technologies , 28 (8), 9827 -9854. - Kim, S., Jang, Y., Kim, W., Choi, S., Jung, H., Kim, S., &amp; Kim, H. (2021). Why and what to teach: AI curriculum for elementary school. Proceedings of the AAAI Conference on Arti /uniFB01 cial Intelligence , 35 (17), 15569 -15576. - Kim, S. W., &amp; Lee, Y. (2022). The arti /uniFB01 cial intelligence literacy scale for middle school students. Journal of the Korea Society of Computer and Information , 27 (3), 225 -238. https://doi.org/ 10.9708/jksci.2022.27.03.225

* Kimiafar, K., Sarbaz, M., Tabatabaei, S. M., Ghaddaripouri, K., Mousavi, A. S., Raei Mehneh, M., &amp; Mousavi Baigi, S. F. (2023). Arti /uniFB01 cial intelligence literacy among healthcare professionals and students: A systematic review. Frontiers in Health Informatics , 12 , 168. - Koehler, M., &amp; Mishra, P. (2009). What is technological pedagogical content knowledge (TPACK)? Contemporary Issues in Technology and Teacher Education , 9 (1), 60 -70. https://citejournal.org/volume-9/issue-1-09/general/what-is-technologicalpedagogicalcontent-knowledge
- Kong, S.-C., Cheung, M. Y. W., &amp; Tsang, O. (2024). Developing an arti /uniFB01 cial intelligence literacy framework: Evaluation of a literacy course for senior secondary students using a projectbased learning approach. Computers and Education: Arti /uniFB01 cial Intelligence , 6 (5), Article 100214. - Kong, S. C., Cheung, M. Y. W., &amp; Zhang, G. (2021). Evaluation of an arti /uniFB01 cial intelligence literacy course for university students with diverse study backgrounds. Computers and Education: Arti /uniFB01 cial Intelligence , 2 (4), Article 100026. https://doi.org/10.1016/j.caeai. 2021.100026
- Laupichler, M. C., Aster, A., Meyerheim, M., Raupach, T., &amp; Mergen, M. (2024). Medical students ' AI literacy and attitudes towards AI: A cross-sectional two-center study using prevalidated assessment instruments. BMC Medical Education , 24 (1), 401. https://doi.org/10. 1186/s12909-024-05400-7
- Laupichler, M. C., Aster, A., Perschewski, J.-O., &amp; Schleiss, J. (2023). Evaluating AI courses: A valid and reliable instrument for assessing arti /uniFB01 cial-intelligence learning through comparative self-assessment. Education Sciences , 13 (10), 978. https://doi.org/10.3390/ educsci13100978
- Laupichler, M. C., Aster, A., &amp; Raupach, T. (2023). Delphi study for the development and preliminary validation of an item set for the assessment of non-experts ' AI literacy. Computers and Education: Arti /uniFB01 cial Intelligence , 4 (8), Article 100126. https://doi.org/10. 1016/j.caeai.2023.100126
* Laupichler, M. C., Aster, A., Schirch, J., &amp; Raupach, T. (2022). Arti /uniFB01 cial intelligence literacy in higher and adult education: A scoping literature review. Computers and Education: Arti /uniFB01 cial Intelligence , 3 (4), Article 100101. - Lawrence, L., Echeverria, V., Yang, K., Aleven, V., &amp; Rummel, N. (2024). How teachers conceptualise shared control with an AI co-orchestration tool: A multiyear teacher-centred design process. British Journal of Educational Technology , 55 (3), 823 -844. https://doi.org/ 10.1111/bjet.13372
- Lee, J., &amp; Jeong, H. (2023). Keyword analysis of arti /uniFB01 cial intelligence education policy in South Korea. IEEE Access , 11 , 102408 -102417. - Lee, S., &amp; Park, G. (2024). Development and validation of ChatGPT literacy scale. Current Psychology , 43 (21), 18992 -19004. * Lee, J., Wu, A. S., Li, D., &amp; Kulasegaram, K. M. (2021). Arti /uniFB01 cial intelligence in undergraduate medical education: A scoping review. Academic Medicine: Journal of the Association of American Medical Colleges , 96 (11S), S62 -S70. https://doi.org/10.1097/ACM. 0000000000004291

- Lehmann, M., Cornelius, P. B., &amp; Sting, F. J. (2024). AI meets the classroom: When does ChatGPT harm learning? No. arXiv:2409.09047; Version 1. arXiv. https://arxiv.org/abs/ 2409.09047
- Lin, P., &amp; Van Brummelen, J. (2021). Engaging teachers to co-design integrated AI curriculum for K-12 classrooms. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, Yokohama, Japan, 8 -13 May, 2021, pp. 1 -12. https://doi.org/10.1145/ 3411764.3445377
- Lintner, T. (2024). A systematic review of AI literacy scales. Npj Science of Learning , 9 (1), 50. - Long, D., &amp; Magerko, B. (2020). What is AI literacy? Competencies and design considerations. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, Honolulu, HI, USA, April, 23, 2020, pp. 1 -16. https://doi.org/10.1145/ 3313831.3376727
- Mertala, P., &amp; Fagerlund, J. (2024). Finnish 5th and 6th graders ' misconceptions about arti /uniFB01 cial intelligence. International Journal of Child-Computer Interaction , 39 , Article 100630. - Moodi Ghalibaf, A., Moghadasin, M., Emadzadeh, A., &amp; Mastour, H. (2023). Psychometric properties of the persian version of the medical arti /uniFB01 cial intelligence readiness scale for medical students (MAIRS-MS). BMC Medical Education , 23 (1), 577. https://doi.org/10. 1186/s12909-023-04553-1
- Morales-Garc´ ı a, W. C., Sairitupa-Sanchez, L. Z., Morales-Garc´ ı a, S. B., &amp; Morales-Garc´ ı a, M. (2024). Adaptation and psychometric properties of a brief version of the general selfef /uniFB01 cacy scale for use with arti /uniFB01 cial intelligence (GSE-6AI) among university students. Frontiers in Education , 9 , 1293437. - Ng, D. T. K., Leung, J. K. L., Chu, K. W. S., &amp; Qiao, M. S. (2021a). Ai literacy: De /uniFB01 nition, teaching, evaluation and ethical issues. Proceedings of the Association for Information Science and Technology , 58 (1), 504 -509. - Ng, D. T. K., Leung, J. K. L., Chu, S. K. W., &amp; Qiao, M. S. (2021b). Conceptualizing AI literacy: An exploratory review. Computers and Education: Arti /uniFB01 cial Intelligence , 2 (2008), Article 100041. * Ng, D. T. K., Lee, M., Tan, R. J. Y ., Hu, X., Downie, J. S., &amp; Chu, S. K. W. (2023). A review of AI teaching and learning from 2000 to 2020. Education and Information Technologies , 28 (7), 8445 -8501. - Oelschlager, R. (2024). Evaluating the impact of hallucinations on user trust and satisfaction in LLM-based systems . https://urn.kb.se/resolve?urn=urn:nbn:se:lnu:diva-130539
* Oyelere, S. S., Sanusi, I. T., Agbo, F. J., Oyelere, A. S., Omidiora, J. O., Adewumi, A. E., &amp; Ogbebor, C. (2022). Arti /uniFB01 cial intelligence in african schools: Towards a contextualized approach. In 2022 IEEE Global Engineering Education Conference (EDUCON), Tunis, Tunisia, 28 -31 March 2022, pp. 1577 -1582. https://doi.org/10.1109/EDUCON52537. 2022.9766550
- Page, M. J., McKenzie, J. E., Bossuyt, P. M., Boutron, I., Hoffmann, T. C., Mulrow, C. D., Shamseer, L., Tetzlaff, J. M., Akl, E. A., Brennan, S. E., Chou, R., Glanville, J., Grimshaw, J. M., Hróbjartsson, A., Lalu, M. M., Li, T., Loder, E. W., Mayo-Wilson, E., McDonald,

- S., … Moher, D. (2021). The prisma 2020 statement: An updated guideline for reporting systematic reviews. BMJ , 372 , n71. - Park, H., Kim, H. S., &amp; Park, H. W. (2021). A scientometric study of digital literacy, ICT literacy, information literacy, and media literacy. Journal of Data and Information Science , 6 (2), 116 -138. - Pinski, M., &amp; Benlian, A. (2023). AI literacy -towards measuring human competency in arti /uniFB01 cial intelligence. In Hawaii International Conference on System Sciences 2023 (HICSS56), Online, 7-1-2023, 3 . https://aisel.aisnet.org/hicss-56/cl/ai\_and\_future\_work/3
- Pinski, M., &amp; Benlian, A. (2024). AI literacy for users -a comprehensive review and future research directions of learning methods, components, and effects. Computers in Human Behavior: Arti /uniFB01 cial Humans , 2 (1), Article 100062. https://doi.org/10.1016/j.chbah.2024. 100062
- Polak, S., Schiavo, G., &amp; Zancanaro, M. (2022). Teachers ' perspective on arti /uniFB01 cial intelligence education: An initial investigation. In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems, New Orleans, LA, USA, 29 April -5 May 2022, pp. 1 -7. - Polatgil, M., &amp; Güler, A. (2023). Bilim. Nicel Aras ¸t ı rmalar Derg , 3 , 99 -114. https://sobinarder. com/index.php/sbd/article/view/65
* Pupic, N., Ghaffari-zadeh, A., Hu, R., Singla, R., Darras, K., Karwowska, A., &amp; Forster, B. B. (2023). An evidence-based approach to arti /uniFB01 cial intelligence education for medical students: A systematic review. PLOS Digital Health , 2 (11), Article e0000255. https://doi.org/10. 1371/journal.pdig.0000255
* Rizvi, S., Waite, J., &amp; Sentance, S. (2023). Arti /uniFB01 cial intelligence teaching and learning in k-12 from 2019 to 2022: A systematic literature review. Computers and Education: Arti /uniFB01 cial Intelligence , 4 , Article 100145. - Robinson, K. A., Brunnhuber, K., Ciliska, D., Juhl, C. B., Christensen, R., Lund, H., &amp; EvidenceBased Research Network. (2021). Evidence-based research series-paper 1: What evidencebased research is and why is it important? Journal of Clinical Epidemiology , 129 (3), 151 -157. - Salamin, A. D., Russo, D., &amp; Rueger, D. (2023). ChatGPT, an excellent liar: How conversational agent hallucinations impact learning and teaching. In Proceedings of the 7th International Conference on Teaching, Learning and Education, Copenhagen, Denmark, 2023-11.
* Sanusi, I. T., Oyelere, S. S., Agbo, F. J., &amp; Suhonen, J. (2021). Survey of resources for introducing machine learning in k-12 context. In 2021 IEEE Frontiers in Education Conference (FIE), Lincoln, NE, USA, 13 -16 October 2021, pp. 1 -9. https://doi.org/10.1109/ FIE49875.2021.9637393
* Sanusi, I. T., Oyelere, S. S., Vartiainen, H., Suhonen, J., &amp; Tukiainen, M. (2023). A systematic review of teaching and learning machine learning in k-12 education. Education and Information Technologies , 28 (5), 5967 -5997. https://doi.org/10.1007/ s10639-022-11416-7
- Schiff, D. (2022). Education for AI, not AI for education: The role of education and ethics in national AI policy strategies. International Journal of Arti /uniFB01 cial Intelligence in Education , 32 (3), 527 -563. - Song, J., Zhang, L., Yu, J., Peng, Y., Ma, A., &amp; Lu, Y. (2022). Paving the way for novices: How to teach AI for K-12 education in China. Proceedings of the AAAI Conference on Arti /uniFB01 cial Intelligence , 36 (11), 12852 -12857. - Song, Y., Tian, X., Regatti, N., Katuka, G. A., Boyer, K. E., &amp; Israel, M. (2024). Arti /uniFB01 cial intelligence unplugged: Designing unplugged activities for a conversational AI summer camp. Proceedings of the 55th ACM Technical Symposium on Computer Science Education V , 1 , 1272 -1278. - Soto-San /uniFB01 el, M. T., Angulo-Brunet, A., &amp; Lutz, C. (2024). The scale of arti /uniFB01 cial intelligence literacy for all (SAIL4ALL): A tool for assessing knowledge on arti /uniFB01 cial intelligence in all adult populations and settings . - Sparks, J. R., Ober, T. M., Tenison, C., &amp; Arslan, B. (2024). Opportunities and challenges for assessing digital and AI literacies .
- Steinbauer, G., Kandlhofer, M., Chklovski, T., Heintz, F., &amp; Koenig, S. (2021). A differentiated discussion about AI education K-12. KI - Künstliche Intelligenz , 35 (2), 131 -137. https://doi. org/10.1007/s13218-021-00724-8
- Su, J., &amp; Zhong, Y. (2022). Arti /uniFB01 cial intelligence (AI) in early childhood education: Curriculum design and future directions. Computers and Education: Arti /uniFB01 cial Intelligence , 3 (1), Article 100072. - *Su, J., Guo, K., Chen, X., &amp; Chu, S. K. W. (2023). Teaching arti /uniFB01 cial intelligence in k -12 classrooms: A scoping review. Interactive Learning Environments , 32 (9), 5207 -5226. * Su, J., Ng, D. T. K., &amp; Chu, S. K. W. (2023). Arti /uniFB01 cial intelligence (AI) literacy in early childhood education: The challenges and opportunities. Computers and Education: Arti/uniFB01 cial Intelligence , 4 (1), Article 100124. - Sutton, A., Clowes, M., Preston, L., &amp; Booth, A. (2019). Meeting the review family: Exploring review types and associated information retrieval requirements. Health Information and Libraries Journal , 36 (3), 202 -222. * Su, J., Zhong, Y., &amp; Ng, D. T. K. (2022). A meta-review of literature on educational approaches for teaching ai at the k-12 levels in the asia-paci /uniFB01 c region. Computers and Education: Arti /uniFB01 cial Intelligence , 3 (1), Article 100065. - Tatar, C., Jiang, S., Ros´ e, C. P., &amp; Chao, J. (2024). Exploring teachers ' views and con /uniFB01 dence in the integration of an arti /uniFB01 cial intelligence curriculum into their classrooms: A case study of curricular co-design program. International Journal of Arti /uniFB01 cial Intelligence in Education . - Tedre, M., Toivonen, T., Kahila, J., Vartiainen, H., Valtonen, T., Jormanainen, I., &amp; Pears, A. (2021). Teaching machine learning in K -12 classroom: Pedagogical and technological trajectories for arti /uniFB01 cial intelligence education. IEEE Access , 9 , 110558 -110572. https://doi. org/10.1109/ACCESS.2021.3097962
- Torrey, L. (2021). Teaching problem-solving in algorithms and AI. Proceedings of the AAAI Conference on Arti /uniFB01 cial Intelligence , 26 (3), 2363 -2367. - Touretzky, D., Gardner-McCune, C., Martin, F., &amp; Seehorn, D. (2019). Envisioning AI for K-12: What should every child know about AI? Proceedings of the AAAI Conference on Arti /uniFB01 cial Intelligence , 33 (01), 9795 -9799. - UNESCO. (2022). K-12 AI curricula: A mapping of government-endorsed AI curricula. https:// unesdoc.unesco.org/ark:/48223/pf0000380602
- U.S. Department of Education, Of /uniFB01 ce of Educational Technology. (2024). EdTech leaders AI toolkit: Guidelines for safe, ethical, and equitable AI integration . https://tech.ed.gov/ /uniFB01 les/ 2024/10/ED-OET-EdLeaders-AI-Toolkit-10.24.24.pdf
- Veluru, C. (2024). Responsible arti /uniFB01 cial intelligence on large scale data to prevent misuse, unethical challenges and security breaches. Journal of Arti /uniFB01 cial Intelligence &amp; Cloud Computing , 1 -6. - Voulgari, I., Zammit, M., Stouraitis, E., Liapis, A., &amp; Yannakakis, G. (2021). Learn to machine learn: Designing a game based approach for teaching machine learning to primary and secondary education students. In Interaction Design and Children, Athens, Greece, 24 June 2021, pp. 593 -598. - Wang, B., Rau, P.-L. P., &amp; Yuan, T. (2023). Measuring user competence in using arti /uniFB01 cial intelligence: Validity and reliability of arti /uniFB01 cial intelligence literacy scale. Behaviour &amp; Information Technology , 42 (9), 1324 -1337. https://doi.org/10.1080/0144929x.2022. 2072768
- Wang, N., &amp; Lester, J. (2023). K-12 education in the age of AI: A call to action for K-12 AI literacy. International Journal of Arti /uniFB01 cial Intelligence in Education , 33 (2), 228 -232. - Wang, X., Li, X., &amp; Huang, J. (2023). Junior high school arti /uniFB01 cial intelligence literacy: Connotation, evaluation and promotion strategy. Open Journal of Social Sciences , 11 (05), 33 -49. - Wang, Y.-Y., &amp; Chuang, Y.-W. (2024). Arti /uniFB01 cial intelligence self-ef /uniFB01 cacy: Scale development and validation. Education and Information Technologies , 29 (4), 4785 -4808. https://doi.org/ 10.1007/s10639-023-12015-w
- Wienrich, C., &amp; Carolus, A. (2021). Development of an instrument to measure conceptualizations and competencies about conversational agents on the example of smart speakers. Frontiers of Computer Science , 3 , 685277. - Williams, R. (2023). A review of assessments in K-12 AI literacy curricula.
- Wu, C., Li, Y., Li, J., Zhang, Q., &amp; Wu, F. (2021). Web-based platform for K-12 AI education in China. Proceedings of the AAAI Conference on Arti /uniFB01 cial Intelligence , 35 (17), 15687 -15694. - Xiong, Y., Ling, Q., &amp; Liu, J. (2023). Educational missions and tactics in the EU arti /uniFB01 cial intelligence strategy in the era of digital transformation: A policy analysis based on EU documents published during 2018 -2022. In C.-H. Chen, A. Scapellato, A. Barbiero, &amp; D. G. Korzun (Eds.), Advances in transdisciplinary engineering . IOS Press. https://doi.org/10. 3233/ATDE230999
- Yau, K. W., Chai, C. S., Chiu, T. K. F., Meng, H., King, I., Wong, S. W.-H., &amp; Yam, Y. (2022). CoDesigning arti /uniFB01 cial intelligence curriculum for secondary schools: A grounded theory of teachers ' experience. In 2022 International Symposium on Educational Technology (ISET), Hong Kong, 19 -22 July 2022, pp. 58 -62. - Yilmaz, F. G. K., &amp; Yilmaz, R. (2023). Yapay Zek ˆ a Okuryazarl ı ˘ g ı ¨ Olçe ˘ ginin Türkçeye Uyarlanmas ı . Bilgi ve ˙ Iletis ¸im Teknolojileri Dergisi , 5 (2), Article 2.

- *Yim, I. H. Y., &amp; Su, J. (2024). Arti /uniFB01 cial intelligence (ai) learning tools in k-12 education: A scoping review. Journal of Computers in Education . - Yue, M., Jong, M. S.-Y., &amp; Ng, D. T. K. (2024). Understanding K -12 teachers ' technological pedagogical content knowledge readiness and attitudes toward arti /uniFB01 cial intelligence education. Education and Information Technologies , 29 (15), 19505 -19536. https://doi.org/10. 1007/s10639-024-12621-2
- *Yue, M., Jong, M. S.-Y., &amp; Dai, Y. (2022). Pedagogical design of k-12 arti /uniFB01 cial intelligence education: A systematic review. Sustainability , 14 (23), Article 15620. https://doi.org/10. 3390/su142315620
- Zhai, C., Wibowo, S., &amp; Li, L. D. (2024). The effects of over-reliance on AI dialogue systems on students ' cognitive abilities: A systematic review. Smart Learning Environments , 11 (1), 28. - Zhang, H., Perry, A., &amp; Lee, I. (2024). Developing and validating the arti /uniFB01 cial intelligence literacy concept inventory: An instrument to assess arti /uniFB01 cial intelligence literacy among middle school students. International Journal of Arti /uniFB01 cial Intelligence in Education , 35 (1), 398 -438. ## Author Biographies

Shan Zhang is a Ph.D. student specializing in Educational Technology at the University of Florida. Her research focuses on designing theoretically grounded AIpowered learning technologies to foster AI literacy, enhance student engagement, and support meaningful interactions. She integrates multimodal learning analytics, educational data mining, and human-centered design principles to gain deeper insights into students ' learning experiences. Her recent work explores the integration of AI into K12 education, leveraging natural language processing techniques to analyze collaborative learning features and affect in STEM+C+AI education, and develops learner models to better understand and support student learning experiences.

Priyadharshini Ganapathy Prasad is a doctoral student in the Educational Technology program at the University of Florida. Her research lies at the intersection of computer science (CS) education, arti /uniFB01 cial intelligence (AI) literacy, and the integration of multimodal data from learning platforms.Through a synthesis of qualitative and quantitative methods, including learning analytics, data mining, and user-centered design, she seeks to uncover actionable insights to support computer science and AI education.

Noah L. Schroeder Ph.D., is a Research Scientist at the University of Florida. His research focuses on pedagogical agents, the design of multimedia learning environments, and research synthesis.