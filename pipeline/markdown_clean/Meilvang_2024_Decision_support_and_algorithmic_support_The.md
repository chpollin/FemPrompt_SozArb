---
source_file: Meilvang_2024_Decision_support_and_algorithmic_support_The.pdf
conversion_date: 2026-02-03T09:10:36.674648
converter: docling
quality_score: 95
---

<!-- image -->

## European Journal of Social Work

ISSN: 1369-1457 (Print) 1468-2664 (Online) 

## Decision support and algorithmic support: the construction of algorithms and professional discretion in social work

Beslutningsstøtte og algoritmisk støtte: Konstruktionen af algoritmer og det professionelle skøn i socialt arbejde

## Marie Leth Meilvang &amp; Anne Marie Dahler

To cite this article: Marie Leth Meilvang &amp; Anne Marie Dahler (2024) Decision support and algorithmic support: the construction of algorithms and professional discretion in social work, European Journal of Social Work, 27:1, 30-42, DOI: 10.1080/13691457.2022.2063806

To link to this article: <!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

Published online: 26 Apr 2022.

Submit your article to this journal

Article views: 1573

View related articles

View Crossmark data

Citing articles: 15 View citing articles

曲

CrossMark

<!-- image -->

<!-- image -->

<!-- image -->

disse algoritmer rekon /uniFB01 gurerer ideers om det professionelle skøn. Gennem analysen af tre vigtige temaer, standardisering, objektivisering og at eliminere bias, dokumenterer vi den tætte relation mellem de analyserede algoritmer og det professionelle skøn og det professionelle skøns ambivalente rolle.

## Introduction: decision support

Arti /uniFB01 cial intelligence and algorithms are increasingly being used in social work (Eubanks, 2018; Gillingham, 2019; Petersen et al., 2021; Ting et al., 2018). These technologies are, among other things, said to enhance the consistency and transparency in social work (Coulthard et al., 2020). Whereas some technologies automate speci /uniFB01 c professional work processes, algorithms in social work are most frequently used as systems that work as ' decision support ' . In ' decision-support ' systems the relationship between professional judgement and algorithms is often more complicated (Dearman, 2005: Gillingham, 2009; Høybye-Mortensen, 2015). These systems are not intended to take over and automate professional discretion, but to ' support ' the process of making decisions, and thus have the potential for shaping professional discretion in novel ways.

Since Lipsky ' s (1980) study of how street level bureaucrats use discretion to interpret and modify formal rules, the role of professional discretion has been theorised and debated as a central part of the practice of social workers (see Evans, 2011; Evans &amp; Harris, 2004; Møller, 2019; Taylor, 2012; Taylor &amp; Whittaker, 2018). There is a growing literature on the e /uniFB00 ects of digitalisation on frontline work and professional discretion (Bovens &amp; Zouridis, 2002; Bu /uniFB00 at, 2015; Busch &amp; Henriksen, 2018; Petersen et al., 2020). In short, these studies highlight how technologies alter professional discretionary practices, either by limiting the room for discretion (e.g. Bovens &amp; Zouridis, 2002) or by enabling other ways of exercising discretionary power (e.g. Bu /uniFB00 at, 2015). In this paper, we add to these studies by taking a closer look at the way managers, developers, and front line workers construct professional discretion in relation to decision-support algorithms. We analyse the intensions, designs, and workings of these systems in casework with vulnerable children and families for the purpose of answering the following questions: What is meant by professionals needing ' decision support ' and how do these algorithms recon /uniFB01 gure ideas about professional discretion?

We follow Bu /uniFB00 at ' s idea that studies of technological change and street-level bureaucracy must /uniFB01 nd a balanced position between ' technological determinism ' and ' social determinism ' (Bu /uniFB00 at, 2015, p. 157). This resonates with a theoretical focus inspired by science and technology-studies, STS, which have demonstrated that technologies are not neutral tools that possess a kind of mechanical objectivity (e.g. Latour, 1992; Mol, 2008), but play an important role in the process of shaping knowledge and professional practices. Thus, we analyse not only what an algorithm does in technical terms, but also the way in which algorithms recon /uniFB01 gure ideas about professional discretion. Empirically, we attend to the area of algorithms in social work with vulnerable children in Denmark. The article will proceed with a theory section where we present the literature on frontline work, technology, algorithms, and professional discretion. Then, we present our data, methods, and analytical strategy. Our analytical section begins with an analysis of the political background for the development and implementation of algorithms in social work, relating this to the way political and public actors problematise professional discretion. Then, we analyse algorithmic decision support systems as solutions to this problem, speci /uniFB01 cally relating to three themes: standardising casework, objectivising casework, and eliminating bias from casework. We conclude that these algorithms do not work on their own, as they are as much supported by professional decisions and discretion as professional discretion is supported by the algorithmic systems. This leaves the role for professional discretion in relation to algorithmic decision-support systems ambivalent and undecided.

## Theory: professional discretion and technological systems

Professional discretion is a central part of professional work (Evans, 2011; Lipsky, 1980). Also in social work, actors in the academic, political, and public arena have debated the role of professional discretion and the knowledge and autonomy of social workers (Høybye-Mortensen, 2015). In general, the knowledge base of social work has been challenged in many Westerns countries and attempts have been made to base social work models and methods on ' evidence ' (e.g. Morago, 2007; Møller, 2019) and thus to standardise the work processes (Burton &amp; Van den Broek, 2009; Parton, 2008; Ponnert &amp; Svensson, 2016). As these studies have shown, the process of standardisation and the political demand for more documentation, speci /uniFB01 cally using tools such as information and communication technologies, challenge professional discretion in a traditional sense.

Technology changes traditional discretionary practices of street-level workers to a process that involves information processing and screen-work (Busch et al., 2018). Bovens and Zouridis (2002) conceptualise the change for street-level bureaucracy in relation to digitalisation and ICT by developing a framework categorising this change from street-level bureaucracy to screen-level bureaucracy to system-level bureaucracy. These three types of bureaucracy depends on, among other things, the role of ICT and the room for professional discretion. As the role of ICTs develops from supportive to leading to decisive, the room for discretion goes from ample to little to no discretion at all. In Bu /uniFB00 at ' s review (2015), he /uniFB01 nds that some studies support this idea that street-level discretion decreases or disappears, when implementing and using ICTs. On the other hand, other studies show how new technologies may shape, but also enable, frontline workers ' discretionary powers (Bu /uniFB00 at, 2015; see also Jorna &amp; Wagenaar, 2007). Speci /uniFB01 cally regarding algorithms and arti /uniFB01 cial intelligence, some studies show how algorithms shift the discretionary power from the street-level bureaucrat to the engineer (Alkhatib &amp; Bernstein, 2019).

The literature speci /uniFB01 cally on algorithms in social work is still sparse (see however a recent comparative study on predictive algorithms in child protection services, Jørgensen et al., 2021). Some studies show that social workers express concern and unease with working with algorithmic systems (Brown et al., 2019), and that case workers, working with AI systems, are concerned with how data about citizens are stored and interpreted and therefore leave certain data out of the systems (Petersen et al., 2021). A study in a Danish context, however, shows that even if social workers are aware of the potential risks of relying too heavily on algorithmic systems, they also welcome algorithms as a way to qualify for instance how to prioritise cases (Lund, 2019). A recent comparative study on predictive algorithms in child protections services analyze the political context of such algorithmic tools, and the way they are constructed in political discourse (Jørgensen et al., 2021). For the Danish case, Jørgensen and colleagues /uniFB01 nd that ' a child-focused protectionist orientation intersects with digitalisation strategies emphasising faster, more e /uniFB03 cient, and better quali /uniFB01 ed decision-making ' (Jørgensen et al., 2021, p. 9).

In our analysis and discussion, we follow the idea that studies of technological change and streetlevel bureaucracy must /uniFB01 nd a balanced position between ' technological determinism ' (everything is decided by the technology) and ' social determinism ' (everything is socially determined) (Bu /uniFB00 at, 2015, p. 157). Inspired by STS, we look at the way in which the algorithmic system will shape and recon /uniFB01 gure the role and meaning of professional discretion, and how the introduction of this speci /uniFB01 c technology also introduces a speci /uniFB01 c way of valuing and considering professional discretion in social work. We therefore analyse the intentions, designs, and workings of decision-support systems in social work.

## Data, methods, and analytical strategy

Our analysis is thus a mapping of the area of algorithms of social work in Denmark, and a case-analysis of three Danish cases. Across these cases, we compare and contrast intentions, design and workings of the algorithms to understand the underlying ideas related to algorithms and professional

<!-- image -->

discretion. Therefore our unit of analysis is the general ideas in the area that we have found across the mapping of the cases.

The study is designed as a two-step qualitative study, involving both digital interviews, observations and document analysis. As our research interest was algorithms and social work, step one was to gain insight through interviews with relevant actors in the /uniFB01 eld. In our initial mapping, we interviewed people from two important organisations related to the area, namely the Danish Association of Social Workers (the vice president) and the National Association of Municipalities (Expert Consultant, digitalisation). We also interviewed developers that had worked for municipalities on similar projects to understand their work with municipalities. The aim of these interviews was to mapour existing projects in the area of social work, and to investigate the intentions with and expectations for algorithms in social work. The interviews led us to three municipal projects with algorithmic support systems in social work with vulnerable children and families, which we, as step two, investigated further in di /uniFB00 erent ways (Table 1):

- The /uniFB01 rst project (Rita Referral) was situated in the capital area of Denmark. In this development project, the municipality tested an algorithm to support social workers in sorting referrals in acute and non-acute referrals. Data from this project consists of interviews with four assistants, who are in charge of registration of referrals, and two social workers, who handle the referrals after the initial registration. Data also include interviews with the project manager of the algorithm project, a digital consultant also involved in the project, the data scientist who developed the algorithm, and the manager of the unit where the algorithm was tested. Lastly, we observed two digital meetings among social workers in the unit discussing referrals, and we observed digitally assistants registering referrals and working with the algorithm.
- The second project (The Algorithm Project) takes place in a municipal department in a large Danish city. In this project, an algorithm for categorising referrals is being developed by IT consultants from a private company. The speci /uniFB01 c design of the algorithm was not decided in the time, we followed the project. Data from this project consists of two expert group-meetings held on-line with various stakeholders in the project (the digital CEO from the municipality, the municipal project manager, the IT consultants and many more). Two of the 11 initial interviews are related to this project.
- The third project (Referrals in Focus) is a continuation of a pilot project that took place between 2017 and 2018. In the /uniFB01 rst part of the project an algorithm to support social workers in risk assessment was developed and tested in two municipalities. In the second part, running from 2019 to 2023, the long term e /uniFB00 ects of using the algorithm are evaluated. From this project, we had informal talks with expert participants and made document analysis of on-line materials and academic papers pertaining to the project.

Municipalities organise their handling of referrals in di /uniFB00 erent ways, but legally, a referral has to be screened within 24 h. The initial handling of referrals, therefore, is a categorization , involving no other information than the information in the referral. Here, referrals are sorted into acute or non-acute. On the other side of the 24-hours assessment, there is more room for discretion (Lund, 2019, p. 24), when social workers decide di /uniFB00 erent measures and interventions. In the initial categorisation, there are no standards or tools for deciding whether social workers should categorise a referral as acute. This decision is therefore based on experience with the /uniFB01 eld and process, a sort of ' knowledge-how ' (Møller, 2022; Ryle, 1945), de /uniFB01 ned as a skill and expertise, grounded in practice and learning-by-doing.

Due to the covid-pandemic all interviews were conducted on-line. The interviews were carried out as semi-structured interviews, inspired by ' active interviewing ' as described by Holstein and Gubrium (1995). The research is conducted in accordance with The Danish Code of Conduct for Research Integrity and the local university ' s policy on research integrity.

<!-- image -->

Table 1. Sites, projects, and materials.

| In Table 1, we describe for the three projects at which point in the referral process, the algorithm is supposed to   | Project name Algorithm: data/function/status                                                                                                                                                                                                                                                                   | Materials                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|-----------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Various                                                                                                               | In general                                                                                                                                                                                                                                                                                                     | 11 interviews with 14 municipal leaders, developers (working for municipalities), and professional and political organisations                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Project 1                                                                                                             | ' The algorithm ' Rita Referral ' Uses historical data/former referrals and works by natural language processing. Supports initial categorisation of acute/non-acute The algorithm was already developed We followed the testing phase                                                                         | 4 interviews with assistants 1 interview with two social workers 1 interview with the project manager of the algorithm project (and several informal talks) 1 interview with a digital consultant from the municipality 1 interview with the data scientist who developed the algorithm 1 interview with the manager of the unit where the algorithm was tested 2 sessions where we observed digitally assistants handling referrals in the mailbox 2 digital sessions where we observed a team of social workers discuss referrals Document analysis of documents related |
| Project 2                                                                                                             | ' The Algorithm project ' Uses historical data/former referrals Still under development, but is expected to support initial categorisation of acute/non-acute                                                                                                                                                  | to the project Observation as participants in two digital expert-group meetings with representatives from the municipality, developers, and other stake-holders                                                                                                                                                                                                                                                                                                                                                                                                            |
| Project 3                                                                                                             | ' Referrals in Focus ' Uses historical data including referrals, former interventions, and various data on the parents. Supports both initial categorisation and the later judgement on proper intervention by giving a risk score The algorithm has been tested for the /uniFB01 rst time, and is in a second | Informal talks with expert participants Document analysis of on-line materials and academic papers pertaining to the project                                                                                                                                                                                                                                                                                                                                                                                                                                               |

The interviews were recorded and transcribed. All transcriptions, observational notes, and documents were coded abductively (Tavory &amp; Timmermans, 2014), using both inductive and deductive codes. With inspiration from situational analysis (Clarke, 2005), we coded various human and nonhuman actors (e.g. social worker, data, algorithm, referral, professional discretion) and their relationships. A coding example is shown in the table below (Table 2).

Subsequently the analysis was guided by analytical questions: How are algorithmic decisions and professional discretion related in the material? What are the intentions of using algorithmic tools? What are algorithms supposed to do to professional discretion and vice versa? What does it tell us about how professional discretion is considered and valued? We worked back and forth between the theoretical repertoire and the coded empirical material; this analytical work led us to

Table 2. Coding example.

| Excerpt from an interview                                                                                                                                                          | Relation                                                                                                 | Code              | Theme           |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|-------------------|-----------------|
| ' now I am categorizing referrals today, and I am supported in this way and led in this direction. Then, it is you tomorrow, and you are led in the same direction ' [C3, manager] | The algorithm is supposed to lead di /uniFB00 erent social workers categorizations in the same direction | Unify social work | Standardization |

the three themes presented below. This is not, we argue, an exhaustive list of themes, but these themes consistently appeared in our analysis of the material and across our cases. Related to for instance implementation of algorithmic systems on a broader scale, other themes could be relevant and important.

The study is limited to examining the development and testing phases of algorithms in social work and access to use of algorithms in practice has been limited. Therefore, our conclusions regard only the intentions, designs, and workings in this initial phase. It is possible that this changes, as municipalities implement algorithmic systems on larger scales.

## Analysis

In our analytical section, we /uniFB01 rst outline the public and political context for the introduction of algorithmic systems in social work. This context is part of how actors will shape and de /uniFB01 ne the technologies -and how technologies will, in turn, shape this context. Next, we perform a more detailed analysis of the intentions of the algorithm and its relation to professional discretion. Speci /uniFB01 cally, we focus on the three important themes, we found in the analysis: standardising, objectivising, and eliminating bias.

## Political context: public criticism of social work with vulnerable children and families

Municipal intervention within the area of social work with vulnerable children and families is closely linked with public criticism of this work, stemming in part from the media coverage of speci /uniFB01 c cases, where municipalities have intervened too late. In Denmark, the public is familiar with the ' Tønder ' and ' Brønderslev ' cases (Tønder and Brønderslev are Danish municipalities), in which children were placed in foster care or institutions after years of abuse and neglect and several referrals to the municipal social authorities. These speci /uniFB01 c cases have led to considerable revised legislation within social work, for instance the rule that educated sta /uniFB00 must screen referrals 24 h after reception.

Even though the Danish government introduced major reforms in 2013, these speci /uniFB01 c cases nonetheless shape the political context for work with decision-support algorithms. All three of the currently active Danish projects, we analyse -and know of -in social work with children and families, are algorithms that categorise referrals. Asked about why a speci /uniFB01 c algorithm-project was about categorising referrals, a manager in a municipality says:

… it ' s the worst part about being in a politically controlled system, when someone panics about worst case, ' Now we have a new Brønderslev or Tønder case ' or whatever they were called. Basically, these are manual human errors [ … ] And if we get this kind of /uniFB01 ltering system, and it ' s /uniFB02 ashing before our eyes, then someone will notice. This means that, also politically, there would be safety in the fact that manual human errors in such serious matters will not happen.

In this and other quotes, municipal managers describe the political ' anxiety ' and ' panic ' about the wrong handling of speci /uniFB01 c cases as a reason for initiating algorithmic decision-support-projects targeted at the categorisation of referrals. The manager here describes it as an issue of safety and security: An algorithm will be secure against human errors. This is a speci /uniFB01 c construction of the professional discretion issue in social work: this manager and others construct the professional process of management and decision concerning referrals as an area where professional judgement is in need of support to secure it against errors.

However, this does not match the professional understanding. Consistently, social workers replied that they did not consider the initial screening of referrals as di /uniFB03 cult or as an area where they might need support. This is not to say that social workers and the managers close to them did not have ideas about how to improve their work, for instance by using algorithms and predictive risk models. However, professionals did not perceive the particular area of categorising referrals as one that was di /uniFB03 cult to manage. The reason, why referrals were picked, then, has

to do with the political need for feeling safe -for feeling that nothing critical has been overlooked. However, as another municipal manager told us, it might also have to do with the fact that there are many referrals in social work: In 2019, there were more than 130,000 nationally and the number is rising. The sheer volume of referrals constitutes ideal data for the development of an algorithm.

Other recent studies of algorithmic systems in Denmark have shown that the political ambition with such systems is, among other things, to minimise errors in case management (Jørgensen et al., 2021). The intention of introducing algorithmic systems, then, is to provide a kind of safety for municipalities and a protection against poorly managed cases. We found that the system is intended to ensure this by addressing (at least) three things in respect of casework: standardising, objectivising, and eliminating bias. Standardisation relates to the professional work processes in social work, objectivity to the way data is constructed, and bias to the algorithm itself. These themes depict how algorithms are intended to perform decision support and also con /uniFB01 gure professional discretion in speci /uniFB01 c ways.

## Standardising: uniformity or contextual information

A recurrent criticism of social work with vulnerable children is that casework is considered arbitrary by the public and is dependent on ' the name of the social worker ' . Both managers, front-level professionals, and people from professional and political organisations recount this criticism as a reason for developing algorithms, and agree to some extend that it is a problem. As one municipal manager says: If it ' s Charles who has it, then it ' ll work perfectly, but if it ' s Anne, then it ' ll take three days longer. This isn ' t okay, and that ' s what it ' s all about. It ' s not fair to the citizens that it ' s accidental who catches it [their case].

One of the ways managers, developers, and front-line professionals expect the algorithm to support professional discretion is by standardising casework. According to one municipal manager, the discussion of eliminating di /uniFB00 erences in casework was an important argument for social workers in seeing the potentials of algorithms and other digital solutions. The manager recounts, how the social workers were aware of the criticism and dissatisfaction of citizens, who had experienced di /uniFB00 erent handling of similar cases. We also found this point of view in interviews with social workers themselves. Many of the interviewed actors expect algorithms to standardise discretion by aligning work processes (Lund, 2019, makes this same point).

Studies describe standardisation as a process of constructing uniformity across time and space through the generation of agreed-upon rules (Bowker &amp; Star, 1999; Timmermans &amp; Epstein, 2010). Such agreed-upon rules are not always evident or obvious in social work with vulnerable children and families, and with respect to algorithms, the algorithm itself constructs the rules based on earlier evaluations. It does not, however, articulate the rules in a way that is easy to understand. Instead, the rules are incorporated into the system. A local professional manager explains how the algorithm can lead professionals in the same direction:

… if it [the algorithm] will be able to support in the same way because we, as human beings, are not similar, and even if we have professional discussions and levels, it will always be di /uniFB00 erent. So, if it … now, today I ' mcategorising all the referrals, and I ' msupported in this speci /uniFB01 c way, I will be led in this direction. Tomorrow it ' s you, and you will be led in the same direction [ … .] So I can vividly see that it [the algorithm] can standardise the way that cases we mark as yellow today, would similarly have been marked yellow three days ago, and will also be marked yellow next week.

In this example, we characterise what the manager refers to as a kind of ' procedural objectivity ' (Williamson &amp; Piattoeva, 2019). ' Procedural objectivity ' is impersonal, standardised methods of investigation. The algorithm in this understanding then becomes a tool of standardisation, enabling ' something that started as local and situated to be made public and replicable across space and experience ' (Williamson &amp; Piattoeva, 2019, p. 66).

However, the professional manager quoted above as well as other informants point out the importance of ' the local and the situated ' in professional discretion. According to the manager, the algorithm in The Algorithm project is supposed to ' read ' referrals and sort them according to the prevalence of terms such as for instance violence, rape and drugs. Depending on the quantity of these speci /uniFB01 c words, the referral will be categorised as either red, yellow or green. The professional manager gives an example from a training workshop with social workers, where it was discussed whether the word ' rape ' should automatically categorise a referral as red:

… yeah, it totally depends on the context. It could be that he has threatened her with rape [ … .] it could be so that these words are present and in this category. But all the words might be present, but appearing in a context, which means that it is not in this category.

The manager gives another example, where none of the ' red ' words are present, but the connection between words in the referral tells the social workers to be alert. Frontline-professionals, managers, and consultants agree that one important role of the algorithm in social work, then, is to induce procedural objectivity and standardise case work as social workers will make divergent and subjective discretion. The algorithm does not specify and articulate the generally agreed-upon rules on which discretion should rely; it rather deduces such rules from historical data and incorporate them into the algorithm. Yet, for the purpose of avoiding context-less decisions in actual cases, the managers and social workers will, themselves, render the social workers ' holistic view a necessary substructure for the algorithm at work. Thus, the algorithm will only function properly when linked with professionals ' provision of contextual information.

## Objectivity: objective data, provided by social workers?

Data are central to various practices involved in the becoming of the digital administration and, hence, data-driven social work (Parton, 2008). Algorithms are built using large quantities of data. In all three of the analysed projects, social workers have made -or are crucial components of -the data. In Rita Referral, the algorithmic system uses historical data in the categorisation of referrals for the purpose of determining whether a referral is critical or not. Such data is based on the /uniFB01 rst labelling of the referral in the municipality, which is carried out by social workers. In the Algorithm project, the algorithm uses data from workshops and the testing of social workers in charge of categorising referrals. Thus, the data for referral categorisation are also provided by social workers. Referrals in Focus uses di /uniFB00 erent kinds of data. Some data is not produced by social workers, such as for instance moving history and number of siblings. The algorithm, however, also uses data on preventive measures concerning the children and the number of out-of-home placements. Social workers have decided and put these interventions into e /uniFB00 ect. This algorithm uses out-of-home placement as a proxy for failure to thrive. Obviously, out-of-home placement is a decision based on the work and decisions of social workers. As such, all three systems uses data that (one way or another) is produced by social workers, and without these data the systems would not work.

In our interviews, data are often referred to as being objective and neutral; as providing factual information and even ' knowledge ' about the social world. A manager in one municipality states that:

Social work is a huge area for changing something with more data substantiation and more data-informed management [ … ] because social work is based on softer values, so if it [data] can help optimising case proceedings where we can kind of factually support it, then I welcome it.

## Another (digital) manager says:

How can we use all the information and numbers we have, so that we can make more informed decisions. This is decision support all the way through, isn ' t it? To be able to navigate loads of knowledge that you wouldn ' t be able to handle on the spot.

These quotes point to a challenge in social work, namely that it is based on ' soft values ' and lacks informed decisions based on facts. Many of the actors we have talked to think that this challenge

can be met by using algorithmic systems to derive factual knowledge from data. Some of the managers we have interviewed not only consider this a possibility, but also an obligation. Referring to critical discussions and public debate on the ethical aspects of using a predictive algorithm, a (digital) manager states:

It is /uniFB01 ne that we have think tanks and professors who are critical; it is an important debate, we have to take. But we are here as an authority with an obligation to help families, so we are obliged to make use of our knowledge.

The quote show that data as such are considered to be knowledge -even evidence -andhaving the data in the databases urges their potential users to utilise this knowledge for the purpose of acting on it.

The data used for these algorithms, however, is often (for instance in the three cases analysed here) based on data generated by the social workers themselves and not in systematic or consistent ways. Talking about the problems related to social data, a municipal manager refers to such data as not being ' uniform and valid ' , further stating that it is important to increase focus on employees ' performance of ' correct registration ' . A digital manager talks about data quality:

Where do we register, in which systems, and how do we extract data, and is our registration practice satisfactory? No, it ' s not. It really isn ' t. People are recording things completely indiscriminately. Some are writing in notebooks, and they don ' t even record their registration in the professional digital systems.

She also points out the challenge involved by the fact that social workers do not use the same methodological framework in their approach to referrals and, hence, to the documentation of their cases. This results in a /uniFB02 uctuating quality of the data.

On the one hand, then, the managers quoted here consider data to be ' knowledge ' , ' evidence ' , and objective facts. This is needed, they say, in social work which is based on ' soft values ' and lacks informed decisions based on facts. On the other hand, managers and developers are well aware that data created by municipalities originates in social work practices and is neither uniform nor even ' valid ' and that the registration practices of social workers are ' unsatisfactory ' . One manager told us that one of the reasons why data is still used, even though it is not considered to be of particular high quality, has to do with the sheer volume. Because of the data quantity, it is considered to be valid knowledge.

## Eliminating bias

A third theme relating to the way algorithmic systems provide decision support for social workers is the issue of bias. On the one hand, this problem (often closely related to the concept of ' ethics ' and ' fairness ' ) represents an issue that can be solved by algorithmic systems, for instance by introducing increased standardisation and objectivity to social work. On the other hand, however, a common public criticism is that algorithms can, themselves, be biased -a criticism voiced in several books and papers that have raised public awareness (such as Eubanks, 2018; O ' Neil, 2016), tying into high-pro /uniFB01 le systems, such as the American sentencing system COMPAS. This public discussion and criticism has been incorporated into the design and development phases, and the discussion concerning bias and fairness is prominently featured in the projects, we follow.

In these projects, managers, developers, and people from political and professional organisations consider the risk of bias in relation to three variables in particular: gender, age, and ethnicity. At one meeting, under the heading ' Bias and ethics ' , it was discussed which role these variables should play in the /uniFB01 nished algorithm. Questioned about how to cope with other as yet unknown risks of potential bias, the developing team and municipality answered that this is why, ultimately, it is preferable to have a project member in charge of performing an assessment (meaning: a person who will be capable of dealing with systemic errors and bias). In some ways, then, algorithmic decisionsupport systems are meant to support professional discretion by eliminating bias. However, when asked to defend possible issues of bias in the algorithm itself, professional discretion is considered an element that will ensure fairness.

## Conclusion: algorithms and professional discretion

In social work with children and families in Denmark, speci /uniFB01 c algorithm types are produced incorporating a particular kind of professional discretion. Some studies have shown that the role of professional discretion in street-level bureaucracy is decreasing and that the bureaucracy, in some places, turns into ' system-level bureaucracy ' (Bovens &amp; Zouridis, 2002) or ' digital bureaucracies ' (Busch &amp; Henriksen, 2018), where the room for professional discretion disappears. Other studies posits that professional discretion in street-level work is delegated from the case worker to the engineer (Alkhatib &amp; Bernstein, 2019). This is not the case in our study, instead, algorithmic systems are consistently across our cases and interviews characterised as providing ' decision support ' to social workers. No one considers fully automated decision-making within social work to be a good idea, and everyone stresses the need for human judgement and professional discretion. This implies that, from the outset, there is a need for professional discretion in relation to algorithms. Our /uniFB01 ndings are thus in line with studies that analyse how new technologies shape, in di /uniFB00 erent ways, frontline workers ' discretionary powers (Bu /uniFB00 at, 2015; Jorna &amp; Wagenaar, 2007). In the political context of striving for faster, more e /uniFB03 cient, and better quali /uniFB01 ed decision-making (Jørgensen et al., 2021), the question, then, will be: how do these systems support professional discretion -and how does professional discretion support the systems?

First of all, the algorithmic systems are meant to induce procedural objectivity and standardise case work because social workers exercise divergent and subjective discretion. However, as the context of actual cases is important in social work, professional discretion, and knowledge of speci /uniFB01 c cases constitutes a necessary support system for the algorithm at work. Secondly, professional discretion is intended to be supported by algorithmic systems based on objective facts. Such ' facts ' and data are, however, mainly provided by social workers. There is a general awareness that municipalities ' data, created through social work practices, is neither uniform nor even ' valid ' ; that social workers ' registration practices are ' unsatisfactory ' ; and that data is recorded ' completely indiscriminately ' . Even though data quality is considered to be poor by several actors, the fact that there are considerable quantities of data convinces them that the results from the algorithm will be more objective than the evaluation made by an individual social worker. Algorithmic systems thus are expected to support decision-making and professional discretion by quanti /uniFB01 -cation: because of the comprehensive quantities of data (even though data quality is generally considered poor), especially on referrals, it is considered valid knowledge, not because of the data quality but because of the sheer volume. Third, in the case of bias, social workers ' professional discretion is considered to be potentially biased. More often, however, bias is referred to as an element relating to the algorithmic decision-support systems as such. In this case, considerable e /uniFB00 orts are made to avoid such bias as relate to gender, ethnicity, and age. With respect to bias relating to other ' variables ' , however, professional discretion is considered to constitute security of fairness.

Algorithms in social work are not expected to function without professionals ' provision of contextual information, the provision of data for the algorithm, and judgements made in respect of potential bias. Professional discretion occupies an undecided place in relation to these algorithms: on the one hand, professional discretion is seen as being too subjective and sensitive -as being related to the risk of mismanagement of cases. The problem concerning professional discretion is what prompts the development of algorithms in the /uniFB01 rst place. On the other hand, professional discretion is fundamental for the establishment, testing, and workings of the algorithmic systems.

As our analysis shows, algorithms should not be deemed to be tantamount to the elimination of professional discretion by way standardisation and objecti /uniFB01 cation. These systems are not, then, just another example of the evidence-wave sweeping across social work and other public administration areas. As of now, at least in Denmark and other Nordic countries, algorithmic decision-support systems take an ambivalent and undecided position in respect of the role of professional discretion.

## Re /uniFB02 ections for practice

Our analysis points to important considerations of which social workers and managers should be aware in the event of developing, testing, and implementing a decision-support algorithm. First, it is important to re /uniFB02 ect on the need for casework standardisation: when and how is it relevant? Politicians, managers, as well as /uniFB01 eld-level professionals express a desire for more standardised processes together with some sort of uniformity in citizens ' encounters with the system. Algorithmic systems can facilitate this by way of automated processes, or for example by notifying social workers of deadlines. However, there is -and should -remain su /uniFB03 cient room for social workers to use their local and situated knowledge in connection with speci /uniFB01 c cases. Secondly, much more attention should be paid to data and questions such as: which kind of data will the algorithm use, how is this data produced, what is the quality of this data? The algorithm is only as good as the data that informs it, and social workers and their immediate managers know much about their own data and data related to citizens within the scope of their work area. They should communicate such knowledge to data scientist and developers, who are not necessarily particularly knowledgeable when it comes to social work. Thirdly, professionals should be aware of potential bias in data and, hence, in algorithmic systems. Discussions about known bias or potential future bias should be a key concern in the development of algorithms and procedures should be implemented with respect to the regular testing of an algorithm for bias.

## Disclosure statement

No potential con /uniFB02 ict of interest was reported by the author(s).

## Notes on contributors

Marie Leth Meilvang is a Assistant Professor in the research programme Municipal Management and Practice at UCL University College. She holds a PhD from the Department of Sociology at the University of Copenhagen. Her current research focusses on the relationship between professions, organisations, and politics, among other things the methods, tools, and technologies in welfare professional work, and questions of legitimacy and morality in (inter-)professional work. She has published in journals such as Journal of Professions and Organization and The Sociological Review.

Anne Marie Dahler is an Associate Professor and leader of the research programme Municipal Management and Practice at UCL University College. Her current research focusses on technologies and normativities in the work of welfare professionals, social inequality and politics, and research interventions. She has been involved in a range of projects on technology, competences, and welfare professional work and has previously published in journals such as Rehabilitation Research and Practice and Disability and Rehabilitation.

## ORCID

Marie Leth Meilvang http://orcid.org/0000-0002-4972-2472 Anne Marie Dahler http://orcid.org/0000-0003-2085-5519

## References

- Alkhatib, A., &amp; Bernstein, M. (2019, May 4 -9). Street-level algorithms: A theory at the gaps between policy and decisions . Proceedings of the 2019 CHI conference on human factors in computing systems (CHI ' 19) ACM, Glasgow, Scotland, UK. Article 530, pp. 1 -13. Bovens, M., &amp; Zouridis, S. (2002). From street-level to system-level bureaucracies: How information and communication technology is transforming administrative discretion and constitutional control. Public Administration Review , 62 (2), 174 -184. Bowker, G. C., &amp; Star, S. L. (1999). Sorting things out. Classi /uniFB01 cation and its consequences . The MIT Press.

- Brown, A., Chouldechova, A., Putnam-Hornstein, E., Tobin, A., &amp; Vaithianathan, R. (2019, May). Toward algorithmic accountability in public services: A qualitative study of a /uniFB00 ected community perspectives on algorithmic decision-

<!-- image -->

making in child welfare services. In Proceedings of the 2019 CHI conference on Human Factors in Computing Systems (pp. 1 -12).

- Bu /uniFB00 at, A. (2015). Street-level bureaucracy and e-government. Public Management Review , 17 (1), 149 -161. https://doi. org/10.1080/14719037.2013.771699
- Burton, J., &amp; Van den Broek, D. (2009). Accountable and countable: Information management systems and the bureaucratization of social work. British Journal of Social Work , 39 (7), 1326 -1342. Busch, P. A., &amp; Henriksen, H. Z. (2018). Digital discretion: A systematic literature review of ICT and street-level discretion. Information Polity , 23 (1), 3 -28. Busch, P. A., Henriksen, H. Z., &amp; Sæbø, Ø. (2018). Opportunities and challenges of digitized discretionary practices: A public service worker perspective. Government Information Quarterly , 35 (4), 547 -556. https://doi.org/10.1016/j.giq. 2018.09.003

Clarke, A. E. (2005). Situational analysis: Grounded theory after the postmodern turn . Sage Publications.

Coulthard, B., Mallett, J., &amp; Taylor, B. (2020). Better decisions for children with ' Big data ' : Can algorithms promote fairness, transparency, and parental engagement? Societies , 10 (97). Dearman, P. (2005). Computerized social casework recording: Autonomy and control in Australia ' s income support agency. Labor Studies Journal , 30 (1), 47 -65. Eubanks, V. (2018). Automating inequality: How high-tech tools pro /uniFB01 le, police, and punish the poor . St. Martin ' s Press.

Evans, T. (2011). Professionals, managers and discretion: Critiquing street-level bureaucracy. British Journal of Social Work , 41 (2), 368 -386. - Evans, T., &amp; Harris, J. (2004). Street-level bureaucracy, social work and the (exaggerated) death of discretion. British Journal of Social Work , 34 (6), 871 -895. Gillingham, P. (2009). The use of assessment tools in child protection: An ethnomethodological study [PhD dissertation]. University of Melbourne.

- Gillingham, P. (2019). Can predictive algorithms assist decision-making in social work with children and families? Child Abuse Review , 28 (2), 114 -126. - Holstein, J. A., &amp; Gubrium, J. F. (1995). The active interview . Sage.
- Høybye-Mortensen, M. (2015). Decision-making tools and their in /uniFB02 uence on caseworker ' s room for discretion. British Journal of Social Work , 45 (2), 600 -615. - Jorna, F., &amp; Wagenaar, P. (2007). The ' iron cage ' strengthened? Discretion and digital discipline. Public Administration , 85 (1), 189 -214. - Jørgensen, A. M., Webb, C., Keddell, E., &amp; Ballantyne, N. (2021). Three roads to Rome? Comparative policy analysis of preditive tools in child protection services in Aotearoa New Zealand, England, and Denmark. Nordic Social Work Research . Advance online publication. - Latour, B. (1992). Where are the missing masses? The sociology of a few mundane artifacts. In W. Bijker &amp; J. Law (Eds.), Shaping technology / building society: Studies in sociotechnical change (pp. 225 -258). MIT Press.

Lipsky, M. (1980). Street-level bureaucracy: The dilemmas of the individual in public service . Russell Sage Foundation.

- Lund, C. S. (2019). Algoritmer i social faglige vurderinger -en undersøgelse af socialarbejderes opfattelse af at anvende algoritmer til vurdering af underretninger. Uden for Nummer , 19 (39), 20 -31. https://socialraadgiverne.dk/wpcontent/uploads/2019/11/39-UdenForNummer.pdf
- Mol, A. (2008). The logic of care: Health and the problem of patient choice . Routledge.
- Morago, P. (2007). Evidence-based practice: From medicine to social work. European Journal of Social Work , 9 (4), 461 -477. - Møller, A. M. (2019). Explicit professionalism. A cross-level study of institutional change in the wake of evidence-based practice. Journal of Professions and Organization , 6 (2), 179 -195. - Møller, A. M. (2022). Mobilizing knowledge in frontline work: A conceptual framework and empirical exploration. Perspectives on Public Management and Governance , 5 (1), 50 -62. O ' Neil, C. (2016). Weapons of math destruction: How big data increases inequality and threatens democracy . Crown.

- Parton, N. (2008). Changes in the form of knowledge in social work: From the ' social ' to the ' informational ' ? The British

Journal of Social Work , 38 (2), 253 -269. - Petersen, A. C., Christensen, L. R., Harper, R., &amp; Hildebrandt, T. (2021). ' We would never write that down ' . Classi /uniFB01 cations of unemployed and data challenges for AI. Proceedings of the ACM on Human-Computer Interaction , 5 (CSCW1), 1 -26. - Petersen, A., Christensen, L. R., &amp; Hildebrandt, T. (2020). The role of discretion in the age of automation. Computer Supported Cooperative Work (CSCW) , 29 (3), 303 -333. - Ponnert, L., &amp; Svensson, K. (2016). Standardisation -the end of professional discretion? European Journal of Social Work , 19 (3 -4), 586 -599. - Ryle, G. (1945). Knowing how and knowing that: The presidential address. Proceedings of the Aristotelian Society , 46 , 1 -16. http://www.jstor.org/stable/4544405
- Tavory, I., &amp; Timmermans, S. (2014). Abductive analysis: Theorizing qualitative research . University of Chicago Press.
- Taylor, B. (2012). Models for professional judgement in social work. European Journal of Social Work , 15 (4), 546 -562. <!-- image -->

- Taylor, B., &amp; Whittaker, A. (2018). Professional judgement and decision-making in social work. Journal of Social Work Practices , 32 (2), 105 -109. - Timmermans, S., &amp; Epstein, S. (2010). A world of standards but not a standard world: Toward a sociology of standards and standardization. Annual Review of Sociology , 36 (1), 69 -89. https://doi.org/10.1146/annurev.soc.012809. 102629
- Ting, M. H., Chu, C. M., Zeng, G., Li, D., &amp; Chng, G. S. (2018). Predicting recidivism among youth o /uniFB00 enders: Augmenting professional judgement with machine learning algorithms. Journal of Social Work , 18 (6), 631 -649. https://doi.org/10. 1177/1468017317743137
- Williamson, B., &amp; Piattoeva, N. (2019). Objectivity as standardization in data -scienti /uniFB01 c education policy, technology and governance. Learning, Media and Technology , 44 (1), 64 -76. https://doi.org/10.1080/17439884.2018.1556215