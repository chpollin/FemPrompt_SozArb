---
source_file: Wajcman_2023_Feminism_Confronts_AI_The_Gender_Relations_of.pdf
conversion_date: 2026-02-03T09:30:06.833109
converter: docling
quality_score: 95
---

## 4

## Feminism Confronts AI

## The Gender Relations of Digitalisation

Judy Wajcman and Erin Young

## Introduction

The rapid development and spread of digital technologies have been pervasive across almost every aspect of socio-political and economic life, including systems of governance, communications and structures of production and consumption. Digitalisation, broadly marking the shift from analogue to digital technologies, is characterised by technological advances ranging from smart phones, the mobile internet, social media, and the internet of things, to artificial intelligence (AI), big data, cloud computing, and robotics. These span public and private industries including healthcare, commerce, education, manufacturing, and finance. As such, this so-called 'fourth industrial revolution' has brought with it a new digital economy across developed and developing economies alike (Schwab 2016). Significantly, AI, underpinned by algorithms and machine learning, has become a defining feature and driving force of this data-driven digitalisation.

Digitalisation presents immense potential to improve social and economic outcomes and enhance productivity growth and population well-being globally. However, despite important research initiatives, interventions, and policies aimed at furthering women's empowerment and gender equality within this 'revolution' , a significant digital gender gap still exists, limiting the equitable realisation of the benefits of digitalisation (Wajcman et al. 2020). Worldwide, roughly 327 million fewer women than men have a smartphone and can access mobile internet (OECD 2018). Analysis from the EQUALS Research Group shows that 'a gender digital divide persists irrespective of a country's overall ICT [information and communication technology] access levels, economic performance, income levels, or geographic location' (Sey and Hafkin 2019: p.25). Women are thus under-represented in this digital revolution across high-, low-, and middle-income countries, despite the possibilities for marshalling greater equality.

Moreover, while the internet was initially viewed as a democratising platform, early emancipatory promises increasingly fall short as a small group of large technology corporations based in the Global North has emerged as a dominant force in the new global economy (see Hampton, Chapter 8 in this volume). These

'tech giants' monopolise markets and wield power over digital data, as major online platforms are found complicit in the spread of misinformation, hate speech and misogynistic (and racist) online abuse and harassment. In particular, there are concerns that unprecedented levels of data mining, or 'data extractivism' , algorithms and predictive risk models could entrench existing inequalities and power dynamics (Eubanks 2018). This is about the danger of encoding-and amplifying-offline inequities into online structures, as these technologies carry over the social norms and structural injustices of the offline world into the digital.

This chapter will examine the gender relations of digitalisation, with a particular focus on AI and machine learning as the most contemporary feature of this.¹ Althoughthereis increasing recognition that technologies are both a reflection and crystallisation of society, we will argue that there is still insufficient focus on the ways in which gendered power relations are integral to and embedded in technoscience. This is as much the case with AI and data science as it was with previous waves of technological change.

We begin by describing women's position in the emerging fields of AI and data science. We then explain how the gender skills gap in STEM education and the AI workforce is based on historically constructed equations between masculinity and technical expertise, long identified in feminist scholarship. Adopting an intersectional technofeminist approach, we argue that technologies are gendered by association and by design, where 'association' refers to the gendering of work environments and to technology stereotypes. The history of engineering/computing as a form of expertise and a set of practices that express a male culture and identity still characterise contemporary tech workplaces and are a key factor in women's continuing under-representation in these fields (see also Cave, Dihal, Drage, and McInerney, Chapter 5 in this volume).

This stark lack of diversity and inclusion in the field of AI and data science has profound consequences (see also Costanza-Chock, Chapter 21 in this volume). In our final section, we argue that the dominance of men working in and designing AI results in a feedback loop whereby bias gets built into machine learning systems. Digital technologies, whether hardware or software, are socially shaped by gender power relations and gendered meanings that influence the process of technological change and are inscribed into technologies. Although algorithms and automated decision-making systems are presented as if they are impartial, neutral, and objective, we show how bias enters, and is amplified through, AI systems at various stages.

Crucially, we stress that algorithmic bias is not solely the product of unconscious sexism or racism, nor bad training data, but the end result of a technoculture that

¹ Artificial Intelligence: 'When a machine or system performs tasks that would ordinarily require human (or other biological) brainpower to accomplish' (The Alan Turing Institute, 2021).

has systematically excluded women and people from marginalised groups from positions of leadership and power.

While digitalisation holds out the promise of greater equality, it also poses the risk of encoding, repeating, and amplifying existing patterns of gender inequities. This perspective is particularly important at a time when digital tools are marketed as the solution to all social problems. If the generation and implementation of new technologies always involve preferences and choices, then there are opportunities to build them in ways that prevent harm and, more so, promote the 'good' that they offer. At a moment when technology is being marshalled to make choices of global consequence, and is affecting the lives of individuals and society in ways both profound and subtle, this warrants urgent attention.

## The Missing Women in AI

We begin by presenting some figures on women in the technology labour force, before delving into the subfields of AI and data science as an integral part of this workforce. At the outset, it is important to note that this is not a story of inexorable progress in terms of gender equality. In fact, the percentage of women in the USA and Western Europe gaining computer science degrees today-15-20 percentis down from nearly 40 percent in the 1980s (Murray 2016). In Europe, only 17 percent of ICT specialists are women (European Commission 2019); in the UK, women comprise 19 percent of UK tech workers (Tech Nation 2018). Further, UNESCO (2021) estimates that women are under-represented in technical and leadership roles in the world's top technology companies, with 23 and 33 percent, respectively, at Facebook.

There is a scarcity of intersectional data on the tech sector, but the little data that is available suggests that women of colour are particularly under-represented. Only 1.6 percent of Google's US workforce in 2020 were Black women. Indeed, diversity policies and training (among other initiatives) have only made a marginal difference in increasing the share of women of colour in tech (Google 2020). As Alegria (2019, p.723) explains, 'women, particularly women of colour, remain numerical minorities in tech despite millions of dollars invested in diversity initiatives' .

The World Economic Forum (2020) estimates that globally women make up approximately 26 percent of workers in data and AI roles more specifically, a number that drops to only 22 percent in the UK. This is partly due to the lack of clarity and newness of these professions-but it is also because of a hesitancy of big tech companies to share this data. As West et al. (2019, pp.10-12) explain, 'the current data on the state of gender diversity in the AI field is dire… the diversity and inclusion data AI companies release to the public is a partial view, and often contains flaws' . This is a significant barrier to research.

Given the scarcity of raw industry data available, researchers have drawn on other sources including online data science platforms, surveys, academia, and conference data. These approaches also provide mounting evidence of serious gaps in the gender diversity of the AI workforce. In 2018, WIRED and Element AI reviewed the AI research pages of leading technology companies and found that only 10-15 percent of machine learning researchers were women (Simonite 2018). Related research found that on average only 12 percent of authors who had contributed to the leading machine learning conferences (NIPS, ICML, and ICLR) in 2017 were women (Mantha and Hudson 2018).

Indeed, there is more information about women in AI research and in the academy, due to the more readily available data. For example, Stathoulopoulos and Mateos-Garcia (2019) found that only 13.8 percent of AI research paper authors were women; at Google, it was 11.3 percent, and at Microsoft, 11.95 percent. Additionally, the AI Index (2021) found that female faculty make up just 16 percent of all tenure-track faculty whose primary focus is AI.

The sparsity of statistics on the demographics of AI professions also motivated us to explore other potentially informative sources. As quickly evolving fields in which practitioners need to stay up-to-date with rapidly changing technologies, online communities are an important feature of data science and AI professions, and so provide an interesting lens through which to view participation in these fields. Thus, we examined a selection of online, global data science platforms (Data Science Central, Kaggle, OpenML, and Stack Overflow). Our research indicates that women are represented at a remarkably consistent, and low, 17-18 percent across the platforms-with Stack Overflow at much lower 7.9 percent.²

The statistics we have reviewed confirm that the newest wings of technology, that is, AI and data science, have poor representation of women. The more prestigious and vanguard the field, the fewer the number of women working in it. As the AI and data science fields are rapidly growing as predominant subfields within the tech sector, so is the pervasive gender gap within them.

Before concluding this section, it is important to acknowledge that so-called intelligent machines also depend on a vast, 'invisible' human labour force: those who carry out skilled technological work such as labelling data to feed algorithms, cleaning code, training machine learning tools, and moderating and transcribing content. These 'ghost workers' , often women in the Global South, are underpaid, undervalued, and lacking labour laws (Gray and Suri 2019; Roberts 2019; Atanasoski and Vora 2019). Given the evidence to date, there is no reason to expect the rise of the 'gig' or 'platform' economy to close the gender gap, particularly in the AI fields.

² See ' Where are the Women? Mapping the Gender Job Gap in AI ' (Young et al. 2021).

## Feminist STS: From Technofeminism to Data Feminism

To understand the gender power dynamics of AI, it is worth recalling that feminist STS (science and technology studies) scholars have been researching the gendering of technology for decades. In TechnoFeminism (2004), Wajcman has detailed the various strands of feminist thought, such as liberal, socialist, and post-modern, that emerged in response to previous waves of digitalisation. A common concern was to document and explain women's limited access to scientific and technical institutions and careers, identifying men's monopoly of technology as an important source of their power. The solution was often posed in terms of getting more women to enter science and technology-seeing the issue as one of equal access to education and employment. However, the limited success of equal opportunity strategies soon led to the recognition that technical skills are embedded in a culture of masculinity and required asking broader questions about how technoscience and its institutions could be reshaped to accommodate women.

Such critiques emphasised that, in addition to gender structures and stereotyping, engrained cultures of masculinity were ubiquitous within tech industriesand they still are, as we will discuss in the next section.

Recognising the complexity of the relationship between women and technology, by the 1980s feminists were exploring the gendered character of technology itself (see Taillandier in this volume). In Harding's (1986, p.29) words, feminist criticisms of science evolved from asking the 'woman question' in science to asking the more radical 'science question' in feminism. Rather than asking how women can be more equitably treated within and by science, the question became how Western science, a male, colonial, racist knowledge project, can possibly be used for emancipatory ends (Haraway 1988). Similarly, feminist analyses of technology were shifting from women's access to technology to examining the very processes by which technology is developed and used, as well as those by which gender is constituted. Both socialist and radical feminists began to analyse the gendered nature of technical expertise, and put the spotlight on artefacts themselves. The social factors that shape different technologies came under scrutiny, especially the way that technology reflects gender divisions and inequalities. Hence, the problem was not only men's monopoly of technology, but also the way gender is embedded in technology itself.

A broad social shaping or 'co-production' framework has now been widely adopted by feminist STS scholars.³ A shared idea is that technological innovation is itself shaped by the social circumstances within which it takes place. Crucially, the notion that technology is simply the product of rational technical imperatives has been dislodged. Objects and artefacts are no longer seen as politically neutral,

³ See, for example, the journal Catalyst: Feminism, Theory, Technoscience . For a recent overview, see Wagman and Parks (2021).

separate from society; rather, they are designed and produced by specific people in specific contexts. As such, artefacts have the potential to embody and reproduce the values and visions of the individuals and organisations that design and build them. And if 'technology is society made durable' (Latour 1990) then, in a patriarchal society, gender power relations will be inscribed into the process of technological change, in turn configuring gender relations.

Such a mutual shaping approach, in common with technofeminist theory, conceives of technology as both a source and consequence of patriarchal relations (Wajcman 2004). In other words, gender relations can be thought of as materialised in technology, and masculinity and femininity in turn acquire their meaning and character through their enrolment and embeddedness in working machines. For example, gendered identities are found to be co-constructed with technologies and technical orientations, often in connection with alignments of race and class (Bardzell 2018; Pérez-Bustos 2018). Empirical research on everything from the microwave oven (Cockburn and Ormrod 1993) and the contraceptive pill (Oudshoorn 1994) to robotics and software agents (Suchman 2008) has clearly demonstrated that the marginalisation of women from the technological community has a profound influence on the design, technical content, and use of artefacts.

The key insights of feminist STS on issues such as the gendering of skills and jobs; the conception of technology as a sociotechnical product and cultural practice; and the critique of binary categories of female/male, nature/culture, emotion/reason, and humans/machines are still foundational resources for contemporary research on gender and technology. In recent years, feminist STS has been enriched by its engagement with intersectional feminist analysis, critical race theorists and post-colonial theory (Crenshaw et al. 1995; Collins 1998; Benjamin 2019; Noble 2018; Sandoval 2000). There is increasing recognition of the ways in which gender intersects with other aspects of difference and disadvantage in the societies within which these technologies sit. Women are a multifaceted and heterogeneous group, with a plurality of experiences. Gender intersects with multiple aspects of difference and disadvantage involving race, class, ethnicity, sexuality, ability, age, and so on. For instance, women who are poor or belong to racial minorities experience the negative effects of digitalisation and automation more acutely (Buolamwini and Gebru 2018).

An intersectional technofeminist lens is particularly pertinent as attention has expanded beyond the sexism and racism of the internet and digital cultures to the growing body of work on AI systems. In Data Feminism (2020: p.105; see Chapter 12 in this volume), D'Ignazio and Klein not only expose the glaring gender data gap-as we noted before-but also show how the data that feeds algorithms is biased. Echoing STS texts on the politics of scientific knowledge production, they highlight the epistemic power of classification systems and the values and judgements they encode (Bowker and Star 2000). Every dataset used to train machine

learning systems contains a worldview, reducing humans to 'false binaries and implied hierarchies, such as the artificial distinctions between men and women' (see also Crawford 2021). This necessitates the 'flattening out of humanity's plurality' (see Browne in this volume). In other words, the very process of classifying data, a core practice in AI, is inherently political. That is why narrow technical solutions to statistical bias and the quest for 'fairer' machine learning systems miss the point.

As Benjamin (2019, p.96) underlines, the injustices are nothing new but 'the practice of codifying existing social prejudices into a technical system is even harder to detect when the stated purpose of a particular technology is to override human prejudice' . In the final section of this chapter we will examine such AI feedback loops: the ways in which algorithms can reflect and amplify existing inequities such as those based on gender and race. But first we explore how and when technical expertise came to be culturally associated with masculinity.

## Masculinity and Technical Expertise

## History of Engineering and Computing as Professions

The stereotype of engineering and computer science as male domains is pervasive (see Cave et al., Chapter 5 in this volume). It affects girls' and women's confidence in their technical skills and proficiencies, shaping their perception of their own identity and discouraging them from entering such fields. Indeed, the underrepresentation of women in the tech sector has traditionally been framed as a 'pipeline problem' , suggesting that the low numbers of women in tech is solely due to a low female talent pool in STEM fields (that is, because girls are uninterested or lack the skills). This perspective, however, shifts the obligation to change onto women and neglects technology companies' failure to attract and retain female talent.

'Masculine defaults' shape professional practices and career pathways, governing technical fields in particular (Cheryan and Markus 2020). As Wajcman (2010, p.144) explains, definitions of technological skill and expertise have been historically constructed in a way that privileges the masculine (as the 'natural' domain of men), rendering femininity as 'incompatible with technological pursuits' . For example, a number of works highlight the role of gender relations in the very definition and configuration of computing as a profession. Feminist historian Hicks (2017; see also Abbate 2012) recalls that computer programming was originally the purview of women. At the advent of electronic computing following the Second World War, software programming in industrialised countries was largely considered 'women's work' , and the first 'computers' were young women. As computer programming became professionalised, however, and power and

money became associated with the field, the gender composition of the industry shifted, marginalising the work of female technical experts by fashioning them into a technical 'underclass' . Structural discrimination edged women out of the newly prestigious computing jobs (Ensmenger 2012). Such persistent cultural associations around technology are driving women away from, and out of, industries which entail more 'frontier' technical skills such as AI.

Indeed, as feminist scholars have long evidenced, when women participate in male-dominated occupations, they are often concentrated in the lower-paying and lower-status subfields. 'Throughout history, it has often not been the content of the work but the identity of the worker performing it that determined its status' , explains Hicks (2017, p.16). As women have begun to enter certain technological subdomains in recent years (often through boot camps and other atypical educational pathways), such as front-end development, these fields have started to lose prestige and experience salary drops (Broad 2019). Meanwhile, men are flocking to the new (and highly remunerated) data science and AI subspecialities.

What is crucial to emphasise here is that technical skill is often deployed as a proxy to keep certain groups in positions of power. As such, it is important to begin to rewrite the narrative, heightening awareness of the gendered history of computing in order to avoid its replication in AI. This is particularly apposite as newlycreated AI jobs are set to be the well-paid, prestigious and intellectually stimulating jobs of the future. Women and other under-represented groups deserve to have full access to these careers, and to the economic and social capital that comes with them. Further, if the women who do succeed in entering tech are stratified into 'less prestigious' subfields and specialities, rather than obtaining those jobs at the forefront of technical innovation, the gender pay gap will be widened.

## The Skills Gap

Once defined by inequalities in access to digital technology, the digital gender divide is now more about deficits in learning and skills. While there is still an 'access gap' between women and men, especially in the Global South, it has greatly improved over the past 25 years. At the same time, however, the gender 'digital skills gap' persists. Despite a number of important interventions and policies aimed at achieving gender equality in digital skills across both developed and developing economies, the divide not only remains large but, in some contexts, is growing wider. This skills divide is underpinned by a deficit in digital literacies among women, particularly in low- and middle-income countries, where many women lack the necessary techno-social capabilities to compete in a global online environment (Gurumurthy et al. 2018).

It is important to note, however, that the gender skills gap is more marked in some countries than in others. For example, in Malaysia some universities

have up to 60 percent women on computer science programmes, with near parity also reported in certain Taiwanese and Thai institutions. Stoet and Geary (2018) even discuss a 'gender-equality paradox' , suggesting that the more gender equality in a country, the fewer women in STEM fields. They propose that women's engagement with technical subjects may be due to life-quality pressures in less gender-equal countries; there is likely a complex set of reasons including national cultures, and varying professional opportunities for women globally. Indeed, Arab countries have between 40 and 50 percent female participation in ICT programmes (a proportion far higher than many of the more gender-equal European countries). However, these examples are not representative of the broader trends worldwide.

A variety of initiatives has been trialled in the past few decades to encourage morediversity in technological fields within higher education. Particularly notable are the successful models provided by the US universities Carnegie Mellon and Harvey Mudd, which have dramatically increased the participation of women in their computer science departments. For example, Carnegie Mellon increased the number of women from 7 percent in 1995 to 42 percent in 2000 (Frieze and Quesenberry 2019). This was achieved through a multi-pronged approach, which included grouping students by experience, as well as coaching for, and buy-in from, senior faculty. By 2019, Harvey Mudd also boasted 49 percent women in computer science. This suggests that steps towards resolving the issues can be straightforward with the right policies and leadership in place.

In concluding it should be noted that the gender skills gap not only limits women's economic opportunities, but also has broader implications for their ability to participate fully as citizens, in government and politics. As the World Economic Forum (2018, p.viii) argues: 'AI skills gender gaps may exacerbate gender gaps in economic participation and opportunity in the future as AI encompasses an increasingly in-demand skillset … technology across many fields is being developed without diverse talent, limiting its innovative and inclusive capacity' . However, as we have been stressing, hiring more women is not enough. As UNESCO's Framework on Gender Equality and AI states (2020, p.23): 'This is not a matter of numbers, but also a matter of culture and power, with women actually having the ability to exert influence' . Bringing women to positions of parity as coders, developers, and decision makers, with intersectionality in mind, is key.

## Technoculture in Organisations

Up to this point we have focused on how the gender skills gap, resulting from the history of computing, skews AI professions. However, the masculine culture of the

workplace itself, in particular the 'brogrammer' and 'geek' cultures synonymous with Silicon Valley, is a key factor hindering women's career progression.

Once women are employed in engineering and technological fields, the rate of attrition is high, with women leaving technology jobs at twice the rate of men (Ashcraft et al. 2016). Women (but not men) taking managerial paths in engineering firms may also be at the greatest risk of attrition (Cardador and Hill 2018). In a similar vein, McKinsey found that while women made up 37 percent of entrylevel roles in technology, only 25 percent reached senior management roles, and 15 percent made executive level (Krivkovich et al. 2016). Exploring the reasons for women's and minorities' high attrition and turnover rates, the Kapor Center highlights that one in ten women in technology experiences unwanted sexual attention (Scott et al. 2017).

Indeed, several studies have revealed subtle cultural practices embedded within technology companies that lead to 'chilly' , unwelcoming workplace climates for women and marginalised groups (Hill et al. 2010). The prevalence of 'masculine defaults' in these spaces results in micro-aggressions, subconscious biases in performance/promotion processes, and other forms of discrimination (Kolhatkar 2017). According to the State of European Tech Survey, 55 percent of Black/African/Caribbean women have experienced discrimination in some form (Atomico 2018). An overwhelming 87 percent of women claim that they have been challenged by gender discrimination compared to 26 percent of men (Atomico 2020). For instance, in 2018 Google staff walked out over how sexual misconduct allegations were being dealt with at the firm (Lee 2018). Tech companies have been slow to react to such findings.

In turn, this 'technoculture' has significant repercussions for recruitment, promotion, career trajectories and pay. For example, Wynn and Correll (2018) suggest that women are alienated at the point of recruitment into technology careers. They found that company representatives often engage in behaviours, such as geek culture references, that create unwelcoming environments for women before joining a firm. As such, companies have a significant opportunity to increase their representation of women by being more mindful of the images projected in recruitment sessions and interviews.

Finally, the 'bro' culture of technological work has resulted in a severe underrepresentation of women in entrepreneurship. This, almost paradoxically, is often heralded as the way for women to 'get ahead' in the digital revolution. Female founders in the United States received only 2.3 percent of venture capital funds in 2020, and women represent just 12 percent of venture capital decision makers (Reardon 2021). Atomico (2020) also found that 93 percent of all start-up funds raised in Europe in 2018 went to all-male founding teams-with just 2 percent to all-female founding teams.

As we will elaborate in the following section, the under-representation of women-especially women of colour (Williams et al. 2022)-on teams of inventors influences what actually gets invented. Patenting inventions has been largely a male endeavour, with a woman cited as the lead inventor on just 7.7 percent of all patents filed between 1977 and 2010 in the United States. Looking specifically at biomedical patents in 2010, Koning et al. (2021) found that teams made up of all women were 35 percent more likely than all-male teams to invent technologies relating to women's health. The researchers estimate that if women and men had produced an equal number of patents since 1976, there would be 6500 more female-focused inventions today.

## AI Feedback Loops: The Amplification of Gender Bias

The stark lack of diversity and inclusion in AI has wider implications. Mounting evidence suggests that the under-representation of women and marginalised groups in AI and data science results in a feedback loop whereby bias gets built into machine learning systems. To quote the European Commission (Quirós et al. 2018): 'Technology reflects the values of its developers… It is clear that having more diverse teams working in the development of such technologies might help in identifying biases and prevent them' .

Although algorithms and automated decision-making systems are presented and applied as if they are impartial, neutral, and objective, bias enters, and is amplified through, AI systems at various stages (e.g. Leavy 2018; Gebru 2020). For example, the data used to train algorithms may under-represent certain groups or encode historical bias against marginalised demographics, due to previous decisions on what data to collect, and how it is curated. Data created, processed, and interpreted within unequal power structures can reproduce the same exclusions and discriminations present in society.

A key example of this is the 'gender data gap'; the failure to collect data on women, that is, gender-disaggregated data. As Criado Perez (2019) explains, in some instances this directly jeopardises women's health and safety: heart failure trials, historically based on male participants, result in women's heart attacks being misdiagnosed, and seat-belt designs based on the male body result in women being morelikely to be seriously injured in car crashes. Furthermore, the gender data gap tends to be larger for low and middle-income countries. A 2019 Data 2X study of national databases in fifteen African countries found that sex-disaggregated data were available for only 52 percent of the gender-relevant indicators. When deep learning systems are trained on data that contain biases, they are reproduced and amplified. Fei-Fei Li, a prominent Stanford researcher in the AI field, describes this simply as 'bias in, bias out' (Hempel 2017).

Indeed, this issue is not only about exclusion, but also about the risks and potential for harm due to the increased visibility that including and collecting data on certain populations might bring them. For example, some communities are more heavily subject to surveillance and therefore more likely to appear in police databases. This is termed the 'paradox of exposure' by D'Ignazio and Klein (2020). The power of data to sort, categorise, and intervene, alongside corporations and states' ability to use data for profit and surveillance, has been stressed by Taylor (2017).

Furthermore, there are often biases in the modelling or analytical processes due to assumptions or decisions made by developers, either reflecting their own (conscious or unconscious) values and priorities or resulting from a poor understanding of the underlying data. Even the choices behind what AI systems are created can themselves impose a particular set of interpretations and worldviews, which could reinforce injustice. As O'Neil (2016, p.21) succinctly states: 'Models are opinions embedded in mathematics' . If primarily white men are setting AI agendas, it follows that the supposedly 'neutral' technology is bound to be inscribed with masculine preferences. Bias introduced at any one stage of the modelling process may be propagated and amplified by knock-on biases, since the results of one round of modelling are often used to inform future system design and data collection (Mehrabi et al. 2019).

Anunderlying problem is that AI systems are presented as objective and neutral in decision making rather than as inscribed with masculine, and other, preferences and values. Machines trained using datasets generated in an unequal society tend to magnify existing inequities, turning human prejudices into seemingly objective facts. Even if designed with the best of intentions, they 'do not remove bias, they launder it, performing a high-tech sleight of hand that encourages users to perceive deeply political decisions as natural and inevitable' (Eubanks 2018, p.224).

A growing number of AI products and systems are making headlines for their discriminatory outcomes. To name only a few: a hiring algorithm developed by Amazon was found to discriminate against female applicants (Dastin 2018); a social media-based chatbot had to be shut down after it began spewing racist and sexist hate speech (Kwon and Yun 2021); the image-generation algorithms OpenAI's iGPT and Google's SimCLR are most likely to autocomplete a cropped photo of a man with a suit and a cropped woman with a bikini (Steed and Caliskan 2021); and marketing algorithms have disproportionately shown scientific job advertisements to men (Maron 2018; Lambrecht and Tucker 2019).

The introduction of automated hiring is particularly concerning, as the fewer the number of women employed within the AI sector, the higher the potential for future AI hiring systems to exhibit and reinforce gender bias, and so on. Perversely, Sánchez-Monedero et al. (2020) found that automated hiring systems that claim to detect and mitigate bias obscure rather than improve systematic discrimination in the workplace.

There has also been concern about AI bias in the context of the pandemic. For example, Barsan (2020) found that computer vision models (developed by Google, IBM, and Microsoft) exhibited gender bias when identifying people wearing masks for COVID protection. The models were consistently better at identifying masked men than women and, most worrisome, they were more likely to identify the mask as duct tape, gags, or restraints when worn by women. Pulse oximetry devices used for warning of low blood oxygenation in COVID-19 were found to significantly underestimate hypoxaemia in Black patients-'current assumptions and algorithms, often derived from heavily white patient populations, may work against black patients' (BMJ 2020).

Several studies on computer vision have highlighted encoded biases related to gender, race, ethnicity, sexuality, and other identities. For instance, facial recognition software successfully identifies the faces of white men but more often fails to recognise those of dark-skinned women (Buolamwini and Gebru 2018). Further, research analysing bias in Natural Language Processing (NLP) systems reveal that word embeddings learned automatically from the way words co-occur in large text corpora exhibit human-like gender biases (Garg et al. 2018). For example, when translating gender-neutral language related to STEM fields, Google Translate defaulted to male pronouns (Prates et al. 2019). Additionally, AI voice assistants (such as Alexa and Siri) that mimic the master/servant relations of domestic service are aestheticised as a native-speaking, educated, white woman (Phan 2019; see also Rhee in this volume).

Finally, it is important to stress that technical bias mitigation (including algorithmic auditing) and fairness metrics for models and datasets are by no means sufficient to resolve bias and discrimination (Hutchinson and Mitchell 2019). Notably, since 'fairness' cannot be mathematically defined, and is rather a deeply political issue, this task often falls to the developers themselves-the very teams in which the diversity crisis lies (Hampton 2021). While we laud the considerable efforts of the Human-Computer Interaction (HCI)/Computer-Supported Cooperative Work community, among others, to design tools to mitigate data, algorithmic, and workers' biases, framing the problem as one of 'bias' risks encoding the premise ' that there is an absolute truth value in data and that bias is just a 'distortion' from that value ' (Miceli et al. 2022, p.4).

As we noted before, feminist STS has demonstrated that the notion of scientific objectivity as a 'view from nowhere' is both androcentric and Eurocentric. It is therefore highly attuned to the privileged and naturalised epistemological standpoints or worldviews inscribed in data and systems that reproduce the status quo. We urgently need more nuanced data and analysis on women in AI to better understand these processes and strengthen efforts to avoid hard-coded discrimination. It is one thing to recall biased technology, but another to ensure that the biased technology is not developed in the first place.

## Conclusion

This chapter has examined the gender relations of digitalisation, with a particular focus on AI. Although there is increasing recognition that technologies are socially shaped by the minds, hands, and cultures of people and, therefore, reflect history, context, choices, and values, we have argued that there is still insufficient focus on the ways in which gender relations are embedded in technology. This is the case whether it is the design of airbags in cars using crash-test dummies modelled on the male body; or the failure to include women in medical trials; or biased datasets used in algorithmic decision making. Gendered practices mediate technological transformations, and the political and socio-economic networks that shape and deploy technical systems.

Adopting such a technofeminist perspective suggests that the dominance of men working in and designing AI is deeply interconnected in a feedback loop with the increasingly identified gender biases in AI. Like the technologies that preceded them, social bias inscribed in AI software is not a glitch or bug in the system, but rather the result of persistent structural inequalities. In other words, while recent developments in data-driven algorithmic systems pose novel and urgent social justice issues, they also reflect the gender power relations long identified in feminist literature on technoscience. The historical relationship between technical expertise and masculinity, in combination with the 'chilly' organisational culture of technological work, is still generating an AI labour force that is unrepresentative of society as a whole.

If then technology is both a reflection and a crystallisation of society, feminist analysis must attend to the economic, political, cultural, and historical forces shaping AI in the current era. It really matters who is in the room, and even more so who is absent, when new technology like AI is developed. Technologies are designed to solve problems, and increasingly real-world societal questions are primarily posed as computational problems with technical solutions. Yet even the ways in which these tools select and pose problems foregrounds particular modes of seeing or judging over others. It has proven to be anything but gender neutral. As Ursula Le Guin (2017, p.150) commented perceptively as long ago as 1969: 'The machine conceals the machinations' .

## References

- Abbate, J. (2012) Recoding Gender: Women's Changing Participation in Computing . Cambridge, MA: MIT Press.
- Alegria, S. (2019) 'Escalator or Step Stool? Gendered Labor and Token Processes in Tech Work' . Gender &amp; Society , 33(5): 723.
- Ashcraft, C., B. McLain, and E. Eger. (2016) 'Women in Tech: The Facts' . National Center for Women &amp; Technology (NCWIT).

- Atanasoski, N. and K. Vora. (2019) Surrogate Humanity. Race, Robots, and the Politics of Technological Futures . Duke University Press.
- Atomico (2018) 'The State of European Tech' . Atomico.
- Atomico (2020) 'The State of European Tech' . Atomico.
- Bardzell, S. (2018) 'Utopias of Participation: Feminism, Design, and the Futures' . ACM Transactions in Computer-Human Interaction 25(1) Article 6 (February).
- Barsan, I. (2020) 'Research Reveals Inherent AI Gender Bias: Quantifying the Accuracy of Vision/Facial Recognition on Identifying PPE Masks' . Wunderman Thompson. www.wundermanthompson.com/insight/ai-and-gender-bias
- Benjamin, R (2019) Race after Technology: Abolitionist Tools for the New Jim Code . Cambridge, MA: Polity Press.
- BMJ (2020) 'Pulse Oximetry may Underestimate Hypoxaemia in Black Patients' . British Medical Journal 371: m4926.
- Bowker, G. and S. L. Star. (2000) Sorting Things Out: Classification and Its Consequences . Cambridge, MA: MIT Press.
- Broad, E. (2019) 'Computer Says No' . Inside Story 29 April.
- Buolamwini, J. and T. Gebru. (2018) 'Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification' . Proceedings of Machine Learning Research 81: 1-15.
- Cardador, M. T. and P. L. Hill. (2018) 'Career Paths in Engineering Firms: Gendered Patterns and Implications' . Journal of Career Assessment 26(1): 95-110.
- Cheryan, S. and H. R. Markus. (2020) 'Masculine defaults: Identifying and mitigating hidden cultural biases' . Psychological Review 127(6): 1022-1052.
- Cockburn, C. and S. Ormrod. (1993) Gender and Technology in the Making . London: Sage.
- Collins, P. H. (1998) 'It's All in the Family: Intersections of Gender, Race, and Nation' . Hypatia 13(3): 62-82.
- Crawford, K. (2021) Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence . New Haven: Yale University Press.
- Crenshaw, K., N. Gotanda, G. Peller, and T. Thomas. (eds.) (1995) Critical Race Theory: The Key Writings that Formed the Movement . New York: New Press.
- Criado Perez, C. (2019) Invisible Women: Exposing Data Bias in a World Designed for Men . London: Chatto &amp; Windus.
- D'Ignazio, C. and L. F. Klein. (2020) Data Feminism , Cambridge, MA: MIT Press.
- Dastin, J. (2018) 'Amazon Scraps Secret AI Recruiting Tool that Showed Bias against Women'. Reuters 10 October.
- Ensmenger, N. L. (2012) The Computer Boys Take Over: Computers, Programmers, and the Politics of Technical Expertise . Cambrige, MA: MIT Press.
- Eubanks, V. (2018) Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor . New York: St Martin's Press.
- European Commission (2019) 'Women in Digital Scoreboard' .
- Frieze, C. and J. Quesenberry. (2019) Cracking the Digital Ceiling . Cambridge: Cambridge University Press.
- Garg, N., L. Schiebinger, D. Jurafsky, and J. Zou. (2018) 'Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes' . Proceedings of the National Academy of Sciences USA 115(16): E3635-E3644.
- Gebru, T. (2020) Race and Gender. In The Oxford Handbook of Ethics of AI , eds M. Dubber, F. Pasquale and S. Das, pp. 253-270. Oxford: OUP.

- Google (2020) 'Google Diversity Annual Report 2020' . 1-61.
- Gray, M. and S. Suri. (2019) Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass . Boston: Harcourt.
- Gurumurthy, A., N. Chami, and C. A. Billorou. (2018) 'Gender Equality in the Digital Economy: Emerging Issues' . Development Alternatives With Women for a New Era (DAWN) August.
- Hampton, L. M. (2021) 'Black Feminist Musings on Algorithmic Oppression' . Preprint at arXiv:2101.09869.
- Haraway, D. (1988) 'Situated Knowledges. The Science Question in Feminism and the Privilege of Partial Perspective' . Feminist Studies 14(3): 575-599.
- Harding, S. (1986) The Science Question in Feminism . Cornell University Press.
- Hempel, J. (2017) 'Melinda Gates and Fei-Fei Li Want to Liberate AI from 'Guys With Hoodies'' . WIRED 4 May.
- Hicks, M. (2017) Programmed Inequality: How Britain Discarded Women Technologists and Lost Its Edge in Computing , Cambridge. MA: MIT Press.
- Hill, C., C. Corbett, and A. St. Rose. (2010) 'Why So Few? Women in Science, Technology, Engineering and Mathematics' . American Association of University Women (AAUW).
- Hutchinson, B. and M. Mitchell. (2019) '50 Years of Test (Un)fairness: Lessons for Machine Learning' . FAT ∗ '19, 29-31 January, Atlanta, GA, USA.
- Kolhatkar, S. (2017) 'The Tech Industry's Gender Discrimination Problem' . The New Yorker .
- Koning, R., S. Samila, and J. P. Ferguson. (2021) 'Who do We Invent For? Patents by Women Focus More on Women's Health, but Few Women get to Invent' . Science 372(6548): 1345-1348.
- Krivkovich, A., L. Lee, and E. Kutcher. (2016) Breaking Down the Gender Challenge . McKinsey.
- Kwon, J. and H. Yun. (2021) 'AI Chatbot Shut Down After Learning to Talk Like a Racist Asshole' . VICE 12 January.
- Lambrecht, A. and C. Tucker. (2019) 'Algorithmic Bias? An Empirical Study of Apparent Gender-Based Discrimination in the Display of STEM Career Ads' . Management Science 65: 2966-2981.
- Latour, B. (1990) 'Technology Is Society Made Durable' . The Sociological Review 38(1): 103-131.
- Le Guin, U. (2017[1969]) The Left Hand of Darkness . London: Orion Books.
- Leavy, S. (2018) 'Gender Bias in Artificial Intelligence: The Need for Diversity and Gender Theory in Machine Learning' . In: Proceedings of the 1st International Workshop on Gender Equality in Software Engineering , pp. 14-16. New York, NY: ACM.
- Lee, D. (2018) 'Google Staff Walk Out over Women's Treatment' . BBC News 1November.
- Mantha, Y. and S. Hudson. (2018) 'Estimating the Gender Ratio of AI Researchers around the World' . Element AI 17 August.
- Maron, D. F. (2018) 'Science Career Ads Are Disproportionately Seen by Men' . Scientific American 25 July.
- Mehrabi, N., F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan. (2019) 'A Survey on Bias and Fairness in Machine Learning' . Preprint at arXiv .
- Miceli, M., J. Posada, and T. Yang. (2022) 'Studying up Machine Learning Data: Why Talk about Bias When We Mean Power?'. Conference Proceedings ACM HCI . 6, GROUP, Article 34 (January).

- Murray, S. (2016) 'When Female Tech Pioneers Were the Future' . Financial Times 7 March.
- Noble, S. (2018) Algorithms of Oppression: How Search Engines Reinforce Racism . New York: NYU Press.
- O'Neil, C. (2016) Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy . London: Allen Lane.
- OECD (2018) 'Empowering Women in the Digital Age: Where Do We Stand?' HighLevel Event at the margin of the 62nd session of the UN Commission on the Status of Women (March): 1-5.
- Oudshoorn, N. (1994) Beyond the Natural Body: An Archaeology of Sex Hormones . London: Routledge.
- Pérez-Bustos, T. (2018) 'Let Me Show You': A Caring Ethnography of Embodied Knowledge in Weaving and Engineering. In A Feminist Companion to the Posthumanities , eds C. Asberg and R. Braidotti, pp. 175-188. New York: Springer.
- Phan, T. N. (2019) 'Amazon Echo and the Aesthetics of Whiteness' . Catalyst: Feminism, Theory, Technoscience 5(1): 1-39.
- Prates, M., P. Avelar, and L. C. Lamb. (2019) 'Assessing Gender Bias in Machine Translation: A Case Study with Google Translate' . Neural Computing and Applications , March: 1-19.
- Quirós, C. T., E. G. Morales, R. R. Pastor, A. F. Carmona, M. S. Ibáñez, and U. M. Herrera. (2018) Women in the Digital Age . Brussels: European Commission.
- Reardon, S. (2021) 'Gender Gap Leads to few US Patents that Help Women' . Nature 597(2)(September): 139-140.
- Roberts, S. T. (2019) Behind the Screen: Content Moderation in the Shadows of Social Media . New Haven: Yale University Press.
- Sánchez-Monedero, J., L. Dencik, and L. Edwards. (2020) 'What Does it Mean to 'Solve' the Problem of Discrimination in Hiring? Social, Technical and Legal Perspectives from the UK on Automated Hiring Systems' . FAT ∗ Conference Proceedings: 458-468.
- Sandoval, C. (2000) Methodology of the Oppressed . Minneapolis: University of Minnesota Press.
- Schwab, K. (2016) The Fourth Industrial Revolution . London: Penguin.
- Scott, A., F. Kapor Klein, and U. Onovakpuri. (2017) 'Tech Leavers Study' . Ford Foundation/Kapor Center for Social Impact , pp. 1-27.
- Sey, A., and N. Hafkin. (2019) 'Taking Stock: Data and Evidence on Gender Equality in Digital Access, Skills, and Leadership' . Preliminary findings of a review by the EQUALS Research Group, United Nations University.
- Simonite, T. (2018) 'AI is the Future - But Where are the Women?' WIRED 17 August. Stathoulopoulos, K. and J. Mateos-Garcia. (2019) 'Gender Diversity in AI Research' . Nesta .
- Steed, R. and A. Caliskan. (2021) 'Image representations learned with unsupervised pre- training contain human-like biases' . FAccT '21 , 3-10 March, Canada. Preprint arXiv .
- Stoet, G. and D. C. Geary. (2018) 'The Gender-Equality Paradox in Science, Technology, Engineering, and Mathematics Education' . Psychological Science 29(4): 581-593.
- Suchman, L. (2008) Human-Machine Reconfigurations . Cambridge: CUP.
- Taylor, L. (2017) 'What is Data Justice? The Case for Connecting Digital Rights and Freedoms Globally' . Big Data and Society 4(2): 1-14.

- Tech Nation (2018) 'Diversity and Inclusion in UK Tech Companies'. https:// technation.io/insights/diversity-and-inclusion-in-uk-tech-companies/
- UNESCO (2020) Artificial Intelligence and Gender Equality: Key Findings of UNESCO's Global Dialogue . UNESCO Publishing.
- UNESCO (2021) UNESCO Science Report: The Race Against Time for Smarter Development . UNESCO Publishing.
- Wagman, K. and L. Parks. (2021) 'Beyond the Command: Feminist STS Research and Critical Issues for the Design of Social Machines' . Proceedings of CSCW '21. ACM, NY.
- Wajcman, J. (2004) TechnoFeminism . Cambridge: Polity Press.
- Wajcman,J.(2010)'Feminist Theories of Technology' . CambridgeJournalofEconomics 34(1): 144.
- Wajcman, J., Young, E. and FitzMaurice, A. (2020) 'The Digital Revolution: Implications for Gender Equality and Women's Rights 25 Years after Beijing' . Discussion Paper No. 36 (August) , UNWomen.
- West, S., M. Whittaker, and K. Crawford. (2019) Discriminating Systems: Gender, Race and Power in AI . New York: AI Now Institute.
- Williams, J. C., R. M. Korn, and A. Ghani. (2022) 'Pinning Down The Jellyfish: The Workplace Experiences of Women of Color in Tech. Worklife Law'. University of California, Hastings College of the Law.
- World Economic Forum. (2018) Global Gender Gap Report .
- World Economic Forum. (2020) Global Gender Gap Report .
- Wynn, A. T. and S. J. Correll. (2018) 'Puncturing the Pipeline: Do Technology Companies Alienate Women in Recruiting Sessions?' Social Studies of Science 48(1): 149-164.
- Young, E., J. Wajcman, and L. Sprejer. (2021) Where Are the Women? Mapping the Gender Job Gap in AI . Public Policy Briefing. The Alan Turing Institute.