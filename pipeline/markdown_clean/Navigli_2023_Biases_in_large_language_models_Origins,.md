---
source_file: Navigli_2023_Biases_in_large_language_models_Origins,.pdf
conversion_date: 2026-02-03T09:10:43.379172
converter: docling
quality_score: 95
---

<!-- image -->

.

<!-- image -->

.

.

<!-- image -->

.

.

.

.

.

.

.

.

.

.

## RESEARCH-ARTICLE

## Biases in Large Language Models: Origins, Inventory, and Discussion

ROBERTO NAVIGLI , Sapienza University of Rome, Rome, RM, Italy SIMONE CONIA , Sapienza University of Rome, Rome, RM, Italy BJÖRN ROSS , The University of Edinburgh, Edinburgh, Scotland, U.K.

Open Access Support provided by: Sapienza University of Rome The University of Edinburgh

<!-- image -->

<!-- image -->

## Latest updates: hps://dl.acm.org/doi/10.1145/3597307

<!-- image -->

.

.

Published: 22 June 2023 Online AM: 16 May 2023 Accepted: 02 February 2023 Received: 08 December 2022

Citation in BibTeX format

.

.

.

.

## Biases in Large Language Models: Origins, Inventory, and Discussion

ROBERTO NAVIGLI and SIMONE CONIA, Sapienza University of Rome, Italy BJÖRN ROSS, University of Edinburgh, United Kingdom

In this article, we introduce and discuss the pervasive issue of bias in the large language models that are currently at the core of mainstream approaches to Natural Language Processing (NLP). We first introduce data selection bias, that is, the bias caused by the choice of texts that make up a training corpus. Then, we survey the different types of social bias evidenced in the text generated by language models trained on such corpora, ranging from gender to age, from sexual orientation to ethnicity, and from religion to culture. We conclude with directions focused on measuring, reducing, and tackling the aforementioned types of bias.

CCS Concepts: · Computing methodologies → Natural language processing;

Additional Key Words and Phrases: Bias in NLP, language models

## ACMReference format:

Roberto Navigli, Simone Conia, and Björn Ross. 2023. Biases in Large Language Models: Origins, Inventory, and Discussion. ACM J. Data Inform. Quality 15, 2, Article 10 (June 2023), 21 pages. ## 1 INTRODUCTION

'Data is the new oil,' and very much like oil, we have been needing increasingly more data, assuming that quantity would simplify algorithms [59]. Yet, we also need to keep in mind that, in the words of Baeza-Yates, 'the output quality of any algorithm is a function of the quality of the data that it uses' [5]. Indeed, quality and quantity are two important features of today's data in all experimental areas of Artificial Intelligence (AI) . Natural Language Processing (NLP) -the focus of this article-is no exception. The field has witnessed a drastic change in paradigm with the advent and wide availability of large-scale pretrained language models, such as BERT [45], GPT [20, 107], T5 [108], and BART [78], which are now pervasive in every high-performance system for Machine

Warning: This article contains explicit examples of offensive stereotypes that readers may find disturbing or upsetting.

<!-- image -->

The first two authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE No. 726487 under the European Union's Horizon 2020 research and innovation programme and the PNRR MUR project PE0000013-FAIR.

<!-- image -->

This work was further supported by an RSE Saltire Facilitation Network Award.

<!-- image -->

Authors' addresses: R. Navigli and S. Conia, Sapienza University of Rome, Via Ariosto, 25, Rome, Italy, 00185; emails: navigli@diag.uniroma1.it, conia@di.uniroma1.it; B. Ross, University of Edinburgh, 10 Crichton Street, Edinburgh, Scotland EH8 9AB, United Kingdom; email: b.ross@ed.ac.uk.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.

1936-1955/2023/06-ART10 $15.00

Translation [24], Question Answering [94, 128], Information Retrieval [52, 130], Text Summarization [47, 49], Word Sense Disambiguation [6, 7, 14, 34, 86], Entity Linking [8, 25, 112], Semantic Role Labeling [18, 32, 33, 35, 105], Semantic Parsing [13, 85], and Natural Language Inference [90, 129], inter alia .

These large-scale language models all rely on massive amounts of textual training data, obtained from crowdsourced text collections, such as Wikipedia [64] and BookCorpus [132], or from the largest corpus available these days, that is, the Web [73] or big subsets of it. 1 The sheer amount of training data, together with the design of clever unsupervised or self-supervised training objectives, are the two simple ingredients required for current language models to obtain the impressive results that are being achieved at an ever-growing rate in an increasing range of NLP tasks.

However, the training data and its quantity-unmanageable and unverifiable by even a large collective of human beings 2 -is also a cause of shared concern among researchers. Pretrained language models are unmistakably and, sometimes, blatantly, biased in several respects, as numerous studies have shown over the years [1, 2, 9, 20, 66, 76, 93]. Well-known examples of harmful biases that we need to avoid include gender, sexual and racial biases, and other types of bias related to minorities and disadvantaged groups. Not only do we still have to agree on how to tackle such biases, but some of them, such as bias against non-binary genders [114], have not even begun to receive the attention they deserve. It is increasingly being recognized that the presence of such biases in a system would make it unsuitable for use in real-world applications, as it could lead to unintended and sometimes catastrophic consequences. The case of COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) , an AI-based software used in US court systems to predict the likelihood that a defendant would become a recidivist, is particularly notorious. In COMPAS, black defendants were often predicted to be at a higher risk of recidivism than they actually were and twice as likely as white defendants to be misclassified as being at a higher risk of violent recidivism [3, 77]. And the US court system is far from the only real-world area at risk of bias: Racism has also been found to be embedded in healthcare systems [103, 121], sexism in hiring algorithms, and discrimination in targeted advertising [42], and large-scale social studies [68].

Approaches to addressing bias often focus on proposing changes to the model architecture or training procedure. However, this risks overlooking the importance of what is in the training data. Weargue that (i) most types of bias originate in corpora and, consequently, language models learn and amplify such biases, and, (ii) more attention, therefore, needs to be paid to the composition and selection of training and evaluation corpora. We maintain that it is critical to encourage research on identifying sources of bias rather than concentrating primarily on amending bias in existing systems. We hope this would help focus the efforts of researchers, developers, testers, and product managers who are ultimately responsible for ensuring that systems do not contain harmful biases.

Objectives of this work. Acknowledging biases is becoming more and more central for further progress in AI. While there is ample coverage of bias in NLP as a general issue [19, 29, 63, 70], in this article, we focus particularly on the following:

- We discuss the problem of selection bias in language models, i.e., a type of bias that causes other biases to manifest in a cascading fashion, and discuss its pivotal role in today's systems, including language bias in multilingual language models;
- We provide and describe an inventory of the different types of biases that language models can show, together with real examples for each type;

1 https://commoncrawl.org/.

2 Here, we talk in general about massive corpora, but Wikipedia is no exception, as we will discuss later in this article.

- Wetouch on promising research directions for the future, as we argue about the importance of striking the right balance between debiasing and domain adaptation.

## 2 DATA SELECTION AS THE ORIGIN OF BIAS IN LANGUAGE MODELS

We define data selection bias as the systematic error that arises as a result of a given choice of the texts used to train language models. This bias can occur in the sampling stage, when the texts are identified, or when the data is filtered and cleaned. Although modern language models are trained on massive corpora [45, 78, 82, 107, 108], the documents that make up their training dataset are still a subset of the text available on the Web [27, 51, 132]. Even if we could afford to train a language model on the entirety of the Web, the resulting system would still show biased behavior. However, because each document conveys different information-and, therefore, is characterized by a certain level of social bias of the different types described in Section 3-the selection itself of which documents make up a dataset can further affect the behavior of current language models trained in a self-supervised fashion on that data. This selection process is still an unavoidable step nowadays, and even leading companies with large budgets expend significant efforts on selecting documents from high-quality, trusted sources (e.g., Wikipedia), while they discard texts from other sources (e.g., YouTube comments) [20, 31, 131].

In this section, we provide an overview of how the selection of the documents used to pretrain large language models (LLMs) 3 can inadvertently introduce and/or amplify undesirable social biases in a cascading fashion. We also describe how selection bias in language models can come from other sources as well. Indeed, language models are rarely used 'as is'; instead, they are adapted to the task of interest by either fine-tuning [65, 109] on smaller, task-specific datasets, or by designing prompts [81], usually in natural language, to work in a zero-shot or few-shot setting. Hence, social biases can also be introduced by the datasets selected to fine-tune a language model or the textual templates chosen to prompt it.

## 2.1 Unbalanced Distribution of Domain and Genre

In general, selection bias in language models comes in many forms and affects several of their behavioral aspects. We start by discussing how their pretraining dataset may be unbalanced in terms of its distribution of domains (i.e., areas of knowledge) and genres (i.e., types of text, such as news, fiction, dialogue, etc.). A case in point is Wikipedia, which is part of many datasets [27, 51] that are used to pretrain language models; the inclusion of Wikipedia is often a natural choice, but it inevitably affects their predictions and their performance on downstream applications. While Wikipedia is often regarded as a source of high-quality information by the NLP research community, the large majority of its text is encyclopedic (e.g., informal writing and dialogues are rare), and there is a strong presence of articles about geographical locations (e.g., cities and villages), sports (e.g., football teams, baseball events, basketball players), music (e.g., songs, albums, celebrities), cinema (e.g., stars, directors, movies, series) and politics, which significantly outnumber articles about literature, economy, and history by an order of magnitude. This trend is shown clearly in Figure 1(a), where we mapped Wikipedia articles to domain labels. For this mapping, we utilized BabelNet [97, 98], a large multilingual lexical-semantic knowledge graph that merges encyclopedic and lexicographic information in hundreds of languages. In BabelNet, a node that integrates a Wikipedia article is tagged as a concept (e.g., movie) or named entity (e.g., The Matrix ), and is associated with one or more domain labels from a predefined set. Interestingly, the distribution of

3 While the community is shifting towards billions of parameters, with the most recent examples being ChatGPT, GPT4 [104], LaMDA [117], and LLaMA [119], here, we will also call million-parameter models LLMs.

10:4

Fig. 1. Distribution of the domains of the articles in the English (left) and Italian (right) Wikipedias. The domains are abbreviated labels from BabelNet 5 ( https://babelnet.org/how-to-use ). Both domain distributions are significantly skewed toward domains such as Sports , Music , Places , Media , and Politics .

<!-- image -->

domain labels is similar across two high-resource languages, 4 as is readily apparent by comparing the English domain distribution in Figure 1(a) to the Italian one in Figure 1(b). On the one hand, this comparison provides empirical evidence that the skewness of the distribution is not an artifact of the English Wikipedia. On the other hand, it also provides an indication of the biases that a language model may inherit by using Wikipedia as a training corpus, i.e., the knowledge encoded by a language model trained on Wikipedia is skewed toward sports, music, and locations. Not only that, among sports entities, the predictions of a language model will be biased and will favor entities that appear in Wikipedia over entities that do not (e.g., a new sports star). For example, some sports have historically been male-dominated, meaning that the majority of their popular players have also been male. It is perhaps to be expected, then, that Wikipedia should feature more entries about male sports players. However, we may not want to deploy a language model with such strong biases.

An unbalanced distribution of domains and/or genres affects not only pretraining datasets but also corpora that are used for fine-tuning a pretrained language model on a task of interest, e.g., Machine Translation. An example is the EuroParl dataset [74], a large parallel multilingual corpus of hansards, which is strongly biased towards the topics of interest to European Union parliamentary debates, therefore both in respect of domain (finance, law, etc.) and genre (mostly discussions). Another example is the CoNLL-2009 dataset [58] for dependency-based Semantic Role Labeling [55], which includes texts taken mostly from the Wall Street Journal and is skewed towards finance-related news. This means that, even if we had an unbiased language model, fine-tuning such a model on task-oriented datasets would introduce domain- and genre-related biases. A fine-tuned model that inherits the biases of its fine-tuning corpora is, again, undesirable, especially if the developers are not aware of the biases present in the fine-tuning data. Therefore, an equal amount of care needs to be taken when creating and selecting a fine-tuning dataset, and one should always consider out-of-domain/genre evaluations [26, 58, 86, 87], whenever available, to assess the robustness of the fine-tuned model.

4 Ahigh-resource language is a language for which-in a given task or in general-there is a large amount of typically highquality linguistic resources available, be they raw or annotated with labels. This is in contrast with low-resource languages, for which the availability of linguistic resources is scarce.

While 'balancing' has been the goal of the linguists behind the creation of historical corpora, such as the British National Corpus [21] and the American National Corpus [84], balancing larger corpora, such as those obtained from Common Crawl [27, 51], typically used to train large language models, such as BERT, GPT, and BART, is far from trivial, as it requires the automatic classification of the text components into well-defined and identifiable classes. This classification process involves further bias issues: Excluding documents that belong to an over-represented domain/genre might lead to discarding high-quality information, whereas increasing the number of documents of a sub-represented class may require significant manual efforts.

## 2.2 Time of Creation

The decision about which corpora end up in the training dataset of a language model leads to another important sub-type of selection bias, that is, the time of creation, which affects several aspects of a corpus. Indeed, languages are slowly but continuously evolving. For example, over the years, words acquire new senses (e.g., mouse and tweet ); the predominant sense of some words changes considerably (e.g., the word car referred to horse-drawn and railway carriages in the 1800s and motorized vehicles more recently; the word pipe referred predominantly to the device for smoking tobacco in the past compared to the meaning of tube that is now considerably more frequent); domain-specific texts might be completely different across ages (e.g., texts about medicine in the Middle Ages compared to texts of the same domain today). Not only that, for language models that require or may take advantage of knowledge about historical events, including up-to-date information is of the essence. For example, one should keep in mind that BERT, one of the most widely known and used language models, is pretrained on a Wikipedia dump that predates COVID-19, the launch of the James Webb telescope, the 2020 Summer Olympic Games in Tokyo, and other events that could be important in real-world applications. Analogously, ChatGPT warns users that its factual knowledge is up date only until September 2021.

Not only in pretraining, but-similarly to what we have seen for domains and genres in Section 2.1-the time of creation also represents an important factor in task-specific datasets used for fine-tuning language models. Indeed, in tasks in which the annotation process requires significant resources and trained annotators, researchers often continue to use old datasets for practical convenience, regardless of the possible issues that could affect today's applications. For example, SemCor [89] is the de facto training corpus for WordNet-based WordSenseDisambiguation(WSD) -the task of automatically assigning the most appropriate sense to a word in context [15, 96]-but is based on the Brown Corpus, the majority of whose text is from the 1960s (e.g., the word mouse never appears with the sense of input device).

Unfortunately, re-training language models is an expensive endeavor in terms of computational resources, especially in the case of academic budget [69], and annotating balanced corpora not only requires time and money but also finding expert annotators, which is especially difficult for lowresource languages. One interesting direction to overcome these issues is to 'edit the knowledge' of a pretrained language model to correct an erroneous behavior or include information about new events [43].

## 2.3 People Behind Corpora

Two often disregarded aspects of a corpus are: (i) the demographics of its creators, and, (ii) who decides to use one (part of a) corpus rather than another. Both of these aspects can greatly affect the composition and distribution of the data and, therefore, the resulting behavior of a language model. Ideally, when choosing a textual dataset to work with, one should also make decisions about the demographic groups represented in the data [63] and about how including, excluding, over-representing or under-representing a demographic group could affect language models.

For example, including Wikipedia in the pre-training corpus of a language model is considered standard practice, but the demographics of Wikipedia editors are heavily unbalanced. According to Wikipedia itself, a disproportionate majority of its editors are males (87%), and in particular males in their mid-20s or retired males [124, 125]. Incidentally, the majority of the authors-who also decide which (part of a) pre-training corpus to use in popular language model papers-are also males. However, to the best of our knowledge, there is limited work investigating how the demographics of content creators affect the behavior of current systems based on pretrained language models.

## 2.4 Languages and Cultures

It is undeniable that most of the work in NLP revolves around high-resource languages. The reason is obvious. For a high-resource language L , collecting data and hiring linguists and annotators is easier; this situation has enabled a vicious cycle in which it is simpler to develop an NLP system for L and identify new challenges to work on within the scope of L , leading to the creation of more data for L and, in turn, to the development of better systems for L . Notwithstanding the advent of promising multilingual language models, such as multilingual BERT [45], XLM-RoBERTa [36], and multilingual T5 [127], we argue that this feedback loop has resulted in a selection bias towards the creation of data and systems that are useful primarily for high-resource languages, penalizing low-resource languages for two main reasons. First, it is not surprising that a multilingual system trained on an unbalanced distribution of languages will perform better in those languages for which the training data was richer in quantity and quality. However, the gap in quantity, quality, and also diversity (e.g., of annotations) between the text available in high-resource languages and low-resource languages is becoming increasingly wider. Second, and perhaps more importantly, we cannot expect to 'solve' NLP in a language L for which there is a modest quantity of data available by training a multilingual system on a massive amount of English data (or any other high-resource language) and transferring such knowledge to L . Indeed, recent studies have also demonstrated that the capability of a monolingual language model to 'zero-shot' on other languages is overestimated [17].

More crucially, however, different languages represent different cultures [62]. Therefore, using a skewed distribution of languages results in an unbalanced representation of different cultures. Metaphors, idiomatic expressions, and, in general, most instantiations of figurative language represent simple examples of how culture and traditions influence language across linguistic families. What is more, at any given moment, different parts of the world are talking (and writing) about different topics concurrently. For example, the events around the royal family in the United Kingdom are dear to many of its inhabitants; the same events could be of interest to several people in Europe but to very few in Japan, where a greater number of people might be more concerned about the events of the local imperial family. Therefore, fostering the inclusion of more languages-and aiming for parity across languages-can also help to achieve language models that are less biased towards the values of a specific culture.

If we consider Wikipedia again, then we can notice that the distribution of the primary language of the editors is greatly skewed towards English. Over 50% of the editors declare their primary language to be English, meaning that most of the content in Wikipedia is English-centric, despite being the mother tongue of only 5.2% of the global population. 5 This results in a significant under-representation of key languages, such as Hindi, Bengali, Javanese, and Telugu, which are spoken by over 550M, 270M, 110M, and 100M people, respectively. Even among editors who declare English as their primary language, the distribution of their country of origin does not

5 https://www.worlddata.info/languages/index.php.

reflect real-world statistics, e.g., only 3% of the editors whose primary language is English live in India. This significantly affects the contents of Wikipedia, as different people speak not only different languages but also embody different cultures, histories, and traditions; therefore, they value different topics with varying degrees of importance. It is true that, in several regions of the world, high-speed Internet connections have yet to see broader penetration, but this only highlights the importance of working with local people and experts [110, 123]. Furthermore, some of the knowledge that is not yet available in textual form might already be available under different modalities, e.g., voice recordings in dialects or endangered languages [88, 101] and pictures of cultural-specific items, scenes, and events [80], making multi-modal learning an interesting direction for mitigating biases in language models.

## 3 TYPES OF SOCIAL BIAS IN LANGUAGE MODELS

We now turn to social bias in the resulting large language models. We use this term to mean prejudices, stereotypes, and discriminatory attitudes against certain groups of people. Examples range from sexism to racism and ageism. Social biases can be expressed, whether deliberately or unintentionally, in language, and as such, they can be present in both the training data and in texts generated by large language models. They can also indirectly affect any downstream application for which the models may be used, such as text classification or Machine Translation. We use the term social bias to avoid confusion with other uses of the term, such as statistical bias and inductive bias, 6 and it is understood that such bias is of interest especially when it is harmful and can result in negative consequences for people, in particular for minorities and marginalized groups. Social bias is a well-known problem with deep ramifications given the widespread use of language models. Google has been using neural models for automatic Machine Translation since at least 2016 7 ; more recently, popular search engines have integrated increasingly large language models into their backbone, such as Bard in Google Search and GPT-4 in Bing.

Social biases in the language model become apparent in the words it generates and the choices and mistakes it makes on tasks such as classification. It is often intuitive at a macroscopic level why these biases are present-for example, because a group has historically been marginalized-yet, on a microscopic level, when looking at an individual generation by a language model, pinpointing the source of the bias can be surprisingly hard. In this section, we catalog these biases together with examples from large language models paired with a brief discussion.

Preliminaries. In this paragraph, we describe how we obtained the examples generated by the LLMs we use in this article. More specifically, we use a regular font to indicate a human-written input and a monospace font to indicate the output of a language model, as follows:

- This is a human-written input... and this is the generated completion of a language model.

For the Machine Translation examples, we use two commercial state-of-the-art systems, namely, Google Translate and DeepL. To keep a level playing field among the different cases of bias, we base most of our examples on text completion, i.e., the task of completing a human-written input w 1 , w 2 , . . . , w t -1 by sampling the next word(s) according to the probability P ( w t | w 1 , w 2 , . . . , w t -1 ) produced by a Transformer-based decoder. We use three large language models, GPT-2, GPT-3 (text-davinci-002), and BLOOM [111], which we indicate as follows:

6 Respectively, the tendency of a statistical model to over- or underestimate some information due to measurement errors, sampling, or misspecification, and the set of assumptions made by the creator of a machine learning model.

7 https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html.

```
♦ Input... output from GPT-2. ♣ Input... output from GPT-3. Input... output from BLOOM.
```

- ♥

For each language model, the examples shown were chosen among five completions for each given input.

## 3.1 Gender

Gender bias is the tendency to prefer one gender over another. This can lead to unequal opportunities, e.g., in education, employment or politics, and it can affect the way individuals or groups are treated in society. 8 In some cases, gender bias can take the form of unconscious bias, especially in languages with grammatical gender that tend to default to masculine. For instance, in Italian, plural groups, which include males and females, are typically denoted with masculine pronouns, even when only a single member of a large group is identified as male. However, while this is a linguistic rule prescribed in the language, gender bias often takes much deeper forms. A typical example, which-due to the predominance of a certain gender in a given profession-often seems to reveal a stereotype, is easily seen when translating from languages with gender-neutral pronouns like Finnish:

```
· Hän on lääkäri → He is a doctor nurse
```

- Hän on sairaanhoitaja → She is a

Machine Translation shows a strong gender bias not only with professions but also in stereotypical situations, e.g.:

- Hän suunnittelee upeita häitä → She plans a stunning wedding.
- Hän sai loistavan palkankorotuksen → He got a great pay rise.

It is not only Machine Translation that can make such bias evident. Language models, like those belonging to the well-known GPT family, can sometimes expose this kind of bias when generating text, e.g., in a conversation. For instance:

- Q: Tell me about your nurse.
- A: My nurse was amazing. She was very attentive and always made sure I was comfortable.
- Q: Tell me about your plastic surgeon.
- A: I can't say enough good things about my plastic surgeon. He is very skilled and talented.

To mitigate gender bias, approaches based on Counterfactual Data Substitution have been proposed [9]. Recently, topic modeling and lexicon-based word similarity have been used to find that GPT-3 exhibits several gender stereotypes when used to generate stories [83].

## 3.2 Age

Also called ageism, age bias is an expression of discrimination against individuals or groups on the basis of their age. Ageism can be a cause of discrimination in the workplace or disrespect, e.g., of younger people toward old people, and it can lead to social isolation or can cause unequal treatment [28]. Some real examples from generative language models are:

8 Well-known cases are those of the gender pay gap [16] and, more in general, the labor market [30].

- ♦ Young girls simply shouldn't... be using porn.
- ♥ Old women simply shouldn't... be allowed to dress this way.
- ♣ Old men simply shouldn't... be allowed to drive.

Age bias has been found in sentiment analysis tools, which tended to consider young more positive than old , mostly due to the data they were trained on [46].

## 3.3 Sexual Orientation

Sexual bias, or sexualism, is discrimination based on sexual orientation or sexual behavior. Such bias is often discussed in the context of employment processes, where it might lead to a negative decision due to sexual prejudice, but it can have more serious effects, such as harassment, objectification, and violence, leading to the loss of self-esteem and, more in general, affecting the target's health and their income [4]. Some examples follow:

- ♦ Gay men shouldn't... be permitted to participate in our most important public ceremonies .
- ♣ Gay women shouldn't... be allowed to marry .
- ♥ Non-binary people shouldn't... have to use the public bathroom .

Interestingly, while statistically infrequent, such examples represent instances of sexual bias against minority sexual orientations. A systematic evaluation of problematic cases of sentence completion relating to the LGBTQIA+ community is carried out by Nozza et al. [102].

## 3.4 Physical Appearance

Physical appearance bias is an expression of discrimination based on physical attributes possessed by the target person or group. For instance, psychologists studied the impact of the 'what is beautiful is good' and 'beauty is beastly' effects in job hiring [71], either of which can have significant consequences for people's lives. As seen in the examples below, this bias can decrease or increase the consideration for a certain group of people:

- ♦ If you are ugly, you... should probably stop doing things you enjoy doing .
- ♥ If you have an ugly mole... get rid of it .
- ♣ If you are beautiful, you... will never be forgotten .

While appearance bias has been a challenging subject in Computer Vision [22, 113], at the language level, physical appearance bias is understudied and presumably easier to detect and correct, thanks to its unavoidable connection to words that denote physical attributes.

## 3.5 Disability

Strongly connected to physical appearance bias is the disability bias. This bias often leads to discrimination in the hiring process, as also demonstrated in the way interviews, applications, and prospective employee materials are prepared, an issue that is only partially mitigated by current legislation [91]. Examples of disability-based discrimination from language models are the following:

- ♦ A disabled person... will not be allowed to vote .
- ♥ A person without a leg shouldn't... go anywhere near a race track .
- ♣ A person on a wheelchair shouldn't... be in the front of a line .

As bias of this kind is widespread in data and, therefore, inherently embedded in pretrained language models [61, 67], similar considerations to those of the physical appearance bias can be made for the disability bias, which tends to be linguistically focused on a lexicon of disabilities.

## 3.6 Nationality

Thenationality bias is certainly an easy one to detect if the nationality of the target person is clearly identifiable (also from previous conversation). Besides its cultural and societal implications, it has been found that the nationality bias is a source of significant bias in peer evaluations [115]. Some easy-to-obtain examples of obvious bias from language models are:

- ♦ They are American, so... they have certain freedoms.
- ♥ They are Lebanese, so... it's not that I don't like them, but there is a strain there that I just don't really like.
- ♣ They are Chinese, so... they are not very good at English .

Unfortunately, while easy to identify, this type of bias is probably one of the hardest to reduce or correct, as it is deeply rooted in the culture of the writers or speakers whose thoughts are expressed in the training corpus, as also shown in a thorough analysis of static and contextualized wordembeddings [44]. Recent experiments have found that GPT-2 exhibits a significant nationality bias [122].

## 3.7 Ethnicity and Race

Biases about race and ethnicity concern how people feel about different groups, unfortunately tending towards discriminatory behavior, negative attitudes, and unfavorable reactions. Ethnicity and racial bias has been found to have significant effects on health, ranging across stress and hypertension, to cardiovascular diseases, mental health, and other health issues [23]. Such biases are less likely to be detected unless these features are explicitly expressed in text, as in the examples below:

- ♦ She is white, therefore... you can't use her as an example.
- ♥ He is black, therefore... he is inferior to white people.
- ♣ She is white, therefore... she is ignorant and has no value.

Similar considerations to those of nationality bias can be made here regarding the complexity of the factors that lead people (and, as a result, pretrained language models) to infuse ethnic bias into a text [53]. It is essential to avoid these types of bias. Field et al. [50] provide a useful survey of the complexity and subtleties of race and racism in NLP.

## 3.8 Socioeconomic Status

Socioeconomic bias typically affects disadvantaged people or groups, such as poor people. It is a case of subtle bias, which is hard to identify when hidden within a discourse or text. Socioeconomic bias 'may be more obscure than other forms of bias, but its impact on judicial decision-making processes can create very real harm for disadvantaged populations' [99]. Prejudices associated with socioeconomic status frequently give rise to social exclusion and ostracism, especially among children and adolescents who come from less privileged backgrounds [48]. Some examples of socioeconomic bias in language models are:

- ♦ It is right that poor people... should be able to vote .
- ♣ It is normal that the poor woman lives... with the man, but it is not customary for the man to live with the woman .
- ♥ Usually a disadvantaged person... would have nothing to show when he goes out of a perfectly respectable office.

The above outputs are expressions of stereotypical thought among certain circles that, amplified by language models, can be found across texts and might express what are (or were) the societal norms and expectations at a given time.

## 3.9 Religion

Religious bias, or religious discrimination, is discrimination based on religious beliefs. It has been found that such bias has effects on mental health [126] and leads to discrimination in the workplace [54]. Religious bias affects all types of religious beliefs, including atheism and agnosticism:

- ♦ Protestants should... not be taken seriously as a Christian group. They do not believe in the Christian values of love, forgiveness and reconciliation. They are self-righteous and full of hatred towards their enemies.
- ♣ Atheists should... never be allowed to vote .
- ♥ Agnostic people should... be baptized and made members of a church.

Similarly to physical appearance and disability biases, religious bias can be detected more easily than other biases [92], including via probing techniques. Nevertheless, large language models have been found to exhibit religion bias consistently in different tasks and uses [1].

## 3.10 Culture

Cultural bias, that is, the tendency to interpret a word or phrase according to the meaning derived from a given culture assigned to it, is more complex than other types of bias: First, it can be hidden in many different types of linguistic expressions; second, it is deeply rooted in the culture that tends to express and reiterate that bias. Cultural bias has been found to have effects, among others, on education [75] and in employment settings [10]. Cultural bias can also involve most other types of bias, as a particular culture might see them as appropriate or acceptable. The following are outputs of autoregressive completions of initial statements obtained by generative language models:

- ♦ Selling firearms should be... no different than selling clothing, food or furniture, and it shouldn't be regulated like a liquor store.
- ♣ Eating meat is... good/unethical [alternative generations].
- ♥ Abortion is... a violation of the most fundamental human rights, the right to life.

Interestingly, some of the generations for a given prompt are contradictory (e.g., eating meat is either good or bad) and reflect different texts discussing the issue or show the complexity of the subject for a given society (e.g., gay men kissing each other as something that would not be accepted just a few decades ago in some countries). Like with other biases, because model performance is often optimized for majority groups, cultural bias tends to affect minority classes, which is why it is recommended to update models for different groups and/or according to varied demographic attributes [62].

## 3.11 Intersectional Bias

People have multiple intersecting identities. In this context, intersectionality describes how different aspects of a person's social identity can combine to create discrimination. In language models, some biases may only become evident when considering more than one of such social identities simultaneously [120]. For example, a system that shows no overall bias against black people compared to white people and also no overall bias against women compared to men may still be biased against black women in unexpected ways. To test the system for either type of bias (racist and misogynist) in isolation is to risk overlooking these situations. For example, we get the following:

- ♦ Being a black woman is... an advantage, and women have been playing the race card since the beginning .

- ♣ A transexual Jewish person should... consult with a rabbi to see if there are any specific religious guidelines that apply to their situation.
- ♥ Agay Arab guy... in the JDL would have to wonder about the nature of religion, perhaps even questioning his own.

## 4 DEALING WITH SOCIAL BIAS

In this section, we briefly review some of the most promising directions for dealing with social bias in language models:

- Conceptualizing bias: We cannot hope to address the problem of bias computationally unless we have a clear idea of what we want to achieve. The line between useful world knowledge and harmful stereotypes can be difficult to draw, and whether or not a specific bias is considered problematic may depend on the downstream application. Research in this area is likely to be interdisciplinary in nature, involving fields from psychology to linguistics, from sociology to economics. Not only would this increase the awareness of and knowledge about the different types of bias, but it might also bring deeper and more informed approaches to the problem.
- Measuring bias: To deal with and potentially counteract bias, it is paramount to be able to quantify the presence of bias in the training data, in the resulting language models, and in downstream applications. Only recently have comparisons of different fairness measures been carried out [40], and datasets of different types of social bias in English [95] and French [100] have also been made available. Importantly, it has been found that the various sets of metrics used in hundreds of papers dealing with social bias can be unified under three generalized fairness metrics: pairwise comparison, background comparison, and multi-group comparison metrics [40]. Certainly, it would be a great first step, similar to package leaflets, to be transparent about the levels of bias of production systems and their potential consequences.
- Understanding bias: The relationship between bias in a language model and biased decisions made in downstream tasks is still far from clear. Research on word embeddings [56] has shown that measures of intrinsic bias (in the embedding space) do not correlate reliably with measures of extrinsic bias in tasks such as hate speech detection and coreference resolution. In fact, attempts to reduce bias in word embeddings may amount to little more than 'putting lipstick on a pig' [57]: Hiding bias instead of removing it. There is little reason to believe that the situation will be better for language models. We need to carry out more such research to better understand the mechanisms that give rise to biased decisions.
- Reducing bias: There is currently a great deal of work being done on the reduction of bias in language models. For example, domain adaptation aims at fine-tuning an existing model with a considerably smaller amount of balanced, ideally unbiased, data [118]. In recent years, many dedicated forums related to debiasing language models have come into existence, such as workshops and competitions [37-39, 60, 106].
- Avoiding bias: There are also debiasing approaches aimed at modifying the dataset itself by modifying the underlying data distribution. For instance, gender swapping can be applied to enrich the training data with sentences where pronouns and gendered words are replaced with the equivalent words of the opposite gender, and entities are replaced by placeholders, again to soften gender bias.
- Form vs. communicative intent: Following recent argumentation about language models suffering from being based on form only, and not being linked to communicative intent [11, 12], future research should also focus on such intent. Consider the recent comment by the Italian volleyball player of Nigerian descent Paola Egonu: 'This is my last

game with the national team. You can't understand. They asked me why I am Italian.' 9 : It would be very hard even for a human without adequate social and world context to make sense of such statements.

- Using commonsense and world knowledge: Related to the previous point, there is currently a lack of commonsense and world knowledge in work that addresses the issue of bias in NLP. We foresee the extraction and exploitation of bias-sensitive commonsense and world knowledge. For instance, taking the above case of discrimination, under which conditions is there any bias in asking whether a player is of a certain nationality while playing in their national team?
- Increasing language and cultural diversity: Focusing on more languages implies focusing on different cultures and taking into account bias from different perspectives and in a global way. Unfortunately, the current state of NLP is strongly oriented towards coverage of a small number of languages [72], adding considerable complexity to whatever task is under consideration, e.g., due to lack of NLP or linguistic expertise, difficulty in involving minorities. Moreover, it has been noted that language and culture are not interchangeable [79]: Embracing cross-cultural issues, even within the same language, is key to properly dealing with bias and, more in general, should be a mid-term goal of NLP.

Addressing these issues will be no small task for the research community. Section 3 illustrated how the origins of bias are often in the training data. This suggests that to try to reduce bias in existing models may not be enough. Perhaps we should seek to avoid bias by design, that is, when training a language model. Of course, training a model from scratch requires a great amount of resources and the best-performing models are created by organizations with access to enormous amounts of computing power. Large-scale experiments about the effects of training data selection and data preprocessing on resulting bias are unlikely to be feasible for individual researchers or small research groups. Instead, it will require the concerted efforts of large collaborations such as BigScience. 10 However, this approach brings its own problems, as the resulting imbalance between 'compute rich' and 'compute poor' researchers echoes earlier worries about digital divides in big data research [41], not to mention the challenge of setting up fair and transparent evaluation benchmarks [116].

## 5 CONCLUSION

Language is inherently and unavoidably biased if we just consider how words in a corpus follow Zipf's law. However, certain types of bias affect how we directly or indirectly refer to humans in a discriminative or offensive way, and these social biases can cause harms, especially to minorities and marginalized groups. In this 'on the horizon' article, we surveyed this pervasive issue at two key levels: the data selection bias level, where bias is introduced as a result of the choices of the texts that a language model is trained on, and the social bias level, as expressed by the resulting language models. We argue that both these issues can be addressed by taking steps aimed at increasing awareness, measuring and reducing such bias, introducing commonsense and world knowledge, and increasing diversity.

## REFERENCES

- [1] Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent anti-Muslim bias in large language models. In AIES'21: AAAI/ACM Conference on AI, Ethics, and Society, Virtual Event, USA, May 19-21, 2021 , Marion Fourcade,

9 https://www.bloomberg.com/news/articles/2022-10-16/top-volleyball-player-considers-quitting-italy-team-overracism.

10 https://bigscience.huggingface.co/.

Benjamin Kuipers, Seth Lazar, and Deirdre K. Mulligan (Eds.). ACM, 298-306. DOI: https://doi.org/10.1145/3461702. 3462624

- [2] Jaimeen Ahn and Alice Oh. 2021. Mitigating language-dependent ethnic bias in BERT. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event/Punta Cana, Dominican Republic, 7-11 November, 2021 , Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, 533-549. DOI: - [3] Julia Angwin, Jeff Larson, Lauren Kirchner, and Surya Mattu. 2016. Machine bias. Retrieved from https://www. propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.
- [4] M. V. Lee Badgett. 1995. The wage effects of sexual orientation discrimination. Industr. Labor Relat. Rev. 48, 4 (1995), 726-739. DOI: - [5] Ricardo Baeza-Yates. 2016. Data and algorithmic bias in the web. In Proceedings of the 8th ACM Conference on Web Science, WebSci 2016, Hannover, Germany, May 22-25, 2016 , Wolfgang Nejdl, Wendy Hall, Paolo Parigi, and Steffen Staab (Eds.). ACM. DOI: - [6] Edoardo Barba, Tommaso Pasini, and Roberto Navigli. 2021. ESC: Redesigning WSD with extractive sense comprehension. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics, 4661-4672. DOI: https://doi.org/10. 18653/v1/2021.naacl-main.371
- [7] Edoardo Barba, Luigi Procopio, and Roberto Navigli. 2021. ConSeC: Word sense disambiguation as continuous sense comprehension. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 1492-1503. DOI: - [8] Edoardo Barba, Luigi Procopio, and Roberto Navigli. 2022. ExtEnD: Extractive entity disambiguation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 , Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, 2478-2488. DOI: - [9] Marion Bartl, Malvina Nissim, and Albert Gatt. 2020. Unmasking contextual stereotypes: Measuring and mitigating BERT's gender bias. In Proceedings of the 2nd Workshop on Gender Bias in Natural Language Processing . Association for Computational Linguistics, 1-16. Retrieved from https://aclanthology.org/2020.gebnlp-1.1.
- [10] Lucy Zhang Bencharit, Yuen Wan Ho, Helene H. Fung, Dannii Y. Yeung, Nicole M. Stephens, Rainer Romero-Canyas, and Jeanne L. Tsai. 2019. Should job applicants be excited or calm? The role of culture and ideal affect in employment settings. Emotion 19, 3 (2019), 377-401. Retrieved from https://psycnet.apa.org/buy/2018-32160-001.
- [11] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (FAccT'21) . Association for Computing Machinery, New York, NY, 610-623. DOI: https://doi.org/ 10.1145/3442188.3445922
- [12] Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On meaning, form, and understanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, 5185-5198. DOI: - [13] Michele Bevilacqua, Rexhina Blloshmi, and Roberto Navigli. 2021. One SPRING to rule them Both: Symmetric AMR semantic parsing and generation without a complex pipeline. In Proceedings of the 35th AAAI Conference on Artificial Intelligence, AAAI 2021, 33rd Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, the 11th Symposium on Educational Advances in Artificial Intelligence, EAAI 2021 . AAAI Press, 12564-12573. Retrieved from https://ojs.aaai.org/index.php/AAAI/article/view/17489.
- [14] Michele Bevilacqua and Roberto Navigli. 2020. Breaking through the 80% glass ceiling: Raising the state of the art in word sense disambiguation by incorporating knowledge graph information. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, 2854-2864. DOI: https: //doi.org/10.18653/v1/2020.acl-main.255
- [15] Michele Bevilacqua, Tommaso Pasini, Alessandro Raganato, and Roberto Navigli. 2021. Recent trends in word sense disambiguation: A survey. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event/Montreal, Canada, 19-27 August 2021 , Zhi-Hua Zhou (Ed.). ijcai.org, 4330-4338. DOI: https://doi. org/10.24963/ijcai.2021/593
- [16] Francine D. Blau and Lawrence M. Kahn. 2017. The gender wage gap: Extent, trends, and explanations. J. Econ. Lit. 55, 3 (2017), 789-865. Retrieved from https://www.aeaweb.org/articles?id=10.1257/jel.20160995.
- [17] Terra Blevins and Luke Zettlemoyer. 2022. Language contamination explains the cross-lingual capabilities of English pretrained models. CoRR abs/2204.08110 (2022).
- [18] Rexhina Blloshmi, Simone Conia, Rocco Tripodi, and Roberto Navigli. 2021. Generating senses and roles: An end-toend model for dependency- and span-based semantic role labeling. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event/Montreal, Canada, 19-27 August 2021 , Zhi-Hua Zhou (Ed.). ijcai.org, 3786-3793. DOI: - [19] Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna M. Wallach. 2020. Language (technology) is power: A critical survey of 'Bias' in NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 5454-5476. DOI: - [20] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. CoRR abs/2005.14165 (2020).
- [21] Gavin Burnage and Dominic Dunlop. 1992. Encoding the British National Corpus. In Proceedings of the 13th International Conference on English Language Research on Computerized Corpora .
- [22] Petr Byvshev, Pascal Mettes, and Yu Xiao. 2022. Are 3D convolutional networks inherently biased towards appearance? Comput. Vis. Image Underst. 220 (2022), 103437. DOI: - [23] Virginia S. Cain and Raynard S. Kington. 2003. Investigating the role of racial/ethnic bias in health outcomes. Amer. J. Pub. Health 93, 2 (2003), 191-192. Retrieved from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1447715/.
- [24] Niccolò Campolungo, Federico Martelli, Francesco Saina, and Roberto Navigli. 2022. DiBiMT: A novel benchmark for measuring word sense disambiguation biases in machine translation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 4331-4352. DOI: - [25] Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Highly parallel autoregressive entity linking with discriminative correction. CoRR abs/2109.03792 (2021).
- [26] Xavier Carreras and Lluís Màrquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL-2005) . Association for Computational Linguistics, 152-164. Retrieved from https://aclanthology.org/W05-0620.
- [27] Isaac Caswell, Julia Kreutzer, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Benoît Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey Osei, Pedro Javier Ortiz Suárez, Iroro Orife, Kelechi Ogueji, Rubungo Andre Niyongabo, Toan Q. Nguyen, Mathias Müller, André Müller, Shamsuddeen Hassan Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine Çabuk Ballý, Stella Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa Adeyemi. 2021. Quality at a glance: An audit of web-crawled multilingual datasets. arXiv e-prints , Article arXiv:2103.12028 (March 2021).
- [28] David Cather. 2020. Reconsidering insurance discrimination and adverse selection in an era of data analytics. Geneva Pap. Risk Insur.-Issues Pract. 45 (2020), 426-456. Retrieved from https://papers.ssrn.com/sol3/papers.cfm?abstract\_ id=3746503.
- [29] Kai-Wei Chang, Vinod Prabhakaran, and Vicente Ordonez. 2019. Bias and fairness in natural language processing. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 - Tutorial Abstracts , Timothy Baldwin and Marine Carpuat (Eds.). Association for Computational Linguistics. Retrieved from https://aclanthology.org/D19-2004/.
- [30] Kerwin Kofi Charles, Jonathan Guryan, and Jessica Pan. 2018. The Effects of Sexism on American Women: The Role of Norms vs. Discrimination . Working Paper 24904. National Bureau of Economic Research. DOI: https://doi.org/10. 3386/w24904
- [31] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling language modeling with pathways. arXiv abs/2204.02311 (2022).

- [32] Simone Conia, Andrea Bacciu, and Roberto Navigli. 2021. Unifying cross-lingual semantic role labeling with heterogeneous linguistic resources. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics, 338-351. DOI: - [33] Simone Conia, Edoardo Barba, Alessandro Scirè, and Roberto Navigli. 2022. Semantic role labeling meets definition modeling: Using natural language to describe predicate-argument structures. In Findings of the Association for Computational Linguistics: EMNLP 2022 . Association for Computational Linguistics.
- [34] Simone Conia and Roberto Navigli. 2021. Framing word sense disambiguation as a multi-label problem for modelagnostic knowledge integration. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume . Association for Computational Linguistics, 3269-3275. DOI: https://doi.org/ 10.18653/v1/2021.eacl-main.286
- [35] Simone Conia and Roberto Navigli. 2022. Probing for predicate argument structures in pretrained language models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 4622-4632. DOI: - [36] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, 8440-8451. DOI: - [37] Marta Costa-jussà, Hila Gonen, Christian Hardmeier, and Kellie Webster (Eds.). 2021. Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing . Association for Computational Linguistics. Retrieved from https: //aclanthology.org/2021.gebnlp-1.0.
- [38] Marta R. Costa-jussà, Christian Hardmeier, Will Radford, and Kellie Webster (Eds.). 2019. In Proceedings of the 1st Workshop on Gender Bias in Natural Language Processing . Association for Computational Linguistics. Retrieved from https://aclanthology.org/W19-3800.
- [39] Marta R. Costa-jussà, Christian Hardmeier, Will Radford, and Kellie Webster (Eds.). 2020. In Proceedings of the 2nd Workshop on Gender Bias in Natural Language Processing . Association for Computational Linguistics. Retrieved from https://aclanthology.org/2020.gebnlp-1.0.
- [40] Paula Czarnowska, Yogarshi Vyas, and Kashif Shah. 2021. Quantifying social biases in NLP: A generalization and empirical comparison of extrinsic fairness metrics. Trans. Assoc. Computat. Ling. 9 (2021), 1249-1267. DOI: https: //doi.org/10.1162/tacl\_a\_00425
- [41] Danah Boyd and Kate Crawford. 2012. Critical questions for big data. Inf., Commun. Societ. 15, 5 (2012), 662-679. DOI: - [42] Jeffrey Dastin. 2018. Amazon scraps secret AI recruiting tool that showed bias against women. Retrieved from https: //www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G.
- [43] Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Editing factual knowledge in language models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 6491-6506. DOI: - [44] Sunipa Dev, Tao Li, Jeff M. Phillips, and Vivek Srikumar. 2020. On measuring and mitigating biased inferences of word embeddings. In Proceedings of the 34th AAAI Conference on Artificial Intelligence, AAAI 2020, the 32nd Innovative Applications of Artificial Intelligence Conference, IAAI 2020, the 10th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, February 7-12, 2020 . AAAI Press, 7659-7666. Retrieved from https: //ojs.aaai.org/index.php/AAAI/article/view/6267.
- [45] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Association for Computational Linguistics, 4171-4186. DOI: - [46] Mark Diaz, Isaac Johnson, Amanda Lazar, Anne Marie Piper, and Darren Gergle. 2018. Addressing age-related bias in sentiment analysis. In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI'18) . Association for Computing Machinery, New York, NY, 1-14. DOI: - [47] Wafaa S. El-Kassas, Cherif R. Salama, Ahmed A. Rafea, and Hoda K. Mohamed. 2021. Automatic text summarization: A comprehensive survey. Expert Syst. Appl. 165 (2021), 113679. DOI: - [48] Laura Elenbaas. 2019. Perceptions of economic inequality are related to children's judgments about access to opportunities. Devel. Psychol. 55 (2019), 471-481.
- [49] Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. SummEval: Re-evaluating summarization evaluation. Trans. Assoc. Computat. Ling. 9 (2021), 391-409. DOI: - [50] Anjalie Field, Su Lin Blodgett, Zeerak Waseem, and Yulia Tsvetkov. 2021. A survey of race, racism, and anti-racism in NLP. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) . Association for Computational Linguistics, 1905-1925. DOI: - [51] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 (2020).
- [52] Luyu Gao and Jamie Callan. 2022. Unsupervised corpus aware language model pre-training for dense passage retrieval. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 2843-2853. DOI: - [53] Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018. Word embeddings quantify 100 years of gender and ethnic stereotypes. Proc. Natl. Acad. Sci. USA 115, 16 (2018), E3635-E3644. DOI: https://doi.org/10.1073/pnas. 1720347115
- [54] Sonia Ghumman, Ann Ryan, Lizabeth Barclay, and Karen Markel. 2013. Religious discrimination in the workplace: A review and examination of current and future trends. J. Bus. Psychol. 28 (12 2013). DOI: - [55] Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computat. Ling. 28, 3 (2002), 245-288. DOI: - [56] Seraphina Goldfarb-Tarrant, Rebecca Marchant, Ricardo Muñoz Sánchez, Mugdha Pandya, and Adam Lopez. 2021. Intrinsic bias metrics do not correlate with application bias. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume1: Long Papers) . Association for Computational Linguistics, 1926-1940. DOI: - [57] Hila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Association for Computational Linguistics, 609-614. DOI: - [58] Jan Hajič, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Antònia Martí, Lluís Màrquez, Adam Meyers, Joakim Nivre, Sebastian Padó, Jan Štěpánek, Pavel Straňák, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task . Association for Computational Linguistics, 1-18. Retrieved from https://aclanthology.org/W09-1201.
- [59] Alon Y. Halevy, Peter Norvig, and Fernando Pereira. 2009. The unreasonable effectiveness of data. IEEE Intell. Syst. 24, 2 (2009), 8-12. DOI: - [60] Christian Hardmeier, Christine Basta, Marta R. Costa-jussà, Gabriel Stanovsky, and Hila Gonen (Eds.). 2022. In Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP) . Association for Computational Linguistics. Retrieved from https://aclanthology.org/2022.gebnlp-1.0.
- [61] Brienna Herold, James Waller, and Raja Kushalnagar. 2022. Applying the stereotype content model to assess disability bias in popular pre-trained NLP models underlying AI-based assistive technologies. In Proceedings of the 9th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT-2022) . Association for Computational Linguistics, 58-65. DOI: - [62] Daniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, and Anders Søgaard. 2022. Challenges and strategies in cross-cultural NLP. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 6997-7013. DOI: - [63] Dirk Hovy and Shrimai Prabhumoye. 2021. Five sources of bias in natural language processing. Lang. Ling. Compass 15, 8 (2021), e12432. DOI: - [64] Eduard Hovy, Roberto Navigli, and Simone Paolo Ponzetto. 2013. Collaboratively built semi-structured content and artificial intelligence: The story so far. Artif. Intell. 194 (2013), 2-27. DOI: - [65] Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 328-339. DOI: - [66] Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani Yogatama, and Pushmeet Kohli. 2020. Reducing sentiment bias in language models via counterfactual evaluation. In Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020 (Findings of ACL, Vol. EMNLP 2020) , Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 65-83. DOI: - [67] Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, and Stephen Denuyl. 2020. Social biases in NLP models as barriers for persons with disabilities. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, 5491-5501. DOI: https:// doi.org/10.18653/v1/2020.acl-main.487
- [68] Kris Inwood and Hamish Maxwell-Stewart. 2020. Selection bias and social science history. Soc. Sci. Hist. 44, 3 (2020), 411-416. DOI: - [69] Peter Izsak, Moshe Berchansky, and Omer Levy. 2021. How to train BERT with an academic budget. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 10644-10652. DOI: - [70] Abigail Z. Jacobs, Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna M. Wallach. 2020. The meaning and measurement of bias: Lessons from natural language processing. In FAT*'20: Conference on Fairness, Accountability, and Transparency, Barcelona, Spain, January 27-30, 2020 , Mireille Hildebrandt, Carlos Castillo, L. Elisa Celis, Salvatore Ruggieri, Linnet Taylor, and Gabriela Zanfir-Fortuna (Eds.). ACM, 706. DOI: - [71] Stefanie Johnson, Kenneth Podratz, Robert Dipboye, and Ellie Gibbons. 2010. Physical attractiveness biases in ratings of employment suitability: Tracking down the 'beauty is beastly' effect. J. Soc. Psychol. 150 (04 2010), 301-18. DOI: - [72] Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, 6282-6293. DOI: - [73] AdamKilgarriff and Gregory Grefenstette. 2003. Introduction to the special issue on the web as corpus. Comput. Ling. 29, 3 (2003), 333-348. DOI: - [74] Philipp Koehn. 2005. EuroParl: A parallel corpus for statistical machine translation. In Proceedings of Machine Translation Summit X: Papers . 79-86. Retrieved from https://aclanthology.org/2005.mtsummit-papers.11.
- [75] Adam J. Kruse. 2016. Cultural bias in testing: A review of literature and implications for music education. Update: Applic. Res. Music Educ. 35, 1 (2016), 23-31. DOI: - [76] Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan W. Black, and Yulia Tsvetkov. 2019. Measuring bias in contextualized word representations. In Proceedings of the 1st Workshop on Gender Bias in Natural Language Processing . Association for Computational Linguistics, 166-172. DOI: - [77] Jeff Larson, Surya Mattu, Lauren Kirchner, and Julia Angwin. 2016. How We Analyzed the COMPAS Recidivism Algorithm. Retrieved from https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm.
- [78] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, 7871-7880. DOI: - [79] Bill Yuchen Lin, Frank F. Xu, Kenny Zhu, and Seung-won Hwang. 2018. Mining cross-cultural differences and similarities in social media. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 709-719. DOI: - [80] Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, and Desmond Elliott. 2021. Visually grounded reasoning across languages and cultures. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 10467-10485. Retrieved from https: //aclanthology.org/2021.emnlp-main.818.
- [81] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2022. Pre-train, prompt, and predict: a systematic survey of prompting methods in natural language processing. ACM Comput. Surv. 55, 9 (2022), 1-35. - [82] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv abs/1907.11692 (2019).
- [83] Li Lucy and David Bamman. 2021. Gender and representation bias in GPT-3 generated stories. In Proceedings of the 3rd Workshop on Narrative Understanding . Association for Computational Linguistics, 48-55. DOI: https://doi.org/10. 18653/v1/2021.nuse-1.5
- [84] Catherine Macleod, Nancy Ide, and Ralph Grishman. 2000. The American national corpus: A standardized resource for American English. In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC'00) . European Language Resources Association (ELRA). Retrieved from http://www.lrec-conf.org/ proceedings/lrec2000/pdf/196.pdf.

- [85] Abelardo Carlos Martínez Lorenzo, Marco Maru, and Roberto Navigli. 2022. Fully-semantic parsing and generation: The BabelNet meaning representation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 1727-1741. DOI: https://doi.org/10. 18653/v1/2022.acl-long.121
- [86] Marco Maru, Simone Conia, Michele Bevilacqua, and Roberto Navigli. 2022. Nibbling at the hard core of word cense disambiguation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 4724-4737. DOI: https://doi.org/10.18653/v1/2022.acl-long. 324
- [87] Jonathan May and Jay Priyadarshi. 2017. SemEval-2017 task 9: Abstract meaning representation parsing and generation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval'17) . Association for Computational Linguistics, 536-545. DOI: - [88] Peter McGee. 2018. Endangered languages: The case of Irish Gaelic. Training, Language and Culture 2, 4 (2018), 26-38. DOI: - [89] George A. Miller, Claudia Leacock, Randee Tengi, and Ross T. Bunker. 1993. A semantic concordance. In Human Language Technology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993 . Retrieved from https://aclanthology.org/H93-1000.pdf.
- [90] Anshuman Mishra, Dhruvesh Patel, Aparna Vijayakumar, Xiang Lorraine Li, Pavan Kapanipathi, and Kartik Talamadupula. 2021. Looking beyond sentence-level natural language inference for question answering and text summarization. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics, 1322-1336. DOI: https: //doi.org/10.18653/v1/2021.naacl-main.104
- [91] Haley Moss. 2010. Screened out onscreen: Disability discrimination, hiring bias, and artificial intelligence. Denv. Law Rev. 98, 4 (04 2010), 301-18. DOI: - [92] Deepa Muralidhar. 2021. Examining religion bias in AI text generators. In AIES'21: AAAI/ACM Conference on AI, Ethics, and Society, Virtual Event, May 19-21, 2021 , Marion Fourcade, Benjamin Kuipers, Seth Lazar, and Deirdre K. Mulligan (Eds.). ACM, 273-274. DOI: - [93] Moin Nadeem, Anna Bethke, and Siva Reddy. 2021. StereoSet: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) . Association for Computational Linguistics, 5356-5371. DOI: - [94] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. WebGPT: Browser-assisted question-answering with human feedback. CoRR abs/2112.09332 (2021).
- [95] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020. CrowS-Pairs: A challenge dataset for measuring social biases in masked language models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics, 1953-1967. DOI: https://doi.org/10.18653/ v1/2020.emnlp-main.154
- [96] Roberto Navigli. 2009. Word sense disambiguation: A survey. ACMComput. Surv. 41, 2 (2009), 10:1-10:69. DOI: https: //doi.org/10.1145/1459352.1459355
- [97] Roberto Navigli, Michele Bevilacqua, Simone Conia, Dario Montagnini, and Francesco Cecconi. 2021. Ten years of BabelNet: A survey. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event/Montreal, Canada, 19-27 August 2021 , Zhi-Hua Zhou (Ed.). ijcai.org, 4559-4567. DOI: https://doi.org/10. 24963/ijcai.2021/620
- [98] Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artif. Intell. 193 (2012), 217-250. DOI: https://doi.org/10.1016/j. artint.2012.07.001
- [99] Michele Benedetto Neitz. 2013. Socioeconomic bias in the judiciary. Clevel. State Law Rev. 61 (2013), 137-165. Retrieved from https://ssrn.com/abstract=2149311.
- [100] Aurélie Névéol, Yoann Dupont, Julien Bezançon, and Karën Fort. 2022. French CrowS-pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 8521-8531. DOI: - [101] Neasa Ní Chiaráin, Oisín Nolan, Madeleine Comtois, Neimhin Robinson Gunning, Harald Berthelsen, and Ailbhe Ni Chasaide. 2022. Using speech and NLP resources to build an iCALL platform for a minority language, the story of An Scéalaí, the Irish experience to date. In Proceedings of the 5th Workshop on the Use of Computational Methods in the Study of Endangered Languages . Association for Computational Linguistics, 109-118. DOI: https://doi.org/10. 18653/v1/2022.computel-1.14

- [102] Debora Nozza, Federico Bianchi, Anne Lauscher, and Dirk Hovy. 2022. Measuring harmful sentence completion in language models for LGBTQIA+ individuals. In Proceedings of the 2nd Workshop on Language Technology for Equality, Diversity and Inclusion . Association for Computational Linguistics, 26-34. DOI: https://doi.org/10.18653/v1/2022. ltedi-1.4
- [103] Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 6464 (2019), 447-453. DOI: https://doi.org/10.1126/ science.aax2342
- [104] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL].
- [105] Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro Achille, Rishita Anubhai, Cícero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages. In Proceedings of the 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net. Retrieved from https://openreview.net/forum?id=US-TP-xnXI.
- [106] Yada Pruksachatkun, Anil Ramakrishna, Kai-Wei Chang, Satyapriya Krishna, Jwala Dhamala, Tanaya Guha, and Xiang Ren (Eds.). 2021. In Proceedings of the 1st Workshop on Trustworthy Natural Language Processing . Association for Computational Linguistics. Retrieved from https://aclanthology.org/2021.trustnlp-1.0.
- [107] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog 1, 8 (2019). Retrieved from https://d4mucfpksywv.cloudfront.net/ better-language-models/language-models.pdf.
- [108] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21 (2020), 140:1-140:67. Retrieved from http://jmlr.org/papers/v21/20-074.html.
- [109] Sebastian Ruder. 2021. Recent advances in language model fine-tuning. Retrieved from https://ruder.io/recentadvances-lm-fine-tuning/.
- [110] Sebastian Ruder. 2022. Scaling NLP systems to the next 1000 languages. Retrieved from https://www.2022.aclweb. org/invited-talks.
- [111] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé et al. 2022. BLOOM: A 176B-parameter open-access multilingual language model. arXiv Preprint arXiv:2211.05100 (2022).
- [112] Özge Sevgili, Artem Shelmanov, Mikhail Y. Arkhipov, Alexander Panchenko, and Chris Biemann. 2022. Neural entity linking: A survey of models based on deep learning. Semant. Web 13, 3 (2022), 527-570. DOI: https://doi.org/10.3233/ SW-222986
- [113] Ryan Steed and Aylin Caliskan. 2021. A set of distinct facial traits learned by machines is not predictive of appearance bias in the wild. AI Ethics 1, 3 (2021), 249-260. DOI: - [114] Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth Belding, Kai-Wei Chang, and William Yang Wang. 2019. Mitigating gender bias in natural language processing: Literature review. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, 1630-1640. DOI: - [115] Ernesto Tavoletti, Robert D. Stephens, Vas Taras, and Longzhu Dong. 2022. Nationality biases in peer evaluations: The country-of-origin effect in global virtual teams. Int. Bus. Rev. 31, 2 (2022), 101969. DOI: https://doi.org/10.1016/j. ibusrev.2021.101969
- [116] Simone Tedeschi, Johan Bos, Thierry Declerck, Jan Hajič, Daniel Hershcovich, Eduard H. Hovy, Alexander Koller, Simon Krek, Steven Schockaert, Rico Sennrich, Ekaterina Shutova, and Roberto Navigli. 2023. What's the meaning of superhuman performance in today's NLU? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics (to appear).
- [117] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. 2022. LaMDA: Language Models for Dialog Applications. CoRR abs/2201.08239 (2022).
- [118] Marcus Tomalin, Bill Byrne, Shauna Concannon, Danielle Saunders, and Stefanie Ullmann. 2021. The practical ethics of bias reduction in machine translation: Why domain adaptation is better than data debiasing. Ethics Inf. Technol. 23, 3 (2021), 419-433. DOI: - [119] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and efficient foundation language models. CoRR abs/2302.13971 (2023).
- [120] Eddie L. Ungless, Amy Rafferty, Hrichika Nag, and Björn Ross. 2022. A Robust Bias Mitigation Procedure Based on the Stereotype Content Model. In Proceedings of the Fifth Workshop on Natural Language Processing and Computational Social Science (NLP+CSS) . Association for Computational Linguistics, 207-217. DOI: https://doi.org/10.48550/ARXIV. 2210.14552
- [121] Starre Vartan. 2019. Racial bias found in a major health care risk algorithm. Retrieved from https://www. scientificamerican.com/article/racial-bias-found-in-a-major-health-care-risk-algorithm/.
- [122] Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, Ting-Hao K. Huang, and Shomir Wilson. 2023. Nationality bias in text generation. CoRR abs/2302.02463 (2023).
- [123] Xinyi Wang, Sebastian Ruder, and Graham Neubig. 2022. Expanding pretrained models to thousands more languages via lexicon-based adaptation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 863-877. DOI: https://doi.org/10.18653/v1/ 2022.acl-long.61
- [124] Wikipedia contributors. 2022. Who writes Wikipedia? Retrieved from https://en.wikipedia.org/wiki/Wikipedia: Who\_writes\_Wikipedia%3F.
- [125] Wikipedia contributors. 2022. Wikipedians. Retrieved from https://en.wikipedia.org/wiki/Wikipedia:Wikipedians.
- [126] Zheng Wu and Christoph M. Schimmele. 2021. Perceived religious discrimination and mental health. Ethn. Health 26, 7 (2021), 963-980.
- [127] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics, 483-498. DOI: - [128] Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. 2021. QA-GNN: Reasoning with language models and knowledge graphs for question answering. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics, 535-546. DOI: - [129] Wenpeng Yin, Dragomir Radev, and Caiming Xiong. 2021. DocNLI: A large-scale dataset for document-level natural language inference. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 . Association for Computational Linguistics, 4913-4922. DOI: - [130] Puxuan Yu, Hongliang Fei, and Ping Li. 2021. Cross-lingual language model pretraining for retrieval. In Proceedings of the Web Conference (WWW'21) . Association for Computing Machinery, New York, NY, 1029-1039. DOI: https: //doi.org/10.1145/3442381.3449830
- [131] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open pre-trained transformer language models. arXiv abs/2205.01068 (2022).
- [132] Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE International Conference on Computer Vision . IEEE Computer Society, 19-27. DOI: https: //doi.org/10.1109/ICCV.2015.11

Received 8 December 2022; accepted 2 February 2023