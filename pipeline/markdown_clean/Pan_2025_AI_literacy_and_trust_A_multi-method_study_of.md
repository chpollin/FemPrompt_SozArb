---
source_file: Pan_2025_AI_literacy_and_trust_A_multi-method_study_of.pdf
conversion_date: 2026-02-03T09:14:04.709579
converter: docling
quality_score: 95
---

<!-- image -->

## Computers in Human Behavior: Artificial Humans

## AI literacy and trust: A multi-method study of Human-GAI team collaboration

<!-- image -->

Zilong Pan

a,* , Ozias A. Moore b , Antigoni Papadimitriou b , Jiayan Zhu a

<!-- image -->

<!-- image -->

- a Department of Education and Human Services, College of Education, Lehigh University, 111 Research Drive, Bethlehem, 18015, PA, USA
- b Department of Management, College of Business, Lehigh University, 621 Taylor Street, Bethlehem, 18015, PA, USA

A R T I C L E  I N F O

Keywords: AI literacy AI teammate Generative AI Human-GAI collaboration Team-based projects

Trust

## 1. Introduction

The  use  of  artificial  intelligence  (AI)  technologies,  particularly generative AI (GAI) -such as large language models (LLMs), defined as systems  capable  of  generating  new  content  and  solutions  based  on training data (Noy &amp; Zhang, 2023) -has been rapidly increasing within organizations and teams (Pan &amp; Froese, 2023). GAI is now commonly referred to as a ' general-purpose technology ' and is increasingly integrated into organizational and team practices (Cockburn et al., 2019). This  integration  has  highlighted  the  need  for  effective  collaboration between  human  and  AI  systems  to  improve  individual,  team,  and organizational effectiveness (Wilson &amp; Daugherty, 2018). As a result, discussions about the future of work often emphasize the importance of human-GAI collaboration (Seeber et al., 2020; Wolf &amp; Stock-Homburg, 2023). To fully understand human-GAI collaboration, we first consider

* Corresponding author.

E-mail addresses: zip322@lehigh.edu (Z. Pan), oam216@lehigh.edu (O.A. Moore), antigoni.papadimitriou1@gmail.com (A. Papadimitriou), jiz621@lehigh.edu (J. Zhu).

## A B S T R A C T

As artificial intelligence (AI) becomes increasingly integrated into team settings for collaboration with humans, understanding the dynamics of trust and AI literacy is essential for enhancing team effectiveness. This study investigates the relationship between trust and AI literacy in human-generative AI (GAI) team collaboration, focusing on how AI literacy affects trust formation in these interactions. Drawing upon foundational teamwork literature and AI literacy frameworks, we conducted a multi-method investigation involving 116 undergraduate team members across 23 project teams throughout a semester. In Study 1, qualitative findings revealed distinct attitudes toward GAI as a teammate, categorized as trust, distrust, and ambivalence. Study 2 employed quantitative methods to determine predictors of trust in GAI, demonstrating that AI knowledge and perceived value -key components of AI literacy -significantly influenced perceptions of trust. Notably, perceptions of GAI accuracy emerged as a critical determinant of trust. Our findings highlight the complex interplay between AI literacy  and  trust  in  human-GAI  collaboration.  We  observed  a  paradox:  increased  AI  literacy  can  enhance collaboration but may also lead to hesitancy in future AI use. We contribute to advancing the understanding of human-AI collaboration by highlighting the critical role of AI literacy in shaping trust and socio-technical team dynamics. Our study provides evidence demonstrating the importance of targeted AI literacy development in building trust and fostering effective collaboration in human-GAI teams. These findings provide a foundation for research aimed at optimizing human-GAI teamwork and developing adaptive AI literacy frameworks, empowering individuals to effectively engage with AI across diverse collaborative settings.

the theoretical foundations established by research on human-human teamwork  across  disciplines  such  as  management,  psychology,  and organizational behavior (Kozlowski et al., 2015; Mathieu et al., 2017; Salas  et  al.,  2017,  2018)  and  communication  studies  (Poole &amp; Hollingshead, 2005). This extensive body of knowledge demonstrates that effective  teamwork  relies  on  fundamental  processes,  including  open communication, coordination, mutual support, and trust (e.g., Marks et al., 2001; Rousseau et al., 2006).

High levels of intrateam trust, in particular, are consistently associated with better team performance and cohesion (De Jong et al., 2016). Trust fosters psychological safety, enabling team members to share information and cooperate more freely (Jarvenpaa and Selander, 2023). Additionally, effective teamwork requires cognitive integration, coordinated action, and interpersonal dynamics, enabling team members to function cohesively. Building on this understanding of team processes,

<!-- image -->

<!-- image -->

Fiore  and  Wiltshire  (2016) describe  how  cognition  can  occur  across members, artifacts, and technology that support their efforts, illustrating the potential for technology to function as a teammate. As Fiore and Wiltshire (2016) note, technology can serve as more than just a tool -it can function as a teammate that contributes to team cognitive processes. This perspective is particularly relevant as we examine how GAI systems are perceived within team structures and how trust and literacy influence  these  perceptions.  Similarly,  Kozlowski  and  colleagues  (2015) differentiate  between  taskwork  (what  teams  do)  and  teamwork  processes (how they do it), emphasizing that effective teamwork processes require coordinated knowledge sharing, behavior regulation, and collective adaptability. Such  perspectives highlight how  individuals increasingly apply social norms and expectations to GAI, perceiving GAI as social actors (Demir et al., 2017; Guzman &amp; Lewis, 2019).

Applying these established principles to human-GAI teams requires acknowledging both similarities and distinct challenges posed by an AI teammate. On one hand, trust remains essential for the work of any team (Georganta &amp; Ulfert, 2024) -human or hybrid -and humans working with AI still need confidence in their teammate ' s reliability and integrity. On the other hand, unique challenges arise, as humans typically establish trust through shared experiences and interpersonal familiarity, resources often unavailable in interactions with GAI. Fiore and Wiltshire (2016) argue that technology can serve as a true teammate by contributing to collective cognitive processes, effectively blurring the boundary between human and machine in teamwork.

Furthermore, foundational research on human teams defines teamwork as the essential behaviors for working collectively, including understanding roles, managing  interdependencies, and engaging in effective  communication  patterns  (Mathieu  et  al.,  2017;  Salas  et  al., 2018). Effective teamwork relies on coordinating collective cognition, affect/motivation,  and  behavior  to  achieve  shared  goals.  Trust,  a cornerstone of human-human collaboration, is generally understood as the belief that another agent will act beneficially towards one ' s goals in situations  involving  uncertainty  and  vulnerability  (Lee &amp; See,  2004; Mayer et al., 1995; O ' Neill et al., 2022). Importantly, the interdependence between a computer ' s activities and those of a human can lead to the  perception  of  GAI  as  legitimate  teammates  (Dennis  et  al.,  2023; O ' Neill  et  al.,  2022;  Musick  et  al.,  2021),  thus  extending  traditional teamwork concepts into the human-GAI domain (Endsley, 2023).

While research on high-performance teams suggests that effective human-GAI  collaboration  is  enhanced  when  individuals  trust  GAI, comprehend  its  nature  and  purpose,  and  acquire  the  skills  to  use  it proficiently (Chowdhury et al., 2022), challenges remain. Specifically, human-GAI collaboration can lead to distrust and emotional challenges and acceptance issues (Bezrukova et al., 2023). These challenges can hinder potential collaboration and diminish the anticipated productivity benefits  (Tong  et  al.,  2021).  This  is  particularly  relevant  in  settings educational settings involving collaborative projects, where members must quickly establish working relationships and coordinate efforts on complex tasks, often with varying levels of technical proficiency and trust  in  novel  tools  like  GAI.  Understanding  how  trust  and  literacy develop in this specific environment is crucial for preparing individuals for  future  work  environments  where  human-GAI  collaboration  is increasingly common.

Such trust and literacy challenges are not only theoretical but have been observed in real-world team settings across diverse domains. In healthcare, clinicians may reject algorithmic recommendations if they do  not  trust  the  AI ' s  reasoning  or  accuracy -as  illustrated  by  IBM ' s Watson for Oncology, where many doctors ignored suggestions due to lack  of  trust  (Ross &amp; Swetlitz,  2018).  Ironically,  when  AI ' s  advice aligned with physicians ' own opinions, it was deemed redundant; when it conflicted, doctors perceived AI as incompetent, exacerbating distrust (Ross &amp; Swetlitz,  2018).  Similarly,  recent  surveys  of  K-12  teachers revealed skepticism about AI as a teaching partner, with only about 6 % believing AI tools have more benefits than drawbacks. This reluctance reflects significant trust deficits among educators, particularly among high school teachers, who exhibit more negative attitudes toward AI than elementary and middle school teachers (Lin, 2024). In domains like software development and finance, teams similarly struggle to calibrate their trust in AI-based decision support systems, balancing over-reliance and skepticism (Siau &amp; Wang, 2018). Together, these examples illustrate that  the  challenges  explored  in  this  study  are  not  just  theoretically relevant,  but  are  also  evident  in  the  everyday  organizational  and educational teams, underscoring the significance of our research.

Drawing on research from the field of information systems and team dynamics (e.g. Pereira et al., 2024; Seeber et al., 2020), we posit that interactions between GAI and team members -where humans and GAI collaborate interdependently towards a common goal -adhere to the same principles that govern human collaboration (O ' Neill et al., 2022). However, it is vital to recognize potential differences. Unlike human teams, where coordination emerges from mutual adjustment and implicit communication (Kozlowski et al., 2015), GAI requires explicit task scaffolding and bounded autonomy to align with team workflows (Fiore &amp; Wiltshire, 2016). This raises important questions about GAI ' s role as a teammate.

Yet, debate persists regarding GAI ' s capacity to function authentically as a teammate. Groom and Nass (2007) argue that genuine teamwork necessitates mutual understanding and shared mental models -capabilities that current AI systems have not fully developed. Similarly, Shneiderman (2020) emphasizes the importance of human control and oversight in human-AI collaboration, which raises questions about  whether  AI  can  truly  be  considered  a  traditional  teammate. Furthermore, while technology can support external cognition -offloading  memory  demands or  scaffolding  complex  team  processes -this fundamentally differs from the nuanced social interactions and trust-building inherent in human teams.

Thus, it is crucial to understand that trust in AI, including GAI, is not solely a matter of user perception or acceptance; it fundamentally depends on the inherent qualities and capabilities of the AI system. The National Institute of Standard and Technology (2023) AI Risk Management Framework highlights key attributes contributing to AI trustworthiness,  such as explainability, accuracy, reliability, and fairness -all critical factors for positioning AI as a trustworthy collaborator. Despite these complexities and ongoing debates, as teams increasingly integrate GAI  into  distinct  roles,  the  perceptions  of  human  team  members regarding GAI as a teammate becomes crucial (O ' Neill et al., 2022). This shift  necessitates  a  deeper  exploration  of  GAI ' s  role  as  a  teammate. Unlike traditional human teammates, GAI excels in processing vast data, generating  human-like  responses,  and  adapting  to  diverse  contexts. However, it lacks essential human traits such as emotional intelligence, ethical reasoning, and the ability to form genuine social connections.

Furthermore,  the  rapid  evolution  of  GAI  tools  and  technologies creates challenges for individuals as they attempt to effectively navigate and understand their role within collaborative team contexts. Consequently, continuous skill development is required to proficiently use GAI tools (Chowdhury et al., 2022). Understanding AI literacy is therefore fundamental to examining human-AI interactions, particularly collaborative settings. Long and Magerko (2020) define AI literacy as understanding basic AI concepts, recognizing its practical applications in daily life,  and  evaluating  its  social  impacts.  This  multifaceted  concept  encompasses diverse skills and knowledge that vary across user contexts, such as recognizing the advantages of employing AI for specific tasks and critically assessing its limitations and potential concerns.

Given the complex and multifaceted nature of AI literacy, varying levels of AI literacy can significantly influence how individuals perceive AI ' s collaborative role. Individuals with higher AI literacy may view GAI as a valuable teammate, while others with lower literacy may remain skeptical or resistant. Such distrust may stem from concerns regarding job  displacement,  ethical  considerations,  or  doubts  regarding  GAI ' s reliability and decision-making capabilities. Consequently, skepticism toward  GAI  may  negatively  affect  individual  and  team  outcomes, diminishing collaboration and commitment (Brougham &amp; Haar, 2017).

Our research empirically explores the factors influencing perceptions of GAI as a teammate, particularly the role of AI literacy and trust. In doing so, we aim to bridge theoretical discussions on AI teamwork and practical challenges by examining how  AI literacy -specifically, knowledge, perceived value, and concerns -shapes trust in GAI. This work builds upon a socio-technical systems perspective that recognizes the complex interdependence between technical capabilities and social dynamics in collaborative settings (Fiore &amp; Wiltshire, 2016; Jarvenpaa &amp; Selander, 2023).

Moreover,  while  existing  research  indicates  that  individuals ' understanding of and interaction with AI significantly influences their trust in GAI and the effectiveness of human-GAI collaboration (Gillespie et al., 2023), several critical questions remain: Which skills or factors facilitate effective  human-GAI collaboration? What drives individuals ' willingness to collaborate with GAI? Our study addresses these questions by examining the interplay between team dynamics, AI literacy, and individual  perceptions  of  GAI  as  a  teammate.  Various  factors,  including anthropomorphism, autonomy, proactivity, reactivity, and adaptability, also significantly impact human-GAI collaboration (Glikson &amp; Woolley, 2020; Pelau et al., 2021). In this study, we specifically focus on team members ' trust perceptions of GAI as a teammate and the role of AI literacy in fostering this trust. Consequently, we seek to advance the understanding of these perceptions and skills essential for facilitating effective human-GAI collaboration.

This study makes three meaningful contributions to research on AI literacy and trust in human-GAI collaboration. First, we highlight the importance of individuals ' perceptions of AI and how these perceptions shape  trust  in  the  collaborative  process.  We  identify  key  attitudinal factors that influence human-GAI collaboration, including trust beliefs and  emotional  responses  to  working  with  AI  systems.  Our  findings illustrate how these elements contribute to both individual engagement and team dynamics when working with GAI. Additionally, we provide insights into the cognitive processes, such as AI literacy, that influence the relationship between AI and trust in collaboration. Our findings also demonstrate the varied facets of AI literacy and their differential impact on trust, thus deepening our understanding of the factors that foster or hinder team members ' trust perceptions in human-GAI collaboration.

Second, by anchoring our study in a comprehensive understanding of AI  literacy  (Pinski &amp; Benlian,  2024),  we  aim  to  contribute  to  the expanding body of knowledge on human-GAI collaboration. We explore how AI literacy influences perceptions of GAI as a collaborative partner, focusing  on  the  complex  interplay  between  AI  literacy,  trust,  and teamwork  dynamics  within  organizational  and  educational  contexts. Our  findings  suggest  that  tasks  facilitated  by  GAI  can  cultivate  new skills,  particularly  in  AI  literacy,  enhancing  individuals ' experiences with  AI  or  algorithmic  team  management.  Furthermore,  our  results highlight the subtle contextual cues and skills that foster trust development and elevate team members ' trust in GAI collaboration. Finally, this research advances theoretical understanding of the critical factors that shape trust and collaboration with AI, contributing to the growing body of theory testing, development, and research on this topic (e.g., Balakrishnan et al., 2022; Bezrukova et al., 2023; Castelo et al., 2019; Longoni et al., 2019; Pinski &amp; Benlian, 2024; Seeber et al., 2020). In particular,  our  study  seeks  to  advance  socio-technical  systems  (STS) theory by examining the complex interdependence between technical capabilities and social dynamics in human-GAI collaboration contexts (Jarvenpaa &amp; Selander, 2023).

To test our model (as shown in Fig. 1) and investigate our research hypotheses,  we  employ  a  mixed-methods  approach  comprising  two studies: a qualitative exploration of trust perceptions of GAI as a teammate (Study 1), followed by an empirical investigation testing hypotheses  regarding  the  effects  of  AI  literacy  and  trust  on  human-GAI collaboration (Study 2). Specifically, we examine how three facets of AI literacy -knowledge, value, and concerns -affect trust perceptions in human-GAI collaboration. Theoretically, our work is grounded in Mayer and colleagues ' (1995) model of trust, which provides a foundation for understanding the factors influencing the perceived trustworthiness of GAI as a team member compared to human teammates. Building on this foundation, we incorporate O ' Neill and colleagues ' (2022) recommendation  for  an  adaptable  model  of  human-GAI  interactions  within  a sociotechnical  framework.  Our  interdisciplinary  approach  integrates insights from  multiple fields,  including  human-GAI  collaboration (Bezrukova et al., 2023), advice-taking (Sniezek &amp; Van Swol, 2001), behavioral  strategy  (Powell  et  al.,  2011),  and  trust  in  automation (Glikson &amp; Woolley, 2020).

## 1.1. Theoretical framework

The  integration  of  GAI  in  teams  has  increased  the  need  for  skill development, particularly in AI literacy (Verma &amp; Singh, 2022). AI literacy, broadly defined as the capacity to comprehend fundamental AI concepts, recognize everyday AI applications, and assess their societal impact (Long &amp; Magerko, 2020), has emerged as a critical competency for  effective  human-GAI  collaboration.  However,  AI  literacy  is  not  a uniform construct but rather a dynamic set of skills and knowledge that varies across different AI user groups (Benlian et al., 2022; Meske &amp; Bunde, 2020). The conceptualization and measurement of AI literacy have been advanced through various frameworks, each with distinct strengths and limitations.

In their comprehensive review, Pinski and Benlian (2024) identify three primary research avenues in AI literacy: learning methods, constituent components, and resultant effects. This multifaceted approach highlights the complexity of AI literacy and underscores the need for a nuanced understanding tailored to different user groups and contexts. For instance, while Long and Magerko (2020) focus on general AI literacy components, other frameworks, such as those by Ng et al. (2021), emphasize  specific  skills  and  knowledge  areas  relevant  to  AI  user groups.

Fig. 1. Conceptual model.

<!-- image -->

We adopt the AI literacy framework developed by Long and Magerko (2020) , which defines AI literacy as the ability to understand fundamental AI concepts, recognize its everyday applications and value in supporting human activities, and assess its societal impact, including benefits  and  concerns.  We  have  selected  this  framework  for  several reasons.  First,  it  provides  a  clear  and  accessible  set  of  components directly relevant to individuals collaborating with GAI in educational settings. Second, it emphasizes both the cognitive understanding of AI technologies and the critical evaluation of their implications, aligning with our focus on how AI literacy enables individuals to make informed decisions, engage effectively in GAI teamwork, and adapt to advancements driven by GAI (Wagner, 2021). Third, this framework addresses the  AI-specific  facets -such  as  autonomy,  inscrutability,  and  learning -that  are  essential  for  distinguishing  AI  literacy  from  general digital literacy (Berente et al., 2021).

By explicitly adopting the Long and Magerko (2020) framework, we aim to provide a well-defined conceptualization of AI literacy within our study ' s organizational context, where AI literacy plays a crucial role in enabling individuals to make informed decisions, engage effectively in GAI teamwork, and adapt to AI-driven advancements (Wagner, 2021).

## 1.2. Literature review and hypotheses development

## 1.2.1. Understanding perceptions of trust for Human-GAI collaboration

Human-GAI  collaboration  has  been  recognized  for  its  ability  to reduce uncertainty (Agrawal et al., 2019), and advancements in AI capabilities offer transformative solutions to the complex, fluid, and dynamic  nature  of  teamwork  (Bankins  et  al.,  2024).  An  effective AI-augmented  strategic  decision-making  process  requires  behavioral integration between machines and humans, allowing them to complement  each  other ' s  strengths  (Davenport,  2016;  Raisch &amp; Krakowski, 2021). Individuals engaged in team activities evaluate and interact with human  and  AI  group  members  differently  (Tomprou &amp; Lee,  2022), influenced by factors such as AI attributes, trust perceptions, and the impact of AI on tasks and team composition.

Trust plays a pivotal role in determining the effectiveness of humanGAI collaboration, as highlighted in the trust and automation literature (Glikson &amp; Woolley, 2020). To fully realize the productivity and efficiency benefits of AI, deliberate and functional collaboration between humans and technology is necessary. For example, the Integrated AI Acceptance-Avoidance  Model  by  Cao  et  al.  (2021) suggests  that  individuals  hold  mixed  perspectives  on  AI,  weighing  its  benefits  and drawbacks. Research utilizing Technology Acceptance and Technology-Organization-Environment Models to examine worker attitudes towards AI (e.g., Baabdullah et al., 2021) indicates that compatibility with existing organizational practices and technologies bolsters AI acceptance through perceived usefulness.

Chatterjee, Chaudhuri, and Vrontis (2021) identified that trust and perceived ease of use are critical factors in improving workers ' attitudes towards AI and their intention to collaborate with it. Trust also plays a key role in the judge-advisor system (JAS) paradigm (Bonaccio &amp; Dalal, 2006; Goodwin et al., 2013; Sniezek &amp; Van Swol, 2001), serving as a crucial antecedent for advice adoption and collaboration. Therefore, we posit that trust is central to understanding the relationship between AI and  human-GAI  collaboration.  Unlike  human-to-human  interaction, trust in human-GAI collaboration emerges differently; individuals may initially assume that AI is flawless, leading to a ' positivity bias ' (Glikson &amp; Woolley, 2020).

Additional  studies  have  shown  that  task  characteristics  (Castelo et al., 2019) and process structure (Lee et al., 2019) are critical for trust development. Team members who perceive a good task-technology fit, such  as  AI  performing  repetitive  tasks  (Sowa  et  al.,  2021),  being specialized  for  a  task  (Sowa &amp; Przegalinska,  2020),  enhancing  their decision-making (Kawaguchi, 2021), and having high system quality (Nguyen &amp; Malik, 2022), are more likely to value the technology, use it effectively, and achieve better performance outcomes.

Current empirical practices aimed at defining and evaluating trust in AI (e.g., Chiou &amp; Lee, 2021; Mayer et al., 1995) have produced mixed findings. Hoff and Bashir (2015) conceptualize trust as a multidimensional  construct  encompassing  dispositional,  situational,  and  learned components, which has led to an ongoing theoretical debate about the nature of ' trust in AI ' and its implications (e.g., Ferrario, 2021; Lewis &amp; Marsh,  2022).  This  perspective  challenges  the  idea  that  individuals naturally trust AI systems from the outset, instead highlighting that trust in AI is shaped by various factors, such as individual predispositions, past  experiences,  and  the  specific  context  of  interaction.  Schoorman et al. (2007) further emphasize that trust is a dynamic process evolving over time as individuals gain more experience with AI systems.

While existing studies have predominantly focused on negative, fearbased, and threat-focused employee attitudes towards AI -which can hinder AI adoption (Bankins et al., 2024) -a more comprehensive understanding of trust dynamics in human-GAI collaborations is needed. It is  crucial  to  recognize  that  trust  formation  and  maintenance  in human-GAI teams is not predetermined but rather results from a complex interplay of human perceptions and AI performance. To address these critical issues and contribute to the growing body of knowledge on human-GAI collaboration, we formulate the following research question for Study 1:

Research Question: What are team members ' perceptions of trust in GAI as a teammate during collaborative projects?

Addressing this question , Study 1 employs a qualitative approach to explore perceptions of trust in human-GAI collaboration within realworld interactions, identifying the diverse socio-technical drivers and inhibitors of such perceptions. This study ' s findings will inform theoretical development and empirical testing in Study 2. By examining trust dynamics in human-GAI teamwork, we aim to provide a nuanced understanding that moves beyond binary views of trust or distrust in GAI as a teammate.

## 1.2.2. AI literacy and trust of GAI as a teammate

The  importance  of  AI-relevant  skills,  particularly  AI  literacy,  has been increasingly recognized among practitioners and researchers alike. AI literacy involves a comprehensive understanding of fundamental AI concepts, the ability to recognize everyday applications of AI, and the capacity  to  evaluate  their  societal  impact  (Long &amp; Magerko,  2020). Simply believing that GAI will improve team collaboration is insufficient for its adoption; workers must also perceive the technology as easy to understand  and  use  with  minimal  effort  (Chatterjee  et  al.,  2021a, 2021b). Additionally, the use of GAI increases the need for skill development among workers (Verma &amp; Singh, 2022). Jaiswal et al. (2021) identified five key areas for AI literacy: data analytic skills, digital skills, complex cognitive skills, decision-making skills, and continuous learning skills.

As emphasized by Pinski and Benlian (2024), understanding the effects of AI literacy is crucial.  Research  suggests  that  improved human-GAI collaboration occurs when individuals trust AI, comprehend its nature and purpose, and have the skills to use it effectively (Bankins et al., 2024; Chowdhury et al., 2022). However, prior studies also show that while AI literacy can enhance human-AI collaboration performance, it may reduce future AI use intentions (Pinski et al., 2023). This paradox highlights the need to examine the multifaceted impacts of AI literacy on trust, teamwork dynamics, and attitudes towards GAI in collaborative settings.

Various frameworks have been proposed for AI literacy, focusing on different aspects. Some emphasize learning methods, advocating formal and informal educational initiatives  tailored  to  different  user  groups (Long et al., 2021), while others focus on the components of AI literacy, such  as  individual  AI  skills  and  specific  knowledge  areas  (Ng  et  al., 2021). Such approaches underscore the need to establish a repository of AI literacy components relevant to diverse user groups and teams. For example, software developers working on AI-driven applications may require in-depth technical knowledge of machine learning algorithms,

marketing professionals may need to understand AI ' s role in customer segmentation and predictive analytics, and teachers may need to grasp AI ' s  role  in  enhancing learning experiences and adapting content for diverse student needs (Jorzik et al., 2023; Kim &amp; Kwon, 2023).

A third area of research examines the effects of AI literacy, investigating how it influences outcomes such as performance in human-AI collaboration and attitudes toward future AI use (Pinski et al., 2023). The varying frameworks used in AI literacy research reveal a fragmented landscape  and  the  need  for  a  comprehensive  understanding  that  addresses  the  specificities  of  AI  technologies  (Pinski &amp; Benlian,  2024). Some frameworks fail to capture AI-specific aspects, often conflating AI literacy with general digital literacy. As AI use expands across diverse domains, it is evident that a one-size-fits-all approach to AI literacy is inadequate  (Benlian  et  al.,  2022;  Meske &amp; Bunde,  2020).  Therefore, adopting  an  AI  literacy  framework  that  is  both  comprehensive  and tailored to the specific context is imperative.

In our study, as previously discussed, we adopt the framework proposed by Pinski and Benlian (2024) and Long and Magerko (2020)), which  conceptualizes  AI  literacy  as  a  multifaceted  construct  encompassing  various  competencies.  These  include  understanding  fundamental AI concepts, recognizing AI ' s applications in everyday contexts, and  critically  evaluating  its  societal  impacts.  This  perspective  aligns with Laupichler et al. (2022), who describe an AI-literate individual as one  capable  of  comprehending,  utilizing,  monitoring,  and  critically reflecting on AI applications. These definitions are particularly relevant in  the  context  of  employing  GAI  to  support  collaboration,  given  its pivotal  role  within  the  broader  spectrum  of  AI.  With  the  growing availability of GAI-integrated educational tools, it has become imperative for learners to acquire foundational knowledge about GAI and understand its potential benefits and limitations in facilitating educational objectives.

The knowledge  facet of  AI  literacy  involves  an  understanding  of fundamental  AI  concepts,  including  algorithmic  models  that  execute cognitive  or  perceptual  tasks  traditionally  associated  with  human thinking, judgment, and reasoning (Leslie et al., 2021). However, being AI-literate does not necessarily mean developing AI models; rather, it requires individuals to interact effectively with AI applications, such as prompting  GAI  tools appropriately to achieve desired  outcomes (Laupichler et al., 2022; Oppenlender et al., 2023). Researchers, therefore, propose extending AI education beyond computer science majors to individuals from diverse fields of study (Kong et al., 2021).

The value facet of AI literacy emphasizes an understanding of the benefits of GAI and proficiency in leveraging its capabilities effectively (Ng et al., 2021). For example, when team members understand the capabilities of GAI models like ChatGPT, particularly their strength in text generation, they can significantly amplify productivity in tasks such as idea generation, crafting arguments, and proofreading -ultimately enhancing project outcomes.

The concern  facet of  AI  literacy  addresses  the  ability  to  critically analyze and reflect on AI-generated responses (Laupichler et al., 2022; Ng et al., 2021). This includes actively monitoring AI applications for optimal performance, identifying irregularities, and upholding ethical standards. It also requires continuous evaluation of AI systems ' accuracy,  reliability,  and  fairness  to  mitigate  potential  biases.  Moreover, AI-literate individuals can critically reflect on broader societal impacts, ethical considerations, and long-term ramifications of AI use, including privacy concerns and inherent biases.

In  summary,  AI  literacy  encompasses  understanding  the  facets  of knowledge, value, and concern; effectively interacting with AI applications; and recognizing their benefits and limitations. Proficient learners understand practical AI implications, use AI to enhance productivity, and critically evaluate AI responses. Investing in these facets of AI literacy  is  essential  for  fostering  trust  and  effective  collaboration  in human-GAI partnerships, ultimately enabling the development of strategies to strengthen human-GAI collaboration.

Building on the preliminary insights from Study 1, which highlighted how varying levels of understanding and critical reflection may influence trust in GAI, we designed Study 2 to examine these relationships more systematically through a quantitative approach.

In Study 2, our first hypothesis suggests that AI literacy -including knowledge,  values,  and  concerns  regarding  AI -will  significantly  influence  users ' trust  in  GAI  as  a  teammate,  thereby  validating  its importance in team-based interactions involving human-GAI collaboration. Thus, we hypothesize the following.

Hypothesis 1 . Team members ' AI  literacy  (i.e.,  knowledge,  values, and concerns regarding AI) will influence their perceptions of trust in GAI as a teammate during collaborative team projects.

Building on this, we further examine how trust in GAI interacts with AI literacy to shape team members ' perceptions of GAI ' s effectiveness as a collaborative teammate. Specifically, we explore whether these factors jointly influence users ' evaluations of GAI ' s contribution to team dynamics and overall team effectiveness.

Hypothesis 2 . Team members ' AI  literacy  (i.e.,  knowledge,  values, and  concerns  regarding  AI)  and  their  trust  in  GAI  will  predict  their perceptions of GAI ' s effectiveness as a teammate during collaborative team projects.

## 2. Methods

In this section, we describe our methodological approach. We first present a comprehensive overview of the research design, detailing the study  context,  sample  characteristics,  human-GAI  team  project  task, procedural steps, and measures employed. We then outline our analytical  framework.  This  research  received  approval  from  the  authors ' Institutional  Review  Board  (IRB),  ensuring  compliance  with  ethical standards for human subjects research. Electronic consent forms were distributed to all potential participants, with data collection limited to those providing explicit consent.

## 2.1. Participants

Both Study 1 and Study 2 involved the same group of 122 undergraduate students enrolled in an Organizational Behavior course at a private university in the Northeastern United States. Participants were primarily first-year students majoring in Business or pursuing a dual major  in  Engineering  and  Business.  Demographic  data  indicate  the following distribution among participants: 38 % identified as female and 62% as male. In terms of race and ethnicity, American Indian or Alaskan Native students comprised 0.1% of the sample; Asian students made up 8.9%;  Black  or  African  American  students  accounted  for  3.3%;  and Hispanic students of any race represented 7.7%. Students identifying as two or more races constituted 3.0%, while those with race and ethnicity unknown made up 2.6%. U.S. Non-Residents represented 10.7%, and White students comprised 63.9% of the sample.

Of  the  122  students  invited,  116  consented  to  participate  in  the study. These students represented the entire enrollment for the course section  in  which  the  study  was  conducted.  Given  this  context,  we employed  a  convenience  sampling  approach,  selecting  participants based on their enrollment in the course where the instructional intervention and data collection occurred. The sample size represents nearly the full population of students exposed to the intervention during the semester, allowing for meaningful analysis within this context.

The course spanned the full semester (15 weeks), with classes being held twice a week for 75 min per session. As a core requirement in the College of Business curriculum, the course focused on the study of individual  and  team  behavior  in  organizational  settings.  This  context provides  an  ideal  environment  for  examining  perceptions  of  GAI  in collaborative settings, as students engage in both theoretical learning and applied team-based projects throughout the semester.

## 2.2. Research context and Human-GAI collaborative project framework

During the first two weeks of the semester, faculty introduced the objectives  of  the  Organizational  Behavior  course,  emphasized  the importance of teamwork for several assignments, and informed students that they were permitted and encouraged, but not required, to use GAI as a teammate in their group projects. Faculty across all Organizational Behavior course sections also introduced ChatGPT-3.5 (OpenAI, 2023) due to its free accessibility and user-friendly interface, which required minimal onboarding for effective use. Moreover, as part of this introductory module, students were given the opportunity to learn how to engage effectively with ChatGPT-3.5. Faculty provided a brief overview of the tool, covering its core functions and offering guidance on how to formulate  effective  prompts.  Students  were  then  invited  to  practice interacting  with  ChatGPT-3.5  by  completing  short,  10 -15-min  tasks related to Organizational Behavior topics.

By the conclusion of the two-week add/drop period, students were randomly assigned to 23 teams using a random team generator tool. Each team consisted of 4 -6 members who collaborated throughout the entire 15-week semester. During this period, teams engaged in a series of case-based  projects  strategically  connected  to  their  final  deliverable. These  smaller,  iterative  projects  served  as  building  blocks,  enabling teams to progressively deepen their understanding and develop the skills necessary for the culminating final project -due in the last week of the semester -which included both a team presentation and a comprehensive written report. Throughout these iterative projects, teams actively interacted with GAI as a teammate, using it to generate ideas, refine analyses, and support decision-making processes. For the final project, teams  analyzed  an  organization  of  their  choice  using  best  practices outlined  in  an  assigned  article.  They  conducted  in-depth  research through company websites, supplementary sources (e.g., news, journals, social media, and recruitment sites), and ideally, interviews with employees. Each team presented their findings in a 20-min presentation and submitted a 10 -12-page report including references. Throughout the semester, teams were encouraged to interact with the ChatGPT-3.5 as a 'teammate ' to assist with their projects. They were instructed to communicate  with  GAI  as  they  would  with  any  other  teammate. Recognizing that trust is a multifaceted and dynamic construct (Knowles et al., 2022), we chose not to impose strict guidelines on how students should engage with GAI. Instead, we encouraged teams to exercise autonomy in determining how best to incorporate GAI into their project work based on their perceptions of trust and the perceived value of GAI ' s contributions.  To  minimize  potential  demand  effects,  students  were explicitly  informed  that  their  level  of  interaction  with  ChatGPT-3.5 would not influence their grades or evaluation, and no incentives were provided based on frequency or intensity of AI use. Overall, this flexible approach was intended to facilitate an authentic exploration of students ' trust in GAI, their perceptions of GAI as a teammate, and the role of AI literacy in shaping these perceptions.

## 2.3. Data collection

During the last three weeks of the semester (weeks 13 -15),  each team member completed an anonymous survey regarding their AI literacy, trust in GAI during human-GAI interaction, and their perceptions of GAI as a teammate in team projects. All 116 students provided their consent prior to participating in the survey. These students completed their surveys across all 23 teams, achieving a 100 % response rate.

## 2.4. Study 1: perceptions of trust in GAI during Human-GAI projects processes

To investigate how individuals perceive and develop trust in GAI within collaborative project contexts, we designed a qualitative study focusing on participants ' authentic experiences. In the following sections, we detail our analytical approach to coding participant responses, describe our measures for capturing trust perceptions, and present the emergent  themes  that  characterize  human-GAI  trust  dynamics  in collaborative settings.

## 2.5. Analytical approach

To capture the multifaceted nature of trust in GAI during the humanGAI project process, we applied an inductive coding approach (Azungah, 2018).  Using  a  constant  comparative  method  (Creswell &amp; Creswell, 2017), two researchers independently categorized data into emergent themes reflecting diverse perceptions of trust in GAI. The coding team meets biweekly to discuss coding decisions, refine the codebook, and ensure consistency. The iterative reconciliation process continued until the complete intercoder agreement (100 % consensus) was achieved on all thematic categories, ensuring analytical rigor and trustworthiness.

## 2.6. Measures

Participants were provided with two related open-ended questions: ' Do you trust generative AI technologies (GAI), such as ChatGPT, as a teammate  in  a  group  project?  Why  or  why  not? ' The  first  question assessed  participants ' trust  stance,  while  the  second  invited  them  to explain their reasoning. To minimize demand effects, these questions were carefully worded with neutral phrasing and placed at the conclusion of the survey, separate from course performance metrics. Participants  were  explicitly  informed  that  their  responses  would  remain confidential  and  would  not  influence  their  course  evaluation.  This approach enabled us to gather authentic perspectives on participants ' trust in GAI as a teammate.

## 2.7. Results

Qualitative analysis revealed that participants reflections on their trust could be coded into three categories: trust (52 %), distrust (26 %), and ambivalence (22 %) (see Fig. 2). We recognized that responses may contain both reasons to trust and to be cautious about GAI, reflecting the complexity of human-AI interactions. Thus, rather than labeling these as ' neutral, ' we categorized them as ' ambivalence, ' acknowledging that trust  in  GAI  can  be  situational  and  multidimensional.  This  refined approach allowed us to capture nuanced discussions of both advantages and disadvantages of using GAI, providing a more accurate representation of participants ' perceptions. In doing so, with an aim to enrich understanding  of  trust  in  GAI  beyond  simple  binary  classifications, accurately reflecting the complex nature of trust in human-AI collaborative contexts.

Fig. 2. Distribution on Participants ' trust in GAI as a Teammate.

<!-- image -->

The primary rationale expressed by participants who trusted GAI as a teammate was its potential assistance in information generation (see Table 1). One participant mentioned, ' I think it is a very good source to come up with preliminary material, edit existing content, or help you brainstorm ideas. ' Another participant stated, ' I trust generative AI to be a teammate throughout this class because it can help explain difficult concepts so that I can have a clearer understanding. The explanations provided by these programs can give might be clearer than those of my peers, making them particularly useful when concepts are confusing. ' Participants ' who expressed trust also indicated the importance of factchecking GAI responses. As one participant stated, ' We trust it  with basic  questions;  however,  we  know  that  sometimes  it  can  provide inconsistent or untrue information, so we will always review responses before submitting them. ' Such representative quotes revealed participants ' critical and reflective stance, even among those who trusted in GAI.

Participants who distrusted GAI primarily cited concerns regarding inaccurate  information.  For  instance,  one  participant  reflected, ' Not really, because sometimes it just provides you with wrong information, which can be frustrating, ' while another said, ' I don ' t trust all the information it provides but, I think ChatGPT is a good starting point for general  knowledge  and  understanding. ' Thus,  participants  who  distrusted GAI often expressed skepticism toward either the information source or the algorithm producing the information.

Participants exhibiting a more skeptical appraisal towards GAI were categorized under the ambivalence; they acknowledged GAI ' s merit in information retrieval but also noted its limitations in credibility. As one participant stated, ' I think it is a great tool if you need information as long as it is not your only source of information because you do not know where ChatGPT is getting this information from, and it may have bias. ' Another echoed this sentiment, stating, ' I trust it sometimes, but other times I do not, because it can get stuff wrong, and I do not know where GAI obtains its information. '

In summary, after working with GAI as a teammate for a semester, slightly over half of the participants (52 %) expressed trust GAI as a teammate;  however,  this  trust  typically  involved  conditions  such  as verifying the accuracy of GAI ' s responses. Among participants who reported  distrust  or  ambivalence,  concerns  regarding  inaccurate  information emerged prominently.

These findings offer valuable insights into the integration of GAI in educational contexts and have broader implications for understanding human-AI  collaboration  in  organizational  settings.  This  qualitative exploration  represents  a  significant  step  towards  developing  a  more comprehensive and nuanced understanding of AI ' s role in teamwork and collaborative problem-solving.

Table 1 Illustrative emergent themes participants ' trust in GAI as a teammate (Study 1).

| Theme       | Select Example Quotes                                                                                                                                                                                                                                                                                                                            |
|-------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Trust       | 1. ' Yes, it helps to do more qualitative research and plan the work. ' 2. ' I would say that ChatGPT is very trustworthy as the data it gives seems to be mostly accurate and always has relevant information regarding my questions. ' 3. ' Yes, you can and should always double check the facts but it is right far more than it is wrong. ' |
| Distrust    | 1. ' No, because their sources are not always correct, meaning the information it gives us is not accurate. ' 2. ' Not really because sometimes it just provides you with wrong information which can be frustrating. ' 3. ' I do not because you do not know where ChatGPT ' s information is taken from. '                                     |
| Ambivalence | 1. ' It depends on the subject as it can have bias. ' 2. ' I somewhat trust the content that is sent out, but I would need to fact check the content because it sometimes quotes sources, but those sources aren ' t real. ' 3. ' Somewhat. I understand that sometimes the information provided can be biased or inaccurate. '                  |

Consequently, informed by the nuanced insights gained in Study 1, we conducted Study 2 to quantitatively test hypotheses derived from emergent qualitative themes. By employing validated instruments and rigorous statistical analysis, Study 2 aims to empirically measure the relationships  among identified  variables,  trust  and  collaborative  outcomes. This approach complements the qualitative findings from Study 1 and advances existing literature by providing evidence-based insights into the dynamics of human-GAI collaboration. Furthermore, by quantitatively  assessing  the  impact  of  these  factors  on  trust,  we  enhance understanding of the mechanisms through which trust in GAI develops and ultimately influences team performance.

## 2.8. Study 2: AI literacy, trust, and GAI perceptions as a teammate

To extend our examination of trust in GAI within collaborative settings, we conducted a quantitative analysis exploring the relationships between AI literacy, trust formation, and teammate perceptions. The following  sections  outline  our  analytical  framework,  detail  our  measurement instruments, and present empirical findings that illuminate these interconnections.

## 2.9. Analytical approach

Hypothesis 1 (AI literacy predicting trust in GAI) was tested using a multinomial  logistic  regression  model,  with  categorical  trust  themes identified  in  Study  1  (trust,  distrust,  ambivalence)  as  the  dependent variable. Independent  variables were  the three AI literacy subconstructs:  knowledge,  perceived  value,  and  concerns.  This  analysis empirically  examined  how  different  aspects  of  AI  literacy  predicted participants ' categorical  trust  in  GAI.  Multinomial  logistic  regression was selected because the dependent variable is categorical with multiple,  unordered  levels,  thus  violating  assumptions  of  ordinary  least squares  regression.  Analyses  were  conducted  using  SPSS  version  29 (IBM Corp, 2024), with maximum likelihood estimation.

For Hypothesis 2 (AI literacy and trust predicting perceptions of GAI as  a  teammate),  we  conducted  a  linear  regression  analysis  with  perceptions of GAI as a teammate as the dependent variable. Independent variables  included  the  three  AI  literacy  sub-constructs:  knowledge, perceived  value,  concerns)  and  trust  in  GAI  (dummy-coded,  with ' distrust ' as the baseline). This regression tested whether AI literacy and trust  significantly  predicted  participants ' perceptions  of  GAI  as  an effective teammate in a collaborative team setting.

## 2.10. Measures

AI Literacy. We adapted Chan and Hu ' s (2023) measure of AI literacy, which consists of three sub-constructs: respondents ' (1) knowledge about GAI, (2) perceived value of GAI, and (3) perceived concerns about GAI. Each sub-construct contains four items, with participants rating statements on a 5-point scale (1 = ' very true ' to 5 = ' not at all true ' ).  Sample  items  from  the  knowledge  about  GAI  sub-construct include: ' I  understand  that  generative  AI  technologies  like  ChatGPT can exhibit biases and unfairness in their output. ' Sample items from the perceived  value  sub-construct  include: ' I  believe  generative  AI  technologies such as ChatGPT can improve my digital competence. ' The overall reliability of the AI literacy measure was acceptable (  = .75), with high internal consistency for each subscale: knowledge (  = .89), perceived value (  = .82), and perceived concerns (  = .78).

Perception of GAI as a Teammate. Participants ' perceptions of GAI as a teammate were measured using four items adapted from Van den Bossche et al. (2006). Respondents used a 5-point frequency scale (1 = ' never ' to 5 = ' frequently ' ) to evaluate items such as: ' With generative AI as one of my teammates, I am satisfied with our team ' s performance ' and ' I can depend on generative AI as a teammate for information and advice. ' Cronbach ' s alpha for the measure was 0.80., indicating good reliability.

## 3. Results

## 3.1. Hypothesis 1: AI literacy predicting trust in GAI

To  test  Hypothesis  1,  a  multinomial  logistic  regression  was  conducted. Descriptive statistics are shown in Table 2. Pearson correlations results  appear  in  Table  3.  Perceived  value  of  GAI  was  moderately correlated with knowledge about GAI ( r = 0.32), while concern showed small correlations with both value ( r = 0.00) and knowledge ( r = 0.16).

The test of multinomial regression model coefficients was statistically significant (  2 = 15.76, df = 6, p &lt; .05), indicating that the model provided a good fit to the data. The model explained approximately 15 % of the variance in group membership (Nagelkerke R 2 = 0.15), representing a modest-to-moderate effect.

Specifically,  as  indicated  in  Table  4,  participants  with  a  greater knowledge of GAI were significantly inclined to distrust rather than trust GAI  as  a  teammate  (  = GLYPH&lt;0&gt; 1.20, p &lt; .01).  Conversely,  participants perceiving higher value in GAI ' s  capabilities  were  significantly  more likely to trust GAI. (  = 1.01, p &lt; .05). Perceived concerns about GAI were not significant predictors (  = GLYPH&lt;0&gt; 0.06, p &gt; .05).

Comparisons between ambivalent and distrusting participants showed no significant predictors for knowledge, (  = GLYPH&lt;0&gt; 0.84, p &gt; .05), perceived value (  = 0.30, p &gt; .05) or concerns (  = 0.28, p &gt; .05).

Results  indicate  that  higher  perceived  value  fosters  trust  in  GAI, while greater knowledge may prompt skepticism or distrust. Thus, offering partial support for Hypothesis 1.

## 3.2. Hypothesis 2: AI literacy and trust predicting perceptions of GAI as a teammate

Regression analysis was conducted to test Hypothesis 2, predicting perceptions of GAI as a teammate. Independent variables were perceived value, knowledge, concerns, and trust categories  (dummy-coded ' Distrust ' as the baseline).

Descriptive intercorrelations among the variables are presented in Table 5. Perceived value of GAI was positively correlated with perception of GAI as a teammate ( r = 0.62) and knowledge about GAI ( r = 0.31). Perceived concerns correlated positively with knowledge about GAI ( r = 0.16). The dummy-coded ' Trust ' variable correlated positively with perceptions of GAI as a teammate ( r = 0.37) and perceived value of GAI ( r = 0.17), but negatively with knowledge about GAI ( r = GLYPH&lt;0&gt; 0.18), and ambivalence ( r = GLYPH&lt;0&gt; 0.56).

The  regression  model  (see  Table  6)  showed  that  perceived  value significantly  predicted perception of GAI as a teammate,  = 0.70, t (116) = 8.01, p &lt; .01. Both dummy-coded trust (  = 0.59, p &lt; .01) and ambivalence (  = 0.39, p &lt; .05) variables were significant. Knowledge (  =GLYPH&lt;0&gt; 0.06, p &gt; .05) and concern (  = 0.03, p &gt; .05) were not significant predictors. The model explained 50 % of the variance, R 2 = 0.50, F (5,

Table 2 Descriptive  statistics  for  participants '

AI  literacy  and  trust  in  GAI  (Study  2, Hypothesis 1).

| AI Literacy Sub-Constructs   | Trust in GAI Categories   |   n |    M |   SD |
|------------------------------|---------------------------|-----|------|------|
| Knowledge about GAI          | Trust                     |  60 | 4.14 | 0.83 |
|                              | Distrust                  |  30 | 4.57 | 0.59 |
|                              | Ambivalence               |  26 | 4.22 | 0.76 |
|                              | Total                     | 116 | 4.27 | 0.77 |
| Perceived value of GAI       | Trust                     |  60 | 4.27 | 0.72 |
|                              | Distrust                  |  30 | 4.04 | 0.58 |
|                              | Ambivalence               |  26 | 4.03 | 0.57 |
|                              | Total                     | 116 | 4.16 | 0.66 |
| Perceived concerns about GAI | Trust                     |  58 | 3.22 | 0.83 |
|                              | Distrust                  |  29 | 3.32 | 0.72 |
|                              | Ambivalence               |  26 | 3.46 | 0.84 |
|                              | Total                     | 113 | 3.3  | 0.81 |

Note . GAI = Generative Artificial Intelligence.

Table 3 Pearson correlation coefficients among variables in Study 2, Hypothesis 1.

| Variable                     |   n |    M |   SD | 1      | 2    | 3   |
|------------------------------|-----|------|------|--------|------|-----|
| Knowledge about GAI          | 113 | 4.25 | 0.78 | -      |      |     |
| Perceived value of GAI       | 113 | 4.15 | 0.66 | 0.32** | -    |     |
| Perceived concerns about GAI | 113 | 3.31 | 0.81 | 0.16*  | 0.00 | -   |

Note . GAI = Generative Artificial Intelligence.

* p &lt; .05, ** p &lt; .01.

Table 4 Multinomial logistic regression results for the effect of participants '

AI literacy on trust in GAI as a teammate (Study 2, Hypothesis 1).

| Outcome and Predictor        | B             | SE   | Wald   | p      | Exp (B)   | 95 % CI for Exp(B)   |
|------------------------------|---------------|------|--------|--------|-----------|----------------------|
| Trust                        |               |      |        |        |           |                      |
| Intercept                    | 1.85          | 2.13 | 0.75   | 0.39   | -         | -                    |
| Knowledge about GAI          | GLYPH<0> 1.20 | 0.40 | 8.92   | 0.00** | 0.30      | [0.14, 0.66]         |
| Perceived value of GAI       | 1.03          | 0.42 | 5.92   | 0.02** | 2.80      | [1.22, 6.41]         |
| Perceived concerns about GAI | GLYPH<0> 0.06 | 0.30 | 0.05   | 0.83   | 0.94      | [0.52, 1.69]         |
| Ambivalence                  |               |      |        |        |           |                      |
| Intercept                    | 1.39          | 2.31 | 0.36   | 0.55   | -         | -                    |
| Knowledge about GAI          | GLYPH<0> 0.84 | 0.44 | 3.63   | 0.06   | 0.43      | [0.18, 1.02]         |
| Perceived value of GAI       | 0.30          | 0.46 | 0.44   | 0.51   | 1.35      | [0.56, 3.31]         |
| Perceived concerns about GAI | 0.28          | 0.35 | 0.65   | 0.42   | 1.33      | [0.67, 2.65]         |

Note . GAI = Generative Artificial Intelligence; CI = Confidence Interval. Reference category: Distrust.

* p &lt; .05, ** p &lt; .01.

116) = 21.09, p &lt; . 01.

Results indicate that perceived value and trust significantly predict positive perceptions of GAI as a teammate. Participants who trusted or were ambivalent toward GAI viewed it more favorably as a teammate compared to those who distrusted it. Thus, Hypothesis 2 was partially supported,  highlighting  the  critical  role  of  perceived  value  in  individuals ' willingness to consider GAI as a teammate.

## 4. Discussion

## 4.1. Participants ' perceptions of trust in GAI for team projects

Findings indicated that 52 % of the participants trusted GAI during human-GAI collaboration, while 26 % expressed distrust and 22 % were ambivalent.  Qualitative  analysis  of  participants ' responses  further revealed  that  their  trust  perceptions  were  closely  tied  to  how  they evaluated GAI ' s contributions to teamwork. For instance, the primary rationale  among  those  who  trusted  GAI  involved  its  usefulness  in retrieving or generating information for the team. As some participants explained ' Yes, I trust ChatGPT because it helps to generate creative ideas and make our work efficient and productive. ' and ' It played a role during the case studies by providing problem and solutions I had not previously considered. ' These findings align with recent studies examining  trust  in  human-GAI  interactions.  For  example,  Georganta  and Ulfert (2024) demonstrated that participants exhibited high levels of interpersonal trust in both human-human and human-GAI teams.

Zhang  et  al.  (2023) further  showed  that  humans  often  display behavioral trust toward GAI team members, sometimes even preferring GAI-generated decisions over those made by human teammates. One reason that over half of our participants trusted GAI in team projects may be their cautious and rational approach toward using GAI. As one

Table 5 Pearson correlation coefficients among variables in Study 2, Hypothesis 2.

| Variable                         |   n |    M |   SD | 1             | 2              | 3             | 4             | 5               | 6   |
|----------------------------------|-----|------|------|---------------|----------------|---------------|---------------|-----------------|-----|
| Perceptions of GAI as a Teammate | 113 | 3.49 | 0.78 | -             |                |               |               |                 |     |
| Knowledge about GAI              | 113 | 4.25 | 0.78 | 0.06          | -              |               |               |                 |     |
| Perceived value of GAI           | 113 | 4.15 | 0.66 | 0.62**        | 0.31**         | -             |               |                 |     |
| Perceived concerns about GAI     | 113 | 3.31 | 0.81 | 0.01          | 0.16*          | 0.00          | -             |                 |     |
| Trust                            | 113 | 0.51 | 0.5  | 0.37**        | GLYPH<0> 0.18* | 0.17*         | GLYPH<0> 0.10 | -               |     |
| Ambivalence                      | 113 | 0.23 | 0.42 | GLYPH<0> 0.05 | GLYPH<0> 0.02  | GLYPH<0> 0.10 | 0.11          | GLYPH<0> 0.56** | -   |

Note . GAI = Generative Artificial Intelligence. Trust and Ambivalence are dummy-coded variables with Distrust as the reference category. * p &lt; .05, ** p &lt; .01.

Multiple regression analysis predicting perceptions of GAI as a teammate (Study

Table 6 2, Hypothesis 2).

| Predictor                    | B             |   SE | t             | p      | 95 % CI       |
|------------------------------|---------------|------|---------------|--------|---------------|
| Constant                     | 0.34          | 0.45 | 0.76          | 0.45   | [-0.55, 1.23] |
| Knowledge about GAI          | GLYPH<0> 0.06 | 0.08 | GLYPH<0> 0.73 | 0.47   | [-0.21, 0.10] |
| Perceived value of GAI       | 0.70          | 0.09 | 8.01          | 0.00** | [0.53, 0.87]  |
| Perceived concerns about GAI | 0.03          | 0.07 | 0.45          | 0.66   | [-0.10, 0.16] |
| Trust                        | 0.59          | 0.14 | 4.31          | 0.00** | [0.32, 0.85]  |
| Ambivalence                  | 0.39          | 0.16 | 2.53          | 0.01*  | [0.09, 0.70]  |

Note . GAI = Generative Artificial Intelligence; CI = Confidence Interval. Trust and  Ambivalence are  dummy-coded variables  with  Distrust  as  the  reference category.

* p &lt; .05, ** p &lt; .01.

participant  stated: ' I  trust  generative  AI  technologies  in  class,  but  I understand that it ' s important to be aware of their limitations, including potential biases, unfairness, or inaccuracies. ' This quote illustrates that participants  trusted  primarily  for  their  generative  capabilities  rather than their accuracy in factual information, reflecting awareness of GAI ' s current limitations. Our findings underscore the balanced understanding of  GAI ' s  capabilities  and  limitations -a  core  aspect  of  AI  literacy -is essential for fostering appropriate trust (Long et al., 2021). Thus, our results  partially  support  the  hypothesis  that  AI  literacy  significantly influences individuals ' trust in GAI within team projects.

## 4.2. The impacts of AI literacy on trust in team projects

Results partially support our hypothesis that AIliteracy influences team members ' trust in GAI. Specifically, perceived value and knowledge were significant predictors, whereas perceived concerns were not. Participants  with  greater  knowledge  about  GAI,  including  an  understanding of its strengths and limitations, were more inclined to distrust in team projects. Conversely, those who perceived higher value in GAI ' s capabilities, such as increased efficiency, were more likely to trust it. This finding highlights that trust in GAI within team projects depend heavily on understanding GAI ' s nuances and recognizing its practical value.

The contrasting effects of knowledge and perceived value may reflect different cognitive processes. Participants with greater knowledge about GAI likely possess more critical and realistic perceptions of GAI (Chen et al., 2024), fostering skepticism when considering GAI as a collaborative  partner.  Conversely,  participants  focused  on  GAI ' s  practical benefits, such as efficiency or idea generation, tend to develop greater trust. Baek and Kim ' s (2023) study similarly found that users driven by practical  motivations  (efficiency,  creativity)  viewed  GAI  positively, reporting increased trust. This divergence is consistent with Glikson and Woolley ' s (2020) review, emphasizing that trust in AI is closely linked to tangibility, transparency, reliability, and responsiveness of AI tools.

Shamim et al. (2023) distinguished between cognitive and emotional trust -cognitive trust emerges from rational evaluation, while emotional trust  stems  from  affective  connections.  In  our  study,  knowledge  and perceived  value  significantly  predicted  trust,  aligning  primarily  with cognitive trust. Participant statements such as ' I  trust  GAI  in certain situations, but I understand that it can produce inaccurate information, ' demonstrate  critical,  rather  than  emotional,  reliance  on  GAI.  This cognitive dimension of trust, built upon critical awareness and verifying of GAI outputs, characterizes most participants ' perceptions.

## 4.3. Theoretical contributions

Our study advances understanding of human -GAI collaboration in team contexts by clarifying complex relationships among AI literacy, trust, and perceptions of GAI as a teammate. By extending Mayer et al. ' s (1995) theoretical model of trust to the context of human-GAI teams, we provide novel insights into trust formation specific to interactions with AI entities. Our findings address critical gaps regarding the specificity of trust  referents  in  human-AI  collaboration,  demonstrating  how  individuals  form  nuanced  trust  relationships  with  AI  teammates.  More specifically, by applying Mayer et al. ' s (1995) trust model, we extend its application beyond traditional organizational contexts. Although extensively applied in organizational research, this model ' s relevance for  human-AI interactions has remained underexplored. Our findings demonstrate that AI literacy -particularly knowledge and value components -significantly  influence  trust  formation.  Thus,  domain-specific literacy plays a pivotal role in trust-building with GAI, enriching our understanding of trust dynamics in digital collaborative contexts.

Further, our research highlights the significance of perceived value in AI literacy in shaping perceptions of GAI as a teammate. This finding extends prior research on shared values and value congruence as antecedents of trust (Sitkin &amp; Roth, 1993a, 1993b), emphasizing that trust in GAI stems not only from perceived reliability but also from the alignment of values between GAI and human teammates.

By situating our research within socio-technical frameworks (Holton &amp; Boyd, 2021; Waefler &amp; Schmid, 2020), we highlight the interconnectedness of social aspects (trust, perceptions) and technical factors (AI literacy,  GAI  capabilities).  Focusing  on  undergraduate  students,  our finding illustrates how novice users -simultaneously developing teamwork and digital competencies -experience human -GAI collaboration. Although our results might not generalize to experienced professionals, they provide context-specific insights for educational interventions and complement  existing  research  conducted  in  workplace  environments supporting effective human-GAI collaboration.

## 4.4. Practical implications

The findings from this study offer several actionable implications for educators, curriculum designers, and academic institutions aiming to effectively integrate generative AI (GAI) into team-based educational contexts. First, our research demonstrates that AI literacy significantly shapes individuals ' trust formation and perceptions of GAI as a teammate.  Thus,  educators  should  prioritize  comprehensive  AI  literacy training that addresses not only technical proficiency but also valuealignment aspects, as our findings indicate that shared values significantly influence trust in GAI as a teammate. Given that the knowledge

and value components of AI literacy particularly influence trust formation  in  human-GAI  teams,  academic  institutions  should  develop  programs  that  explicitly  demonstrate  practical  GAI  applications  while fostering  critical  understanding  of  how  value  congruence  between humans and GAI systems affects collaborative outcomes (Blackie et al., 2024).  These  educational  approaches  should  be  tailored  to  diverse disciplinary contexts, recognizing that domain-specific literacy plays a pivotal role in trust-building with technological tools in collaborative settings.

Moreover, our findings highlight that trust in GAI as a teammate is nuanced and context-dependent rather than absolute. Individuals typically demonstrate selective trust toward GAI as a teammate, approaching collaboration with critical evaluation mechanisms that align with Mayer et al. ' s (1995) trust framework. This suggests educators should design activities that enhance students ' ability to evaluate GAI ' s trustworthiness across varying collaborative scenarios, particularly focusing on the alignment of values between human teammates and GAI systems. Furthermore, instructors might incorporate team exercises that explicitly address how trust referents operate in human-GAI teams, encouraging  students  to  develop  sophisticated  mental  models  of  GAI  as  a collaborative partner rather than merely a tool.

Additionally,  our  research  confirms  that  perceived  value  congruence, as identified by Sitkin and Roth (1993a, 1993b), emerges as a critical factor influencing trust formation in human-GAI teams. Consequently, institutions should explicitly address value alignment between humans and GAI systems in their educational approaches. Workshops and training sessions should not only communicate GAI ' s technical capabilities but also facilitate discussions about how shared values influence  trust  formation  in  human-GAI  collaboration.  As  our  findings demonstrate  that  AI  literacy -particularly  its  knowledge  and  value components -significantly  shapes  perceptions  of  GAI  as  a  teammate, educational programs should develop students ' capacity to recognize when value alignment exists or is lacking in collaborative contexts. Ultimately, educational approaches should focus on helping students understand the complex interplay between AI literacy, trust, and effective collaboration in human-GAI teams, preparing them to navigate these dynamics in future professional contexts where human-AI collaboration will become increasingly prevalent.

## 5. Limitations and future research directions

Despite providing valuable insights, this research has several limitations that offer directions for future investigations. First, our study utilized  a  convenience  sample  of  undergraduate  business  students enrolled in a single course at one institution. While Wheeler, Shanine, Leon, and Whitman (2014) found that student samples in organizational research  can  yield  valid  results  that  do  not  substantially  differ  from non-student samples in terms of demographics or practical conclusions, we recognize that this sampling strategy may constrain the generalizability  of  our  findings  beyond  similar  educational  contexts.  Future research should employ more diverse samples, encompassing students from various disciplines, educational levels, cultural backgrounds, and professional experiences to enhance the external validity and robustness of  results.  Additionally,  extending  this  research  to  workplace  professionals with established teamwork practices would provide valuable comparative insights on how AI integration might differ across educational and professional settings. Second, the tasks examined were primarily situated within management-oriented contexts (e.g., analyzing business cases). This narrow focus potentially limits our understanding of  human-GAI interactions across different task types. Future studies should investigate collaborative tasks across diverse contexts -such as technical problem-solving, creative projects, and ethical decision-making scenarios -to examine whether and how task characteristics influence trust formation and team perceptions of GAI.

Third, while our measure of perceptions toward GAI as a teammate provided valuable insights, it relied on a limited number of items and dimensions. Future studies should develop more comprehensive measurement tools capable of capturing additional aspects of how students perceive and interact with GAI as a teammate. Specifically, exploring varying  interaction patterns -such  as teams  with  distributed vs. centralized  interaction  with  GAI -could  yield  richer  insights  into optimal  team  configurations  for  effective  GAI  use.  Additionally,  our cross-sectional design provides a snapshot of students ' trust perceptions at one point in time. Longitudinal studies tracking trust development over the duration of team projects or academic terms would yield deeper insights into how trust dynamics evolve with sustained GAI engagement. Such research might pinpoint temporal factors influencing trust trajectories,  guiding  targeted interventions to facilitate sustained, effective human-GAI collaborations. Finally, this study explored AI literacy using specific  subcomponents -knowledge,  perceived  value,  and  concerns. Future research could adopt alternative frameworks or more comprehensive  conceptualizations  of  AI  literacy  to  further  understand  its multifaceted impact on trust and GAI engagement. Additionally, future studies could further explore how value congruence, as conceptualized by Sitkin and Roth (1993a, 1993b), shapes trust formation in human-GAI teams across different contexts. As generative AI continues to evolve, future research must also examine how advancements -such as improved  accuracy, transparency, or adaptive learning capabilities -affect trust  formation and  collaborative  team  dynamics. Investigating how these evolving technological features impact human-GAI interactions will provide invaluable guidance for educators, organizational leaders, and developers designing next-generation collaborative AI systems.

## 6. Conclusion

This  study extends traditional  trust frameworks  to the innovative context  of  human -GAI  collaboration,  demonstrating  that  domainspecific AI literacy significantly shapes trust dynamics in collaborative team settings. By highlighting how  distinct facets of AI literacy -particularly  knowledge  and perceived  value -influence  individual ' trust and perceptions of GAI as a teammate, our research provides novel insights into the nuanced processes underlying effective human--AI  interaction.  Furthermore,  our  findings  emphasize  the  situational nature of trust, underscoring that individuals selectively trust GAI based on task context and value alignment, highlighting the importance of critical  appraisal  skills  in  evaluating  GAI  as  a  potential  teammate. Importantly, our results contribute to both theoretical advancement and practical guidance. Theoretically, we enrich trust research by applying and extending Mayer et al. ' s (1995) model specifically to human -GAI interactions, offering a nuanced understanding of trust development in human-AI collaborative relationships. Our work clarifies the role of trust referents in human-AI collaboration and demonstrates how AI literacy shapes perceptions of GAI ' s trustworthiness as a teammate. Practically, we  underscore  the  critical  role  educators  and  institutions  play  in fostering  balanced  AI  literacy,  equipping  individuals  to  effectively leverage generative AI as a teammate while maintaining an informed skepticism  regarding  its  limitations.  As  generative  AI  continues  to become  deeply  embedded  in  educational  and  professional  environments, the ability to build and maintain appropriate, context-sensitive trust  in  these  technologies  as  teammates  rather  than  mere  tools  becomes  increasingly  crucial.  This  research  provides  foundational  evidence, offering both empirical insights and practical recommendations that inform strategies for effectively integrating GAI into team-based projects.

## CRediT authorship contribution statement

Zilong Pan: Writing -review &amp; editing, Writing -original draft, Data curation, Conceptualization. Ozias A. Moore: Writing -review &amp; editing, Writing -original draft. Antigoni Papadimitriou: Writing -review &amp; editing, Writing -original draft. Jiayan Zhu: Writing -review

&amp; editing.

## Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

## Data Availability Statement

Data  and  supplementary  materials  are  available  from  the  corresponding author upon reasonable request.

## References

- Agrawal, A., Gans, J. S., &amp; Goldfarb, A. (2019). Exploring the impact of artificial intelligence: Prediction versus judgment. Information Economics and Policy, 47 , 1 -6. - Azungah, T. (2018). Qualitative research: Deductive and inductive approaches to data analysis. Qualitative Research Journal, 18 (4), 383 -400. https://doi.org/10.1108/ QRJ-D-18-00035
- Baabdullah, A. M., Alalwan, A. A., Slade, E. L., Raman, R., &amp; Khatatneh, K. F. (2021). SMEs and artificial intelligence (AI): Antecedents and consequences of AI-based B2B practices. Industrial Marketing Management, 98 , 255 -270. https://doi.org/10.1016/j. indmarman.2021.09.003
- Baek, T. H., &amp; Kim, M. (2023). Is ChatGPT scary good? How user motivations affect creepiness and trust in generative artificial intelligence. Telematics and Informatics, 83 , Article 102030. - Balakrishnan, M., Ferreira, K., &amp; Tong, J. (2022). Improving human-algorithm collaboration: Causes and mitigation of over-and under-adherence. SSRN Electronic Journal . - Bankins, S., Ocampo, A. C., Marrone, M., Restubog, S. L. D., &amp; Woo, S. E. (2024). A multilevel review of artificial intelligence in organizations: Implications for organizational behavior research and practice. Journal of Organizational Behavior, 45 (2), 159 -182. - Benlian, A., Wiener, M., Cram, W. A., Krasnova, H., Maedche, A., M  ohlmann, M., &amp; Remus, U. (2022). Algorithmic management: Bright and dark sides, practical implications, and research opportunities. Business &amp; Information Systems Engineering, 64 (6), 825 -839.
- Berente, N., Gu, B., Recker, J., &amp; Santhanam, R. (2021). Managing artificial intelligence. MIS Quarterly, 45 (3).
- Bezrukova, K., Griffith, T. L., Spell, C., Rice, V., &amp; Yang, H. E. (2023). Artificial intelligence and groups: Effects of attitudes and discretion on collaboration. Group &amp; Organization Management, 48 (2), 629 -670. https://doi.org/10.1177/ 10596011231160574
- Blackie, M., McKenna, S., Kramm, N., &amp; Pallitt, N. (2024). Interrogating assessment in the age of generative AI. African Journal of Inter/Multidisciplinary Studies, 6 (1), 1 -11. - Bonaccio, S., &amp; Dalal, R. S. (2006). Advice taking and decision-making: An integrative literature review, and implications for the organizational sciences. Organizational Behavior and Human Decision Processes, 101 (2), 127 -151. https://doi.org/10.1016/j. obhdp.2006.07.001
- Brougham, D., &amp; Haar, J. (2017). Smart technology, artificial intelligence, robotics, and algorithms (STARA): Employees ' perceptions of our future workplace. Journal of Management and Organization, 24 (2), 239 -257. https://doi.org/10.1017/ jmo.2016.55
- Cao, G., Duan, Y., Edwards, J. S., &amp; Dwivedi, Y. K. (2021). Understanding managers ' attitudes and behavioral intentions towards using artificial intelligence for organizational decision-making. Technovation, 106 , Article 102312. https://doi.org/ 10.1016/j.technovation.2021.102312
- Castelo, N., Bos, M. W., &amp; Lehmann, D. R. (2019). Task-dependent algorithm aversion. Journal of Marketing Research, 56 (5), 809 -825. https://doi.org/10.1177/ 0022243719851788
- Chan, C. K. Y., &amp; Hu, W. (2023). Students ' voices on generative AI: Perceptions, benefits, and challenges in higher education. International Journal of Educational Technology in Higher Education, 20 (1). - Chatterjee, S., Chaudhuri, R., &amp; Vrontis, D. (2021). Does data-driven culture impact innovation and performance of a firm? An empirical examination. Annals of Operations Research, 333 (2 -3), 601 -626. - Chatterjee, S., Rana, N. P., Khorana, S., Mikalef, P., &amp; Sharma, A. (2021). Assessing organizational users ' intentions and behavior to AI integrated CRM systems: A metaUTAUT approach. Information Systems Frontiers, 25 (4), 1299 -1313. https://doi.org/ 10.1007/s10796-021-10181-1
- Chen, D., Jefferson, C., Hix, J., Qin, N., &amp; Cao, Y. (2024). Exploring generative artificial intelligence (GAI): Business professionals ' surveys and perceptions on GAI. Journal of Behavioral and Applied Management, 24 (2), 79 -89. https://doi.org/10.21818/ 001c.122143
- Chiou, E. K., &amp; Lee, J. D. (2021). Trusting automation: Designing for responsivity and resilience. Human Factors, 65 (1), 137 -165. https://doi.org/10.1177/ 00187208211009995
- Chowdhury, S., Budhwar, P., Dey, P. K., Joel-Edgar, S., &amp; Abadie, A. (2022). AI-employee collaboration and business performance: Integrating knowledge-based view, socio-
- technical systems and organisational socialisation framework. Journal of Business Research, 144 , 31 -49. - Cockburn, I., Henderson, R., &amp; Stern, S. (2019). The impact of artificial intelligence on innovation: An exploratory analysis . The University of Chicago Press. https://doi.org/ 10.7208/chicago/9780226613475.003.0004
- Creswell, J. W., &amp; Creswell, J. D. (2017). Research design: Qualitative, quantitative, and mixed methods approaches (5th ed.). Thousand Oaks, CA: SAGE.
- Davenport, T. (2016). Rise of the strategy machines . MIT Sloan Management Review. Retrieved from https://www.scirp.org/reference/referencespapers?reference id = 2918756.
- De Jong, Dirks, K. T., &amp; Gillespie, N. (2016). Trust and team performance: A metaanalysis of main effects, moderators, and covariates. Journal of Applied Psychology, 101 (8), 1134 -1150. - Demir, R., Wennberg, K., &amp; McKelvie, A. (2017). The strategic management of highgrowth firms: A review and theoretical conceptualization. Long Range Planning, 50 (4), 431 -456. - Dennis, A. R., Lakhiwal, A., &amp; Sachdeva, A. (2023). AI agents as team members: Effects on satisfaction, conflict, trustworthiness, and willingness to work with. Journal of Management Information Systems, 40 (2), 307 -337. https://doi.org/10.1080/ 07421222.2023.2196773
- Endsley, M. R. (2023). Supporting human-AI teams: Transparency, explainability, and situation awareness. Computers in Human Behavior, 140 , Article 107574.
- Ferrario, A. (2021). Design publicity of black box algorithms: A support to the epistemic and ethical justifications of medical AI systems. Journal of Medical Ethics, 48 (7), 492 -494. - Fiore, S. M., &amp; Wiltshire, T. J. (2016). Technology as teammate: Examining the role of external cognition in support of team cognitive processes. Frontiers in Psychology, 7 , 1531. - Georganta, E., &amp; Ulfert, A. S. (2024). Would you trust an AI team member? Team trust in human -AI teams. Journal of Occupational and Organizational Psychology . https://doi. org/10.1111/joop.12504
- Gillespie, N., Lockey, S., Curtis, C., Pool, J., &amp; Akbari, N. A. (2023). Trust in artificial intelligence: A global study . KPMG Australia: The University of Queensland. https:// doi.org/10.14264/00d3c94
- Glikson, E., &amp; Woolley, A. W. (2020). Human trust in artificial intelligence: Review of empirical research. The Academy of Management Annals, 14 (2), 627 -660. https://doi. org/10.5465/annals.2018.0057
- Goodwin, P., G  onl, M. S., &amp;  Onkal, D. (2013). Antecedents and effects of trust in forecasting advice. International Journal of Forecasting, 29 (2), 354 -366. https://doi. org/10.1016/j.ijforecast.2012.08.001
- Groom, V., &amp; Nass, C. (2007). Can robots be teammates?: Benchmarks in human -robot teams. Interaction Studies, 8 (3), 483 -500.
- Guzman, A. L., &amp; Lewis, S. C. (2019). Artificial intelligence and communication: A human -machine communication research agenda. New Media &amp; Society, 22 (1), 70 -86. - Hoff, K. A., &amp; Bashir, M. (2015). Trust in automation: Integrating empirical evidence on factors that influence trust. Human Factors, 57 (3), 407 -434. https://doi.org/ 10.1177/0018720814547570
- Holton, R., &amp; Boyd, R. (2021). 'Where are the people? What are they doing? Why are they doing it? ' (mindell) situating artificial intelligence within a socio-technical framework. Journal of Sociology, 57 (2), 179 -195. https://doi.org/10.1177/ 1440783319873046. https://hbr.org/2018/07/collaborative-intelligence-hum ans-and-ai-are-joining-forces
- IBM Corp. (2024). IBM SPSS statistics for windows (version 29.0) . IBM Corp [Computer software].
- Jaiswal, A., Arun, C. J., &amp; Varma, A. (2021). Rebooting employees: Upskilling for artificial intelligence in multinational corporations. International Journal of Human Resource Management, 33 (6), 1179 -1208. https://doi.org/10.1080/ 09585192.2021.1891114
- Jarvenpaa, S. L., &amp; Selander, L. (2023). Between scale and impact: Member prototype ambiguity in digital transformation. European Journal of Information Systems, 32 (3), 390 -408. - Jorzik, P., Yigit, A., Kanbach, D. K., Kraus, S., &amp; Dabi  c, M. (2023). Artificial intelligenceenabled business model innovation: Competencies and roles of top management. IEEE Transactions on Engineering Management, 71 , 7044 -7056.
- Kawaguchi, K. (2021). When will workers follow an algorithm? A field experiment with a retail business. Management Science, 67 (3), 1670 -1695. https://doi.org/10.1287/ mnsc.2020.3599
- Kim, K., &amp; Kwon, K. (2023). Exploring the AI competencies of elementary school teachers in South Korea. Computers and Education: Artificial Intelligence, 4 , Article 100137. - Knowles, B., Richards, J. T., &amp; Kroeger, F. (2022). The many facets of trust in AI: Formalizing the relation between trust and fairness, accountability, and transparency. arXiv preprint arXiv:2208.00681 .
- Kong, S. C., Cheung, W. M. Y., &amp; Zhang, G. (2021). Evaluation of an artificial intelligence literacy course for university students with diverse study backgrounds. Computers and Education: Artificial Intelligence, 2 , Article 100026. https://doi.org/10.1016/j. caeai.2021.100026
- Kozlowski, S. W. J., Grand, J. A., Baard, S. K., &amp; Pearce, M. (2015). Teams, teamwork, and team effectiveness: Implications for human systems integration. In D. BoehmDavis, F. Durso, &amp; J. Lee (Eds.), The handbook of human systems integration . Washington, DC: APA.
- Laupichler, M. C., Aster, A., Schirch, J., &amp; Raupach, T. (2022). Artificial intelligence literacy in higher and adult education: A scoping literature review. Computers and Education: Artificial Intelligence, 3 , Article 100101. https://doi.org/10.1016/j. caeai.2022.100101

- Lee, M. D., Criss, A. H., Devezer, B., Donkin, C., Etz, A., Leite, F. P., et al. (2019). Robust modeling in cognitive science. Computational Brain &amp; Behavior, 2 , 141 -153. https:// doi.org/10.1007/s42113-019-00029-y
- Lee, J. D., &amp; See, K. A. (2004). Trust in automation: Designing for appropriate reliance. Human Factors, 46 (1), 50 -80. - Leslie, D., Mazumder, A., Peppin, A., Wolters, M., &amp; Hagerty, A. (2021). Does ' AI ' stand for augmenting inequality in the era of COVID-19 healthcare? BMJ, n304 . https:// doi.org/10.1136/bmj.n304
- Lewis, P. R., &amp; Marsh, S. (2022). What is it like to trust a rock? A functionalist perspective on trust and trustworthiness in artificial intelligence. Cognitive Systems Research, 72 , 33 -49. - Lin, L. (2024). A quarter of U.S. teachers say AI tools do more harm than good in K-12 education . Pew Research Center. Retrieved from https://www.pewresearch.or g/short-reads/2024/05/15/a-quarter-of-u-s-teachers-say-ai-tools-do-more-harm-tha n-good-in-k-12-education/.
- Long, D., Blunt, T., &amp; Magerko, B. (2021). Co-designing AI literacy exhibits for informal learning spaces. Proceedings of the ACM on Human-Computer Interaction, 5 (CSCW2), 1 -35. - Long, D., &amp; Magerko, B. (2020). What is AI literacy? Competencies and design considerations. In Proceedings of the 2020 CHI conference on human factors in computing systems (pp. 1 -14). - Longoni, C., Bonezzi, A., &amp; Morewedge, C. K. (2019). Resistance to medical artificial intelligence. Journal of Consumer Research, 46 (4), 629 -650. https://doi.org/ 10.1093/jcr/ucz013
- Mathieu, J. E., Hollenbeck, J. R., van Knippenberg, D., &amp; Ilgen, D. R. (2017). A century of work teams in the Journal of Applied Psychology. Journal of Applied Psychology, 102 (3), 452 -467.
- Mayer, R. C., Davis, J. H., &amp; Schoorman, F. D. (1995). An integrative model of organizational trust. Academy of Management Review, 20 (3), 709 -734. https://doi. org/10.2307/258792
- Meske, C., &amp; Bunde, E. (2020). Transparency and trust in human-AI-interaction: The role of model-agnostic explanations in computer vision-based decision support. In Artificial intelligence in HCI: First international conference, AI-HCI 2020, held as part of the 22nd HCI international conference, HCII 2020, Copenhagen, Denmark, july 19 -24, 2020, proceedings 22 (pp. 54 -69). Springer International Publishing.
- Musick, G., O ' Neill, T. A., Schelble, B. G., McNeese, N. J., &amp; Henke, J. B. (2021). What happens when humans believe their teammate is an AI? An investigation into humans teaming with autonomy. Computers in Human Behavior, 122 , Article 106852. - Ng, D. T. K., Leung, J. K. L., Chu, S. K. W., &amp; Qiao, M. S. (2021). Conceptualizing AI literacy: An exploratory review. Computers and Education. Artificial Intelligence, 2 , Article 100041. - Nguyen, T., &amp; Malik, A. (2022). Impact of knowledge sharing on employees ' service quality: The moderating role of artificial intelligence. International Marketing Review, 39 (3), 482 -508. - Noy, S., &amp; Zhang, W. (2023). Experimental evidence on the productivity effects of generative artificial intelligence. Science, 381 (6654), 187 -192. https://doi.org/ 10.1126/science.adh2586
- O ' Neill, T., McNeese, N., Barron, A., &amp; Schelble, B. (2022). Human -autonomy teaming: A review and analysis of the empirical literature. Human Factors, 64 (5), 904 -938. - OpenAI. (2023). ChatGPT (version 3.5) [large language model]. https://openai.com/ch atgpt.
- Oppenlaender, J., Linder, R., &amp; Silvennoinen, J. (2023). Prompting AI art: An investigation into the creative skill of prompt engineering. https://arxiv.org/abs/2 303.13534. (Accessed 9 March 2024).
- Pan, Y., &amp; Froese, F. J. (2023). An interdisciplinary review of AI and HRM: Challenges and future directions. Human Resource Management Review, 33 (1), Article 100924. - Pelau, C., Dabija, D., &amp; Ene, I. (2021). What makes an AI device human-like? The role of interaction quality, empathy and perceived psychological anthropomorphic characteristics in the acceptance of artificial intelligence in the service industry. Computers in Human Behavior, 122 , Article 106855. https://doi.org/10.1016/j. chb.2021.106855
- Pereira, L., Jer  onimo, C., Sim  oes, F., Dias,  A., Costa, R. L. D., &amp; Gonalves, R. (2024). Project virtual teams: Systematic literature review. International Journal of Agile Systems and Management, 17 (1), 15 -45. https://doi.org/10.1504/ IJASM.2024.135370
- Pinski, M., Adam, M., &amp; Benlian, A. (2023). AI knowledge: Improving AI delegation through human enablement. In Proceedings of the 2023 CHI conference on human factors in computing systems (pp. 1 -17).
- Pinski, M., &amp; Benlian, A. (2024). AI literacy for users -A comprehensive review and future research directions of learning methods, components, and effects. Computers in Human Behavior: Artificial Humans, 2 , Article 100062. https://doi.org/10.1016/j. chbah.2024.10006
- Poole, M. S., &amp; Hollingshead, A. B. (Eds.). (2005). Theories of small groups: Interdisciplinary perspectives . Sage Publications.
- Powell, T. C., Lovallo, D., &amp; Fox, C. R. (2011). Behavioral strategy. Strategic Management Journal, 32 (13), 1369 -1386. - Raisch, S., &amp; Krakowski, S. (2021). Artificial intelligence and management: The automation -augmentation paradox. Academy of Management Review, 46 (1),
- 192 -210. - Ross, C., &amp; Swetlitz, I. (2018). IBM ' s Watson supercomputer recommended 'unsafe and incorrect ' cancer treatments, internal documents show . STAT News. Retrieved from https://www.statnews.com/2018/07/25/ibm-watson-recommended-unsafe-incorre ct-treatments/.
- Rousseau, V., Aub  e, C., &amp; Savoie, A. (2006). Teamwork behaviors: A review and an integration of frameworks. Small Group Research, 37 (5), 540 -570.
- Salas, E., Kozlowski, S. W., &amp; Chen, G. (2017). A century of progress in industrial and organizational psychology: Discoveries and the next century. Journal of Applied Psychology, 102 (3), 589 -598.
- Salas, E., Reyes, D. L., &amp; McDaniel, S. H. (2018). The science of teamwork: Progress, reflections, and the road ahead. American Psychologist, 73 (4), 593.
- Schoorman, F. D., Mayer, R. C., &amp; Davis, J. H. (2007). An integrative model of organizational trust: Past, present, and future. Academy of Management Review, 32 (2), 344 -354. - Seeber, I., Bittner, E., Briggs, R. O., De Vreede, T., De Vreede, G., Elkins, A., Maier, R., Merz, A. B., Oeste-Rei  , S., Randrup, N., Schwabe, G., &amp; S  ollner, M. (2020). Machines as teammates: A research agenda on AI in team collaboration. Information &amp; Management, 57 (2), Article 103174. - Shamim, S., Yang, Y., Zia, N. U., Khan, Z., &amp; Shariq, S. M. (2023). Mechanisms of cognitive trust development in artificial intelligence among front line employees: An empirical examination from a developing economy. Journal of Business Research, 167 , Article 114168. - Shneiderman, B. (2020). Human-centered artificial intelligence: Reliable, safe &amp; trustworthy. International Journal of Human-Computer Interaction, 36 (6), 495 -504. Siau, K., &amp; Wang, W. (2018). Building trust in artificial intelligence, machine learning, and robotics. Cutter business technology journal, 31 (2), 47.
- Sitkin, S. B., &amp; Roth, N. L. (1993a). Explaining the limited effectiveness of legalistic ' remedies ' for trust/distrust. Organization Science, 4 (3), 367 -392.
- Sitkin, S. B., &amp; Roth, N. L. (1993b). Explaining the limited effectiveness of legalistic ' remedies ' for trust/distrust. Organization Science, 4 (3), 367 -392. https://doi.org/ 10.1287/orsc.4.3.367
- Sniezek, J. A., &amp; Van Swol, L. M. (2001). Trust, confidence, and expertise in a judgeadvisor system. Organizational Behavior and Human Decision Processes, 84 (2), 288 -307. - Sowa, K., &amp; Przegalinska, A. (2020). Digital coworker: Human-AI collaboration in work environment, on the example of virtual assistants for management professions. In Digital transformation of collaboration: Proceedings of the 9th international COINs conference (pp. 179 -201). Springer International Publishing. https://doi.org/ 10.1007/978-3-030-48993-9\_13.
- Sowa, K., Przegalinska, A., &amp; Ciechanowski, L. (2021). Cobots in knowledge work. Journal of Business Research, 125 , 135 -142. https://doi.org/10.1016/j. jbusres.2020.11.038
- Tomprou, M., &amp; Lee, M. K. (2022). Employment relationships in algorithmic management: A psychological contract perspective. Computers in Human Behavior, 126 , Article 106997. - Tong, S., Jia, N., Luo, X., &amp; Fang, Z. (2021). The Janus face of artificial intelligence feedback: Deployment versus disclosure effects on employee performance. Strategic Management Journal, 42 (9), 1600 -1631. - Van Den Bossche, P., Gijselaers, W. H., Segers, M., &amp; Kirschner, P. A. (2006). Social and cognitive factors driving teamwork in collaborative learning environments. Small Group Research, 37 (5), 490 -521. - Verma, S., &amp; Singh, V. (2022). Impact of artificial intelligence-enabled job characteristics and perceived substitution crisis on innovative work behavior of employees from high-tech firms. Computers in Human Behavior, 131 , Article 107215. https://doi.org/ 10.1016/j.chb.2022.107215
- Waefler, T., &amp; Schmid, U. (2020). Explainability is not enough: Requirements for humanAI-partnership in complex socio-technical systems. In M. Florinda (Ed.), Proceedings of the 2nd European Conference on the Impact of artificial Intelligence and robotics (ECIAIR 2020) .
- Wagner, G. (2021). Liability for artificial intelligence: A proposal of the European parliament. Social Science Research Network . - Wheeler, A. R., Shanine, K. K., Leon, M. R., &amp; Whitman, M. V. (2014). Student-recruited samples in organizational research: A review, analysis, and guidelines for future research. Journal of Occupational and Organizational Psychology, 87 (1), 1 -26. https:// doi.org/10.1111/joop.12042
- Wilson, H. J., &amp; Daugherty, P. R. (2018). Collaborative intelligence: Humans and AI are joining forces. Harvard Business Review . ISBN: 978-1633693869.
- Wolf, F., &amp; Stock-Homburg, R. (2023). Can you be my teammate? Human-robot teams in organizations. Social science space. https://www.socialsciencespace.com/2023/05 /can-you-be-my-teammate-human-robot-teams-in-organizations/.
- Zhang, G., Chong, L., Kotovsky, K., &amp; Cagan, J. (2023). Trust in an AI versus a Human teammate: The effects of teammate identity and performance on Human-AI cooperation. Computers in Human Behavior, 139 , Article 107536. https://doi.org/ 10.1016/j.chb.2022.107536
- National Institute of Standards and Technology. (2023). AI risk management framework. U.S. Department of Commerce. Retrieved from https://www.nist.gov/itl/ai-risk-m anagement-framework.