---
source_file: Studeny_2025_Digitale_Werkzeuge_und_Machtasymmetrien.pdf
conversion_date: 2026-02-03T09:26:08.143974
converter: docling
quality_score: 100
---

<!-- image -->

## DIGITALE WERKZEUGE UND MACHTASYMMETRIEN?

EINE KRITISCHE BETRACHTUNG VON TECHNOLOGIE UND ABH√ÑNGIGKEITEN IN DER SOZIALEN ARBEIT. OGSATAGUNG AM 25. M√ÑRZ 2025 IN GRAZ

## Inhaltsverzeichnis

| 1.   | Projekt Konrad................................................................................................................................. 1                                                                                          |
|------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 2.   | Macht in der Sozialen Arbeit - Reflexion, Verantwortung und digitale Einflussfaktoren............... 2                                                                                                                                     |
| 3.   | Digitale Macht - Verst√§rkung von Ungleichheiten oder neue Handlungsspielr√§ume? .................. 5                                                                                                                                        |
| 4.   | Datenmacht - Kontrolle, Transparenz und die Rechte von Klient:innen........................................ 7                                                                                                                              |
| 5.   | Digitale √úberwachung - Zwischen Kontrolle und Vertrauen........................................................ 10                                                                                                                         |
| 6.   | Digitale Steuerung - Zwischen Selbstbestimmung und unsichtbarer Lenkung ............................ 12                                                                                                                                    |
| 7.   | Digitale Systeme und Selbstbestimmung - Technik beeinflusst Wahrnehmung, Vertrauen und                                                                                                                                                     |
|      | Autonomie..................................................................................................................................... 14                                                                                          |
| 8.   | K√ºnstliche Intelligenz - Transparenz, Gerechtigkeit und menschliche Entscheidungsmacht....... 16                                                                                                                                           |
| 9.   | Diskriminierung durch Algorithmen - Soziale Arbeit zwischen Gerechtigkeit und digitaler Ungleichheit................................................................................................................................... 17 |
| 10.  | Digitale Teilhabe und Ausschluss - zwischen Gerechtigkeit und Ungleichheit............................. 19                                                                                                                                 |
| 11.  | Digitale Souver√§nit√§t - Unabh√§ngigkeit statt Big-Tech-Abh√§ngigkeit .......................................... 21                                                                                                                           |
| 12.  | Digitale Bildung - Schl√ºssel f√ºr soziale Gerechtigkeit ................................................................... 23                                                                                                              |
| 13.  | Digitale Transformation - Mitbestimmung statt Fremdbestimmung........................................... 24                                                                                                                                |
| 14.  | Digitale Werkzeuge - Reflexion, Verantwortung und der Mensch im Mittelpunkt...................... 25                                                                                                                                       |
| 15.  | Gerechtigkeit, Transparenz und Verantwortung gestalten........................................................... 26                                                                                                                       |
| 16.  | Politische Stimme f√ºr eine gerechte Digitalisierung ..................................................................... 27                                                                                                               |
| 17.  | Soziale Arbeit - Eine Stimme f√ºr digitale Gerechtigkeit ................................................................ 29                                                                                                                |
| 18.  | Reflexionsfragen............................................................................................................................ 30                                                                                            |
| 19.  | Literaturempfehlung...................................................................................................................... 32                                                                                               |

<!-- image -->

## 1. PROJEKT KONRAD

Das Video, in dem Konrad die Fragen beantwortet, ist hier zu finden:

## KONRAD: https://youtu.be/CEh-GVhxpLk

K√ºnftig wird Konrad nicht nur bei Workshops anwesend sein und mich unterst√ºtzen, sondern auch Instagram und YouTube unsicher machen.

## Instagram: @soznetz

https://www.instagram.com/reel/DHOVzxpOtXs/?utm\_source=ig\_web\_button\_share\_sheet&amp;igsh= MzRlODBiNWFlZA==

YouTube: @sainetz

https://youtu.be/GOf9wc6ovGg?si=6J90DR93gdoXAp7v

## Wer oder was ist Konrad?

Konrad, der sprechende Kolkrabe, ist ein digitaler Avatar, den ich entwickelt habe, um Inhalte rund um Digitalisierung in der Sozialen Arbeit auf unterhaltsame, charmante und kritische Weise zu vermitteln -ohne  mich  selbst  in  den  Vordergrund  zu  stellen.  Denn  ich  m√∂chte  zwar  sichtbar  Inhalte  in  den Sozialen Medien teilen, aber nicht als Person im Rampenlicht stehen. Konrad √ºbernimmt das f√ºr mich -mit viel Charakter und einem frechen Schnabel.

Die Idee dazu entstand aus meiner Faszination f√ºr Rabenv√∂gel. Diese V√∂gel haben mich schon immer fasziniert. Sie sind extrem intelligent, sozial, verspielt -und manchmal ein bisschen frech. Genau diese Mischung  wollte  ich  nutzen.  Konrad  ist  nicht  nur  ein  visuelles  Gimmick,  sondern  verk√∂rpert  eine eigenst√§ndige  Pers√∂nlichkeit:  Er  ist  schlau,  frech,  humorvoll  und  nimmt  sich  kein  Blatt  vor  den Schnabel.

## Wie funktioniert Konrad?

Konrad kann auf zwei Arten eingesetzt werden:

1. Interaktiv mit KI (ChatGPT): In dieser Variante ist Konrad mit ChatGPT verbunden. Man kann ihm Fragen stellen oder sich mit ihm unterhalten, und er antwortet eigenst√§ndig -inklusive Bewegung und Mimik.
2. Manuell gesteuert mit Text:

Alternativ steuere ich Konrad selbst und gebe ihm vorbereitete Texte, die er dann spricht. Das eignet sich besonders f√ºr Erkl√§rvideos, Vortr√§ge oder geplante Inhalte.

Die technische Umsetzung -besonders die Sprachsynchronisation -war eine ziemliche Herausforderung. Aber der Moment, als Konrad das erste Mal selbstst√§ndig sprach, war gro√üartig. Da wusste ich: Der Aufwand hat sich gelohnt.

Wenn ihr Lust habt, mehr √ºber Konrad oder das dahinterliegende Konzept zu erfahren, freue ich mich √ºber den Austausch!

<!-- image -->

## 2. MACHT IN DER SOZIALEN ARBEIT -REFLEXION, VERANTWORTUNG UND DIGITALE EINFLUSSFAKTOREN

Macht  pr√§gt  die  Soziale  Arbeit -sichtbar,  unsichtbar  und  digital  vermittelt.  Sie  zeigt  sich  in Beziehungen  (Max  Weber),  Gemeinschaften  (Hannah  Arendt),  unsichtbaren  Strukturen  (Michel Foucault) und als Streben nach Kontrolle (Thomas Hobbes). Fachkr√§fte treffen Entscheidungen √ºber Zug√§nge, Leistungen und Dokumentationen -ihre  Macht  kann  st√§rken  oder kontrollieren.  Digitale Tools ver√§ndern diese Machtverh√§ltnisse, indem sie neue M√∂glichkeiten der Teilhabe schaffen, aber auch Kontrolle und Ausgrenzung verst√§rken.

Digitale  Systeme  (z.  B.  Fallmanagement,  Algorithmen)  sind  nie  neutral  und  beeinflussen  soziale Prozesse. Reflexion, Transparenz und ein bewusster Umgang mit Macht sind essenziell f√ºr ethisches Handeln. Klient:innen m√ºssen √ºber Entscheidungen, Datenverarbeitung und digitale Einfl√ºsse offen informiert und beteiligt werden.

- Macht pr√§gt die Soziale Arbeit -auch und besonders durch Digitalisierung.
- Macht zeigt sich in Beziehungen (Weber), Gemeinschaft (Arendt), unsichtbaren Strukturen (Foucault) und als st√§ndiges Streben nach Kontrolle (Hobbes).
- Digitale Tools k√∂nnen Macht neu verteilen, aber auch Kontrolle und Ausgrenzung verst√§rken.
- Soziale Arbeit muss eigene Macht reflektieren und digitale Macht kritisch hinterfragen.
- Macht ist in der Sozialen Arbeit immer pr√§sent -auch durch digitale Tools.
- Fachkr√§fte entscheiden √ºber Zug√§nge, Leistungen, Dokumentationen -Macht kann st√§rken oder kontrollieren.
- Digitale Systeme (z. B. Fallmanagement, Algorithmen) ver√§ndern Machtverh√§ltnisse -und sind nie neutral.
- Reflexion, Transparenz und bewusster Umgang mit Macht sind Voraussetzung f√ºr ethisches Handeln.
- Klient:innen m√ºssen √ºber Entscheidungen, Daten und digitale Einfl√ºsse offen informiert und beteiligt werden.

## Wie Johannes Herwig-Lempp es ausdr√ºckt:

'Sozialarbeiter:innen  verf√ºgen  √ºber  erhebliche  Macht.  Denn  wenn  wir  Macht  als  das  Verm√∂gen betrachten, das M√∂gliche wirklich werden zu lassen, dann verf√ºgen Sozialarbeiter:innen ganz zweifellos √ºber Macht -auch wenn sie das selbst zun√§chst nicht so wahrne hmen.'

## Kernsatz zum Mitnehmen:

Digitale Soziale Arbeit erfordert kritische Reflexion und Verantwortung im Umgang mit Macht -um Kontrolle, Teilhabe und Gerechtigkeit bewusst zu gestalten.

üìå Fallbeispiel: Ein:e Sozialarbeiter:in nutzt Social Media, um Klient:innen  zu erreichen. Doch wer kontrolliert, welche Inhalte der Algorithmus sichtbar macht?

<!-- image -->

## Zusatzinformationen

Um das komplexe Ph√§nomen "Macht" besser zu verstehen, lohnt sich ein Blick auf verschiedene bekannte Theorien:

## Hannah Arendt -Macht als kollektive Handlungskraft

Hannah Arendt betrachtet Macht als etwas, das in der Gemeinschaft entsteht. Macht entsteht f√ºr Arendt dort, wo Menschen gemeinsam handeln, sich zusammenschlie√üen und im Einvernehmen Entscheidungen treffen.

'Macht entspringt der menschlichen F√§higkeit, nicht nur zu handeln oder etwas zu tun, sondern sich mit anderen zusammenzuschlie√üen und im Einvernehmen mit ihnen zu handeln. √úber Macht verf√ºgt niemals ein Einzelner, sie ist im Besitz einer Gruppe und bleibt nur so lange existent, als die Gruppe zusammenh√§lt.'

Das bedeutet: Macht kann positive Gestaltungskraft sein, wenn Menschen gemeinsam handeln. F√ºr  die  Soziale  Arbeit  ist  dieses  Verst√§ndnis  wichtig,  weil  es  die  Idee  der  Partizipation  und Selbsterm√§chtigung  (Empowerment)  von  Klient:innen  und  Fachkr√§ften  betont.  Digitale  Tools k√∂nnten also auch neue R√§ume von gemeinschaftlicher Macht schaffen -wenn sie transparent und partizipativ genutzt werden.

## Max Weber -Macht als Durchsetzungsverm√∂gen in sozialen Beziehungen

Max Weber hingegen definiert Macht aus einer anderen Perspektive:

'Macht bedeutet jede Chance, innerhalb einer sozialen Beziehung den eigenen Willen auch gegen Widerstreben durchzusetzen, gleichviel worauf diese Chance beruht.'

Hier wird Macht verstanden als die F√§higkeit, sich durchzusetzen, auch wenn andere dagegen sind. Macht zeigt sich bei Weber also vor allem in asymmetrischen Beziehungen, in denen eine Person oder Institution mehr Einfluss hat als andere.

F√ºr die Soziale Arbeit bedeutet das: Fachkr√§fte befinden sich oft in einer Machtposition gegen√ºber Klient:innen,  weil  sie  √ºber  Ressourcen,  Zug√§nge  zu  Hilfen  oder  Bewertungen  von  Situationen entscheiden. Durch digitale Systeme (z. B. automatisierte Entscheidungen) kann diese Form der Macht  unsichtbarer,  aber  auch  massiver  werden,  wenn  etwa  Algorithmen  entscheiden,  wer Unterst√ºtzung bekommt.

## Michel Foucault -Macht durch Diskurse und Strukturen

Michel Foucault erweitert den Machtbegriff um eine ganz zentrale Perspektive: Macht ist nicht nur sichtbar als Befehl oder Durchsetzung, sondern wirkt unsichtbar √ºber Normen, Sprache, Wissen und gesellschaftliche Strukturen.

Macht 'durchdringt die gesamte Gesellschaft' und ist 'in allt√§glichen Praktiken und Denkweisen verankert' .

Foucault spricht von der Mikrophysik der Macht: Macht zeigt sich auch in kleinen Routinen, in Sprache, in institutionellen Abl√§ufen -und in den unsichtbaren Regeln, nach denen entschieden wird, was normal und was abweichend ist.

<!-- image -->

F√ºr die Digitalisierung bedeutet das: Algorithmen, digitale Plattformen und Dokumentationssysteme sind nicht neutral, sondern tragen diese gesellschaftlichen Normen und Machtstrukturen in sich. Wenn z. B. eine KI entscheidet, ob jemand Hilfe bekommt, dann ist diese Entscheidung von den Vorstellungen der Entwickler:innen und den eingegebenen Daten gepr√§gt -und nicht objektiv. Macht zeigt sich hier subtil, aber wirkm√§chtig.

## Thomas Hobbes -Macht als permanentes Streben nach mehr

Thomas  Hobbes  hat  ein  sehr  pessimistisches  Bild  von  Macht  und  menschlicher  Natur.  F√ºr  ihn streben Menschen st√§ndig nach mehr Macht, weil sie sich selbst absichern und sch√ºtzen wollen.

'So halte ich an erster Stelle ein fortw√§hrendes und rastloses Verlangen nach immer neuer Macht f√ºr einen allgemeinen Trieb der gesamten Menschheit, der nur mit dem Tode endet.'

Nach  Hobbes  geht  es  bei  Macht  also  um  Konkurrenz  und  Dominanz.  In  einer  Welt,  die  immer unsicher bleibt, versuchen Menschen, ihre Macht zu sichern -notfalls auf Kosten anderer.

Auch in der Digitalisierung l√§sst sich dieses Denken wiederfinden: Tech-Konzerne konkurrieren um Daten und Einfluss, Staaten nutzen digitale √úberwachung, um Kontrolle auszu√ºben. F√ºr die Soziale Arbeit bedeutet das: Es braucht bewusste Auseinandersetzung mit Machtfragen, um zu verhindern, dass Klient:innen durch digitale Kontrolle weiter marginalisiert werden.

<!-- image -->

## 3. DIGITALE MACHT -VERST√ÑRKUNG VON UNGLEICHHEITEN ODER NEUE HANDLUNGSSPIELR√ÑUME?

Digitale  Tools  und  Algorithmen  ver√§ndern  Machtverh√§ltnisse  in  der  Sozialen  Arbeit.  Sie  k√∂nnen bestehende Ungleichheiten verst√§rken, indem sie Personen mit geringeren Ressourcenausschlie√üen, Kontrolle in die H√§nde von Institutionen und Unternehmen legen und Fachkr√§ften sowie Klient:innen Handlungsspielr√§ume  nehmen.  √úberwachung,  Zugangsh√ºrden  und  algorithmische  Entscheidungen bleiben oft unsichtbar, beeinflussen jedoch Teilhabe und Gerechtigkeit.

K√ºnstliche Intelligenz und digitale Systeme entziehen Fachkr√§ften und Klient:innen mitunter Einfluss, w√§hrend Verantwortung unklar bleibt. Algorithmen k√∂nnen Diskriminierung verst√§rken, da sie mit voreingenommenen Daten arbeiten. Soziale Arbeit muss Digitalisierung kritisch begleiten, Transparenz und Widerspruchsm√∂glichkeiten einfordern sowie Alternativen offenhalten.

- Digitale Tools verst√§rken oft bestehende Ungleichheiten und Machtverh√§ltnisse.
- Algorithmen, √úberwachung, Zugangsh√ºrden -wer wenig Ressourcen hat, wird ausgeschlossen.
- Institutionen &amp; Unternehmen gewinnen an Kontrolle, Fachkr√§fte und Klient:innen verlieren Handlungsspielr√§ume.
- Digitale  Tools  und  Algorithmen  schaffen  neue,  oft  unsichtbare  Formen  von  Macht  und Kontrolle in der Sozialen Arbeit.
- Entscheidungen durch KI entziehen Fachkr√§ften und Klient:innen Einfluss -Verantwortung bleibt unklar.
- √úberwachung  durch  Apps  und  Systeme  schr√§nkt  Freiheit  und  Selbstbestimmung  von Klient:innen und Fachkr√§ften ein.
- Algorithmen verst√§rken Diskriminierung, weil sie mit voreingenommenen Daten arbeiten.
- √úberwachung  durch  Apps  und  Systeme  schr√§nkt  Freiheit  und  Selbstbestimmung  von Klient:innen und Fachkr√§ften ein.
- Algorithmen verst√§rken Diskriminierung, weil sie mit voreingenommenen Daten arbeiten.

## Kernsatz zum Mitnehmen:

Digitale  Macht  ist  oft  unsichtbar -Soziale  Arbeit  muss  sie  erkennen,  kritisch  hinterfragen  und  f√ºr Teilhabe, Gerechtigkeit und Selbstbestimmung eintreten.

<!-- image -->

üìå Fallbeispiel: Ein Jobcenter verwendet ein automatisiertes Scoring-System, das entscheidet, wer F√∂rderungen erh√§lt. Personen mit L√ºcken im Lebenslauf werden systematisch benachteiligt.

<!-- image -->

## Was ist neu an digitaler Macht?

Macht wird nicht mehr nur sichtbar von Person zu Person ausge√ºbt (z. B. durch Anweisungen), sondern zunehmend √ºber digitale Systeme, Algorithmen und Plattformen. Diese Systeme wirken auf  eine  Weise,  die  f√ºr  Betroffene  oft  nicht  nachvollziehbar  ist.  Entscheidungen  werden  von Maschinen vorbereitet oder getroffen, ohne dass die dahinterliegenden Prozesse transparent sind.

Digitale Macht ist dadurch oft unsichtbar, indirekt und schwer kontrollierbar.

## Beispiel f√ºr neue digitale Machtaus√ºbung

KI-basierte Tools und Scoring-Modelle bewerten automatisch, ob eine Person "hilfebed√ºrftig" ist oder welches Risiko in einer bestimmten  Situation besteht (z. B.  Kindeswohlgef√§hrdung, Schuldner:innen-Beratung). Diese Systeme entscheiden oft √ºber Leistungen, Unterst√ºtzungen oder Ma√ünahmen, bevor eine Fachkraft √ºberhaupt mit der Person gesprochen hat. Die Fachkraft steht dann unter Druck, die "Vorschl√§ge" der Maschine zu √ºbernehmen, auch wenn diese problematisch erscheinen.

## Machtasymmetrie

Die Entscheidungsmacht wird vom Menschen zur Maschine verschoben, die Klient:innen werden Objekt der Bewertung, ohne selbst Einfluss zu haben.

Besonders problematisch wird das, wenn diese Macht nicht offen ausge√ºbt, sondern verinnerlicht wird -also  internalisiert,  ohne  dass  die  betroffenen  Menschen  sich  dessen  bewusst  sind. Diese unsichtbare Form von Kontrolle betrifft Klient:innen genauso wie Fachkr√§fte.

## Was bedeutet "Internalisierung von Machtbeziehungen"?

Internalisierung hei√üt: Menschen √ºbernehmen √§u√üere Erwartungen, Regeln und Bewertungen und machen sie zu ihren eigenen.

## In Bezug auf digitale Systeme bedeutet das:

- Menschen  beginnen,  sich  so  zu  verhalten,  wie  es  die  digitalen  Vorgaben  und  Systeme erwarten.
- Sie  passen  ihr  Denken  und  Handeln  an  unsichtbare Regeln  der  Technik  an -auch  ohne direkten Zwang.

Folge: Kontrolle wirkt nicht mehr offen von au√üen, sondern wirkt von innen -√ºber die Anpassung an digitale Normen.

## Beispiele aus der Praxis

- Eine App zur Schuldenregulierung erinnert Klient:innen t√§glich an Sparziele -auch wenn diese aktuell psychisch belastet sind. Die Person f√ºhlt sich als Versager:in, weil sie das Ziel nicht erreicht.
- Ein Algorithmus stuft eine Mutter als Risikofall ein -obwohl die Fachkraft das nicht so sieht. Trotzdem  werden  "pr√§ventive  Ma√ünahmen"  eingeleitet,  um  "auf  Nummer  sicher  zu gehen".
- Ein Fallmanagementsystem gibt vor, welche Kategorien dokumentiert werden m√ºssen -die Fachkraft richtet sich nach diesen Vorgaben, auch wenn sie der Meinung ist, dass andere Aspekte wichtiger w√§ren.

## Zusatzinformationen

<!-- image -->

## 4. DATENMACHT -KONTROLLE, TRANSPARENZ UND DIE RECHTE VON KLIENT:INNEN

Daten sind Macht: Sie entscheiden dar√ºber, wer Unterst√ºtzung erh√§lt und wie Menschen in digitalen Systemen  wahrgenommen  werden.  Klient:innen  sollten  selbst  bestimmen,  was  mit  ihren  Daten geschieht, doch digitale Systeme untergraben diese Kontrolle. Daten werden oft ohne Wissen oder Zustimmung gesammelt, geteilt und durch Algorithmen ausgewertet -mit Risiken wie Diskriminierung, kommerziellen Interessen und Intransparenz.

Fachkr√§fte stehen im Spannungsfeld zwischen Dokumentationspflicht, institutionellen Anforderungen und ihrer Verantwortung gegen√ºber Klient:innen. Ohne Datenhoheit werden Menschen zu Objekten der Kontrolle statt zu Subjekten mit Rechten. Soziale Arbeit muss daher Transparenz, Mitbestimmung und  einen  kritischen  Umgang  mit  Daten  sicherstellen,  um  digitale  Macht  sichtbar  zu  machen  und Missbrauch zu begrenzen.

- Datenhoheit bedeutet: Klient:innen entscheiden selbst, was mit ihren Daten passiert.
- Digitale Systeme gef√§hrden diese Kontrolle -Daten werden oft ohne Wissen oder Zustimmung gesammelt, geteilt und ausgewertet.
- Fachkr√§fte stehen zwischen Dokumentationspflicht und Verantwortung gegen√ºber Klient:innen.
- Ohne Datenhoheit werden Menschen zu Objekten von Kontrolle statt Subjekte mit Rechten.
- Daten sind Macht: Sie bestimmen, wer Hilfe bekommt -oft ohne Wissen oder Einfluss der Klient:innen.
- Algorithmen  und  Big  Tech  entscheiden  mit -auf  Basis  undurchsichtiger  Datenfl√ºsse  und kommerzieller Interessen.
- Gefahr von Diskriminierung: Algorithmen √ºbernehmen Vorurteile aus alten Daten -Menschen werden pauschal bewertet.
- Fachkr√§fte verlieren Einfluss  und  werden zu Datensammler:innen -statt  Anw√§lt:innen  der Klient:innen.

## Kernsatz zum Mitnehmen:

Daten entscheiden √ºber Menschen -Soziale Arbeit muss Datenmacht sichtbar machen, Begrenzungen einfordern und die Rechte der Klient:innen sch√ºtzen.

üìå Fallbeispiel: Eine  obdachlose  Person  beantragt  Unterst√ºtzung,  doch  ihre  Daten  werden  mit anderen Beh√∂rden geteilt -ohne ihr Wissen. Dadurch wird sie pl√∂tzlich von der Polizei √ºberwacht.

<!-- image -->

üìå Fallbeispiel: Ein:e  Sozialarbeiter:in  arbeitet  mit  einem  digitalen  Fallmanagementsystem,  das automatisch  Risikobewertungen  erstellt  (z.  B.  bei  Kindeswohlgef√§hrdung).  Die  Fachkraft  muss  sich bewusst machen, dass die Entscheidung √ºber Ma√ünahmen nicht nur von ihr selbst, sondern auch von diesem System beeinflusst wird. Gleichzeitig muss sie sich fragen: Wie kann ich die betroffene Familie transparent dar√ºber informieren, dass ein System diese Einsch√§tzung gemacht hat? Und: Wie kann ich

<!-- image -->

sicherstellen,  dass  ich  mich  nicht  blind  auf  die  Technik  verlasse,  sondern  meine  professionelle Einsch√§tzung einbringe?

## Zusatzinformationen

Immer mehr zentrale Infrastrukturen des √∂ffentlichen Lebens -von Verwaltung √ºber Bildung bis zur  Sozialen  Arbeit -laufen  √ºber  digitale  Systeme.  Doch  diese  Systeme  werden  oft  nicht  von √∂ffentlichen Stellen, sondern von privaten  Konzernen bereitgestellt. Damit entsteht eine gef√§hrliche Abh√§ngigkeit, die nicht nur die Soziale Arbeit, sondern die gesamte Gesellschaft betrifft.

## Was ist mit "gesellschaftlicher Infrastruktur" gemeint?

Gesellschaftliche  Infrastruktur  umfasst alle Grundlagen, die  eine  Gesellschaft  am Laufen  halten, beispielsweise:

- Kommunikationssysteme.
- Plattformen f√ºr Bildung und Beratung.
- Software f√ºr Verwaltungen, Schulen, Sozialdienste.
- Cloud-Systeme, auf denen Daten gespeichert werden.
- Digitale Tools f√ºr den Zugang zu Sozialleistungen.

Digitale Infrastruktur wird damit Teil des √∂ffentlichen Lebens und der sozialen Versorgung.

## Wie kontrollieren Konzerne diese Infrastruktur?

Gro√üe Tech-Konzerne wie Microsoft, Amazon, Google, Meta, SAP und andere stellen die wichtigsten digitalen Werkzeuge zur Verf√ºgung:

- Cloud-Dienste (z.  B.  Amazon  AWS, Microsoft  Azure),  auf  denen  Sozialdaten  gespeichert werden.
- Kommunikationsplattformen wie Microsoft Teams, Zoom oder WhatsApp, die in Sozialer Arbeit genutzt werden.
- Verwaltungssoftware und Fallmanagementsysteme, die von privaten Anbietern programmiert und kontrolliert werden.
- KI-Systeme und Algorithmen, die √ºber die Vergabe von Sozialleistungen oder F√∂rderungen mitentscheiden.

Folge: Wenige private Unternehmen bestimmen, wie √∂ffentliche Aufgaben technisch umgesetzt werden -und damit auch, wie Menschen Zugang zu Hilfe und Teilhabe bekommen.

## Was bedeutet "staatliche digitale Souver√§nit√§t"?

- Die F√§higkeit eines Staates, seine digitalen Infrastrukturen und Daten selbst zu kontrollieren.
- Unabh√§ngigkeit von kommerziellen Anbietern, um gesellschaftliche Aufgaben zu erf√ºllen.
- Selbstbestimmung  √ºber  technische  Standards,  Datensicherheit  und  den  Schutz  von B√ºrger:innen.

Ohne digitale Souver√§nit√§t sind Staat und Soziale Arbeit abh√§ngig von privaten Anbietern, die ihre eigenen Interessen verfolgen.

## Wo fehlen heute staatliche digitale Infrastrukturen?

- Sozialverwaltungen nutzen private Software, z. B. f√ºr Fallmanagement und Antragsbearbeitung.

<!-- image -->

- Kommunikation zwischen Beh√∂rden und Klient:innen l√§uft √ºber kommerzielle Plattformen (z. B. Microsoft Teams).
- Cloud-Dienste, auf denen Sozialdaten gespeichert werden, geh√∂ren zu Amazon (AWS) oder Microsoft (Azure).
- Schulen und Bildungseinrichtungen arbeiten mit Google Classroom oder Microsoft 365 -auch dort werden Sozialdaten verarbeitet.

Folge: Kernaufgaben des Staates laufen auf Systemen, die der Staat nicht kontrolliert.

Die Art und Weise, wie Digitalisierung aktuell vorangetrieben wird, ist oft alles andere als gerecht:

- Private Konzerne bestimmen die Regeln.
- Algorithmen und Systeme entscheiden √ºber Menschen, ohne dass diese mitreden k√∂nnen.
- Viele  Menschen  werden  von  digitalen  Angeboten  ausgeschlossen,  weil  ihnen  Wissen, Technik oder Zug√§nge fehlen.

Deshalb  braucht  es  eine  klare  gesellschaftliche  und  soziale  Vision: Eine  faire,  inklusive  und demokratische Digitalisierung, die allen Menschen Teilhabe erm√∂glicht und soziale Gerechtigkeit in den Mittelpunkt stellt.

<!-- image -->

## 5. DIGITALE √úBERWACHUNG -ZWISCHEN KONTROLLE UND VERTRAUEN

Digitale  Systeme  und  Algorithmen  √ºberwachen  zunehmend  Klient:innen  und  Fachkr√§fte -oft unsichtbar,  aber  mit  weitreichenden  Folgen.  Predictive  Analytics  und  Scoring-Systeme  bewerten Menschen automatisiert, h√§ufig ohne Einzelfallpr√ºfung oder Mitspracherecht. Klient:innen werden 'gl√§sern',  verlieren  Privatsph√§re  und  Kontrolle  √ºber  ihre  Daten,  w√§hrend  Fachkr√§fte  unter  Druck stehen, alles zu dokumentieren. Vertrauen und Beziehung geraten dabei in den Hintergrund.

Digitale √úberwachung birgt Risiken wie Stigmatisierung, Diskriminierung und den Verlust individueller Einsch√§tzungen. Fachkr√§fte werden von Unterst√ºtzer:innen zu Kontrollinstanzen degradiert. Soziale Arbeit muss sich gegen maschinelle Vorverurteilungen einsetzen, Entscheidungsspielr√§ume verteidigen und Schutzr√§ume statt √úberwachung fordern.

- Digitale Tools wie Apps, digitale Fallmanagementsysteme  oder  Plattformen erlauben es Institutionen,  das  Verhalten  von  Klient:innen  in  Echtzeit  zu  √ºberwachen  (z.  B.  ob  Termine eingehalten werden, ob "Verhaltensvorgaben" erf√ºllt sind).
- Auch Fachkr√§fte werden durch digitale Systeme selbst st√§rker √ºberwacht (z. B. Dokumentationspflichten, automatische Arbeitszeiterfassung, Tracking von T√§tigkeiten).

Machtaus√ºbung: Kontrolle findet st√§ndig und l√ºckenlos statt, ohne dass die betroffenen Menschen noch Einfluss auf die Daten haben.

- Digitale  Systeme  f√ºhren  zu  neuer  √úberwachung  von  Klient:innen  und  Fachkr√§ften -oft unsichtbar und umfassend.
- Klient:innen werden "gl√§sern", verlieren Privatsph√§re und Kontrolle √ºber ihre Daten.
- Fachkr√§fte geraten unter Druck, alles zu dokumentieren -Beziehung und Vertrauen bleiben auf der Strecke.
- Predictive  Analytics  und  Scoring-Systeme  bewerten  Menschen  automatisiert -oft  ohne Einzelfallpr√ºfung.
- Digitale  √úberwachung  sammelt  Daten  √ºber  Klient:innen -meist  ohne  deren  Wissen  oder Kontrolle.
- Risiken: Stigmatisierung, Diskriminierung, Verlust von Individualit√§t und Mitsprache.
- Fachkr√§fte verlieren Entscheidungsspielraum, werden zu Kontrollinstanzen statt Helfer:innen.

## Kernsatz zum Mitnehmen:

Soziale Arbeit braucht Vertrauen statt √úberwachung -f√ºr Klient:innen und Fachkr√§fte, gegen digitale Kontrolle und maschinelle Vorverurteilung.

<!-- image -->

üìå Fallbeispiel: Ein  Jugendwohnheim  installiert  Kameras  zur  'Sicherheit'.  Jugendliche  f√ºhlen  sich permanent beobachtet und ver√§ndern ihr Verhalten.

<!-- image -->

üìå Fallbeispiel: Apps,  die  Jobcenter-Klient:innen  zur  "Selbstoptimierung"  nutzen  sollen,  inklusive Ortungsfunktion oder automatischer Daten√ºbermittlung.

<!-- image -->

## Was sind Scoring-Modelle?

Scoring-Modelle bewerten Menschen anhand von Punkten, die automatisch berechnet werden. Grundlage sind verschiedene Merkmale, z. B.:

- Einkommen, Alter, Herkunft.
- Wohnort, Schulbildung, fr√ºhere Hilfeleistungen.
- Verhaltensdaten aus digitalen Akten.

Ergebnis: Ein numerischer Wert (Score), der angibt, wie "riskant", "hilfebed√ºrftig" oder "f√∂rderw√ºrdig" eine Person ist.

## Was sind automatisierte Risikobewertungen?

Automatisierte Risikobewertungen nutzen Algorithmen, um die Wahrscheinlichkeit von bestimmten Ereignissen oder Problemen vorherzusagen, z. B.:

- Kindeswohlgef√§hrdung.
- Langzeitarbeitslosigkeit.
- Wohnungsverlust.

Die  Systeme  berechnen  aus  den  vorhandenen  Daten,  wie  hoch  das  Risiko  ist -und  geben Fachkr√§ften Handlungsvorschl√§ge (z. B. "dringender Handlungsbedarf").

## Wie wirken diese Instrumente zusammen?

- Scoring-Modelle und Risikobewertungen entscheiden, wer als "Problemfall" gilt und wie viel Unterst√ºtzung jemand bekommt.
- Digital Nudging  sorgt daf√ºr, dass Klient:innen  und  Fachkr√§fte  die  "vorgesehenen" Ma√ünahmen umsetzen.
- Das  f√ºhrt  zu  einer  unsichtbaren  Steuerung  von  Verhalten  und  Entscheidungen -ohne offene Aushandlung oder Reflexion.

Gefahr: Entscheidungen wirken "objektiv", sind aber von Software und Datenlogik vorgegeben -nicht vom echten Bedarf der Menschen.

## Beispiele aus der Praxis

- Jugendhilfe: Ein Algorithmus schl√§gt "hohes Risiko" vor, weil eine Mutter alleinstehend ist und in einem bestimmten Stadtteil wohnt -obwohl die Fachkraft die Situation als stabil einsch√§tzt.
- Jobcenter:  Ein  Klient  erh√§lt  nur  noch  ein  "Online-Coaching",  weil  sein  Score  "geringe F√∂rderf√§higkeit" anzeigt -obwohl pers√∂nliche Beratung dringend notwendig w√§re.
- Schuldnerberatung: Menschen werden nach einem automatisierten Risikomodell vorsortiert -wer nicht "dringlich genug" erscheint, bekommt erst sp√§t einen Termin.

## Zusatzinformationen

<!-- image -->

## 6. DIGITALE STEUERUNG -ZWISCHEN SELBSTBESTIMMUNG UND UNSICHTBARER LENKUNG

Digitale  Tools,  Algorithmen  und  Scoring-Modelle  beeinflussen  das  Verhalten  von  Klient:innen  und Fachkr√§ften -oft unsichtbar und ohne Mitbestimmung. Digital Nudging, automatisierte Risikobewertungen  und  standardisierte  Vorgaben  lenken  Entscheidungen,  ohne  dass  Alternativen erkennbar sind. Dadurch verlieren Klient:innen Selbstbestimmung und Fachkr√§fte ihre professionelle Autonomie.

Die  Gefahr: Menschen  werden  auf  Datenprofile  reduziert,  w√§hrend  individuelle  Bed√ºrfnisse  und Beziehungen  in  den  Hintergrund  geraten.  Intransparente  Entscheidungen  und  algorithmische Verzerrungen (Bias) verst√§rken Ungleichheiten und Vorurteile. Soziale Arbeit muss digitale Steuerung erkennen, hinterfragen und f√ºr Transparenz, Mitbestimmung und individuelle L√∂sungen eintreten.

- Digitale Tools steuern Verhalten von Klient:innen und Fachkr√§ften -oft unsichtbar und ohne Mitbestimmung.
- Algorithmen,  Vorgaben  und  Apps  lenken  Entscheidungen,  ohne  dass  Alternativen  sichtbar sind.
- Verlust  von  Selbstbestimmung f√ºr Klient:innen, Entwertung professioneller  Einsch√§tzungen von Fachkr√§ften.
- Scoring, Risikobewertungen  und Digital Nudging  steuern Klient:innen und Fachkr√§fte unsichtbar -oft ohne Mitbestimmung.
- Gefahr:  Menschen  werden  zu  Datenprofilen,  individuelle  Bed√ºrfnisse  und  Beziehungen geraten in den Hintergrund.
- Probleme:  Intransparente  Entscheidungen,  Verst√§rkung  von  Vorurteilen  (Bias),  Verlust  an Selbstbestimmung und Handlungsspielraum.

## Kernsatz zum Mitnehmen:

Soziale  Arbeit  braucht  Beziehung  statt  Algorithmus -digitale  Tools  d√ºrfen  nicht  steuern,  sondern m√ºssen unterst√ºtzen.

üìå Fallbeispiel: Eine App zur Suchtpr√§vention schl√§gt basierend auf Nutzungsdaten Therapieoptionen vor. Doch wer entscheidet, welche Methoden bevorzugt werden?

## Zusatzinformationen

## Was ist "Digital Nudging"?

"Nudging" bedeutet, Menschen durch subtile Anst√∂√üe ("Schubser") zu einem bestimmten Verhalten zu bewegen, ohne Zwang.

## Digital Nudging passiert in digitalen Anwendungen, z. B.:

- Voreinstellungen in Apps, die ein bestimmtes Verhalten "empfehlen".
- Pop-up-Nachrichten, die an "erw√ºnschte" Handlungen erinnern.
- Fortschrittsanzeigen, die motivieren, Ma√ünahmen abzuschlie√üen.

<!-- image -->

Ziel: Menschen sollen sich "richtig" verhalten, z. B. Antr√§ge p√ºnktlich stellen, an Programmen teilnehmen, Regeln einhalten -ohne bewusst dar√ºber zu entscheiden.

## Steuerung von Fachkr√§ften

- ‚Üí Fallmanagement-Systeme legen vor, welche Felder ausgef√ºllt werden m√ºssen.
- ‚Üí Vorgegebene Abl√§ufe und Checklisten bestimmen die Fallbearbeitung.
- ‚Üí Algorithmische Risikoeinsch√§tzungen beeinflussen die Wahrnehmung von Klient:innen: Wer als "Risiko" eingestuft wird, bekommt mehr Kontrolle -wer "unproblematisch" erscheint, weniger Aufmerksamkeit.

Beispiel: Eine Fachkraft bekommt eine "dringliche Empfehlung" zur Kindeswohlgef√§hrdung, weil ein Algorithmus ein Risiko errechnet. Obwohl die Fachkraft pers√∂nlich anders einsch√§tzt, f√ºhlt sie sich gezwungen, der Empfehlung zu folgen, um sich rechtlich abzusichern.

Folge: Fachkr√§fte  werden  zu  Umsetzer:innen  algorithmischer  Vorgaben -ihre  professionelle Einsch√§tzung verliert an Bedeutung.

## Warum ist diese unsichtbare Steuerung problematisch?

## Verlust von Selbstbestimmung

- Klient:innen werden in bestimmte Verhaltensmuster gedr√§ngt, statt frei Entscheidungen zu treffen.
- Fachkr√§fte  handeln  nach  "systemkonformen"  Vorgaben,  statt  sich  an  den  individuellen Bed√ºrfnissen der Menschen zu orientieren.

## Verst√§rkung von Kontrolle statt Unterst√ºtzung

- Digitale  Tools  werden  zu  Instrumenten  der  Verhaltens√ºberwachung,  wenn  Klient:innen st√§ndig an Pflichten erinnert oder sanktioniert werden.
- Die Unterst√ºtzung r√ºckt in den Hintergrund -es  geht  mehr  um  Anpassung  an Systemlogiken.

## Machtverschiebung zu den Systementwickler:innen

- Nicht mehr die Fachkraft oder die Klient:in entscheidet, sondern die Programmierung der Software legt fest, was als "richtiges Verhalten" gilt.
- Die  Menschen  hinter  den  Systemen  (Softwareentwickler:innen,  Verwaltung)  haben  die Macht, Normen und Erwartungen festzulegen.

Frage: Wer  entscheidet  dann,  was  ein  "gutes  Leben"  ist -die  Klient:in,  die  Fachkraft,  oder  ein Algorithmus?

<!-- image -->

## 7. DIGITALE SYSTEME UND SELBSTBESTIMMUNG -TECHNIK BEEINFLUSST WAHRNEHMUNG, VERTRAUEN UND AUTONOMIE

Digitale Systeme beeinflussen unsichtbar das Denken und Handeln von Klient:innen und Fachkr√§ften. Algorithmen, Scores und automatisierte Bewertungen formen Selbstbilder, verst√§rken Kontrolle und verfestigen Vorurteile. Die Gefahr: Menschen √ºbernehmen technische Einsch√§tzungen als Wahrheit ('Problemfall'), verlieren Vertrauen in sich selbst und werden zunehmend fremdgesteuert.

Auch Fachkr√§fte geraten unter Druck, Systemvorgaben zu folgen, anstatt individuelle Einsch√§tzungen vorzunehmen. Soziale Arbeit muss sich aktiv gegen diese unsichtbare Macht stellen: Selbstwahrnehmung und Autonomie st√§rken, Vertrauen aufbauen, eigene Fachlichkeit bewahren und Technik kritisch hinterfragen.

- Digitale  Systeme  beeinflussen  unsichtbar  das  Denken  und  Handeln  von  Klient:innen  und Fachkr√§ften.
- Gefahr:  Menschen  √ºbernehmen  technische  Bewertungen  (Scores,  Risiken)  als  Selbstbild -Fachkr√§fte folgen Systemvorgaben statt eigener Einsch√§tzung.
- Folge: Verlust von Selbstbestimmung, St√§rkung von Kontrolle, Verfestigung von Vorurteilen.
- Digitale Systeme beeinflussen Selbstwahrnehmung, Vertrauen und Autonomie von Klient:innen und Fachkr√§ften.
- Gefahr:  Menschen  √ºbernehmen  Bewertungen  von  Algorithmen  ("Problemfall"),  verlieren Vertrauen in sich und die Fachkr√§fte, und werden fremdgesteuert.
- Fachkr√§fte geraten unter Druck, den Systemvorgaben zu folgen, statt individuelle Wege zu gehen.

## Kernsatz zum Mitnehmen:

Digitale  Systeme  d√ºrfen  nicht  bestimmen,  wer  wir  sind -Soziale  Arbeit  muss  Selbstbestimmung, Vertrauen und Autonomie sch√ºtzen.

<!-- image -->

üìå Fallbeispiel: Ein  KI-gest√ºtztes  Chat-Tool hilft  Klient:innen  bei  Antr√§gen.  Einige  vertrauen der KI mehr als den Sozialarbeiter:innen -aber die KI macht Fehler.

## Zusatzinformationen

Selbstwahrnehmung: Wenn digitale Systeme bestimmen, wer ich bin

Digitale Systeme wie Scoring-Modelle, Fallmanagement-Software oder Risiko-Algorithmen bewerten Menschen anhand von Daten.

## Was passiert, wenn mir eine Software "sagt", wer ich bin?

- Menschen √ºbernehmen die Bewertungen, die ein System √ºber sie erstellt:
- ‚Üí "Ich bin ein Hochrisikofall."
- ‚Üí "Ich gelte als nicht f√∂rderf√§hig."
- Die eigene Lebensgeschichte wird auf Zahlen und Kategorien reduziert:
- ‚Üí "Ich bin eine Zahl im System."

<!-- image -->

- Wer st√§ndig erinnert wird, was noch "zu erledigen" ist (durch Apps, automatische Mails), f√ºhlt sich dauerhaft ungen√ºgend oder kontrolliert.

Folge: Menschen verlieren das Vertrauen in die eigene Wahrnehmung und ihre eigene Geschichte. Sie sehen sich so, wie die Technik sie einordnet.

## Autonomie: Wenn Technik Entscheidungen vorgibt

Digitale Tools k√∂nnen  den Handlungsspielraum von Klient:innen und Fachkr√§ften massiv einschr√§nken.

## F√ºr Klient:innen:

- Wenn ein System vorgibt, welche Ma√ünahme "am besten passt", k√∂nnen Klient:innen kaum mitentscheiden.
- Wenn Apps oder digitale Plattformen st√§ndige Aufgaben und Ziele setzen, bleibt wenig Raum f√ºr individuelle Wege.
- Wenn Klient:innen ein schlechtes Scoring bekommen, wird ihnen vielleicht gar nicht mehr die Hilfe angeboten, die sie brauchen.

## F√ºr Fachkr√§fte:

- Wenn Systeme standardisierte Abl√§ufe vorschreiben, k√∂nnen Fachkr√§fte weniger flexibel auf individuelle Situationen eingehen.
- Eigene  Einsch√§tzungen  geraten  in  den  Hintergrund,  wenn  sie  nicht  zu  den  digitalen Vorgaben passen.

Folge: Menschen  verlieren  die  Autonomie,  √ºber  ihr  eigenes  Leben  und  ihre  Unterst√ºtzung mitzubestimmen.  Fachkr√§fte  verlieren  die  Autonomie,  nach  professioneller  Einsch√§tzung  zu handeln.

<!-- image -->

## 8. K√úNSTLICHE INTELLIGENZ -TRANSPARENZ, GERECHTIGKEIT UND MENSCHLICHE ENTSCHEIDUNGSMACHT

K√ºnstliche Intelligenz beeinflusst zunehmend Entscheidungen in Sozialverwaltungen und Fallmanagement.  Was  fr√ºher  Fachkr√§fte  entschieden,  √ºbernehmen  heute  Algorithmen -oft intransparent  und  voller  Vorurteile  (Bias).  Klient:innen  verlieren  Einfluss,  w√§hrend  Diskriminierung durch automatisierte Prozesse verst√§rkt wird. Fachkr√§fte stehen unter Druck, den KI-Empfehlungen zu folgen, wodurch ihre eigene Fachlichkeit entwertet wird.

KI  ist  nicht  neutral:  Sie  reproduziert gesellschaftliche Ungleichheiten und trifft Entscheidungen, die schwer anfechtbar sind. Soziale Arbeit muss  sich f√ºr Transparenz,  Widerspruchsrechte  und menschliche  Entscheidungsmacht  einsetzen.  Technik  darf  nicht  √ºber  Menschenrechte  stehen -Gerechtigkeit und Teilhabe m√ºssen auch im digitalen Raum sichergestellt bleiben.

- KI  trifft  Entscheidungen,  die  fr√ºher  Fachkr√§fte  getroffen  haben -Klient:innen  verlieren Einfluss.
- Algorithmen sind nicht neutral -sie verst√§rken Diskriminierung und treffen intransparente, kaum anfechtbare Entscheidungen.
- Fachkr√§fte geraten unter Druck, den "Vorschl√§gen" der KI zu folgen -eigene Einsch√§tzungen werden entwertet.
- KI und Algorithmen entscheiden zunehmend mit -doch sie sind oft intransparent und voller Vorurteile (Bias).
- Diskriminierung wird automatisiert: Alte Vorurteile aus Daten stecken in den Systemen und treffen besonders benachteiligte Gruppen.
- Fachkr√§fte und Klient:innen wissen oft nicht, wie Entscheidungen zustande kommen -und k√∂nnen sich kaum wehren.
- Technik darf nicht √ºber Menschenrechte stehen -Gerechtigkeit und Teilhabe m√ºssen auch digital gesichert bleiben.

## Kernsatz zum Mitnehmen:

KI darf nicht √ºber Menschen entscheiden -Soziale Arbeit muss Transparenz fordern, Diskriminierung bek√§mpfen und Fachlichkeit verteidigen.

<!-- image -->

üìå Fallbeispiel: Eine KI soll Sozialhilfeantr√§ge bewerten. Schwarze und migrantische Antragsteller:innen werden systematisch schlechter eingestuft.

<!-- image -->

## 9. DISKRIMINIERUNG DURCH ALGORITHMEN -SOZIALE ARBEIT ZWISCHEN GERECHTIGKEIT UND DIGITALER UNGLEICHHEIT

Algorithmen  und  digitale  Technologien  sind  nicht  neutral -sie  √ºbernehmen  und  verst√§rken gesellschaftliche  Vorurteile.  Automatisierte  Systeme  wie  Scoring-Modelle  und  Gesichtserkennung stufen Menschen pauschal als 'Risiko' ein, ohne individuelle Pr√ºfung ode r Widerspruchsm√∂glichkeit. Besonders betroffen sind Menschen mit Migrationsgeschichte, People of Color, Frauen, Menschen mit Behinderungen und sozial Benachteiligte.

Soziale  Arbeit  darf  sich  nicht  von  diskriminierender  Technik  steuern  lassen.  Sie  muss  digitale Ungleichheit  erkennen,  Transparenz  fordern,  eigene  Einsch√§tzungen  einbringen  und  f√ºr  gerechte Entscheidungen k√§mpfen. Ohne kritische Reflexion verliert die Soziale Arbeit ihren Handlungsspielraum und kann ihrer Aufgabe, Benachteiligung abzubauen, nicht mehr gerecht werden.

- Algorithmen  und  KI  sind  nicht  neutral -sie  √ºbernehmen  Vorurteile  aus  alten  Daten  und versch√§rfen soziale Ungleichheit.
- Folgen:  Klient:innen  werden  falsch  bewertet,  stigmatisiert  und  ausgeschlossen -ohne Widerspruchsm√∂glichkeit.
- Digitale Technologien wie Gesichtserkennung und KI sind nicht neutral, sondern verst√§rken Diskriminierung und Ungleichheit.
- Besonders betroffen: Menschen mit Migrationsgeschichte, People of Color, Frauen, Menschen mit Behinderungen, Arme.
- Systeme wie Scoring-Modelle und automatische Entscheidungen stufen Menschen pauschal als "Risiko" ein -ohne individuelle Pr√ºfung.
- Soziale Arbeit verliert durch solche Technik an Handlungsspielraum und kann ihre Aufgabe, Benachteiligung abzubauen, nicht mehr erf√ºllen.

## Kernsatz zum Mitnehmen:

Soziale Arbeit muss Menschen sch√ºtzen -nicht diskriminierende Technik √ºbernehmen! Algorithmen d√ºrfen keine soziale Ungleichheit verst√§rken.

<!-- image -->

üìå Fallbeispiel: Ein  automatisiertes Bewerbungsportal filtert Menschen mit Migrationshintergrund systematisch aus, weil die Trainingsdaten rassistisch voreingenommen sind.

## Zusatzinformationen

Voreingenommene (bias) Algorithmen und diskriminierende Systeme

- Algorithmen  werden  mit  bestehenden  Daten  trainiert,  die  oft  diskriminierende  Muster enthalten  (z.  B.  Vorurteile  gegen  Menschen  mit  Migrationsgeschichte,  arme  Menschen, Menschen mit psychischen Erkrankungen).
- Das  bedeutet:  Systematische  Benachteiligungen  werden  nicht  aufgehoben,  sondern technisch verst√§rkt und "unsichtbar gemacht".
- Beispiel: Wenn fr√ºhere Daten zeigen, dass Menschen aus bestimmten Stadtteilen seltener Unterst√ºtzung bekommen, wird der Algorithmus dieses Muster √ºbernehmen.

<!-- image -->

Macht  durch  Technik: Diskriminierung  wird  automatisiert  und  normalisiert,  und  es  gibt  kaum Widerspruchsm√∂glichkeiten.

## Beispiele f√ºr Bias in der Praxis

- Kindeswohl-Algorithmen , die automatisch h√∂here Gef√§hrdung bei Familien mit Migrationshintergrund  sehen,  weil  fr√ºhere  F√§lle  das  "belegen" -auch  wenn  es  f√ºr  den Einzelfall keinen Grund gibt.
- Jobcenter-Systeme ,  die  automatisch Menschen mit bestimmten Merkmalen (z. B. Alter, Schulbildung, Stadtteil) als "nicht vermittelbar" einstufen und weniger Hilfen bieten.
- Scoring-Modelle bei  Schuldenregulierungen,  die  Menschen  aus  einkommensschwachen Stadtteilen schlechtere Chancen einr√§umen.

Fazit: Solche  Bias  sind  unsichtbar,  weil  sie  als  "neutrale  KI-Entscheidung"  erscheinen,  aber  in Wirklichkeit alte Vorurteile technisch zementieren.

<!-- image -->

## 10. DIGITALE TEILHABE UND AUSSCHLUSS -ZWISCHEN GERECHTIGKEIT UND UNGLEICHHEIT

Digitalisierung  kann  soziale  Ungleichheiten  verst√§rken,  wenn  Menschen  keinen  Zugang,  keine Kompetenzen oder keine Kontrolle √ºber ihre Daten haben. Der Digital Divide zeigt sich auf drei Ebenen: fehlender  Zugang  zu  Ger√§ten  und  Internet  (First-Level  Divide),  mangelnde  digitale  Kompetenzen (Second-Level Divide) und der Ausschluss von Mitbestimmung und Datenkontrolle (Third-Level Divide).

Ohne  digitale  Teilhabe  verlieren  Menschen  nicht  nur  Anschluss,  sondern  auch  soziale  Rechte, Bildungschancen  und  politische  Mitsprache.  Besonders  gef√§hrdet  sind  Menschen  in  Armut,  √§ltere Menschen, Menschen mit Behinderungen und Personen mit geringer Bildung oder Migrationserfahrung. Soziale Isolation, Abh√§ngigkeit und Ausschluss sind die Folgen. Soziale Arbeit muss digitale Gerechtigkeit aktiv f√∂rdern: Zugang schaffen, digitale Kompetenzen st√§rken, Mitbestimmung erm√∂glichen und analoge Alternativen bewahren.

- Digitalisierung schlie√üt aus, wenn Menschen keinen Zugang, keine Kompetenzen oder keine Kontrolle haben (Digital Divide).
- First-Level Divide: Kein Zugang zu Ger√§ten und Internet.
- Second-Level Divide: Fehlende digitale Kompetenzen und Verst√§ndnis.
- Third-Level Divide: Keine Mitbestimmung und Kontrolle √ºber eigene Daten.
- Gefahr:  Soziale  Ungleichheit  wird  digital  fortgesetzt -genau  die  Zielgruppen  der  Sozialen Arbeit bleiben au√üen vor.
- Digitale Teilhabe bedeutet mehr als Zugang -es geht um Mitbestimmung, Selbstbestimmung und soziale Teilhabe.
- Ohne  digitale  Teilhabe  verlieren  Menschen  soziale  Rechte,  Zugang  zu  Hilfe,  Bildung  und politische Mitsprache.
- Besonders betroffen: Menschen in Armut, √§ltere Menschen, Menschen mit Behinderungen, geringe Bildung, Migrationserfahrung.
- Risiken: Abh√§ngigkeit von anderen, soziale Isolation, Ausgrenzung.

## Kernsatz zum Mitnehmen:

Ohne  digitale  Gerechtigkeit  wird  Digitalisierung  zur  Ausgrenzung -Soziale  Arbeit  muss  f√ºr  echte Teilhabe und soziale Rechte k√§mpfen.

<!-- image -->

üìå Fallbeispiel: Ein  Supermarkt  setzt  auf  digitale  Rabatte  in  einer  App.  √Ñltere  Menschen  und Menschen ohne digitale Kompetenzen sind ausgeschlossen.

## Zusatzinformationen

First-Level Digital Divide: Zugang zu Technik und Internet

Die erste Stufe der digitalen Spaltung betrifft den grundlegenden Zugang zu Ger√§ten und Internet. Wer wird ausgeschlossen?

- Menschen  mit  geringem  Einkommen,  die  sich  kein  Smartphone,  Tablet  oder  Computer leisten k√∂nnen.
- Menschen ohne stabile Internetverbindung, vor allem in l√§ndlichen Gebieten.

<!-- image -->

- Wohnungslose Menschen oder Personen in prek√§ren Lebenslagen, die keinen festen Ort f√ºr digitale Kommunikation haben.

## Beispiel aus der Praxis:

- Eine Klient:in soll ein Online-Antragsformular ausf√ºllen, hat aber kein eigenes Ger√§t und keinen Internetzugang.

Folge: Diese Menschen k√∂nnen digitale Angebote nicht nutzen, auch wenn sie verf√ºgbar w√§ren.

## Second-Level Digital Divide: Digitale Kompetenzen

Die zweite Stufe betrifft die F√§higkeiten, digitale Ger√§te und das Internet sinnvoll zu nutzen. Es reicht nicht, Zugang zu haben -man muss auch wissen, wie man damit umgeht.

## Wer wird ausgeschlossen?

- √Ñltere Menschen, die wenig Erfahrung mit digitalen Ger√§ten haben.
- Menschen mit geringen Sprachkenntnissen, die digitale Anwendungen nicht verstehen.
- Menschen mit Lernschwierigkeiten oder kognitiven Einschr√§nkungen, die sich in komplexen Systemen nicht zurechtfinden.
- Menschen,  die  Angst  vor  Technik  haben  oder  negative  Erfahrungen  mit  Beh√∂rden  und digitalen Systemen gemacht haben.

## Beispiel aus der Praxis:

- Ein  Online-Portal  f√ºr  Sozialleistungen  ist  zwar  verf√ºgbar,  aber  so  kompliziert  aufgebaut, dass viele Menschen nicht selbstst√§ndig einen Antrag stellen k√∂nnen.

Folge: Auch wenn Zugang besteht, bleiben Menschen faktisch ausgeschlossen, weil sie die Technik nicht nutzen k√∂nnen.

## Third-Level Digital Divide: Teilhabe und Einflussm√∂glichkeiten

Die dritte Stufe betrifft die Frage, wer wirklich von der Digitalisierung profitiert -wer also nicht nur Zugang und Wissen, sondern auch Chancen und Kontrolle √ºber die eigene digitale Teilhabe hat.

## Wer wird ausgeschlossen?

- Menschen, die zwar Zugang und Wissen haben, aber nicht √ºber ihre Daten bestimmen k√∂nnen.
- Menschen, deren digitale Spuren (z. B. in Scoring-Systemen) gegen sie verwendet werden.
- Menschen, die keine Stimme haben, wenn digitale Systeme √ºber ihr Leben entscheiden.
- Klient:innen,  die  nicht  gefragt  werden,  ob  sie  ein  digitales  Angebot  nutzen  wollen  oder lieber analoge Hilfe w√ºnschen.

## Beispiel aus der Praxis:

- Ein Algorithmus stuft eine Familie als "Problemfall" ein, ohne dass die Betroffenen wissen, dass sie bewertet wurden. Sie haben keinen Einfluss auf diese Entscheidung.

Folge: Menschen  werden  zum  Objekt  der  digitalen  Verwaltung,  ohne  die  Chance,  aktiv  und selbstbestimmt mitzugestalten.

<!-- image -->

## 11. DIGITALE SOUVER√ÑNIT√ÑT -UNABH√ÑNGIGKEIT STATT BIG-TECH-ABH√ÑNGIGKEIT

Big Tech (Google, Meta, Microsoft, Amazon, Apple) kontrolliert gro√üe Teile der digitalen Infrastruktur -auch  in  der  Sozialen  Arbeit.  Viele  √∂ffentliche  und  soziale  Einrichtungen  nutzen  Systeme  privater Anbieter, was zu Abh√§ngigkeiten, Datenverlust und ethischen Dilemmata f√ºhrt. Die Kontrolle √ºber Daten, Kommunikation und digitale Prozesse geht zunehmend in die H√§nde von Konzernen √ºber.

F√ºr Klient:innen bedeutet dies oft den Verlust von Datenschutz, Vertrauen und Selbstbestimmung. Fachkr√§fte m√ºssen sich an fremde Vorgaben anpassen und verlieren professionelle Autonomie. Soziale Arbeit darf sich nicht Big Tech ausliefern, sondern muss digitale Souver√§nit√§t st√§rken: Alternativen nutzen,  Datenschutz  sichern,  politische  L√∂sungen  einfordern  und  f√ºr  eine  digitale  Infrastruktur  im Sinne des Gemeinwohls eintreten.

- Big  Tech  (Google,  Meta,  Microsoft,  Amazon,  Apple)  kontrolliert  gro√üe  Teile  der  digitalen Infrastruktur -auch in der Sozialen Arbeit.
- Gefahr:  Soziale  Einrichtungen  werden  abh√§ngig  von  privaten  Konzernen -Kontrolle  √ºber Daten und Kommunikation geht verloren.
- Risiken f√ºr Klient:innen: Verlust von Datenschutz, Vertrauen und Zugang zu Hilfe.
- Ziel: Eine digitale Welt, die dem Gemeinwohl dient -nicht den Konzernen.
- Digitale Systeme in der Sozialen Arbeit stammen oft von privaten Anbietern wie Microsoft, Amazon, Google.
- Risiken:  Verlust  von  Kontrolle  √ºber  Daten,  Anpassung  an  fremde  Vorgaben,  finanzielle Abh√§ngigkeit, fehlende Alternativen.
- Folgen  f√ºr  Klient:innen:  Zwang  zur  Nutzung,  fehlender  Datenschutz,  keine  Kontrolle  √ºber eigene Daten.
- Fachkr√§fte verlieren professionelle Autonomie und geraten in ethische Dilemmata.

## Kernsatz zum Mitnehmen:

Soziale Arbeit braucht digitale Souver√§nit√§t -nicht Abh√§ngigkeit von Big Tech und privat kontrollierten Systemen.

üìå Fallbeispiel: Eine Beratungsstelle nutzt WhatsApp zur Kommunikation, merkt aber, dass sensible Daten an Meta √ºbermittelt werden.

## Zusatzinformationen

## Was bedeutet digitale Souver√§nit√§t?

- Kontrolle √ºber die eigene digitale Infrastruktur, Daten und Prozesse.
- Freiheit von Abh√§ngigkeit gegen√ºber privaten Tech-Konzernen.
- Selbstbestimmung bei der Auswahl und Gestaltung digitaler Tools.
- M√∂glichkeit, digitale L√∂sungen so zu gestalten, dass sie den Werten und Zielen der Sozialen Arbeit entsprechen: Gerechtigkeit, Teilhabe, Menschenw√ºrde.

<!-- image -->

Kurz gesagt: Digitale Souver√§nit√§t hei√üt, die digitale Welt nicht anderen zu √ºberlassen, sondern aktiv mitzugestalten.

## Was bedeutet Monopolstellung von Big Tech?

Ein Monopol entsteht, wenn ein oder wenige Unternehmen einen bestimmten Markt vollst√§ndig oder fast vollst√§ndig kontrollieren. Im digitalen Raum zeigt sich das z. B. dadurch:

- Google dominiert die Internetsuche und Werbung.
- Meta (Facebook, WhatsApp, Instagram) kontrolliert einen Gro√üteil der sozialen Kommunikation.
- Microsoft und Amazon (AWS) stellen die wichtigsten Cloud- und Server-Dienste bereit, auf denen auch viele soziale Dienste und Beratungsstellen ihre Daten speichern.
- Apple und Google bestimmen den Zugang zu digitalen Angeboten √ºber ihre App-Stores.

Folge: Wenige Konzerne entscheiden, wie digitale Kommunikation, Information und Datenspeicherung funktionieren -auch im Bereich der Sozialen Arbeit.

## Was bedeutet die Abh√§ngigkeit von privaten Anbietern?

- Beh√∂rden und soziale Tr√§ger nutzen f√ºr ihre Arbeit digitale Systeme, die von kommerziellen Firmen bereitgestellt und gewartet werden.
- Dazu geh√∂ren:
- ‚Üí Fallmanagement-Software.
- ‚Üí Datenbanken und Cloud-Systeme.
- ‚Üí Kommunikationsund Kooperationsplattformen (z. B. Microsoft Teams,  Zoom, WhatsApp).
- ‚Üí KI-gest√ºtzte Analysetools und Scoring-Modelle.
- Diese  Anbieter  bestimmen  die  Bedingungen,  unter  denen  die  Software  genutzt  wird, speichern und verarbeiten Daten, und nehmen Einfluss auf die technische Gestaltung der sozialen Arbeit.

Folge: √ñffentliche und soziale Einrichtungen sind von den Entscheidungen und Interessen privater Anbieter abh√§ngig -oft ohne echte Alternativen.

## Beispiele aus der Praxis

- Digitale  Beratungsplattformen,  die  auf  Microsoft  Teams  laufen -inklusive  Speicherung sensibler Daten in US-Clouds.
- Antragssysteme f√ºr Sozialleistungen, die nur noch online √ºber ein privates Portal m√∂glich sind, ohne analoge Alternativen.
- KI-gest√ºtzte  Systeme,  die  Entscheidungen  √ºber  Sozialleistungen  vorbereiten,  entwickelt von privaten Firmen -ohne Transparenz √ºber die Algorithmen.
- Jobcenter-Apps, die Arbeitslose regelm√§√üig zu Aktivit√§ten auffordern, "um ihre Motivation zu steigern" -inklusive automatischer Erinnerungen und Fristen.
- Fallmanagement-Software,  die  automatisch  "n√§chste  Schritte"  vorschl√§gt,  die  kaum hinterfragt werden.
- Gamification-Modelle in Apps, die Klient:innen Punkte geben, wenn sie Ma√ünahmen folgen -und damit subtile Verhaltenssteuerung betreiben.
- Scoring-Systeme, die das "Risiko" von Klient:innen berechnen und damit beeinflussen, ob und wie intensiv Hilfe angeboten wird.

<!-- image -->

## 12. DIGITALE BILDUNG -SCHL√úSSEL F√úR SOZIALE GERECHTIGKEIT

Digitale Bildung ist eine Grundvoraussetzung f√ºr Teilhabe, Selbstbestimmung und soziale Gerechtigkeit -sowohl f√ºr Fachkr√§fte als auch f√ºr Klient:innen. Fachkr√§fte m√ºssen digitale Tools, Algorithmen und Datenschutz  verstehen,  um  professionell  und  kritisch  handeln  zu  k√∂nnen.  Klient:innen  brauchen digitale Kompetenzen, um nicht ausgeschlossen zu werden und ihre Rechte wahrzunehmen.

Gleichzeitig muss Soziale Arbeit digitale Souver√§nit√§t anstreben: Abh√§ngigkeit von gro√üen Konzernen gef√§hrdet Datenschutz, professionelle Standards und  soziale Prinzipien. Eigene, sichere und gemeinwohlorientierte  Systeme  sind  notwendig,  um  Menschenrechte  auch  digital  zu  sch√ºtzen. Fachkr√§fte  und  Klient:innen  m√ºssen  mitentscheiden  k√∂nnen,  welche  digitalen  Tools  eingesetzt werden, und politische Rahmenbedingungen m√ºssen geschaffen werden, um digitale Gerechtigkeit langfristig zu sichern.

- Digitale Bildung ist Voraussetzung f√ºr Teilhabe und Gerechtigkeit in der Sozialen Arbeit -f√ºr Fachkr√§fte und Klient:innen.
- Fachkr√§fte brauchen Wissen √ºber digitale Tools, Algorithmen und Datenschutz, um kritisch, fair und professionell handeln zu k√∂nnen.
- Klient:innen brauchen digitale Kompetenzen, um nicht von Hilfen ausgeschlossen zu werden und ihre Rechte wahrzunehmen.
- Digitale  Bildung  umfasst  Technik,  Datenschutz,  kritischen  Umgang  mit  Algorithmen  und Selbstbestimmung.
- Digitale  Souver√§nit√§t  bedeutet:  Soziale  Arbeit  muss  selbst  bestimmen  k√∂nnen,  welche digitalen  Tools  eingesetzt  werden -im  Sinne  von  Gerechtigkeit,  Teilhabe  und  Schutz  der Klient:innen.
- Abh√§ngigkeit  von  gro√üen  Konzernen  gef√§hrdet  Datenschutz,  Beziehungsgestaltung  und professionelle Standards.
- Eigene,  sichere  und  gemeinwohlorientierte  Systeme  sind  n√∂tig,  um  Menschenrechte  und soziale Prinzipien auch digital zu sichern.
- Fachkr√§fte und Klient:innen m√ºssen mitentscheiden und aktiv an der Entwicklung passender Tools beteiligt werden.
- Digitale Souver√§nit√§t braucht auch politische Rahmenbedingungen und Investitionen -als Teil der Daseinsvorsorge.

## Kernsatz zum Mitnehmen:

Digitale  Bildung  und  Souver√§nit√§t  sind  essenziell  f√ºr  soziale  Gerechtigkeit -Soziale  Arbeit  muss Teilhabe sichern und Abh√§ngigkeiten vermeiden.

<!-- image -->

üìå Fallbeispiel: Ein  sozialer  Tr√§ger bietet Digitalkurse f√ºr Senior:innen an. Eine Teilnehmerin kann nach dem Kurs erstmals online ihre Sozialhilfe beantragen.

<!-- image -->

## 13. DIGITALE TRANSFORMATION -MITBESTIMMUNG STATT FREMDBESTIMMUNG

Die  Digitalisierung  ver√§ndert  nicht  nur  die  Arbeitsweise  sozialer  Organisationen,  sondern  auch Machtverh√§ltnisse und Beziehungen. Sie bietet Chancen wie effizientere Abl√§ufe, neue Beratungsformen und bessere Erreichbarkeit, birgt aber auch Risiken wie den Verlust pers√∂nlicher Beziehungen, √úberwachung und den Ausschluss von Menschen ohne digitalen Zugang.

Damit Digitalisierung den Menschen dient und nicht umgekehrt, m√ºssen Fachkr√§fte und Klient:innen von Anfang an in die Gestaltung digitaler Systeme einbezogen werden. Nur gemeinsam entwickelte Tools  passen  zur  Praxis  und  verhindern  Kontrolle  und  Ausgrenzung.  Partizipative,  sozial  gerechte Digitalisierung bedeutet: Mitbestimmung, Feedback und analoge Alternativen bleiben unverzichtbar.

- Digitalisierung ver√§ndert auch die innere Struktur und Arbeitsweise sozialer Organisationen -mit Chancen und Risiken.
- Chancen: Effizientere Abl√§ufe, neue Beratungsformen, bessere Erreichbarkeit.
- Risiken: Verlust von Beziehung, √úberwachung der Fachkr√§fte, Machtverschiebung zur Technik, Ausschluss von Menschen ohne digitalen Zugang.
- Wichtig:  Bewusste,  partizipative  und  sozial  gerechte  Gestaltung  der  Digitalisierung  in Organisationen.
- Fachkr√§fte und Klient:innen m√ºssen mitentscheiden -analoge Alternativen bleiben unverzichtbar.
- Digitale Systeme ver√§ndern die Soziale Arbeit -aber oft ohne Beteiligung der Fachkr√§fte und Klient:innen.
- Nur  gemeinsam  entwickelte  Tools  passen  zur  Praxis  und  sch√ºtzen  vor  Ausgrenzung  und Kontrolle.
- Beteiligung von Anfang an: Fachkr√§fte und Klient:innen m√ºssen mitentscheiden, was digitale Tools k√∂nnen -und was nicht.

## Kernsatz zum Mitnehmen:

Digitale Transformation in der Sozialen Arbeit muss partizipativ gestaltet werden -mit den Menschen, nicht √ºber ihre K√∂pfe hinweg.

üìå Fallbeispiel: In  einer  Wohngruppe  wird  eine  digitale  Plattform  zur Mitbestimmung  eingef√ºhrt. Doch viele Jugendliche verstehen die Tools nicht oder haben keinen Zugang.

<!-- image -->

## 14. DIGITALE WERKZEUGE -REFLEXION, VERANTWORTUNG UND DER MENSCH IM MITTELPUNKT

Digitalisierung  ver√§ndert  Macht,  Beziehungen  und  Entscheidungen  in  der  Sozialen  Arbeit -oft unbemerkt. Digitale Tools sind nie neutral und beeinflussen, wer Zugang zu Hilfe erh√§lt, wer Kontrolle √ºber  Daten  hat  und  wie  Fachkr√§fte  handeln.  Soziale  Arbeit  muss  digitale  Technologien  kritisch reflektieren,  Transparenz  fordern  und  sicherstellen,  dass  Technik  den  Menschen  dient -nicht umgekehrt.

Reflexion ist dabei kein Zusatz, sondern eine Grundlage professionellen Handelns. Wichtige Fragen sind: Wer entscheidet? Wer  profitiert? Werden  Menschen  ausgeschlossen? Methoden  wie Fallbesprechungen mit Digital-Fokus,  EthikCaf√©s oder  ein  'Macht -Check' f√º r  digitale  Tools  helfen, Technik  bewusst  einzusetzen.  Denn  digitale  Unterst√ºtzung  darf  Beziehung  und  Vertrauen  nicht ersetzen.

- Digitalisierung ver√§ndert Macht, Beziehungen und Entscheidungen in der Sozialen Arbeit -oft unbemerkt.
- Reflexion hilft, digitale Machtstrukturen zu erkennen, kritisch zu hinterfragen und gerecht zu handeln.
- Wichtige Fragen: Wer entscheidet? Wer profitiert? Wie wirkt Technik auf die Beziehung zu Klient:innen?
- Methoden: Fallbesprechungen mit Digital-Fokus, Reflexions-Checklisten, Ethik-Caf√©s, "MachtCheck" f√ºr Tools.
- Reflexion  ist  kein  Extra,  sondern  Grundlage  professionellen  Handelns  und  Grundlage  f√ºr politische Forderungen.
- Digitale Tools sind nie neutral -sie beeinflussen Macht, Entscheidungen und Beziehungen in der Sozialen Arbeit.
- Fachkr√§fte m√ºssen kritisch pr√ºfen: Wem n√ºtzt das Tool? Wer kontrolliert die Daten? Schlie√üt es Menschen aus?
- Technik soll unterst√ºtzen, nicht ersetzen -Beziehung und Vertrauen bleiben zentral.
- Transparenz, Mitbestimmung  und  Alternativen  sind n√∂tig,  um  Selbstbestimmung  der Klient:innen zu wahren.

## Kernsatz zum Mitnehmen:

Soziale Arbeit muss digitale Werkzeuge kritisch pr√ºfen -Technik darf den Menschen und die Beziehung niemals ersetzen.

üìå Fallbeispiel: Eine Sozialarbeiterin nutzt eine KI-gest√ºtzte App zur Bedarfsanalyse, entscheidet aber bewusst, sich nicht nur auf die Vorschl√§ge der KI zu verlassen.

<!-- image -->

## 15. GERECHTIGKEIT, TRANSPARENZ UND VERANTWORTUNG GESTALTEN

Digitalisierung ist kein neutraler Prozess -sie beeinflusst, wer Hilfe bekommt, und kann Ausgrenzung, √úberwachung und Diskriminierung verst√§rken. Soziale Arbeit muss diesen Wandel aktiv mitgestalten, damit Technik den Menschen dient und nicht anderen Interessen. Algorithmen und digitale Systeme d√ºrfen keine Benachteiligung verst√§rken, sondern m√ºssen fair, transparent und verantwortungsvoll eingesetzt werden.

Soziale Arbeit ist gefragt als Anw√§ltin f√ºr soziale Gerechtigkeit, um f√ºr digitale Teilhabe, Datenschutz und  klare  Verantwortlichkeiten  einzutreten.  Klient:innen  und  Fachkr√§fte  m√ºssen  verstehen,  wie digitale Entscheidungen getroffen werden, und sich dagegen wehren k√∂nnen, wenn sie ungerecht sind. Ohne Transparenz, Fairness und Verantwortung verliert die Digitalisierung ihren sozialen Auftrag.

- Digitalisierung  ist  kein  neutraler  Prozess -Soziale  Arbeit  muss  sie  aktiv  und  gerecht mitgestalten.
- Technik darf nicht ausgrenzen, √ºberwachen oder bewerten, sondern soll Menschen st√§rken.
- Soziale  Arbeit  muss  sich  einmischen:  f√ºr  digitale  Teilhabe,  Datenschutz,  Transparenz  und gegen Diskriminierung durch Technik.
- Fachkr√§fte  sollten  kritisch  pr√ºfen:  Dient  diese  Technik  den  Menschen  oder  anderen Interessen?
- Soziale Arbeit ist gefragt als Anw√§ltin, Mitgestalterin und Stimme f√ºr soziale Gerechtigkeit -auch im digitalen Raum.
- Digitale Systeme beeinflussen zunehmend, wer Hilfe bekommt -doch oft intransparent, unfair und ohne klare Verantwortung.
- Transparenz: Klient:innen und Fachkr√§fte m√ºssen wissen, wie und warum digitale Entscheidungen getroffen werden.
- Fairness: Algorithmen d√ºrfen keine Menschen benachteiligen -Diskriminierung muss erkannt und gestoppt werden.
- Verantwortung: Es muss klar sein, wer haftet, wenn Systeme falsch entscheiden -Fachkr√§fte und Klient:innen brauchen Schutz.

## Kernsatz zum Mitnehmen:

Soziale Arbeit muss digitale Gerechtigkeit gestalten -f√ºr Teilhabe, Transparenz und Verantwortung, damit Technik den Menschen dient.

üìå Fallbeispiel: Eine Petition fordert, dass Sozialleistungen nicht durch Algorithmen gesteuert werden d√ºrfen -mit Erfolg.

<!-- image -->

## 16. POLITISCHE STIMME F√úR EINE GERECHTE DIGITALISIERUNG

Digitalisierung darf nicht allein von Konzernen und Verwaltung bestimmt werden -sie braucht klare soziale Regeln, um Menschen zu sch√ºtzen, statt sie zu √ºberwachen oder auszuschlie√üen. Staatliche Regulierung muss Sozialdaten sch√ºtzen, Diskriminierung verhindern, digitale Teilhabe sichern und faire Alternativen schaffen.

Soziale Arbeit muss sich als politische Akteurin verstehen, die aktiv f√ºr Gerechtigkeit, Datenschutz und Transparenz  eintritt.  Sie  muss  klare  Forderungen  stellen:  Verbot  diskriminierender  Systeme,  faire digitale Arbeitsbedingungen und die Mitbestimmung von Fachkr√§ften und Klient:innen. Nur so kann Digitalisierung sozial gerecht gestaltet werden.

- Digitalisierung  braucht  klare  staatliche  Regeln,  damit  Technik  den  Menschen  dient -nicht Konzernen.
- Sozialdaten  sch√ºtzen,  Diskriminierung  verhindern,  Teilhabe  sichern  und  faire  Alternativen schaffen -das muss gesetzlich geregelt werden.
- Soziale Arbeit muss sich f√ºr soziale und gerechte Digitalisierung einsetzen und mitbestimmen, wie Technik gestaltet wird.
- Ohne Regulierung drohen √úberwachung, Ausgrenzung und Verlust von Kontrolle √ºber soziale Hilfe.
- Soziale Arbeit muss Digitalisierung aktiv politisch mitgestalten -f√ºr  Gerechtigkeit, Teilhabe und Menschenw√ºrde.
- Digitalisierung  darf  nicht  von  Konzernen  und  Verwaltung  allein  bestimmt  werden -sonst drohen Ausschluss, √úberwachung und Diskriminierung.
- Soziale Arbeit braucht klare politische Forderungen: Digitale Teilhabe, Datenschutz, Transparenz, faire Arbeitsbedingungen, Verbot diskriminierender Systeme.
- Fachkr√§fte und Klient:innen m√ºssen beteiligt, gesch√ºtzt und gest√§rkt werden -auch innerhalb der eigenen Organisationen.

## Kernsatz zum Mitnehmen:

Digitalisierung braucht klare soziale Regeln -und Soziale Arbeit als politische Kraft, die Gerechtigkeit, Teilhabe und Menschenw√ºrde einfordert!

<!-- image -->

üìå Fallbeispiel: Eine  Initiative  k√§mpft  f√ºr  gesetzliche  Regelungen  zu  fairen  Algorithmen  in  der Sozialverwaltung.

## Zusatzinformationen

Staatliche Regulierung ist notwendig, um die Rechte von Klient:innen und Fachkr√§ften zu sch√ºtzen und um soziale Gerechtigkeit auch im digitalen Raum zu sichern.

## Warum reicht es nicht, Digitalisierung "einfach laufen zu lassen"?

- Ohne  klare  Regeln  nutzen  gro√üe  Konzerne  (wie  Google,  Microsoft,  Amazon,  Meta)  die Digitalisierung, um Gewinne zu machen -und nicht, um soziale Probleme zu l√∂sen.

<!-- image -->

- Digitale  Systeme  (z.  B.  Algorithmen,  Scoring-Modelle,  automatisierte  Entscheidungen) k√∂nnen Menschen benachteiligen und ausschlie√üen, wenn niemand sie kontrolliert.
- Daten  √ºber  Klient:innen  landen  oft  unkontrolliert  bei  privaten  Firmen -ohne  dass  die Betroffenen wissen, was damit passiert.
- Fachkr√§fte  werden  gezwungen,  mit  Tools  zu  arbeiten,  die  nicht  zu  ihren  ethischen Prinzipien passen.

Fazit: Ohne  staatliche  Regulierung  droht  eine  Digitalisierung,  die  Macht  bei  den  Konzernen konzentriert und soziale Ungleichheiten verst√§rkt.

<!-- image -->

## 17. SOZIALE ARBEIT - EINE STIMME F√úR DIGITALE GERECHTIGKEIT

Digitalisierung  muss  fair,  inklusiv  und  demokratisch  gestaltet  werden -sonst  entstehen  neue Ausschl√ºsse und Diskriminierungen. Digitale  Menschenrechte  sch√ºtzen  W√ºrde, Teilhabe und Gerechtigkeit und m√ºssen auch in der Sozialen Arbeit aktiv verteidigt werden.

Soziale Arbeit hat die Aufgabe, sich politisch und praktisch f√ºr eine soziale Digitalisierung einzusetzen: gegen digitale  Diskriminierung,  √úberwachung  und  Ausgrenzung,  f√ºr  Datenschutz,  Transparenz  und analoge Alternativen. Digitalisierung darf nicht allein von Konzernen gesteuert werden -sie muss von und mit den betroffenen Menschen gestaltet werden.

- Digitalisierung muss fair, inklusiv und demokratisch gestaltet werden -sonst entstehen neue Ausschl√ºsse und Diskriminierungen.
- Soziale Arbeit muss sich aktiv einmischen, um Menschenrechte, Teilhabe und Gerechtigkeit auch im digitalen Raum zu sichern.
- Soziale  Arbeit  hat  die  Aufgabe,  digitale  Angebote  gerecht  zu  gestalten,  Klient:innen  zu unterst√ºtzen und politisch f√ºr soziale Digitalisierung einzutreten.
- Digitalisierung darf nicht von Konzernen bestimmt werden, sondern muss von und mit den betroffenen Menschen gestaltet werden.
- Digitale Menschenrechte sch√ºtzen W√ºrde, Teilhabe und Gerechtigkeit -auch in der Sozialen Arbeit.
- Soziale  Arbeit  muss  gegen  digitale  Diskriminierung,  √úberwachung  und  Ausgrenzung  aktiv werden.
- Klient:innen  brauchen  Schutz  ihrer  Daten,  Transparenz  bei  digitalen  Entscheidungen  und analoge Alternativen.

## Kernsatz zum Mitnehmen:

Soziale Arbeit muss f√ºr digitale Menschenrechte k√§mpfen -f√ºr W√ºrde, Gerechtigkeit und Teilhabe in einer digitalen Welt!

<!-- image -->

üìå Fallbeispiel: Ein  Verband  der  Sozialen  Arbeit  fordert,  dass  KI  in  der  Verwaltung  nur  mit menschlicher Kontrolle eingesetzt wird.

<!-- image -->

## 18. REFLEXIONSFRAGEN

## 1. Macht in der Sozialen Arbeit -Reflexion, Verantwortung und digitale Einflussfaktoren

- Wer kontrolliert eigentlich den:die Kontrollierende:n, wenn digitale Tools im Spiel sind?
- Wie oft wird Macht in der Sozialen Arbeit ausge√ºbt, ohne dass sich Fachkr√§fte dessen bewusst sind -und was bedeutet das f√ºr die digitale Praxis?

## 2. Digitale Macht -Verst√§rkung von Ungleichheiten oder neue Handlungsspielr√§ume?

- Sind digitale Tools in der Sozialen Arbeit eher Werkzeuge der Emanzipation oder der stillen Ausgrenzung?
- Ist die Vorstellung von 'neuen Handlungsspielr√§umen' durch Digitalisierung nicht eine Illusion f√ºr ressourcenschwache Kontexte?

## 3. Datenmacht -Kontrolle, Transparenz und die Rechte von Klient:innen

- Wessen Interesse steht im Vordergrund: der Schutz der Daten oder die Verwertbarkeit f√ºr das System?
- Sind wir als Fachkr√§fte bereits zu blo√üen 'Datenerfasser:innen' geworden?

## 4. Digitale √úberwachung -Zwischen Kontrolle und Vertrauen

- Wann wird aus Dokumentation √úberwachung -und wo ziehen wir die Grenze?
- F√∂rdert die Digitalisierung Vertrauen -oder ersetzt sie es durch Kontrolle?

## 5. Digitale Steuerung -Zwischen Selbstbestimmung und unsichtbarer Lenkung

- Wenn Algorithmen Entscheidungen beeinflussen, wer tr√§gt dann noch Verantwortung?
- Wie freiwillig ist Selbstoptimierung, wenn digitale Systeme st√§ndig mitdenken?

## 6. Digitale Systeme und Selbstbestimmung

- Ver√§ndert Technik, wie Klient:innen sich selbst sehen -und merken wir das √ºberhaupt?
- Wie viel Autonomie bleibt √ºbrig, wenn Systeme definieren, was 'normal' oder 'richtig' ist?

## 7. K√ºnstliche Intelligenz -Transparenz, Gerechtigkeit und menschliche Entscheidungsmacht

- Sollte  KI  √ºberhaupt  in  sensiblen  Bereichen  wie  Kinderschutz  oder  Schuldnerberatung eingesetzt werden?
- K√∂nnen Fachkr√§fte wirklich gegen KI-Empfehlungen entscheiden -oder handeln sie aus Angst vor Konsequenzen?

## 8. Diskriminierung durch Algorithmen

- Ist digitale Diskriminierung das neue Gesicht struktureller Ungleichheit?
- Widerspricht  automatisierte  Kategorisierung  nicht  grunds√§tzlich  der  Haltung  der  Sozialen Arbeit?

## 9. Digitale Teilhabe und Ausschluss

- Ist der digitale Ausschluss von marginalisierten Gruppen  ein Kollateralschaden oder Systemfehler?
- M√ºssen wir nicht aufh√∂ren, Digitalisierung als Fortschritt zu feiern, solange sie neue Exklusion erzeugt?

## 10. Digitale Souver√§nit√§t -Unabh√§ngigkeit statt Big-Tech-Abh√§ngigkeit

- Wie souver√§n kann Soziale Arbeit sein, wenn ihre Infrastruktur in der Cloud von Amazon liegt?
- Warum akzeptieren wir Standards von Tech-Konzernen, die unserer Ethik widersprechen?

## 11. Digitale Bildung -Schl√ºssel f√ºr soziale Gerechtigkeit

- Ist digitale Bildung in der Sozialen Arbeit Pflicht oder Luxus?

<!-- image -->

- K√∂nnen wir digitale Kompetenzen f√∂rdern, ohne gleichzeitig Abh√§ngigkeiten zu verst√§rken?
12. Digitale Transformation -Mitbestimmung statt Fremdbestimmung
- Ist die digitale Transformation wirklich gestaltbar -oder √ºberrollt sie die Praxis?
- Warum werden Fachkr√§fte oft erst einbezogen, wenn Entscheidungen l√§ngst gefallen sind?
13. Digitale Werkzeuge -Reflexion, Verantwortung und der Mensch im Mittelpunkt
- Wie oft fragen wir uns im Alltag wirklich: Wem dient dieses Tool -und wem schadet es?
- Ist Technik in der Praxis wirklich ein Hilfsmittel -oder ein stiller Akteur mit eigenen Interessen?
14. Gerechtigkeit, Transparenz und Verantwortung gestalten
- Reicht es, Missst√§nde zu benennen -oder braucht es aktiven digitalen Ungehorsam?
- Wie  kann  Soziale  Arbeit  Verantwortung  √ºbernehmen,  wenn  die  Entscheidungslogik  im Verborgenen bleibt?
15. Politische Stimme f√ºr eine gerechte Digitalisierung
- Ist Soziale Arbeit zu leise, wenn es um digitale Gerechtigkeit geht?
- Wie politisch darf oder muss Soziale Arbeit im digitalen Raum eigentlich sein?

## 16. Soziale Arbeit -Eine Stimme f√ºr digitale Gerechtigkeit

- Wenn nicht wir, wer dann? Wer k√§mpft sonst f√ºr digitale Menschenrechte im Alltag?
- W√§re es an der Zeit, eine eigene digitale Ethik-Charta f√ºr die Soziale Arbeit zu formulieren?

<!-- image -->

## 19. LITERATUREMPFEHLUNG

- ‚Üí Distelmeyer Jan (2021): Kritik der Digitalit√§t. Wiesbaden, Heidelberg: Springer VS (Medienwissenschaft).
- ‚Üí Kaminsky Carmen, Seelmeyer Udo, Siebert Scarlet, Werner  Petra (Hg.) (2020): Digitale Technologien zwischen Lenkung und Selbsterm√§chtigung. Interdisziplin√§re Perspektiven. Juventa Verlag. 1. Auflage. Weinheim, Basel: Beltz Juventa.
- ‚Üí Kettemann, Matthias C.; Rachinger, Felicitas; Vural, Meryem (2022): Menschenrechte im Digitalen. Wie wir Freiheit im digitalen Raum sichern : Handlungsoptionen f√ºr die Bundesregierung. Berlin: Friedrich-Ebert-Stiftung (FES Diskurs).
- ‚Üí Kutscher Nadia, Ley Thomas, Seelmeyer Udo, Siller Friederike, Tillmann Angela, Zorn Isabel (Hg.) (2020): Handbuch Soziale Arbeit und Digitalisierung. Juventa Verlag. 1. Auflage. Weinheim, Basel: Beltz Juventa.
- ‚Üí Martinsen Franziska (Hg.) (2018): Wissen -Macht -Meinung. Demokratie und Digitalisierung : die 20. Hannah-Arendt-Tage 2017. Velbr√ºck GmbH B√ºcher und Medien. Erste Auflage. Weilerswist: Velbr√ºck Wissenschaft.
- ‚Üí Spektrum der Wissenschaft (Hg.): Die dunkle Seite der Macht. Spektrum Kompakt 08.10.2018.
- ‚Üí
- Aus Politik und Zeitgeschichte. Demokratie in Gefahr? https://www.bpb.de/system/files/dokument\_pdf/APuZ\_2024-27\_online\_DemokratieInGefahr.pdf
- ‚Üí Machtsensibilit√§t. Ein Handlungskonzept f√ºr p√§dagogische bzw. sozialarbeiterische Interaktionen. https://www.socialnet.de/materialien/29731.php
- ‚Üí Digitalisierung in der sozialen Arbeit: Chancen und Risiken https://limani-bildung.de/digitalisierung-in-der-sozialen-arbeit-chancen-und-risiken/
- ‚Üí Digitalisierung soll Sozialarbeit bei Entscheidungsfindung helfen
- https://www.arbeitswelt-portal.de/berufe-im-wandel/artikel/digitalisierung-soll-sozialarbeit-beientscheidungsfindung-helfen
- ‚Üí M√ºssen Plattformen wie Facebook auch proaktiv Inhalte l√∂schen? https://www.wbs.legal/medienrecht/renate-kuenast-kaempft-gegen-hasskommentare-muessenplattformen-wie-facebook-auch-proaktiv-inhalte-loeschen-81638
- ‚Üí Eine gef√§hrliche Machtasymmetrie
- https://www.politik-kommunikation.de/politik/eine-gefaehrliche-machtasymmetrie/
- ‚Üí Informationsasymmetrie: Konflikte in der Kommunikation vermeiden
- https://www.allensbach-hochschule.de/informationsasymmetrie-konflikte-in-derkommunikation-vermeiden/
- ‚Üí Standards f√ºr ethische KI in Europa setzen
- https://www.pressetext.com/news/standards-fuer-ethische-ki-in-europa-setzen.html
- ‚Üí Macht https://www.socialnet.de/lexikon/Macht
- ‚Üí
- Digital Streetwork Bayern https://www.digital-streetwork-bayern.de