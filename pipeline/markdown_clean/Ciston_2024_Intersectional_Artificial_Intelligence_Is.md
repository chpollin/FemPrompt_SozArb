---
source_file: Ciston_2024_Intersectional_Artificial_Intelligence_Is.pdf
conversion_date: 2026-02-03T08:45:35.176365
converter: docling
quality_score: 95
---

## Intersectional Artificial Intelligence Is Essential: Polyvocal, Multimodal, Experimental Methods to Save AI

## Sarah Ciston

Media Arts + Practice, School of Cinematic Arts, University of Southern California

----- ciston@usc.edu

-----

## ABSTRACT

Artificial  Intelligence  is  quietly  shaping  social structures and private lives. Although it promises parity and efficiency, its computational processes mirror  biases  of  existing  power  even  as  oftenproprietary data practices and cultural perceptions of computational magic obscure those influences. However,  intersectionalitywhich  foregrounds  an  analysis  of  institutional power  and  incorporates  queer,  feminist,  and critical race theories-can help to rethink Artificial Intelligence. An intersectional framework can be used  to  analyze  the  biases  and  problems  built into  existing  Artificial  Intelligence,  as  well  as  to uncover alternative ethics from its counterhistories.  This  paper  calls  for  the  application  of intersectional strategies to Artificial Intelligence at every level, from data to design to implementation, from technologist to user. Drawing on intersectional theories, the research argues these strategies are polyvocal, multimodal,  and  experimental-suggesting  that community-focused  and  artistic  practices  can help imagine Artificial Intelligence's intersectional possibilities and help begin to address its biases.

## KEYWORDS

Artificial  Intelligence;  Intersectionality;  Gender; Critical Race Theory; Sexuality; Feminism; Algorithms; Bias; Experimental Practice; Computational Media

## 1 | INTRODUCTION

Artificial Intelligence (AI) should be created, critiqued,  and  reframed  with  an  awareness  of power-valuing multiple perspectives and methodologies-in  order  to  address  the  social inequalities, it reinforces. An intersectional framework can be used to analyze existing AI and to uncover alternative possibilities from its counter-histories, as well as to help shift approaches to designing new AI.

After  briefly  contextualizing  current  arguments about AI bias, and exploring how intersectionality analyzes institutional power not individual identity alone, this paper will offer examples of intersectional strategies from Black feminist, mixed race, and queer communities that might be applied to algorithm design and implementation. It  will  also  pull  interdisciplinary  techniques  from design and humanities to show how these might be engaged. Finally, it looks to several experimental  and  artistic  examples  that  offer  a preliminary imaginary for intersectional AI.

## 2 | AI IS NOT AN AUTHORITY, NOT A MIRROR

Despite  the  democratizing  promise  of  digital technologies, identity markers are being reinforced and even extracted as capital through algorithmic  systems.  'What  gets  framed  as  a matter  of  [nature  or]  preference  is  linked  to  a system  in  which  whiteness  holds  more  value' (Niesen, 2016, p. 171). The examples are widespread  and  troubling,  like  the  ProPublica study of AI criminal risk assessment that found it terrifyingly inaccurate: 'The score proved remarkably unreliable in forecasting violent crime:  Only  20  percent  [...]  actually  went  on  to [commit them]. [...] The formula was particularly likely  to  falsely  flag  black  defendants  as  future criminals, [...]. White defendants were mislabeled as  low  risk  more  often  than  black  defendants' (Angwin et al., 2016). This exemplifies what Ruha Benjamin (2019) calls 'the New Jim Code.'

Such biases precede the use of digital technologies (Chun, 2018, p. 64) but are further disguised by them. Cultural misunderstandings of AI allow their resulting data to assume a status of impartial  fact,  even  as  they  operate  by  human intervention  at  every  level.  For  example,  the ImageNet database was created with categories pulled  from  the  older  WordNet  database  and images pulled from the Internet:

The researchers then enlisted fifty thousand low-paid workers through Amazon's crowdsourcing labor platform,

Amazon Mechanical Turk, to apply labels to the images. The laborers' biases were ultimately embedded into the project, and [...] the prejudices of that labor pool are reflected in the AI technologies drawing from the data. Since AI is used not only by  tech  giants  and  academic  labs,  but also  by  state  and  federal  governments and law enforcement agencies, flaws in sets  can  have  wide-ranging its data impact. ( Artforum , 2019, n.p.)

This  labor  is  horrifyingly  evident  in  the  field  of commercial content moderation (CCM) [1], which Sarah T. Roberts (2016) argues lends a dangerous sense of naturalization to racist and biased content: 'Companies' desire to keep CCM work in the shadows [...] gives the impression that such content is just what is out there in the culture [...] and hides the human decision-making processes'  (p.  157).  Here  AI  'autonomy'  is carefully curated for corporate profit, but contingent  on  human  systems-who  are  acting as technology, while ignored and exploited on its behalf.

This curation also allows AI's operations  to remain opaque. The conflation of various types of AI (general and narrow, deep learning and neural networks)-along with its conflation with the big data it utilizes and with other types of algorithms-all contributes to AI's aura of unquestioned truthiness, which does little to undo the infrastructural inequalities it sometimes amplifies.

## 3 | INTERSECTIONALITY IS NOT JUST MORE REPRESENTATION, NOT JUST MORE DATA

Reading  AI  through  an  intersectional  lens  can help decode these power structures, and using intersectional approaches to design and implement AI creates the possibility for restructuring them. Brittney Cooper (2016) reminds us how intersectionality is often misunderstood:  Intersectionality  is  not  merely shorthand for discussing individual identity representation in strings of hyphenates-rather, it examines and critiques systems of power and how  those systems structure themselves to impact groups and individuals unequally.

Further, it is not enough to add more data to the neural network or to represent additional identities-these  too  will  be  opportunities  for marketing. Molly Niesen argues that 'the fracturing of users based on identity categories is, in  fact,  a  key  mechanism  of  capital  to  provide such data to advertisers' (Niesen, 2016, p. 168). And  as  Wendy  Chun  (2018)  points  out,  even feminist intersectional theory, when misread as a means  of  sifting  data  for  sameness  through identity difference, can be misappropriated toward racist ends (p. 65). However, when used instead  to  consider  institutional  structures  that feed those data, intersectional readings of technology  are  essential,  because  as  Safiya Noble (2018) claims, they offer 'new [...] interpretations for understanding the implications of  such  problematic  positions  about  the  benign instrumentality of technologies' (p. 31).

## 4  |  INTERSECTIONAL APPROACHES: SELFREFLEXIVE, POLYVOCAL, MULTIMODAL

Because intersectional  theory  owes  its  roots  to Black feminist thought, the strategies employed by women  of  color  are at the core of an intersectional critical praxis. Safiya Noble, Brendesha Tynes, and Joshua Schuschke (2016) argue that the queer women of color who founded Black  Lives  Matter  offer  a  model  for  coalitionbuilding through skills honed in community:

[因 the movement's reflexivity, the ability to  counter  hegemonic  narratives,  and self-care  are  key  components  of  digital intersectionality. By modeling the standard of reflexivity, the movement is able to critique and  correct its  own narrative and practices. (p. 28)

These could be incorporated into intersectional AI at the development, implementation, analysis, or data-gathering stages-and these methods could work to destabilize existing standards and biases.

In  particular,  strategies  from  mixed  race,  trans, bisexual, and femme communities-whose identities are not easily categorized, who sometimes maneuver by passing within systems-may  also  help  engage  and  subvert normative  algorithmic  practices,  operating  on multiple  valences  of  infrastructural  power  and intersectional disenfranchisement. Myra Washington (2017) argues for a mixed-race ethos that can cut across categories: 'transracialness is about  the  'potential  mutual  transformation'  of those categories [...,] how people [...] move within this spectrum of power and is not so much about identity' (p. 14-15).

And because queer theory troubles presumptions about a clearly  defined,  natural  category  called female  (Cipolla,  Gupta  &amp;  Rubin,  2017,  p.  7), reading  AI  through  intersectional  queer  theory can  push  back  on  assumptions  in  AI  about gender-and using queer strategies to disorient those categories can also push back on assumptions about technologies themselves. Geographers Daniel Cockayne and Lizzie Richardson  (2017)  read  queer  theory  through software studies because 'queer approaches are invested in conceptualizing and (therefore)

challenging  both  social  and  digital  code(s)-or the norm-to show how they constrain normativity but also how forms of intimate life can transgress, disrupt, and distribute what is normal' (p. 1643).

Queer-of-color activists reclaim less-visible identities as sites of strength. Zuleikha Mahmood and Leah Lakshmi Piepzna-Samarasinha, authors  of the Femme  Shark  Manifesto , for example, redefine 'femme' to make it legible and instrumental  in  their  community:  'being  fierce women of color or down white girls [...] who see the connection between everything in our lives' (Mahmood &amp; Piepzna-Samarasinha, 2008, p. 4).

## 5 | IMAGINING INTERSECTIONAL AI: UTILIZING UNCERTAINTY &amp; ENTANGLEMENT

Such attitudes can inspire intersectional connections and possibilities for AI that challenge how technologies are both connecting and othering individuals. They help frame how intersectional AI might instrumentalize precarious orientations, rework stereotypes of passing, and utilize instability in order to reprogram technologies of gender and agency.

Speculating a more intersectional technoimaginary,  Kara  Keeling  (2014)  proposes  the Queer OS: 'to make queer into the logic of 'an operating system of a larger order' that unsettles the common senses that secure those presently hegemonic social relations [...but] acknowledges its  own  imbrication  with  and  reliance  on  those logics while still striving to forge new relationships and connections' (p. 154).

Keeling calls it a 'malfunction with a capacity to reorder things' (p. 157), which moves away from the  urge  to  read  neutrality  and  rationality  into algorithms,  and  echoes  Luciana  Parisi's  (2017) suggestion that machine learning can be read as a  new  form  of  knowing,  or:  'reasoning  through and with uncertainty' (p. 8).

Elizabeth Wilson (2010) also asks how malfunction  might  contribute  to  more  advanced artificial  agents:  'Is  error  (and  [...]  shame  and anger  and  contempt)  the  limit  of  an  artificial system, or [...]? Might there be artificial systems that can tolerate their own inadequacies?' (p. 57).

## 6 | IMAGINING INTERSECTIONAL AI: POLYVOCAL DESIGN, MULTIMODAL ANALYSIS

In addressing AI bias, I want to remain mindful of cheerful  calls  for  collectivity  and  diversity  that ignore the nuance of inequality. Lauren Berlant (2016) warns us that the so-called commons can threaten 'to cover over the very complexity of [...] interdependence  it responds  to,' but should instead point to 'the difficulty of convening a world conjointly' (p. 395).

Thus, imagining intersectional AI cannot be done from  a  single  subject  position.  This  work  must happen in multiple communities and take shape in  intersecting  forms,  morphing  and  subverting, with no singular ethic or aesthetic-but rather a meta-ethics  of multiplicity and  intersubjective relations.

Applying  intersectional  tactics  to  AI  could  offer material  impacts,  but  those  may  be  difficult  to trace-requiring a combination of interdisciplinary methods and multi-sensory tools.

Catherine  D'Ignazio  and  Lauren  F.  Klein  make the  design  process  itself  more  intersectional, arguing that 'data, design, and community of use, are inextricably intertwined' (p. 2). They propose six  principles  for  feminist  data  visualization  that could be adopted for AI: 'rethink binaries,' 'embrace pluralism,' 'examine power and aspire to empowerment,' 'consider context,' 'legitimize embodiment and affect,' and 'make labor visible' (p. 2-3).

From the social sciences and humanities comes Critical Technocultural Discourse  Analysis, a research method that examines both 'the technological artifact and user discourse, framed by cultural theory, to unpack semiotic and material  connections  between  form,  function, belief, and meaning' (Sweeney &amp; Brock, 2014, p. 3). The techniques above draw on design thinking and cultural studies methodologies to both critique and intervene at any phase of technological processes.

## 7 | IMAGINING INTERSECTIONAL AI: COMMUNITY  METHODOLOGIES  &amp;  ARTISTIC EXPERIMENTS

Several case studies offer inspiration for intersectional AI to come. The Algorithmic Justice League intervenes on what they call the 'coded gaze' with poems, corporate pledges, and code modules  (Buolamwini,  2017,  2018).  The  Data Nutrition Label (Holland et al., 2018) addresses bias at the point of data collection, using a visual aid for assessing problems in potential datasets to prevent bad outcomes early on (p. 13) and help researchers  build  better  habits  by  'questioning datasets through analysis and interrogation techniques'  (p.  15).  While  not  themselves  AI, these practices bring intersectional thinking into tech spaces, helping shift an entrenched mindset with creative and helpful, playful and interventionist tools alike. Imagining intersectional AI means intervening with practical and speculative approaches simultaneously.

One experimental approach is to create absurdist tactical media, such as ladymouth , a chatbot that tries  to  explain  feminism  to  online  misogynists (Ciston, 2019a). Its initial prototype posted quotations  from  feminist  scholars  to  subreddits like The Red Pill and  used  responses  to  make text-based  performance  and  video.  Designed with the idea that the tool should be adaptable to other intersectional issues, it inspired a collaborative  project  that  uses  its  technology  to help address diversity labor in STEM workplaces (Billard et al., 2019). These show how a speculative project can spawn additional practical solutions for other audiences by opening a space to ideate and iterate on intersectional possibilities.

Such spaces thrive in community, and the organization Feminist.AI  takes  a  communitydriven approach to rethinking what it calls 'hegemonic  AI.'  The  Feminist.AI  'Cultural  AI Design' philosophy emphasizes multiple modes of access in order to draw from different knowledge  systems,  community-sourcing  their own data, and revisiting their process with each new project (Feminist.AI, n.d.). Co-founder Christine Meinders (2019) says that feeling safe to play and create rather than focusing on being 'right' is essential, and the organization considers resources such as childcare an important part of its workshops and collaborations.

In another community-focused technology space, Color  Coded  is  'a  collective  holding  space  for POC to co-teach, co-create, and co-own technologies because #TechIsNotNeutral'. They provide  technology-rich  environments  only  for historically  excluded  people  to  collaborate  and learn. Recent workshops have addressed topics like algorithmic streaming biases and 'how blockchain technology might support and sustain community-centered  collectives'  (Color  Coded, n.d.). Unlike organizations that emphasize inclusion,  this  model  foregrounds  exclusivity  in order  to  amplify  resources  for  communities  of color.  They  cite  Kelsey  Blackwell  (2018)  who argues that, 'Being together can offer resiliency for  bringing  our  fullness  into  integrated  spaces where  it will inevitably be challenged.'  She continues:

[因 merely inviting more people of color into  a  space  does  not  in  and  of  itself make  that  space  inclusive.  Patterns  of white dominance suffuse the space just like  other  spaces  we  occupy,  only  this time,  we're  calling  it  'inclusive.'  That's more painful and frustrating than being in spaces that are blind. (Blackwell, 2018)

Thus,  a  combination  of  inclusive  and  reserved spaces  best  supports  a  variety  of  communities and  intersecting  identities,  in  order  to  create safety and access, to share and prioritize resources and techniques as needed. Showcasing a mix of approaches and outcomes rather than a one-size-fits-all approach is at the heart of an intersectional methodology.

In another example of intersectional group practice,  this  mix  of  approaches  informs  the intersectional pedagogy for hacker-style spaces where student media artists can learn programming. University of Southern California's Creative Code Collective offers co-learning across disciplines, from a project-driven perspective. Its ethos emphasizes 'scrappy artistic  strategies  not  perfect  code;  growth  not mastery' (Ciston, 2019b). In the collective, artistteachers  model  their  approaches  to  working creatively  with  existing  and  emerging  tech-for example  using  natural  language  processing  in poetic  practices  that  draw  out  combinations  of critical considerations and  aesthetic oddities, foregrounding the ethics embedded in their tools and practices.

## 5 | CONCLUSION

These kinds of artistic experiments and interventions that make alternative use of AI are one  approach  to  develop  possibilities  for  more thoughtful  and  inclusive  technologies.  Through their  provocation  'ImageNet  Roulette,'  which allowed users to view how their webcam image would be categorized by a neural network trained on the ImageNet database, Kate Crawford and Trevor Paglen (2019) point out:

[因 the automated interpretation of images is an inherently social and political project, rather  than  a  purely technical  one.  [...]  Regardless  of  the supposed  neutrality of any particular category, the selection of images skews the meaning in ways that are gendered, racialized, ableist, and ageist. ImageNet is  an  object  lesson,  if  you  will,  in  what happens  when  people  are  categorized like objects. (n.p.)

The conversation started by 'ImageNet Roulette' has led to direct changes to the database itself ( Artforum , 2019, n.p.), which do not eliminate bias but at least begin to acknowledge its existenceafter decades of use as a 'neutral' tool, one that clearly  has  not  aged  well,  with  its  history  being traceable  to  'boy's  club'  technologist  cultures (Crawford &amp; Paglen, 2019, n.p.).

Such works are a call for all of us to demand a better way, to heed those already speaking up for it. This paper is not, and should not be, the only perspective on intersectional AI, nor does it claim

to  be  the  correct  one.  Rather,  it  argues  thatthrough many conversations and interventionsintersectional AI is a set of possibilities we must construct together.

Whether designing new AI, examining and experimenting  with  existing  tools,  or  supporting others' work-we can use intersectional aesthetics,  ethics,  and  tactics  to  re-imagine  AI. Fostering communities and technologies in which multiple  voices  feel  valued-and  feel  free  to experiment-is essential.

## ACKNOWLEDGEMENTS

Thank  you  to  all  reviewers  for  their  generous feedback on this and prior iterations of this work. Thank you also to the many fantastic models for intersectionality I have witnessed in classrooms, communities, and art practice.

## ENDNOTES

[1]  As  laid  out  by  Sarah  T.  Roberts  (2016), commercial content moderation is a dispersed set of  practices  by  which  companies  protect  their platforms and brands. It provides the illusion of automation by relying on precarious labor:

the work is almost always done in secret for  low  wages  by  relatively  low-status workers,  who  must  review,  day  in  and day  out,  digital  content  that  may  be pornographic, violent, disturbing, or disgusting.  The  workers  act  as  digital gatekeepers  [地nd]  play  a  significant role  in  crafting  the  flavor  of  a  site  and deciding  what  is  permissible  and  what crosses lines into removable territory. (p. 147-148)

## REFERENCES

Angwin, J., Larson, J., Mattu, S., &amp; Kirchner, L. (2016). Machine Bias. ProPublica . https://www.propublica.org/article/machinebias-risk-assessments-in-criminal- sentencing

Artforum . Image Database Purges 600K Photos After Trevor Paglen Project Reveals Biases. (2019). Artforum.

https://www.artforum.com/news/trevorpaglen-and-kate-crawford-s-reveals-itsracist-misogynistic-biases-imagenet-topurge-600-000-facial-images-fromdatabase-80829

Berlant, L. (2016). The commons: Infrastructures for troubling times*. Environment and Planning D: Society and Space , 34 (3), 393-419.

Billard, T.J., Ciston, S., &amp; Loup, A. (2019). Chats with diversibot: Automating the Emotional

Labor of Diversifying STEM. Annenberg Graduate Research Symposium. Los Angeles. Apr 18, 2019.

Benjamin, R. (2019). Race After Technology: Abolitionist Tools for the New Jim Code. Medford, MA: Polity.

Blackwell, K. (2018). Why People of Color Need

Spaces Without White People. The Arrow : https://arrow-journal.org/why-people-ofcolor-need-spaces-without-white-people/

Buolamwini, J. (2017). How I'm fighting bias in algorithms | Joy Buolamwini.

https://www.youtube.com/watch?v=UG\_X\_ 7g63rY

\_\_\_. (2018). AI, Ain't I A Woman? - Joy Buolamwini.

https://www.youtube.com/watch?v=QxuyfW oVV98

Cipolla, C., Gupta, K., &amp; Rubin, D.A. (2017). Queer Feminist Science Studies: A Reader. Seattle: University of Washington Press.

Chun, W. H. K. (2018). 'Queerying Homophily.' Pattern Discrimination . ed. Apprich, C., Chun, W. H. K., Cramer, F., &amp; Steyerl, H. Minneapolis: U of Minnesota Press.

Ciston, S. (2019a). ladymouth: Anti-SocialMedia Art As Research. Ada: A Journal of Gender, New Media &amp; Technology , (15). https://adanewmedia.org/2019/02/issue15ciston/

\_\_\_. (2019b). 'Creative Code Collective Introduction.' University of Southern California. Los Angeles. Jan 18, 2019.

Cockayne, D. G., &amp; Richardson, L. (2017). Queering code/space: The co-production of socio-sexual codes and digital technologies. Gender, Place &amp; Culture, 24(11), 1642-1658.

9672

Color Coded. (n.d.). Color Coded: https://colorcoded.la

Cooper, B. (2016). Intersectionality. In L. Disch &amp; M. Hawkesworth (Eds.), The Oxford Handbook of Feminist Theory (Vol. 1). https://doi.org/10.1093/oxfordhb/97801993 28581.013.20

Crawford, K., &amp; Paglen, T. (2019). Excavating AI: The Politics of Images in Machine Learning Training Sets.

https://www.excavating.ai

D'Ignazio, C., &amp; Klein, L. (n.d.). Feminist Data Visualization.

https://www.academia.edu/28173807/Femi nist\_Data\_Visualization

Feminist.AI. (n.d.) https://www.feminist.ai/ Griffiths, C. (2018). Visual Tactics Toward An Ethical Debugging. Digital Culture &amp; Society , 4 (1), 217-226.

Holland, S., Hosny, A., Newman, S., Joseph, J., &amp; Chmielinski, K. (2018). The Dataset Nutrition Label: A Framework To Drive Higher Data Quality Standards. ArXiv:1805.03677 [Cs] .

http://arxiv.org/abs/1805.03z677

Keeling, K. (2014). Queer OS. Cinema Journal , 53 (2), 152-157.

Mahmood, Z., &amp; Piepzna-Samarasinha, L. L. (2008). All Our Holes Are Hungry: Hungry for Justice and Fucking. Femme Shark Communique , (1).

http://archive.qzap.org/index.php/Detail/Obj ect/Show/object\_id/432

Meinders, C. (2019). Personal correspondence. Niesen, M. (2016). Love, Inc.: Toward Structural Intersectional Analysis of Online Dating Sites and Applications. In Digital Formations, Vol. 105 . The intersectional Internet: Race, sex, class and culture online (pp. 161-178). New York: Peter Lang Publishing, Inc.

Noble, S. U. (2018). Algorithms of Oppression: How Search Engines Reinforce Racism .

http://search.ebscohost.com/login.aspx?dir ect=true&amp;db=nlebk&amp;AN=1497317&amp;authtyp e=sso&amp;custid=s8983984

Parisi, L. (2017). Reprogramming Decisionism. E-Flux , Journal #85 . http://www.eflux.com/journal/85/155472/reprogramming -decisionism/

Roberts, S. T. (2016). Commercial Content Moderation: Digital Laborers' Dirty Work. In Digital Formations, Vol. 105 . The intersectional Internet: Race, sex, class and culture online (pp. 147-160). New York: Peter Lang Publishing, Inc.

Sweeney, M. E. (2016). The Intersectional Interface. In Digital Formations, Vol. 105 . The intersectional Internet: Race, sex, class and culture online (pp. 215-228). New York: Peter Lang Publishing, Inc.

\_\_\_., &amp; Brock, A. (2014). Critical informatics: New methods and practices. Proceedings of the American Society for Information Science and Technology , 51 (1), 1-8. https://doi.org/10.1002/meet.2014.1450510 1032

Tynes, B. M., Schuschke, J., &amp; Noble, S. U. (2016). Digital Intersectionality Theory and the #Blacklivesmatter Movement. In Digital Formations, Vol. 105 . The intersectional Internet: Race, sex, class and culture online (pp. 21-40). New York: Peter Lang Publishing, Inc.

Washington, M. S. (2017). Blasian invasion: Racial mixing in the celebrity industrial complex . Jackson: University Press of Mississippi.

Wilson, E. A. (2011). Affect and Artificial Intelligence . Seattle: U of Washington P

## BIOGRAPHICAL INFORMATION

Sarah Ciston (she/they) is a computational media artist and experimental writer, named one of San Francisco  Weekly 's 'Best Writers Without a Book.'  Her  work  includes  an  AI  interface  that reprograms  the  inner  critic  and  a  chatbot  that explains  feminism  to  online  misogynists.  She holds an MFA from UC San Diego and is a PhD Candidate  and  an  Annenberg  Fellow  in  Media Arts + Practice at University of Southern California's School of Cinematic Arts, where she organizes Creative Code Collective-a community of students learning programming in a welcoming, interdisciplinary environment that values  project-driven  approaches  and  creative criticality. http://sarahciston.com