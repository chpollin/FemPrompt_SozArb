---
source_file: Klein_2024_Data_Feminism_for_AI.pdf
conversion_date: 2026-02-03T09:02:53.207246
converter: docling
quality_score: 95
---

<!-- image -->

.

<!-- image -->

.

.

<!-- image -->

.

.

.

.

.

.

.

Latest updates: hps://dl.acm.org/doi/10.1145/3630106.3658543

.

.

## RESEARCH-ARTICLE Data Feminism for AI

LAUREN KLEIN , Emory University, Atlanta, GA, United States

CATHERINE D'IGNAZIO , Massachuses Institute of Technology, Cambridge, MA, United States

Open Access Support provided by:

Emory University

Massachuses Institute of Technology

<!-- image -->

<!-- image -->

<!-- image -->

.

.

Published: 03 June 2024

Citation in BibTeX format

FAccT '24: The 2024 ACM Conference on Fairness, Accountability, and Transparency

June 3 - 6, 2024 Rio de Janeiro, Brazil

.

.

.

.

.

.

## ABSTRACT

This paper presents a set of intersectional feminist principles for conducting equitable, ethical, and sustainable AI research. In Data Feminism (2020), we offered seven principles for examining and challenging unequal power in data science. Here, we present a rationale for why feminism remains deeply relevant for AI research, rearticulate the original principles of data feminism with respect to AI, and introduce two potential new principles related to environmental impact and consent. Together, these principles help to 1) account for the unequal, undemocratic, extractive, and exclusionary forces at work in AI research, development, and deployment; 2) identify and mitigate predictable harms in advance of unsafe, discriminatory, or otherwise oppressive systems being released into the world; and 3) inspire creative, joyful, and collective ways to work towards a more equitable, sustainable world in which all of us can thrive.

## CCS CONCEPTS

· Computingmethodologies → Artificial intelligence ; · Humancentered computing → Collaborative and social computing theory, concepts and paradigms ; · Applied computing → Arts and humanities .

## KEYWORDS

feminism, data feminism, data justice, ai ethics, responsible ai

## ACMReference Format:

Lauren Klein and Catherine D'Ignazio. 2024. Data Feminism for AI. In The 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT '24), June 03-06, 2024, Rio de Janeiro, Brazil. ACM, New York, NY, USA, 13 pages. ## 1 INTRODUCTION

Data Feminism [51] was published in March 2020, in the first week of what would become a world-altering pandemic, and in the wake of over a decade of increasing awareness of the power of data when collected, analyzed, and deployed. Our motivation for writing the book was the overabundance of evidence of the power of data, and of how that power was being wielded unequally. More specifically, it was being wielded by corporations, governments, and other well-resourced institutions to enhance their own power and profit,

∗ Both authors contributed equally to this research.

<!-- image -->

This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike International 4.0 License.

FAccT '24, June 03-06, 2024, Rio de Janeiro, Brazil © 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0450-5/24/06 ## Data Feminism for AI

## Lauren Klein ∗

lauren.klein@emory.edu Emory University Atlanta, GA, USA

## Catherine D'Ignazio ∗

dignazio@mit.edu MIT Cambridge, MA, USA

with significant personal, political, and financial costs for everyone else. There was already a rich scholarly conversation about how data was being used to amplify existing structural inequalities [13, 18, 22, 55, 117, 152]. The contribution of Data Feminism was to show how feminism, because of its analytic focus on the root causes of structural inequalities, could help challenge and rebalance that power. The seven principles of data feminism-examine power, challenge power, rethink binaries and hierarchies, elevate emotion and embodiment, consider context, embrace pluralism, and make labor visible-were intended to operationalize what we saw as the most relevant tenets of feminist thinking for data science. Our goal was to provide a clear set of guidelines and examples for people working with data, who wanted to work with data, or who wanted to refuse to work with data on political or personal grounds. We wanted to show how feminism was not only relevant but essential to data science and, to model how data scientists, computer scientists, digital humanists, policymakers, urban planners, journalists, educators, students, and others could put feminism into practice in their work.

Since the publication of the book, the principles of Data Feminism have been taken up across academia [23, 39, 73, 94, 96, 148, 156] and in the public sector [7, 50, 64, 82, 89, 110]. But as the conversation has shifted from data science to AI, we see a need to revisit these principles. This paper presents a rationale for why feminism remains deeply relevant for AI research, rearticulates the original principles of data feminism with respect to AI, and introduces the possibility of two additional principles, related to environmental impact and consent, in order to help to 1) account for the unequal, undemocratic, extractive, and exclusionary forces at work in AI research, development, and deployment; 2) identify and mitigate predictable harms in advance of unsafe, discriminatory, or otherwise oppressive systems being released into the world; and 3) inspire creative, joyful, and collective ways to work towards a more equitable, sustainable world in which all of us can thrive..

## 2 BACKGROUND

## 2.1 What is Feminism?

Feminism has a long, varied, and often contested history. While it exceeds the scope of this paper to summarize the entirety of this history, it is important to clarify the definition of feminism we mobilize here. At its most basic level, feminism entails a belief in the equality of all genders. This includes women and men, as well as Two Spirit, genderqueer, travesti, nonbinary people, and more. But until gender equality is realized in the world, our feminism also requires organized activity to make this goal of equality a reality. A third aspect of our feminism derives from its intellectual heritage, and a crucial part of this heritage is intersectional feminism , which comes to us from the work of women of color feminists, and Black feminists in the United States in particular. The contributions of

intersectional feminism are twofold: first, to bring additional facets of social difference to the conversation about gender inequality, including but not limited to racial and economic inequality; and second to insist that we concern ourselves with structural power: the reasons why people experience privilege on the one hand, or oppression on the other. Intersectional feminism offers models (in the conceptual sense, not the machine learning sense) that explain the causal mechanisms of complex systems of power and guide action to transform them towards justice. These include the Combahee River Collective's observation about 'interlocking systems of oppression' [33], Patricia Hill Collins's formulation of the 'matrix of domination' [76], and Kimberlé Crenshaw's term 'intersectionality' [36]. Note that these intersectional feminist models of power are not the only models of structural power that exist; others have theorized the workings of structural inequality from the perspective of capitalism, colonialism, and so on. We are drawn to intersectional feminist theories of power because of how they bridge personal experience and structural frameworks, and because they are explicit about their goal: understanding present imbalances of power in the world so that they can be challenged, rebalanced, and changed.

## 2.2 Feminism Today

In the years since Data Feminism was published, there have been several significant alterations to the social and political fabric of the United States and the world, many of which bring feminist considerations to the fore. Most directly, in the US, where the authors of this paper are located, we have experienced the overturning of the constitutional right to abortion, which has set off a cavalcade of increasing incursions into the autonomy and privacy of those with child-bearing bodies. These long-standing feminist concerns now play out in digital (if not exclusively AI-driven) spaces, as personal data has become a key legal weapon wielded against people seeking abortions [135]. Cases such as these also underscore the close relation between reproductive justice and trans justice, in that attacks on the bodily autonomy of some are attacks on the autonomy of all. It is not a coincidence that restrictions on abortion access have been accompanied by restrictions on gender-affirming care, and other legal efforts to police the bodies of trans people. A key feminist lesson here, born from over a century of exclusionary history, is that these are interconnected struggles, and we cannot defend bodily autonomy with an essentialist definition of 'women' [137]. A related lesson comes to the US from Latin America. There, the decades-long 'marea verde' (green wave) of feminist activism has been successful in large part due to its expansiveness and economic populism as well as its insistence on linking labor issues to gender issues [126]. As the US rolled back abortion rights, for example, Argentina legalized abortion and it was decriminalized in Colombia and Mexico, some of the most populous countries in the hemisphere. These successes were the result of several decades of protest and activism.

Attacks on women and trans people are not happening in a vacuum. Communications scholars W. Lance Bennett and Marianne Kneuer recently argued that we are witnessing the intensification of "illiberal public spheres" around the world [15]. These are characterized by a set of anti-democratic communications tactics that include the denigration and exclusion of minoritized people, targeted attacks on the press and political institutions, and transgression of norms of civil discourse. Such tactics are enabled and amplified by the social media platforms owned by Big Tech, whose business model elevates attention above all other metrics, such that they profit richly off extremism, threats, spectacle, and lies. The success of this corporate partnership with right-wing extremism is evident in the proliferation of book bans [103], the demonization of DEI and critical race theory [37], and mob tactics to doxx, intimidate, misrepresent, and otherwise silence people that study misinformation, speak out against racial and gender violence, or rule against insurrectionists [53]. At the same time, we have seen the continued work of feminists, including Latin American, Indigenous, and abolitionist feminists in the US and globally, doing the work of imagining alternate worlds [38, 43, 127, 139]. This paper represents our own first attempt to imagine a more equitable, sustainable, feminist world for AI research.

## 3 RELATED WORK

## 3.1 Feminism and FAccT

Within the FAccT community, we have already seen examples of how feminism can play a substantive role in guiding AI/ML research. As early as 2021, Leila Marie Hampton [68] introduced Black feminism as a lens through which to understand and critique algorithmic oppression. That same year, Hancox-Li and Kumar [70] introduced the concept of feminist epistemology to the FAccT community, offering suggestions on how to align AI/ML research with feminist ideas about the value of situated knowledge [72] and of multiple ways of knowing more broadly. In subsequent years, work at FAccT has demonstrated how a suite of feminist concepts drawn from Black feminism, feminist STS, and feminist new materialism could be applied to ML research [90] and how intersectionality had thus far been (weakly) operationalized in AI fairness research [91].

Feminism has also informed a range of applied work, including research on the harms of online advertising systems [134] and workplace surveillance [28], as well as participatory processes for ML design [145]. Other papers at FAccT have explored the topic of gender more concretely, including the issue of gender bias in publicfacing tools (e.g. autocomplete [97]) and research methods (e.g. NLP [48]), as well as in the field of computer science itself [27]. But this work remains a small minority. In their 2022 meta-analysis of AI ethics research conducted at FAccT, Birhane et al. concluded that 'the field would benefit from an increased focus on ethical analysis grounded in concrete use-cases, people's experiences, and applications as well as from approaches that are sensitive to structural and historical power asymmetries' [17]. The principles offered here respond to that call by providing a feminist framework to structure this necessary work.

## 3.2 Feminism and AI

Looking more broadly, scholarship on feminism and AI stretches back at least to Alison Adam's 1998 book, Artificial Knowing [3], which, as Keyes and Creel [85] remind us, used feminist epistemology to challenge the presumed universality of AI research of the time. One of the most widely-cited scholarly papers in AI ethics

today, by Joy Buolamwini and Timnit Gebru, employed an intersectional feminist perspective to analyze corporate computer vision systems [22]. As the hype surrounding AI has increased, additional work explicitly linking feminism and AI has emerged. Communications scholar Sophie Toupin [147] conducted a critical survey of this literature to create a typology of six ways that feminism and AI have been linked, including feminist ML models, design-based approaches, and feminist influences on AI policy, culture, discourse and science. A recent edited volume, Feminist AI [20], assembled twenty-one chapters ranging in focus from the dearth of women in AI research [151] to Afrofeminist digital futures [110] to the intersection of AI and racial capitalism [69]. In addition, global feminist networks like the Feminist Internet Research Network [112] and the &lt;A+&gt; Alliance [59] have emerged to support action on issues of algorithmic bias, labor and the economy, AI-induced gender violence, and more. Civil society organizations have also contributed to the conversation about feminist, decolonial and emancipatory approaches to AI. These are too numerous to comprehensively list, but some examples include Coding Rights in Brazil [130], IT For Change in India [131], Data Género in Argentina [64], and Pollicy in Uganda [121]. Taken together, these show the wide-ranging relevance and utility of feminism for AI research.

## 4 DATA FEMINISM FOR AI

In the sections that follow, we review the seven principles of data feminism and explain how they can be adapted to AI research. We also discuss the possibility of two additional principles that address new considerations brought about by AI's increasing scope and impact on both people and the planet.

## 4.1 Principle 1: Examine Power

Data feminism begins by analyzing how power operates in the world.

The first principle of data feminism is to examine power: 'the current configuration of structural privilege and structural oppression in which some groups experience unearned advantages-because systems have been designed by people like them and work for people them-and other groups experience systematic disadvantages-because those same systems were not designed by them or with people like them in mind' [51]. When connecting this understanding of power to data science, we focused on issues of unequal power with respect to minoritized groups-and in particular, on the effects of the under-representation of women and other minoritized groups 1) in the field of data science; 2) as the shapers of research questions; and 3) as the subjects of data-scientific research. The predominance of cisgender men - and the exclusion, even banishment, of women (cis and trans) and nonbinary people, as well as Black, Indigenous, and other people of color, especially when they speak out about AI harms - like Timnit Gebru and Margaret Mitchell is even more acute in AI research and systems development [149]. While Data Feminism touched on the role of corporate interest in determining the focus of data creation efforts and research agendas, we were not yet required to contend with the near-total 'capture' of AI research and deployment by corporations that has since taken place [155]. Given this, it is clear that examining power in AI must also centrally involve examining economic power, and the capitalist systems that facilitate the extraction, aggregation, and consolidation of financial resources.

Recent scholarship has also drawn attention to the colonial dynamics of AI research [34, 127, 146]. We see this clearly in the outsourcing of low-paid, traumatizing data-labeling jobs to economically vulnerable people in the Global South which we discuss further in Principle 7 on labor. Here, we emphasize how these capitalist and colonialist dynamics leave so-called "externalities": effects not accounted for in corporate profit models. These include stark environmental impacts, the erosion of job quality, the increased surveillance of workers, a range of harmful incursions on civil rights, outright discrimination, and even death, as we are witnessing right now in Gaza (and discussed in Principle 6, on context). From a macro perspective, when monetary gain is the primary driver of research, we remain in a world in which issues of war, colonialism, racism, and sexism remain unaddressed, since these "externalities" do not contribute to the corporate bottom line.

The capitalism at work in the US, the current epicenter of corporate AI research, has been variously named surveillance capitalism [160], oligarchic capitalism [60], and even neofeudalism [44]. Looking back, we can also see its power emerge in the racial capitalism described by Cedric Robinson and others. This is the idea that racialized exploitation has gone hand in hand with capitalist accumulation-indeed that markets are reliant on the production of social hierarchies whereby some groups may own, accumulate, and thrive, and others are excluded, exploited, and marked for premature death. Racial capitalism is also always gendered capitalism , as work by Angela Davis [42], Silvia Federici [57], Verónica Gago [61] and other Marxist feminists have shown. These are systems in which wage gaps, property laws, gender norms, debt structures, and the lack of reproductive rights conspire to maintain women and genderqueer people as a global economic underclass, with additional repercussions for those who are also Black and brown. In short, capitalism is premised upon the preservation of unequal power: of the enforcement of the racial, gendered, and other social hierarchies which enable the extraction of labor, and therefore value, from the many for the profit of the few [35]. These dynamics are clearly visible in the current landscape of AI, in which research agendas are similarly set by the few. It is no surprise, then, that their goal is to maximize corporate profit and to preserve (or even increase) the social and political power that enables it.

The artist Mimi O . nu . o . ha's Library of Missing Datasets [116] underscores this point. For O . nu . o . ha, missing datasets serve as 'cultural and colloquial hints of what is deemed important' and what is not. Whencorporations operate without restrictions or regulations, they determine which issues (or groups of people) are worth collecting data about, and which issues (or people) can be ignored. Shareholder interest and public interest are rarely aligned. Corporate choices determine what research questions can be asked, what analysis can be undertaken, which models can be trained, and which users will ultimately be served by those models. We have seen this play out very visibly with respect to LLMs, where researchers such as Emily Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell were quick to point out how the models' training data was far from representative, even if its size was substantial [12]. Subsequent research has documented more specific

biases against women [92], Muslim people [2], racial and ethnic minorities [115, 132], specific social roles [101], and more.

These evaluations are necessary for both generative and predictive AI. In terms of predictive AI, in the housing sector, for example, we have seen how automated tenant screening 'services' rely on eviction records, arrest records, and credit scores in order to rate tenants on their predicted ability to pay rent. These records are low-quality due to the non-standardized ways in which they are published and scraped, as well as the highly racialized nature of the housing sector in the US [47, 111, 128, 141]. Relying on them disproportionately impacts people of color, and systematically furthers rather than mitigates structural oppressions.

Compounding the issue is how these models continue to be both framed and used as 'foundation' models, implying that they serve as a stable basis on which trusted research can be built. Until this misconception is corrected, and likely in perpetuity, we must insist on preventative evaluation, accompanied by an intersectional analysis of power, as a basic starting point. This is in line with the USOffice of Science and Technology Policy's Blueprint for an AI Bill of Rights [114] and the more recent White House Executive Order on AI, which calls for "robust, reliable, repeatable, and standardized evaluations of AI systems, as well as policies, institutions, and, as appropriate, other mechanisms to test, understand, and mitigate risks from these systems before they are put to use " (emphasis ours) [77].

Here is also where feminist and anti-capitalist critiques of power converge, pointing to the social and historical causes for why certain data sources may be biased against certain populations, as well as to the economic causes for why oppressive and extractive systems exist in the first place. Put another way: a feminist approach to examining power in AI must 'ask the other question,' as critical race theorist Mari Matsuda explains, 'look[ing] for both the obvious and nonobvious relationships of domination,' and allowing us to 'see that no form of subordination ever stands alone' [106]. In the case of the housing sector, it is both power and profit that drives the informatic asymmetry between the landlords, who can know almost everything about their tenants because they have so much more data and AI on their side, and the tenants, who can know very, very little about their landlords. Tenants do not have the data or the tools to explore, for example, their landlord's history of evicting people unfairly, or of not maintaining their properties. In short, they do not have the tools to help them build power. A feminist, anti-capitalist approach to AI would focus on designing systems in the service of tenant power - and in the service of all of those excluded by the current rich-get-richer scheme of corporate AI research and development.

## 4.2 Principle 2: Challenge Power

Data feminism commits to challenging power and working towards justice.

The second principle of data feminism is to challenge the unequal distributions of power that we encounter in the world. In Data Feminism , we proposed several methods for challenging power in datasets and data projects, as well as for using data science to directly confront corporations and governments. These included collecting counterdata, analyzing data of all kinds with a justiceoriented lens, imagining alternative ways of doing data science, and teaching - laying the groundwork for others to continue this work through data and statistical literacy efforts. In the context of AI, many of these approaches hold. We still need to respond to political demands to collect missing data about underrepresented people and understudied issues. We also need additional ways of devising models and interpreting their output in ways that work towards justice. We need to provide holistic AI education that integrates social and ethical concerns with a healthy dose of history-and crucially, taught by historians and other humanities scholars-so that we stop graduating CS students who are overconfident and underprepared for the complexities of the real world. We need sound laws and policies to reign in corporate power. Finally, we need more creative and participatory and democratic ways to imagine alternative uses of AI, and the space to reject the use of AI if desired or required.

We also need to continue to envision projects that employ AI in the service of justice, such as generating alt-text for images to enhance visual accessibility when it is missing [104]. At the moment, however, there are far too few examples of justice-oriented AI work. As has been observed, this may be due to the fact that the data requirements for LLMs and other generative AI models are so large that they cannot account for small-data approaches [88]. It might also be due to the fact that these models must be trained on data from the past, and the past is rife with structural inequalities that the models learn [29]. Furthermore, these models intentionally generate output from the center of any probability distribution, rather amplify lower-probability possibilities. By contrast, feminists contend that outliers-in language as in life-tell us far more than data points at the center [49] [154]. This combination of "features" means that both predictive and generative AI are status quo machines -truly excellent at reproducing existing conditions and shaping the future in the image of the racist, sexist, ableist, transphobic past.

Examples of each of these aspects to challenging the power of AI are already underway. For example, in New Zealand, a group of Indigenous-led researchers are training an ML-backed speechto-text system to assist in revitalizing Te Reo Maori, the language spoken by the Maori people [125]. Meanwhile, LLM researchers are working to create explicitly multilingual datasets [81] and models for low-resource languages [98]. Of course, more data collection is not always the "solution" to problems of inequality. In many cases, additional data collection can lead to demonstrable harm. This is the 'paradox of exposure' that we name in Data Feminism , "the double bind that places those who stand to significantly gain from being counted in the most danger from that same counting (or classifying) act" (p 105). So as we celebrate these specific examples, we must also remind ourselves to ask before beginning any new project whether a technical intervention is even appropriate, as well as whether we along with the frontline communities we serve have together considered the range of possible harms [30, 159].

Viewed another way, however, the probabilistic basis of these models can be harnessed to challenge and rebalance power. As Wendy Kui Hyong Chun helpfully articulates via the example of models of climate change, these models 'offer us the most probable future, given past and current actions, not so that we will accept their predictions as inevitable, but rather so we will use them to help

change the future ' (emphasis ours) (26). Following Chun, what if we treated the biased output of LLMs not as any ground truth but as but as motivation for intervention so that those biases are no longer experienced in the world?

At the same time, the scope and scale of the corporate capture of AI also requires a commitment to collective visioning and collective action. There is valuable work being done on data trusts as an alternative to standard data repositories, such as the Worker Info Exchange, which pools data from rideshare workers so that they can ask questions about their employers [56]. In the LLM space, we might look to the BLOOM model, an attempt at training a fully-documented large language model in an open, collaborative way [158]. Researchers are also considering how they might form independent research coalitions in order to lobby for access to the data of Big Tech that they need in order to conduct their work [105]. And workers whose livelihoods are being threatened by AI are turning to unions and organizing in order to advocate for the conditions they need in order to thrive [8, 153].

It is also possible to use both generative and predictive AI to speculate in the Afrofuturist sense-to assist in envisioning otherworldly [26, 32] or alternate futures. For example, work led by Wonyoung So [141] employs reparative algorithms in order to evaluate which possible interventions might be most effective to close the wealth gap between Black and white families in the United States, an example of how AI can work towards rectifying an unjust status quo.

These interventions at the level of civil society must be accompanied by government regulation and policy. The EU AI Act, if implemented with force, promises to become a powerful tool to limit the incursion of Big Tech into personal lives, as does the US Executive Order on AI-for as long as it remains in effect. Some countries, such as Canada [25] and Denmark [129], have gone even further, incorporating explicitly feminist and anti-oppressive policies into their governments. We will, of course, always continue to require resistance in the form of care, community, solidarity, and mutual aid - nothing less than a recognition of the shared humanity that capitalism would have us forget. If we are going to resist the complete capture of AI by capitalist forces, then we must return to these emphatically human and anticapitalist models as our guides.

## 4.3 Principle 3: Rethink Binaries and Hierarchies

Data feminism requires us to challenge the gender binary, along with other systems of counting and classification that perpetuate oppression.

The third principle of data feminism derives from the false binary that Western culture has constructed between the category of 'man' and the category of 'woman.' There are more than two genders, of course, and a fundamental commitment of feminist thought is to equality for all genders. Furthermore, binaries are often hiding hierarchies, and the gender binary is a key example. It hides a hierarchy-patriarchy-in which cisgender men are on top, dominating social institutions from corporate boards to government leadership positions; women, trans, and genderqueer people are intentionally kept on the bottom; and there is no space at all for anyone in between. One need only look at the skewed gender balance of the signers of the 'AI pause' letter, those invited to testify before the US Congress about AI, or the placement of Larry Summers-infamous for his derogatory comments about women and science [74]-at the head of the reconstituted board of OpenAI, in order to see the obvious issues with inclusion in AI. But the issues with the gender binary and with the hierarchy it maintains run much deeper-into the formation of the field of AI research.

We also see this power move taking place on a conceptual level, in which consumers of AI technologies are being encouraged to believe that its capabilities are magical and mysterious, and thus impossible for any normal person to understand. This, too, has a historical antecedent in the way that software developers became figured as 'wizards' and 'sorcerers' in a way that made the acquisition of technical expertise seem off-putting to those without access to formal training mechanisms [54, 75]. But it also serves a strategic purpose: it makes it appear to end-users that they should not question or make demands of such tools; to outside researchers that they should not demand any documentation or ability to peek under the hood; and to government regulators that they lack the knowledge to draft policy or legislation to govern AI. We run the risk of repeating this pattern when we accept the language of AI's 'emergent properties' and the hagiography of the field's supposed founders, as echoed in a 2023 New York Times article that elevated exclusively men, mostly ultra-rich and white [86]. This is why the inclusion implied by the slogan of the Algorithmic Justice League is so important at this present moment: "If you have a face, you have a place in the conversation." [93].

The field of data science-the focus of Data Feminism -has never been known for its exemplary inclusion. But the scope and scale of the conversation around the power and perils of data did lead to an observable change-if not a full recalibration-in inclusivity in the field. In under a decade, Harvey Mudd College, under the leadership of president Maria Klawe, increased the percentage of women computer science majors to over 50%, demonstrating that CS professors and administrators that whine about "the pipeline" have no leg to stand on [87, 157]. But it is not a coincidence that as the broader fields of data science and computer science have grown more inclusive, a subset of researchers from within those fields have begun to pull away in order to reconstitute the field of AI. That this group is WEIRD-Western, educated, industrialized, rich, and (arguably) democratic [138]-and also heavily dominated by white cisgender men, appears to us as evidence of another iteration of a common trend in technical fields: the emergence of new gatekeeping mechanisms in order ensure that those at the top are able to maintain their elite status in the field.

It is also important to recognize the very real ways the gender binary has been operationalized and even weaponized in AI research. Beyond ample evidence of the gender biases against cisgender women, and trans and nonbinary people, that are entrenched in LLMs, AI researchers appear to have an antiquated understanding of gender itself [48, 84, 136, 142]. This lack of understanding of gender, in turn, leads to the inaccurate (at best) and harmful (at worst) research design and applications. And sex fares no better [5]. There is a near-universal failure to understand that neither sex nor gender are essential, "natural", or fixed properties, and they certainly cannot be "detected."

One might extend this gender trouble to other projects that seek to identify, measure, or otherwise quantify complex aspects of social and physical difference. From the entire industry being built around 'Emotion AI,' which seeks to measure emotion in images and in real-time video, to employment screening services that purport to measure employability, to what has been termed neo-phrenology : an attempt to measure emphatically immeasurable qualities such as intelligence or criminality using algorithmic means [143]. In these and any new proposed use cases, we must ask ourselves whether they are reinforcing binaries - gender binaries, patriarchal hierarchies, and paternalistic stereotypes - and if so, instead think otherwise about how to model gender, how to build tools for gender safety, and how to create a world where technologies cease to erase and harm trans and non-binary people.

## 4.4 Principle 4: Elevate Emotion and Embodiment

Data feminism teaches us to value multiple forms of knowledge, including the knowledge that comes from people as living, feeling bodies in the world.

Elevating emotion and embodiment with respect to data is connected to the previous principle, because emotion is often viewed in binary opposition to reason, as a "feminized" element that should be exiled from scientific research because it is subjective as opposed to objective; "soft" (feminine stereotype) instead of "hard" (masculine stereotype). With a critique of hidden hierarchies in mind we can also return to the binary between reason and emotion and recognize 1) how it is a false binary, and there is no such thing as purely rational science [83]; and 2) how this binary is hiding a hierarchy, in which emotion, embodiment, lived experience, and other feminized ways of knowing are relegated to lower status forms of knowledge.

This feminist objectivity bears relevance to AI research in myriad ways, not the least in how the conversation around the risks and harms related to AI has unfolded over the past year or so. On the one hand, we have seen those in positions of power (who, not coincidentally, themselves represent dominant social groups) sound the alarm on future hypothetical harm-of AI robots developing nuclear weapons and other questions of moral 'alignment.' On the other, we have seen AI researchers with equal technical expertise, enhanced by the 'empiricism of lived experience' as women (cis and trans), queer people, and/or people of color, calling attention

The tools to challenge this false binary come from feminism. Feminist philosopher Donna Haraway's concept of situated knowledges -the idea that knowledge originates at a particular time, in a particular place, and from within a particular set of social and political contexts-helps us recognize how all knowledge, including scientific knowledge, is shaped by the perspectives of the people who produce it [72]. More recently, Black feminist theorist Katherine McKittrick points us to how there are multiple systems of knowledge-making, and to how these must be understood as sites of "collaborative praxis" in and of themselves, and as relational with respect to one another [107]. Crucially, a refined awareness of multiple knowledge systems enhances, rather than detracts from, our collective knowledge.

to the harms being perpetuated by AI systems right now [66]. Famously, Dr. Joy Buolamwini, a Black woman computer scientist, undertook an "evocative audit," and used the example of her own face to demonstrate the poor accuracy of facial recognition software. She then connected that evidence to the greater harms being unleashed as these inaccurate software systems were (and remain) deployed in police departments which are already inextricable from structural racism [21]. We have also seen research documenting the misogyny and racism, as well as instances of rape and pornography, in multimodal datasets such as LAION, resulting in LAION pulling its datasets from circulation [65]. Very crucially, this research was first performed by Black women-led research teams who brought their personal experience with these datasets to their research. Yet it was only when this research was replicated by white researchers at Stanford that the team behind LAION took note. This is unfortunately already a pattern in AI research in which Black women who bring their embodied experience to their research have their concerns dismissed, only to have those concerns validated by people from dominant groups, often years later [12, 149].

It is not just that the full range of human experience should be incorporated into assessments of AI harms. Another binary that must be challenged has to do with our perception that work with data and AI can only serve to increase efficiency, automation, and control. Recent research describing the data practices of feminicide data activists has shown how the production of data can become a tool of intimacy, relationality, care, and memory work [63]. Scholar and activist Helena Súarez Val has described these works as "affect amplifiers," translating feminist grief and rage into public action [150]. This form of collective, community-engaged, intentionally emotion-laden work can also remind us of our interdependence, a key idea from disability studies that emphasizes how "we rely on each other, and our actions can have consequences on others" [9]. As we move forward, we must continue to consider how to craft AI systems that focus on "practicing alliance," create "carewebs and pods," and are designed for "misfitting.' [46, 62, 120]. Emerging steps towards such systems are happening in human computer interaction with conversations around trauma-informed computing, healing databases, and restorative/transformative data science, and should be expanded to AI research.

## 4.5 Principle 5: Embrace Pluralism

Data feminism insists that the most complete knowledge comes from synthesizing multiple perspectives, with priority given to local, Indigenous, and experiential ways of knowing.

The principle of embracing pluralism builds from the previous principle, which emphasizes alternate forms of knowing. Here, the emphasis is on how we might represent multiple perspectives in our work. Crucially, 'multiple perspectives' does not simply mean multiple opinions. Building on the work of Sandra Harding, and more recently, the Design Justice Network, 'multiple perspectives' has become a beacon for the wide range of experiences, social positions, and places in the world from which people produce knowledge. The central premise of embracing pluralism is that we can gain better, more detailed, more accurate, and ultimately more truthful

knowledge if we pool these perspectives together, paying particular attention to the perspectives of those who are most directly impacted by the issue at hand.

The prospect of embracing pluralism becomes harder in terms of both architecture and access when scaling up to larger models. LLMs must be pre-trained by those with access to both data and compute, introducing barriers to early-stage participation that had previously not existed. While the push for more open-source models (e.g. BLOOM, OLMo) and more transparent models (e.g. the Stanford Transparency Index) are certainly welcome, the technical requirements of training such models make it far more difficult to involve impacted individuals and small groups in their creation. One recent example that suggests a path forward comes from a project led by Maria Antoniak [10] which surveyed both pregnant people and healthcare providers about their ideas and fears about the possibility for an AI-assisted chatbot that might help to navigate the experience of pregnancy. Like the feminicide classifier, Antoniak et al. very crucially did not build the chatbot first and then ask for feedback; instead, they created a study that included people with multiple forms of expertise and that enabled them to share that knowledge. The output of the project was a set of values, distilled from the participants themselves, that might guide future work.

Embracing pluralism is of crucial importance in the context of AI research, both because of the narrow demographic composition of AI researchers themselves (see Principle 3, on binaries and hierarchies), and because of the imbalance of power between those currently designing AI systems, and those subject to their decisions (see Principles 1 and 2, on power). Put simply: the field of AI needs to develop more participatory, more responsible, and more humble methods for being in dialogue with impacted communities. Catherine had the opportunity to learn this first-hand in a project involving the co-design of AI tools in collaboration with grassroots feminist data activists across the Americas. Together, they decided to develop an ML classifier for detecting news articles about feminicide, a systematic violation of human rights that involves the gender-related killings of women and girls. The tool was intended to streamline the work of the activists. But the activists pushed back on fully automating the process for two reasons: (1) the impossibility of generalizing any definition of feminicide, and (2) because of how they saw their work as a form of memory justice: a way to honor the lives of the women killed so that they would not be forgotten. The result was a tool that has seen impressive uptake by human rights monitoring groups around the globe. This stands in contrast to the numerous ML/AI tools created without consultation, which lack both users and use.

There is another level at which we might conceive of pluralism, which is at the scale of government and those democratic bodies which have been designed to serve the public interest. We see this approach to embracing pluralism in the ideas of public-interest AI [19] and digital public infrastructure [161]. Like physical infrastructure - roads, parks, schools - these systems would be conceived and designed through public processes, guided by the public interest, and come with transparency and governance requirements. While we have seen important lawsuits and other regulatory processes directed at corporations and the technologies they develop, we might also envision citizen-led design processes in which ideas are sourced from communities and projects are funded (and maintained) with taxpayer dollars.

Computer science continues to push out women and people of color at a scale not seen in other sciences. It systematically fails to teach the value of context-sensitive, culturally appropriate technology development, and engages in masculinist fantasies of foundational models ("one model to rule them all"). The singular hubris of this field seems to know no bounds, even as evidence of harm surfaces over and over again. We have also observed how fields such as HCI and social computing draw from expertise in, for example, anthropology, sociology, and literary studies, even as CS departments refuse to hire scholars with PhDs in those fields to train their students. If we are sincere in our belief that the best, most accurate, most truthful knowledge comes from the pooling of multiple perspectives, then we must restructure our own research processes and funding models in order to account for this fact.

These examples of participatory design notwithstanding, it is important to acknowledge that 'participation is not a design fix for machine learning,' as Mona Sloane et al. have explained [140]. Participation can be tokenizing, extractive, under-resourced by researchers inexperienced with the investment required, or incorporated too late in the design process to influence its outcomes, among other issues that the authors raise. We must continue to push back against what Lorraine Daston might call the 'epistemic virtues' of AI/ML research, which echo those of the larger field of computer science: its emphasis on novelty, generalizability, and efficiency, which not only over-determine the kinds of research that are support but are also implicitly believed to be best [40]. While much attention has been devoted to the capture of technical research by corporations, it is also true that there exists a capture of academic research by the field of computer science. Those of us in academia have observed the trends in the composition of college majors, and resultant faculty hiring, leading to outsized resources being invested in computer science and the forms of research it supports to the detriment of all other fields.

## 4.6 Principle 6: Consider Context

Data feminism asserts that data are not neutral or objective. They are the products of unequal social relations, and this context is essential for conducting accurate, ethical analysis.

The sixth principle, to consider context, is at once universally applicable to AI research and yet (nearly) universally ignored. This principle applies most directly to the issue of training data, which we began to discuss with respect to language models in the principle on power. Related work has shown how additional (human) decisions made during the training and filtering process introduce additional biases into these datasets, such as for text written by those of higher socioeconomic status [67], and those who occupy specific social and professional roles [101]. Related research has explored text-to-image models, with findings related to the reproduction of sexist, racist, and colonial biases [113, 124]. What we learn from feminism is how understanding the contexts in which these datasets are created, and more broadly, a recognition of how all data is shaped by unequal social relations, is essential for identifying any downstream biases or potential harms. It is also necessary

for ensuring that research questions are properly framed, that evidence is properly analyzed, and that any claims that might be made on the basis of that analysis are properly scoped.

A final consideration with respect to context has to do with historical context-and in particular, of the historical context of AI research itself. This history is an uncomfortable one, as it points to the longstanding complicity of the field of computer science with US military research. This dates back to the DARPA-funded creation of ARPANET, the precursor to the internet [4]. It carries through to the construction of OntoNotes, the hand-labeled language model underlying many common NLP libraries, which was another DARPA-funded project [123]; and to specific approaches to language modeling, such as topic modeling, which came about through a government desire to monitor global newswire messages at scale [16]. More recently, news reports have drawn our attention to the role that Amazon has played in cloud storage for the US Immigration and Customs Enforcement agency (ICE) [71]; and how Google developed technology to make drone strikes more accurate, which they then sold to the US government [118]. These developments are no longer theoretical, as evidence emerges that they are being put to use by Israel against Palestine, and with horrific human costs [41]. As advocacy efforts such as #NoTechForApartheid [11] and publications such as Logic(s) Magazine [1] powerfully remind us, these inhumane, destructive, and-in the context of Palestine, genocidal-contexts are those in which far too many technical innovations are put to use. We, as AI researchers, can no longer claim ignorance about these contexts as among the uses for our work.

There exist several valuable recommendations for how to restore context to ML models more broadly, such as Eun Seo Jo and Timnit Gebru's suggestion to adopt practices similar to those employed by archivists when curating and documenting datasets [80], and Mitchell et al. 's proposal of 'model cards for model reporting' when releasing a model for general use [108]. We would be well-served by considering how these might be adapted to the current AI/ML landscape, particularly given that neural-network-based architectures of generative AI models make it difficult for the end user (or anyone) to trace any specific model output back to the sources that contributed to it. But to do this work, we must challenge the present stratification of labor in the data science pipeline - the erroneous idea that curating, labeling, and documenting data is somehow unskilled labor and that the analysis and modeling part of the pipeline is where the "science" is at [58]. As archivists and others in the humanities know well, the work of creating a dataset that accurately represents the research question at hand is long, painstaking, and premised on deep expertise. Yet the dominant mantra of Big Tech remains 'move fast and break things.' This fundamental mismatch of value folds back into the capitalist critique that began this paper; so long as the production and curation of data is not seen as valuable, then we will not be able to sufficiently support this type of human expertise. Moreover, the AI systems that we produce with naive and ill-considered data will just plain get things wrong. These are the "data cascades" that Sambivasan et al. identify as amplifying into major quality problems downstream [133].

## 4.7 Principle 7: Make Labor Visible

The work of data science, like all work in the world, is the work of many hands. Data feminism makes this labor visible so that it can be recognized and valued.

This final principle highlights the many people whose labor enables work with data. In Data Feminism , we observed how the work of data science replicated professional hierarchies, with credentialed data scientists at the top, and those perceived to occupy less technical roles-such as data annotation and content moderation-on the bottom. We also observed how this professional hierarchy could be mapped onto gendered, raced, and ultimately colonial hierarchies, with those in the Global North occupying the high-status and highcompensation roles, and those in the Global South occupying those at the bottom.

The workforce of the Global North is not exempt from the incursion of AI, of course. In the US, we have seen lawsuits by Getty Photography and The New York Times brought against companies peddling generative AI technologies. In addition, the summer and fall of 2023 saw major strikes by the Writers Guild of America [8] and the Screen Actors Guild [153]. These culminated in necessary protections against the use of AI to revise scripts and in the creation of likenesses of human actors. We also celebrate the larger trend towards collective action in the tech sector, and in white collar jobs more broadly, as a counterforce to the relentless individualism championed by capitalism. Graduate students at elite engineering universities in the US are beginning to unionize, joining long-standing unions at public institutions including the University of California system, the Cal State system, and the City University of New York, among others. We must continue to ensure that these efforts at building solidarity cut across lines otherwise drawn by technical expertise. After all, the history of colonialism tells us that those at the lower end of labor hierarchies will be the ones most impacted by any move to increase profit or workplace efficiency. Without solidarity across class, race, gender, and work sector, capitalist power will only continue to accrue.

Because the current configuration of AI research is premised upon the consolidation of significant resources-technical and economic as well as human-this colonial structure has become solidified as the fundamental framework on which AI depends. One need only look at the investigative reporting that followed the initial release of ChatGPT, which showed that this 'artificial' intelligence depended on very human workers in Kenya screening potentially offensive responses in real-time [119]. The location of these workers is not a coincidence. Scholars such as Julian Posada have asserted that companies in the Global North exploit political instability and capitalize on catastrophe to enrich themselves, a familiar and longstanding historical pattern [122]. This joins a long line of research (and evidence in the world) that documents how capitalism is fundamentally dependent upon resource extraction-and on paying as little as possible for those resources, including human labor, in order to maximize profit [35, 127].

## 5 DISCUSSION: FUTURE PRINCIPLES FOR FEMINIST AI

The previous pages document our current thinking about how the seven principles defined in Data Feminism can be applied to AI research, but two topics require additional attention: the environmental impact of AI research and deployment, and issues surrounding consent. Here we summarize our present thoughts on each.

## 5.1 Data Feminism and the Environment

In many ways, questions about the environmental impact of AI follow from how its development and deployment reinforce historical patterns of capitalism and colonialism. Resource extraction, after all, is as much about natural resources as it is about human labor. It has long been observed that the environmental impacts of this resource extraction are experienced unequally, with people in the Global South experiencing the deleterious effects of climate change in far greater measure than those in the Global North, even as they contribute far less to global emissions. Google, for example, used 5.6 billion gallons of water in 2023, up 20% from the prior year [78]. An average Meta data center consumes as much electricity as 150,000 average homes [109]. As current research into the energy and water requirements of LLMs has shown [99], AI seems positioned to further exacerbate these effects. Again, these systems seem positioned to benefit elite users in the Global North, even as they exact their cost on those in the Global South. This is an environmental issue, but it is also a feminist issue, as these effects are not only experienced unequally in terms of geography, but also in terms of gender. A feminist principle for AI about the environment might draw from the several decades of ecofeminist scholarship which has worked to establish the connection between environmental harms and other forms of structural oppression. It might also look to work by Indigenous feminists in Latin America, who view the 'cuerpo-territorio' (body-land) as an interconnected system, and by North American Indigenous feminists who similarly link body and land sovereignty while working towards the end of structural violence against both [24, 45, 100, 139].

## 5.2 Data Feminism and Consent

Consent is also a longstanding feminist concern due to the high rates of rape and sexual violence faced by women, trans and nonbinary people around the world living under cisheteropatriarchy. Most Western laws that address rape and sexual violence have their basis in some form of consent, and there are various feminist formulations of what that might mean, such as the popular 2016 FRIES model from Planned Parenthood where consent is: Freely given, Reversible, Informed, Enthusiastic, and Specific. With that said, there are numerous feminist critiques of consent as being too individually focused, too simplistic, and too binary (e.g. reinforcing heteronormative, gendered stereotypes of aggressive men and gatekeeping women and ignoring queer relations entirely) [6, 52, 102]. Because of the violence and harms already being propagated by AI systems, and because of the likelihood that these harms will continue to increase, we still think it may be useful to formulate a feminist principle for AI about consent in expanded forms, i.e. queer, collective and/or interdependent consent.

AI is currently facilitating the mass, non-consensual exploitation of pornographic images of women in the form of deep fakes. As Danielle Citron teaches us, this is consistent with a number of "intelligent" technologies such as networked cars, mobile phone apps, and more which have been exploited by abusive perpetrators to control and dominate their partners[31]. Then, there are the broader issues of consent that relate to the data sources used to train LLMs and generative AI systems, as discussed in Principle 6. Are online data-including social media posts, family photos, original artwork, journalistic reporting and personal blogs-fair game for inclusion in massive data sets without the creators' knowledge? As we await the development of informed guidelines for fair use, we can be certain that something other than the current system-in which Big Tech steals people's work, exploits it, makes money, and facilitates structural violence along the way-is required. There has already been significant work around consent and technology, including the Consentful Tech Project by Allied Media Projects, and work on what consent means in human computer interaction [79, 95, 144] which we can use to build more relational, inclusive, and liberating systems-and to reject them if they are not.

## 6 CONCLUSION

While the forces of racial, gendered capitalism that are currently shaping AI research are powerful, they are also predictable. They operate in ways we have observed and experienced for centuries. Weoffer these thoughts on feminist principles for AI research, along with our hope, because we know what will happen if we do not change course. The predictability of late-stage capitalism, in some ways, gives us an easy goal: if the status quo is not what we what, then we must follow Ruha Benjamin's call to 'craft the worlds you cannot live without, just as you dismantle the ones you cannot live within' [14].

## ACKNOWLEDGMENTS

We would like to thank Nikki Stevens and Isadora Cruxên for their feedback on early drafts of this paper. This work has been partially supported by a grant from the Mellon Foundation (G-2211-14240).

## REFERENCES

- [1] Khadijah Abdurahman. 2024. "Logic(s) Magazine". https://logicmag.io/. [Accessed 30-04-2024].
- [3] Alison Adam. 1998. Artificial knowing . Routledge, London, England.
- [2] Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent Anti-Muslim Bias in Large Language Models. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society (AIES '21) . Association for Computing Machinery, New York, NY, USA, 298-306. - [4] "Defense Advanced Research Projects Agency". 2024. "ARPANET". https://www. darpa.mil/about-us/timeline/arpanet. [Accessed 30-04-2024].
- [6] Linda Martín Alcoff. 2018. "Rape and Resistance" . Polity, "New York". https: //www.wiley.com/en-us/Rape+and+Resistance-p-9780745691916
- [5] Kendra Albert and Maggie Delano. 2022. Sex trouble: Sex/gender slippage, sex confusion, and sex obsession in machine learning using electronic health records. Patterns 3, 8 (Aug. 2022), 100534. - [7] "Data-Pop Alliance". 2022. "Data Feminism: Intersectional Data-Driven Advocacy and Policy for Gender Equality". https://datapopalliance.org/ourwork/thematic-programms/program\_data\_feminism/
- [9] Natasha Ansari. 2024. "Welcome to the Data + Feminism Lab! LAB HANDBOOK". "https://docs.google.com/document/d/1B48\_ Gub2ik6FA7Ly0zxn9q7ioQXLf7htMkFLbKxo\_zE/"
- [8] Dani Anguiano and Lois Beckett. 2023. "How Hollywood writers triumphed over AI - and why it matters". https://www.theguardian.com/culture/2023/ oct/01/hollywood-writers-strike-artificial-intelligence

- [10] Maria Antoniak, Aakanksha Naik, Carla S. Alvarado, Lucy Lu Wang, and Irene Y. Chen. 2023. "Designing Guiding Principles for NLP for Healthcare: A Case Study of Maternal Health". https://doi.org/10.48550/arXiv.2312.11803 arXiv:2312.11803 [cs].
- [12] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21) . Association for Computing Machinery, New York, NY, USA, 610-623. - [11] "No Tech For Apartheid". 2024. "No Tech For Apartheid". https://www. notechforapartheid.com/. [Accessed 30-04-2024].
- [13] Ruha Benjamin. 2019. Race after technology: abolitionist tools for the new Jim code . Polity, Medford, MA.
- [15] WLance Bennett and Marianne Kneuer. 2023. "Communication and democratic erosion: The rise of illiberal public spheres". European Journal of Communication 39, 2 (Dec. 2023), 02673231231217378. https://doi.org/10.1177/ 02673231231217378
- [14] Ruha Benjamin. 2022. Viral justice: how we grow the world we want . Princeton University Press, Princeton, New Jersey.
- [16] Jeffrey M. Binder. 2016. "Debates in the Digital Humanities 2016" . "University of Minnesota Press", "Minneapolis", Chapter "Alien Reading: Text Mining, Language Standardization, and the Humanities", 201-217.
- [18] Meredith Broussard. 2018. Artificial unintelligence: how computers misunderstand the world . The MIT Press, Cambridge, Massachusetts.
- [17] Abeba Birhane, Elayne Ruane, Thomas Laurent, Matthew S. Brown, Johnathan Flowers, Anthony Ventresque, and Christopher L. Dancy. 2022. The Forgotten Margins of AI Ethics. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22) . Association for Computing Machinery, New York, NY, USA, 948-958. - [19] Meredith Broussard. 2023. More than a glitch: confronting race, gender, and ability bias in tech . The MIT Press, Cambridge, Massachusetts.
- [21] Joy Buolamwini. 2023. Unmasking AI . RandomHouse, "New York". https://www. penguinrandomhouse.com/books/670356/unmasking-ai-by-joy-buolamwini/
- [20] Jude "Browne, Stephen Cave, Eleanor Drage, and Kerry" McInerney (Eds.). 2024. "Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligent Machines" . "Oxford University Press", Oxford, New York.
- [22] Joy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency . PMLR, New York, 77-91. https://proceedings.mlr.press/v81/buolamwini18a.html
- [24] Lorena Cabnal. 2010. Feminismos diversos: el feminismo comunitario. https://porunavidavivible.files.wordpress.com/2012/09/feminismoscomunitario-lorena-cabnal.pdf
- [23] Daniene Byrne. 2022. Data Feminism-Catherine D'Ignazio and Lauren F. Klein (Cambridge, MA, USA: MIT Press, 2020, 314 pp.). IEEE Technology and Society Magazine 41, 4 (Dec. 2022), 16-18. - [25] "Women Canada and Gender Equality". 2021. Gender-based Analysis Plus (GBA Plus). https://women-gender-equality.canada.ca/en/gender-based-analysisplus.html Last Modified: 2022-10-13.
- [27] Marc Cheong, Kobi Leins, and Simon Coghlan. 2021. Computer Science Communities: Who is Speaking, and Who is Listening to the Women? Using an Ethics of Care to Promote Diverse Voices. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21) . Association for Computing Machinery, New York, NY, USA, 106-115. https: //doi.org/10.1145/3442188.3445874
- [26] Kriston Capps. 2023. "An Architect Uses AI to Explore Surreal Black Worlds". https://www.bloomberg.com/news/features/2023-09-23/with-ai-anarchitect-imagines-a-world-that-centers-blackness
- [28] Shreya Chowdhary, Anna Kawakami, Mary L Gray, Jina Suh, Alexandra Olteanu, and Koustuv Saha. 2023. Can Workers Meaningfully Consent to Workplace Wellbeing Technologies?. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT '23) . Association for Computing Machinery, New York, NY, USA, 569-582. - [30] Marika Cifor, Patricia Garcia, T. L. Cowan, Jasmine Rault, Tonia Sutherland, Anita Chan, Jennifer Rode, Anna Lauren Hoffmann, Niloufar Salehi, and Lisa Nakamura. 2019. Feminist Data Manifest-No.
- [29] Wendy Hui Kyoong Chun. 2021. "Discriminating Data: Correlation, Neighborhoods, and the New Politics of Recognition" . "MIT Press", "Cambridge, MA".
- [31] Danielle Keats Citron. 2022. "The fight for privacy" . "Chatto &amp; Windus", "London, England".
- [33] "Combahee River Collective". 1977. The Combahee River Collective Statement . Combahee River Collective, Boston.
- [32] Beth Coleman. 2024. "Octavia Butler AI". https://realitywaswhateverhappened. com/OBAI
- [34] Nick Couldry. 2019. The costs of connection: how data is colonizing human life and appropriating it for capitalism . Stanford University Press, Stanford, California.
- [35] Nick Couldry and Ulises A. Mejias. 2019. Data Colonialism: Rethinking Big Data's Relation to the Contemporary Subject. Television &amp; New Media 20, 4

(May 2019), 336-349. - [37] Kimberlé Crenshaw. 2022. The Panic Over Critical Race Theory Is an Attempt to Whitewash U.S. History (3 ed.). Routledge, New York, 362-364. https://doi.org/ 10.4324/b23210-38
- [36] Kimberlé Crenshaw. 1989. Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine, Feminist Theory and Antiracist Politics. University of Chicago Legal Forum 1989 (1989), 139-167. Issue 1.
- [38] Patrice Cullors. 2021. "Imagining Abolition". https://blacklivesmatter.com/ imagining-abolition/
- [40] Lorraine Daston. 2014. "The making of the humanities. Vol. 3: The modern humanities" . Amsterdam University Press, Amsterdam, Chapter "Objectivity and impartiality: epistemic virtues in the humanities", 24-41.
- [39] Shiva Darian, Aarjav Chauhan, Ricky Marton, Janet Ruppert, Kathleen Anderson, Ryan Clune, Madeline Cupchak, Max Gannett, Joel Holton, Elizabeth Kamas, Jason Kibozi-Yocka, Devin Mauro-Gallegos, Simon Naylor, Meghan O'Malley, Mehul Patel, Jack Sandberg, Troy Siegler, Ryan Tate, Abigil Temtim, Samantha Whaley, and Amy Voida. 2023. Enacting Data Feminism in Advocacy Data Work. Proceedings of the ACM on Human-Computer Interaction 7, CSCW1 (April 2023), 47:1-47:28. - [41] Harry Davies, Bethan McKernan, and Dan Sabbagh. 2023. 'The Gospel': how Israel uses AI to select bombing targets in Gaza. https://www.theguardian.com/ world/2023/dec/01/the-gospel-how-israel-uses-ai-to-select-bombing-targets
7. Women, race, &amp; class identified, Place of publication not identified.
- [42] Angela Y. (Angela Yvonne) Davis. 2011. . publisher not
- [43] Angela Y. (Angela Yvonne) Davis. 2022. Abolition. Feminism. Now . Haymarket Books, Chicago, IL.
- [45] Sarah Deer. 2015. The Beginning and End of Rape: Confronting Sexual Violence in Native America . University of Minnesota Press, Minneapolis.
- [44] Jodi Dean. 2021. Architecture and Collective Life . Routledge, New York, Chapter Neofeudalism: The end of capitalism?, 42-54.
- [46] Victor Del Hierro, Daisy Levy, and Margaret Price. 2016. "We Are Here: Negotiating Difference and Alliance in Spaces of Cultural Rhetorics". https: //enculturation.net/we-are-here
- [48] Hannah Devinney, Jenny Björklund, and Henrik Björklund. 2022. Theories of 'Gender' in NLP Bias Research. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22) . Association for Computing Machinery, New York, NY, USA, 2083-2102. https://doi.org/10.1145/ 3531146.3534627
- [47] Matthew Desmond. 2016. Evicted: Poverty and Profit in the American City . Crown, New York.
- [49] Catherine D'Ignazio. 2021. Uncertain archives: critical keywords for big data . MIT Press, Cambridge, MA, Chapter Outlier, 377-387.
- [51] Catherine D'Ignazio and Lauren Klein. 2020. Data feminism . The MIT Press, Cambridge, Massachusetts.
- [50] Catherine D'Ignazio. 2022. "Data as a Cornerstone for a Feminist Development Cooperation". https://www.blog-datalab.com/home/data-feminism-eventseries/
- [52] Avery C. Edenfield. 2019. Queering consent: design and sexual consent messaging. Communication Design Quarterly 7, 2 (Aug. 2019), 50-63. https: //doi.org/10.1145/3358931.3358938
- [54] Nathan L. Ensmenger. 2012. "The computer boys take over" . "MIT Press", "London, England".
- [53] Colbi Edmonds. 2024. Maine Secretary of State Targeted by 'Swatting' After Trump Ballot Decision. https://www.nytimes.com/2024/01/01/us/shennabellows-politicians-swatting.html
- [55] Virginia Eubanks. 2018. Automating inequality: how high-tech tools profile, police, and punish the poor (first edition ed.). St. Martin's Press, New York, NY.
- [57] Silvia Federici. 2022. Foreword: The Significance of 'Racial Capitalism. Journal of Law and Political Economy 2, 2 (2022), 119-121. https://papers.ssrn.com/ abstract=4576305
- [56] James Farrar, Cansu Safak, and Sidra Zabit-Foster. 2024. "Worker Info Exchange". https://www.workerinfoexchange.org/
- [58] Melanie Feinberg. 2022. Everyday Adventures with Unruly Data . MIT Press, Cambridge.
- [60] Joe Foweraker. 2021. Oligarchy in the Americas: comparing oligarchic rule in Latin America and the United States . Palgrave Macmillan, Cham.
- [59] "&lt;A+&gt; Alliance for Inclusive Algorithms". 2024. "&lt;A+&gt; Alliance for Inclusive Algorithms". https://aplusalliance.org/
- [61] Verónica Gago. 2017. Neoliberalism from below: popular pragmatics and baroque economies . Duke University Press, Durham.
- [63] Kathomi Gatwiri, Savia Hasanova, Anna Kapushenko, Lyubava Malysheva, Saide Mobayed Vega, Audrey Mugeni, Rosalind Page, Ivonne Ramírez, Helena Suárez Val, Dawn Wilcox, and Aimee Zambrano Ortiz. 2023. Feminicide Data Activism (1 ed.). Routledge, London, 103-113. - [62] Rosemarie Garland-Thomson. 2011. Misfits: A Feminist Materialist Disability Concept. Hypatia 26, 3 (July 2011), 591-609. 9781003202332-13

- [65] Sharon Goldman. 2023. A free AI image dataset, removed for child sex abuse images, has come under fire before. https://venturebeat.com/ai/a-free-ai-imagedataset-removed-for-child-sex-abuse-images-has-come-under-fire-before/
- [64] "Data Genero". 2023. https://datagenero.org/proyectos/feminismo-de-datos/
- [66] Anita Gurumurthy. 2018. "Keynote: Anita Gurumurthy (IT for Change)". https: //datajusticelab.org/conference-programme/
- [68] Lelia Marie Hampton. 2021. Black Feminist Musings on Algorithmic Oppression. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21) . Association for Computing Machinery, New York, NY, USA, 1. - [67] Suchin Gururangan, Dallas Card, Sarah Dreier, Emily Gade, Leroy Wang, Zeyu Wang, Luke Zettlemoyer, and Noah A. Smith. 2022. Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 2562-2580. - [69] Lelia Marie Hampton. 2023. Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligent Machines . Oxford University Press, Oxford, Chapter TechnoRacial Capitalism: A Decolonial Black Feminist Marxist Perspective, 119-126. - [71] Karen Hao. 2018. Amazon is the invisible backbone of ICE's immigration crackdown. https://www.technologyreview.com/2018/10/22/139639/amazonis-the-invisible-backbone-behind-ices-immigration-crackdown/
- [70] Leif Hancox-Li and I. Elizabeth Kumar. 2021. Epistemic values in feature importance methods: Lessons from feminist epistemology. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21) . Association for Computing Machinery, New York, NY, USA, 817-826. - [72] Donna Haraway. 1988. Situated Knowledges: The Science Question in Feminism and the Privilege of Partial Perspective. Feminist Studies 14, 3 (1988), 575-599. - [74] Daniel J. Hemel. 2005. "Summers' Comments on Women and Science Draw Ire". https://www.thecrimson.com/article/2005/1/14/summers-comments-onwomen-and-science/
- [73] Megan E. Hatch, Elora Lee Raymond, Benjamin F. Teresa, and Kathryn Howell. 2023. A data feminist approach to urban data practice: Tenant power through eviction data. Journal of Urban Affairs 0, 0 (2023), 1-20. https://doi.org/10.1080/ 07352166.2023.2262629
- [75] Mar Hicks. 2018. "Programmed inequality" . "MIT Press", "London, England".
- [77] "The White House". 2023. Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. https://www.whitehouse.gov/ briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safesecure-and-trustworthy-development-and-use-of-artificial-intelligence/
- [76] Patricia Hill Collins. 2000. Black feminist thought: knowledge, consciousness, and the politics of empowerment (revised 10th anniversary edition. ed.). Routledge, New York.
- [78] Kevin Hurler. 2023. "Google Is Really, Really Thirsty". https://gizmodo.com/ google-water-usage-exploding-with-ai-development-1850673427
- [80] Eun Seo Jo and Timnit Gebru. 2020. Lessons from archives: strategies for collecting sociocultural data in machine learning. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* '20) . Association for Computing Machinery, New York, NY, USA, 306-316. - [79] Jane Im, Jill Dimond, Melody Berton, Una Lee, Katherine Mustelier, Mark S. Ackerman, and Eric Gilbert. 2021. Yes: Affirmative Consent as a Theoretical Framework for Understanding and Imagining Social Platforms. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI '21) . Association for Computing Machinery, New York, NY, USA, 1-18. https://doi. org/10.1145/3411764.3445778
- [81] Eun So Jo and Nils Reimers. 2024. "Clover - Multilingual Dense Retrieval". https://huggingface.co/CloverSearch
- [83] Evelyn Fox Keller. 1996. Reflections on Gender and Science: Tenth Anniversary Paperback Edition (anniversary edition ed.). Yale University Press, New Haven.
- [82] Shubhankar Kashyap and Avantika Singh. 2021. Testing Data Feminism in India. Scholars Journal of Arts, Humanities and Social Sciences 9, 10 (May 2021), 516-530. - [84] Os Keyes. 2018. The Misgendering Machines: Trans/HCI Implications of Automatic Gender Recognition. Proceedings of the ACM on Human-Computer Interaction 2, CSCW (Nov. 2018), 88:1-88:22. - [86] Theodore Kim. 2023. Opinion: AI isn't magic. It's just knowledge sausage. https://www.latimes.com/opinion/story/2023-05-14/ai-googlechatgpt-code-emergent-properties
- [85] Os Keyes and Kathleen Creel. 2022. Artificial Knowing Otherwise. Feminist Philosophy Quarterly 8, 3/4 (Dec. 2022), 1-25. https://ojs.lib.uwo.ca/index.php/ fpq/article/view/14313
- [87] Maria Klawe. 2013. Increasing Female Participation in Computing: The Harvey Mudd College Story. Computer 46, 3 (March 2013), 56-58. https://doi.org/10. 1109/MC.2013.4
- [89] Lauren Klein and Brandeis Marshall. 2022. "Social Justice Frameworks for Leveraging Data Science to Advance Gender Equity".
- [88] Lauren Klein. 2022. Are Large Language Models Our Limit Case? https: //doi.org/10.5281/ZENODO.6567985
- [90] Goda Klumbyt˙ e, Claude Draude, and Alex S. Taylor. 2022. Critical Tools for Machine Learning: Working with Intersectional Critical Concepts in Machine Learning Systems Design. In 2022 ACM Conference on Fairness, Accountability, and Transparency . ACM, Seoul Republic of Korea, 1528-1541. https://doi.org/ 10.1145/3531146.3533207
- [92] Hadas Kotek, Rikker Dockum, and David Sun. 2023. Gender bias and stereotypes in Large Language Models. In Proceedings of The ACM Collective Intelligence Conference (CI '23) . Association for Computing Machinery, New York, NY, USA, 12-24. - [91] Youjin Kong. 2022. Are 'Intersectionally Fair' AI Algorithms Really Fair to Women of Color? A Philosophical Analysis. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22) . Association for Computing Machinery, New York, NY, USA, 485-494. https://doi.org/10. 1145/3531146.3533114
- [93] "Algorithmic Justice League". 2024. "Algorithmic Justice League". https: //www.ajl.org/
- [95] Una Lee and Dann Toliver. 2017. Building Consentful Tech. http://www. consentfultech.io/wp-content/uploads/2019/10/Building-Consentful-Tech.pdf
- [94] Marion Lean. 2021. Materialising Data Feminism - How Textile Designers Are Using Materials to Explore Data Experience. Journal of Textile Design Research and Practice 9, 2 (May 2021), 184-209. https://doi.org/10.1080/20511787.2021. 1928987
- [96] Victor R. Lee, Daniel R. Pimentel, Rahul Bhargava, and Catherine D'Ignazio. 2022. Taking data feminism to school: A synthesis and review of pre-collegiate data science education projects. British Journal of Educational Technology 53, 5 (2022), 1096-1113. - [98] "Lesan". 2024. "Lesan". https://lesan.ai/
- [97] Alina Leidinger and Richard Rogers. 2023. Which Stereotypes Are Moderated and Under-Moderated in Search Engine Autocompletion?. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT '23) . Association for Computing Machinery, New York, NY, USA, 1049-1061. - [99] Pengfei Li, Jianyi Yang, Mohammad A. Islam, and Shaolei Ren. 2023. "Making AI Less 'Thirsty'": Uncovering and Addressing the Secret Water Footprint of AI Models". arXiv:2304.03271 [cs.LG]
- [101] Li Lucy, Suchin Gururangan, Luca Soldaini, Emma Strubell, David Bamman, Lauren Klein, and Jesse Dodge. 2024. "AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters". https: //doi.org/10.48550/arXiv.2401.06408 arXiv:2401.06408 [cs].
- [100] Annita Hetoevehotohke'e Lucchesi. 2022. Mapping Violence against Indigenous Women and Girls: Beyond Colonizing Data and Mapping Practices. ACME: An International Journal for Critical Geographies 21, 44 (May 2022), 389-398.
- [102] Catharine A. MacKinnon. 2016. Rape Redefined General Essays. Harvard Law &amp; Policy Review 10, 2 (2016), 431-478.
- [104] John Mah. 2024. "Alt Text Generator". https://huggingface.co/spaces/JMah/ Alt\_Text\_Generator
- [103] Tasslyn Magnusson. 2024. Book Censorship Database by Dr. Tasslyn Magnusson. https://www.everylibraryinstitute.org/book\_censorship\_database\_ magnusson
- [105] J. Nathan Matias, Rebekah Tromble, Susan Benesch, and Alex Abdo. 2024. Coalition for Independent Technology Research. https://independenttechresearch. org/
- [107] Katherine McKittrick. 2021. "Dear science and other stories" . "Duke University Press", Durham.
- [106] Mari J. Matsuda. 1991. Beside My Sister, Facing the Enemy: Legal Theory Out of Coalition. Stanford Law Review 43 (1991), 1183-1192. Issue 6. http: //hdl.handle.net/10125/67582
- [108] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model Cards for Model Reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency (Atlanta, GA, USA) (FAT* '19) . Association for Computing Machinery, New York, NY, USA, 220-229. - [110] Iyer Neema, Chair Chenai, and Achieng Garnett. 2023. Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligent Machines . Oxford University Press, Oxford, Chapter Afrofeminist Data Futures, 347-388. https://doi.org/10. 1093/oso/9780192889898.003.0020
- [109] Sebastian Moss. 2023. "Meta begins paying underpaid Irish data center electricity bill after ESB Networks error". https://www.datacenterdynamics. com/en/news/meta-begins-paying-underpaid-irish-data-center-electricitybill-after-esb-networks-error/

- [111] Ashley Nellis. 2021. The Color of Justice: Racial and Ethnic Disparity in State Prisons. https://www.sentencingproject.org/reports/the-color-of-justice-racialand-ethnic-disparity-in-state-prisons-the-sentencing-project/
- [113] Leonardo Nicoletti and Dina Bass Technology + Equality. 2023. Humans Are Biased. Generative AI Is Even Worse. https://www.bloomberg.com/graphics/ 2023-generative-ai-bias/
- [112] "Feminist Internet Research Network". 2024. "Feminist Internet Research Network". "https://firn.genderit.org/"
- [114] "Office of Science and Technology Policy". 2023. "Blueprint for an AI Bill of Rights". https://www.whitehouse.gov/ostp/ai-bill-of-rights/
- [116] Mimi Onuoha. 2024. MimiOnuoha/missing-datasets. https://github.com/ MimiOnuoha/missing-datasets
- [115] Jesutofunmi A. Omiye, Jenna C. Lester, Simon Spichak, Veronica Rotemberg, and Roxana Daneshjou. 2023. Large language models propagate race-based medicine. npj Digital Medicine 6, 11 (Oct. 2023), 1-4. - [117] Cathy O'Neil. 2016. Weapons of math destruction: how big data increases inequality and threatens democracy (first edition. ed.). Crown, New York.
- [119] Billy Perrigo. 2023. "Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic". https://time.com/6247678/openaichatgpt-kenya-workers/
- [118] Cheryl Pellerin. 2017. "Project Maven to Deploy Computer Algorithms to War Zone by Year's End". https://www.defense.gov/News/NewsStories/Article/Article/1254719/project-maven-to-deploy-computeralgorithms-to-war-zone-by-years-end/
- [120] Leah Lakshmi Piepzna-Samarasinha. 2018. "Care work: dreaming disability justice" . Arsenal Pulp Press, Vancouver.
- [122] Julian Posada. 2022. The Coloniality of Data Work: Power and Inequality in Outsourced Data Production for Machine Learning . PhD Dissertation. University of Toronto, Toronto. https://tspace.library.utoronto.ca/bitstream/1807/126388/ 1/Posada\_Gutierrez\_Julian\_\_Alberto\_202211\_PhD\_thesis.pdf
- [121] "Pollicy". 2024. "Pollicy". https://pollicy.org/
- [123] Sameer S. Pradhan, Eduard Hovy, Mitch Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2007. "OntoNotes: A Unified Relational Semantic Representation". In "International Conference on Semantic Computing (ICSC 2007)" . IEEE, Irvine, 517-526. - [125] "Papa Reo". 2024. "Papa Reo". https://papareo.nz
- [124] Rida Qadri, Renee Shelby, Cynthia L. Bennett, and Emily Denton. 2023. AI's Regimes of Representation: A Community-centered Study of Text-to-Image Models in South Asia. In 2023 ACM Conference on Fairness, Accountability, and Transparency . ACM, Chicago IL USA, 506-517. https://doi.org/10.1145/3593013. 3594016
- [126] Marisa Revilla Blanco. 2019. Del ¡Ni una más! al #NiUnaMenos: movimientos de mujeres y feminismos en América Latina. Política y Sociedad 56, 1 (2019), 47-67. - [128] Lisa Rice. 2019. "Missing Credit: How the U.S. Credit System Restricts Access to Consumers of Color". https://democrats-financialservices.house.gov/ uploadedfiles/hhrg-116-ba00-wstate-ricel-20190226.pdf
- [127] Paola Ricaurte. 2019. Data Epistemologies, The Coloniality of Power, and Resistance. Television &amp; New Media 20, 4 (March 2019), 350-365. https://doi. org/10.1177/1527476419831640
- [129] Lisa Ann Richey. 2001. "In Search of Feminist Foreign Policy: Gender, Development, and Danish State Identity". , "177-212" pages. https://genderandsecurity.org/projects-resources/research/search-feministforeign-policy-gender-development-and-danish-state
- [131] "Coding Rights". 2024. "Coding Rights: Bridging Development Realities and Technological Possibilities". https://itforchange.net/
- [130] "Coding Rights". 2024. "Coding Rights". https://codingrights.org/en/
- [132] Abel Salinas, Parth Vipul Shah, Yuzhong Huang, Robert McCormack, and Fred Morstatter. 2023. The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations. , 15 pages. https://doi.org/10.1145/3617694.3623257 arXiv:2308.02053 [cs].
- [134] Princess Sampson, Ro Encarnacion, and Danaë Metaxa. 2023. Representation, Self-Determination, and Refusal: Queer People's Experiences with Targeted Advertising. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT '23) . Association for Computing Machinery, New York, NY, USA, 1711-1722. - [133] Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. 2021. 'Everyone wants to do the model work, not the data work': Data Cascades in High-Stakes AI. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (&lt;conf-loc&gt;, &lt;city&gt;Yokohama&lt;/city&gt;, &lt;country&gt;Japan&lt;/country&gt;, &lt;/conf-loc&gt;) (CHI '21) . Association for Computing Machinery, New York, NY, USA, Article 39, 15 pages. - [135] Runa Sandvik. 2023. How US police use digital data to prosecute abortions. https: //techcrunch.com/2023/01/27/digital-data-roe-wade-reproductive-privacy/
- [136] Morgan Klaus Scheuerman, Jacob M. Paul, and Jed R. Brubaker. 2019. How Computers See Gender: An Evaluation of Gender Classification in Commercial

Facial Analysis Services. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (Nov. 2019), 144:1-144:33. - [138] Ali Akbar Septiandri, Marios Constantinides, Mohammad Tahaei, and Daniele Quercia. 2023. WEIRD FAccTs: How Western, Educated, Industrialized, Rich, and Democratic is FAccT?. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT '23) . Association for Computing Machinery, New York, NY, USA, 160-171. - [137] Kyla Schuller. 2021. The Trouble with White Women . Bold Type, New York. https://www.hachettebookgroup.com/titles/kyla-schuller/the-troublewith-white-women/9781645036883/?lens=bold-type-books
- [139] Leanne Betasamosake Simpson. 2017. As We Have Always Done: Indigenous Freedom through Radical Resistance . Uof Minnesota Press, Minneapolis. GoogleBooks-ID: MCp0DwAAQBAJ.
- [141] Wonyoung So. 2023. Which Information Matters? Measuring Landlord Assessment of Tenant Screening Reports. Housing Policy Debate 33, 6 (Nov. 2023), 1484-1510. - [140] Mona Sloane, Emanuel Moss, Olaitan Awomolo, and Laura Forlano. 2022. Participation Is not a Design Fix for Machine Learning. In Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization (&lt;conf-loc&gt;, &lt;city&gt;Arlington&lt;/city&gt;, &lt;state&gt;VA&lt;/state&gt;, &lt;country&gt;USA&lt;/country&gt;, &lt;/conf-loc&gt;) (EAAMO '22) . Association for Computing Machinery, New York, NY, USA, Article 1, 6 pages. https://doi.org/10.1145/ 3551624.3555285
- [142] Karolina Stanczak and Isabelle Augenstein. 2021. A Survey on Gender Bias in Natural Language Processing. http://arxiv.org/abs/2112.14168 arXiv:2112.14168 [cs].
- [144] Yolande Strengers, Jathan Sadowski, Zhuying Li, Anna Shimshak, and Florian 'Floyd' Mueller. 2021. What Can HCI Learn from Sexual Consent?: A Feminist Process of Embodied Consent for Interactions with Emerging Technologies. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . ACM, Yokohama Japan, 1-13. - [143] Catherine Stinson. 2020. "The Dark Past of Algorithms That Associate Appearance and Criminality". https://www.americanscientist.org/article/the-darkpast-of-algorithms-that-associate-appearance-and-criminality
- [145] Harini Suresh, Rajiv Movva, Amelia Lee Dogan, Rahul Bhargava, Isadora Cruxen, Angeles Martinez Cuba, Guilia Taurino, Wonyoung So, and Catherine D'Ignazio. 2022. Towards Intersectional Feminist and Participatory ML: A Case Study in Supporting Feminicide Counterdata Collection. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22) . Association for Computing Machinery, New York, NY, USA, 667-678. https://doi.org/10. 1145/3531146.3533132
- [147] Sophie Toupin. 2023. Shaping feminist artificial intelligence. New Media &amp; Society 26, 1 (Feb. 2023), 580-595. - [146] Jasmina Tacheva and Srividya Ramasubramanian. 2023. AI Empire: Unraveling the interlocking systems of oppression in generative AI's global order. Big Data &amp; Society 10, 2 (July 2023), 20539517231219240. https://doi.org/10.1177/ 20539517231219241
- [148] Georgiana Turculet. 2023. Data feminism and border ethics: power, invisibility and indeterminacy. Journal of Global Ethics 19, 3 (Sept. 2023), 323-334. https: //doi.org/10.1080/17449626.2023.2278533
- [150] Helena Suárez Val. 2021. Networked Feminisms: Activist Assemblies and Digital Practices . Lexington Books/Fortress Academic, Lanham, MD, Chapter Affect Amplifiers: Feminist Activists and Digital Cartographies of Feminicide, 163-186.
- [149] Katlyn Turner, Danielle Wood, and Catherine D'Ignazio. 2021. The Abuse and Misogynoir Playbook . Montreal AI Ethics Institute, Montreal, 14-34.
- [151] Judy Wajcman and Erin Young. 2023. "Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligent Machines" . Oxford University Press, Oxford, Chapter "Feminism Confronts AI: The Gender Relations of Digitalisation", 47-64. - [153] Angela Wattercutter and Will Bedingfield. 2023. "Hollywood Actors Strike Ends With a Deal That Will Impact AI and Streaming for Decades". https: //www.wired.com/story/hollywood-actors-strike-ends-ai-streaming/
- [152] Maggie Walter. 2013. "Indigenous statistics: a quantitative research methodology" . Left Coast Press, Walnut Creek, CA.
- [154] Brooke Foucault Welles. 2014. On minorities and outliers: The case for making Big Data small. Big Data &amp; Society 1, 1 (April 2014), 205395171454061. https: //doi.org/10.1177/2053951714540613
- [156] Katianne Williams. 2021. Data Feminism: D'ignazio and Klein Call Out Inequality in Data. IEEE Women in Engineering Magazine 15, 1 (June 2021), 21-23. https: //doi.org/10.1109/MWIE.2021.3062921
- [155] Meredith Whittaker. 2021. The steep cost of capture. Interactions 28, 6 (Nov. 2021), 50-55. - [157] Laura Winig. 2021. "Harvey Mudd College: Promoting Women in Computer Science through Inclusive Education". https://case.hks.harvard.edu/harveymudd-college-promoting-women-in-computer-science-through-inclusiveeducation/

[158] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Taşar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M. Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Muñoz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A. Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio

Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S. Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2023. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. https://doi.org/10.48550/arXiv.2211.05100 arXiv:2211.05100 [cs].

- [160] Shoshana Zuboff. 2019. The age of surveillance capitalism: the fight for a human future at the new frontier of power (first edition. ed.). PublicAffairs, New York.
- [159] Jonathan Zong and J. Nathan Matias. 2024. Data Refusal from Below: A Framework for Understanding, Evaluating, and Envisioning Refusal as Design. ACM J. Responsib. Comput. 1, 1, Article 10 (mar 2024), 23 pages. https: //doi.org/10.1145/3630107
- [161] Ethan Zuckerman. 2020. "What Is Digital Public Infrastructure?". https://www. journalismliberty.org/publications/what-is-digital-public-infrastructure

## A RESEARCH ETHICS AND SOCIAL IMPACT A.1 Ethical Considerations Statement

This paper is primarily a literature review and theory contribution, thus we did not engage in any human subjects research, systems development or deployment. Our work has been guided by considerations for citational justice, and we have specifically sought to cite the work of scholars from marginalized backgrounds, especially BIWOC and queer people of color, as well as forms of knowledge production not recognized by the academy, including activism, journalism, art, design and creative communication projects.

## A.2 Researcher Positionality Statement

We are a two-person writing team that brings domain expertise in the humanities (including historical and literary scholarship), digital humanities (including NLP/ML), urban planning, software development, data science, and data visualization. As two cisgender women, we share an interest in and commitment to gender equality for all genders which draws us to the intersectional, transinclusive feminist frameworks outlined in this paper. That said, there are manyexperiences of intersectional oppression that we do not have weare white settlers, cisgender, (mostly) heterosexual, (mostly) nondisabled. For these reasons, we have tried to listen to, learn from and cite authors who write from these intersections. We strongly believe in the logic of co-liberation. As the Combahee River Collective states, "None of us are free until all of us are free."

## A.3 Adverse Impact Statement

With this work, we seek to spark conversations across disciplines about the social and political inequalities being exacerbated by AI. Rather than "calling out" specific people or institutions, we hope this critique serves as a "call in" other scholars to join together and work collectively towards building the AI systems that we all deserve.

submitted 22 January 2024; accepted 30 March 2024