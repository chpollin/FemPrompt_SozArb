---
title: "Künstliche Intelligenz in der Sozialen Arbeit: Grundlagen für Theorie und Praxis"
authors: ["Gesa Linnemann", "Julian Löhe", "Beate Rottkemper"]
year: 2025
type: book
language: de
categories:
  - AI_Literacies
  - Generative_KI
  - KI_Sonstige
  - Soziale_Arbeit
  - Bias_Ungleichheit
  - Gender
  - Diversitaet
  - Fairness
processed: 2026-02-05
source_file: Linnemann_2025_Künstliche_Intelligenz_in_der_Sozialen_Arbeit.md
---

# Künstliche Intelligenz in der Sozialen Arbeit: Grundlagen für Theorie und Praxis

## Kernbefund

KI birgt sowohl erhebliche Chancen (Inklusion, Assistenz, Dokumentation) als auch Risiken (Bias, Automatisierungsfehler, Marginalisierung) für die Soziale Arbeit; ein reflexierter, an Fachlichkeit und Ethik orientierter Umgang erfordert systematische KI-Kompetenzentwicklung in Studium, Praxis und Organisationen sowie kritische Inklusionsperspektiven.

## Forschungsfrage

Wie können Fachkräfte der Sozialen Arbeit KI-Systeme verantwortungsvoll verstehen, einsetzen und kritisch reflektieren, um vulnerable Gruppen zu schützen und professionelle Handlungsfähigkeit zu bewahren?

## Methodik

Theoretisch/Review; Sammelband mit 16 Beiträgen zu Grundlagen, Anwendungsfeldern, ethischen und rechtlichen Fragestellungen; kombiniert konzeptionelle Analysen mit Fallbeispielen und Forschungsüberblicken
**Datenbasis:** Nicht empirisch; theoretische und konzeptionelle Analyse, Literaturüberblick, Beispiele aus Praxis und Forschung

## Hauptargumente

- KI-Systeme verstärken bestehende gesellschaftliche Ungleichheiten und Diskriminierungen durch verzerrte Trainingsdaten und Design; dies betrifft besonders marginalisierte Gruppen (Frauen, LGBTQIA+, Menschen mit Behinderungen, ältere Menschen), die in KI-Systemen systematisch unterrepräsentiert sind.
- Der Einsatz von KI in der Sozialen Arbeit muss ethisch und rechtlich reflektiert erfolgen, da Fachkräfte mit vulnerablen Gruppen arbeiten, die sich oft nicht selbst gegen KI-Systeme wehren können; Transparenz, Datenschutz und professionelle Kontrolle sind zentral.
- KI-Kompetenz ist für Sozialarbeiter:innen eine Schlüsselqualifikation der Zukunft; sie müssen nicht nur KI-Systeme kritisch evaluieren können, sondern auch aktiv an Gestaltung und Entwicklung digitaler Technologien partizipieren, um professionelle Werte zu schützen und vulnerable Menschen zu schützen.
- KI-Technologien haben differenzierte Potenziale für Inklusion: Sie können marginalisierte Menschen unterstützen (Assistenztechnologien, Sprachausgabe, Zugänglichkeit), aber auch als Kontrollwerkzeuge missbraucht werden; eine kritische Inklusionsperspektive muss hinterfragen, in welche sozialen Systeme tatsächlich inkludiert wird.
- Automation Bias und Deskilling sind reale Risiken: Fachkräfte könnten KI-Systeme unkritisch überbewerten und ihre professionelle Urteilskraft abschwächen, was gerade in der Sozialen Arbeit zu Schädigungen vulnerabler Personen führt.

## Kategorie-Evidenz

### AI_Literacies

Zentral für den Band: 'Ein zentrales Anliegen ist die Entwicklung von KI-Kompetenz in Studium, Praxis und Organisationen.' Kapitel 'Künstliche Intelligenz in der Lehre der Sozialen Arbeit' widmet sich explizit Curricula und Kompetenzvermittlung.

### Generative_KI

Project Debater und Sprachmodelle werden im Vorwort diskutiert; Kapitel behandeln Natural Language Processing und Anwendungen von Sprachmodellen in Beratung und Dokumentation.

### KI_Sonstige

Umfassende Behandlung: Predictive Analytics (Kindeswohlgefährdung, AFST), Assistenzsysteme, Robotik für Ältere, algorithmische Entscheidungssysteme im Sozialmanagement, Computer Vision für Sprachausgabe.

### Soziale_Arbeit

Kernfokus des gesamten Bandes: Direkte Anwendungen auf Beratung, Dokumentation, Inklusion, Jugendhilfe, Kindesschutz, Behindertenarbeit, Älterenhilfe, Sozialmanagement.

### Bias_Ungleichheit

Extensive Diskussion: 'Frauen, LGBTQIA+-Communitys, ältere Menschen und Menschen mit Behinderungen sind in oder durch KI geprägte(n) Technologien systematisch unterrepräsentiert.' Eubanks-Analyse zur Automatisierung von Ungleichheit zitiert; Risiken für benachteiligte Bevölkerungsgruppen thematisiert.

### Gender

Mehrfach erwähnt: 'Beispielsweise sind nur 14% der Editor:innen des Journals Artificial Intelligence Frauen' (Fosch-Villaronga/Poulsen 2022); Gender-Bias in KI-Training und Entwicklung; Frauen in Technologieentwicklung unterrepräsentiert.

### Diversitaet

Zentral: 'Fosch-Villaronga und Poulsen (2022) zeigen auf, dass Frauen, LGBTQIA+-Communitys, ältere Menschen und Menschen mit Behinderungen systematisch unterrepräsentiert sind.' Intersektionale Perspektiven auf Behinderung, Ethnizität, Alter, Geschlecht.

### Fairness

Explizit behandelt in Ethik-Kapitel und Inklusions-Diskussionen: Fairness-Fragen bei Kindeswohleinschätzung (AFST-System), Algorithmic Fairness vs. Practitioner Bias, Equitable Access zu Assistenztechnologien.

## Assessment-Relevanz

**Domain Fit:** Höchste Relevanz für die Schnittstelle AI/Soziale Arbeit/Kritische Perspektiven; das einzige deutschsprachige Handbuch dieser Art, das systematisch KI-Chancen und -Risiken für vulnerable Zielgruppen der Sozialen Arbeit behandelt und dabei Bias, Inklusion und professionelle Ethik zentral setzt.

**Unique Contribution:** Erste umfassende deutschsprachige Publikation, die Künstliche Intelligenz als professionelle Gestaltungsaufgabe der Sozialen Arbeit rahmt und dabei kritische Perspektiven auf Diskriminierung, Inklusion und Marginalisierung mit praktischen Anwendungsszenarien verbindet.

**Limitations:** Primär konzeptionell-theoretisch ohne systematische Evaluation von KI-Systemen in realen Sozialarbeit-Organisationen; wenig quantitative Daten zur tatsächlichen Verbreitung und Nutzung von KI in deutschsprachigen Sozialeinrichtungen; Fokus auf Chancen und Risiken, weniger auf konkrete technische Lösungsstrategien.

**Target Group:** Primär: Sozialarbeiter:innen, Sozialmanager:innen, Lehrende der Sozialen Arbeit, Studierende; Sekundär: KI-Entwickler:innen mit Interesse für Fairness und vulnerable Zielgruppen, Policymaker im Sozialbereich, Ethiker:innen und Kritiker:innen von Algorithmic Decision Systems, Organisationen der Sozialwirtschaft.

## Schlüsselreferenzen

- [[Buolamwini_Gebru_2018]] - Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification
- [[Eubanks_2018]] - Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor
- [[Noble_2018]] - Algorithms of Oppression: How Search Engines Reinforce Racism
- [[FoschVillaronga_Poulsen_2022]] - KI und Inklusion: Unterrepräsentation marginalisierter Gruppen
- [[Shams_Zowghi_Bano_2023]] - Metastudie zu Inklusion und Diversität in KI-Technologien
- [[Orwat_2020]] - Diskriminierungsrisiken durch die Verwendung von Algorithmen
- [[Gillingham_Schiffhauer_Seelmeyer_2020]] - Internationale Forschung zum Einsatz digitaler Technik in der Sozialen Arbeit
- [[Floridi_et_al_2018]] - AI4People – An Ethical Framework for a Good AI Society
- [[GoldhaberFiebert_Prince_2019]] - Evaluation des Allegheny Family Screening Tool
- [[Ibison_et_al_2024]] - KI in NGOs und Graswurzelbewegungen: Potenziale und Barrieren
