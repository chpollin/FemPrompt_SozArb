---
title: "Avoiding Catastrophe Through Intersectionality in Global AI Governance"
authors: ["Laine McCrory"]
year: 2025
type: workingPaper
language: en
categories:
  - AI_Literacies
  - KI_Sonstige
  - Soziale_Arbeit
  - Bias_Ungleichheit
  - Diversitaet
  - Feministisch
  - Fairness
processed: 2026-02-05
source_file: Laine_2025_Avoiding_Catastrophe_Through_Intersectionality_in.md
---

# Avoiding Catastrophe Through Intersectionality in Global AI Governance

## Kernbefund

Keine der sieben analysierten globalen AI-Sicherheitsinitiativen erfüllt die Ziele eines intersektionalen feministischen Ansatzes; sie vernachlässigen gegenwärtige Schäden marginalisierter Gruppen und mangelt es an aussagekräftiger Partizipation betroffener Gemeinschaften.

## Forschungsfrage

Wie können intersektionale und feministische Perspektiven in die globale AI-Sicherheitsgovernance integriert werden, um gegenwärtige Schäden marginalisierter Gruppen mit zukünftigen existenziellen Risiken zu verbinden?

## Methodik

Theoretisch/Analyse: Feminist Policy Analysis Framework angewendet auf 7 internationale AI-Sicherheitsinitiativen mittels eines entwickelten 4-Ziel-Bewertungssystems (Intersectionality, Context, Neutrality, Power)
**Datenbasis:** Dokumentenanalyse von 7 internationalen AI-Governance-Initiativen (UN-Resolution, OECD-Prinzipien, Asilomar Principles, Bletchley Declaration, Seoul Declaration, Frontier AI Safety Commitments, weitere Industrie-Commitments)

## Hauptargumente

- Die AI-Safety-Bewegung fokussiert auf zukünftige existenzielle Risiken durch AGI, während sie gegenwärtige, bereits auftretende existenzielle Schäden für marginalisierte Gruppen (durch diskriminierende Algorithmen, Datenextraktion, Umweltschäden) ignoriert und damit Verantwortung für aktuelle Harms abwälzt.
- Ein intersektionaler feministischer Policy-Rahmen muss vier zentrale Ziele verfolgen: Intersectionality promoten, diverse Kontexte berücksichtigen, falsche Neutralitätsannahmen bekämpfen und Macht für marginalisierte Gruppen erhöhen – ein Ansatz, den bestehende Governance-Initiativen systematisch verfehlen.
- Accountability und Partizipation sind zentral: Zukünftige AI-Sicherheitspolitik muss von betroffenen marginalierten Communities mitgestaltet werden, kompound identities systematisch adressieren und strukturelle Diskriminierung als Fundament AGI-geleiteter Systeme bekämpfen.

## Kategorie-Evidenz

### AI_Literacies

Die OECD definiert 'AI knowledge' als 'skills and resources required to understand and participate in the AI system lifecycle' – McCrory kritisiert, dass gelebte Erfahrung marginalisierter Gruppen ausgeschlossen wird und alternative Epistemologien in Expertise-Definitionen fehlen.

### KI_Sonstige

Analysen von AI-Safety-Instituten, AGI-Risiken, algorithmischen Entscheidungssystemen und deren Implementierung in globalen Governance-Rahmen; Fokus auf ML-Modelle und deren strukturelle Verzerrungen.

### Soziale_Arbeit

Expliziter Bezug zu Sozialpolitik-Analyse (McPhail, Kanenberg), Vulnerable Populationen, Social Welfare Systeme (Eubanks-Referenz zu Automating Inequality in Sozialhilfe-Kontexten), Care und Community Governance.

### Bias_Ungleichheit

Kern-These: 'AI reinforces inequalities related to race, sexuality, class and gender'; diskriminierende Algorithmen als existenziell für low-income families; 'technological redlining' durch systematische Benachteiligung; Power Dynamics und strukturelle Diskriminierung.

### Diversitaet

Intersectionality als Framework-Kern; marginalisierte Communities, race, ethnicity, gender identity, class, carceral status, ability als überlappende Identitäten; Forderung nach 'diverse perspectives within technical development' und Repräsentation in Policy-Narrativen.

### Feministisch

Explizit feministische Theorie: Crenshaw's Intersectionality (1991), D'Ignazio & Klein's Data Feminism (2020), McPhail's Feminist Policy Analysis Framework (2003), Kanenberg et al.'s intersektionale Policy-Analyse; feministische Prinzipien wie Accountability, Partizipation, Power-Analyse zentral.

## Assessment-Relevanz

**Domain Fit:** Extrem relevant für die Schnittstelle AI/Soziale Arbeit/Gender Studies: Das Paper verbindet AI-Governance-Kritik mit sozialpolitischer Theorie, marginalisierte Populationen und strukturelle Gerechtigkeit – zentral für kritische Soziale Arbeit im digitalen Zeitalter.

**Unique Contribution:** Entwicklung eines operationalisierbaren feministischen AI-Policy-Analyse-Frameworks mit empirischer Anwendung auf 7 globale Governance-Initiativen, die systematisch zeigt, dass AI-Safety-Diskurse gegenwärtige Harms ignorieren und Partizipation marginalisierter Gruppen ausschließen.

**Limitations:** Dokumentenanalyse ohne primäre Daten von betroffenen Communities; Framework-Validität nicht durch externe Reviewer getestet; begrenzte Sample (7 Initiativen); keine Empfehlungen zu Implementierungsmetriken oder Monitoring-Mechanismen für Fem-AI-Standards.

**Target Group:** Policy-Maker in AI-Governance, Feminist AI-Forschende, Sozialarbeiter:innen in digitalen Kontexten, Critical Data Studies Scholar:innen, marginalisierte Community-Organisationen, Tech-Worker mit ethischem Fokus, Aktivist:innen für Tech-Gerechtigkeit

## Schlüsselreferenzen

- [[Buolamwini_Gebru_2018]] - Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification
- [[Crenshaw_1991]] - Mapping the Margins: Intersectionality, Identity Politics, and Violence Against Women of Color
- [[DIgnazio_Klein_2020]] - Data Feminism
- [[Eubanks_2018]] - Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor
- [[Crawford_2021]] - The Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence
- [[Benjamin_2019]] - Race after Technology: Abolitionist Tools for the New Jim Code
- [[Gebru_Torres_2024]] - The TESCREAL bundle: Eugenics and the promise of utopia through artificial general intelligence
- [[Kanenberg_Leal_Erich_2020]] - Revising McPhail's Feminist Policy Analysis Framework: Updates for Use in Contemporary Social Policy Research
- [[Noble_2018]] - Algorithms of Oppression: How Search Engines Reinforce Racism
- [[Bostrom_2014]] - Superintelligence: Paths, Dangers, Strategies
