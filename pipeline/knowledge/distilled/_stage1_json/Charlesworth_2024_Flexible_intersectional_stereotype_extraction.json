{
  "metadata": {
    "title": "Extracting Intersectional Stereotypes from Embeddings: Developing and Validating the Flexible Intersectional Stereotype Extraction Procedure",
    "authors": [
      "Tessa Charlesworth",
      "Kshitish Ghate",
      "Aylin Caliskan",
      "Mahzarin R. Banaji"
    ],
    "year": 2024,
    "type": "journalArticle",
    "language": "en"
  },
  "core": {
    "research_question": "Wie sind verschiedene intersektionale Identitätsgruppen (basierend auf Rasse, Geschlecht und Klasse) in englischsprachigen Internettext-Daten repräsentiert und welche Konsequenzen hat dies für KI-Systeme?",
    "methodology": "Empirisch - Natural Language Processing; Entwicklung und Anwendung des FISE-Verfahrens (Flexible Intersectional Stereotype Extraction) auf 840 Milliarden Wörter englischsprachiger Internettext zur Analyse von Trait-Assoziationen",
    "key_finding": "59% der Traits in der englischen Sprache sind mit weißen Männern assoziiert, während nur 5% mit schwarzen Frauen assoziiert sind. Diese Imbalancen in der Sprachrepräsentation führen zu systematischen Verzerrungen in KI-Systemen, wobei Klassenzugehörigkeit ein übergeordneter Faktor ist.",
    "data_basis": "840 Milliarden Wörter aus englischsprachigen Internetquellen"
  },
  "arguments": [
    "Weiße Männer werden in englischsprachigen Internetdaten überrepräsentiert, während schwarze Frauen unsichtbar sind. Dies führt zu einer Dominanz-versus-Invisibilität-Dichotomie, die direkt in KI-Systeme übertragen wird.",
    "KI-Systeme reproduzieren und verstärken menschliche Verzerrungen, da sie auf von Menschen erstellten Daten trainiert werden. Wenn die Trainingsdaten verzerrt sind, werden auch die KI-Outputs verzerrt sein.",
    "Soziale Klasse ist ein übergeordneter Faktor bei der Repräsentation von Intersektionalität. Wohlhabende Identitäten werden unabhängig von Rasse und Geschlecht mit 78% positiven Traits assoziiert, während arme und schwarze Identitäten nur 21% positive Traits aufweisen."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": true,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": true,
    "Diversitaet": true,
    "Feministisch": true,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Die Studie zielt darauf ab, Industrien zu zeigen, wie Sprache Stereotypen verkörpert, verbreitet und intensiviert: 'Our research can show industries that such findings and methods illustrate the societal significance of how language embodies, propagates and even intensifies stereotypes'",
    "Generative_KI": "Explizite Bezüge zu generativen KI-Systemen: 'you can see these kinds of imbalances coming out in the outputs' bei Google, ChatGPT, DALL-E",
    "KI_Sonstige": "Verwendung von Natural Language Processing (NLP) und Word Embeddings: 'an approach built on earlier versions of natural language processing (word embeddings) compared to the more advanced large language models'",
    "Bias_Ungleichheit": "Zentral: '59% of traits in the English language are associated with white men, Black women are associated with only about 5% of the traits' und 'Imbalances in English language create imbalances in AI systems'",
    "Gender": "Expliziter Gender-Fokus: 'the way gender, race and social class intersect and relate to systems of oppression, domination or discrimination' und geschlechtsspezifische Befunde wie Überrepräsentation weiblicher positiver Traits bei Wohlstand",
    "Diversitaet": "Intersektionale Perspektive auf Repräsentation: 'underrepresentation of Black women specifically in the training data' und 'When women of color are made invisible in the data, it is likely that the applications of large language models and AI will be particularly inaccurate'",
    "Feministisch": "Die Studie bezieht sich auf Intersektionalitätstheorie und deren führende Theoretiker:innen (impliziert Crenshaw). Explizite Verbindung von intersektionaler Theorie mit KI-Kritik zur Analyse von Machtverhältnissen und Unterdrückung",
    "Fairness": "Algorithmic Fairness als zentrales Anliegen: 'The research tells us that even in these intersectional spaces where we are having race, gender and social class interacting, social class seems to be an overwhelming factor on how a person is represented' mit Fokus auf faire Repräsentation"
  },
  "references": [
    {
      "author": "Charlesworth, T., Ghate, K., Caliskan, A., & Banaji, M. R.",
      "year": 2024,
      "short_title": "Extracting Intersectional Stereotypes from Embeddings (FISE)"
    },
    {
      "author": "Buolamwini & Gebru",
      "year": 2018,
      "short_title": "Gender Shades - Intersectional Bias in Face Recognition"
    },
    {
      "author": "nicht angegeben",
      "year": 2024,
      "short_title": "Implizite Referenz auf frühere Studien zu Bias in Image Models und Facial Recognition"
    }
  ],
  "assessment": {
    "domain_fit": "Hochgradig relevant für die Schnittstelle KI und Gender/Diversität. Das Paper demonstriert empirisch, wie sprachliche Verzerrungen in KI-Trainingsdaten intersektionale Ungleichheiten perpetuieren. Die Verbindung zu intersektionaler Theorie und KI-Systemen ist zentral für kritische KI-Literacies und Fairness-Diskurse.",
    "unique_contribution": "Entwicklung und empirische Validierung des FISE-Verfahrens zur systematischen Messung intersektionaler Stereotypen in großen Textkorpora, was die dominante Rolle von Klassenposition in der Stereotypisierung erstmals quantitativ demonstriert.",
    "limitations": "Das Paper analysiert nur englischsprachige Internetquellen und Word Embeddings; Limitierung auf English-Bias-Muster; keine Analyse von Kontextfaktoren oder diachroner Veränderung"
  },
  "target_group": "KI-Entwickler:innen, Policymaker, Gender Studies und Diversity-Spezialist:innen, KI-Ethiker:innen, Forscher:innen in Computational Social Science, Organisationen mit Fairness-Anforderungen bei KI-Systemen"
}