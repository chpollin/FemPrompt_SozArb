{
  "metadata": {
    "title": "EDPS TechDispatch on Explainable Artificial Intelligence",
    "authors": [
      "Vítor Bernardo"
    ],
    "year": 2023,
    "type": "report",
    "language": "en"
  },
  "core": {
    "research_question": "Wie können KI-Systeme transparent und erklärbar gestaltet werden, um Datenschutz, Fairness und Vertrauen zu gewährleisten?",
    "methodology": "Theoretisch/Review - Analyse und Darstellung von Konzepten, Risiken und Best Practices der Explainable AI (XAI) im Kontext des Datenschutzes",
    "key_finding": "XAI ist ein essentielles Mittel zur Förderung von Transparenz, Rechenschaftspflicht und Fairness bei KI-Systemen, kann aber selbst neue Risiken schaffen und muss durch menschenzentrierte Design-Ansätze und kritische Reflexion flankiert werden.",
    "data_basis": "nicht empirisch - konzeptionelle und normative Analyse basierend auf Literatur, Rechtsprechung und Best Practice"
  },
  "arguments": [
    "Der 'Black-Box-Effekt' bei komplexen KI-Systemen führt zu Risiken für Individuen durch verborgene Diskriminierung, Bias und mangelnde Rechenschaftspflicht, besonders bei automatisierten Entscheidungen durch öffentliche Behörden.",
    "XAI kann durch Transparency, Interpretability und Explainability-Mechanismen Datenschutzprinzipien unterstützen und Compliance mit GDPR gewährleisten, muss aber sorgfältig umgesetzt werden.",
    "Die Implementierung von XAI birgt eigene Risiken: Misinterpretation, Sicherheitslücken, Disclosure von Geschäftsgeheimnissen und Überreliance auf Systeme - daher ist ein menschenzentrierter, kontextsensitiver Ansatz erforderlich."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": false,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Fokus auf Verständnis und kritische Reflexion von KI-Systemen durch verschiedene Stakeholder: 'XAI empowers individuals with understandable insights into how their personal data is being handled'",
    "KI_Sonstige": "Breite Behandlung von ML, Deep Learning, neuronalen Netzwerken und algorithmischen Entscheidungssystemen: 'AI systems such as machine learning (ML) or deep learning (DL) use algorithms learned by their own process of training'",
    "Bias_Ungleichheit": "Explizite Analyse von Diskriminierungsrisiken und Bias in KI: 'when AI is used to select job applicants, systems might inadvertently favour candidates from certain demographics or backgrounds due to biased training data'",
    "Fairness": "Zentrale Behandlung von Fairness-Anforderungen und Fairness-Implementierung: 'the limitations of black box approaches should be considered when trying to assess the fairness of the models'"
  },
  "references": [
    {
      "author": "Miller, T. H.",
      "year": 2017,
      "short_title": "Explainable AI: Beware of inmates running the asylum"
    },
    {
      "author": "Ribeiro, M. T.",
      "year": 2016,
      "short_title": "Why should I trust you? Explaining predictions of any classifier (LIME)"
    },
    {
      "author": "Gunning, D. S.",
      "year": 2019,
      "short_title": "XAI-Explainable Artificial Intelligence"
    },
    {
      "author": "Burrell, J.",
      "year": 2016,
      "short_title": "How the machine 'thinks': Understanding opacity in machine learning algorithms"
    },
    {
      "author": "Lepri, B. O.",
      "year": 2018,
      "short_title": "Fair, transparent, and accountable algorithmic decision-making processes"
    },
    {
      "author": "Lipton, Z. C.",
      "year": 2018,
      "short_title": "The mythos of model interpretability"
    },
    {
      "author": "Mittelstadt, B. R.",
      "year": 2019,
      "short_title": "Explaining explanations in AI"
    },
    {
      "author": "Kuppa, A.",
      "year": 2021,
      "short_title": "Adversarial XAI methods in cybersecurity"
    },
    {
      "author": "Peters, U.",
      "year": 2023,
      "short_title": "Explainable AI lacks regulative reasons: why AI and human decision-making are not equally opaque"
    }
  ],
  "assessment": {
    "domain_fit": "Das Paper ist primär für Datenschutz-, KI-Governance und Tech-Policy relevant, nicht direkt für Soziale Arbeit. Es bietet jedoch wichtige Erkenntnisse für Sozialarbeiter:innen, die mit algorithmischen Systemen in Bedarfserkennung, Ressourcenallokation oder Fallmanagement arbeiten.",
    "unique_contribution": "Die systematische Integration von technischen XAI-Ansätzen mit Datenschutzrecht (GDPR), Fairness-Anforderungen und menschenzentrierten Designprinzipien unter Berücksichtigung von Risiken der XAI-Implementierung selbst.",
    "limitations": "Begrenzte Analyse spezifischer Sektoren (erwähnt Gesundheit, Finanzen, keine detaillierte Behandlung von Soziale-Arbeit-Kontexten); keine empirischen Fallstudien oder Evaluationen von XAI-Implementierungen"
  },
  "target_group": "Datenschutzbeauftragte, KI-Entwickler:innen, Policy-Maker, Regulatorische Behörden, Organisationen die KI-Systeme einsetzen, sekundär: Sozialarbeiter:innen die mit automatisierten Entscheidungssystemen arbeiten"
}