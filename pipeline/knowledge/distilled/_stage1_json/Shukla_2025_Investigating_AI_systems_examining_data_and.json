{
  "metadata": {
    "title": "Investigating AI systems: examining data and algorithmic bias through hermeneutic reverse engineering",
    "authors": [
      "Nishanshi Shukla"
    ],
    "year": 2025,
    "type": "journalArticle",
    "language": "en"
  },
  "core": {
    "research_question": "Wie können KI-Systeme durch hermeneutisches Reverse Engineering untersucht werden, um Bias in Daten und Algorithmen aufzudecken und alternative, gerechtere Zukünfte zu gestalten?",
    "methodology": "Theoretisch/Review - Literaturbasierte kritische Analyse mit Einsatz von hermeneutischem Reverse Engineering als Rahmenwerk; kritische Daten-, Algorithmen- und Code-Studien sowie feministische Wissenschafts- und Technologiestudien",
    "key_finding": "AI-Systeme als Boundary Objects zu betrachten und durch hermeneutisches Reverse Engineering kritisch zu untersuchen ermöglicht es, versteckte Bias in Daten und Algorithmen aufzudecken und alternative, inklusivere Futures zu imaginieren. Bias ist kein Fehler sondern systemische Kodierung der dominanten sozialen Struktur.",
    "data_basis": "nicht angegeben"
  },
  "arguments": [
    "AI-Systeme haben Politiken und üben Macht aus; Bias in KI ist nicht zufällig sondern systemic coding of dominant social fabric, der rassistische, geschlechtsspezifische und ableistische Geschichte reproduziert und verstärkt.",
    "Daten sind politische Werkzeuge, die immer subjektiv und partiell sind; die Trennung von Daten aus ihrem historischen und kontextuellen Ursprung maskiert subjektive Bedeutungen und ermöglicht diskriminatorische algorithmische Entscheidungen.",
    "Algorithmen und Code sind ebenfalls politische Werkzeuge; ihre Untersuchung durch hermeneutisches Reverse Engineering mit intersektional-feministischer Perspektive offenbart Power Play und ermöglicht die Imagination alternativer, nicht-diskriminatorischer Realitäten."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": true,
    "Bias_Ungleichheit": true,
    "Gender": true,
    "Diversitaet": true,
    "Feministisch": true,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Paper argumentiert für kritische Untersuchung und Verständnis von KI-Systemen: 'examining each part related to the building and working of AI systems is essential for unpacking the political play and potential insert points of biases'",
    "KI_Sonstige": "Fokus auf AI systems generell, deren Daten, Algorithmen, Code und deren Auswirkungen: 'AI systems, like all other technologies, have politics and exercise power'",
    "Soziale_Arbeit": "Paper diskutiert Impact auf verschiedene soziale Gruppen und marginalisierte Communities: 'the groups most impacted by the results of injustices enacted by AI are often devoid of resources to design and deploy these systems'",
    "Bias_Ungleichheit": "Zentral: 'Bias based on race, gender, ability, language, class, economic background, and religion, among many other indicators, perpetuating in the AI systems are not just a glitch or an error, but systemic coding of the dominant social fabric'",
    "Gender": "Explizites Genderbeispiel: 'Keyes (2018) showcases how a conventional binary understanding of gender within tools of Automatic Gender Recognition (AGR) operationalizes non-inclusive and harmful technologies for transgender people'",
    "Diversitaet": "Intersektionale Perspektive durchgehend: 'such as women, people of color, disabled people, and queer people'; 'interlocking systems of structural oppression'",
    "Feministisch": "Explizit intersektional-feministische Theorie: 'based in intersectional feminist thought of acknowledging and understanding interlocking systems of structural oppression as stated by the Combahee River Collective Statement'; verwendet Combahee River Collective (1978) als theoretisches Fundament",
    "Fairness": "Fokus auf algorithmische Fairness und Gerechtigkeit: 'actionable AI audits that lead to the reduction of biased results'; 'strategies based on alignments between AI practitioners and fairness literature'; 'tensions of fairness at the intersection of individual and group needs'"
  },
  "references": [
    {
      "author": "Buolamwini, J.",
      "year": 2023,
      "short_title": "Unmasking AI: My mission to protect what is human in a world of machines"
    },
    {
      "author": "Benjamin, R.",
      "year": 2019,
      "short_title": "Race after technology: abolitionist tools for the new Jim code"
    },
    {
      "author": "Buolamwini, J. & Gebru, T.",
      "year": 2018,
      "short_title": "Gender Shades: Intersectional accuracy disparities in commercial gender classification"
    },
    {
      "author": "Crawford, K.",
      "year": 2021,
      "short_title": "Atlas of AI"
    },
    {
      "author": "Eubanks, V.",
      "year": 2019,
      "short_title": "Automating inequality: how high-tech tools profile, police, and punish the poor"
    },
    {
      "author": "Noble, S. U.",
      "year": 2018,
      "short_title": "Algorithms of oppression: how search engines reinforce racism"
    },
    {
      "author": "Gebru, T.",
      "year": 2020,
      "short_title": "Race and gender: data-driven claims about race and gender"
    },
    {
      "author": "Balsamo, A.",
      "year": 2011,
      "short_title": "Designing culture: the technological imagination at work"
    },
    {
      "author": "Broussard, M.",
      "year": 2023,
      "short_title": "More than a glitch: Confronting race, gender, and ability bias in tech"
    },
    {
      "author": "Combahee River Collective",
      "year": 1978,
      "short_title": "A Black Feminist Statement"
    }
  ],
  "assessment": {
    "domain_fit": "Sehr relevant für Schnittstelle AI/Soziale Arbeit/Gender - das Paper verbindet KI-Kritik explizit mit sozialer Gerechtigkeit, marginalisierten Gruppen und intersektional-feministischer Theorie, was direkt für Soziale Arbeit als gesellschaftlich orientierte Disziplin relevant ist.",
    "unique_contribution": "Das Paper bietet einen systematischen methodologischen Rahmen (hermeneutisches Reverse Engineering) zur kritischen Analyse von AI-Bias unter explizit intersektional-feministischer Perspektive und konkretisiert dies durch Analyse von Daten, Algorithmen und Code.",
    "limitations": "Das Paper ist primär theoretisch-konzeptuell; es fehlen empirische Fallstudien oder konkrete Anwendungen der hermeneutischen Reverse Engineering-Methodik auf spezifische AI-Systeme."
  },
  "target_group": "AI-Entwickler und -Praktiker, Sozialarbeiter und Sozialwissenschaftler, Policymaker und Regulatoren, Genderforschung, Critical Algorithm Studies-Forschende, marginalisierte Communities, Tech-Ethics-Professionelle"
}