{
  "metadata": {
    "title": "Unequal Voices: How LLMs Construct Constrained Queer Narratives",
    "authors": [
      "Atreya Ghosal",
      "Ashim Gupta",
      "Vivek Srikumar"
    ],
    "year": 2025,
    "type": "conferencePaper",
    "language": "en"
  },
  "core": {
    "research_question": "Wie konstruieren Large Language Models eingeengte und systematisch verzerrte Narrative über queere Menschen im Vergleich zu nicht-queeren Personen?",
    "methodology": "Empirisch mit vier hypothesengesteuerten Experimenten: (1) Frequenzanalyse spezifischer Begriffe (Diversität, Inklusion), (2) Thematische Analyse von LLM-Outputs mittels LLM-as-Judge, (3) Systematischer Vergleich zwischen Queer (Q) und Not-Queer (¬Q) Personas, (4) Topic Divergence Messung. Getestete Modelle: Llama 3.1/3.3, Gemma, Qwen über fünf soziale Kontexte (Housing, Medical, Persona, Recommendation, Work).",
    "key_finding": "LLMs generieren für queere Personas signifikant häufiger Begriffe der Diversität und Inklusion, fokussieren überproportional auf identitätsbezogene Themen und zeigen eine systematische Topic-Divergenz gegenüber nicht-queeren Personas, was als subtile Form der diskursiven Othering und Repräsentationseinengung wirkt.",
    "data_basis": "Automatisierte Prompt-basierte Generierung aus definierten Kontexten und Identitätsausdrücken mit Multiple-Choice-Evaluation durch LLM-Judge; genaue Samplegröße nicht explizit angegeben, aber systematische Tests über mindestens 6 verschiedene Modelle und 5 Kontexte mit jeweils Identity=User und Identity=Model Varianten."
  },
  "arguments": [
    "Marginalisierte Gruppen werden in Diskursen oft auf stereotype, begrenzte Themen reduziert, während Dominanzgruppen die volle Komplexität menschlicher Existenz zugestanden wird. LLM-Outputs verstärken dieses Muster durch systematische Assoziationen, die auch bei oberflächlich nicht-negativen Outputs diskriminierend wirken.",
    "Repräsentationsharms müssen nicht explizit negativ oder beleidigend sein: Schmale, identitätsfokussierte Darstellungen queerer Menschen (analog zum 'Trans Broken Arm Syndrome' in der klinischen Praxis) können durch Umlenking von primären Anliegen zu Allokationsharms führen, insbesondere in Anwendungen wie Chatbot-Therapie.",
    "Discursive Othering durch Überkorrektur: LLMs foregrunden Diversität-, Inklusions- und Community-Konzepte bei queeren Kontexten unabhängig von faktischer Relevanz, was queere Personen auch in identitätsneutralen Alltags-Settings als 'anders' markiert und subtile soziale Marginalisierung perpetuiert."
  ],
  "categories": {
    "AI_Literacies": false,
    "Generative_KI": true,
    "Prompting": true,
    "KI_Sonstige": true,
    "Soziale_Arbeit": true,
    "Bias_Ungleichheit": true,
    "Gender": true,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "Generative_KI": "Analyse von LLM-Outputs aus mehreren großen Sprachmodellen (Llama 3.1/3.3, Gemma, Qwen) in unterschiedlichen Persona-Szenarien.",
    "Prompting": "Systematische Prompt-Templates über fünf soziale Kontexte (Housing, Medical, Persona, Recommendation, Work) mit kontrollierten Identitätsausdrücken als Variablen.",
    "KI_Sonstige": "NLP-Analyse mit Porter Stemmer und NLTK-Library zur Begriffsfrequenz-Messung sowie LLM-as-Judge Evaluationsmethodik.",
    "Soziale_Arbeit": "Explizites Fallbeispiel eines Therapie-Chatbots mit queeren Patienten als Demonstrationsszenario für potenzielle Schädigungen; Fokus auf Mental-Health-Chatbots und klinische Interaktionspraxis.",
    "Bias_Ungleichheit": "Zentrale These: Systematische Bias in LLM-Outputs reproduziert und perpetuiert soziale Marginalisierung queerer Menschen durch Repräsentationsharms und Allokationsharms.",
    "Gender": "Expliziter Fokus auf LGBTQ+ Identitäten, speziell queere, schwule, lesbische und trans Personas als Subjekte der Repräsentationsanalyse.",
    "Diversitaet": "Analyse der Repräsentation marginalisierter Communities (queere Menschen) versus Dominanzgruppen; Intersektionalität implizit durch Fokus auf Othering-Prozesse.",
    "Fairness": "Vier Hypothesen zur Messung systematischer Fairness-Unterschiede (H1-H4); Bezug zu Fairness-Harms und Konzepten like Equalized Representation; Test auf diskriminatorische Assoziationsmuster."
  },
  "references": [
    {
      "author": "Blodgett et al.",
      "year": 2020,
      "short_title": "Language (technology) is power: A critical survey of 'bias' in NLP"
    },
    {
      "author": "Barocas, Hardt & Narayanan",
      "year": 2023,
      "short_title": "Fairness and Machine Learning: Limitations and Opportunities"
    },
    {
      "author": "Gadiraju et al.",
      "year": 2023,
      "short_title": "'I wouldn't say offensive but...': Disability-centered perspectives on large language models"
    },
    {
      "author": "Noble",
      "year": 2018,
      "short_title": "Algorithms of Oppression"
    },
    {
      "author": "Wall et al.",
      "year": 2023,
      "short_title": "Trans Broken Arm Syndrome in clinical practice"
    },
    {
      "author": "Spivak",
      "year": 1985,
      "short_title": "Othering and epistemic violence"
    },
    {
      "author": "Young",
      "year": 2014,
      "short_title": "Inspiration porn"
    },
    {
      "author": "Meretoja",
      "year": 2017,
      "short_title": "The Ethics of Storytelling: Narrative Hermeneutics, History, and the Possible"
    }
  ],
  "assessment": {
    "domain_fit": "Hochgradig relevant für die Schnittstelle KI/Soziale Arbeit: Das Paper untersucht systematisch, wie LLMs marginalisierte Menschen (queere Personen) in praktischen Anwendungsszenarien (klinische Beratung, Therapie-Chatbots) darstellen und damit Schädigungen verursachen können – zentral für die sichere Implementierung von KI in sozialarbeiterischen Kontexten.",
    "unique_contribution": "Operationalisierung subtiler, nicht-explizit negativer Repräsentationsharms durch drei konzeptuelle Kategorien (harmful, narrow, othering representations) und empirische Messung via vier testbare Hypothesen mit detaillierter Topic-Divergence-Analyse.",
    "limitations": "Eingeschränkter Satz von Szenarien und Identitätstermen (nur englischsprachig, explizite Identitätsmarkierung durch Prompt erforderlich); latente Identitätsdarstellungen nicht untersucht; Generalisierbarkeit über Llama/Gemma/Qwen begrenzt; keine tiefen qualitativen Analysen einzelner problematischer Outputs."
  },
  "target_group": "KI-Entwickler:innen und Sicherheitsauditor:innen (insbesondere für generative KI-Systeme in sensiblen Anwendungen), Sozialarbeit und klinische Praktiker:innen (Therapeut:innen, Berater:innen mit Chatbot-Einsatz), LGBTQ+-Advocacy und Diversitätsbeauftragte, AI Ethics und Fairness-Forscher:innen, Policymaker im Bereich KI-Regulation."
}