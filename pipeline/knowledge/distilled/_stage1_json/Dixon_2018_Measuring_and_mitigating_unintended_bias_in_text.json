{
  "metadata": {
    "title": "Measuring and Mitigating Unintended Bias in Text Classification",
    "authors": [
      "Lucas Dixon",
      "John Li",
      "Jeffrey Sorensen",
      "Nithum Thain",
      "Lucy Vasserman"
    ],
    "year": 2018,
    "type": "conferencePaper",
    "language": "en"
  },
  "core": {
    "research_question": "Wie können unbeabsichtigte Verzerrungen in Textklassifikationsmodellen gemessen und mitigiert werden, insbesondere wenn demographische Informationen nicht verfügbar sind?",
    "methodology": "Empirisch: Experimentelle Evaluierung mit Convolutional Neural Networks, Synthetic Test Set Design mit Template-basierten Phrasen, Daten-Balancing-Ansatz, Multiple Evaluationsmetriken (AUC, Error Rate Equality Difference, Pinned AUC)",
    "key_finding": "Durch strategisches Hinzufügen von nicht-toxischen Trainingsbeispielen mit bestimmten Identitätstermen lässt sich unbeabsichtigte Verzerrung in Textklassifikatoren reduzieren, ohne die Gesamtmodellleistung zu beeinträchtigen. Die neu eingeführte 'Pinned AUC'-Metrik ermöglicht schwellenwertunabhängige Erkennung von Verzerrungen.",
    "data_basis": "127.820 annotierte Wikipedia Talk Page Kommentare (Trainingsdaten), 31.866 gehaltene Testkommentare, synthetisches Test-Set mit 77.000 generierten Phrasen mit 51 Identitätstermen"
  },
  "arguments": [
    "Unbeabsichtigte Verzerrungen entstehen durch unausgewogene Darstellung von Identitätstermen in Trainingsdaten: Der Begriff 'gay' erscheint in 3% toxischer Kommentare, aber nur 0,5% aller Kommentare, was zu Überanpassung führt und das Modell veranlasst, Identitätsterme mit Toxizität zu assoziieren.",
    "Eine unsupervised Bias-Mitigation durch Hinzufügen nicht-toxischer Beispiele aus Wikipedia-Artikeln (statt manuell gekennzeichnete Kommentare) ist kosteneffizient und effektiv: Mit 4.620 zusätzlichen Trainingsamples wurde die False Positive Equality Difference von 74,13 auf 52,94 reduziert.",
    "Die Unterscheidung zwischen 'unbeabsichtigter Verzerrung' im Modell und 'Unfairness' in der Anwendung ist kritisch: Derselbe Bias kann je nach Einsatzszenario (automatisches Löschen vs. Review-Priorisierung vs. Batch-Veröffentlichung) unterschiedliche Auswirkungen haben."
  ],
  "categories": {
    "AI_Literacies": false,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "KI_Sonstige": "Fokus auf Text-Klassifikation mit Convolutional Neural Networks, Trainingsmethoden mit TensorFlow/Keras, Evaluierung von Modellleistung",
    "Bias_Ungleichheit": "Zentrale These: 'Initial versions of text classifiers trained on this data showed problematic trends for certain statements... Clearly non-toxic statements containing certain identity terms, such as 'I am a gay man', were given unreasonably high toxicity scores. We call this false positive bias.' Analyse von Diskriminierung durch Überrepräsentation bestimmter Identitätstermen in toxischen Trainingsdaten.",
    "Diversitaet": "Untersuchung von 51 verschiedenen Identitätstermen (atheist, queer, gay, transgender, lesbian, homosexual, feminist, black, white, muslim, etc.) und deren unterschiedliche Repräsentation in toxischen vs. nicht-toxischen Kommentaren. Fokus auf marginalisierte Communities und deren Darstellung in Trainingsdaten.",
    "Fairness": "Explizite Definition von Fairness-Metriken: 'Equality of Odds' und Error Rate Equality Difference. Entwicklung der 'Pinned AUC'-Metrik als schwellenwertunabhängiges Fairness-Evaluationsinstrument. Ziel: 'A more fair model will have similar values across all terms, approaching the equality of odds ideal.'"
  },
  "references": [
    {
      "author": "Hardt, M., Price, E., Srebro, N.",
      "year": 2016,
      "short_title": "Equality of Opportunity in Supervised Learning"
    },
    {
      "author": "Feldman, M., Friedler, S.A., Moeller, J., Scheidegger, C., Venkatasubramanian, S.",
      "year": 2015,
      "short_title": "Certifying and Removing Disparate Impact"
    },
    {
      "author": "Bolukbasi, T., Chang, K.W., Zou, J.Y., Saligrama, A., Kalai, A.",
      "year": 2016,
      "short_title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings"
    },
    {
      "author": "Beutel, A., Chen, J., Zhao, Z., Chi, E.H.",
      "year": 2017,
      "short_title": "Data Decisions and Theoretical Implications when Adversarially Learning Fair Representations"
    },
    {
      "author": "Blodgett, S.L., O'Connor, B.",
      "year": 2017,
      "short_title": "Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English"
    },
    {
      "author": "Hovy, D., Spruit, S.L.",
      "year": 2016,
      "short_title": "The Social Impact of Natural Language Processing"
    },
    {
      "author": "Kleinberg, J.M., Mullainathan, S., Raghavan, M.",
      "year": 2016,
      "short_title": "Inherent Trade-Offs in the Fair Determination of Risk Scores"
    }
  ],
  "assessment": {
    "domain_fit": "Hochgradig relevant für KI-Fairness und Bias-Mitigation, mit Anwendungspotenzial in Community Moderation und Content Filtering. Der praktische Fokus auf Szenarien ohne demographische Metadaten ist besonders wertvoll. Schwach relevant für Soziale Arbeit, könnte aber indirekt für digitale Unterstützungssysteme bedeutsam sein.",
    "unique_contribution": "Erstmalige systematische Definition und Messung von 'unbeabsichtigter Verzerrung' in Textklassifikation mit praktischer Unterscheidung von Modell-Bias und Anwendungs-Unfairness; Einführung der schwellenwertunabhängigen 'Pinned AUC'-Metrik als innovatives Evaluationsinstrument.",
    "limitations": "Ansatz ist auf manuell identifizierte Identitätsterme beschränkt (51 Terme); Bias-Mitigation fokussiert nur auf False-Positive-Bias durch Hinzufügen negativer Beispiele; Evaluation erfolgt hauptsächlich auf synthetischen Daten mit Template-generierten Phrasen; Generalisierbarkeit auf andere Sprachen und Kontexte nicht getestet."
  },
  "target_group": "KI-Entwickler und ML-Engineers, Fairness-Forscher, Content-Moderation-Teams, Plattformbetreiber (insbesondere Wikipedia und ähnliche Community-Plattformen), Policy-Maker im Bereich algorithmische Governance, Ethik-Committees in Tech-Unternehmen"
}