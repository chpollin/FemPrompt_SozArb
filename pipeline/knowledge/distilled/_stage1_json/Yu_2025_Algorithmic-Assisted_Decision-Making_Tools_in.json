{
  "metadata": {
    "title": "Algorithmic-Assisted Decision-Making Tools in Child Welfare Practice: A Systematic Review",
    "authors": [
      "Meng-Hsuan Yu",
      "Roderick A. Rose"
    ],
    "year": 2025,
    "type": "journalArticle",
    "language": "en"
  },
  "core": {
    "research_question": "What are the key factors associated with successfully implementing algorithmic decision-making tools in child welfare practice, what challenges arise, and how do agencies address fairness, equity, and ethics issues?",
    "methodology": "Systematische Literaturreview (PRISMA-Richtlinien) mit Einschluss von grauer Literatur; Analyse von 9 Studien zu algorithmischen Tools in der Kinderschutzhilfe mittels angepasstem Implementierungswissenschaft-Framework (CFIR)",
    "key_finding": "Algorithmisch unterstützte Entscheidungshilfen zeigen Potenzial für den Kinderschutz, erfordern aber systematische Aufmerksamkeit für ethische Implementierung und Fairness, insbesondere bezüglich rassischer Disparitäten und Datenqualität. Bedeutende methodische und empirische Lücken bleiben bestehen.",
    "data_basis": "9 Studien aus Datenbanken (PubMed, JSTOR, ProQuest, Google Scholar) und grauer Literatur; Zeitraum der Veröffentlichungen variiert, Fokus auf konkrete Implementierungsfälle in Kindesschutz-Agenturen"
  },
  "arguments": [
    "Algorithmische Entscheidungshilfen versprechen höhere Genauigkeit und Konsistenz als traditionelle Risikobewertungstools, können aber existierende Ungleichheiten verstärken, wie das Beispiel des Tools zeigt, das Black Children überproportional für Missbrauchsuntersuchungen flaggt.",
    "Erfolgreiche Implementierung erfordert nicht nur technische Validität und Zuverlässigkeit, sondern auch strukturelle Voraussetzungen (Dateninfrastruktur, IT-Systeme, Budget), organisationale Kommunikation, Training und Stakeholder-Engagement, insbesondere mit betroffenen Gemeinschaften.",
    "Ethische Überlegungen müssen von der Tool-Entwicklung bis zur Deployment-Phase berücksichtigt werden; Transparenz über Grenzen, Validierung über Subgruppen hinweg, und Mechanismen zur Feedbacksammlung und externen ethischen Überprüfung sind zentral."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": true,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Explicit focus on training to enhance understanding of algorithmic tools and mitigating automation bias (CFIR table on 'Access to knowledge and information'). The review emphasizes need for caseworkers and agency staff to develop competencies in using decision-support systems effectively.",
    "KI_Sonstige": "Central focus on predictive analytics, machine learning models, algorithmic-assisted decision-making, and computational algorithms in child welfare. Tools reviewed include LASSO logistic regression, random forests, neural networks, natural language processing, and risk prediction models.",
    "Soziale_Arbeit": "Systematic review of algorithmic tools implemented in child welfare practice across multiple decision-making stages: referral screening, investigation, substantiation, placement, and permanency decisions. Direct engagement with social workers' decision-making, professional judgment, and practical utility in frontline child protective services.",
    "Bias_Ungleichheit": "Extensive discussion of algorithmic bias and racial disparities: 'one tool, for instance, disproportionately flagged Black children for mandatory maltreatment investigations' (Ho & Burke, 2022). Review examines whether tools 'function consistently across subgroups' and analyzes accuracy across demographic groups, particularly by race and gender.",
    "Diversitaet": "Multiple references to marginalized communities and intersectional concerns: examination of whether algorithmic tools 'worsen racial disparities in the CW system,' analysis of overrepresentation of Black children, and equity-focused reports examining racial disproportionality in Los Angeles County model. Consideration of how tools impact diverse families and communities.",
    "Fairness": "Core focus on algorithmic fairness as one of three central themes. Framework explicitly addresses fairness across subgroups (Russell 2015), uses fairness correction techniques (Purdy & Glass 2023), analyzes model equity through AUC/ROC values, and evaluates whether algorithms function consistently across demographic groups. Drake et al. (2020) framework includes fairness assessment at each stage of development and deployment."
  },
  "references": [
    {
      "author": "Eubanks",
      "year": 2018,
      "short_title": "Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor"
    },
    {
      "author": "Chouldechova et al.",
      "year": 2018,
      "short_title": "A Case Study of Algorithm-Assisted Decision Making in Child Maltreatment Hotline Screening Decisions"
    },
    {
      "author": "Drake et al.",
      "year": 2020,
      "short_title": "A Practical Framework for Considering the Use of Predictive Risk Modeling in Child Welfare"
    },
    {
      "author": "Russell",
      "year": 2015,
      "short_title": "Predictive Analytics and Child Protection: Constraints and Opportunities"
    },
    {
      "author": "Saxena et al.",
      "year": 2020,
      "short_title": "A Human-Centered Review of Algorithms Used Within the U.S. Child Welfare System"
    },
    {
      "author": "Hall et al.",
      "year": 2023,
      "short_title": "A Systematic Review of Sophisticated Predictive and Prescriptive Analytics in Child Welfare: Accuracy, Equity, and Bias"
    },
    {
      "author": "Ho & Burke",
      "year": 2022,
      "short_title": "An Algorithm That Screens for Child Neglect Raises Concerns"
    },
    {
      "author": "Glaberson",
      "year": 2019,
      "short_title": "Coding Over the Cracks: Predictive Analytics and Child Protection"
    },
    {
      "author": "Putnam-Hornstein et al.",
      "year": 2022,
      "short_title": "Los Angeles County Risk Stratification Model: Methodology and Implementation Report"
    },
    {
      "author": "Saxena & Guha",
      "year": 2024,
      "short_title": "Algorithmic Harms in Child Welfare: Uncertainties in Practice, Organization, and Street-Level Decision-Making"
    }
  ],
  "assessment": {
    "domain_fit": "Hochgradig relevant für die Schnittstelle KI und Soziale Arbeit. Das Paper adressiert direkt die praktische Implementierung von ML-Systemen in hochriskanten sozialen Kontext mit Fokus auf Fairness, Equity und Ethik – zentrale Anliegen der Soziale Arbeit.",
    "unique_contribution": "Erste umfassende systematische Analyse, die algorithmische Entscheidungshilfen im Kinderschutz durch eine Implementierungswissenschaft-Linse (CFIR) untersucht und dabei ethische Erwägungen von Tool-Entwicklung bis Deployment berücksichtigt, mit explizitem Fokus auf reale Praktiken und Stakeholder-Perspektiven.",
    "limitations": "Nur 9 Studien eingeschlossen; erhebliche methodische und empirische Lücken in der bestehenden Literatur; begrenzte Evidenz zu Stakeholder-Perspektiven jenseits von Entwicklern; fehlende empirische Daten zu tatsächlichen Outcomes und langfristigen Auswirkungen auf Familien und Gemeinden."
  },
  "target_group": "Sozialarbeiter und Kindesschutzfachleute; KI-Entwickler und Data Scientists im Social-Sector; Policymaker und Verwaltungsfachleute in Kinderschutz-Agenturen; Forscher in Implementierungswissenschaft und Algorithmischer Fairness; Community-Organisationen und Vertreter betroffener Familien; Ethiker und Governance-Experten im Kontext von High-Stakes-Algorithmen"
}