{
  "metadata": {
    "title": "Gender in a stereo-(gender)typical EU AI law: A feminist reading of the AI act",
    "authors": [
      "Anastasia Karagianni"
    ],
    "year": 2025,
    "type": "journalArticle",
    "language": "en"
  },
  "core": {
    "research_question": "Wie adressiert die EU AI Act Geschlechterdebatten, Nicht-Diskriminierung und systemische Machtungleichgewichte aus einer feministischen Perspektive?",
    "methodology": "Theoretisch; kritische Textanalyse mit feministischen Rechtstheorien, Hermeneutische Analyse von AI Act-Bestimmungen durch die Linse feministischer Epistemologie und dekolonialer Theorie",
    "key_finding": "Obwohl die AI Act Massnahmen zur Mitigation geschlechterspezifischer Risiken vorsieht, adressiert sie nicht die strukturellen Biase in KI-Technologien, die marginalisierte Gruppen überproportional schädigen. Eine intersektionale feministische Perspektive enthüllt formale Gleichheitskonzepte, die substantive Ungleichheiten perpetuieren.",
    "data_basis": "nicht empirisch; kritische Analyse von EU 2024/1689 AI Act-Text, Recitals und Artikeln; literaturgestützte theoretische Argumentation"
  },
  "arguments": [
    "Die AI Act behandelt algorithmische Verzerrung als technisches statt strukturelles Problem und übersieht dadurch die patriarchalen Normen, die in Datensammlungen, Modelltraining und Regulierungsaufsicht eingebettet sind (MacKinnon's Dominanztheorie).",
    "Das Konzept der 'Vulnerabilität' in der AI Act ist unklar und stammt aus informatischen Kontexten, nicht aus Gender Studies oder sozialen Wissenschaften, wo es Autonomie und Abhängigkeit im Kontext kolonialer und patriarchaler Machtdynamiken bedeutet.",
    "Die AI Act verweist nur zweimal auf 'Geschlechtergleichstellung' in den Recitals und einmal in Artikel 95, während sie sich nicht auf inklusive Geschlechtsidentitäten (trans, nicht-binär, intersex) bezieht, was die Marginalität von Gender in der Regulierung anzeigt.",
    "Standardisierungsprozesse in der AI Act-Implementierung bleiben von Konzernen und Regierungen dominiert und riskieren Tokenismus, wenn marginalisierte Gruppen nicht substantiv in die Entwicklung von Fairness-Standards einbezogen werden.",
    "Feminist Impact Assessments sind notwendig, um zu prüfen, wie AI-Systeme intersektionale Ungleichheiten (Geschlecht, Rasse, Klasse, Behinderung) exazerbieren, besonders in Kontexten wie Einstellung, Gesundheitsversorgung und Strafverfolgung."
  ],
  "categories": {
    "AI_Literacies": false,
    "Generative_KI": true,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": true,
    "Bias_Ungleichheit": true,
    "Gender": true,
    "Diversitaet": true,
    "Feministisch": true,
    "Fairness": true
  },
  "category_evidence": {
    "Generative_KI": "Paper adressiert 'generative AI raises legal concerns, particularly in the proliferation of non-consensual sexualised deepfakes, which constitute a form of gender-based violence'",
    "KI_Sonstige": "Analysiert high-risk AI systems unter AI Act, predictive algorithms (SyRI-Fall), recruitment tools, healthcare diagnostics, AI-driven systems für soziale Wohlfahrt",
    "Soziale_Arbeit": "Bezug zu social services als high-risk Kontexte: 'in high-risk domains such as law enforcement and social services, to ensure compliance with the GDPR and safeguard fundamental rights'",
    "Bias_Ungleichheit": "Kernthema: 'AI systems have been shown to disproportionately misclassify racialised and gender-diverse individuals, reinforcing structural inequalities' und detaillierte Analyse algorithmischer Diskriminierung",
    "Gender": "Explizites Gender-Focus durchgehend: 'gynaecologic cancer detection, operate within binary gender frameworks, often misgendering transgender and non-binary patients'; Analyse geschlechterspezifischer Auswirkungen",
    "Diversitaet": "Intersektionale Perspektive zentral: 'assessments must account for the ways in which gender intersects with race, class, disability and other identity factors' und Fokus auf marginalisierte Communities",
    "Feministisch": "Explizit feministische Theorie verwendet: Miranda Fricker (hermeneutical injustice), Catharine MacKinnon (dominance theory), Kimberlé Crenshaw (intersectionality), Donna Haraway (agential realism), Ann Julia Cooper; 'feminist legal methods', 'decolonial feminist approach'",
    "Fairness": "Diskutiert algorithmische Fairness, Fairness in Hiring/Healthcare, 'intersectional data analysis in AI conformity assessments', Fairness-aware standards und Bias-Audits"
  },
  "references": [
    {
      "author": "Fricker, M.",
      "year": 2007,
      "short_title": "Epistemic Injustice: Power and the Ethics of Knowing"
    },
    {
      "author": "MacKinnon, C. A.",
      "year": 2013,
      "short_title": "Feminist Legal Theory on Male Dominance"
    },
    {
      "author": "Crenshaw, K.",
      "year": 1991,
      "short_title": "Intersectionality"
    },
    {
      "author": "Quijano, A.",
      "year": 2000,
      "short_title": "Coloniality of Power and Eurocentrism in Latin America"
    },
    {
      "author": "Mignolo, W.",
      "year": 2012,
      "short_title": "Decolonizing Western Epistemology"
    },
    {
      "author": "Haraway, D.",
      "year": 2014,
      "short_title": "Situated Knowledges"
    },
    {
      "author": "Andrews, L. & Bucher, B.",
      "year": 2022,
      "short_title": "Amazon's AI Recruitment Tool and Gender Bias"
    },
    {
      "author": "van Bekkum, M. & Borgesius, F. Z.",
      "year": 2021,
      "short_title": "Digital Welfare Fraud Detection and the Dutch SyRI Judgment"
    },
    {
      "author": "Keyes, O.",
      "year": 2018,
      "short_title": "The Misgendering Machines: Trans/HCI Implications of Automatic Gender Recognition"
    },
    {
      "author": "Ricaurte, P. & Zasso, M.",
      "year": 2023,
      "short_title": "AI, Ethics and Coloniality: A Feminist Critique"
    }
  ],
  "assessment": {
    "domain_fit": "Hochgradig relevant für KI/Soziale Arbeit/Gender-Schnittstelle: Das Paper analysiert regulatorische Frameworks (AI Act) mit feministischer Epistemologie und deckt auf, wie Soziale Dienste als high-risk AI-Kontexte betroffen sind, während marginalisierte Gruppen (Frauen, trans, PoC) überproportional Schaden erleiden.",
    "unique_contribution": "Erste systematische feministische Rechtsanalyse der EU AI Act, die Miranda Fricker's Hermeneutical Injustice, MacKinnon's Dominanztheorie und dekoloniale Ansätze integriert, um strukturelle Gender-Biase in KI-Governance zu kritisieren.",
    "limitations": "Paper ist rein theoretisch-analytisch ohne empirische Daten; fokussiert auf EU AI Act und nicht auf internationale Regulierungsmodelle; begrenzte Analyse von implementierungspraktiken nach Gesetzeskraft."
  },
  "target_group": "Policymaker und Regulatoren (EU), KI-Ethiker und -Entwickler, Sozialarbeiter und Soziale Dienste (betroffen von AI-Regulierung), Feminist und Dekolonial-Scholars, Menschenrechts- und Gleichstellungsorganisationen, digitale Rechts-Aktivisten"
}