{
  "metadata": {
    "title": "Evaluating the Prompt Steerability of Large Language Models",
    "authors": [
      "Erik Miehling",
      "Michael Desmond",
      "Karthikeyan Natesan Ramamurthy",
      "Elizabeth M. Daly",
      "Kush R. Varshney",
      "Eitan Farchi",
      "Pierre Dognin",
      "Jesus Rios",
      "Djallel Bouneffouf",
      "Miao Liu",
      "Prasanna Sattigeri"
    ],
    "year": 2024,
    "type": "conferencePaper",
    "language": "en"
  },
  "core": {
    "research_question": "In welchem Ausmaß können Large Language Models durch Prompting allein gesteuert werden, um verschiedene Personas und Wertsysteme widerzuspiegeln?",
    "methodology": "Empirisch: Benchmark-Entwicklung mit formaler Definition von Prompt-Steerability, Evaluation Profiles basierend auf Score-Funktionen, Analyse von Steerability-Indizes über mehrere Persona-Dimensionen hinweg unter Verwendung des Anthropic Persona Datasets.",
    "key_finding": "Aktuelle Modelle zeigen begrenzte Steerability durch Prompting, mit asymmetrischen Fähigkeiten über Persona-Dimensionen und bei steering-Richtungen. Größere Modelle sind steuerbarer, aber alle zeigen Schwierigkeiten, von ihrer Baseline-Persönlichkeit abzuweichen.",
    "data_basis": "Anthropic Persona Dataset mit Statements für multiple Persona-Dimensionen (agreeableness, conscientiousness, political views, moral frameworks etc.); Evaluationen an 6 verschiedenen LLMs; yes/no Antworten auf Profiling-Fragen als primäre Datenform"
  },
  "arguments": [
    "AI-Pluralismus erfordert Modelle, die verschiedene Wertsysteme und Persönlichkeiten darstellen können, nicht ein durchschnittliches Menschenpräferenz-Alignment. Steerability ist ein notwendiger Mechanismus für solche pluralistischen Systeme.",
    "Prompting ist die praktisch machbarste Steer-Methode für Nutzer ohne Zugriff auf Modellgewichte oder Computekapazität, obwohl Fine-Tuning und Aktivierungs-Steering technisch effektiver sind.",
    "Die gemessene Baseline-Persönlichkeit eines Modells bestimmt seinen Widerstand gegen prompt-basierte Steering; asymmetrische Steerability in verschiedenen Richtungen hindert Modelle daran, das volle Spektrum möglicher Personas anzunehmen."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": true,
    "Prompting": true,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Benchmark zur Bewertung der Fähigkeit von Modellen, verschiedene Wertsysteme zu repräsentieren; 'understanding how much a model can be steered along a given dimension' als Kernkompetenz für AI-Literalität",
    "Generative_KI": "Fokus auf Large Language Models und deren Prompt-Verhalten; Evaluierung von LLMs wie GPT-4, Llama-3, Phi-3, Granite-Modellen",
    "Prompting": "Zentral: 'Our investigation focuses on prompting, primarily due to its simplicity in modifying model behavior'; Design von Steering-Funktionen σ über Prompts; Analyse von Persona-Statements als Prompting-Prinzipien",
    "KI_Sonstige": "Alignment-Forschung, Kontrolltheorie für LLMs, In-Context Learning, formally definition von Modellverhalten durch Evaluation Profiles",
    "Bias_Ungleichheit": "Asymmetrische Steerability führt zu ungleichen Möglichkeiten, verschiedene Perspektiven zu vertreten; Baseline-Skew erzeugt Verzerrungen in der Steerability über Dimensionen",
    "Diversitaet": "'designing models that are able to be shaped to represent a wide range of value systems and cultures'; Fokus auf Pluralismus und Repräsentation verschiedener Persönlichkeiten und Wertsysteme",
    "Fairness": "Algorithmic Fairness durch Steerability: 'designing steerable models' für faire Behandlung verschiedener Wertesysteme; Evaluation von Fairness über multiple Persona-Dimensionen"
  },
  "references": [
    {
      "author": "Klingefjord et al.",
      "year": 2024,
      "short_title": "AI/Algorithmic Pluralism"
    },
    {
      "author": "Perez et al.",
      "year": 2022,
      "short_title": "Red Teaming Language Models with Language Models"
    },
    {
      "author": "Wolf et al.",
      "year": 2023,
      "short_title": "Existence theorem on prompt manipulation"
    },
    {
      "author": "Bhargava et al.",
      "year": 2023,
      "short_title": "Control-theoretic perspective on prompt steerability"
    },
    {
      "author": "Li et al.",
      "year": 2023,
      "short_title": "Persona embeddings via prompt tuning"
    },
    {
      "author": "Brown et al.",
      "year": 2020,
      "short_title": "Language Models are Few-Shot Learners"
    },
    {
      "author": "Yao et al.",
      "year": 2023,
      "short_title": "FULCRA: Moral Values in Language Models"
    },
    {
      "author": "Feng et al.",
      "year": 2024,
      "short_title": "Community LMs for Pluralistic AI"
    },
    {
      "author": "Anil et al.",
      "year": 2024,
      "short_title": "Many-Shot Jailbreaking"
    },
    {
      "author": "Sorensen et al.",
      "year": 2024,
      "short_title": "AI Pluralism"
    }
  ],
  "assessment": {
    "domain_fit": "Das Paper hat begrenzte direkte Relevanz für Soziale Arbeit, ist aber hochgradig relevant für die Schnittstelle von KI-Governance, Fairness und pluralistischen Systemen, die vulnerable Gruppen repräsentieren müssen. Die Erkenntnisse zur Baseline-Persönlichkeit und asymmetrischen Steerability könnten für sozialarbeiterische Anwendungen von KI-Systemen wichtig sein.",
    "unique_contribution": "Erste formalisierte Benchmark zur Messung von Prompt-Steerability mit kontrollierbaren Metriken (Steerability Indices), die baseline-Verhalten berücksichtigen und vergleichbare Analysen über Modelle und Persona-Dimensionen ermöglichen.",
    "limitations": "Limitierungen: Abhängigkeit von Datensatzqualität (Persona-Statements); Binary Response-Format als mögliche Vereinfachung von realem Modellverhalten; keine Joint-Steerability über mehrere Dimensionen; Single-Turn Steering statt Multi-Turn Sequenzen; Möglichkeit von Caricature-Effekten; potentielle Ähnlichkeit zu Many-Shot Jailbreaking könnte Modell-Resistenz erklären."
  },
  "target_group": "KI-Entwickler und Forscher (primär), AI Governance und Policy-Maker, Alignment-Forscher, Plattformdesigner von LLM-Systemen, sekundär: Sozialarbeiter und Care-Professionelle, die KI-Systeme einsetzen oder regulieren"
}