{
  "metadata": {
    "title": "Decision support and algorithmic support: the construction of algorithms and professional discretion in social work",
    "authors": [
      "Marie Leth Meilvang",
      "Anne Marie Dahler"
    ],
    "year": 2024,
    "type": "journalArticle",
    "language": "en"
  },
  "core": {
    "research_question": "How do decision-support algorithms in social work reconfigure ideas about professional discretion, and what is meant by professionals needing 'decision support'?",
    "methodology": "Mixed methods: Two-step qualitative study combining semi-structured interviews (n=11 initial interviews + project-specific interviews), observations, and document analysis of three Danish municipal algorithmic projects in social work with vulnerable children and families.",
    "key_finding": "Algorithms in Danish social work create an ambivalent and undecided relationship with professional discretion: they are designed to constrain subjective judgment through standardization and objectification, yet fundamentally depend on professional discretion for contextual information, data provision, and bias assessment.",
    "data_basis": "11 initial expert interviews; interviews with 4 assistants, 2 social workers, 1 project manager, 1 digital consultant, 1 data scientist, 1 unit manager (Rita Referral project); 2 expert group meetings with multiple stakeholders (Algorithm Project); informal talks and document analysis (Referrals in Focus project). Total: 3 municipal projects analyzed."
  },
  "arguments": [
    "Political anxiety about high-profile child protection failures (Tønder and Brønderslev cases) drives algorithmic decision-support development, framing professional discretion as a liability requiring technological control rather than as a legitimate source of expertise.",
    "Algorithms are intended to standardize casework, objectify data, and eliminate bias, but these aims contradict the necessity of contextual judgment in social work—creating a paradox where algorithms require the very professional discretion they are designed to constrain.",
    "The claim of 'objective' data is undermined by the fact that social workers themselves produce or fundamentally shape the data feeding algorithms, while data quality is acknowledged as poor—suggesting validity comes from volume rather than quality."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": true,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "The study examines how professionals understand, interpret, and use algorithmic systems in their practice: 'professionals should be aware of potential bias in data and, hence, in algorithmic systems. Discussions about known bias or potential future bias should be a key concern in the development of algorithms.'",
    "KI_Sonstige": "Focuses on algorithmic decision-support systems in social work: 'algorithms in social work are most frequently used as systems that work as decision support' and analyzes predictive and categorization algorithms used in referral screening.",
    "Soziale_Arbeit": "Direct engagement with social work practice, casework, professional discretion in child protection services: 'the analysis of intentions, designs, and workings of decision-support algorithms in casework with vulnerable children and families' across three Danish municipalities.",
    "Bias_Ungleichheit": "Examines algorithmic bias and concerns about discrimination: 'considerable efforts are made to avoid such bias as relate to gender, ethnicity, and age' and discusses how poor data quality and subjective decision-making create systematic biases.",
    "Diversitaet": "Discusses marginalized communities and representation: concern that algorithms may replicate patterns of discrimination based on protected characteristics (gender, ethnicity, age) and the importance of including diverse professional perspectives in algorithm design.",
    "Fairness": "Directly addresses algorithmic fairness: 'eliminating bias from casework' is identified as a key theme, and the paper discusses fairness in algorithmic categorization and the need for regular testing of algorithms for bias."
  },
  "references": [
    {
      "author": "Lipsky",
      "year": 1980,
      "short_title": "Street-level Bureaucracy"
    },
    {
      "author": "Bovens & Zouridis",
      "year": 2002,
      "short_title": "From Street-level to System-level Bureaucracies"
    },
    {
      "author": "Busch & Henriksen",
      "year": 2018,
      "short_title": "Digital Discretion"
    },
    {
      "author": "Eubanks",
      "year": 2018,
      "short_title": "Automating Inequality"
    },
    {
      "author": "Gillingham",
      "year": 2019,
      "short_title": "Predictive Algorithms in Child Protection"
    },
    {
      "author": "Bowker & Star",
      "year": 1999,
      "short_title": "Sorting Things Out: Classification and Its Consequences"
    },
    {
      "author": "Petersen et al.",
      "year": 2021,
      "short_title": "We would never write that down"
    },
    {
      "author": "Jørgensen et al.",
      "year": 2021,
      "short_title": "Comparative Policy Analysis of Predictive Tools in Child Protection Services"
    },
    {
      "author": "Williamson & Piattoeva",
      "year": 2019,
      "short_title": "Objectivity as Standardization in Data-scientific Education Policy"
    },
    {
      "author": "Parton",
      "year": 2008,
      "short_title": "Changes in the Form of Knowledge in Social Work"
    }
  ],
  "assessment": {
    "domain_fit": "Highly relevant for AI/Social Work intersection. The paper provides empirical analysis of how algorithmic systems reshape professional practice in child protection, addressing tensions between automation, standardization, and professional expertise in vulnerable populations' casework.",
    "unique_contribution": "Reveals the paradoxical and ambivalent relationship between algorithms and professional discretion in social work—showing that algorithms depend on the very professional judgment they are designed to replace, rather than eliminating discretion as technological determinism might suggest.",
    "limitations": "Study limited to development and testing phases of algorithms in Danish municipalities; implementation at scale may reveal different dynamics. Access to algorithm use in practice was limited, and conclusions may not generalize to other national contexts or types of social work interventions."
  },
  "target_group": "Social work practitioners and managers, algorithmic designers/developers working in public administration, policymakers overseeing algorithmic implementation in welfare services, researchers in social work, digital governance, and critical AI studies, organizational leaders in municipal social services."
}