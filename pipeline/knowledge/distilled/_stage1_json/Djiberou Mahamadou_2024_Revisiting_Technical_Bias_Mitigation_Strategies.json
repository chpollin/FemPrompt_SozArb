{
  "metadata": {
    "title": "Revisiting Technical Bias Mitigation Strategies",
    "authors": [
      "Abdoul Jalil Djiberou Mahamadou",
      "Artem A. Trotsyuk"
    ],
    "year": 2024,
    "type": "journalArticle",
    "language": "en"
  },
  "core": {
    "research_question": "Welche praktischen Limitationen haben technische Bias-Mitigationsstrategien in Gesundheitssystemen, und wie können stakeholder-zentrierte Ansätze diese adressieren?",
    "methodology": "Narrative Review mit systematischer Analyse von fünf Dimensionen (Who, Which, When, To Whom, Where) anhand empirischer Studien im Healthcare-Kontext",
    "key_finding": "Technische Bias-Mitigationsstrategien sind notwendig aber unzureichend; ein wertsensibler, partizipativer Ansatz, der Stakeholder in alle Entwicklungsphasen einbezieht, ist erforderlich, um echte Fairness zu erreichen.",
    "data_basis": "Sekundäranalyse empirischer Studien aus Healthcare und Biomedizin; keine primären Datenerhebungen"
  },
  "arguments": [
    "Technische Lösungen zu Bias und Fairness sind bisher erfolgreich in spezifischen klinischen Kontexten (z.B. Hautläsionen-Diagnose, Mortalitätsvorhersage), aber nur ~22% der KI-Implementierungen zeigen direkte Auswirkungen auf Gesundheitsergebnisse, was auf translatorische Herausforderungen hindeutet.",
    "Die Definition von Bias und Fairness ist nicht neutral: KI-Entwickler dominieren diese Prozesse, während Patient:innen und Cliniker:innen häufig widersprechende Fairness-Verständigungen haben, was zu Systemen führt, die nicht die Werte der betroffenen Populationen widerspiegeln.",
    "Dutzende technische Mitigationsstrategien sind inkonsistent und inkompatibel; die Wahl zwischen statistischen und kausalen Ansätzen, die Timing im Entwicklungsprozess (Pre-, In-, Post-Processing), die Zielpopu­lationen und kulturelle Kontexte sind kritische Entscheidungspunkte, bei denen derzeit keine systematischen Richtlinien existieren.",
    "Value-Sensitive AI und partizipatives Design können diese Lücken schließen, indem sie Stakeholder-Engagement in alle Phasen des KI-Entwicklungszyklus integrieren und sicherstellen, dass technische Lösungen mit lokalen Werten, kulturellen Normen und gelebten Erfahrungen von betroffenen Gruppen übereinstimmen."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Education is critical for meaningful patient engagement and understanding the technical complexities and social implications of bias. The authors advocate for AI literacy among deployed stakeholders.",
    "KI_Sonstige": "Paper adressiert Machine Learning, Algorithmen zur Bias-Detektion, Fairness-Metriken, und KI-Systeme im Healthcare (Diagnose, Vorhersage).",
    "Bias_Ungleichheit": "AI can exhibit and perpetuate inherent social biases amplifying health inequities. Discusses data biases, algorithmic biases, user interaction biases, minority bias, label bias; examines how interconnection between bias sources exacerbates effects.",
    "Diversitaet": "Diversity in AI developers can address misalignment of interests; Hispanic, Black, and African Americans account for only 3.2% and 2.4% of new AI PhDs in the US. Advocates for diversity extending beyond workforce to include voices of those impacted by AI systems. Examines intersectionality, underrepresented populations, marginalized groups.",
    "Fairness": "Central focus on algorithmic fairness metrics (Demographic Parity, Equalized Odds, Calibration), fairness definitions across stakeholders, Stakeholders' Agreement on Fairness framework, statistical vs. causal fairness approaches, fairness trade-offs, context-specific fairness in healthcare applications."
  },
  "references": [
    {
      "author": "Friedman & Nissenbaum",
      "year": 1996,
      "short_title": "Bias in computer systems"
    },
    {
      "author": "Mehrabi et al.",
      "year": 2021,
      "short_title": "A Survey on Bias and Fairness in Machine Learning"
    },
    {
      "author": "Chen et al.",
      "year": 2021,
      "short_title": "Ethical Machine Learning in Healthcare"
    },
    {
      "author": "Buolamwini & Buolamwini",
      "year": 2023,
      "short_title": "Fairness of artificial intelligence in healthcare"
    },
    {
      "author": "Schuler & Namioka",
      "year": 2017,
      "short_title": "Participatory Design: Principles and Practices"
    },
    {
      "author": "Freedman et al.",
      "year": 2020,
      "short_title": "Adapting a kidney exchange algorithm to align with human values"
    },
    {
      "author": "Asiedu et al.",
      "year": 2024,
      "short_title": "The Case for Globalizing Fairness: A Mixed Methods Study on Colonialism, AI, and Health in Africa"
    },
    {
      "author": "Miller",
      "year": 2022,
      "short_title": "Stakeholder roles in artificial intelligence projects"
    },
    {
      "author": "Chen et al.",
      "year": 2023,
      "short_title": "Human-Centered Design to Address Biases in Artificial Intelligence"
    },
    {
      "author": "National Academies of Sciences, Engineering, and Medicine",
      "year": 2023,
      "short_title": "Using Population Descriptors in Genetics and Genomics Research"
    }
  ],
  "assessment": {
    "domain_fit": "Hochgradig relevant für die Schnittstelle KI und gesellschaftliche Auswirkungen, mit starkem Healthcare-Fokus. Für Soziale Arbeit begrenzt relevant, da kein direkter Bezug zur Profession, aber fundamentale Erkenntnisse zu Bias-Mitigation und Stakeholder-Partizipation sind übertragbar auf sozialtechnische Systeme in der Sozialen Arbeit.",
    "unique_contribution": "Das Paper leistet einen innovativen strukturierten Vergleich technischer Limitationen entlang fünf Dimensionen (Who, Which, When, To Whom, Where) und integriert Value-Sensitive AI mit Community-Based Participatory Research-Praktiken für Healthcare-Kontexte.",
    "limitations": "Paper ist narrativer Review ohne primäre Datenerhebung; der vorgeschlagene VSAI-Rahmen wird als theoretisch beschrieben und nicht in realen Implementierungen validiert; eingeschränkter Fokus auf angelsächsische Kontexte trotz Kritik an Western-centric Ansätzen."
  },
  "target_group": "KI-Entwickler:innen und Data Scientists in Healthcare, Health Policy Maker, Clinical stakeholders (Ärzt:innen, Pflegepersonal), Bioethiker:innen, Fairness- und AI-Governance-Spezialist:innen, Gesundheitssystemmanager:innen"
}