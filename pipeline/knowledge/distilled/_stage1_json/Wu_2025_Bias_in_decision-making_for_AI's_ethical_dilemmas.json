{
  "metadata": {
    "title": "Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study of ChatGPT and Claude",
    "authors": [
      "Yile Yan",
      "Yuqi Zhu",
      "Wentao Xu"
    ],
    "year": 2025,
    "type": "conferencePaper",
    "language": "en"
  },
  "core": {
    "research_question": "Weisen Large Language Models (LLMs) Bias bei geschützten Attributen auf und unterscheiden sich GPT-3.5 Turbo und Claude 3.5 Sonnet in ihren ethischen Entscheidungsmustern systematisch?",
    "methodology": "Empirisch - Simulationsstudie mit systematischer Evaluation von LLM-Antworten zu ethischen Dilemmata; 11.200 experimentelle Durchläufe mit kontrollierten Variationen von geschützten Attributen (Alter, Geschlecht, Rasse, Aussehen, Behinderungsstatus) in einzelnen und intersektionalen Kombinationen; quantitative Analyse mittels Normalized Frequency, Ethical Preference Priority, Sensitivity, Stability und Clustering-Analysen.",
    "key_finding": "Beide LLMs zeigen signifikante Biases bei geschützten Attributen in ethischen Entscheidungen: GPT-3.5 Turbo bevorzugt stereotypisch dominante Gruppen (nicht-behinderte, männliche, hellhäutige, mittelaltrige Personen), während Claude 3.5 Sonnet ausgewogenere Präferenzen zeigt. Beide Modelle bevorzugen stark 'Good-looking' Personen, und die ethische Sensibilität sinkt drastisch in komplexeren intersektionalen Szenarien. Linguistische Referenten (z.B. 'Yellow' vs. 'Asian') beeinflussen die ethischen Bewertungen erheblich.",
    "data_basis": "n=11.200 experimentelle Trials (5.600 pro Modell); 7 Gruppen geschützter Attribute (20 Attribute insgesamt); 50 Iterationen pro Attributgruppe; 4 Wiederholungsrunden; Vergleich von GPT-3.5 Turbo und Claude 3.5 Sonnet"
  },
  "arguments": [
    "LLMs prägen Menschenrechte und Gerechtigkeit durch ihre ethischen Entscheidungsmuster: Die starke Präferenz für 'Good-looking' und die Diskriminierung von 'Unpleasant-looking', 'African', 'Yellow' und 'Disabled' Personen dokumentiert, dass Biases nicht zufällig sind, sondern systematische Muster menschlicher Diskriminierung reproduzieren, die in Trainingsdaten kodiert sind.",
    "Intersektionale Komplexität enthüllt versteckte Biases und verschärft ethische Risiken: Während Single-Attribute-Szenarios bereits Biases zeigen, sinkt die ethische Sensibilität (Unselected Frequency) in intersektionalen Szenarien bei beiden Modellen deutlich ab. Dies deutet darauf hin, dass LLMs mit mehrfach marginalisierten Personen besonders problematisch umgehen und ihre Vulnerabilität nicht erfassen können.",
    "Unterschiedliche Modellarchitekturen perpetuieren unterschiedliche Unterdrückungsmuster: GPT-3.5 zeigt Bias zugunsten traditioneller Machtstrukturen (Non-disabled, Masculine, Caucasian), während Claude diversere Präferenzen aufweist. Dies suggeriert, dass Trainings- und Fine-Tuning-Prozesse nicht neutral sind, sondern konkrete Machtverhältnisse widerspiegeln und bei Deployment in autonomen Systemen zu unterschiedlichen Diskriminierungsmustern führen."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": true,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": true,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Die Studie adressiert kritisches Verständnis von LLM-Funktionsweisen und deren ethischen Limitationen: 'it is undeniable that ethical limitations in AI still exist and should be publicly acknowledged' und 'it's crucial to demystify their performance in ethical contexts'",
    "Generative_KI": "Fokus auf zwei prominente LLM-Modelle: 'Using two prominent models - GPT-3.5 Turbo and Claude 3.5 Sonnet' und systematische Analyse ihrer Antwortmuster in ethischen Dilemmata",
    "KI_Sonstige": "Breites KI-Fairness-Framework: 'bias being a prominent representative' von ethischen Problemen in ML-Systemen und Analyse von 'algorithmische Biases' im Allgemeinen",
    "Bias_Ungleichheit": "Zentrale Fokussierung auf Diskriminierung und strukturelle Benachteiligung: 'The biased outputs may lead to unfair treatment to the underrepresented individuals or groups of people, exacerbate the pre-existing inequalities' und 'systematic neglect of others'",
    "Gender": "Geschlecht als geschütztes Attribut systematisch untersucht: 'Gender' in den 7 Attributgruppen mit Masculine, Feminine, Androgynous; Befund: 'Claude 3.5 Sonnet demonstrated more diverse protected attribute choices' inkl. Feminine-Präferenzen",
    "Diversitaet": "Intersektionale und diversitätsorientierte Analyse: '7 groups of 20 attributes', 'intersectional protected attribute combinations', Analyse von Race (Asian, Caucasian, African), Age, Disability Status; Befund: 'ethical sensitivity significantly decreases in more complex scenarios involving multiple protected attributes'",
    "Fairness": "Explizite Fairness-Fokussierung: 'justice and fairness' als zentrale Ethik-Prinzipien; 'fairness, justice, and accountability of ethical AI'; systematische Fairness-Metriken (Normalized Frequency, Preference Priority, Sensitivity, Stability, Clustering); algorithmic fairness Framework"
  },
  "references": [
    {
      "author": "Obermeyer et al.",
      "year": 2019,
      "short_title": "Dissecting racial bias in an algorithm used to manage the health of populations"
    },
    {
      "author": "Jobin, Ienca, Vayena",
      "year": 2019,
      "short_title": "The global landscape of AI ethics guidelines"
    },
    {
      "author": "Floridi & Cowls",
      "year": 2022,
      "short_title": "A unified framework of five principles for AI in society"
    },
    {
      "author": "Barocas, Hardt, Narayanan",
      "year": 2023,
      "short_title": "Fairness and Machine Learning: Limitations and Opportunities"
    },
    {
      "author": "Kearns et al.",
      "year": 2018,
      "short_title": "Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness"
    },
    {
      "author": "Stahl & Stahl",
      "year": 2021,
      "short_title": "Ethical issues of AI"
    },
    {
      "author": "Gallegos et al.",
      "year": 2024,
      "short_title": "Bias and fairness in large language models: A survey"
    },
    {
      "author": "Corbett-Davies et al.",
      "year": 2024,
      "short_title": "The measure and mismeasure of fairness"
    },
    {
      "author": "Hofmann et al.",
      "year": 2024,
      "short_title": "AI generates covertly racist decisions about people based on their dialect"
    },
    {
      "author": "Naveed et al.",
      "year": 2023,
      "short_title": "A comprehensive overview of large language models"
    }
  ],
  "assessment": {
    "domain_fit": "Das Paper ist hochrelevant für Schnittstellen von KI und sozialer Gerechtigkeit, hat aber keinen direkten Bezug zu Sozialer Arbeit als Berufspraxis. Für KI-Ethik, Fairness und die Analyse von Diskriminierungsmustern in autonomen Systemen ist es zentral; die Erkenntnisse sind für Soziale Arbeit relevant, falls KI-Systeme in Entscheidungen mit betroffenen Gruppen eingebunden werden (z.B. Risikobewertung, Ressourcenallokation).",
    "unique_contribution": "Erste systematische vergleichende Analyse von GPT und Claude über 11.200 Trials mit intersektionalen geschützten Attributen in ethischen Dilemmata, die zeigt, dass ethische Sensibilität in komplexeren Szenarien kollabiert und linguistische Referenten zentral sind – methodologisch rigorose Quantifizierung von LLM-Biases jenseits einzelner Attribute.",
    "limitations": "Keine Angabe zur Validierung der 'ethischen Korrektheit' der Modell-Antworten gegen normative Standards; begrenzt auf zwei Modelle und englischsprachige Prompts; kein Einbezug von Open-Source-Modellen; Mechanismen hinter Biases unklar aufgrund mangelnder Modell-Transparenz; keine Analyse möglicher Interventionen oder Debiasing-Strategien."
  },
  "target_group": "KI-Entwickler und Designteams (insbes. bei ChatGPT/Claude-Integration), AI Ethics Researcher, Policy Maker in AI Governance, Organisationen mit autonomen Entscheidungssystemen (Autonomous Driving, Disaster Response), Sozialarbeiter:innen die KI-Systeme kritisch evaluieren müssen, Fairness und Accountability Auditor:innen"
}