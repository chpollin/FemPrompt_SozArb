{
  "metadata": {
    "title": "Advancing Accountability in AI: Governing and Managing Risks Throughout the Lifecycle for Trustworthy AI",
    "authors": [
      "Karine Perset",
      "Luis Aranda"
    ],
    "year": 2023,
    "type": "report",
    "language": "en"
  },
  "core": {
    "research_question": "Wie können Risikomanagement-Ansätze die Umsetzung der OECD-KI-Prinzipien über den gesamten KI-System-Lebenszyklus hinweg ermöglichen und zu vertrauenswürdiger KI beitragen?",
    "methodology": "Theoretisch/Review - Systematische Analyse von Frameworks, Best Practices und technischen Ansätzen; basierend auf Konsultationen von ca. 200 Expert:innen in zwei Arbeitsgruppen (Classification & Risk, Tools & Accountability) zwischen Februar 2020 und Dezember 2022",
    "key_finding": "Das Papier präsentiert ein strukturiertes DEFINE-ASSESS-TREAT-GOVERN Rahmenwerk, das zeigt, wie Risikomanagement-Praktiken über den gesamten KI-Lebenszyklus (Planung, Datenerfassung, Modellentwicklung, Validierung, Deployment, Betrieb) konkrete technische und prozessuale Maßnahmen zur Umsetzung der OECD-KI-Prinzipien ermöglichen kann.",
    "data_basis": "Nicht empirisch; Policy-Analyse basierend auf Expert-Input von 200 Teilnehmer:innen aus Regierung, Industrie, Zivilgesellschaft und akademischen Institutionen"
  },
  "arguments": [
    "Accountability in KI erfordert ein systematisches, lebenszyklusbasiertes Risikomanagement-Rahmenwerk (DEFINE-ASSESS-TREAT-GOVERN), das technische und prozessuale Ansätze kombiniert und kontinuierliche Überwachung, Dokumentation und Stakeholder-Konsultation einbezieht.",
    "Vertrauenswürdige KI-Systeme müssen mehrere Dimensionen adressieren: Human-centered values und Fairness, Transparenz und Erklärbarkeit, Robustheit/Sicherheit/Safety sowie Nutzen für Mensch und Planet, mit expliziten Strategien zur Behandlung von Zielkonflikten zwischen diesen Prinzipien.",
    "Organisationen müssen eine Risikokultur entwickeln, die technische Expertise mit prozessualen Governance-Strukturen verbindet, klare Verantwortlichkeiten für verschiedene Akteure (Provider, Nutzer, Regulatoren) definiert und dokumentation sowie Konsultation von Stakeholdern über alle Lifecycle-Phasen hinweg einbezieht."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Das Papier fordert Dokumentation, technisches Verständnis und Kompetenz in Risikomanagement: 'Provide technical documentation and user manuals for operators and users of the system' und 'Ensure that insights and disclosures are directed to the end-users affected by the model'",
    "KI_Sonstige": "Umfassende Behandlung von ML-Systemen, Robustness, Security, Safety, Adversarial Examples, Model Verification: 'AI systems should be robust, secure, and safe throughout their lifespan so that they function appropriately in conditions of normal use, foreseeable misuse, or other adverse conditions'",
    "Bias_Ungleichheit": "Explizite Behandlung von Fairness, Bias-Mitigation und Diskriminierungsrisiken: 'Risks to human-centred values and fairness' einschließlich Proxy Discrimination, mit Verweis auf 'Prince and Schwarcz (2020) on Proxy Discrimination in the Age of Artificial Intelligence and Big Data'",
    "Diversitaet": "Betonung von Inklusion und verschiedenen Stakeholder-Perspektiven: 'Making AI Inclusive: 4 Guiding Principles for Ethical Engagement' und Einbeziehung von Civil Society, Geschäftsverbänden und Gewerkschaften in Governance-Prozesse",
    "Fairness": "Kernfokus auf Fairness-Metriken und -Behandlung: 'Human-centred values and fairness' als wesentliche OECD-Prinzipien mit Ansätzen wie Fairness Constraints, Learning Fair Representations, Adversarial Learning zur Bias-Mitigation"
  },
  "references": [
    {
      "author": "OECD",
      "year": 2019,
      "short_title": "OECD AI Principles"
    },
    {
      "author": "Raji et al.",
      "year": 2020,
      "short_title": "Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing"
    },
    {
      "author": "Reisman et al.",
      "year": 2019,
      "short_title": "Algorithmic impact assessment: a practical framework for public agency accountability"
    },
    {
      "author": "Schwartz et al.",
      "year": 2021,
      "short_title": "Proposal for Identifying and Managing Bias in Artificial Intelligence"
    },
    {
      "author": "Suresh & Guttag",
      "year": 2021,
      "short_title": "A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle"
    },
    {
      "author": "Molnar, Casalicchio & Bischl",
      "year": 2020,
      "short_title": "Explainable Machine Learning"
    },
    {
      "author": "Prince & Schwarcz",
      "year": 2020,
      "short_title": "Proxy Discrimination in the Age of Artificial Intelligence and Big Data"
    },
    {
      "author": "Paleyes, Urma & Lawrence",
      "year": 2020,
      "short_title": "Challenges in Deploying Machine Learning: a Survey of Case Studies"
    },
    {
      "author": "Park",
      "year": 2022,
      "short_title": "Making AI Inclusive: 4 Guiding Principles for Ethical Engagement"
    },
    {
      "author": "Zafar et al.",
      "year": 2019,
      "short_title": "Fairness Constraints: A Flexible Approach for Fair Classification"
    }
  ],
  "assessment": {
    "domain_fit": "Das Papier ist relevant für die Schnittstelle KI/Governance, adressiert aber nicht explizit Soziale Arbeit oder Gender-Perspektiven. Es liefert jedoch wichtige Governance- und Accountability-Frameworks, die für Sozialarbeiter:innen relevant sind, die mit KI-gestützten Entscheidungssystemen arbeiten oder diese kritisch evaluieren müssen.",
    "unique_contribution": "Das Papier bietet ein systematisches, international abgestimmtes (OECD) Rahmenwerk zur Operationalisierung von KI-Ethik-Prinzipien durch konkrete technische und prozessuale Risikomanagement-Maßnahmen über den gesamten AI-Lebenszyklus, was über bisherige abstrakte Principles-Diskussionen hinausgeht.",
    "limitations": "Das Papier adressiert nicht explizit Kontexte von Marginalisierung, Machtasymmetrien oder strukturelle Ungleichheitsauswirkungen von KI-Systemen auf vulnerable Populationen (etwa in Sozialleistungsverwaltung); auch fehlt eine differenzierte Perspektive auf die Machtdynamiken zwischen verschiedenen Stakeholdern (Provider vs. betroffene Communitys)"
  },
  "target_group": "Policymaker, Regulatoren, KI-Entwickler und Governance-Experten; sekundär relevant für Sozialarbeiter:innen und Fachkräfte in Kontexten, in denen KI-Systeme auf vulnerable Populationen angewendet werden; Organisationen, die KI implementieren oder regulieren"
}