{
  "metadata": {
    "title": "Trustworthy AI and the Logics of Intersectional Resistance",
    "authors": [
      "Bran Knowles",
      "Jasmine Fledderjohann",
      "John T. Richards",
      "Kush R. Varshney"
    ],
    "year": 2023,
    "type": "conferencePaper",
    "language": "en"
  },
  "core": {
    "research_question": "Wie können wir die legitimen Gründe für Misstrauen gegenüber KI bei marginalisierten Menschen verstehen und in die Konzeptualisierung von 'vertrauenswürdiger KI' integrieren?",
    "methodology": "Theoretisch-konzeptuell. Kritische Analyse von Trustworthy AI Literatur kombiniert mit sozialwissenschaftlichen Perspektiven auf Vertrauen, intersektionale Theorie und Machtkritik.",
    "key_finding": "Distrust von KI durch marginalisierte Gruppen ist rational und gerechtfertigt, nicht irrational. Aktuelle Trustworthy AI Ansätze ignorieren die strukturellen Gründe für dieses Misstrauen und können dadurch öffentliches Misstrauen sogar verstärken.",
    "data_basis": "nicht empirisch; theoretisch-konzeptuelle Analyse mit Literaturbasierung"
  },
  "arguments": [
    "Distrust gegenüber KI bei marginalisierten Menschen ist nicht einfach cognitive Voreingenommenheit, sondern rationales, wohlbegründetes Misstrauen basierend auf historischen und gegenwärtigen Erfahrungen struktureller Gewalt durch Institutionen und Technologien.",
    "KI wird verwendet um eine 'digitale Underclass' zu konstruieren, die als 'undeserving' (unverdient) dargestellt wird, um strukturelle Gewalt moralisch zu rechtfertigen und der privilegierten Bevölkerung zu ermöglichen, ihre eigene Unbewusstheit zu bewahren.",
    "Vertrauen ist nicht objektiv; es ist teilweise subjektiv und durch Machtverhältnisse strukturiert. Menschen mit weniger Ressourcen können sich rationales Misstrauen eher leisten als marginalisierte Menschen, deren Risiko bei Vertrauen höher ist."
  ],
  "categories": {
    "AI_Literacies": false,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": true,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": true,
    "Fairness": true
  },
  "category_evidence": {
    "KI_Sonstige": "Fokus auf algorithmic decision-making systems, predictive analytics im Kontext von Ressourcenallokation, Risikobewertung und sozialen Kontrollmechanismen.",
    "Soziale_Arbeit": "Explizite Bezüge zu Vulnerability, Care-Arbeit, Sozialschutz, marginalisierte Bevölkerungsgruppen als Zielgruppen von KI-Systemen. Beispiele: Sozialhilfesysteme, Jugendhilfe, Risikoeinschätzung.",
    "Bias_Ungleichheit": "Zentrale These: 'AI is being used to construct a digital underclass that is rhetorically labelled as undeserving' [Abstrakt]. Analyse struktureller Gewalt ('structural violence') durch KI, Reproduktion von Ungleichheit, Digital Divide.",
    "Diversitaet": "Multiply marginalized people und intersektionale Perspektive durchziehen das gesamte Paper. Spezifische Analyse unterschiedlicher Marginalisierungsformen (Race, Class, Disability etc.) und deren Kumulation.",
    "Feministisch": "Explizite Verwendung von intersectionality (Crenshaw), Referenzen auf D'Ignazio & Klein 'Data Feminism', care ethics und Jean Watson's theory of human caring. Feminist ML approaches zitiert. Intersektionale und Care-theoretische Rahmung."
  },
  "references": [
    {
      "author": "D'Ignazio, Catherine & Klein, Lauren F.",
      "year": 2020,
      "short_title": "Data Feminism"
    },
    {
      "author": "Benjamin, Ruha",
      "year": 2019,
      "short_title": "Race After Technology / Racial logics of trust"
    },
    {
      "author": "Eubanks, Virginia",
      "year": 2018,
      "short_title": "Automating Inequality"
    },
    {
      "author": "Noble, Safiya Umoja",
      "year": 2018,
      "short_title": "Algorithms of Oppression"
    },
    {
      "author": "Crenshaw, Kimberlé",
      "year": 1989,
      "short_title": "Intersectionality concept"
    },
    {
      "author": "Baier, Annette",
      "year": 2020,
      "short_title": "Trust and Trustworthiness"
    },
    {
      "author": "Green, Ben",
      "year": 2020,
      "short_title": "The False Promise of Risk Assessments"
    },
    {
      "author": "McQuillan, Dan",
      "year": 2022,
      "short_title": "Resisting AI: An Anti-fascist Approach"
    },
    {
      "author": "Fricker, Miranda",
      "year": 2007,
      "short_title": "Epistemic Injustice"
    },
    {
      "author": "Roberts, Dorothy",
      "year": 2014,
      "short_title": "Complicating the triangle of race, class and state"
    }
  ],
  "assessment": {
    "domain_fit": "Hochgradig relevant für Schnittstelle KI/Soziale Arbeit/Gender. Das Paper adressiert zentral, wie KI in Systemen sozialer Kontrolle und Ressourcenallokation Marginalisierte schadet und warum Sozialarbeiter:innen kritische Perspektiven auf KI brauchen.",
    "unique_contribution": "Das Paper rekonzeptualisiert Distrust als rationale, intersektionale Widersetzung statt als kognitiver Fehler und schlägt grundsätzliche Reformulierungen von Fairness, Accountability und Transparency vor, die auf Care und Stewardship basieren.",
    "limitations": "Nicht empirisch; Aussagen zur Prävalenz von Distrust-Einstellungen sind ankündigt ('future work'). Konzeptionelle und literaturbasierte Analyse ohne empirische Verifikation der theoretischen Modelle."
  },
  "target_group": "KI-Ethiker:innen, Policymaker, Sozialarbeiter:innen, marginalisierte Communities, kritische Tech-Researcher:innen, Aktivist:innen, Fairness-ML Praktiker:innen, Care workers, Organisationen im Sozialsektor"
}