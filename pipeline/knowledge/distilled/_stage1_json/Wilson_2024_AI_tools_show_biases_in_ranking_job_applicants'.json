{
  "metadata": {
    "title": "Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval",
    "authors": [
      "Kyra Wilson",
      "Aylin Caliskan"
    ],
    "year": 2024,
    "type": "conferencePaper",
    "language": "en"
  },
  "core": {
    "research_question": "Zeigen Massive Text Embedding Models Bias bei der automatisierten Lebenslauf-Screening basierend auf geschlechtlichen und rassischen Signalen in Namen, und wie beeinflussen Dokumentlänge und Namenhäufigkeit diese Bias-Messungen?",
    "methodology": "Empirisch: Audit-Studie mit über 500 öffentlich verfügbaren Lebensläufen und 500 Stellenbeschreibungen über neun Berufe hinweg. Verwendung von 120 frequenzgesteuerten Namen für vier intersektionale Gruppen (Schwarze Männer, Schwarze Frauen, Weiße Männer, Weiße Frauen). Document-Retrieval-Framework mit Cosine-Similarity und Chi-Quadrat-Tests zur Bias-Detektion.",
    "key_finding": "Massive Text Embedding Models zeigen signifikante Bias: Weiße Namen werden in 85,1% der Fälle bevorzugt, während schwarze männliche Namen in bis zu 100% der Fälle benachteiligt werden. Dokument-Länge und Namenhäufigkeit beeinflussen die Bias-Messung erheblich.",
    "data_basis": "Über 500 Lebensläufe, über 500 Stellenbeschreibungen, 120 Frequenz-kontrollierte Namen, über 3 Millionen Vergleiche zwischen Lebensläufen und Stellenbeschreibungen, neun Berufsgruppen"
  },
  "arguments": [
    "Algorithmische Hiring-Tools, insbesondere LLMs und Massive Text Embedding Models, zeigen systematische Diskriminierungsmuster gegen Kandidaten mit schwarzen und weiblichen Namen, die reale Beschäftigungsdiskriminierung replizieren und verstärken.",
    "Intersektionale Analysen offenbaren, dass schwarze Männer die größte Benachteiligung erleben, während die Geschlechtsunterschiede hauptsächlich zwischen schwarzen Frauen und schwarzen Männern bestehen, nicht zwischen weißen Männern und Frauen.",
    "Textuelle Features wie Dokumentlänge und Namenhäufigkeit im Trainings-Corpus beeinflussen Bias-Messungen messbar: Kürzere Lebensläufe führen zu 22,2% mehr Bias-Fällen, und Frequenz-Matching-Strategien können bestimmen, ob schwarze oder weiße Namen bevorzugt werden."
  ],
  "categories": {
    "AI_Literacies": false,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": true,
    "Bias_Ungleichheit": true,
    "Gender": true,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "KI_Sonstige": "Fokus auf Massive Text Embedding Models als spezialisierte LLMs für Retrieval-Aufgaben: 'While many studies have characterized the biases of foundation or instruction-tuned LLMs, very few have investigated the biases of MTEs'",
    "Soziale_Arbeit": "Direkter Bezug zu Einstellungsprozessen und deren Auswirkungen auf Chancengleichheit und gesellschaftliche Ungleichheit: 'AI hiring tools have revolutionized resume screening... it is estimated that 99% of Fortune 500 companies are already using some sort of AI assistance'",
    "Bias_Ungleichheit": "Zentrale Fokussierung auf algorithmischen Bias und Diskriminierung: 'We find that the MTEs are biased, significantly favoring White-associated names in 85.1% of cases and female-associated names in only 11.1% of cases'",
    "Gender": "Explizite Gender-Analyse und Geschlechts-Bias: 'While male names were also favored compared to female names in the majority of experiments, the disparities were less than those demonstrated using Black versus White names'",
    "Diversitaet": "Intersektionale Analyse mehrerer marginalisierter Gruppen und deren Repräsentation: 'Intersectional comparisons reveal resumes that contain Black male names are highly unfavored in resume screening'",
    "Fairness": "Fokus auf Fairness in algorithmischen Systemen und Fairness-Metriken: 'We use a chi-square test to determine whether the selected resumes are distributed uniformly amongst relevant groups'"
  },
  "references": [
    {
      "author": "Bertrand and Mullainathan",
      "year": 2004,
      "short_title": "Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination"
    },
    {
      "author": "Dastin",
      "year": 2018,
      "short_title": "Amazon scraps secret AI recruiting tool that showed bias against women"
    },
    {
      "author": "Baert",
      "year": 2018,
      "short_title": "Hiring discrimination: An overview of (almost) all correspondence experiments since 2005"
    },
    {
      "author": "Pager",
      "year": 2003,
      "short_title": "The mark of a criminal record"
    },
    {
      "author": "Ghavami and Peplau",
      "year": 2013,
      "short_title": "An intersectional analysis of gender and ethnic stereotypes: Testing three hypotheses"
    },
    {
      "author": "Caliskan et al.",
      "year": 2022,
      "short_title": "Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics"
    },
    {
      "author": "Raghavan et al.",
      "year": 2020,
      "short_title": "Mitigating bias in algorithmic hiring: evaluating claims and practices"
    },
    {
      "author": "Elder and Hayes",
      "year": 2023,
      "short_title": "Signaling race, ethnicity, and gender with names: Challenges and recommendations"
    }
  ],
  "assessment": {
    "domain_fit": "Das Paper ist hochrelevant für die Schnittstelle KI und Soziale Arbeit, da es zeigt, wie algorithmische Systeme zur Stellenbesetzung marginalisierte Gruppen systematisch benachteiligen und damit zentrale Fragen von Gerechtigkeit und Chancengleichheit aufwirft, die direkt mit sozialer Gerechtigkeit und Empowerment in der Sozialen Arbeit verbunden sind.",
    "unique_contribution": "Erste umfassende Audit-Studie von Massive Text Embedding Models im Resume-Screening-Kontext mit fokussierter intersektionaler Analyse und Untersuchung von textualem Feature-Impact (Dokumentlänge, Namenhäufigkeit) auf Bias-Messungen.",
    "limitations": "Die Studie ist auf Englisch-Sprache, zwei Rassen- und zwei Geschlechtsidentitäten beschränkt und repräsentiert Rasse und Geschlecht nur durch Namen, was eine 'reductive and incomplete way of representing these facets of identity' ist."
  },
  "target_group": "KI-Entwickler, HR-Praktiker, Policy-Maker im Bereich Arbeitsmarktregulation, Forscher in AI Ethics und Fairness, Sozialarbeiter und Organisationen, die sich mit Chancengleichheit und Diskriminierungsschutz beschäftigen, sowie Arbeitnehmerschutzorganisationen"
}