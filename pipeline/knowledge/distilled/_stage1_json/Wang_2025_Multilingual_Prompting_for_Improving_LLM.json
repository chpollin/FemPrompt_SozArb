{
  "metadata": {
    "title": "Multilingual Prompting for Improving LLM Generation Diversity",
    "authors": [
      "Qihan Wang",
      "Shidong Pan",
      "Tal Linzen",
      "Emily Black"
    ],
    "year": 2025,
    "type": "conferencePaper",
    "language": "en"
  },
  "core": {
    "research_question": "Wie können Multilingual-Prompting-Techniken die kulturelle und demografische Diversität in den Ausgaben von Large Language Models erhöhen?",
    "methodology": "Empirisch: Experimentelle Evaluationen mit GPT-4o, GPT-4o-mini, LLaMA 70B und LLaMA 8B; Vergleich mehrerer Prompting-Strategien (Monolingual, High Temperature, Personas, Multilingual, Multicultural); menschliche Annotationsstudien (n=84 Annotatoren); Entropie-basierte Diversitätsmessung.",
    "key_finding": "Multilingual Prompting übertrifft bestehende Diversitäts-Techniken konsistent und erhöht demografische, kulturelle und perspektivische Vielfalt in LLM-Outputs, während die Genauigkeit bei faktischen Aufgaben erhalten bleibt.",
    "data_basis": "Evaluationen über mehrere Modelle; 105 berufsbezogene Prompts aus dem People Diversity Dataset; Social-Chem-101 Datensatz für Normen-Fragen; 420 Namen-Annotationen mit 3 Annotatoren pro Name; Sanity Check mit adversarialen Multiple-Choice-Fragen (9-10/10 Genauigkeit)"
  },
  "arguments": [
    "LLMs generieren kulturell homogene und monolinguale Antworten, die westliche oder dominierende Perspektiven überrepräsentieren und marginalisierte Gruppen systematisch vernachlässigen, was zu unfairer Exposition und unzureichender Repräsentation führt.",
    "Multilingual Prompting nutzt die language-spezifischen Wissensencodierungen in LLMs, indem es Prompts in mehreren Sprachen mit kulturellen Hinweisen generiert und die Responses kombiniert, wodurch ein breiteres Spektrum kulturellen Wissens aktiviert wird.",
    "Die Kombination von Sprache und kulturellen Hinweisen (Namen, Geburtsort, Essen) ist effektiver als nur kulturelle Hinweise auf Englisch, da die native Sprache Halluzinationen über kulturell spezifische Informationen reduziert und Diversität erhöht."
  ],
  "categories": {
    "AI_Literacies": false,
    "Generative_KI": true,
    "Prompting": true,
    "KI_Sonstige": false,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "Generative_KI": "Das Paper adressiert Large Language Models (GPT-4o, GPT-4o-mini, LLaMA 70B, 8B) und deren Fähigkeit zur Generierung diverser Inhalte.",
    "Prompting": "Zentrale Methodik: 'multilingual prompting - a prompting method which generates several variations of a base prompt with added cultural and linguistic cues from several cultures' sowie Vergleich mit anderen Prompting-Techniken wie 'step-by-step recall prompting', 'personas prompting', und High Temperature Sampling.",
    "Bias_Ungleichheit": "Das Paper thematisiert explizit: 'lack of demographic diversity when queried about individuals can lead to unfair lack of exposure of artists, academics, and other professionals on the basis of their race, ethnicity, or nationality' und 'LLMs generate largely monocultural responses...often leaning towards expressing Western values'.",
    "Diversitaet": "Primärer Fokus auf 'demographic diversity', 'cultural diversity', 'perspective diversity', und 'overall diversity in LLM generations'. Messung durch Entropie über 30 generierte Namen, mit Annotation nach Nationalität, Ethnizität und geografischer Region.",
    "Fairness": "Das Paper argumentiert für 'equitable manner' der Informationsexposition durch LLMs und untersucht, ob die Outputs 'reflect the diversity of real-world perspectives' sowie die Reduktion von Halluzinationen bei kulturell spezifischen Informationen."
  },
  "references": [
    {
      "author": "Aggarwal et al.",
      "year": 2025,
      "short_title": "Language models' factuality depends on the language of inquiry"
    },
    {
      "author": "Santurkar et al.",
      "year": 2023,
      "short_title": "Whose opinions do language models reflect?"
    },
    {
      "author": "Wang et al.",
      "year": 2025,
      "short_title": "Large language models that replace human participants can harmfully misportray and flatten identity groups"
    },
    {
      "author": "Wu et al.",
      "year": 2024,
      "short_title": "Generative monoculture in large language models"
    },
    {
      "author": "Bommasani et al.",
      "year": 2022,
      "short_title": "Picking on the same person: Does algorithmic monoculture lead to outcome homogenization?"
    },
    {
      "author": "Kwok et al.",
      "year": 2024,
      "short_title": "Evaluating cultural adaptability of a large language model via simulation of synthetic personas"
    },
    {
      "author": "Padmakumar & He",
      "year": 2023,
      "short_title": "Does writing with language models reduce content diversity?"
    },
    {
      "author": "Forbes et al.",
      "year": 2020,
      "short_title": "Social Chemistry 101: Learning to reason about social and moral norms"
    },
    {
      "author": "Hayati et al.",
      "year": 2023,
      "short_title": "How far can we extract diverse perspectives from large language models?"
    },
    {
      "author": "Lahoti et al.",
      "year": 2023,
      "short_title": "Improving diversity of demographic representation in large language models via collective-critiques and self-voting"
    }
  ],
  "assessment": {
    "domain_fit": "Das Paper adressiert die Schnittstelle zwischen KI-Systemen und sozialer Gerechtigkeit durch die Frage, wie generative KI-Modelle unterschiedliche kulturelle und demografische Gruppen repräsentieren. Während nicht explizit auf Soziale Arbeit bezogen, ist die Problematik der fairen und diversitären Informationsbereitstellung für Felder wie Beratung, Advocacy und Community-Arbeit relevant.",
    "unique_contribution": "Die Erkenntnis, dass Multilingual Prompting—die gezielte Kombination von Prompts in mehreren Sprachen mit kulturellen Hinweisen—konsistent wirksamer ist als bestehende Diversitäts-Techniken, liefert eine praktische und skalierbare Methode zur Mitigation von kulturellem Bias in LLM-Outputs.",
    "limitations": "Das Paper ist auf drei Sprachen (Englisch, Chinesisch, Japanisch) beschränkt und zeigt Probleme mit Instruction-Following in niedrig-ressourcenstarken Sprachen und ungleiche Resultate über Modellgrößen hinweg; der Evaluationsfokus liegt primär auf demografischer Repräsentation von Namen, nicht auf Inhaltsqualität oder echte kulturelle Authentizität."
  },
  "target_group": "KI-Entwickler und Modellierer (besonders bei Anwendung von LLMs), Fairness- und Ethik-Forscher, Content-Moderator:innen, Organisationen die LLMs für User-Studies oder Annotation einsetzen, Policy-Maker im Bereich AI Governance, potenziell auch Sozialarbeiter:innen die LLMs in der Praxis einsetzen"
}