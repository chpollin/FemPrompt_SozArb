{
  "metadata": {
    "title": "AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?",
    "authors": [
      "Armin Grunwald",
      "Daniel Schneider",
      "Karsten Weber",
      "Daniel Minkin",
      "Lou Therese Brandner",
      "Manuela Marquardt",
      "Philipp Graf",
      "Eva Jansen",
      "Stefan Hillmann",
      "Jan-Niklas Voigt-Antons"
    ],
    "year": 2024,
    "type": "journalArticle",
    "language": "de"
  },
  "core": {
    "research_question": "Welche sozialen Auswirkungen, ethischen Herausforderungen und Regulierungsoptionen entstehen durch den Einsatz von KI-Systemen zur Unterstützung von Entscheidungen in verschiedenen gesellschaftlichen Bereichen?",
    "methodology": "Mixed Methods: Qualitative Interviewstudien mit szenariobasierten Ansätzen, Literaturanalyse, Technikfolgenabschätzung (TA), transdisziplinäre Perspektive",
    "key_finding": "Die Erklärbarkeit von KI-Systemen ist zentral für deren legitime Verwendung, muss aber situativ ausgestaltet werden und darf professionelle Rollen nicht infrage stellen. Vertrauen wird durch funktionale Legitimation als Zweitmeinung aufgebaut, nicht primär durch technische Transparenz.",
    "data_basis": "Szenariobasierte Interviews mit Akteuren des Gesundheitswesens (n nicht explizit angegeben); Analyse von Gerichtsentscheidungen und Grenzpolizeiverfahren; qualitative Auswertung zu medizinischen Szenarien (Diagnostik und Dokumentation)"
  },
  "arguments": [
    "KI-Systeme zur Entscheidungsunterstützung erfordern spezifische Formen der Erklärbarkeit, die kontextabhängig und adressatinnengerecht gestaltet werden müssen, nicht nur technisch transparent sein müssen.",
    "Automatisierte Entscheidungssysteme an den EU-Grenzen (ADDS) perpetuieren und verstärken bereits bestehende Diskriminierungsdynamiken, insbesondere gegenüber Migrant*innen, durch die Kodifikation von Bias in Algorithmen.",
    "Vertrauen in KI-Systeme wird durch ihre funktionale Rolle als Unterstützungsinstrument (Zweitmeinung) legitimiert; virtuelle Verkörperung kann sprachbasierte Erklärungen verbessern und die Akzeptanz erhöhen."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Zentrale Anforderung ist 'Erklärbarkeit' von KI-Systemen als 'Bereitstellung von adressat*innengerechten Informationen über ihre Funktionsweise' und der Aufbau von Vertrauen durch Transparenz in Entscheidungsprozessen.",
    "KI_Sonstige": "Fokus auf AI-based decision support systems in Medizin, Justiz und Grenzpolizei; algorithmische Entscheidungssysteme, Machine Learning, Gesichtserkennung, Deception Detection Systeme.",
    "Bias_Ungleichheit": "ADDS-Systeme an EU-Grenzen 'continue inequalities and discriminatory dynamics but, by automating them, embed them further into the social fabric'; Diskriminierung in algorithmischen Systemen wird analysiert als strukturelle Benachteiligung von Migrant*innen und vulnerablen Gruppen.",
    "Diversitaet": "Analyse verschiedener Stakeholder-Perspektiven (Richter*innen, Patient*innen, Grenzbeamte, Ärzt*innen); marginalisierte Communities (Migrant*innen) als besonders von automatisierten Entscheidungssystemen betroffen.",
    "Fairness": "Analyse von Fairness in justiziellen Entscheidungssystemen, Debiasing-Potential von KI, Algorithmic Fairness in Policing und Grenzkontrollen; kritische Reflexion über Fairness-Metriken und deren Grenzen in real-world Anwendungen."
  },
  "references": [
    {
      "author": "Ammicht Quinn, Regina",
      "year": 2015,
      "short_title": "Trust generating security generating trust"
    },
    {
      "author": "Bacchini, Fabio & Lorusso, Ludovica",
      "year": 2019,
      "short_title": "Race, again. How face recognition technology reinforces racial discrimination"
    },
    {
      "author": "Feldman Barrett, Lisa et al.",
      "year": 2019,
      "short_title": "Emotional expressions reconsidered. Challenges to inferring emotion from human facial movements"
    },
    {
      "author": "Gillespie, Nicole et al.",
      "year": 2023,
      "short_title": "Trust in artificial intelligence. A global study"
    },
    {
      "author": "Hobson, Zoë et al.",
      "year": 2023,
      "short_title": "Artificial fairness? Trust in algorithmic police decision-making"
    },
    {
      "author": "Sánchez-Monedero, Javier & Dencik, Lina",
      "year": 2022,
      "short_title": "The politics of deceptive borders. 'Biomarkers of deceit' and the case of iBorderCtrl"
    },
    {
      "author": "Selbst, Andrew",
      "year": 2017,
      "short_title": "Disparate impact in big data policing"
    },
    {
      "author": "Starke, Christoph et al.",
      "year": 2022,
      "short_title": "Fairness perceptions of algorithmic decision-making. A systematic review"
    }
  ],
  "assessment": {
    "domain_fit": "Das Special Topic hat hohe Relevanz für die Schnittstelle KI und Gesellschaft, adressiert aber primär Technikfolgenabschätzung und ethische Fragen von Entscheidungsunterstützungssystemen, nicht spezifisch für Soziale Arbeit. Allerdings relevant für soziale Auswirkungen algorithmischer Systeme auf vulnerable Gruppen.",
    "unique_contribution": "Die transdisziplinäre und szenariobasierte Herangehensweise kombiniert technische, ethische und gesellschaftliche Perspektiven auf KI-Entscheidungssysteme und betont die Bedeutung von Kontextualität und Situativität gegenüber universalen Lösungsansätzen.",
    "limitations": "Geringe direkte Verbindung zu Sozialer Arbeit als Disziplin; Fallstudien konzentrieren sich auf Medizin, Justiz und Grenzkontrolle; Geschlechterperspektive wird nicht explizit behandelt; Gender-spezifische Auswirkungen algorithmischer Systeme nicht analysiert."
  },
  "target_group": "Technikfolgenabschätzung-Forschende, Policy-Maker, Ethiker*innen im KI-Kontext, Praktiker*innen in Justiz und Medizin, Organisationen der Grenzpolizei, allgemein interessierte transdisziplinäre Wissenschaftler*innen und Stakeholder in KI-Governance."
}