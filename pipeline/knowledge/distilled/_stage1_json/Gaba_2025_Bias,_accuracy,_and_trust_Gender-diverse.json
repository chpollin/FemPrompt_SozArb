{
  "metadata": {
    "title": "Bias, Accuracy, and Trust: Gender-Diverse Perspectives on Large Language Models",
    "authors": [
      "Aimen Gaba",
      "Emily Wall",
      "Tejas Ramkumar Babu",
      "Yuriy Brun",
      "Kyle Wm. Hall",
      "Cindy Xiong Bearfield"
    ],
    "year": 2025,
    "type": "conferencePaper",
    "language": "en"
  },
  "core": {
    "research_question": "Wie nehmen gender-diverse Populationen (cisgender Männer, cisgender Frauen, non-binär/transgender Personen) Bias, Genauigkeit und Vertrauenswürdigkeit in großen Sprachmodellen wahr?",
    "methodology": "Mixed Methods: Empirisch. 25 semi-strukturierte Interviews mit gender-diversen Teilnehmern kombiniert mit quantitativen Vertrauensmessungen. Analyse von ChatGPT-Antworten auf gendered und neutrale Prompts.",
    "key_finding": "Gendered Prompts führen zu identitätsspezifischeren Reaktionen; non-binäre Teilnehmende berichten von herablassenden und stereotypischen Darstellungen, während Männer höheres Vertrauen zeigen und Frauen traditionelle emotionale Stereotypen kritisieren.",
    "data_basis": "n=25 Interviews (non-binär/transgender, cisgender männlich, cisgender weiblich)"
  },
  "arguments": [
    "Gendered Prompts elicitieren reichhaltigere, identitätsspezifischere Reaktionen von ChatGPT, während neutrale Prompts zu generischeren Antworten führen, wobei das Modell trotzdem Geschlechter zuweist.",
    "Non-binäre Teilnehmende sind besonders anfällig für cis-zentrische, herablassende und stereotypische Darstellungen ihrer Identität, die sie als reduktionistisch und depersonalisierend erleben.",
    "Die wahrgenommene Genauigkeit variiert weniger nach Geschlecht, sondern hängt stark ab von Vertrautheit mit dem Thema und der Art der Aufgabe (technisch vs. kreativ), während Vertrauenswürdigkeit signifikant nach Geschlecht variiert, mit höherem Vertrauen bei Männern."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": true,
    "Prompting": true,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": true,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Die Studie untersucht, wie Nutzer KI-Systeme interpretieren und evaluieren: 'we study the perceived utility of a real-world LLM application by understanding users' perception of bias, accuracy, and trust'",
    "Generative_KI": "Fokus auf ChatGPT als Large Language Model: 'we selected ChatGPT for two key reasons. First, its widespread use and accessibility allow us to study how biases manifest in real-world LLM applications'",
    "Prompting": "Systematische Analyse von gendered und neutralen Prompts: 'we studied how gendered and neutral prompts (e.g., 'man', 'woman', 'nonbinary', 'person') influence the responses generated by ChatGPT'",
    "KI_Sonstige": "Kontext in NLP und algorithmischen Systemen: 'Recent studies have shown that they can exhibit biases and other social risks against particular religious groups, produce gender stereotypes, and generate stigmatizing language'",
    "Bias_Ungleichheit": "Zentrale Fokus auf algorithmischen Bias: 'This study examines how gender-diverse populations perceive bias, accuracy, and trustworthiness in LLMs' und 'numerous concerns about bias in LLMs exist'",
    "Gender": "Expliziter Geschlechtsfokus durchgehend: 'non-binary participants particularly susceptible to condescending and stereotypical portrayals' und 'Trustworthiness varied by gender, with men showing higher trust'",
    "Diversitaet": "Repräsentation marginalisierter Gruppen: 'we study men, women, and non-binary/transgender participants, revealing varying reactions to bias' und 'the need for gender-diverse perspectives in LLM development'",
    "Fairness": "Algorithmische Fairness und faire KI-Systeme: 'To foster more inclusive and trustworthy systems' und Diskussion von gleichmäßiger Darstellung: 'ensuring equal depth in gendered responses'"
  },
  "references": [
    {
      "author": "Scheuerman et al.",
      "year": 2019,
      "short_title": "How Computers See Gender: An Evaluation of Gender Classification in Commercial Facial Analysis Services"
    },
    {
      "author": "Ghosh & Caliskan",
      "year": 2023,
      "short_title": "ChatGPT perpetuates gender bias in machine translation and ignores non-gendered pronouns"
    },
    {
      "author": "Nozza et al.",
      "year": 2022,
      "short_title": "Measuring Harmful Sentence Completion in Language Models for LGBTQIA+ Individuals"
    },
    {
      "author": "Leavy",
      "year": 2018,
      "short_title": "Gender bias in artificial intelligence: the need for diversity and gender theory in machine learning"
    },
    {
      "author": "Holstein & Wortman Vaughan",
      "year": 2023,
      "short_title": "Disclosure and Mitigation of Gender Bias in LLMs"
    },
    {
      "author": "Lee, Montgomery & Lai",
      "year": 2024,
      "short_title": "Large Language Models Portray Socially Subordinate Groups as More Homogeneous"
    },
    {
      "author": "Bartl & Leavy",
      "year": 2023,
      "short_title": "Gender-inclusive dataset development for bias mitigation in LLMs"
    },
    {
      "author": "Park, Shin & Fung",
      "year": 2018,
      "short_title": "Reducing gender bias in abusive language detection"
    },
    {
      "author": "Weidinger et al.",
      "year": 2021,
      "short_title": "Ethical and social risks of harm from language models"
    },
    {
      "author": "Mehrabi et al.",
      "year": 2021,
      "short_title": "A survey on bias and fairness in machine learning"
    }
  ],
  "assessment": {
    "domain_fit": "Hochrelevant für die Schnittstelle KI/Gender/Inklusion. Das Paper adressiert direkt, wie marginalisierte Geschlechtsidentitäten KI-Systeme erleben und welche Implikationen dies für faire, vertrauenswürdige KI-Entwicklung hat. Für Soziale Arbeit relevant insofern als vulnerable Gruppen betroffen sind.",
    "unique_contribution": "Das Paper leistet einen wichtigen Beitrag durch die systematische Analyse, wie non-binäre und transgender Personen spezifische Formen von Bias und Stereotypisierung in LLMs wahrnehmen, und kombiniert qualitative tiefe Interviews mit quantitativen Vertrauensmessungen.",
    "limitations": "Sample mit n=25 ist klein für Generalisierungen; Fokus nur auf ChatGPT; keine intersektionalen Analysen mit anderen Diskriminierungsdimensionen (Race, Klasse, Disability); keine Längsschnittdaten zur Vertrauensentwicklung."
  },
  "target_group": "AI-Entwickler und Designerinnen (insbesondere bei LLM-Entwicklung), HCI/CSCW-Forschende, Fairness-Spezialistinnen, Policy-Maker im AI-Governance, LGBTQ+-Interessenvertreter, Akteure in der kritischen Algorithmen-Forschung"
}