{
  "metadata": {
    "title": "AI & Intersectionality: A Toolkit for Fairness & Inclusion for the Industry Sector",
    "authors": [
      "DIVERSIFAIR Project Team",
      "CorTexter",
      "Eticas",
      "Sciences Po",
      "TNO",
      "Turing College",
      "University College Dublin",
      "Women4Cyber",
      "Women in AI"
    ],
    "year": 2024,
    "type": "report",
    "language": "en"
  },
  "core": {
    "research_question": "Wie können Organisationen intersektionale Vorurteile in KI-Systemen identifizieren und abbauen, um faire und inklusive Technologien zu entwickeln?",
    "methodology": "Mixed-Methods Review: Literaturanalyse, Stakeholder-Interviews (KI-Community und Policy-Sektor), Fokusgruppen, Best-Practice-Sammlung, Fallstudienanalyse, Toolkit-Entwicklung",
    "key_finding": "Intersektionale Vorurteile in KI entstehen durch Überlagerung mehrerer Diskriminierungsformen und erfordern ganzheitliche, disziplinübergreifende Strategien zur Mitigation, die über isolierte Bias-Ansätze hinausgehen.",
    "data_basis": "Nicht spezifiziert; basierend auf Interviews und Fokusgruppen mit AI-Community und Policy-Experten, Literaturanalyse von Fallstudien und Forschungsergebnissen"
  },
  "arguments": [
    "Intersektionale Vorurteile in KI sind komplexer als eindimensionale Bias-Ansätze: Sie entstehen durch Überlagerung von Rassismus, Sexismus, Ableismus und Kolonialismus und beeinflussen Menschen mit multiplen marginalisierten Identitäten in spezifischen Weisen (z.B. Black women in Gesichtserkennung oder immigrant families in Wohlfahrtssystemen).",
    "Technische Lösungen allein sind unzureichend; notwendig sind ganzheitliche Strategien, die Datenpraxis, Teamdiversität, Transparenz, Disziplinübergreifende Zusammenarbeit und kritische Reflexion der Systemexistenz einbeziehen.",
    "Organisationen (Industrie, Public Sector, Governance) müssen konkrete, rollenspezifische Maßnahmen implementieren: Entwickler müssen intersektional designen, Führungskräfte müssen Fairness budgetieren, Governance-Teams müssen Accountability etablieren, HR muss diverse Teams aufbauen und AI-Literacy fördern."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": true,
    "Diversitaet": true,
    "Feministisch": true,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Build awareness and AI literacy: 'Provide AI literacy training across teams, focusing on ethics, intersectionality, and societal impact. Debunk AI myths, such as the notion of AI neutrality, through internal workshops and communication.' (S. 18-19)",
    "KI_Sonstige": "Fokus auf klassische ML-Systeme und algorithmische Entscheidungssysteme: Predictive Policing, Healthcare Algorithms, Facial Recognition, Hiring Algorithms, Credit Scoring, Welfare Fraud Detection",
    "Bias_Ungleichheit": "Intersectional bias in AI describes 'the AI harms as experienced by people due to multiple intersecting and often marginalised parts of their identity.' Beispiele: Dutch childcare scandal (immigrant families), predictive policing (low-income communities of colour), healthcare disparities (uninsured/underinsured populations)",
    "Gender": "Gender bias dokumentiert in Amazon Recruiting Tool, Apple Credit Card, Austrian Unemployment Agency; Frauen in Tech-Rollen unterrepräsentiert: '21% of leaders are women, 4% are women of colour, 1% are Black women' (S. 9)",
    "Diversitaet": "Zentral: 'Embed inclusivity and cultural sensitivity: Prioritise localised solutions tailored to specific cultural or regional needs. Plan for marginal use cases, allocating resources to support the most vulnerable groups.' Fallstudien zu marginalisierten Gruppen: Migranten, Racial minorities, disabled persons, single-parent households",
    "Feministisch": "Explizit feministische Perspektive: Referenzen auf Kimberlé Crenshaw (Intersectionality Begriffsschöpferin), Suresh et al. 'Towards Intersectional Feminist and Participatory ML' (2022), D'Ignazio & Bhargava zu Data Justice, Partizipatorischer ML-Ansatz, Community-basierte Ansätze zur Gegendatensammlung (Feminicide Counterdata Collection)",
    "Fairness": "Fairness definiert als: 'designing systems that promote equitable outcomes for all individuals, regardless of identity.' Kritik an single-axis Fairness-Ansätzen: 'Many approaches to AI fairness focus on addressing just one type of bias at a time, such as gender or race. However, this approach ignores the complex ways biases overlap' (S. 8)"
  },
  "references": [
    {
      "author": "Crenshaw, Kimberlé",
      "year": 1989,
      "short_title": "Demarginalizing the Intersection of Race and Sex (Foundational intersectionality concept)"
    },
    {
      "author": "Buolamwini & Gebru",
      "year": 2018,
      "short_title": "Gender Shades: Intersectional Accuracy Disparities in AI"
    },
    {
      "author": "Suresh et al.",
      "year": 2022,
      "short_title": "Towards Intersectional Feminist and Participatory ML: Case Study in Feminicide Counterdata Collection"
    },
    {
      "author": "D'Ignazio & Klein",
      "year": 2020,
      "short_title": "Data Feminism"
    },
    {
      "author": "Howard, Ayanna",
      "year": 2021,
      "short_title": "Real Talk: Intersectionality and AI"
    },
    {
      "author": "Ulnicane, Inga",
      "year": 2024,
      "short_title": "Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action"
    },
    {
      "author": "Amnesty International",
      "year": 2021,
      "short_title": "Xenophobic Machines: The Dutch Child Benefit Scandal"
    },
    {
      "author": "Eticas Foundation",
      "year": 2024,
      "short_title": "Automating (In)Justice: An Adversarial Audit of RisCanvi"
    },
    {
      "author": "Dastin, Jeffrey",
      "year": 2018,
      "short_title": "Amazon scraps secret AI recruiting tool that showed bias against women"
    },
    {
      "author": "UNESCO",
      "year": 2020,
      "short_title": "Artificial Intelligence and Gender Equality"
    }
  ],
  "assessment": {
    "domain_fit": "Hochgradig relevant für Schnittstelle KI/Fairness/Intersektionalität: Das Toolkit adressiert explizit, wie algorithmische Systeme marginalisierte Gruppen in ihrer Mehrfachvulnerabilität treffen. Während es nicht direkt auf Soziale Arbeit fokussiert, sind deren Zielgruppen (vulnerable Populationen, Migrant:innen, Familien in Wohlfahrtssystemen) zentrale Fallbeispiele.",
    "unique_contribution": "Das Toolkit verbindet intersektionale Theorie (Crenshaw) mit KI-Praxis durch konkrete, rollenspezifische Strategien für verschiedene Organisationstypen (Entwickler, Führung, Governance, HR) und illustriert Konzepte mit EU-basierten Fallstudien.",
    "limitations": "Nicht angegeben; Document ist Toolkit und nicht Forschungspaper mit expliziten Methodenlimitationen. Datengrundlage der Interviews/Fokusgruppen (n, Sampling, Analyse) ist nicht dokumentiert."
  },
  "target_group": "Primär: Industry Professionals, AI Developers, Executives, Governance Teams, HR Professionals in Unternehmen. Sekundär: Policymakers, Public Sector Leaders, Civil Society Organisations. Tertär: Alle Stakeholder mit Interesse an fairer KI und Intersektionalität (Lehrende, Nutzer:innen von KI-Systemen)"
}