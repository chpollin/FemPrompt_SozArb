{
  "metadata": {
    "title": "Feminist Perspectives on AI: Ethical Considerations in Algorithmic Decision-Making",
    "authors": [
      "Uzair Ahmed"
    ],
    "year": 2024,
    "type": "journalArticle",
    "language": "en"
  },
  "core": {
    "research_question": "Wie können feministische ethische Rahmenwerke dazu beitragen, algorithmische Verzerrungen in KI-Entscheidungssystemen zu identifizieren und zu mindern?",
    "methodology": "Mixed-Methods: Quantitative statistische Analyse (SPSS Regressionsanalyse, ANOVA, t-Tests) von AI-Hiring-Daten und Facial-Recognition-Systemen kombiniert mit qualitativen Surveys unter KI-Praktikern und Policymakers, sowie feministische partizipative Forschungsprinzipien (Fokusgruppen, Expert:inneninterviews)",
    "key_finding": "Empirische Analyse zeigt signifikante geschlechtsspezifische und rassifizierte Verzerrungen in KI-Systemen: männliche Kandidaten erhalten höhere Hiring-Scores als gleich qualifizierte Frauen (p=0.001), und Black Women erleben 34,5% Fehlerrate in Facial-Recognition-Systemen gegenüber 1,2% bei White Men. Allerdings zeigt sich eine Lücke zwischen Awareness für Fairness-Probleme und deren praktischer Implementierung.",
    "data_basis": "n=125 Surveys unter AI-Praktikern und Policymakers; Regressionsanalyse von AI-Hiring-Algorithmen; vergleichende Fehlerquoten-Analyse von Facial-Recognition-Systemen; qualitative Thematic Analysis von Surveys und Expert:inneninterviews"
  },
  "arguments": [
    "KI-Systeme sind nicht objektiv oder neutral, sondern eingebettet in gesellschaftliche Machtstrukturen, die Frauen und marginalisierte Gruppen historisch benachteiligen. Algorithmische Verzerrungen entstehen durch historisch verzerrte Trainingsdaten, fehlende Repräsentation von Frauen in AI-Entwicklungsteams und kapitalistische Logiken, die Profit über soziale Gerechtigkeit priorisieren.",
    "Feministische ethische Rahmenwerke betonen Transparenz, Fairness und Inklusion als zentrale Lösungsansätze. Dies erfordert über technische Fixes hinausgehende Maßnahmen: Explainable AI (XAI), partizipatives Design mit marginaliisierten Gemeinschaften, kontinuierliche Bias-Audits und interdisziplinäre Zusammenarbeit zwischen Gender Studies, Soziologie und kritischer Data Science.",
    "Strukturelle Machtungleichgewichte im AI-Sektor müssen adressiert werden durch diversere Repräsentation in Entwicklungsteams, demokratischere Governance-Modelle und regulatorische Maßnahmen, die Transparenz und Rechenschaftspflicht erzwingen. Partizipatorisches Design mit von Algorithmen betroffenen Gemeinschaften ist essentiell für ethische AI-Entwicklung."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": true,
    "Bias_Ungleichheit": true,
    "Gender": true,
    "Diversitaet": true,
    "Feministisch": true,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Kritische Reflexion über KI-Systeme und deren Design: 'AI is not an objective or neutral technology; rather, it is embedded within societal power structures' und Forderung nach interdisziplinärer Zusammenarbeit in AI-Governance und Bildung.",
    "KI_Sonstige": "Fokus auf AI-driven Entscheidungssysteme, speziell Hiring-Algorithmen, Facial-Recognition-Technologien und automatisierte Klassifizierungssysteme, sowie deren technische und ethische Aspekte wie 'black box' Problem.",
    "Soziale_Arbeit": "Direkter Bezug zu sozialarbeiterischen Zielgruppen und Gerechtigkeitsprinzipien: 'Feminist ethics emphasizes transparency, fairness, and inclusivity, challenging the patriarchal and corporate-driven narratives' und Fokus auf marginalisierte Gemeinschaften, Menschen in Armut und betroffene Populationen.",
    "Bias_Ungleichheit": "Zentrale Analyse von algorithmischem Bias und strukturellen Ungleichheiten: 'Algorithmic bias disproportionately affects marginalized groups, reinforcing societal inequalities in areas such as hiring, healthcare, and law enforcement' mit empirischer Evidenz (Regression p=0.001 für Gender-Bias, ANOVA für Facial-Recognition).",
    "Gender": "Expliziter Gender-Fokus durchgehend: 'AI-driven hiring tools have been found to discriminate against female candidates' und 'facial recognition technologies have demonstrated racial and gender biases, leading to higher error rates for women' sowie spezifische Analyse von Gender-Representation in AI-Teams.",
    "Diversitaet": "Starker Fokus auf Diversität und Inklusion: 'lack of diversity in AI training data' und 'underrepresentation of women and marginalized groups in AI development teams' mit Forderung nach partizipatorischem Design mit 'diverse stakeholders, including women, non-binary individuals, and marginalized communities'.",
    "Feministisch": "Explizite Verwendung feministischer Theorie und Methodik: 'Feminist perspectives provide a critical lens' und Referenzen auf D'Ignazio & Klein (Data Feminism), Benjamin (Race After Technology), feministische partizipative Forschungsprinzipien und intersektionaler Feminismus. Der gesamte theoretische Rahmen basiert auf feministischer Ethik und Epistemologie.",
    "Fairness": "Algorithmic Fairness zentral: Regressionsanalyse zeigt Gender-Bias in Hiring (Koeffizient 0.45, p=0.001), ANOVA-Test vergleicht Fehlerquoten in Facial-Recognition über demografische Gruppen (White Men 1,2%, Black Women 34,5%), und Surveys messen Fairness-Awareness. Forderungen nach Fairness-Metriken, Bias-Audits und XAI-Frameworks."
  },
  "references": [
    {
      "author": "Buolamwini & Gebru",
      "year": 2018,
      "short_title": "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification"
    },
    {
      "author": "D'Ignazio & Klein",
      "year": 2020,
      "short_title": "Data Feminism"
    },
    {
      "author": "Crawford",
      "year": 2021,
      "short_title": "Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence"
    },
    {
      "author": "Benjamin",
      "year": 2019,
      "short_title": "Race After Technology: Abolitionist Tools for the New Jim Code"
    },
    {
      "author": "Noble",
      "year": 2018,
      "short_title": "Algorithms of Oppression: How Search Engines Reinforce Racism"
    },
    {
      "author": "O'Neil",
      "year": 2016,
      "short_title": "Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy"
    },
    {
      "author": "Criado-Perez",
      "year": 2019,
      "short_title": "Invisible Women: Data Bias in a World Designed for Men"
    },
    {
      "author": "Eubanks",
      "year": 2018,
      "short_title": "Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor"
    },
    {
      "author": "West, Whittaker & Crawford",
      "year": 2019,
      "short_title": "Discriminating Systems: Gender, Race, and Power in AI"
    },
    {
      "author": "Pasquale",
      "year": 2015,
      "short_title": "The Black Box Society: The Secret Algorithms That Control Money and Information"
    }
  ],
  "assessment": {
    "domain_fit": "Das Paper ist hochgradig relevant für die Schnittstelle KI/Soziale Arbeit/Gender: Es verbindet algorithmische Gerechtigkeit mit feministischen Ethikprinzipien und betont die Auswirkungen auf marginalisierte Gemeinschaften, die zentrale Zielgruppen Sozialer Arbeit sind. Die Forderung nach partizipatorischem Design und Transparenz spricht direkt professionelle Standards der Sozialen Arbeit an.",
    "unique_contribution": "Das Paper liefert eine seltene empirische Mixed-Methods-Kombination von SPSS-basierten Analysen algorithmischer Bias mit explizit feministischen theoretischen und methodischen Rahmenwerken, was über reine technische Fairness-Ansätze hinausgeht und strukturelle Machtverhältnisse in den Mittelpunkt stellt.",
    "limitations": "Das Paper basiert auf selbstberichteten Survey-Daten (n=125) ohne Angabe der Stichprobenziehungsmethode oder Response-Rate; die Fallstudien zu Bias (Amazon Hiring, Facial Recognition) sind sekundäre Analyseergebnisse, nicht originäre Datenerhebung des Autors; regionale und kulturelle Kontextspezifität (Pakistan-basierte Institution) wird nicht reflektiert."
  },
  "target_group": "Multiprofessionelle Zielgruppe: KI-Entwickler:innen und -Ethiker:innen (für kritische Reflexion über Designprozesse), Sozialarbeiter:innen und Care-Professionelle (zur Sensibilisierung für algorithmische Diskriminierung ihrer Klient:innen), Policymakers und Governance-Verantwortliche (für Regulierungsempfehlungen), Gender Studies und Critical Data Science Forscher:innen, sowie Organisationen mit Zugang zu marginalisierter Populations (für partizipatives Mitdesign)."
}