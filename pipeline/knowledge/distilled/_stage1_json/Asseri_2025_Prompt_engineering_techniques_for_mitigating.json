{
  "metadata": {
    "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review",
    "authors": [
      "Bushra Asseri",
      "Estabrag Abdelaziz",
      "Areej Al-Wabil"
    ],
    "year": 2025,
    "type": "report",
    "language": "en"
  },
  "core": {
    "research_question": "Welche Prompt-Engineering-Techniken sind wirksam bei der Minderung kultureller Vorurteile gegen Araber und Muslime in Large Language Models?",
    "methodology": "Mixed-Methods Systematic Review; PRISMA-Richtlinien und Kitchenham's Systematic Review Methodik; Datenbanksuche (IEEE Xplore, ACM Digital Library, Scopus, Web of Science) 2021-2024; Covidence für Referenzmanagement; Litmaps für Citation Tracking",
    "key_finding": "Fünf primäre Prompt-Engineering-Ansätze identifiziert (kulturelles Prompting, affektive Priming, Self-Debiasing, strukturierte Multi-Step-Pipelines, parameteroptimierte kontinuierliche Prompts); strukturierte Multi-Step-Pipelines zeigen höchste Effektivität (bis 87,7% Bias-Reduktion), während kulturelles Prompting beste Zugänglichkeit bietet (71-81% Verbesserung). Kritische Forschungslücke: Nur 8 empirische Studien identifiziert trotz umfassender Suchstrategie.",
    "data_basis": "8 empirische Studien (2021-2024) in systematischer Analyse synthetisiert; keine primäre Datenerhebung"
  },
  "arguments": [
    "LLMs als 'stochastische Papageien' (Bender et al. 2021) perpetuieren Orientalismus (Said 2019) und die in Trainingsdaten eingebetteten westlich-zentristischen Darstellungen von Arabern und Muslimen als 'Andere', was koloniale Machtstrukturen in digitale Systeme überträgt.",
    "Prompt Engineering bietet praktisch zugängliche Bias-Mitigation ohne Modellparameter-Zugriff und ermöglicht schnelle Anpassungen an sich ändernde geopolitische Kontexte (Meade et al. 2022; Gallegos et al. 2024), muss aber durch relationale Ethik und Adressierung struktureller Ungleichheiten ergänzt werden (Birhane 2021).",
    "Effektivität von Prompt-Engineering variiert stark nach Bias-Typ: Kulturelles Prompting am wirksamsten bei oberflächlichen Stereotypen (76-82% Reduktion) aber weniger wirksam bei religiösen Vorurteilen (34% Reduktion) und historischen Fehldarstellungen (41% Reduktion), was die Notwendigkeit mehrschichtiger Interventionen unterstreicht."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": true,
    "Prompting": true,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Schwerpunkt auf Kompetenzentwicklung für Nutzer und Praktiker: 'providing evidence-based guidance for researchers and practitioners' und 'accessibility of prompt engineering for mitigating cultural bias without requiring access to model parameters'",
    "Generative_KI": "Expliziter Fokus auf Large Language Models: 'Large language models (LLMs) have demonstrated remarkable capabilities across various domains, yet concerns about cultural bias—particularly towards Arabs and Muslims—pose significant ethical challenges'",
    "Prompting": "Kernfokus auf fünf Prompt-Engineering-Techniken: 'cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts'",
    "KI_Sonstige": "Natural Language Processing und transformer architectures: 'Prakash and Lee (2023) further extend this analysis through the notion of 'layered bias,' highlighting how bias can be embedded not just at the dataset or annotation level, but also within the internal layers of transformer architectures themselves'",
    "Bias_Ungleichheit": "Zentrales Thema: 'cultural bias against Arabs and Muslims, which has been documented across various LLMs and poses significant ethical challenges' und 'perpetuating harmful stereotypes and marginalization'",
    "Diversitaet": "Expliziter Fokus auf marginalisierte Communities: 'This bias often takes the form of associating these groups with terrorism, violence, or religious extremism, thus reinforcing damaging stereotypes that can fuel marginalization and discrimination' und 'comprising nearly two billion people globally'",
    "Fairness": "Algorithmische Fairness als Kernthema: 'effectiveness metrics including Bias Benchmark for QA (BBQ), StereoSet, CrowS-Pairs, and SEAT' sowie 'performance-fairness trade-offs' und Fairness-Metriken wie 'Survey-alignment gap' und 'Violent-completion rate'"
  },
  "references": [
    {
      "author": "Bender, Gebru, McMillan-Major et al.",
      "year": 2021,
      "short_title": "On the Dangers of Stochastic Parrots"
    },
    {
      "author": "Said, Edward W.",
      "year": 2019,
      "short_title": "Orientalism"
    },
    {
      "author": "Abid, Farooqi & Zou",
      "year": 2021,
      "short_title": "Persistent Anti-Muslim Bias in Large Language Models"
    },
    {
      "author": "Birhane, Amanuel",
      "year": 2021,
      "short_title": "Algorithmic injustice: a relational ethics approach"
    },
    {
      "author": "Hovy & Prabhumoye",
      "year": 2021,
      "short_title": "Five sources of bias in natural language processing"
    },
    {
      "author": "Naous, Ryan, Ritter et al.",
      "year": 2024,
      "short_title": "Having Beer after Prayer? Measuring Cultural Bias in LLMs"
    },
    {
      "author": "Tao, Viberg, Baker et al.",
      "year": 2024,
      "short_title": "Cultural bias and cultural alignment of large language models"
    },
    {
      "author": "Xu, Chen, Tang et al.",
      "year": 2024,
      "short_title": "Mitigating Social Bias via Multi-Objective Multi-Agent Framework"
    },
    {
      "author": "Crawford, Kate",
      "year": 2021,
      "short_title": "The Atlas of AI: Power, Politics, and Planetary Costs"
    },
    {
      "author": "Noble, Safiya U.",
      "year": 2018,
      "short_title": "Algorithms of Oppression"
    }
  ],
  "assessment": {
    "domain_fit": "Stark relevant an Schnittstelle KI/Ethik/Diversität. Für Soziale Arbeit indirekt relevant durch Fokus auf marginalisierte Communities und gerechtere KI-Systeme, die in sozialarbeiterischen Kontexten (Beratung, Case-Management, Ressourcenallokation) zunehmend eingesetzt werden. Direkte Anwendungen begrenzt, aber theoretischer Rahmen (Orientalismus, strukturelle Ungleichheit) relevant.",
    "unique_contribution": "Erste systematische Review speziell zu Prompt-Engineering für Arab/Muslim-Bias-Mitigation; empirische Quantifizierung der Forschungslücke (nur 8 Studien) als eigenständiger Befund; Synthese fünf distinkter Techniken mit Effektivitäts-Vergleich und Implementierungs-Roadmaps.",
    "limitations": "Nur 8 Studien identifiziert (extrem kleine Basis); auf englische Publikationen beschränkt (ausschließt arabisches/muslimisches Forschungs-Output); 'Arabs and Muslims' als monolithische Kategorie behandelt (ignoriert intersektionale Variation); heterogene Evaluationsmetriken erschweren direkte Vergleichbarkeit."
  },
  "target_group": "KI-Entwickler und Datenwissenschaftler; Policy-Maker und Regulatoren im AI-Governance; Sozialarbeiter und Praktiker in Organisationen, die LLMs einsetzen; Forscher in AI Ethics, Cultural Studies, und Digital Justice; Vertreter arabischer und muslimischer Communities; Educators für AI Literacy"
}