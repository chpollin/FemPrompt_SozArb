{
  "metadata": {
    "title": "Algorithmic discrimination: examining its types and regulatory measures with emphasis on US legal practices",
    "authors": [
      "Xukang Wang",
      "Ying Cheng Wu",
      "Xueliang Ji",
      "Hongpeng Fu"
    ],
    "year": 2024,
    "type": "journalArticle",
    "language": "en"
  },
  "core": {
    "research_question": "Welche Typen von algorithmischer Diskriminierung existieren und welche regulatorischen Maßnahmen sind in den USA und international vorhanden, um diese zu bekämpfen?",
    "methodology": "Theoretisch/Mixed Methods - Systematische Literaturanalyse (85 Artikel), Analyse von Rechtsdokumenten und vergleichende Fallstudien über mehrere Sektoren und Länder",
    "key_finding": "Das Paper identifiziert fünf primäre Typen algorithmischer Voreingenommenheit (Bias durch algorithmische Agenten, diskriminierende Feature-Selektion, Proxy-Diskriminierung, Disparate Impact, zielgerichtete Werbung) und kategorisiert US-Regulierungsrahmen in fünf Ansätze (principled regulation, preventive controls, consequential liability, self-regulation, heteronomy regulation).",
    "data_basis": "Systematische Literaturanalyse von 85 peer-reviewed Artikeln aus ACM Digital Library, IEEE Xplore, LexisNexis, HeinOnline und Google Scholar; Analyse von Gerichtsfällen und Regulierungsdokumenten"
  },
  "arguments": [
    "Algorithmische Diskriminierung kann sich in mehreren verschiedenen Formen manifestieren - von explizit intendierter Diskriminierung bis zu unbewussten Verzerrungen in Trainingsdaten, Feature-Auswahl oder Modellgestaltung - und erfordert daher differenzierte regulatorische Ansätze.",
    "Bestehende US-Antidiskriminierungsgesetze sind unzureichend für die Regulierung algorithmischer Systeme; es werden proaktive Impact Assessments, Transparenzanforderungen und Auditierungsmechanismen benötigt, um systematische Diskriminierung zu verhindern.",
    "Juristische Überprüfung muss sowohl intentionale als auch unintentionale algorithmische Diskriminierung erfassen; unintentionale Diskriminierung sollte durch Analyse von Datenmustern, Impact-Assessment und Ursachenanalyse erkannt werden, um strukturelle Ungleichheiten zu adressieren."
  ],
  "categories": {
    "AI_Literacies": false,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": true,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "KI_Sonstige": "Das Paper konzentriert sich auf 'algorithmic decision-making systems' in Kriminalitätsprognose, Einstellung, Bildung und Kreditvergabe und behandelt klassische Machine Learning, nicht generative KI.",
    "Soziale_Arbeit": "Explizite Behandlung von Algorithmen im Criminal Justice System ('judges to estimate the risk of reoffending'), Arbeitseinstellung und Bildung ('schools to choose whether to admit students'), die zentrale Domänen Sozialer Arbeit und ihrer Schnittstellen sind.",
    "Bias_Ungleichheit": "Kernfokus des Papers: 'Algorithmic discrimination can manifest in various forms, such as bias by the algorithmic agents, biased feature selection... These different types of algorithmic bias can lead to unfair treatment and disparate impacts on protected groups, raising concerns about equal rights, due process, and social justice'",
    "Diversitaet": "Paper diskutiert 'disparate impacts on protected groups' und bezieht sich auf verschiedene marginalisierte Gruppen in Kontexten wie Criminal Justice Risk Assessments und Hiring Algorithms, mit Bezug zu 'lack of diversity in the development teams' als Ursache.",
    "Fairness": "Zentrales Thema durchgehend: 'algorithmic fairness, transparency, and accountability', spezifische Behandlung von Fairness-Metriken, Fairness Constraints in ML und 'algorithmic auditing and impact assessments' zur Messung und Gewährleistung von Fairness."
  },
  "references": [
    {
      "author": "Selbst and Barocas",
      "year": 2016,
      "short_title": "Big Data's Disparate Impact"
    },
    {
      "author": "Buolamwini and Gebru",
      "year": 2018,
      "short_title": "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification"
    },
    {
      "author": "Kroll et al.",
      "year": 2017,
      "short_title": "Accountable Algorithms"
    },
    {
      "author": "Crawford and Schultz",
      "year": 2014,
      "short_title": "Big Data and Due Process"
    },
    {
      "author": "Pasquale",
      "year": 2015,
      "short_title": "The Black Box Society"
    },
    {
      "author": "O'Neil",
      "year": 2017,
      "short_title": "Weapons of Math Destruction"
    },
    {
      "author": "West et al.",
      "year": 2019,
      "short_title": "Discriminating Systems: Gender, Race and Power in AI"
    },
    {
      "author": "Reisman et al.",
      "year": 2018,
      "short_title": "Algorithmic Impact Assessments: A Practical Framework for Public Agency"
    },
    {
      "author": "Prince and Schwarcz",
      "year": 2019,
      "short_title": "Proxy Discrimination in the Age of Artificial Intelligence and Big Data"
    },
    {
      "author": "Berk et al.",
      "year": 2021,
      "short_title": "Fairness in Criminal Justice Risk Assessments: The State of the Art"
    }
  ],
  "assessment": {
    "domain_fit": "Das Paper ist hochrelevant für die Schnittstelle KI und Soziale Arbeit, da es konkrete algorithmische Systeme in Bereichen adressiert, die zentral für Soziale Arbeit sind (Criminal Justice, Employment, Education), und strukturelle Mechanismen der Diskriminierung analysiert, die marginalisierte Gruppen betreffen.",
    "unique_contribution": "Der Hauptbeitrag liegt in der systematischen Taxonomie von fünf Typen algorithmischer Diskriminierung und der vergleichenden Analyse von US-Regulierungsrahmen mit internationalen Perspektiven, kombiniert mit praktischen Fallstudien zur Illustration realer Auswirkungen.",
    "limitations": "Das Paper konzentriert sich primär auf den US-Kontext und das Rechtssystem; es fehlt eine explizit intersektionale oder feministische Analyse der Diskriminierungsmechanismen, und die Perspektive betroffener Communitys ist unterrepräsentiert."
  },
  "target_group": "Policymaker, Jurist:innen, KI-Entwickler:innen, Regulierungsbehörden, Sozialarbeiter:innen in Justiz- und Sozialverwaltung, Forscher:innen in KI-Ethik und Algorithmic Fairness, Aktivist:innen im Bereich Algorithmic Justice"
}