{
  "metadata": {
    "title": "Tech workers' perspectives on ethical issues in AI development: Foregrounding feminist approaches",
    "authors": [
      "Jude Browne",
      "Eleanor Drage",
      "Kerry McInerney"
    ],
    "year": 2024,
    "type": "journalArticle",
    "language": "en"
  },
  "core": {
    "research_question": "Welche ethischen Fragen sehen Tech-Arbeiter bei der KI-Entwicklung als am dringendsten an, und wie können feministische Ansätze zur Analyse dieser Perspektiven beitragen?",
    "methodology": "Empirisch-qualitativ: In-depth interviews (n=63+) mit Tech-Mitarbeitern verschiedener Disziplinen (Engineering, Data Science, Legal, HR) in einem großen KI-Unternehmen (2020-2021); feministische Narrative-Analyse nach Sara Ahmed",
    "key_finding": "Der Begriff 'Bias' verursacht Verwirrung unter Tech-Workern und kann seine beabsichtigte ethische Funktion nicht erfüllen; Tech-Worker sehen keinen direkten Zusammenhang zwischen DEI-Agenden und KI-Entwicklung; Legacy-Systeme stellen erhebliche Herausforderungen für ethische KI-Entwicklung dar.",
    "data_basis": "63 qualitative Interviews mit Tech-Arbeitern in einem führenden KI-Unternehmen (Global North), verschiedene Funktionsbereiche"
  },
  "arguments": [
    "Der Begriff 'Bias' ist polysem und wird von Tech-Workern in verschiedenen Bedeutungen verwendet (mathematisch vs. ethisch), was zu Missverständnissen in der Implementierung von AI-Ethics-Frameworks führt. Der Vorschlag ist stattdessen, von 'AI-Partiality' zu sprechen, um die partiale Perspektive jeden ML-Modells zu betonen.",
    "DEI-Initiativen werden von Tech-Workern nicht mit AI-Entwicklung verbunden; nur 16% der Befragten sehen einen Zusammenhang zwischen DEI-Programmen und KI-Produktentwicklung, trotz der Betonung in unternehmensweiter Rhetorik. Wirtschaftlicher Druck (War for Talent) untergrätbt Integration von Diversität.",
    "Legacy-Systeme und fehlendes Lifecycle-Management von KI-Systemen stellen praktische Barrieren für ethische KI dar. Tech-Worker sind besorgt über unzureichende Monitoring- und Wartungsprotokolle für bestehende Systeme, die die Entwicklung neuer, ethischerer Produkte gefährden."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": true,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Fokus auf Verständnis und Kompetenzlücken von Tech-Workern: 'a 'lack of ethical knowledge' that made it difficult to expand and scale AI ethics principles in industry contexts' (Khan et al. 2023, zitiert im Paper)",
    "KI_Sonstige": "Umfassende Analyse von AI-Ethics-Frameworks, Bias-Mitigation-Strategien, ML-Modellen und algorithmischen Systemen in der KI-Produktentwicklung",
    "Bias_Ungleichheit": "Zentrale Analyse von Bias-Konzepten und struktureller Ungleichheit: 'data should be understood as always laden with the uneven experiences of the unequal contexts in which it emerges'; Diskussion von Geschlecht und Rasse in Daten (Wikipedia-Beispiel)",
    "Diversitaet": "Explizit DEI-fokussiert mit Analyse marginalisierter Gruppen: 'marginalized users and consumers'; Untersuchung von Repräsentation in der Datenbeschaffung und unternehmensweiter Diversität",
    "Feministisch": "Explizit feministische Methodik und Theorie durchgehend: 'we explicitly draw on feminist insights' und 'follow the methodological approach of feminist and critical race scholar Sara Ahmed (2012)'; Verwendung von Haraway's 'situated knowledges', D'Ignazio & Klein, Ahmed zur Analyse",
    "Fairness": "Diskussion von Fairness-Konzepten in AI: 'justice, fairness and equity' als zentrales AI-Ethics-Thema; kritische Analyse von Fairness-Mitigation-Ansätzen und Unmöglichkeit der Neutralität"
  },
  "references": [
    {
      "author": "Ahmed, S.",
      "year": 2012,
      "short_title": "On Being Included: Racism and Diversity in Institutional Life"
    },
    {
      "author": "Haraway, D.",
      "year": 1988,
      "short_title": "Situated Knowledges: The Science Question in Feminism"
    },
    {
      "author": "D'Ignazio, C. & Klein, L.",
      "year": 2020,
      "short_title": "Data Feminism"
    },
    {
      "author": "Benjamin, R.",
      "year": 2019,
      "short_title": "Race after Technology: Abolitionist Tools for the New Jim Code"
    },
    {
      "author": "Buolamwini, J. & Gebru, T.",
      "year": 2018,
      "short_title": "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification"
    },
    {
      "author": "Amoore, L.",
      "year": 2020,
      "short_title": "Cloud Ethics: Algorithms and the Attributes of Ourselves and Others"
    },
    {
      "author": "Khan, A., et al.",
      "year": 2023,
      "short_title": "AI Ethics: An Empirical Study on the Views of Practitioners and Lawmakers"
    },
    {
      "author": "Vakkuri, V., et al.",
      "year": 2020,
      "short_title": "AI Ethics in Industry: A Research Framework"
    },
    {
      "author": "Munn, L.",
      "year": 2022,
      "short_title": "The Usefulness of AI Ethics"
    },
    {
      "author": "Whittaker, M.",
      "year": 2021,
      "short_title": "The Steep Cost of Capture"
    }
  ],
  "assessment": {
    "domain_fit": "Hochrelevant für die Schnittstelle AI/Gender/Diversität. Das Paper kombiniert explizit feministische Theorie mit empirischer Forschung zu KI-Entwicklung und adressiert strukturelle Ungleichheiten. Die Perspektive marginalisierter Arbeitnehmer und systemische Barrieren haben große Relevanz für soziale Gerechtigkeit, lassen sich aber nur indirekt auf Soziale Arbeit anwenden.",
    "unique_contribution": "Das Paper ist innovativ in seiner Kombination feministischer Methodologie mit empirischer Industrie-Feldforschung und gibt Tech-Workern explizit eine Stimme, die in AI-Ethics-Diskursen meist ausgeschlossen sind; es dekonstruiert zentrale Konzepte (Bias) und zeigt konzeptuelle Verwirrung in der Praxis auf.",
    "limitations": "Fallstudie in einem einzigen Unternehmen (Tech Company X); begrenzte Generalisierbarkeit auf andere Organisationen; keine explizite Analyse von Geschlechts- oder Rasse-Positionen der Interviewten; keine longitudinalen Daten zur Veränderung über Zeit."
  },
  "target_group": "AI-Praktiker und Entwickler, AI-Ethik-Forscher, Policymaker und AI-Governance-Experten, Tech-Unternehmen, feministische Technologiewissenschaftler:innen, Kritische Datenforschung und STS-Community; begrenzt relevant für Sozialarbeiter:innen (indirekt durch strukturelle Ungleichheitsanalyse)"
}