{
  "metadata": {
    "title": "Understanding how users may work around algorithmic bias",
    "authors": [
      "Hannah Overbye-Thompson",
      "Ronald E. Rice"
    ],
    "year": 2025,
    "type": "journalArticle",
    "language": "en"
  },
  "core": {
    "research_question": "Wie reagieren und passen sich Nutzer:innen an vier epistemische Kategorien von algorithmischem Bias an (existent/wahrgenommen, existent/nicht wahrgenommen, nicht existent/wahrgenommen, nicht existent/nicht wahrgenommen)?",
    "methodology": "Theoretisch - Konzeptuelle Analyse unter Anwendung der HAII-TIME (Human-AI Interaction Theory of Interactive Media Effects) auf Workaround-Strategien; Systematische Literaturanalyse zu Bias-Quellen und Manifestationen",
    "key_finding": "Das Paper entwickelt einen theoretischen Rahmen, der zeigt, wie Nutzer:innen durch Cue-Routes (Wahrnehmung) und Action-Routes (Verhalten) unterschiedliche Workaround-Strategien entwickeln, um mit algorithmischem Bias umzugehen. Die vier Kategorien von Bias führen zu verschiedenen Ergebnissen: von Empowerment bei erfolgreichen Workarounds bis zu technologischer Naivität bei unwahrgenommenen Bias.",
    "data_basis": "nicht angegeben (konzeptuelles theoretisches Paper)"
  },
  "arguments": [
    "Algorithmischer Bias ist ein weit verbreitetes Problem mit dokumentierten Fällen in Gesundheitswesen, Justiz, Einstellungsverfahren und sozialen Medien, das bestimmte Gruppen (besonders Menschen mit dunkler Hautfarbe, Frauen, marginalisierte Communities) systematisch benachteiligt.",
    "Workarounds sind ein geeignetes Konzept zur Charakterisierung von Nutzer:innen-Reaktionen auf algorithmischen Bias, da sie sowohl technische Anpassungen (VPNs, Incognito-Modus, Einstellungsänderungen) als auch verhaltensbasierte Strategien (Content-Curation, veränderte Kommunikationsmuster) umfassen.",
    "Die HAII-TIME-Theorie ermöglicht ein nuanciertes Verständnis dafür, wie Nutzer:innen Bias erkennen (Cue-Route) und dann Workarounds entwickeln (Action-Route) durch Interaktion, Agentur, soziale Austauschprozesse und gegenseitige Verstärkung mit KI-Systemen."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": true,
    "Bias_Ungleichheit": true,
    "Gender": true,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Das Paper diskutiert explizit 'algorithmic literacy' und wie Nutzer:innen ihre Fähigkeit entwickeln, algorithmische Systeme zu verstehen und zu navigieren: 'Understanding these adaptive strategies provides crucial insights for developing inclusive technologies and fostering algorithmic literacy'.",
    "KI_Sonstige": "Fokus auf algorithmische Entscheidungssysteme (nicht generativ) in verschiedenen Bereichen wie Gesichtserkennungsalgorithmen, Kreditscoring, Einstellungsalgorithmen, Healthcare-Algorithmen, Empfehlungssysteme.",
    "Soziale_Arbeit": "Expliziter Fokus auf vulnerable und marginalisierte Gruppen, die durch algorithmischen Bias benachteiligt werden; Bezug zu sozialen Wohlfahrtssystemen und deren algorithmischen Kontrolle: 'Algorithms leading to widespread rejection of welfare benefit claims for low-income applicants'; Zitation von Eubanks (2018) 'Automating Inequality'.",
    "Bias_Ungleichheit": "Zentrale Thematik des gesamten Papers; dokumentiert systematische Benachteiligung durch Algorithmen: 'smartwatches have been found to provide less accurate data...to those with dark skin'; 'facial recognition systems more frequently misgender individuals with darker skin'; 'healthcare algorithms systematically favor White patients over Black patients'.",
    "Gender": "Mehrfach explizit behandelt: 'justice algorithms more often label women as high recidivism risks when determining parole'; 'hiring algorithms discriminating based on gender, race, and personality'; COMPAS-Algorithmus mit geschlechtsspezifischen Effekten; 'NIST audit finding...facial algorithms perform worse for women'.",
    "Diversitaet": "Intersektionale Perspektive auf verschiedene marginalisierte Gruppen: Menschen mit dunkler Hautfarbe, Frauen, Menschen mit afrikanisch-amerikanischem Englisch, Mieter:innen mit schwarzem/hispanischem Hintergrund, Menschen mit Behinderungen (blind assistive technology).",
    "Fairness": "Diskussion von fairness-relevanten Konzepten: 'proxy bias' (Variablen als unbeabsichtigte Proxies für geschützte Merkmale), 'equalized odds', Fairness-Metriken in ML, 'bias mitigation' Strategien, verschiedene Fairness-Ansätze in Algorithmendesign."
  },
  "references": [
    {
      "author": "Buolamwini & Gebru",
      "year": 2018,
      "short_title": "Gender Shades - Intersectional Accuracy Disparities in Commercial Gender Classification"
    },
    {
      "author": "Eubanks",
      "year": 2018,
      "short_title": "Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor"
    },
    {
      "author": "Noble",
      "year": 2018,
      "short_title": "Algorithms of Oppression: How Search Engines Reinforce Racism"
    },
    {
      "author": "Obermeyer et al.",
      "year": 2019,
      "short_title": "Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations"
    },
    {
      "author": "Friedman & Nissenbaum",
      "year": 1996,
      "short_title": "Bias in Computer Systems"
    },
    {
      "author": "Sweeney",
      "year": 2013,
      "short_title": "Discrimination in Online Ad Delivery"
    },
    {
      "author": "Sundar",
      "year": 2020,
      "short_title": "Rise of Machine Agency: A Framework for Studying the Psychology of Human-AI Interaction (HAII)"
    },
    {
      "author": "Hamilton",
      "year": 2019,
      "short_title": "The Sexist Algorithm"
    },
    {
      "author": "O'Neil",
      "year": 2016,
      "short_title": "Weapons of Math Destruction: How Big Data Increases Inequality"
    },
    {
      "author": "Lee et al.",
      "year": 2019,
      "short_title": "Algorithmic Bias Detection and Mitigation: Best Practices and Policies to Reduce Consumer Harms"
    }
  ],
  "assessment": {
    "domain_fit": "Das Paper ist hochgradig relevant für die Schnittstelle KI/Soziale Arbeit/Gender Studies, da es systematisch dokumentiert, wie algorithmische Systeme vulnerable Gruppen (insbesondere marginalisierte Communities, Frauen, Menschen mit dunkler Hautfarbe) in kritischen Bereichen wie Wohlfahrt, Justiz und Gesundheit benachteiligen, und Ansätze zur Nutzer:innen-Agency gegen diese Systeme entwickelt.",
    "unique_contribution": "Der innovative Beitrag liegt in der konzeptuellen Integration von Workaround-Theorie mit HAII-TIME zur Entwicklung eines 2x2-Typologierahmens (vier epistemische Kategorien von Bias), der differenziert erklärt, wie und wann Nutzer:innen Bias-Workarounds entwickeln und welche Outcomes entstehen.",
    "limitations": "Das Paper ist rein theoretisch-konzeptuell ohne empirische Validierung; es übersieht strukturelle und kollektive Faktoren zugunsten individueller kognitiver Prozesse; die HAII-TIME-Theorie ist neu und wenig validiert; die Analyse berücksichtigt nicht ausreichend, dass marginalisierte Gruppen Bias-Umgang durch kollektive Erfahrung statt nur technische Cues entwickeln."
  },
  "target_group": "Wissenschaftler:innen in KI/Mensch-Computer-Interaktion, Sozialarbeiter:innen und Policy-Maker:innen, die mit algorithmischer Diskriminierung konfrontiert sind, Technologiedesigner:innen, Bildungsverantwortliche für KI-Literacy, Forscher:innen in Digital-Divide und Gender Studies"
}