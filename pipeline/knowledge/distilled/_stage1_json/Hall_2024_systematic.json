{
  "metadata": {
    "title": "A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions",
    "authors": [
      "Alexandra Chouldechova",
      "Emily Putnam-Hornstein",
      "Diana Benavides-Prado",
      "Oleksandr Fialko",
      "Rhema Vaithianathan"
    ],
    "year": 2024,
    "type": "conferencePaper",
    "language": "en"
  },
  "core": {
    "research_question": "Wie können Predictive Risk Models in der Kinderschutz-Hotline-Screening-Entscheidungsfindung fair und wirksam eingesetzt werden, und welche Bias-Probleme entstehen dabei?",
    "methodology": "Mixed Methods: Empirische Fallstudie mit Modellentwicklung, Fairness-Auditing, Validierung und Deployment-Analyse. Machine Learning Methoden (Logistische Regression, Random Forest, XGBoost, SVM). Kalibrierungsanalyse und Fehlerrate-Analyse stratifiziert nach Rasse/Ethnizität.",
    "key_finding": "Das Allegheny Family Screening Tool (AFST) zeigt verbesserte Vorhersagegenauigkeit gegenüber menschlichen Urteilen, weist aber erhebliche Kalibrierungsprobleme bei schwarzen und weißen Kindern auf (z.B. 50% vs. 30% Platzierungsrate bei höchstem Score). In der Praxis werden die Modellvorgaben jedoch schwach umgesetzt, da Supervisoren 1 von 4 obligatorischen Screen-Ins überrufen.",
    "data_basis": "32.086 Referrals zur Modelltraining, 14.417 Referrals zur Validierung aus Allegheny County, PA; Implementierungsdaten von 11.157 Referrals nach Deployment"
  },
  "arguments": [
    "Statistische Modelle können objektiver sein als menschliche Entscheidungsträger, da sie konsistente Schwellwerte anwenden und nicht von kognitiven Verzerrungen (Recency Bias, Diskriminierung) beeinflusst werden, aber die zugrundeliegende Modellkalibrierung muss fairnessgerecht überprüft werden.",
    "Rassische Diskrepanzen in Kinderschutzmeldungen entstehen durch vier Faktoren: disproportionale Bedürfnisse, geografischer Kontext, ungleiche Ressourcenverteilung und rassische Vorurteile von Fachkräften; Modelle können Faktoren 1-2 nicht beheben, aber Ressourcenallokation (3) und Bias-Mitigation (4) unterstützen.",
    "Kalibrierungsfairness allein ist unzureichend: Modelle können kalibriert sein, aber ungleiche Fehlerraten über Rassengruppen hinweg aufweisen; es gibt unvermeidbare Trade-offs zwischen verschiedenen Fairness-Metriken bei unterschiedlichen Prävalenzraten über Gruppen."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": true,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Diskussion der Notwendigkeit, dass Fachkräfte verstehen, dass 'scores do not reflect anything about the certainty of the present allegations' und kritisches Verständnis von Modellgrenzen (Training, Überriding-Praktiken).",
    "KI_Sonstige": "Entwicklung und Evaluierung von Predictive Risk Models unter Verwendung von Machine Learning Methoden (Random Forest, XGBoost, Support Vector Machines) für algorithmische Entscheidungsunterstützung.",
    "Soziale_Arbeit": "Direkter Fokus auf Kinderschutzhilfe, Hotline-Screening-Entscheidungen, Fallbearbeiter-Praktiken und Integration von Modellen in reale Sozialarbeit-Workflows in Allegheny County Department of Human Services.",
    "Bias_Ungleichheit": "Explizite Analyse rassischer Diskrepanzen: 'screened-in referrals that score a 20 on the AFST ventile scale are observed to result in placement in 50% of cases involving Black children and only 30% of cases involving White children'; statistische Diskriminierung durch Verwendung von Zip-Code als Proxy.",
    "Diversitaet": "Analyse stratifiziert nach Rasse/Ethnizität, Armut und Geschlecht; Fokus auf marginalisierte Gruppen (schwarze Kinder, Familien in Armut); Diskussion der Überrepräsentation von Schwarzen Kindern im Kinderschutz.",
    "Fairness": "Zentrale Thematisierung von Fairness-Metriken: Kalibrierung, Accuracy Equity (AUC-Gleichheit), False Positive/False Negative Rates; Diskussion der Unavoidability von Trade-offs bei Fairness-Kriterien; Vergleich mit COMPAS-Debatte."
  },
  "references": [
    {
      "author": "Chouldechova, Alexandra",
      "year": 2017,
      "short_title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments"
    },
    {
      "author": "Corbett-Davies, Sam et al.",
      "year": 2017,
      "short_title": "Algorithmic decision making and the cost of fairness"
    },
    {
      "author": "Kleinberg, Jon et al.",
      "year": 2016,
      "short_title": "Inherent trade-offs in the fair determination of risk scores"
    },
    {
      "author": "Angwin, Julia et al.",
      "year": 2016,
      "short_title": "How we analyzed the COMPAS recidivism algorithm"
    },
    {
      "author": "Dettlaff, Alan J. et al.",
      "year": 2011,
      "short_title": "Disentangling substantiation: The influence of race, income, and risk on the substantiation decision in child welfare"
    },
    {
      "author": "Fluke, John et al.",
      "year": 2011,
      "short_title": "Disparities and disproportionality in child welfare: Analysis of the research"
    },
    {
      "author": "Meehl, Paul E.",
      "year": 1954,
      "short_title": "Clinical versus statistical prediction: A theoretical analysis and a review of the evidence"
    },
    {
      "author": "Vaithianathan, Rhema et al.",
      "year": 2013,
      "short_title": "Children in the public benefit system at risk of maltreatment: Identification via predictive modeling"
    },
    {
      "author": "Shroff, Ravi",
      "year": 2017,
      "short_title": "Predictive analytics for city agencies: Lessons from children's services"
    },
    {
      "author": "Skeem, Jennifer L. & Lowenkamp, Christopher T.",
      "year": 2016,
      "short_title": "Risk, race, and recidivism: predictive bias and disparate impact"
    }
  ],
  "assessment": {
    "domain_fit": "Hochgradig relevant für die Schnittstelle AI/Soziale Arbeit: Das Paper adressiert direkt die Implementierung von ML-Systemen in kritischen Kinderschutztätigkeiten, analysiert rassische Bias-Implikationen und untersucht, wie algorithmische Fairness in der Sozialen Arbeit praktiziert werden kann.",
    "unique_contribution": "Die Fallstudie bietet seltene empirische Daten aus einer tatsächlich deplierten AI-Anwendung in der Sozialen Arbeit mit detaillierter Kalibrierungs- und Fairnessanalyse sowie realen Implementierungsergebnissen, die zeigen, dass Fairness ein Prozess-Property und nicht nur Modell-Property ist.",
    "limitations": "Hauptlimitation: Selective Labels Problem – Platzierungsergebnisse sind nur für gescreente Fälle beobachtbar, nicht für gescreente-out Fälle; zudem wurden Analysen hauptsächlich auf Black/White Kategorien fokussiert und andere Rassengruppen unterrepräsentiert, sowie Übertragbarkeit auf andere Jurisdiktionen unklar."
  },
  "target_group": "Primär: Sozialpolitiker und Kinderschutzbehördenleiter, KI-Entwickler in Government/Public Sector, Forschende in algorithmischer Fairness und Child Welfare; Sekundär: Sozialarbeiter, Ethiker, Datenschützer, Community Advocates"
}