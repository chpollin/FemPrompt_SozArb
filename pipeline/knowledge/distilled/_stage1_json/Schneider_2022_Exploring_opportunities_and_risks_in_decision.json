{
  "metadata": {
    "title": "Exploring Opportunities and Risks in Decision Support Technologies for Social Workers: An Empirical Study in the Field of Disabled People's Services",
    "authors": [
      "Diana Schneider",
      "Angelika Maier",
      "Philipp Cimiano",
      "Udo Seelmeyer"
    ],
    "year": 2022,
    "type": "journalArticle",
    "language": "en"
  },
  "core": {
    "research_question": "Wie können Fachkräfte der Sozialen Arbeit in der Teilhabeplanung für Menschen mit Behinderung durch Entscheidungsunterstützungssysteme (DSSs) unterstützt werden, und welche Erwartungen, Befürchtungen und ethischen Implikationen sind damit verbunden?",
    "methodology": "Mixed Methods: Empirisch qualitativ mit zwei Teilen - ESI Study (Interviews zu Erwartungen und Befürchtungen, n nicht spezifiziert) und User Study (Prototyp-Testing mit 5 Fachkräften). Prospektive Technologiebewertung mit Antizipationsmethoden.",
    "key_finding": "DSSs mit Visualisierungen der Klient*innen-Entwicklung werden als unterstützend wahrgenommen; es besteht Bedarf für partizipative Entscheidungsfindung; technische und professionelle Zuverlässigkeit dürfen nicht verwechselt werden.",
    "data_basis": "22 Klient*innen-Dateien mit 295.812 Datensätzen von 2 Wohneinrichtungen; Interviews mit Fachkräften von Leistungserbringern und Teilhabebehörden; Prototyp-Teststudie mit 5 Sozialarbeiter*innen"
  },
  "arguments": [
    "Visuelle Darstellungen von Klient*innen-Entwicklungen durch KI-basierte DSSs können berufliche Reflexion fördern und einen Mehrwert bieten, insofern sie subjektive Perspektiven transparenter machen und die professionelle Urteilsbildung unterstützen.",
    "Gegenwärtige Vorstellungen von DSSs fokussieren primär auf Professional-Algorithmus-Interaktion und ignorieren die Notwendigkeit partizipativer Entscheidungsfindung mit Service-Nutzer*innen, was ein kritisches Defizit in der Konzeptentwicklung darstellt.",
    "Die in professionelle Dokumentation eingeflossenen Biases, Subjektivitäten und Datenqualitätsprobleme stellen grundsätzliche Herausforderungen dar und erfordern Datenkompetenz und kritisches Verständnis der Unterschiede zwischen technischer und professioneller Zuverlässigkeit."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": true,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Data literacy und Verständnis technischer Prozesse werden als erforderlich benannt: 'Keeping this crucial distinction in mind and accounting for it in daily work with algorithms requires data literacy and an understanding of the technical processes'",
    "KI_Sonstige": "Fokus auf AI-basierte Vorhersagesysteme (LONA-Scoring), algorithmische Entscheidungssysteme und natürliche Sprachverarbeitung: 'The system relies on an artificial intelligence (AI) based system trained to predict levels of need for assistance (LONA) from textual documentations'",
    "Soziale_Arbeit": "Expliziter Fokus auf Soziale Arbeit in Teilhabeplanung für Menschen mit Behinderung, professionelle Urteilsbildung und Fachkräfte-Kompetenzen: 'MAEWIN project, therefore, addresses the question of how professionals of social care providers could be supported in the context of SSP by DSSs'",
    "Bias_Ungleichheit": "Kritische Analyse von Biases in Dokumentation und Datenbasis: 'documentation may contain hidden biases, biased perspectives, or prejudices' und Diskriminierungsrisiken von Algorithmen: 'systemic discrimination'",
    "Diversitaet": "Fokus auf Menschen mit Behinderung als marginalisierte Gruppe und deren Partizipation in Entscheidungsprozessen: 'shared decision-making processes with the persons entitled to benefits'",
    "Fairness": "Fairness-Konzepte in algorithmischen Entscheidungssystemen und Anforderung fairer Darstellung: 'data basis used by algorithms is quality controlled and free of biases caused by data reflecting the perceptions of specific stakeholders'"
  },
  "references": [
    {
      "author": "Gillingham",
      "year": 2019,
      "short_title": "Decision support systems, social justice and algorithmic accountability in social work"
    },
    {
      "author": "Crawford",
      "year": 2013,
      "short_title": "The hidden biases in big data"
    },
    {
      "author": "Raji",
      "year": 2020,
      "short_title": "How our data encodes systematic racism"
    },
    {
      "author": "Wachter, Mittelstadt & Floridi",
      "year": 2017,
      "short_title": "Transparent, explainable, and accountable AI"
    },
    {
      "author": "Collingridge",
      "year": 1980,
      "short_title": "The social control of technology"
    },
    {
      "author": "Braun et al.",
      "year": 2020,
      "short_title": "Primer on an ethics of AI-based decision support systems in the clinic"
    },
    {
      "author": "Schneider & Seelmeyer",
      "year": 2019,
      "short_title": "Challenges in using big data to develop decision support systems for social work in Germany"
    },
    {
      "author": "Chiusi et al.",
      "year": 2020,
      "short_title": "Automating society report 2020"
    }
  ],
  "assessment": {
    "domain_fit": "Hochgradig relevant für die Schnittstelle KI/Soziale Arbeit. Das Paper adressiert zentrale Fragen der Implementierung von KI-Systemen in einer kritischen Profession und untersucht Auswirkungen auf vulnerable Zielgruppen (Menschen mit Behinderung) und professionelle Praxis mit Fokus auf Partizipation.",
    "unique_contribution": "Einzigartig ist die Beteiligung von Sozialarbeiter*innen als Antizipant*innen in frühen Entwicklungsphasen von DSSs kombiniert mit kritischer Analyse von Subjektivität und Bias in professioneller Dokumentation sowie die Forderung nach partizipativen (statt nur technokratischen) Entscheidungsmodellen.",
    "limitations": "Kleine Stichprobe in User Study (n=5); fehlende explizite Perspektive von Service-Nutzer*innen (Menschen mit Behinderung) selbst; Geschlechter- und intersektionale Dimensionen werden nicht systematisch analysiert; Fokus auf Deutschland begrenzt Generalisierbarkeit."
  },
  "target_group": "Sozialarbeiter*innen, Fachkräfte der Behindertenhilfe, KI-Entwickler*innen mit Anwendungsfokus Soziale Arbeit, Policymaker im Sozialsektor, Forscher*innen zu Technology Assessment und Verantwortungsvoller Innovation, Vertreter*innen von Behindertenorganisationen"
}