{
  "metadata": {
    "title": "Ideology Prediction from Scarce and Biased Supervision: Learn to Disregard the 'What' and Focus on the 'How'!",
    "authors": [
      "Chen Chen",
      "Dylan Walker",
      "Venkatesh Saligrama"
    ],
    "year": 2023,
    "type": "conferencePaper",
    "language": "en"
  },
  "core": {
    "research_question": "Wie können politische Ideologien zuverlässig aus Textdaten vorhergesagt werden, wenn Trainungsdaten knapp und durch Selection Bias verzerrt sind?",
    "methodology": "Empirisch: Deep Learning mit Variational Autoencoder (VAE)-Rahmen, statistische Modellierung zur Dekomposition von Texteinbettungen, Crowdsourcing-Validierung, experimentelle Evaluierung auf zwei Benchmark-Datensätzen (Congressional Reports, Debatepolitics Forum)",
    "key_finding": "Das Modell BBBG dekomponiert Dokumente in kontextuelle (neutrale) und ideologische Vektoren und erreicht damit signifikant bessere Vorhersagen bei nur 5% biased Trainingsdaten als bisherige Methoden, mit besonderer Effektivität bei der Vorhersage ideologischer Positionen von moderaten (nicht polarisierten) Autoren.",
    "data_basis": "Congressional Reports Corpus; Posts von Debatepolitics.com Forum; Crowdsourced Annotationen via Prolific.com (n nicht exakt angegeben, aber multiple Bewertungen für Neutralitätsvalidierung)"
  },
  "arguments": [
    "Selection Bias in politischen Etiketten ist ein systematisches Problem: Selbstgemeldete Zugehörigkeiten sind spärlich und extrem polarisiert, während die schweigsame Mehrheit ideologisch gemäßigt aber unterrepräsentiert ist. Dies führt zu Modellen, die auf extremistische Positionen überoptimiert sind und moderate Bevölkerungsgruppen nicht abbilden.",
    "Die Dekomposition von Texteinbettungen in kontextuelle (themenbezogene, neutrale) und ideologische Positionsvektoren ermöglicht es, irrelevante Varianz zu filtern und reine Ideologie-Signale zu extrahieren. Contextual Filtering erzeugt 'ideological purity' und verbessert Generalisierung zu Out-of-Distribution-Daten.",
    "Das multimodale Prior mit K=2 Gaußschen Komponenten reflektiert die empirische Bipolarisierung der US-Politik und ermöglicht Wissenstransfer von extremen zu moderaten Autoren und von bekannten zu unbekannten Themen, was für realistische Vorhersagen der 'silent majority' essentiell ist."
  ],
  "categories": {
    "AI_Literacies": false,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "KI_Sonstige": "Entwicklung eines Deep Learning Modells (VAE mit bi-modalen Priors) für Political Ideology Prediction (NLP, Supervised Learning, Domain Adaptation). Klassisches ML-Problem mit innovativer statistischer Modellierung.",
    "Bias_Ungleichheit": "Paper adressiert explizit Selection Bias durch Überrepräsentation von 'vocal minority' gegenüber 'silent majority': 'Such self-reported affiliations are generally sparse, and when reported tend to be of extreme polarity.' Die Justification betont: 'A black-box model trained exclusively on scarce and polarized group is likely to perform poorly on the under-observed moderates who are the majority.'",
    "Diversitaet": "Fokus auf Repräsentation unterrepräsentierter Bevölkerungsgruppen (moderate, nicht-extreme politische Positionen): 'it is particularly useful to predict and evaluate the stance of the non-extreme group who tends to politically inactive.' Kritik an Überrepräsentation extremer Positionen und Vernachlässigung von Minderheitsgruppen in Trainingsdaten.",
    "Fairness": "Fairness-Aspekt durch Adressierung von Repräsentationsbias und Entwicklung eines Modells, das bei knapper und verzerrter Supervision fair across polarization spectrum performt. Validierung zeigt bessere Generalisierung zu moderaten Gruppen als Baselines."
  },
  "references": [
    {
      "author": "Poole & Rosenthal",
      "year": 2001,
      "short_title": "Party ideology and legislative behavior"
    },
    {
      "author": "Bakshy et al.",
      "year": 2015,
      "short_title": "Exposure to ideologically diverse news and opinion on Facebook"
    },
    {
      "author": "Conover et al.",
      "year": 2011,
      "short_title": "Predicting the political alignment of Twitter users"
    },
    {
      "author": "Cohen & Ruths",
      "year": 2013,
      "short_title": "Classifying political orientation on Twitter"
    },
    {
      "author": "Gentzkow & Shapiro",
      "year": 2010,
      "short_title": "What drives media slant? Evidence from US daily newspapers"
    },
    {
      "author": "Kingma & Welling",
      "year": 2013,
      "short_title": "Autoencoding Variational Bayes"
    },
    {
      "author": "Kohut & Bowman",
      "year": 2018,
      "short_title": "The vocal minority in US politics"
    },
    {
      "author": "McClain",
      "year": 2021,
      "short_title": "70% of U.S. social media users never or rarely post about political issues"
    },
    {
      "author": "Tomczak & Welling",
      "year": 2018,
      "short_title": "VAE with a Gaussian Mixture prior"
    }
  ],
  "assessment": {
    "domain_fit": "Das Paper liegt an der Schnittstelle KI/NLP und hat begrenzte direkte Relevanz für Soziale Arbeit, adressiert aber wichtige Aspekte von Bias, Repräsentation und Fairness in algorithmschen Systemen, die für die kritische Analyse von datengestützten Entscheidungssystemen in der Sozialen Arbeit (z.B. Prognose-Instrumente, Ressourcenallokation) relevant sind.",
    "unique_contribution": "Innovative Dekompositions-Architektur (Kontext vs. Ideologie) in einem VAE-Framework, die unter Datenknappheit und Selection Bias funktioniert und explizit auf Transfer von extremer zu moderater Polarisierung optimiert ist—eine bisher nicht adressierte Herausforderung in der Ideology-Prediction-Literatur.",
    "limitations": "Begrenzt auf US-Politik und zwei Englisch-Datensätze; die Annahme von K=2 modalen Komponenten ist US-politikspezifisch; keine theoretische Analyse der Orthogonalität zwischen Kontext- und Positionsvektoren; potenzielle Überverallgemeinerung auf andere Domänen wird erwähnt aber nicht empirisch getestet."
  },
  "target_group": "KI-Forscher und NLP-Praktiker (Primary); Computational Social Scientists; Policy-Makers interessiert an Understanding public opinion; kritisch: ML-Praktiker die mit biased/sparse Daten arbeiten; sekundär relevant für Sozialarbeitspraktiker und -forscher die algorithmische Fairness und Bias in datengestützten Systemen kritisch analysieren wollen"
}