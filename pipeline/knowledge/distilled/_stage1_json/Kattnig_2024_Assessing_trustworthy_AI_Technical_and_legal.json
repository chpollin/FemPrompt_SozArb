{
  "metadata": {
    "title": "Assessing trustworthy AI: Technical and legal perspectives of fairness in AI",
    "authors": [
      "Markus Kattnig",
      "Alessa Angerschmid",
      "Thomas Reichel",
      "Roman Kern"
    ],
    "year": 2024,
    "type": "journalArticle",
    "language": "en"
  },
  "core": {
    "research_question": "Wie können technische Methoden zur Bias-Mitigation mit den rechtlichen Anforderungen der EU, insbesondere des AI Act, in Einklang gebracht werden, um faire und nicht-diskriminierende KI-Systeme zu gewährleisten?",
    "methodology": "Theoretisch-vergleichende Review: Analyse von State-of-the-Art Bias-Mitigationsmethoden und deren Gegenüberstellung mit EU-Rechtlichen Anforderungen (AI Act, GDPR, ECHR, Non-Discrimination Law)",
    "key_finding": "Obwohl zahlreiche technische Methoden zur Bias-Mitigation existieren, erfüllen nur wenige die bestehenden EU-Rechtsanforderungen. Eine interdisziplinäre Herangehensweise ist notwendig, um die Kluft zwischen technischen und rechtlichen Konzepten von Fairness zu überbrücken.",
    "data_basis": "nicht angegeben"
  },
  "arguments": [
    "Fairness ist ein komplexes, interdisziplinäres Konzept, das in Rechtswissenschaften, Informatik und Sozialwissenschaften unterschiedlich definiert wird, was zu Umsetzungskhallengen führt.",
    "Bias in KI-Systemen entsteht durch systematische und historische Ungleichheiten in Trainingsdaten und kann durch Feedback-Loops verstärkt werden, was bereits benachteiligte Gruppen weiter diskriminiert.",
    "Technische Bias-Mitigationsmethoden (Pre-, In-, Post-Processing) müssen mit EU-Rechtlichen Anforderungen (Nicht-Diskriminierung, Fairness, Transparenz, Kontrollierbarkeit) abgeglichen werden, was bisher unzureichend geschieht."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Das Paper behandelt das Verständnis und die kritische Reflexion von KI-Systemen: 'it is crucial to examine the legal requirements for AI systems to ensure fairness and non-discrimination. This paper examines the legal framework for AI in the EU and provides insights into the challenges and opportunities of addressing bias mitigation in AI systems'",
    "KI_Sonstige": "Umfassende Behandlung von Machine Learning Methoden, algorithmischen Entscheidungssystemen und deren technischen Implementierungen: 'Automated decision-making systems (ADMs) gather and process data in order to make qualitative decisions with minimal to no human intervention'",
    "Bias_Ungleichheit": "Zentraler Fokus auf algorithmischen Bias und soziale Ungleichheit: 'bias refers to the presence of systematic and unfair behaviour and errors in AI systems. Hence, a bias may lead to discriminatory outcomes and unfair treatment, which further implicates the importance of fairness' und 'systematic or historical inequalities, which exacerbate unfair treatment of already disadvantaged groups'",
    "Diversitaet": "Thematisiert Repräsentation und Benachteiligung verschiedener Gruppen: 'they especially targeted poor and minority neighbourhoods' und Diskussion von Group Fairness vs. Individual Fairness für unterschiedliche Populationen",
    "Fairness": "Zentrales Thema des gesamten Papers mit detaillierter Analyse von Fairness-Konzepten: 'Fairness is considered the central starting point for the application of AI in society' und umfassende Behandlung von Fairness-Maßnahmen, Group Fairness und Individual Fairness"
  },
  "references": [
    {
      "author": "Calders et al.",
      "year": 2010,
      "short_title": "Massaging and Reweighting for Bias Mitigation"
    },
    {
      "author": "Tyler",
      "year": 2000,
      "short_title": "Procedural Fairness Elements"
    },
    {
      "author": "O'Neil",
      "year": 2016,
      "short_title": "Weapons of Math Destruction / Feedback Loops in Automated Systems"
    },
    {
      "author": "Rawls",
      "year": null,
      "short_title": "Fair Procedures and Court Proceedings"
    },
    {
      "author": "Lind & Tyler",
      "year": 1988,
      "short_title": "Procedural Justice in Court Systems"
    },
    {
      "author": "Colquitt",
      "year": 2001,
      "short_title": "Organisational Justice"
    },
    {
      "author": "Skitka et al.",
      "year": 2000,
      "short_title": "Automation Bias in Decision-Support Systems"
    },
    {
      "author": "Hussain et al.",
      "year": 2022,
      "short_title": "Adversarial Attacks on Fairness in Machine Learning"
    },
    {
      "author": "Dwork et al.",
      "year": 2018,
      "short_title": "Decoupled Classifiers for Group-Fair Machine Learning"
    },
    {
      "author": "Wachter, Mittelstadt & Russell",
      "year": 2021,
      "short_title": "Why Fairness Cannot Be Automated"
    }
  ],
  "assessment": {
    "domain_fit": "Das Paper ist hochrelevant für die Schnittstelle von KI und Regulierung, behandelt aber Soziale Arbeit nicht explizit. Es ist jedoch von großer Bedeutung für Sozialarbeiter:innen, die mit automatisierten Entscheidungssystemen (z.B. in Hilfevergabe, Risikoeinschätzung) konfrontiert sind und deren diskriminierendes Potenzial verstehen müssen.",
    "unique_contribution": "Der besondere Beitrag liegt in der systematischen Gegenüberstellung von technischen Bias-Mitigationsmethoden mit EU-Rechtsanforderungen und der Aufzeigung der Lücke zwischen technischen und juridischen Fairness-Konzepten.",
    "limitations": "Das Paper ist primär auf die EU fokussiert und behandelt Soziale Arbeit nicht als Anwendungsfeld; empirische Überprüfung der Rechtskonformität von Mitigationsmethoden fehlt."
  },
  "target_group": "KI-Entwickler:innen, Compliance-Officer, Policymaker, Rechtsexpert:innen im Bereich KI-Regulierung, kritische Informatiker:innen; sekundär relevant für Sozialarbeiter:innen, die mit automatisierten Entscheidungssystemen arbeiten"
}