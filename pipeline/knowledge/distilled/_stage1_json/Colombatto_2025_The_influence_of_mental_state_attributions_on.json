{
  "metadata": {
    "title": "The influence of mental state attributions on trust in large language models",
    "authors": [
      "Clara Colombatto",
      "Jonathan Birch",
      "Stephen M. Fleming"
    ],
    "year": 2025,
    "type": "journalArticle",
    "language": "en"
  },
  "core": {
    "research_question": "Wie beeinflussen Zuschreibungen von mentalen Zuständen (insbesondere Bewusstsein und andere Mentalisierungen) das Vertrauen der Nutzer in Ratschläge von großen Sprachmodellen?",
    "methodology": "Empirisch: Preregistriertes Experiment (N=410) mit zwei Aufgaben (Advice-Taking-Task und Mental State Ratings), kombiniert mit Bayesian und frequentist Analysen",
    "key_finding": "Attributionen von Intelligenz-Merkmalen erhöhen signifikant die Annahme von KI-Ratschlägen, während Attributionen von Erfahrungs-Merkmalen (Emotionen, Empfindungen) diese reduzieren. Bewusstseinszuschreibungen zeigen keine positive, sondern eine schwache negative Korrelation mit Ratsannahmen.",
    "data_basis": "N=410 US-amerikanische Erwachsene (stratifizierte Stichprobe, 204 Frauen, 199 Männer, 7 andere; Durchschnittsalter 47,19 Jahre), vollständig vorregistriert, anonymisierte Daten auf OSF verfügbar"
  },
  "arguments": [
    "Nutzer schreiben LLMs vermehrt mentale Zustände zu, die nicht mit wissenschaftlichen Erkenntnissen über KI-Bewusstsein übereinstimmen, aber diese folk-psychologischen Attributionen beeinflussen das Vertrauen und Verhalten im Umgang mit KI-Systemen.",
    "Mentale Zustandszuschreibungen sind keine unitäre Dimension: Die Dimensionen 'Intelligenz' (Reasoning, Planung) und 'Erfahrung' (Emotionen, Empfindungen) wirken differenziert auf Vertrauen und Ratsannahmen – Intelligenz positiv, Erfahrung negativ.",
    "Es existiert eine kritische Diskrepanz zwischen explizitem Vertrauen (gemessen in Selbstauskünften) und tatsächlichem Verhalten (Ratsannahmen): Bewusstseinszuschreibungen korrelieren mit selbstberichteter Vertrauensbewertung, nicht aber mit realen Entscheidungen, was auf die Bedeutung von Verhaltensmaßen hinweist."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": true,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": false,
    "Gender": false,
    "Diversitaet": false,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Das Paper untersucht, wie Nutzer Kompetenzen und mentale Zustände von LLMs verstehen und interpretieren: 'the extent to which current AI systems possess consciousness remains contentious' und 'the majority of a representative sample of the public attributes some possibility of human-like consciousness to large language models'",
    "Generative_KI": "Fokus auf ChatGPT und Large Language Models: 'We recruited a stratified sample of US adults and probed their intuitions about the capacity for consciousness and a variety of other mental states in a prominent LLM, ChatGPT.'",
    "KI_Sonstige": "Thematisiert Anthropomorphismus, Mentalisierung und Trust in KI-Systemen allgemein: 'the tendency to assign mental states to AI systems is independent from whether these systems truly possess them'",
    "Fairness": "Das Paper adressiert faire und kalibrierte Vertrauensbeziehungen zwischen Menschen und KI: 'Further investigation will help the AI sector achieve well-calibrated and balanced trust, finding the middle ground between mistrust and over-reliance.'"
  },
  "references": [
    {
      "author": "Gray, H. M., Gray, K., & Wegner, D. M.",
      "year": 2007,
      "short_title": "Dimensions of mind perception"
    },
    {
      "author": "Waytz, A., Cacioppo, J., & Epley, N.",
      "year": 2010,
      "short_title": "Who sees human? The stability and importance of individual differences in anthropomorphism"
    },
    {
      "author": "Epley, N., Waytz, A., & Cacioppo, J. T.",
      "year": 2007,
      "short_title": "On seeing human: a three-factor theory of anthropomorphism"
    },
    {
      "author": "De Visser, E. J. et al.",
      "year": 2016,
      "short_title": "Almost human: anthropomorphism increases trust resilience in cognitive agents"
    },
    {
      "author": "Waytz, A., Heafner, J., & Epley, N.",
      "year": 2014,
      "short_title": "The mind in the machine: anthropomorphism increases trust in an autonomous vehicle"
    },
    {
      "author": "Colombatto, C., & Fleming, S. M.",
      "year": 2024,
      "short_title": "Folk psychological attributions of consciousness to large language models"
    },
    {
      "author": "Hancock, P. A. et al.",
      "year": 2011,
      "short_title": "A meta-analysis of factors affecting trust in human-robot interaction"
    },
    {
      "author": "Chalmers, D. J.",
      "year": 2023,
      "short_title": "Could a large language model be conscious?"
    }
  ],
  "assessment": {
    "domain_fit": "Das Paper ist bedingt relevant für Soziale Arbeit, da es untersucht, wie Menschen KI-Systeme wahrnehmen und vertrauen – Erkenntnisse, die für KI-gestützte Entscheidungsfindung in sozialen Kontexten relevant sind. Es adressiert jedoch weder spezifische Anwendungen in der Sozialen Arbeit noch deren Zielgruppen.",
    "unique_contribution": "Erstmalige experimentelle Trennung der Effekte von Intelligenz- vs. Erfahrungs-Attributionen auf tatsächliches Vertrauen in LLM-Ratschläge mit Bayesian-Analysen und Verhaltensmaßen statt nur Selbstauskünften.",
    "limitations": "Das Paper untersucht nur einen generalen Knowledge-Task mit kontrollierten Ratschlägen; die Erkenntnisse könnten in emotional oder persönlich relevanten Entscheidungskontexten (z.B. therapeutische oder beratende Situationen) anders ausfallen, wie die Autoren selbst eingestehen."
  },
  "target_group": "KI-Forschende, Psychologen, UX/UI-Designer für KI-Systeme, Policymaker im Bereich KI-Regulierung, Entwickler von Human-AI Interaction Systemen; begrenzt relevant für Sozialarbeiter, die mit KI in Praxis arbeiten"
}