{
  "metadata": {
    "title": "Feminism Confronts AI: The Gender Relations of Digitalisation",
    "authors": [
      "Judy Wajcman",
      "Erin Young"
    ],
    "year": 2023,
    "type": "book",
    "language": "en"
  },
  "core": {
    "research_question": "Wie sind Geschlechterverhältnisse in die Entwicklung und Gestaltung von KI-Systemen eingebettet, und welche Konsequenzen hat die Unterrepräsentation von Frauen in KI-Feldern für die Verfestigung von Bias?",
    "methodology": "Theoretisch/Review: Intersektionale technofeministische Analyse mit empirischen Daten zu Geschlechterrepräsentation in Tech/AI-Feldern, Literaturanalyse der feministischen STS-Tradition, Dokumentenanalyse von Industrie- und Akademiedaten",
    "key_finding": "Die Dominanz von Männern in AI- und Datenwissenschaftsteams führt zu einem Feedback-Loop, in dem geschlechtliche und rassistische Bias in algorithmische Systeme eingebettet und verstärkt werden. Dies ist nicht das Resultat unbewusster Diskriminierung, sondern das Ergebnis einer Technokultur, die Frauen und marginalisierte Gruppen systematisch aus Führungs- und Gestaltungspositionen ausgeschlossen hat.",
    "data_basis": "Sekundäranalyse: Statistiken zur Geschlechterrepräsentation in Tech (OECD, UNESCO, Wired/Element AI, Kaggle, Stack Overflow, Google, Microsoft); akademische Publikationsdaten; Umfragedaten (McKinsey, Kapor Center, Atomico); Fallstudien algorithmischer Bias (z.B. Amazon-Recruiting-Tool, Gender Shades)"
  },
  "arguments": [
    "Frauen sind dramatisch unterrepräsentiert in KI und Datenwissenschaft (global 26%, UK 22%, nur 10-15% bei führenden Tech-Unternehmen), und diese Zahlen sind sogar niedriger als in früheren Computerberufen (1980er: ~40% → heute: 15-20%), was zeigt, dass die Bejahung technischer Expertise mit Männlichkeit kulturell historisch konstruiert wurde.",
    "Die geschlechtliche Segregation in Tech ist nicht primär ein 'Pipeline-Problem' mangelnder Fähigkeiten, sondern resultat aus: (a) der historischen Neudefinition von Computerprogrammierung als männliche Profession seit den 1950ern, (b) einer durchdringenden 'Masculine Defaults'-Kultur in Unternehmen, und (c) hoher Abwanderungsraten von Frauen aus Tech-Jobs (2x höher als Männer) aufgrund von Diskriminierung, sexueller Belästigung und unwillkommener Arbeitsklimate.",
    "Algorithmische Bias ist nicht ein Glitch oder Resultat schlechter Trainingsdaten allein, sondern eine strukturelle Konsequenz einer von Männern dominierten Technokultur: Bias wird in alle Phasen von ML-Systemen eingebettet (Datenerstellung, Datenauswahl, Modellierungsentscheidungen, Problem-Definition), verstärkt sich durch Feedback-Loops, und wird durch die Abwesenheit von Frauen und marginalisierten Perspektiven bei der Systemgestaltung perpetuiert."
  ],
  "categories": {
    "AI_Literacies": true,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": true,
    "Diversitaet": true,
    "Feministisch": true,
    "Fairness": true
  },
  "category_evidence": {
    "AI_Literacies": "Diskussion der Digital Skills Gap und fehlender techno-sozialer Kompetenzen von Frauen: 'the gender 'digital skills gap' persists... divided not only remains large but, in some contexts, is growing wider.'",
    "KI_Sonstige": "Fokus auf Machine Learning, Algorithmen, automatisierte Entscheidungssysteme: 'AI, underpinned by algorithms and machine learning, has become a defining feature and driving force of this data-driven digitalisation.'",
    "Bias_Ungleichheit": "Zentrale Analyse von Algorithmic Bias und struktureller Benachteiligung: 'Data created, processed, and interpreted within unequal power structures can reproduce the same exclusions and discriminations present in society.'",
    "Gender": "Expliziter Gender-Fokus durchgehend: 'women are under-represented in this digital revolution... despite the possibilities for marshalling greater equality'; 'historical relationship between technical expertise and masculinity'",
    "Diversitaet": "Intersektionale Perspektive auf marginalisierte Gruppen: 'women of colour are particularly under-represented... Only 1.6 percent of Google's US workforce in 2020 were Black women'; 'a technoculture that has systematically excluded women and people from marginalised groups'",
    "Feministisch": "Explizite Verwendung feministischer STS-Theorie (Harding, Haraway, D'Ignazio, Suchman): 'Adopting an intersectional technofeminist approach... feminist STS scholars have been researching the gendering of technology for decades'; Referenzen auf Crenshaw (1995), Collins (1998), Sandoval (2000) für intersektionale feministische Theorie",
    "Fairness": "Kritik an Fairness-Konzepten und deren Limitations: 'Notably, since 'fairness' cannot be mathematically defined, and is rather a deeply political issue, this task often falls to the developers themselves'; 'bias in, bias out'"
  },
  "references": [
    {
      "author": "Hicks",
      "year": 2017,
      "short_title": "Programmed Inequality: How Britain Discarded Women Technologists"
    },
    {
      "author": "Buolamwini & Gebru",
      "year": 2018,
      "short_title": "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification"
    },
    {
      "author": "D'Ignazio & Klein",
      "year": 2020,
      "short_title": "Data Feminism"
    },
    {
      "author": "Criado Perez",
      "year": 2019,
      "short_title": "Invisible Women: Exposing Data Bias in a World Designed for Men"
    },
    {
      "author": "Eubanks",
      "year": 2018,
      "short_title": "Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor"
    },
    {
      "author": "West, Whittaker & Crawford",
      "year": 2019,
      "short_title": "Discriminating Systems: Gender, Race and Power in AI"
    },
    {
      "author": "O'Neil",
      "year": 2016,
      "short_title": "Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy"
    },
    {
      "author": "Noble",
      "year": 2018,
      "short_title": "Algorithms of Oppression: How Search Engines Reinforce Racism"
    },
    {
      "author": "Wajcman",
      "year": 2004,
      "short_title": "TechnoFeminism"
    },
    {
      "author": "Haraway",
      "year": 1988,
      "short_title": "Situated Knowledges: The Science Question in Feminism"
    }
  ],
  "assessment": {
    "domain_fit": "Das Paper ist hochrelevant für die Schnittstelle KI/Geschlechtergerechtigkeit und mit Einschränkungen auch für Soziale Arbeit: Es zeigt, wie KI-Systeme durch männlich-dominierte Techno-Kulturen geprägt sind und damit strukturelle Ungleichheiten reproduzieren—ein zentrales Anliegen kritischer Sozialer Arbeit. Allerdings behandelt es Soziale Arbeit nicht explizit als Anwendungsfeld oder Berufsgruppe.",
    "unique_contribution": "Das Paper verbindet historische Analyse der Technikentwicklung (Computing als ursprünglich weibliche Arbeit) mit zeitgenössischer intersektionaler feministischer Kritik an KI und bietet damit eine durchgängige Argumentation gegen die Mythenbildung um technische Objektivität.",
    "limitations": "Begrenzte Behandlung von Lösungsansätzen jenseits der Forderung nach mehr Frauen in Tech; keine detaillierten empirischen Fallstudien zu spezifischen AI-Systemen; fokussiert primär auf westliche/Global-North-Perspektive, obwohl Global-South-Dynamiken erwähnt werden; keine Diskussion von Intersektionalität jenseits Gender, Race und Class."
  },
  "target_group": "KI-Entwickler und Softwarearchitekten (Reflexion über technische Entscheidungsfindung), Tech-Unternehmen und HR-Leitung (Diversitätsstrategie), Policymaker und Regulatoren (Tech-Governance), feministische Theoretiker und Gender Studies, kritische Sozialarbeiter und Sozialwissenschaftler mit Interesse an technologischen Systemen und Ungleichheit, Anwälte im Bereich Antidiskriminierung"
}