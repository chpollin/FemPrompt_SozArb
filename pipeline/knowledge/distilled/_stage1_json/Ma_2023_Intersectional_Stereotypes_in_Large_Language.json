{
  "metadata": {
    "title": "Intersectional Stereotypes in Large Language Models: Dataset and Analysis",
    "authors": [
      "Weicheng Ma",
      "Brian Chiang",
      "Tong Wu",
      "Lili Wang",
      "Soroush Vosoughi"
    ],
    "year": 2023,
    "type": "conferencePaper",
    "language": "en"
  },
  "core": {
    "research_question": "Inwiefern propagieren Large Language Models intersektionale Stereotypen gegenüber Gruppen, die mehrere demografische Merkmale kombinieren?",
    "methodology": "Empirisch: Automatische Datengenerierung mit ChatGPT zur Erstellung eines Stereotypen-Datensatzes (14 demografische Merkmale über 6 Kategorien), manuelle Validierung durch 3 Annotatoren (98,33% Übereinstimmung), anschließende Prüfung von GPT-3 und ChatGPT mittels simulierter Rollen und Stereotype Degree (SDeg)-Messung über 16 Stereotypenkategorien.",
    "key_finding": "Trotz Debiasing-Maßnahmen zeigen moderne LLMs (GPT-3, ChatGPT) komplexe intersektionale Stereotypen gegenüber 106 verschiedenen Gruppen, mit unterschiedlichen Stereotypisierungsmustern pro Modell, was spezifische Mitigationsmaßnahmen erfordert.",
    "data_basis": "Automatisch generierter Datensatz von 478 Stereotypen (4,53 durchschnittlich pro Gruppe) für 106 intersektionale Gruppen, validiert durch 3 menschliche Annotatoren; Test mit 10 simulierten Rollen pro intersektionaler Gruppe"
  },
  "arguments": [
    "Bisherige Stereotypen-Forschung in LLMs fokussiert isolierte Kategorien (Rasse, Geschlecht), ignoriert aber intersektionale Effekte, wo sich mehrere marginalisierte Identitäten überschneiden und eigenständige, komplexere Stereotype erzeugen.",
    "ChatGPT kann effektiv zur Generierung realistischer Stereotypen bis zu vier intersektionalen Merkmalen eingesetzt werden, wenn durch Prompt-Design (Problem Statement, Regulation, Disclaimer) und nachfolgende Validierung Halluzinationen und Overgeneralisierung minimiert werden.",
    "Unterschiedliche LLMs entwickeln spezifische intersektionale Bias-Muster (z.B. GPT-3 stärker bei 'junge schwarze Menschen', ChatGPT bei 'schwarze Menschen ohne Behinderung'), daher ist eine differenzierte, modell- und gruppenspezifische Debiasing-Strategie notwendig."
  ],
  "categories": {
    "AI_Literacies": false,
    "Generative_KI": true,
    "Prompting": true,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "Generative_KI": "Das Paper untersucht explizit zwei generative LLMs: 'we probed the presence of stereotypes within two contemporary LLMs, GPT-3 (Brown et al., 2020) and ChatGPT (GPT-3.5)' und nutzt ChatGPT zur Datengenerierung.",
    "Prompting": "Detailliertes Prompt-Design mit drei Komponenten: 'The design of our prompts, which are used to retrieve stereotypes from ChatGPT, encompasses three key components: the problem statement, regulation, and disclaimer.' (Section 2.2)",
    "KI_Sonstige": "NLP-Analyse, Stereotypen in Sprachmodellen, Untersuchung von Bias in trainierten Modellen über Generierungsaufgaben.",
    "Bias_Ungleichheit": "Zentrales Thema: 'Despite many stereotypes targeting intersectional demographic groups, prior studies on stereotypes within Large Language Models (LLMs) primarily focus on broader, individual categories.' Die Paper dokumentiert systematisch Diskriminierungsmuster.",
    "Diversitaet": "Expliziter Fokus auf intersektionale Gruppen: 'we significantly broaden our scope by considering six demographic categories: race (white, black, and Asian), age (young and old), religion (non-religious, Christian, and Muslim), gender (men and women), political leaning (conservative and progressive), and disability status'",
    "Fairness": "Evaluation algorithmischer Fairness durch Stereotype Degree (SDeg)-Metriken zur Quantifizierung von Stereotypisierungsgrad in LLMs: 'We quantify the stereotype exhibited by the LLM by examining the maximum frequency with which the ten responses generated by the LLM match each expected answer.'"
  },
  "references": [
    {
      "author": "Cheng et al.",
      "year": 2023,
      "short_title": "Marked personas: Using natural language prompts to measure stereotypes in language models"
    },
    {
      "author": "Cao et al.",
      "year": 2022,
      "short_title": "Theory-grounded measurement of U.S. social stereotypes in English language models"
    },
    {
      "author": "Hassan et al.",
      "year": 2021,
      "short_title": "Unpacking the interdependent systems of discrimination: Ableist bias in nlp systems through an intersectional lens"
    },
    {
      "author": "Zhao et al.",
      "year": 2018,
      "short_title": "Gender bias in coreference resolution: Evaluation and debiasing methods"
    },
    {
      "author": "Nadeem et al.",
      "year": 2021,
      "short_title": "StereoSet: Measuring stereotypical bias in pretrained language models"
    },
    {
      "author": "Nangia et al.",
      "year": 2020,
      "short_title": "CrowS-pairs: A challenge dataset for measuring social biases in masked language models"
    },
    {
      "author": "Tan & Celis",
      "year": 2019,
      "short_title": "Assessing social and intersectional biases in contextualized word representations"
    },
    {
      "author": "Rudinger et al.",
      "year": 2018,
      "short_title": "Gender bias in coreference resolution"
    },
    {
      "author": "Brown et al.",
      "year": 2020,
      "short_title": "Language models are few-shot learners"
    },
    {
      "author": "Ferrara",
      "year": 2023,
      "short_title": "Should chatgpt be biased? challenges and risks of bias in large language models"
    }
  ],
  "assessment": {
    "domain_fit": "Das Paper ist für die Schnittstelle KI und Fairness hochrelevant, da es systematisch aufzeigt, wie moderne generative KI-Systeme strukturelle Diskriminierungsmuster gegen marginalisierte, intersektionale Gruppen propagieren. Die Erkenntnisse haben direkte Implikationen für die verantwortungsvolle Entwicklung und den Einsatz von KI-Systemen.",
    "unique_contribution": "Das Paper schließt eine Forschungslücke durch die erste umfassende, intersektional differenzierte Untersuchung von Stereotypen in LLMs (6 demografische Kategorien, bis zu 4+ intersektionale Merkmale, 16 Stereotypenkategorien) und demonstriert zugleich eine innovative Methodik zur Datengenerierung und -validierung mithilfe von LLMs selbst.",
    "limitations": "Das Paper weist selbst darauf hin, dass ChatGPT als Datengenerierungs-Tool möglicherweise eigene 'Standpunkte' und soziale Werte in die generierten Stereotypen einbringt; zudem funktioniert die automatische Generierung ab 5+ intersektionalen Merkmalen schlecht, was größere Kombinationen ausschließt."
  },
  "target_group": "KI-Entwickler und Machine Learning-Forscher (Debiasing-Strategien), NLP-Community, Policy-Maker im Bereich KI-Regulierung und Fairness, sowie Organisationen, die Large Language Models evaluieren und einsetzen (Compliance, Risikobewertung)"
}