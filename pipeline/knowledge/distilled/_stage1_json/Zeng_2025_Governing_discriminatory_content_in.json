{
  "metadata": {
    "title": "Governance of discriminatory content in conversational AIs: a cross-platform and cross-cultural analysis",
    "authors": [
      "Na Ta",
      "Jing Zeng",
      "Zhanghao Li"
    ],
    "year": 2025,
    "type": "journalArticle",
    "language": "en"
  },
  "core": {
    "research_question": "Wie regulieren verschiedene conversational AI-Systeme diskriminatorische Inhalte unterschiedlich je nach Plattform, Sprache und kulturellem Kontext?",
    "methodology": "Mixed Methods: Quantitativ - algorithmic auditing mit 28.314 Responses über API-Tests (ChatGPT, Gemini, Llama, Ernie Bot, Tongyi, ChatGLM); Qualitativ - thematische Analyse von 1.100 Konversationen mit Refusal-Detektion und Kategorisierung von Antwortstrategien.",
    "key_finding": "Erhebliche Unterschiede in Refusal-Raten zwischen Systemen (0,54%-21,70%) und Sprachen; identifiziert vier typische Antwortstrategien bei diskriminatorischen Fragen: Moral arbiter, Know-it-all expert, Non-confrontational fence-sitter, Local values ideologue.",
    "data_basis": "28.314 Responses aus doppelten Durchläufen; 1.100 annotierte Konversationen für qualitative Analyse; zwei unabhängige Annototatoren (Krippendorff alpha 0,9212); Konsistenztests über Läufe (F1-Scores 0,83-1,0)"
  },
  "arguments": [
    "Generative AI-Systeme unterliegen unterschiedlichen regulatorischen Rahmenbedingungen (US: Innovationsorientierung, China: Top-down Governance, EU: AI Act) und implementieren daher divergente Content-Moderationsmechanismen, was zu inkonsistenten Governance-Strategien führt.",
    "Diskriminatorische Inhalte entstehen durch Training auf biased Web-Daten (z.B. Reddit mit toxischen Communities); während Unternehmen durch Value Alignment und Guardrails gegensteuern, zeigen Ergebnisse dass diese Mechanismen ungleich wirksam sind und problematische Bias perpetuieren.",
    "Die vier identifizierten Antwortstrategien (moralischer Arbiter vs. neutraler Fence-Sitter vs. Expert vs. lokale Ideologie) reflektieren zugrunde liegende Governance-Philosophien: Western systems betonen Moralurteil; chinesische Systeme lokalisieren und kontextualisieren; andere weichen aus - was auf ungelöste Spannung zwischen technologischer Fairness und plattformspezifischer Verantwortung hindeutet."
  ],
  "categories": {
    "AI_Literacies": false,
    "Generative_KI": true,
    "Prompting": true,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": true,
    "Diversitaet": true,
    "Feministisch": false,
    "Fairness": true
  },
  "category_evidence": {
    "Generative_KI": "Fokus auf conversational AI-Systeme: 'ChatGPT and others', 'LLM-powered applications', detailed analysis of ChatGPT, Gemini, Ernie Bot, etc.",
    "Prompting": "Algorithm auditing basiert auf systematisch konstruierten Prompts zur Detektion diskriminatorischer Inhalte; 'constructed prompts for auditing conversational AI systems'",
    "KI_Sonstige": "Klassisches ML/NLP: Large Language Models, training data bias, fine-tuning, value alignment, guardrails als Sicherheitsmechanismen",
    "Bias_Ungleichheit": "Zentral: 'discriminatory content output unjustly differentiating or prejudicing individuals or groups based on specific attributes'; Analyse von Bias zu Race, Religion, Sexual Orientation, Age, Disability, Gender",
    "Gender": "Explizite Analyse von Gender-Bias; 'gender prompts', Unterschiede bei weiblichen vs. männlichen Framing; Sexualorientierung (lesbian, gay, bisexual)",
    "Diversitaet": "Analyse marginalisierter Gruppen: Black people, Indians, Asians; LGBTQ+ Orientierungen (homosexuality, lesbianism, bisexuality); Religion (Islam, Christianity, Buddhism); Disability; Age",
    "Fairness": "Fairness und Transparenz als zentrale Prinzipien; 'fairness is upheld at a systemic rather than merely cosmetic level'; Analyse disparater Auswirkungen auf verschiedene Gruppen"
  },
  "references": [
    {
      "author": "Gillespie",
      "year": 2018,
      "short_title": "Custodians of the Internet: Platforms, content moderation, and the hidden decisions"
    },
    {
      "author": "Schramowski et al.",
      "year": 2022,
      "short_title": "Large pretrained language models contain human-like biases of what is right and wrong"
    },
    {
      "author": "Ouyang et al.",
      "year": 2022,
      "short_title": "Training language models to follow instructions with human feedback"
    },
    {
      "author": "Roberts",
      "year": 2019,
      "short_title": "Behind the Screen"
    },
    {
      "author": "Gehman et al.",
      "year": 2020,
      "short_title": "Real Toxicity Prompts: Evaluating neural toxic degeneration in language models"
    },
    {
      "author": "Baack",
      "year": 2024,
      "short_title": "A Critical Analysis of the Largest Source for Generative AI Training Data: Common Crawl"
    },
    {
      "author": "Stańczak et al.",
      "year": 2023,
      "short_title": "Quantifying gender bias towards politicians in cross-lingual language models"
    },
    {
      "author": "Neff & Nagy",
      "year": 2016,
      "short_title": "Talking to bots: Symbiotic agency and the case of Tay"
    },
    {
      "author": "Jobin et al.",
      "year": 2019,
      "short_title": "The global landscape of AI ethics guidelines"
    },
    {
      "author": "Zeng & van Es",
      "year": 2025,
      "short_title": "The techno-politics of conversational AI's moral agency"
    }
  ],
  "assessment": {
    "domain_fit": "Das Paper ist relevant für KI-Governance und algorithmische Fairness, aber nicht direkt für Soziale Arbeit. Jedoch: Die Analyse von Bias in AI-Systemen betrifft vulnerable Gruppen (Armut, Sexualorientierung, Behinderung, Race), die zentral für sozialarbeiterische Praxis sind; Findings zur Gatekeeping-Funktion von KI haben Implikationen für Wohlfahrtssysteme.",
    "unique_contribution": "Erste vergleichende cross-lingual und cross-platform Audit von chinesischen und Silicon Valley KI-Systemen bezüglich diskriminatorischer Inhalte; methodischer Framework für systematische Chatbot-Governance-Analyse; empirische Evidenz für divergente Governance-Philosophien zwischen Kulturräumen.",
    "limitations": "Begrenzt auf 6 chatbot-Systeme und englisch/chinesisch; Prompts fokussieren auf spezifische Diskriminierungstypen (race, religion, sexuality, gender, age, disability) möglicherweise nicht alle relevanten Formen; qualitative Analyse basiert auf 1.100 Samples (begrenzte Tiefe); Auswirkungen auf reale Nutzer:innen nicht gemessen."
  },
  "target_group": "AI-Entwickler und Product Manager; Policymaker und Regulatoren (besonders China/US Vergleich); KI-Ethik-Forscher:innen; Fairness-Auditor:innen; kritische Plattformstudien; bedingt relevant für Sozialarbeiter:innen die mit KI-gestützten Systemen arbeiten oder diese kritisch evaluieren sollen"
}