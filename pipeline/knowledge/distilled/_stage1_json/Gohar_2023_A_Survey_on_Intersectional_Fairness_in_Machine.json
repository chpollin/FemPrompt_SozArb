{
  "metadata": {
    "title": "A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges",
    "authors": [
      "Usman Gohar",
      "Lu Cheng"
    ],
    "year": 2023,
    "type": "conferencePaper",
    "language": "en"
  },
  "core": {
    "research_question": "Wie können wir Intersektionale Fairness in Machine-Learning-Systemen verstehen, messen und mitigieren?",
    "methodology": "Literaturreview und systematische Taxonomie von Fairness-Notionen und Mitigationstechniken",
    "key_finding": "Der Survey präsentiert die erste umfassende Taxonomie für intersektionale Fairness-Notionen und Fair-Learning-Methoden und identifiziert zentrale Herausforderungen wie Data Sparsity bei kleineren Subgruppen und die Unzulänglichkeit von Mittigationstechniken, die auf unabhängige Gruppen optimiert sind.",
    "data_basis": "nicht empirisch; Synthese von 90+ wissenschaftlichen Arbeiten zur intersektionalen Fairness in ML"
  },
  "arguments": [
    "Intersektionale Fairness ist fundamentaler als traditionelle Group Fairness, da die Diskriminierungserfahrung von Individuen an der Schnittmenge mehrerer geschützter Attribute (z.B. Rasse und Geschlecht) qualitativ unterschiedlich ist und nicht durch Fairness auf individuellen Dimensionen erfasst wird.",
    "Existing Fairness-Notionen (Statistical Parity, Equality of Opportunity) können auf intersektionalen Gruppen unfair sein, wie Buolamwini & Gebru (2018) mit Gender Shades demonstrierten, wo Black Women signifikant höhere Fehlerquoten in Gesichtserkennungssystemen erlebten als andere Gruppen.",
    "Die Mitigation intersektionaler Bias erfordert spezialisierte technische Ansätze wie Subgroup Fairness, Multicalibration und Differential Fairness, die jeweils unterschiedliche Trade-offs zwischen Fairness-Garantien, Recheneffizienz und Data-Sparsity-Problemen aufweisen."
  ],
  "categories": {
    "AI_Literacies": false,
    "Generative_KI": false,
    "Prompting": false,
    "KI_Sonstige": true,
    "Soziale_Arbeit": false,
    "Bias_Ungleichheit": true,
    "Gender": true,
    "Diversitaet": true,
    "Feministisch": true,
    "Fairness": true
  },
  "category_evidence": {
    "KI_Sonstige": "Survey fokussiert auf Machine Learning Systeme, algorithmische Entscheidungssysteme in hochriskanten Anwendungen (criminal sentencing, bank loans, hiring decisions), sowie NLP und Ranking-Systeme.",
    "Bias_Ungleichheit": "Zentral ist die Analyse von algorithmischen Diskriminierungen: 'Machine learning (ML) has been increasingly used in high-stake applications such as loans, criminal sentencing, and hiring decisions with reported fairness implications for different demographic groups'",
    "Gender": "Explicit gender focus: 'Black woman's experience of discrimination differs from both women and Black people in general' und extensive Diskussion von Gender-Bias in NLP-Modellen (BERT, GPT-2), Sentiment Analysis und Gender Classification.",
    "Diversitaet": "Intersektionalität als Kernkonzept: Paper diskutiert Repräsentation marginalisierter Gruppen an Schnittmengen mehrerer Identitätsdimensionen (race, gender, disability, sexuality, religion) und deren Underrepresentation in Daten.",
    "Feministisch": "Explizit auf Crenshaw (1989) Intersektionalitätstheorie aufbauend: 'recent works have identified a more nuanced case of group unfairness that spans multiple subgroups based on Crenshaw's theory of intersectionality called intersectional group fairness'. Intersektionalität ist ursprünglich eine feministische kritische Theorie.",
    "Fairness": "Kernthema: umfassende Taxonomie von Fairness-Notionen (Subgroup Fairness, Multicalibration, Multiaccuracy, Differential Fairness, Max-Min Fairness, Metric-based Fairness) mit mathematischen Definitionen und Mitigationsmethoden."
  },
  "references": [
    {
      "author": "Crenshaw",
      "year": 1989,
      "short_title": "Demarginalizing the Intersection of Race and Sex"
    },
    {
      "author": "Buolamwini & Gebru",
      "year": 2018,
      "short_title": "Gender Shades: Intersectional Accuracy Disparities"
    },
    {
      "author": "Kearns et al.",
      "year": 2018,
      "short_title": "Preventing Fairness Gerrymandering: Subgroup Fairness"
    },
    {
      "author": "Hebert-Johnson et al.",
      "year": 2018,
      "short_title": "Multicalibration: Calibration for the Computationally-Identifiable Masses"
    },
    {
      "author": "Foulds et al.",
      "year": 2020,
      "short_title": "An Intersectional Definition of Fairness (Differential Fairness)"
    },
    {
      "author": "Tan & Celis",
      "year": 2019,
      "short_title": "Assessing Social and Intersectional Biases in Contextualized Word Representations"
    },
    {
      "author": "Mehrabi et al.",
      "year": 2021,
      "short_title": "A Survey on Bias and Fairness in Machine Learning"
    },
    {
      "author": "Dwork et al.",
      "year": 2012,
      "short_title": "Fairness through Awareness"
    },
    {
      "author": "Hashimoto et al.",
      "year": 2018,
      "short_title": "Fairness without Demographics through Distributionally Robust Optimization"
    },
    {
      "author": "Kirk et al.",
      "year": 2021,
      "short_title": "Bias Out-of-the-Box: Intersectional Occupational Biases in Language Models"
    }
  ],
  "assessment": {
    "domain_fit": "Hochrelevant für die Schnittmenge AI/Gender Studies/Diversität, aber mit begrenzter direkter Relevanz für Soziale Arbeit, da der Paper rein technisch-algorithmisch fokussiert und nicht auf Implementierung in Sozialen Diensten oder Arbeit mit vulnerablen Gruppen eingeht.",
    "unique_contribution": "Erste umfassende Taxonomie intersektionaler Fairness-Notionen mit kritischer Analyse ihrer Limitationen und ein systematischer Überblick über State-of-the-Art-Mitigationstechniken sowie offene Forschungsfragen.",
    "limitations": "Paper ist rein theoretisch-konzeptuell; empirische Evaluationen der Fairness-Notionen auf realen Datensätzen sind begrenzt; wenig Behandlung von Kontextfaktoren jenseits statistischer Fairness; keine Integration qualitativer/sozialer Perspektiven von betroffenen Gruppen."
  },
  "target_group": "ML-Forscher und -Entwickler, AI-Ethiker, Policymaker im Tech-Bereich, Gender Studies und Diversity-Spezialist:innen; begrenzt relevant für Sozialarbeiter:innen ohne technischen Hintergrund"
}