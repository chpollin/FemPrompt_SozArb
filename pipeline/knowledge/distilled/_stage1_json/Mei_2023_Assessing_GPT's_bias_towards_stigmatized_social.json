{
  "metadata": {
    "title": "Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia",
    "authors": [
      "Kashif, Afifah",
      "Patel, Heer"
    ],
    "year": 2023,
    "type": "conferencePaper",
    "language": "en"
  },
  "core": {
    "research_question": "Wie intersektieren sich GPT-Vorurteile gegenüber Nationalität und psychischen Erkrankungen und wie beeinflussen sie marginalisierte Gruppen?",
    "methodology": "Empirisch; strukturierte Prompt-Serie mit systematischer Variation von zwei Nationalitäten (USA, Nordkorea) und sechs psychischen Erkrankungen über fünf Alltagsszenarien, Likert-Skalen-Bewertungen (regulär und invertiert) sowie qualitative Analyse von Modell-Erklärungen.",
    "key_finding": "GPT-Modelle zeigen systematisch stärkere negative Vorurteile gegenüber Nordkoreaner:innen im Vergleich zu Amerikaner:innen, insbesondere bei Kombination mit psychischen Erkrankungen. Die Modelle zeigen zudem Inkonsistenzen bei invertierten Skalen und treffen implizite kulturelle Annahmen.",
    "data_basis": "Systematische Prompt-Tests mit GPT-3.5, GPT-4 und GPT-4o (240 Prompts in Stufen 1-2 à 120 Prompts, 2 Prompts Stufe 3), 6 mentale Erkrankungskombinationen (bipolar, Depression, Schizophrenie - jeweils remittiert und symptomatisch), 5 Szenarien."
  },
  "arguments": [
    "GPT-Modelle zeigen einen USA-zentrisch geprägten Bias und verweisen bei Nordkoreaner:innen explizit auf 'kulturelle Unterschiede', während solche Annahmen bei Amerikaner:innen nicht gemacht werden, was auf tiefere nationale Stereotypisierung hindeutet.",
    "Psychische Erkrankungen werden von GPT-Modellen nicht als Spektrum mit graduellen Symptomen diskutiert, sondern absolutistisch, was zu pauschaler Stigmatisierung führt und intersektionale Nuancierungen ignoriert.",
    "Numerische Inkonsistenzen bei Skalierung und Neu-Prompting zeigen systemische Unreliabilität in der Bewertung, die die Validität von LLM-basierten Bias-Metriken in Frage stellt und methodologische Vorsicht erfordert."
  ],
  "categories": {
    "AI_Literacies": false,
    "Generative_KI": true,
    "Prompting": true,
    "KI_Sonstige": true,
    "Soziale_Arbeit": true,
    "Bias_Ungleichheit": true,
    "Gender": false,
    "Diversitaet": true,
    "Feministisch": true,
    "Fairness": true
  },
  "category_evidence": {
    "Generative_KI": "Fokus auf GPT-3.5/4/4o LLMs: 'Recent studies have separately highlighted significant biases within foundational large language models (LLMs) against certain nationalities and stigmatized social groups.'",
    "Prompting": "Strukturierte Prompt-Engineering mit Likert-Skalen und invertierten Skalen: 'We prompt GPT-3.5, GPT-4, and GPT-4o in three steps... Ask to answer the question with a Likert scale... Re-prompt with a flipped Likert scale.'",
    "KI_Sonstige": "NLP und algorithmische Bias-Analyse: 'evaluates model responses to several scenarios involving American and North Korean nationalities with various mental disabilities'",
    "Soziale_Arbeit": "Thematisiert vulnerable Gruppen (Menschen mit psychischen Erkrankungen, Migrant:innen) und alltägliche soziale Interaktionen (Wohnen, Kinderbetreuung, Arbeit): 'our prompts assess GPT's bias in everyday scenarios like renting, cohabiting, working, childcare, and marriage'",
    "Bias_Ungleichheit": "Explizite Analyse struktureller Diskriminierung: 'Findings reveal significant discrepancies in empathy levels with North Koreans facing greater negative bias, particularly when mental disability is also a factor. This underscores the need for improvements in LLMs designed with a nuanced understanding of intersectional identity.'",
    "Diversitaet": "Intersektionale Perspektive mit explizitem Fokus auf marginalisierte Gruppen: 'This research seeks to explore the intersectional nature of LLM biases... how do GPT biases towards nationality and mental disabilities intersect and affect marginalized groups?'",
    "Feministisch": "Verwendung intersektionaler Theorie und Perspektive auf marginalisierte Identitäten, die in der feministischen Kritik sozialer Systeme verankert ist: 'apply a novel approach in using an intersectional lens towards nationality prejudice and psychophobia'",
    "Fairness": "Evaluation von algorithmischer Fairness und Equity: 'evaluate model responses... Findings reveal significant discrepancies... underscores the need for improvements in LLMs designed with a nuanced understanding of intersectional identity... ensure equitable treatment of global users'"
  },
  "references": [
    {
      "author": "Jiang et al.",
      "year": 2022,
      "short_title": "Nationality Bias in Language Models"
    },
    {
      "author": "Mei et al.",
      "year": 2023,
      "short_title": "Bias against Stigmatized Groups in Language Models"
    },
    {
      "author": "Bianchi et al.",
      "year": 2023,
      "short_title": "Demographic Stereotypes in Text-to-Image Generation"
    },
    {
      "author": "Caliskan et al.",
      "year": 2017,
      "short_title": "Semantics and Human-like Biases in Language Corpora"
    },
    {
      "author": "Charlesworth et al.",
      "year": 2022,
      "short_title": "Historical Representations of Social Groups in Word Embeddings"
    },
    {
      "author": "Guo & Caliskan",
      "year": 2021,
      "short_title": "Intersectional Biases in Contextualized Word Embeddings"
    },
    {
      "author": "Caliskan & Lewis",
      "year": 2022,
      "short_title": "Social Biases in Word Embeddings and Human Cognition"
    }
  ],
  "assessment": {
    "domain_fit": "Hochgradig relevant für die Schnittstelle AI/Soziale Arbeit/Gender: Das Paper analysiert systematisch, wie KI-Systeme vulnerable Gruppen diskriminieren, die zentral für Soziale Arbeit sind (Menschen mit psychischen Erkrankungen, Migrant:innen), und nutzt dabei intersektionale Perspektiven, die für kritische Sozialarbeitspraxis essentiell sind.",
    "unique_contribution": "Das Paper trägt eine novel intersektionale Analyse ein, die explizit die Verschränkung von Nationalität und psychischer Erkrankung untersucht und dabei grundsätzliche Inconsistenzen in LLM-Bias-Bewertungen aufdeckt, was bestehende Bias-Messungen in Frage stellt.",
    "limitations": "Kleine Prompt-Menge (240 Prompts), ausschließlich kommerzielle LLMs (keine Generalisierbarkeit auf andere Modelle), begrenzte kulturelle Diversität (nur USA/Nordkorea), Abhängigkeit von subjektiven Interpretationen von Bias und Skalierungsinvarianzen, keine Durchführung statistischer Signifikanztests."
  },
  "target_group": "KI-Ethiker:innen, Sozialarbeiter:innen in digitalisierten Kontexten, Policymaker in AI-Governance, Entwickler:innen von LLMs, Vertreter:innen marginalisierter Gruppen, kritische AI-Forscher:innen mit intersektionaler Perspektive"
}