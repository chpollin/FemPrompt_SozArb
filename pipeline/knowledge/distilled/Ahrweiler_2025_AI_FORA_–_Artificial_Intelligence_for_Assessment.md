---
title: "Participatory Artificial Intelligence in Public Social Services. From Bias to Fairness in Assessing Beneficiaries"
authors: ["Petra Ahrweiler"]
year: 2025
type: book
language: en
processed: 2026-02-04
source_file: Ahrweiler_2025_AI_FORA_–_Artificial_Intelligence_for_Assessment.md
confidence: 95
---

# Participatory Artificial Intelligence in Public Social Services. From Bias to Fairness in Assessing Beneficiaries

## Kernbefund

Gerechtigkeitskriterien für staatliche Leistungen sind kultur- und kontextabhängig, daher reicht ein standardisiertes KI-System nicht aus. Stattdessen sind flexible, adaptive Systeme notwendig, die unter Einbeziehung aller gesellschaftlichen Akteure, besonders vulnerabler Gruppen, entwickelt werden.

## Forschungsfrage

Wie kann Künstliche Intelligenz bei der Verteilung öffentlicher sozialer Leistungen weltweit fairer und gerechter gestaltet werden, unter Berücksichtigung kultureller und kontextabhängiger Fairnesskriterien?

## Methodik

Mixed Methods: Internationale vergleichende Fallstudien in neun Ländern auf vier Kontinenten kombiniert mit partizipativer Forschung; ergänzt durch Modellierungs- und Simulationsforschung
**Datenbasis:** Neun Länder auf vier Kontinenten (Deutschland, Spanien, Estland, Ukraine, USA, Nigeria, Iran, Indien, China); 3,5-jähriges Forschungsprojekt; Sammelband mit ca. 300 Seiten; partizipative Forschungsmethoden

## Hauptargumente

- Künstliche Intelligenz wird zunehmend weltweit bei sozialen Bewertungen eingesetzt (Renten, Arbeitslosengeld, Asyl, Kindergartenplätze), aber die zugrunde liegenden Fairnesskonzepte sind kulturabhängig (z.B. Kastensystem in Indien, Bürgerscore in China), weshalb standardisierte globale Systeme inadäquat sind.
- Innerhalb von Gesellschaften existieren unterschiedliche und ständig ausgehandelte Perspektiven auf Gerechtigkeit, die sich in der Technologie niederschlagen müssen - eine Ein-Größe-passt-allen-Lösung ist nicht möglich.
- Die Entwicklung fairer KI-Systeme für öffentliche Leistungsvergabe ist auf partizipative, kontextspezifische und inklusive Entwicklungsprozesse angewiesen, die die Perspektiven von vulnerablen und marginalisierten Gruppen einbeziehen.

## Kategorie-Evidenz

### Evidenz 1

Das Projekt befasst sich mit der Notwendigkeit, dass 'alle gesellschaftlichen Akteure' zur 'Gestaltung von partizipativer, kontextspezifischer und fairer KI' beitragen müssen, was kritisches Verständnis und Kompetenzen im Umgang mit KI erfordert.

### Evidenz 2

Das Projekt untersucht KI-Systeme für automatisierte soziale Bewertungen und algorithmische Entscheidungsfindung in der öffentlichen Verwaltung.

### Evidenz 3

Der direkte Fokus liegt auf der Verteilung öffentlicher sozialer Leistungen (Renten, Arbeitslosengeld, Asylbewilligung, Kindergartenplätze) und damit auf Kernbereichen der Sozialen Arbeit und sozialer Dienste.

### Evidenz 4

Das Projekt adressiert explizit 'Diskriminierungsprobleme' ('Bias'), strukturelle Benachteiligung und die Tatsache, dass KI-Systeme unterschiedliche Gruppen ungleich behandeln können je nach Fairnesskonzept.

### Evidenz 5

Die Studie untersucht neun Länder mit unterschiedlichen kulturellen, politischen und sozialen Kontexten und betont die Notwendigkeit, 'vulnerable Gruppen' und die Perspektiven von 'marginalisierten' Akteuren in die KI-Entwicklung einzubeziehen.

### Evidenz 6

Das gesamte Projekt konzentriert sich auf 'Fairnesskriterien' und 'Fairness bei der Verteilung', wobei die Fallstudien zeigen, dass Fairnesskonzepte kultur- und kontextabhängig sind und nicht standardisiert sein können.

## Assessment-Relevanz

**Domain Fit:** Hochgradig relevant für die Schnittstelle KI-Systeme und Soziale Arbeit. Das Paper adressiert direkt, wie KI-Systeme in sozialen Diensten fairere und gerechtere Entscheidungen treffen können und betont dabei die Notwendigkeit partizipativer, kontextsensibler Entwicklung unter Einbeziehung vulnerabler Gruppen.

**Unique Contribution:** Das Projekt bietet einen seltenen internationalen, vergleichenden Überblick (9 Länder, 4 Kontinente) über KI-Systeme in sozialen Bewertungen und entwickelt ein theoretisches und praktisches Rahmenwerk für kontextabhängige, adaptive und partizipative KI-Systeme statt standardisierter Lösungen.

**Limitations:** Als Pressemitteilung sind Methodische Details begrenzt; die vollständige methodische Tiefe wird erst in den Publikationen (Sammelband und folgendes Buch) erkennbar.

**Target Group:** Policymaker und Administratoren in sozialen Diensten, KI-Entwickler und Data Scientists im öffentlichen Sektor, Sozialarbeiter und Fachkräfte in sozialen Diensten, Vertreter von marginalisierten Gruppen und Interessenverbänden, Wissenschaftler im Bereich KI, Soziale Arbeit, und Gerechtigkeit

## Schlüsselreferenzen

- [[Ahrweiler_Petra_Ed_2025]] - Participatory Artificial Intelligence in Public Social Services. From Bias to Fairness in Assessing Beneficiaries
