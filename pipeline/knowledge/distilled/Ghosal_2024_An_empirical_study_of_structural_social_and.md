---
title: "Intersectional analysis of visual generative AI: the case of stable diffusion"
authors: ["Petra Jääskeläinen", "Nickhil Kumar Sharma", "Helen Pallett", "Cecilia Åsberg"]
year: 2025
type: journalArticle
language: en
categories:
  - AI_Literacies
  - Generative_KI
  - Prompting
  - Soziale_Arbeit
  - Bias_Ungleichheit
  - Gender
  - Diversitaet
  - Feministisch
  - Fairness
processed: 2026-02-05
source_file: Ghosal_2024_An_empirical_study_of_structural_social_and.md
confidence: 95
---

# Intersectional analysis of visual generative AI: the case of stable diffusion

## Kernbefund

Stable Diffusion perpetuiert bestehende Machtsysteme wie Sexismus, Rassismus, Heteronormativität und Ableismus, indem es einen Standardindividuum als weiß, körperlich fit und maskulin präsentierend darstellt. Die hegemonialen kulturellen Werte in der Bildgeneration lassen sich auf institutionelle Kontexte zurückführen, insbesondere auf Euro- und Nordamerika-zentrische Perspektiven.

## Forschungsfrage

Wie werden verschiedene soziale Kategorien ästhetisch in Stable Diffusion-generierten Bildern dargestellt, wie verstärken Institutionen Marginalisierung bestimmter sozialer Gruppen, und wie intersektieren sich Machtsysteme wie Rassismus, Kolonialismus und Kapitalismus in der visuellen Ästhetik dieser Technologie?

## Methodik

Empirisch-qualitativ: Visuelle und intersektionale Analyse von 180 mit Stable Diffusion generierten Bildern unter Verwendung von Methoden der feministischen Science and Technology Studies, intersektionalen kritischen Theorie und feministischen Medien- und Visual Culture Studies. Interpretative Analyse auf Mikro-, Meso- und Makro-Ebene.
**Datenbasis:** n=180 Stable Diffusion-generierte Bilder; ergänzt durch Literaturanalyse und Online-Quellen zu institutionellen Kontexten

## Hauptargumente

- Visual Generative AI-Systeme wie Stable Diffusion funktionieren nicht als kulturell oder ästhetisch neutrale Werkzeuge, sondern spiegeln und verstärken aktiv gesellschaftliche Machtdynamiken, Machtverhältnisse und tiefgreifende Annahmen über Kategorialisierung, Werte und ästhetische Repräsentationen wider.
- Intersektionale Analysen offenbaren Muster, die eindimensionale Analysen (z.B. nur Geschlecht oder nur Rasse) verbergen würden: SD-Bilder zeigen typischerweise nur eine Charakteristik vom hegemonialen Norm ab, nicht mehrere, was die Ausblendung von komplexen intersektionalen Identitäten perpetuiert.
- Institutionelle Kontexte - insbesondere die Verankerung in Global-North-Strukturen, die BigTech-Kultur der USA, fragwürdige Unternehmensethik und die Abhängigkeit von Trainingsdaten (LAION-5B) - tragen systematisch zur Marginalisierung bestimmter sozialer Gruppen bei und ermöglichen die Fortführung von Ungerechtigkeit durch Technologie.

## Kategorie-Evidenz

### AI_Literacies

Die Autor:innen argumentieren für 'critical examination and careful analysis' von vGenAI und betonen 'the need for acknowledging and rendering visible the cultural-aesthetic politics of this technology', was direkt auf kritische KI-Medienkompetenz abzielt.

### Generative_KI

Fokus auf Stable Diffusion (SDXL), einen visual generative AI-Tool, mit Analyse von 180 generierten Bildern und Untersuchung von vGenAI-Technologien allgemein (DALL-E, Midjourney).

### Prompting

Detaillierte Analyse verschiedener Prompting-Strategien: 'A color photograph of a conservative', 'A color photograph of a feminist', 'nurse' vs. 'Indian nurse', Variationen bei 'people eating' und kulturspezifischen Begriffen wie 'Fufu'.

### Soziale_Arbeit

Das Paper adressiert vulnerable Gruppen und soziale Gerechtigkeit, mit explizitem Fokus auf 'a reparative and social justice-oriented approach to vGenAI', relevant für sozialarbeiterische Praxis und kritische Theorie der Sozialen Arbeit.

### Bias_Ungleichheit

Zentral sind Analysen von algorithmischem Bias, struktureller Benachteiligung, und wie 'the power systems around SD result in the continual reproduction of harmful and violent imagery', einschließlich Rassismus, Sexismus, Ableismus und Heteronormativität.

### Gender

Expliziter Gender-Fokus: 'heteronormative masculine/feminine representations' werden systematisch analysiert; Befunde zeigen 'heteronormative masculine representations' in hochbezahlten Berufen und 'heteronormative feminine representations' in niedrigbezahlten Positionen.

### Diversitaet

Intersektionale Repräsentationsanalyse: 'wealthy people of color', 'transmasculine feminists', 'feminists of color', Menschen mit Behinderungen, Global South-Communities. Kritik an oberflächlichen Diversity-Initiativen.

### Feministisch

Explizite Verwendung feministischer STS (Wajcman, D'Ignazio & Klein, Adkins), intersektionaler kritischer Theorie (Crenshaw), feministischer Medien- und Visual Culture Studies (Evans, Hall, Sturken). Konzepte wie 'coded gaze' (von Buolamwini, abgeleitet vom 'male gaze'). Ansatz als 'intersectional feminist approach'.

### Fairness

Kritik an fehlender 'transparency and fairness', 'algorithmic injustice', und Forderung nach 'reparative approaches' und 'restorative justice', sowie Diskussion von Accountability-Mechanismen wie 'assigning criminal culpability for AI imagery-related harms'.

## Assessment-Relevanz

**Domain Fit:** Das Paper ist hochrelevant für die Schnittstelle KI/Soziale Arbeit/Gender Studies. Es verbindet kritische KI-Analyse mit explizit sozialjustizieller Perspektive, intersektionaler Theorie und einer Kritik an Technologie als Reproduktionsmittel struktureller Ungleichheit - zentral für kritische Soziale Arbeit.

**Unique Contribution:** Erste empirische, umfassende intersektionale Analyse einer spezifischen vGenAI-Technologie (Stable Diffusion XL) mit Fokus auf Verflechtung von Macht-, Diskriminierungs- und Kolonialisierungssystemen auf Mikro-, Meso- und Makro-Ebene.

**Limitations:** Analyse limitiert auf 180 Bilder und Stable Diffusion SDXL (nicht auf andere vGenAI-Systeme verallgemeinerbar); qualitative Interpretationsmethode ohne quantitative Validierung; keine direkten Angaben zu intersektionalen Subgruppenanalysen (z.B. Vergleiche zwischen verschiedenen Marginalisierungsachsen zahlenmäßig).

**Target Group:** Kritische KI-Forscher:innen, Sozialarbeiter:innen, Gender Studies und Diversity-Spezialist:innen, KI-Entwickler:innen und Designforschende mit Interesse an anti-oppressiver Technologiegestaltung, Policymakers und Regulatoren, Künstler:innen und kreative Fachkräfte, Aktivist:innen im Bereich Algorithmic Justice und sozialer Gerechtigkeit.

## Schlüsselreferenzen

- [[Buolamwini_2019]] - Gender Shades und Coded Gaze
- [[DIgnazio_Klein_2020]] - Data Feminism
- [[Crenshaw_1989]] - Intersectionality
- [[Benjamin_2019]] - Race After Technology
- [[Wajcman_2010]] - Feminist theories of technology
- [[Hall_1997]] - The spectacle of the other
- [[Bender_et_al_2021]] - On the Dangers of Stochastic Parrots
- [[Noble_2018]] - Algorithms of Oppression
- [[Sturken_Cartwright_2017]] - Practices of Looking
- [[Luccioni_et_al_2023]] - Stable Bias: Analyzing Societal Representations in Diffusion Models
