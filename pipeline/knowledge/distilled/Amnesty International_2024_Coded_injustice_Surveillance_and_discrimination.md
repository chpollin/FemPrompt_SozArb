---
title: "Coded Injustice: Surveillance and Discrimination in Denmark's Automated Welfare State"
authors: ["Amnesty International"]
year: 2024
type: report
language: en
categories:
  - AI_Literacies
  - KI_Sonstige
  - Soziale_Arbeit
  - Bias_Ungleichheit
  - Gender
  - Diversitaet
  - Fairness
processed: 2026-02-04
source_file: Amnesty International_2024_Coded_injustice_Surveillance_and_discrimination.md
confidence: 95
---

# Coded Injustice: Surveillance and Discrimination in Denmark's Automated Welfare State

## Kernbefund

Dänemarks automatisiertes Wohlfahrtssystem nutzt diskriminierende Algorithmen zur Betrugserkennung, die systematisch gegen marginalisierte Gruppen (Migranten, Menschen mit Behinderungen, Frauen, Arme) diskriminieren und fundamentale Menschenrechte verletzen, während gleichzeitig Transparenz und Rechenschaftspflicht völlig fehlen.

## Forschungsfrage

Wie untergraben KI-gestützte Betrugskontrollalgorithmen in Dänemarks Sozialhilfesystem Menschenrechte durch Überwachung und Diskriminierung marginalisierter Gruppen?

## Methodik

Mixed Methods: Dokumentenanalyse, Interviews mit Betroffenen, Freedom of Information Requests (FOIs), Analyse von Algorithmen und deren Auswirkungen auf Menschenrechte, qualitative Fallstudien
**Datenbasis:** Qualitative Analyse von Udbetaling Danmark (UDK)-Systemen, FOI-Anfragen zu Algorithmen-Performance, Interviews mit Betroffenen, Analyse von 2023 Kontrollstatistiken (412 Fälle 'Really Single' Modell mit 292 etablierten Kontrollvorgängen)

## Hauptargumente

- Dänemarks Fusion großer Regierungsdatenbanken für Betrugskontrolle schafft ein Massenüberwachungssystem, das die Privatsphäre von Sozialhilfeempfängern fundamental verletzt, indem es ein 360-Grad-Profil ihrer gesamten Lebenssituation erstellt, ohne angemessene Transparenz oder Rechenschaftsmechanismen.
- Die verwendeten Algorithmen wirken diskriminierend durch Proxys für geschützte Merkmale wie Migrationsstatus, ethnische Herkunft und Einkommensstatus und verstärken bestehende strukturelle Ungleichheiten, besonders gegen Migranten, Geflüchtete und Menschen mit niedrigem Einkommen.
- Die Digitalisierung des Sozialhilfesystems schließt systematisch marginalisierte Gruppen (Frauen in Krisenzentren, ältere Menschen, Menschen mit Behinderungen) aus und erzwingt für andere eine diskriminierende Einbeziehung, was Zugangs- und Menschenrechtsverletzungen verstärkt.
- Der komplette Mangel an Transparenz, unabhängigen Audits, Evaluierungsmetriken und Möglichkeiten zur Beschwerde macht es unmöglich, Diskriminierung zu erkennen oder zu bekämpfen und verstößt gegen EU AI Act, GDPR und internationale Menschenrechtsstandards.
- Die Verantwortung liegt nicht nur bei der dänischen Regierung, sondern auch beim privaten ATP-Konzern, der die Systeme entwickelt und betreibt, ohne angemessene Human Rights Due Diligence durchzuführen.

## Kategorie-Evidenz

### AI_Literacies

Das Paper thematisiert kritisch den mangelnden Aufbau von Kompetenzen bei Caseworkern im Umgang mit Algorithmen: 'Ensure that Udbetaling Danmark/ATP and municipalities provide caseworkers with additional training and capacity building where necessary to address and prevent issues such as automation bias.' Es behandelt Fragen algorithmischer Transparenz und das Verständnis der Öffentlichkeit für KI-Systeme.

### KI_Sonstige

Detaillierte Analyse von Machine-Learning-basierten Betrugskontrollalgorithmen, die Fraud-Detection durchführen: 'algorithms are deployed by UDK to classify or predict a person's circumstances, such as their relationship status, or whether they've left the country without informing the welfare agency' sowie Discussion von Register-Merging, Risk-Scoring und semi-automatisierten Entscheidungssystemen.

### Soziale_Arbeit

Direkte Analyse des Sozialhilfesystems und dessen digitaler Transformation: 'welfare system known as Udbetaling Danmark (Pay Out Denmark), undermines human rights through its use of analogue and digital forms of surveillance to detect social benefits fraud.' Fokus auf Zugang zu Sozialhilfe, Caseworker-Praktiken, Betroffene von Sozialpolitik.

### Bias_Ungleichheit

Zentrale Thematisierung von algorithmischem Bias und struktureller Diskriminierung: 'These characteristics or variables can act as proxies for race, migration status, and socio-economic status and can encourage discrimination' sowie 'Structural discrimination, heightened risk of algorithmic discrimination' und Analyse wie Algorithmen gegen Migranten, ethnische Minderheiten, Menschen mit Behinderungen, arme Menschen discriminieren.

### Gender

Explizite Thematisierung geschlechtsspezifischer Auswirkungen: 'Discrimination is also present as a result of the digitalization of the benefits system. This is because for some marginalised groups, including women in crisis and people with disabilities, accessing a digital service independently is not possible' sowie 'intersectional harms of technologies' und Empfehlungen für Frauen in Krisenzentren.

### Diversitaet

Durchgehende intersektionale Perspektive auf marginalisierte Gruppen: 'low-income groups, racialised groups, including migrants and people who have been granted refugee status in Denmark, ethnic minorities, people with disabilities, and older people' sowie Discussion von 'intersectional discrimination' und 'foreignness' als Diskriminierungsgrund.

### Fairness

Explizite Thematisierung algorithmischer Fairness und deren Fehlen: 'absence of demographic data prevents statistical bias and fairness testing' sowie Diskussion von Performance-Metriken, Evaluation, Confusion Matrix und die Forderung nach 'bias and fairness testing' und Fairness-Audits.

## Assessment-Relevanz

**Domain Fit:** Das Paper hat sehr hohe Relevanz für die Schnittstelle KI/Soziale Arbeit/Gleichheit: Es analysiert konkret, wie automatisierte Systeme in einem zentralen Sozialhilfekontext (Wohlfahrtsstaat) implementiert sind und wie sie marginalisierte Gruppen treffen, die klassische Zielgruppen Sozialer Arbeit sind. Es verbindet technische KI-Kritik mit sozialpolitischen und menschenrechtlichen Perspektiven.

**Unique Contribution:** Das Paper leistet einen seltenen und wertvollen Beitrag durch eine gründliche, empirisch fundierte Menschenrechtsanalyse eines real existierenden, national implementierten Wohlfahrts-KI-Systems mit explizitem Fokus auf Intersektionalität und strukturelle Diskriminierung marginalisierter Gruppen sowie durch die Forderung nach Human Rights Due Diligence von KI-Anbietern.

**Limitations:** Das Paper konzentriert sich primär auf Dänemark und kann daher begrenzte Generalisierbarkeit für andere Länter aufweisen; die empirische Datenbasis zur direkten Messung von Algorithmic Bias ist begrenzt, da UDK demografische Daten nicht zur Verfügung stellte; es werden nur begrenzt technische Details der Algorithmen offengelegt.

**Target Group:** Primär: Policymaker, Menschenrechtsorganisationen, Sozialarbeiter und deren Verbände, Gewerkschaften, Betroffenengruppen und ihre Advocacy-Organisationen. Sekundär: KI-Ethiker, Regulatoren (insbes. EU-Kommission), Datenschützer, Wissenschaftler in kritischen KI- und Algorithmik-Studien, Wohlfahrtsstaats-Forscher

## Schlüsselreferenzen

- [[UN_Guiding_Principles_on_Business_and_Human_Rights_2011]] - UN Guiding Principles on Business and Human Rights
- [[UN_Special_Rapporteur_on_the_right_to_privacy_2018]] - Working Draft Legal Instrument on Government-led Surveillance and Privacy
- [[GDPR__European_Union_2018]] - General Data Protection Regulation (GDPR)
- [[EU_AI_Act_2024]] - Regulation (EU) 2024/1689 on Artificial Intelligence
- [[Committee_on_Economic_Social_and_Cultural_Rights_CESCR_2017]] - General Comment 24 on State Obligations under ICESCR
- [[OECD_None]] - Due Diligence Guidance for Responsible Business Conduct
- [[UN_Special_Rapporteur_on_contemporary_forms_of_racism_None]] - Racial Discrimination and Emerging Digital Technologies: A Human Rights Analysis
- [[CERD_Committee_2005]] - General Recommendation 31 on Racial Discrimination
- [[Charter_of_Fundamental_Rights_of_the_European_Union_2012]] - Charter of Fundamental Rights
- [[Danish_Data_Protection_Act_2018]] - Data Protection Act (No. 502 of 23 May 2018)
