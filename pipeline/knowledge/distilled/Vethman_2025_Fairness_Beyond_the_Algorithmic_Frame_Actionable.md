---
title: "Fairness Beyond the Algorithmic Frame: Actionable Recommendations for an Intersectional Approach"
authors: ["Steven Vethman", "Nina M. van Liebergen", "Quirine T. S. Smit", "Cor J. Veenman"]
year: 2025
type: conferencePaper
language: en
categories:
  - AI_Literacies
  - KI_Sonstige
  - Soziale_Arbeit
  - Bias_Ungleichheit
  - Diversitaet
  - Feministisch
  - Fairness
processed: 2026-02-05
source_file: Vethman_2025_Fairness_Beyond_the_Algorithmic_Frame_Actionable.md
confidence: 95
---

# Fairness Beyond the Algorithmic Frame: Actionable Recommendations for an Intersectional Approach

## Kernbefund

Die aktuelle AI-Fairness-Forschung reduziert Intersektionalität auf einen engen technischen Rahmen (Fairness-Metriken für Subgruppen), während ein echter intersektionaler Ansatz strukturelle Ungleichheit, Machtdynamiken und marginalisierte Stimmen zentralstellen muss. Das Paper identifiziert fünf konkrete Handlungsempfehlungen für AI-Expert:innen, die über den algorithmischen Rahmen hinausgehen.

## Forschungsfrage

Welche konkreten und umsetzbaren Empfehlungen können AI-Expert:innen befolgen, um einen intersektionalen Ansatz für gerechtere KI-Systeme zu verankern?

## Methodik

Mixed Methods: Thematische Analyse (Braun & Clarke) von 60 AI-Fairness-Papern aus ACM-Konferenzen (AIES, FAccT, FAT*) kombiniert mit gemeinschaftlicher Evaluation durch einen Workshop mit AI-Expert:innen (n=nicht spezifiziert, europäische Forschungseinrichtung).
**Datenbasis:** 60 ausgewählte Fachliteraturpapiere aus ACM-Konferenzen (2019-2023); Workshop-Evaluation mit AI-Expert:innen aus einer europäischen Forschungseinrichtung; Thematische Analyse mit iterativer Kodierung (206 Codes → 17 → 5 Themenkomplexe)

## Hauptargumente

- Die aktuelle AI-Fairness-Forschung nutzt Intersektionalität auf reduzierte Weise (narrow/weak interpretation) durch den Fokus auf statistische Fairness-Metriken für demographische Subgruppen, was zentrale Aspekte wie Machtrelationen, soziale Gerechtigkeit und strukturelle Ungleichheit ignoriert, die im Black Feminist-Ursprung von Intersektionalität zentral sind.
- Ein authentisch intersektionaler Ansatz für AI-Fairness erfordert ein Verständnis der sozialen Kontexte, Machtdynamiken und strukturellen Ungleichheiten (socio-technical frame), nicht nur technische Optimierungen innerhalb des algorithmischen Rahmens, wie die Analyse des Beispiels von Buolamwini & Gebru zeigt.
- AI-Expert:innen haben eine zentrale Verantwortung und entscheidende Position in AI-Entwicklung und -Einsatz, müssen aber über ihre traditionelle technische Ausbildung hinausgehend Fähigkeiten in interdisziplinärer Zusammenarbeit, kritischer Reflexion, Gemeinschaftsbeteiligung und kritischem Verständnis von Machtstrukturen entwickeln.
- Fünf konkrete, umsetzbare Empfehlungen können AI-Expert:innen helfen, intersektionale Gerechtigkeit zu praktizieren: (1) Interdisziplinäre Zusammenarbeit, (2) Reflexion von Positionaliät, (3) echte Gemeinschaftsbeteiligung mit Co-Ownership, (4) Analyse von Machtdynamiken und sozialen Kontexten, (5) kritische Bewertung von Daten, Metriken und deren Limitationen.
- Zentrale Barrieren für AI-Expert:innen sind Tech-Optimismus, Angst vor unzureichendem Wissen und der Wunsch nach perfektionistisch strukturierten Checklisten statt iterativer, adaptiver Prozesse, aber die Empfehlungen können als Kommunikationshilfen bei Stakeholdern und für bessere Projektergebnisse funktionieren.

## Kategorie-Evidenz

### AI_Literacies

Starker Fokus auf Bildung und Kompetenzen von AI-Expert:innen: 'This intersectional approach requires moving beyond the algorithmic frame, which is not the core of the AI expert's background, including their formal education and professional experience.' Empfehlungen zu kritischer Datenwissenschaftlicher Ausbildung werden explizit erwähnt.

### KI_Sonstige

Fokus auf ML-basierte AI-Systeme, algorithmische Entscheidungssysteme in Hochrisiko-Bereichen (Wohlfahrt, Fraud Detection, Hiring, Bail). 'we adopt this same scope for our study' auf ML-basierte AI.

### Soziale_Arbeit

Explizite Bezüge zu Sozialleistungssystemen und Wohlfahrt: Dutch childcare benefits scandal (26.000 Familien), französisches automated fraud detection in welfare allocation. Marginalisierte Communities und Wohlfahrtszugang sind zentrale Anwendungsbeispiele.

### Bias_Ungleichheit

Kernfokus auf algorithmischen Bias und strukturelle Ungleichheit: 'These cases demonstrate how Artificial Intelligence (AI), or broader automated decision-making systems, can reinforce and propagate racism, sexism, and social inequality.' Structural inequality und systemic threat werden zentral behandelt.

### Diversitaet

Intersektionale Perspektive auf marginalisierte Communities: Black women, Surinamese-descent families, immigrant backgrounds, queer communities. 'hearing diverse voices and recognizing multiple kinds of knowledge' ist ein Schlüsselelement.

### Feministisch

Explizit verankert in Black Feminist-Theorie und Praxis: 'Intersectionality, rooted in Black Feminist movements', Referenz auf Kimberlé Crenshaw, Patricia Hill Collins und Sirma Bilge, 'critical inquiry and praxis' (Examination und Action). Paper verwendet 'critical inquiry and practice towards social justice' als Rahmen. Feminist participatory design (Suresh et al.) wird als Best Practice zitiert.

### Fairness

Zentrale Diskussion von AI Fairness-Forschung, Fairness-Metriken und deren Limitationen: 'group fairness', 'intersectional (subgroup) fairness', 'fairness metrics', 'Equalized Odds, Demographic Parity' werden diskutiert als narrow interpretation.

## Assessment-Relevanz

**Domain Fit:** Hochgradig relevant für die Schnittstelle AI/Soziale Arbeit/Gender Studies. Das Paper adressiert direkt, wie KI-Systeme marginalisierte Gruppen (einschließlich sozialer Dienstleistungsempfänger) schädigen und bietet konkrete Empfehlungen für eine gerechtere Praxis, die über technische Lösungen hinausgeht.

**Unique Contribution:** Das Paper leistet eine kritische Differenzierung zwischen 'narrow' technischer und 'broad' intersektionaler Fairness-Interpretation und übersetzt Black Feminist-Theorie in konkrete, umsetzbare Handlungsempfehlungen speziell für AI-Expert:innen, die durch Community-Engagement evaluiert wurden.

**Limitations:** Die Evaluation basiert auf selbstselegierten, motivierten, europäischen (niederländischen) AI-Expert:innen aus einer Forschungseinrichtung; Unterrepräsentation von aktivistischen Stimmen außerhalb der Akademia; Paper-Analyse begrenzt auf zwei Konferenzen (AIES, FAccT), nicht auf kritische Rechtswissenschaften oder Politikwissenschaften.

**Target Group:** Primär: AI-Entwickler, Data Scientists, ML-Forscher:innen und AI-Expert:innen, die ihre Praxis gerechter gestalten wollen. Sekundär: Interdisziplinäre Teams (Ethiker:innen, Sozialwissenschaftler:innen, Aktivist:innen), Policy-Maker im Bereich AI-Governance, Sozialarbeiter:innen die mit AI-Systemen interagieren, Lehrende in kritischer Datenwissenschaft und AI-Ethics.

## Schlüsselreferenzen

- [[Crenshaw_Kimberlé_1989]] - Demarginalizing the Intersection of Race and Sex
- [[Collins_Patricia_Hill_Bilge_Sirma_2020]] - Intersectionality
- [[Buolamwini_Joy_Gebru_Timnit_2018]] - Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification
- [[Kong_Qingqing_2022]] - Intersectionality in AI Fairness (kritische Analyse)
- [[DIgnazio_Catherine_Klein_Lauren_F_2020]] - Data Feminism
- [[Suresh_Harini_et_al_2022]] - Towards Intersectional Feminist and Participatory ML: A Case Study in Supporting Feminicide Counterdata Collection
- [[Selbst_Andrew_D_et_al_2019]] - Fairness and Abstraction in Sociotechnical Systems
- [[Young_Iris_Marion_2011]] - Responsibility for Justice
- [[Raji_Inioluwa_Deborah_et_al_2020]] - Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing
- [[Bender_Emily_M_et_al_2021]] - On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?
