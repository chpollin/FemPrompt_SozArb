---
title: "Algorithmic-Assisted Decision-Making Tools in Child Welfare Practice: A Systematic Review"
authors: ["Meng-Hsuan Yu", "Roderick A. Rose"]
year: 2025
type: journalArticle
language: en
categories:
  - AI_Literacies
  - KI_Sonstige
  - Soziale_Arbeit
  - Bias_Ungleichheit
  - Diversitaet
  - Fairness
processed: 2026-02-05
source_file: Yu_2025_Algorithmic-Assisted_Decision-Making_Tools_in.md
confidence: 95
---

# Algorithmic-Assisted Decision-Making Tools in Child Welfare Practice: A Systematic Review

## Kernbefund

Algorithmisch unterstützte Entscheidungshilfen zeigen Potenzial für den Kinderschutz, erfordern aber systematische Aufmerksamkeit für ethische Implementierung und Fairness, insbesondere bezüglich rassischer Disparitäten und Datenqualität. Bedeutende methodische und empirische Lücken bleiben bestehen.

## Forschungsfrage

What are the key factors associated with successfully implementing algorithmic decision-making tools in child welfare practice, what challenges arise, and how do agencies address fairness, equity, and ethics issues?

## Methodik

Systematische Literaturreview (PRISMA-Richtlinien) mit Einschluss von grauer Literatur; Analyse von 9 Studien zu algorithmischen Tools in der Kinderschutzhilfe mittels angepasstem Implementierungswissenschaft-Framework (CFIR)
**Datenbasis:** 9 Studien aus Datenbanken (PubMed, JSTOR, ProQuest, Google Scholar) und grauer Literatur; Zeitraum der Veröffentlichungen variiert, Fokus auf konkrete Implementierungsfälle in Kindesschutz-Agenturen

## Hauptargumente

- Algorithmische Entscheidungshilfen versprechen höhere Genauigkeit und Konsistenz als traditionelle Risikobewertungstools, können aber existierende Ungleichheiten verstärken, wie das Beispiel des Tools zeigt, das Black Children überproportional für Missbrauchsuntersuchungen flaggt.
- Erfolgreiche Implementierung erfordert nicht nur technische Validität und Zuverlässigkeit, sondern auch strukturelle Voraussetzungen (Dateninfrastruktur, IT-Systeme, Budget), organisationale Kommunikation, Training und Stakeholder-Engagement, insbesondere mit betroffenen Gemeinschaften.
- Ethische Überlegungen müssen von der Tool-Entwicklung bis zur Deployment-Phase berücksichtigt werden; Transparenz über Grenzen, Validierung über Subgruppen hinweg, und Mechanismen zur Feedbacksammlung und externen ethischen Überprüfung sind zentral.

## Kategorie-Evidenz

### AI_Literacies

Explicit focus on training to enhance understanding of algorithmic tools and mitigating automation bias (CFIR table on 'Access to knowledge and information'). The review emphasizes need for caseworkers and agency staff to develop competencies in using decision-support systems effectively.

### KI_Sonstige

Central focus on predictive analytics, machine learning models, algorithmic-assisted decision-making, and computational algorithms in child welfare. Tools reviewed include LASSO logistic regression, random forests, neural networks, natural language processing, and risk prediction models.

### Soziale_Arbeit

Systematic review of algorithmic tools implemented in child welfare practice across multiple decision-making stages: referral screening, investigation, substantiation, placement, and permanency decisions. Direct engagement with social workers' decision-making, professional judgment, and practical utility in frontline child protective services.

### Bias_Ungleichheit

Extensive discussion of algorithmic bias and racial disparities: 'one tool, for instance, disproportionately flagged Black children for mandatory maltreatment investigations' (Ho & Burke, 2022). Review examines whether tools 'function consistently across subgroups' and analyzes accuracy across demographic groups, particularly by race and gender.

### Diversitaet

Multiple references to marginalized communities and intersectional concerns: examination of whether algorithmic tools 'worsen racial disparities in the CW system,' analysis of overrepresentation of Black children, and equity-focused reports examining racial disproportionality in Los Angeles County model. Consideration of how tools impact diverse families and communities.

### Fairness

Core focus on algorithmic fairness as one of three central themes. Framework explicitly addresses fairness across subgroups (Russell 2015), uses fairness correction techniques (Purdy & Glass 2023), analyzes model equity through AUC/ROC values, and evaluates whether algorithms function consistently across demographic groups. Drake et al. (2020) framework includes fairness assessment at each stage of development and deployment.

## Assessment-Relevanz

**Domain Fit:** Hochgradig relevant für die Schnittstelle KI und Soziale Arbeit. Das Paper adressiert direkt die praktische Implementierung von ML-Systemen in hochriskanten sozialen Kontext mit Fokus auf Fairness, Equity und Ethik – zentrale Anliegen der Soziale Arbeit.

**Unique Contribution:** Erste umfassende systematische Analyse, die algorithmische Entscheidungshilfen im Kinderschutz durch eine Implementierungswissenschaft-Linse (CFIR) untersucht und dabei ethische Erwägungen von Tool-Entwicklung bis Deployment berücksichtigt, mit explizitem Fokus auf reale Praktiken und Stakeholder-Perspektiven.

**Limitations:** Nur 9 Studien eingeschlossen; erhebliche methodische und empirische Lücken in der bestehenden Literatur; begrenzte Evidenz zu Stakeholder-Perspektiven jenseits von Entwicklern; fehlende empirische Daten zu tatsächlichen Outcomes und langfristigen Auswirkungen auf Familien und Gemeinden.

**Target Group:** Sozialarbeiter und Kindesschutzfachleute; KI-Entwickler und Data Scientists im Social-Sector; Policymaker und Verwaltungsfachleute in Kinderschutz-Agenturen; Forscher in Implementierungswissenschaft und Algorithmischer Fairness; Community-Organisationen und Vertreter betroffener Familien; Ethiker und Governance-Experten im Kontext von High-Stakes-Algorithmen

## Schlüsselreferenzen

- [[Eubanks_2018]] - Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor
- [[Chouldechova_et_al_2018]] - A Case Study of Algorithm-Assisted Decision Making in Child Maltreatment Hotline Screening Decisions
- [[Drake_et_al_2020]] - A Practical Framework for Considering the Use of Predictive Risk Modeling in Child Welfare
- [[Russell_2015]] - Predictive Analytics and Child Protection: Constraints and Opportunities
- [[Saxena_et_al_2020]] - A Human-Centered Review of Algorithms Used Within the U.S. Child Welfare System
- [[Hall_et_al_2023]] - A Systematic Review of Sophisticated Predictive and Prescriptive Analytics in Child Welfare: Accuracy, Equity, and Bias
- [[Ho_Burke_2022]] - An Algorithm That Screens for Child Neglect Raises Concerns
- [[Glaberson_2019]] - Coding Over the Cracks: Predictive Analytics and Child Protection
- [[PutnamHornstein_et_al_2022]] - Los Angeles County Risk Stratification Model: Methodology and Implementation Report
- [[Saxena_Guha_2024]] - Algorithmic Harms in Child Welfare: Uncertainties in Practice, Organization, and Street-Level Decision-Making
