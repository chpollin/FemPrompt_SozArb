---
title: "A Formal Account of AI Trustworthiness: Connecting Intrinsic and Perceived Trustworthiness"
authors: ["Piercosma Bisconti", "Letizia Aquilino", "Antonella Marchetti", "Daniele Nardi"]
year: 2024
type: conferencePaper
language: en
processed: 2026-02-05
source_file: Bisconti_2024_A_formal_account_of_AI_trustworthiness_Connecting.md
confidence: 88
---

# A Formal Account of AI Trustworthiness: Connecting Intrinsic and Perceived Trustworthiness

## Kernbefund

Vertrauenswürdigkeit ist ein relationales Attribut, das durch die Übereinstimmung zwischen beobachtetem Systemverhalten und mentalen Modellen von Beobachtern entsteht. Die Arbeit formalisiert dies durch mathematische Funktionen, die Transparenz, Agency Locus und menschliche Überwachung als zentrale Faktoren der wahrgenommenen Vertrauenswürdigkeit identifizieren.

## Forschungsfrage

Wie können intrinsische und wahrgenommene Vertrauenswürdigkeit von KI-Systemen theoretisch und formal integriert werden, um eine umfassende Messung von Vertrauenswürdigkeit zu ermöglichen?

## Methodik

Theoretisch / Formalisierung mit mathematischen Modellen. Der Ansatz entwickelt ein theoretisches Rahmenwerk zur Vertrauenswürdigkeit und präsentiert eine formale mathematische Schematisierung.
**Datenbasis:** Keine empirische Datenbasis. Theoretisches und formales Konzeptpapier; experimentale Validierung für zukünftige Arbeiten geplant.

## Hauptargumente

- Vertrauenswürdigkeit ist nicht eine intrinsische Systemeigenschaft, sondern ein relationales Attribut, das Interaktion zwischen Trustor und Trustee erfordert. Nur wenn ein System nicht vollständig vorhersehbar ist, wird das Konzept Trust relevant.
- Die ISO-Definition von Vertrauenswürdigkeit als Fähigkeit zur Erfüllung von Stakeholder-Erwartungen sollte als Attribut umformuliert werden, das entsteht, wenn mentale Modelle von Beobachtern mit tatsächlichem Systemverhalten übereinstimmen.
- Drei zentrale wahrgenommene Charakteristiken beeinflussen Vertrauenswürdigkeit durch Reduktion wahrgenommener Unsicherheit: Transparenz (Informationsverfügbarkeit), Agency Locus (Ursprung der Systementscheidungen: menschlich oder maschinell) und Human Oversight (Überwachung durch Menschen). Diese werden durch individuelle Faktoren wie Big-Five Persönlichkeitszüge und Vertrauenspropensität vermittelt.

## Kategorie-Evidenz

### Evidenz 1

Das Paper adressiert kritisches Verständnis von KI-Systemen: 'A solid understanding of the concept of trustworthiness is of paramount importance for the future of design and policies' und entwickelt konzeptuelle Fähigkeiten zum Verständnis von Vertrauenswürdigkeit als Schlüsselkompetenz für Stakeholder.

### Evidenz 2

Fokus auf algorithmische Entscheidungssysteme, autonome Systeme und Robotik. 'AI systems are distinguished from other technologies by their autonomy and adaptability' und 'Therefore, the trustworthiness of AI is critical not only for ensuring safety and compliance with ethical standards'.

### Evidenz 3

Behandelt unterschiedliche Stakeholder-Perspektiven und deren unterschiedliche Erwartungen: 'the stakeholders impacted directly are at least the physician and the patient. It is easy to note that already these two have profoundly different expectations, concerns, target goals.' Adressiert auch strukturelle Unterschiede in Wahrnehmung und Verzerrungen durch individuelle Unterschiede.

### Evidenz 4

Berücksichtigung verschiedener Stakeholder-Perspektiven und individueller Unterschiede: 'Our schematization cannot completely capture the complexity of stakeholders' perceived trustworthiness, since it is not sensitive a) to the perspective and the specific point of observation of each specific stakeholder and b) therefore to the fact that each stakeholder might value specific characteristics of the AI system differently.' Einbeziehung von Persönlichkeitszügen (Big Five) und Vertrauenspropensität als mediating Faktoren.

### Evidenz 5

Expliziter Fokus auf faire KI-Systeme und ethische Standards: 'ensuring they meet both functional and ethical criteria' und 'fostering more ethical and widely accepted AI technologies'. Discussion von Fairness im Kontext von EU AI Act und Regulierung zur Sicherstellung gerechter Behandlung.

## Assessment-Relevanz

**Domain Fit:** Das Paper ist relevant für KI-Governance und Ethics, besitzt aber keinen direkten Bezug zu Sozialer Arbeit. Es bietet jedoch einen konzeptionellen Rahmen, der für die Bewertung von KI-Systemen in sozialen Kontexten nutzbar sein könnte, insbesondere bezüglich der Heterogenität von Stakeholder-Erwartungen und wahrgenommener Fairness.

**Unique Contribution:** Die formale mathematische Schematisierung der Vertrauenswürdigkeit durch explizite Integration intrinsischer und wahrgenommener Aspekte ist neuartig und bietet einen operationalisierbaren Rahmen zur Messung von KI-Vertrauenswürdigkeit jenseits technischer Metriken.

**Limitations:** Das Papier ist rein theoretisch ohne empirische Validierung; die Dichotomie zwischen 'intrinsischer' und 'wahrgenommener' Vertrauenswürdigkeit wird als epistemologisch problematisch anerkannt; die Schematisierung berücksichtigt nicht ausreichend kontextuelle, kulturelle und institutionelle Variabilität sowie die dynamische Natur von Vertrauen.

**Target Group:** KI-Entwickler und Designer, Policy-Maker und Regulierungsbehörden (EU AI Act Kontext), KI-Ethiker und Vertrauensforschende, Stakeholder von autonomen Systemen (Robotik, medizinische Systeme, autonome Fahrzeuge). Potenziell relevant für Sozialarbeiter, die mit KI-gestützten Entscheidungssystemen umgehen müssen.

## Schlüsselreferenzen

- [[Mayer_et_al_1995]] - An Integrative Model of Organizational Trust
- [[McKnight_Cummings_Chervany_1998]] - Initial Trust Formation in New Organizational Relationships
- [[Lee_See_2004]] - Trust in automation framework
- [[Mittelstadt_et_al_2016]] - The ethics of algorithms: Mapping the debate
- [[Shin_2021]] - Trustworthiness as driver for AI adoption
- [[Liu_2021]] - In AI We Trust? Effects of Agency Locus and Transparency on Uncertainty Reduction
- [[Felzmann_et_al_2020]] - Towards transparency by design for artificial intelligence
- [[Graziani_et_al_2023]] - A global taxonomy of interpretable AI
