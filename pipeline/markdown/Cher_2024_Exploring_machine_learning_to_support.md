---
source_file: Cher_2024_Exploring_machine_learning_to_support.pdf
conversion_date: 2026-02-03T08:45:06.326347
converter: docling
quality_score: 95
---

1234567890();,:

1234567890();,:

## ORIGINAL PAPER

## Exploring Machine Learning to Support Decision-Making for Placement Stabilization and Preservation in Child Welfare

Ka Ho Brian Chor 1 ● Zhidi Luo 2 ● Kit T. Rodolfa 3 ● Rayid Ghani 4

Accepted: 17 December 2024 / Published online: 7 January 2025 +

© The Author(s), under exclusive licence to Springer Science Business Media, LLC, part of Springer Nature 2025

## Abstract

The Family First Prevention Services Act requires youth ' s placement in residential care to be clinically appropriate, timelimited, and only when youth ' s needs cannot be met in family-like settings in foster care. State child welfare agencies can bene /uniFB01 t from upstream, empirical decision support to preempt youth ' s placement disruption, coordinate proactive placement stabilization services, prevent unnecessary step-up to residential care, and improve outcomes for the youth. This statewide case study explores the potential bene /uniFB01 t to child welfare decision support for placement stabilization and diversion from residential care, by comparing predictive machine learning (ML) models with conventional regression models. We analyzed child welfare spells of 12,621 youth in one large Midwestern state between January 2017 and January 2020. Caseworkers could refer youth to a placement stabilization and preservation program. To predict youth ' s monthly program need in the next 6 months, we developed and validated a wide grid of ML models -random forest, regularized logistic regression, decision tree, dummy classi /uniFB01 er -and a conventional unregularized logistic regression model, using literature-informed predictors from child welfare administrative data. We retrained, retested, and compared all models over time using temporal hold-out sets. Based on anticipated program capacity, model evaluation focused on accuracy in identifying the 100 highestneed youth, fairness, and equity of resource allocation. Random forest models produced the best performance with a precision (positive predictive value) 10 times greater than baseline precision. Common important predictors across models included youth ' s age, history of placement changes, and emotional/behavioral needs. We discuss potential applications of ML to support preventive child welfare decisions, adapt to policy changes, and allocate limited resources.

Keywords Machine learning ● Predictive model ● Child welfare ● Decision support ● Casework practice

## Highlights

- Machine learning (ML) predictions can inform preventive services for youth at risk placement disruption in foster care.
- A wide grid of ML and regression predictive models predicted youth ' s need for a Midwestern state placement stabilization program.
- Random forest models consistently outperformed other models; all models were further compared on fairness and equity.
- Well-designed ML predictive models can support proactive casework decision-making and preventive resource allocation.

* Ka Ho Brian Chor bchor@chapinhall.org

- 1 Chapin Hall at the University of Chicago, Chicago, IL, USA

2 Department of Psychiatry and Behavioral Sciences, Northwestern University Feinberg School of Medicine, Chicago, IL, USA

3 Regulation, Evaluation, and Governance Lab, Stanford Law School, Stanford, CA, USA

- 4 Machine Learning Department, Heinz College of Information Systems and Public Policy, Carnegie Mellon University, Pittsburg, PA, USA

<!-- image -->

The U.S. child welfare system is founded on the ' least restrictive setting ' principle, by which youth are best served in the most family-like, least intrusive setting to promote normative childhood experience whenever possible (Child Welfare Information Gateway, 2020). In practice, however, limited availability of family-like settings and the complexity of youth ' s needs result in increased reliance on residential care, the most restrictive setting (Havlicek, 2011). Residential care encompasses congregate, boarding settings in which youth reside in 24/7 facilities, receive

<!-- image -->

intensive treatments and services from professional staff, as well as education and other ancillary supports (American Academy of Pediatrics &amp; Chapin Hall at the University of Chicago, 2023).

## Policy Context for Levels of Care in Child Welfare

The Family First Prevention Services Act (Family First) (P. L. 115-123) has accelerated important /uniFB01 scal and clinical gatekeeping of residential care. Family First represents federal efforts through public child welfare policy and funding mechanisms to prevent foster care entry and incentivize family-based settings for youth in foster care. Family First ' s focus on prevention and family-based care is highlighted in the amended Title IV of the Social Security Act (P. L. 74-271), with which state child welfare agencies must comply on the use of ' child care institution ' for it to be reimbursed by federal Title IV-E foster care maintenance payment. A speci /uniFB01 c type of ' child care institution, ' Quali /uniFB01 ed Residential Treatment Program (QRTP), sets unprecedented standards for residential care. A QRTP must: (1) use a trauma-informed treatment model; (2) have registered/ licensed nursing staff and clinical staff; (3) facilitate family participation in youth ' s treatment; (4) provide outreach to youth ' s family; (4) plan for discharge and family-based aftercare; (5) be licensed and accredited by a national accrediting organization. Independent clinical assessments must be completed by ' quali /uniFB01 ed individuals ' within 30 days of youth ' s QRTP placement, followed by court approval within 60 days. Child welfare agencies must obtain court approval for extending youth ' s stay in a QRTP longer than 12 consecutive months (if younger than age 13) or 18 non-consecutive months (if ages 13 or older). These QRTP requirements went into effect October 1, 2021.

Independent of the Family First requirements, most state child welfare agencies have existing mechanisms through training, policies, and procedures (e.g., multidisciplinary team planning) to help caseworkers safeguard placement changes, especially those escalating to residential care or precipitated by risk of placement disruption (Annie E. Casey Foundation, 2002; Chor et al., 2015). Because Family First cements further upstream prevention efforts and the judicial use of QRTP, caseworkers can bene /uniFB01 t from early detection of youth ' s placement needs and additional decision support to proactively stabilize youth in the community. In this way, caseworkers can more capably serve youth at risk of placement disruption through preventive case and service planning to minimize unnecessary placement moves or residential care placement, which have negative, downstream consequences.

Relevant literature shows that placement moves have a snowballing effect in delaying youth ' s pathway to permanency, disrupting meaningful social relationships, leaving a sense of loss (Chambers et al., 2018), and culminating in placement in residential care when youth ' s behavioral and emotional needs remain unresolved in less restrictive settings (James, 2004). Youth often /uniFB01 nd themselves ' failing up ' from less restrictive settings to residential care as a last resort (Epstein, 2004; McCurdy &amp; McIntyre, 2004). Unlike youth placed in family-like settings such as foster or kinship foster home, youth placed in residential care are often furthest removed from family reuni /uniFB01 cation, adoption, guardianship, or other permanent settings (Pecora et al., 2018).

## Child Welfare Research and Practice Context for Level-of-Care Decision-Making and Machine Learning

Existing level-of-care decision support tools and algorithms that support placement decision-making have demonstrated varying degrees of adoption in child welfare. Retrospective studies of decision-making by caseworkers and multidisciplinary teams found that following placement or level-of-care recommendations decision support algorithms was associated with improvement in youth ' s placement stability, clinical, and functioning outcomes (Chor et al., 2015; Epstein et al., 2015). Yet, some caseworkers still deviated from algorithm-based recommendations (Chor et al., 2015). These /uniFB01 ndings highlight missed opportunities for careful integration of datainformed, empirical methods to support and augment caseworkers ' placement decision-making for youth under their care, especially those who face placement disruption or potential needs for residential care.

Applications of decision support algorithms in child welfare have continued to evolve. Notable applications of predictive analytics, predictive modeling, and its variants, have focused on the ' frontend ' of the child welfare system to identify and divert low-risk families from child protective service investigations so that only high-risk families are screened-in for investigations as appropriate (Allegheny County Department of Human Services, 2017; Chouldechova et al., 2018; Schwartz et al., 2017). Other applications have focused on the ' backend ' of the child welfare system to plan for aftercare for youth aging out of care (Ahn et al., 2021) and for youth who have reuni /uniFB01 ed with their families but are at risk of reentering the child welfare system (Wisconsin Department of Children and Families, 2014). In contrast, there is substantially less work on empirical assistance on casework decisionmaking for youth in foster care, despite promises shown

<!-- image -->

in proof-of-concept predictive model applications to predict youth running away from care (Chor et al., 2022) or entering residential care (Chor et al., 2023). Yet, administrative burdens to care for in-care youth are substantial because understaffed and underfunded caseworkers have large caseloads but limited time to make objective and quality decisions (Pecora et al., 2018). High turnover rates also stymy transfer of experience and knowledge within the volatile child welfare workforce (Kim &amp; Kao, 2014).

Decision support tools and algorithms in child welfare have evolved from consensus-based assessments, decision rule-based checklists, to more recently predictive models using regression and machine learning (Russell, 2015; Saxena et al., 2020). Machine learning (ML) consists of a family of methods that ' learn ' from complex data over time and detect underlying or unobserved patterns associated with an outcome or prediction of interest (Ghani &amp; Schierholz, 2020). MLlearns from experience (e.g., historical child welfare data) regarding a task (e.g., predicting child maltreatment) and performance measure (e.g., prediction accuracy), and task performance improvement with experience. Although the speci /uniFB01 c methods might range from regression to random forest to deep learning, most ML applications allow the freedom to learn complex, non-linear data patterns. When a child welfare outcome such as service use or placement disruption exhibits complex relationships with many predictors operating in a non-linear fashion (Janczewski &amp; Nitkowski, 2023; Negriff et al., 2022), ML predictive models that have been validated to predict behaviors of unseen observations often have greater predictive utility than explanatory models that focus on explaining the underlying cause of a behavior to generate theories (Yarkoni &amp; Westfall, 2017).

Child welfare agencies are often interested in empirical detection and prediction of a phenomenon (e.g., Will a child be maltreated?) based on imperfect detection and measurement (Jenkins, 2021; Keddell, 2019). Thus, there are concerns about interpretability (e.g., ML ' black box ' ), errors (e.g., How often a predictive model identi /uniFB01 es true positives of severe harm?), and bias (e.g., against protected classes) that ultimately could impact youth in the child welfare system. These concerns are justi /uniFB01 ed as blindly relying on predictive models can result in youth ' s harm (Jackson &amp; Marx, 2017). In response, child welfare researchers have advocated for broad standards -validity, equity, reliability, and usefulness -for evaluating predictive models in child welfare applications (Drake et al., 2020; Hall et al., 2023; Russell, 2015; Schwartz et al., 2017). Nonetheless, studies tend to focus on a subset of these standards or do not provide suf /uniFB01 cient details that operationalize these standards (Chouldechova et al., 2018; Hall et al., 2023).

<!-- image -->

## Current Study

Under these child welfare policy, research, and practice contexts, this study targeted three goals. First, we addressed the research gaps in preventing placement disruption and restrictive levels of care, and policy and practice gaps in caseworker decision support on preventive planning for youth in foster care. Focusing on one large Midwestern state child welfare agency, we examined caseworkers ' use of a multidisciplinary, team-based placement stabilization and preservation program aimed at preventing placement disruption and diverting entry to restrictive settings such as residential care. A referral to this program signals risks for placement breakdown in the community and opportunities for placement stabilization services. Our study predicted youth ' s need for this program in advance so that caseworkers can engage in upstream, preventive case, placement, and service planning before the youth reaches the point of placement breakdown. For youth at any point in the agency ' s care who were placed in the community (e.g., kinship care, traditional foster care, specialized foster care, transitional living program) and not in residential care, we explored different models to predict youth ' s need for the program in the next 6 months to support preventive services.

Second, we addressed the research gaps in developing, comparing, and re /uniFB01 ning ML models as applied in the child welfare context for youth in foster care. Empirical studies in this area often focus on the child protective service investigation stage (Cuccaro-Alamin et al., 2017; Schwartz et al., 2017) or identifying the ' best ' performing model (e.g., most accurate) while overlooking more nuanced model comparisons regarding fairness, equity, and their tradeoffs (Chouldechova et al., 2018; Hall et al., 2023). Child welfare agencies that contemplate using ML for quality improvement need guidance on ' how ' to conduct this work. For these reasons, we situated our case study within a conceptual framework for integrating ML to support child welfare decisions (Chor, Rodolfa, et al., 2022).

Third, we compared ML models with an unregularized, multivariate regression model, which represents a proxy for a priori predictive models based on empirical literature on placement disruption or placement in residential care. Empirical studies often rely on regression-based predictions from a small pre-de /uniFB01 ned set of predictors -demographic characteristics (e.g., age, gender), child welfare history (e.g., out-of-home placement type, prior abuse/neglect type, number of prior child welfare spells and placements), and clinical/functioning characteristics (e.g., internalizing needs) -that are associated with youth ' s risk of running away from care, placement in residential care, or placement disruption (Chor et al., 2023; Chor et al., 2022; Courtney &amp; Zinn, 2009; Epstein et al., 2015). Our study compared

bottom-up, ' atheoretical ' ML approaches using a comprehensive set of available predictors with a top-down, theorydriven regression model that was more commonly found in the literature to expand the decision support options for child welfare agencies to consider. We used ML to predict whether youth in care will receive a placement stabilization and preservation program ( ' the program ' ) in the next 6 months. The models re /uniFB02 ected how caseworkers identi /uniFB01 ed youth ' s need for the program based on caseworkers ' assessment of placement disruption or need for residential care, but through the prediction provided an early detection of these needs so that caseworkers could intervene more proactively and effectively.

## Methods

## Study Setting

The study ' s use case was predicting whether youth in foster care would receive the placement stabilization and preservation program in a large Midwestern state. For over 10 years the child welfare agency has implemented the program designed to improve placement stabilization of youth under the agency ' s care by preserving youth and family connections in the community and minimizing placement changes (Illinois Department of Children and Family Services, 2013). The program is facilitator-guided and consensusand team-based. Program staf /uniFB01 ng involves caseworkers who bring together youth, family members, and key people in the youth ' s life, with the assistance and support of trained facilitators who lead discussions on the youth ' s needs, motivation, and capabilities.

Caseworkers can refer a youth to the program whenever: (1) a placement change from a home-based setting (e.g., traditional, specialized, relative foster home) or a transitional living program is considered because of dif /uniFB01 culties associated with youth ' s behavioral or emotional needs; (2) a safe, non-traditional placement is considered for youth who have reached the age of majority; or (3) a youth is in a temporary living arrangement (e.g., shelter) without an identi /uniFB01 ed placement. Program policy also focuses on youth with a history of police involvement, frequent school truancy, runaway behaviors, and/or untreated psychiatric disorders as characteristics of the target population. Ultimately, caseworkers identify speci /uniFB01 c youth on their caseloads to convene program staf /uniFB01 ng.

## Sample

We examined child welfare spells or episodes of 12,621 youth under the legal custody of a public child welfare agency of one large Midwestern state between January 2,

2017, and January 2, 2020. A youth ' s child welfare spell at a point in time during the study period was the unit of analysis. The state is legally responsible for a youth during a child welfare spell in which the youth lives in out-of-home care. Youth could have more than one spell during the study period and all these spells were included; within a spell, youth could also have more than one placement (e.g., traditional foster home), though at each prediction time point in this study, youth could not be placed in residential care. We used a combination of information about child welfare cases and the periods during which the state had legal responsibility for youth to de /uniFB01 ne spells. For the state to have legal responsibility for a youth, there must be an open child welfare case. However, the state does not have legal responsibility for all youth with an open case or for all periods of time between a case opening and case closing. For this reason, we excluded cases that opened but the state never had legal responsibility for the youth, including cases where the youth was taken into protective custody temporarily. We also excluded any period during which the state did not have legal responsibility for a youth and any period less than 8 days when the state had legal responsibility. The resulting 12,621 spells for youth in the sample consisted of 50.88% male, 59.9% White youth, 38.97% Black youth, 53.20% youth ages 0 -5, 26.07% ages 6 -11, and 20.73% ages 12 -17 (Appendix 1).

## Data Sources

Although the program has been operational for over 10 years, consistent data collection about the program only became more robust in the past 5 years. Thus, we used administrative data from the State Automated Child Welfare Information System (SACWIS) and other data provided by the state child welfare agency to identify the study sample of 12,621 spells for youth in care between January 20, 2017, and January 2, 2020. These administrative data were also used to operationalize the predicted outcome and predictors in the models, as follows.

## Prediction Outcome of Interest

We chose our prediction outcome as a binary classi /uniFB01 cation of program receipt in the next 6 months for the 12,621 spells for youth in the sample between January 20, 2017, and January 2, 2020. We de /uniFB01 ned the prediction period for the outcome (i.e., program receipt) as within 6 months of the time of prediction (Table 1). Thus, each model in each temporal cohort predicted whether program receipt occurred in the next 6 months (from prediction time) and whether this prediction was accurate using the prediction accuracy metric of precision (also known as positive predictive value), as de /uniFB01 ned below in the ' Model

<!-- image -->

Table 1 Temporal cohorts for training set and testing set

| Temporal Cohort       | Sample (N = 12,621)                   | Sample (N = 12,621)       | Sample (N = 12,621)       | Sample (N = 12,621)      | Sample (N = 12,621)      | Sample (N = 12,621)      |
|-----------------------|---------------------------------------|---------------------------|---------------------------|--------------------------|--------------------------|--------------------------|
| Temporal Cohort       | Training Set (n = 10,394)             | Training Set (n = 10,394) | Training Set (n = 10,394) | Testing Set (n = 11,161) | Testing Set (n = 11,161) | Testing Set (n = 11,161) |
| Temporal Cohort       | Prediction Date                       | n                         | Prediction Period         | Prediction Date          | n                        | Prediction Period        |
| Cohort #1 (4 months)  | 2 nd day of month on 1/2/17 - 4/2/17  | 1184                      | 1/2/17 - 10/1/17          | 10/2/17                  | 3547                     | 10/2/17 - 4/2/18         |
| Cohort #2 (7 months)  | 2 nd day of month on 1/2/17 - 7/2/17  | 2451                      | 1/2/17 - 1/1/18           | 1/2/18                   | 4651                     | 1/2/18 - 7/2/18          |
| Cohort #3 (10 months) | 2 nd day of month on 1/2/17 - 10/2/17 | 3830                      | 1/2/17 - 4/1/18           | 4/2/18                   | 5917                     | 4/2/18 - 10/2/18         |
| Cohort #4 (13 months) | 2 nd day of month on 1/2/17 - 1/2/18  | 5114                      | 1/2/17 - 7/1/18           | 7/2/18                   | 7000                     | 7/2/18 - 1/2/19          |
| Cohort #5 (16 months) | 2 nd day of month on 1/2/17 - 4/2/18  | 6626                      | 1/2/17 - 10/1/18          | 10/2/18                  | 8169                     | 10/2/18 - 4/2/19         |
| Cohort #6 (19 months) | 2 nd day of month on 1/2/17 - 7/2/18  | 8049                      | 1/2/17 - 1/1/19           | 1/2/19                   | 8455                     | 1/2/19 - 7/2/19          |
| Cohort #7 (22 months) | 2 nd day of month on 1/2/17 - 10/2/18 | 9623                      | 1/2/17 - 4/1/19           | 4/2/19                   | 8555                     | 4/2/19 - 10/2/19         |
| Cohort #8 (25 months) | 2 nd day of month on 1/2/17 - 1/2/19  | 10,394                    | 1/2/17 - 7/1/19           | 7/2/19                   | 8564                     | 7/2/19 - 1/2/20          |

Performance ' section. In practice, this prediction period meant that we replicated the existing use of the program and provided caseworkers with the prediction at most 6 months in advance. The 6-month prevention window re /uniFB02 ected the potential time caseworkers would need to coordinate care, services, and placement resources for youth who would otherwise likely experience placement disruption in the future. We operationalized the prediction as a ' need ' score for the program in the next 6 months, generated monthly to assist case management of youth on the caseworkers ' caseloads.

## Predictors Generated

We used the state child welfare agency administrative data to operationalize all predictors (also known as features). Prior exploratory studies of predictive modeling for the same state child welfare agency (Chor et al., 2023; Chor et al., 2022) guided the selection and operationalization of the predictors in this study, including:

1. Demographic characteristics: Youth ' s gender (male, female); race (White, Black, other combining Asian, Hawaiian or Paci /uniFB01 c Islander, and Native American due to small sample sizes); and age at the beginning of each placement within a child welfare spell.
2. Child welfare and placement characteristics: Administrative region of the child welfare spell (A, B, C, D); number of prior spells ever during and before the study period; a binary indicator variable of whether youth were assigned a handicap code suggesting the

<!-- image -->

presence of one or more type of developmental disability; binary indicators on the presence or absence of the eight types of child abuse or neglect allegation investigated most recently preceding each placement in the spell (sexual abuse, physical abuse, substance exposed infants, emotional abuse, lack of supervision, environmental neglect, other neglect, substantial risk of harm); number of prior placements during each spell; a binary indicator of placement with sibling in the same placement; prior placement type (home-based, independent living, residential, interruption, other); number of prior experiences with the program; and number of prior level-of-care assessment, the Child and Adolescent Service Intensity Instrument (CASII), administered as part of the program referral process but was in itself not suf /uniFB01 cient for program enrollment (e.g., CASII severity not justifying program need).

3. Clinical characteristics: Youth ' s Child and Adolescent Needs and Strengths (CANS) assessment is administered periodically per child welfare agency policies and procedures to all youth to inform service and treatment planning. The CANS uses a scale from ' 0 ' to ' 3 ' for each item. All CANS users must be certi /uniFB01 ed with respect to standards set by the CANS-using child welfare agency and are trained to integrate youth and caregiver voices in this collaborative assessment of needs and strengths (Lyons, 2009). Ratings of ' 0 ' or ' 1 ' indicate no evidence of need or intervention necessary. ' Actionable ' needs and strengths are items that are rated as a ' 2 ' or ' 3 ' and require direct action

to address those needs and strengths in treatment. In this study, we focused on /uniFB01 ve CANS domains -School Achievement (one item), Traumatic Stress Symptoms ( /uniFB01 ve items), Emotional/Behavioral needs (13 items), Risk Behaviors (10 items), and Social Functional Behavior (three items) -as indicators of youth ' s needs. Item scores in each CANS domain were averaged based on prior use of the CANS in other predictive modeling studies (Chor et al., 2023; Chor et al., 2022) on the same state child welfare agency as this study ' s. Averaged CANS domain scores also facilitated interpretation across domains using the same rating scale of the CANS (i.e., ' 0 ' , ' 1 ' , ' 2 ' , ' 3 ' ). For each CANS domain for each youth, we examined the prior (within 3 years, 1 year, 3 months, or 30 days) number of CANS administered, average prior average CANS domain scores, smallest prior average CANS domain scores, and largest prior average CANS domain scores. We also examined a binary indicator of whether youth had a most recent CANS, how long ago (days) the most recent CANS was, and the average most recent CANS domain scores. Although state child welfare agency policies and procedures require periodic administration of the CANS to all youth in care (e.g., in administrative case reviews), in practice the CANS is not always administered or administered in a timely fashion for many reasons (e.g., heavy caseloads). Thus, in addition to average most recent CANS scores, by domain, a missing CANS binary indicator was also included to denote missing prior CANS (within 3 years, 1 year, 3 months, or 30 days) and missed opportunities to detect youth ' s needs.

The above predictors were further divided into a small set (60 predictors resembling conventional model building) and the full set (all 154 predictors). Each level in a predictor category (e.g., race = Black) constituted one predictor. This small set vs. full set contrast allowed the models to explore whether the relative gains of using more predictors would come at the cost of diminishing return or over /uniFB01 tting the models (Yarkoni &amp; Westfall, 2017).

## Data Analysis

We performed all data analytic steps described below -development of the training sets, testing (or validation) sets, ML models, model comparisons, and model evaluation -using the open-source ML toolkit, Triage (Ackermann et al., 2018), in a Python 3.7 environment (Python Software Foundation, 2020). Post-hoc analyses on equity were performed using R 4.0.3 (The R Foundation for Statistical Computing, 2020). This study received ethics approval from the University of Chicago Crown Family School of Social Work, Policy, and Practice and Chapin Hall Institutional Review Board (IRB) and the Illinois Department of Children and Family Services IRB. A waiver of informed consent was obtained and approved by the IRBs for the use of secondary data collected by DCFS for youth in DCFS care.

## Model Training and Testing Sets

The study sample of 12,621 spells were divided into eight temporal validation folds/cohorts (Table 1 and Appendix 2) for iterative training and testing the predictive models to ensure that model selection would re /uniFB02 ect performance and generalize when used for future policy and implementation context (Hyndman &amp; Athanasopoulos, 2018). Although the same youth ' s spell could appear in more than one cohort if the spell met the eligibility criteria at a given time (i.e., youth not in residential care), the outcome prediction windows between the training set and the testing set could not overlap. Each temporal cohort was separated by 3 months to allow for variations in training the predictive models. The latter temporal cohorts had a growing and longer window for training the model than earlier temporal cohorts (e.g., 25-month training window for temporal Cohort #8 vs. 4-month training window for Cohort #1), allowing the models to learn from a longer range of historical training data as more data became available.

Within each training set, we sampled predictors and outcomes monthly to provide the models with a reasonable degree of variation to learn from -sampling too infrequently might miss details such as seasonality or changes in child welfare practice while sampling too frequently might provide redundant information that could increase computational burdens. As Appendix 2 shows, model performance was evaluated in 3-month intervals, re /uniFB02 ecting the need to periodically retrain and retest models to avoid degradation in performance due to changes in policy, practice, or context. This temporal training-testing split therefore accounted for changes in how caseworkers used the program over time such that the predictive models would be sensitive to these contextual changes.

We used a rolling, temporal validation process to train and test the predictive models. Each predictive model was trained using monthly prediction dates for each temporal cohort in Table 1. For example, in the training set of temporal Cohort #1, each model used eligible spells observed on 1/2/17, 2/2/17, 3/2/17, and 4/2/17 to learn its model parameters (e.g., coef /uniFB01 cients in a regression model). Each trained model was then tested once per temporal cohort on a validation set to evaluate its ability to be generalized to unseen future data. For example, in the testing set of

<!-- image -->

Table 2 Predictive Model Grid

| Predictive Model Type                   | Hyperparameter                                                                                                                                                                                                                      | Number of Models in Each Training- Validation Cohort   | Number of Models in Each Training- Validation Cohort   |
|-----------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------|--------------------------------------------------------|
|                                         |                                                                                                                                                                                                                                     | Small Set of Predictors                                | Full Set of Predictors                                 |
| Machine Learning (ML)                   |                                                                                                                                                                                                                                     |                                                        |                                                        |
| Random Forest                           | • Number of trees: 100, 1000, 10,000 • Number of tree levels: No limit, 5, 10, 50 • Maximum number of predictors per tree: square root, log 2 • Minimum number of sample observations required to split an internal node: 1, 10, 50 | 72                                                     | 72                                                     |
| Regularized Logistic (Logit) Regression | • Strength of inverse regularization c-parameter: 0.00001, 0.001, 0.01, 0.1, 1, 2, 10 • Regularization penalty: L1 (Lasso), L2 (Ridge)                                                                                              | 14                                                     | 14                                                     |
| Decision Tree                           | • Number of tree levels: No limit, 1, 2, 5, 10, 50 • Minimum number of sample observations required to split an internal node: 10, 50, 100 • Minimum number of sample observations required to be at a leaf note: 1, 10, 50, 100    | 72                                                     | 72                                                     |
| Dummy Classi /uniFB01 er (Baseline)     | • None                                                                                                                                                                                                                              | 1                                                      | 1                                                      |
| Conventional Unregularized Logit        | • None                                                                                                                                                                                                                              | 1                                                      | 1                                                      |
| Total Number of Models                  |                                                                                                                                                                                                                                     | 160                                                    | 160                                                    |
| Total Number of Models across Eight     | Prediction Time Points                                                                                                                                                                                                              | 1280                                                   | 1280                                                   |

temporal Cohort #1, each model was tested once on 10/2/17 for eligible spells. In this way, models trained and tested on earlier cohorts would re /uniFB02 ect usage of the program further in the past whereas models trained on latter cohorts would re /uniFB02 ect cumulative and recent usage of the program over time.

## ML Model Types

For each training-testing cohort described in Table 1, we explored three types of supervised learning, predictive ML models using the same predictors and the same outcome de /uniFB01 ned above, as compared to one baseline model: (1) Random forests, which build on the ability of decision trees to capture non-linear relationships while guarding against over /uniFB01 tting by making use of many decision trees, each trained on a subset of the examples (via bootstrap resampling) and random sample of the predictors, averaging across the predicted scores of the constituent trees (Breiman, 2001; Liaw &amp; Wiener, 2002); (2) Regularized logistic regression, in predicting a binary outcome classi /uniFB01 cation, applies penalty terms to safeguard against over /uniFB01 tting a regression (Lee et al., 2006); (3) Single decision trees, which capture some non-linear relationships and interactions in the data while still providing a relatively simple, readily-interpretable model (Myles et al., 2004); and (4) Dummy classi /uniFB01 er model, which makes predictions from the most frequent outcome, rather than from learning the underlying data patterns. This

<!-- image -->

model should be equivalent to the base rate expectation for the study outcome (i.e., program receipt), though it varied from one training-validation cohort to another. We varied hyperparameters under different assumptions for each model type to optimize performance and capture the relationships between the data and predicted outcome (Table 2). In total, 160 models were trained and tested using the small set of predictors and 160 models using the full set of predictors. Applied to the eight prediction time points in the testing sets (Table 1), 2,560 models (i.e., 1,280 using the small set of predictors; 1,280 using the full set of predictors) were run.

## Unregularized Regression as a Conventional Predictive Model for Comparison with ML Models

We developed an unregularized logistic regression model (Table 2) to represent a conventional predictive model used in child welfare and human services studies (Janczewski &amp; Nitkowski, 2023; Negriff et al., 2022). The predictor selection (i.e., small set vs. full set), outcome de /uniFB01 nition, and performance of this pre-speci /uniFB01 ed regression model was examined the same way as the ML models in each trainingtesting cohort described in Table 1.

## Model Performance

We operationalized model performance based on the ef /uniFB01 -ciency of program referrals, which depended on

caseworkers ' caseload sizes and program capacity. Ideally, every non-residential care youth at any moment at risk of placement disruption should be identi /uniFB01 ed and could be referred to the program. However, caseworkers might have large caseloads (e.g., &gt;20 cases) and the program might only have a limited capacity (e.g., 100 youth). This means ef /uniFB01 ciency of the program can be optimized by proactively identifying youth who need this program in advance.

To focus our model selection on the speci /uniFB01 cation that performed best under these capacity constraints, we used precision (also known as positive predictive value) among the 100 highest-need youth (i.e., precision at top 100) to evaluate the global prediction accuracy of each model for each of the eight testing sets -that is, among youth not in residential care predicted to receive the program in the next 6 months, precision among the top 100 highest-need youth measures the percent of youth who actually received the program in the next 6 months. This metric re /uniFB02 ected how ef /uniFB01 ciently resources (e.g., caseworker time, preventive services) used to assist those 100 youth would be allocated relative to their needs for the program. The top 100 threshold was commensurate with the average caseload and bandwidth of the program to prioritize youth ' s needs in a 6-month period. Because we were most interested in identifying model speci /uniFB01 cation whose performance was robust over time, we examined precision at each time point in the testing set from eight predictions made at 3-month intervals between 10/2/17 and 7/2/19 (Table 1).

## Model Comparisons and Interpretation

We trained and validated the grid of predictive models (Table 2) across the eight validation sets (Table 1), yielding a total of 1,280 models with a small predictor set and 1280 models with the full predictor set. Predictive models were /uniFB01 rst compared on accuracy (i.e., precision) across the eight temporal cohorts (Table 1) to observe model performance over time in the testing sets among the top 100 youth with the highest need for the program within 6 months of each prediction. We prioritized average precision over time given the longevity of the program and therefore the importance of the models to be consistently precise over time. Other metrics for model comparisons included: (1) AUC (area under the curve [AUC], the Receiver Operating Characteristic [ROC] curve) as a measure of model /uniFB01 t without choosing a need score threshold; (2) Jaccard similarity for identifying the overlaps of the top 100 highest-need youth across the models; and (3) Jaccard correlation of top 10 predictors across the models. We examined the top important predictors of the models based on Gini importance, a measure of total decrease in node impurity averaged across all trees in a random forest (Archer &amp; Kimes, 2008), and the magnitude of regression coef /uniFB01 cients. Additionally, we examined the top 100 highest-need youth recommended for the program by each model about their demographic characteristics, needs, and case histories to inform prevention strategies.

## Model Fairness and Equity

Understanding the appropriate ways to measure model fairness in context re /uniFB02 ects stakeholder and societal values and the harms that might result from different errors (Ahn et al., 2021; Rodolfa, Saleiro, et al., 2020). In this study, there would be greater harm to youth who needed the program but were not selected (i.e., false negatives) than providing assistance to youth whose actual need was lower (i.e., false positives). Thus, we evaluated and audited fairness among the predictive models as of the most recent prediction time point (7/2/19) by focusing on true positive rate (TPR), also known as recall or sensitivity, within subgroups across key attributes (e.g., race, gender) to ensure that the program could equitably serve diverse youth. In this study, TPR meant among all youth with a given attribute who were referred to the program in the next 6 months (i.e., actual need), the percent of youth a model also predicted the youth to receive the program in the next 6 months (i.e., true positive). This fairness metric addressed ' equality of opportunity, ' by which a program that cannot serve everyone with a need can at least serve a subset of individuals who are representative of the need distribution across protected classes in a population (Hardt et al., 2016).

Focusing on the program capacity for 100 youth, we examined TPR when the top 100 highest need scores were used to de /uniFB01 ne a positive prediction of program receipt in the next 6 months (i.e., the youth who would receive the program if the model was put into practice) and all lower need scores were used to de /uniFB01 ne a negative prediction of referral to the program in the next 6 months (i.e., the youth who would not receive the program per the model). We examined TPR by youth ' s attribute (e.g., race, gender) to quantify potential disparities between subgroups (e.g., Black youth vs. White youth) within and across the predictive models.

## Results

The model grid search in Table 2 and the a priori focus on consistent precision in the eight temporal periods identi /uniFB01 ed candidate models for each model type. Table 3 summarizes key model results focusing on three model types: (1) ML random forest; (2) ML regularized regression; and (3) Unregularized regression, by small vs. full predictor set used to build the models. Decision tree and dummy classi /uniFB01 er models provided baseline or business-as-usual

<!-- image -->

<!-- image -->

Table 3 Key Comparisons of Candidate Predictive Models

|                               |             |                               |                 |            |            |             |            |                       | of prior (ever program of prior year) CASII Number of placements; highest and CANS Risk Most recent days) average Behaviors                                                                                                                                                                                                                                                                                                                 |
|-------------------------------|-------------|-------------------------------|-----------------|------------|------------|-------------|------------|-----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Unregularized Regression (UR) | Full Set    | 0.01, 0.33                    | 0.00, 0.33      | 0.03, 0.31 | 0.00, 0.30 | 0.00, 0.33  | 0.04, 0.36 | 0.02, 0.42 0.03, 0.49 | Age; Number and 1 year) receipts; Number (ever and 1 assessments; prior (1 month) Prior (ever) lowest average Behaviors; (ever and 5 CANS Risk                                                                                                                                                                                                                                                                                              |
| Unregularized Regression (UR) | Small Set   | 0.24, 0.79                    | 0.24, 0.80      | 0.27, 0.81 | 0.27, 0.79 | 0.18, 0.77  | 0.15, 0.78 | 0.21, 0.78 0.17, 0.76 | Prior (3 months) CANS Emotional/Behavioral Needs; Prior interruption; (ever) spells; average CANS Functional Behaviors, Behaviors; Most allegation of prior to each Prior (3 months) CANS Traumatic Symptoms; Administrative region C (ever); allegation of of Harm prior placement                                                                                                                                                         |
| Regularized Regression (RR)   | Full Set d  | 0.22, 0.79                    | 0.32, 0.80 0.38 | , 0.80     | 0.35, 0.80 | 0.28 , 0.78 | 0.21, 0.79 | 0.21, 0.79 0.24, 0.79 | months) Most prior of (ever) Substance (ever) Abuse Prior interruption; allegation each Number of prior placements; Number prior (1 month) receipts; Number (1 month) placements; Number of prior program receipts; Prior (3 months) average CANS Behavioral Needs; (ever) lowest average CANS Social Behaviors; Most (5 days) CANS Emotional/Behavioral Needs; Prior (1 lowest average Traumatic Stress Symptoms; Prior (3 months) highest |
| Regularized Regression (RR)   | Small Set c | 0.20, 0.78                    | 0.23, 0.80      | 0.26, 0.80 | 0.28, 0.79 | 0.21, 0.77  | 0.21, 0.78 | 0.24, 0.78 0.23, 0.76 | prior Risk Number of placements; average CANS Behavioral Needs; recent (ever) Environmental to each placement; prior (ever) allegation of Exposure prior placement; Most allegation of prior to each (ever) placement Most recent of Sexual Abuse placement                                                                                                                                                                                 |
| Regularized Regression (RR)   | Set b       | 0.80                          | 0.84            | 0.82       | 0.84       | 0.79        | 0.79       | 0.80 0.79             | Age; Number of (1 year) placements; Number of prior and 1 month) program receipts; Number (ever) placements; Number of prior program receipts; (ever) average CANS Behaviors; Prior highest average Emotional/Behavioral Needs, Risk Behaviors                                                                                                                                                                                              |
| Regularized Regression (RR)   | Full        | 0.23,                         | 0.31,           | 0.33,      | 0.42,      | 0.26,       | 0.29,      | 0.41, 0.41,           | Number of prior placements; Prior average CANS Behaviors, Emotional/ Needs; Prior placement Prior average CANS Stress Social Behaviors; of prior (ever) (ever) allegation Risk of to each Prior average CANS Achievement                                                                                                                                                                                                                    |
| Random Forest                 | Small Set a | Precision, 10/2/17 0.27, 0.81 | 0.30, 0.81      | 0.26, 0.82 | 0.31, 0.82 | 0.22, 0.79  | 0.19, 0.79 | 0.25, 0.80 0.27, 0.78 | Predictors on Age; (ever) (3 months) Risk Behavioral (ever) interruption; (3 months) Traumatic Symptoms, Functional Number spells; No of Substantial Harm prior placement; (3 months) School                                                                                                                                                                                                                                                |

CANS Risk Behaviors

Small/Full set = small/full set of predictors; Precision = precision among the top 100 highest-need youth; AUC = area under the curve; Bold numbers denote highest precision or AUC on a

prediction date

1000 trees, 10 tree levels, maximum of log2 predictors, 10 minimum splits

a

10,000 trees, 50 tree levels, maximum of predictors square root, 50 minimum splits

b

Inverse regularization c-parameter: 1; Regularization penalty: L2 (Ridge)

c

d Inverse regularization c-parameter: 2; Regularization penalty: L1 (Lasso)

comparisons for the three model types and are referenced as appropriate.

## Model Accuracy: Precision of Predicted Top 100 Highest-Need Youth

Table 3 shows that across the ML models (i.e., random forest, regularized regression) and across time (i.e., eight prediction time points in the testing sets), having the full predictor set than the small predictor set resulted in greater precision among the top 100 highest-need youth predicted to receive the program in the next 6 months. This gain in accuracy from an expanded predictor set, however, was not evident in the conventional, unregularized logistic regression models, in which the model with the full predictor set was unable to converge. Within the ML models, random forest using the full predictor set consistently outperformed random forest using the small predictor set and regularized regression. Relative to the baseline precision or program receipt rate of 4.31% from the dummy classi /uniFB01 er model, the precision of random forest using the full predictor set increased over time and reached 41%, almost 10 times more accurate than the baseline precision. Thus, of the top 100 highest-need youth for the program identi /uniFB01 ed by random forest using the full predictor set, 41 went on to receive the program in the next 6 months and this model could have potentially helped provide an early warning for those individuals.

## Additional Metric to Consider: AUC

AUC reached 0.80 and indicated excellent ranking and discriminant classi /uniFB01 cation across ML models (random forest and regularized regression) and the unregularized regression models using a small predictor set (Table 3). Random forest using the full predictor set consistently achieved the highest AUC. However, unregularized regression models using the full predictor set consistently produced poor (worse than random) discriminant classi /uniFB01 -cation (&lt;0.50).

## Model Similarities

## Identifying Top 10 Most Important Predictors

While some models had comparable model accuracy, model /uniFB01 t, or even highest-need youth identi /uniFB01 ed, they did not necessarily have the same important predictors of youth ' s program need, which means selecting one model over another could mean adopting different interventions strategies to address different risk or protective factors. Focusing on the top 10 important predictors, all models had close to no or at most small correlations (0.00 -0.10) with each other

(Appendix 3), which suggested different important predictors across the models. Among each model ' s top 10 most important predictors at the most recent prediction time point on 7/12/19 (Table 3), however, the predictors were categorically similar across the models. Youth ' s age, number of prior placement changes, and prior ratings in the CANS Emotional/Behavioral Needs domain were among the top 10 most important predictors across three or more models. The top 10 most important predictors within each model type also highlighted the kinds of predictors each model type prioritized. In random forest (both small and full predictor sets), unregularized or regularized regression using the full predictor set, the top 10 most important predictors concerned youth ' s prior functioning per the CANS assessment or prior program receipt. In the unregularized or regularized regression using the small predictor set, the top 10 most important predictors included speci /uniFB01 c types of prior abuse allegations.

## Identifying the Top 100 Highest-Need Youth

While some models had comparable model accuracy or model /uniFB01 t, they did not necessarily identify the same highestneed youth over time, which means model selection could impact different youth. As Appendix 2 shows, random forest using the full predictor set consistently demonstrated small to moderate overlaps of identi /uniFB01 ed youth (0.20 -0.42) with random forest using the small predictor set and regularized regression using the full predictor set. However, random forest using the small predictor set consistently demonstrated small to moderate overlaps of identi /uniFB01 ed youth (0.21 -0.52) with unregularized and regularized regression using the small predictor set. Regularized regression using the small predictor set exclusively demonstrated moderate to large overlaps of identi /uniFB01 ed youth (0.49 -0.89) with unregularized regression using the small predictor set. Regularized regression using the full predictor set consistently demonstrated small to moderate overlaps of identi /uniFB01 ed youth (0.25 -0.68) with regularized regression using the small predictor set. Unregularized regression using the small predictor set exclusively demonstrated moderate to large overlaps of identi /uniFB01 ed youth (0.49 -0.89) with regularized regression using the small predictor set. Yet, unregularized regression using the full predictor set had no overlap with any other models.

## Model Fairness and Equity

Appendix 4 shows model fairness (TPR) and equity (TPR disparity) across youth ' s attributes and candidate models among the top 100 highest-need youth identi /uniFB01 ed by each model as of 7/2/19 (n = 8564). A TPR disparity of 1.00 indicates equality in TPR relative to a reference group; a

<!-- image -->

TPR disparity above or below 1.00 indicates inequality. All candidate models except for regularized regression and unregularized regression using the small predictor set had higher TPRs for White youth than for Black youth (TPR disparity range: 1.48 -2.12). Random forest using the small predictor set, regularized regression using the small predictor set, and unregularized regression using the full predictor set indicated lower TPRs for male youth than for female youth (TPR disparity range: 0.45 -0.82), while the other models indicated relatively equal TPRs.

Regarding youth ' s most serious allegation prior to each placement within a child welfare agency spell, all models were generally less sensitive to detecting youth with different types of allegations compared to sexual abuse (TPR disparity range: 0.00 -0.74), though random forest using the small or full predictor set was more sensitive to detecting emotional abuse (TPR disparity: 1.33). Regarding prior child welfare involvement, all models except for unregularized regression using the full predictor set were more sensitive to detecting youth with prior entry to the child welfare system than those with no prior history (TPR disparity range: 1.05 -2.74). All models except for unregularized regression were more sensitive to detecting youth at the time of prediction who were not in a placement (i.e., interruption) than youth placed in home-based settings (TPR disparity range: 1.99 -6.58); but were less sensitive to detecting youth who were placed with a sibling than those without a sibling or in different placements than their siblings (TPR disparity range: 0.13 -0.44).

All models were less sensitive to detecting youth from regions outside of the state ' s central region A (TPR disparity range: 0.00 -0.79). However, random forest, regularized regression, and unregularized regression models using the small predictor set were more sensitive to detecting youth from the state ' s northern region C (TPR disparity range: 1.22 -3.35). Certain attributes of youth who received the program were such rare events that TPR might not be meaningful to interpret, including race other than Black or White (n = 1), developmental disability status (n = 4), prior allegation of emotional abuse (n = 2), and placement in an independent living setting (n = 4).

## Discussion

This study examined a use case for conceptualizing, developing, and applying ML predictive models to help caseworkers in a state child welfare system identify youth who need advanced supports from an existing placement stabilization and preservation program to prevent placement disruption and entry to residential care. The potential for this type of predictive, preventive decision support for services for youth in care (Janczewski &amp; Nitkowski, 2023)

<!-- image -->

is timely and critical under the Family First Prevention Services Act (Family First; P. L. 115-123), which prioritizes placement in home-based settings and clinically appropriate use of residential care (American Academy of Pediatrics &amp; Chapin Hall at the University of Chicago, 2023). We found preliminary evidence that ML predictive models outperformed conventional unregularized regression regarding accuracy and fairness/equity.

Speci /uniFB01 cally, we answered the question, ' Can we identify and support youth in child welfare who are not in residential care, but will be at risk for placement disruption and need the placement stabilization program in the next 6 months? ' Using 3 years of child welfare administrative data on 12,621 spells for youth in the care of the child welfare agency in one large Midwestern state, we developed a wide grid of ML predictive models (random forest, regularized regression) to juxtapose conventional unregularized regression models in predicting youth with the highest need for the program to inform proactive case management decision-making. Unlike common predictive model approaches that rely on one set of data to train the models and one holdout set of data to test and validate the models (Chouldechova et al., 2018), we temporally retrained, retested the grid of models, and varied their hyperparameters and number of predictors, over eight pairs of training-testing sets (Ye et al., 2019), thereby allowing more nuanced model comparisons regarding accuracy (i.e., precision among the top 100 highest-need youth, AUC), similarities (i.e., identi /uniFB01 ed youth and important predictors), fairness (i.e., TPR), and equity (i.e., TPR disparity across key youth attributes), with nuanced research, practice, and policy implications in child welfare.

## Research Implications

Results highlighted the superior performance of random forest in prediction accuracy and accuracy stability over time relative to the other model types, whose performance was subpar or degraded over time. The random forest model ' s prediction accuracy further bene /uniFB01 ted from using the full set rather than a small set of predictors, which suggested an outcome such as child welfare program need likely has complex relationships with its predictors operating at multiple levels and therefore is better predicted using non-parametric ML models such as random forest (Janczewski &amp; Nitkowski, 2023; Negriff et al., 2022). On the other hand, as a counterpoint to ML models, traditional unregularized regression consistently underperformed in prediction accuracy and stability and could not converge when using the full set of predictors likely due to multicollinearity. The contrasting /uniFB01 ndings between ML models and unregularized regression in this study show that when conceptualized, designed, and evaluated with care -using

multiple temporal cross-validation sets, different indices of prediction accuracy, and many literature-informed predictors (Negriff et al., 2022; Yarkoni &amp; Westfall, 2017) -ML models can augment prediction in ways that traditional statistical methods cannot achieve due to constraints such as statistical assumptions.

Our use case focused on the top 100 youth identi /uniFB01 ed by each model monthly as having the highest need for the program. We also compared models based on the overlap in these identi /uniFB01 ed youth across models over time, even when model accuracy was comparable. The most accurate model -random forest using the full predictor set -tended to identify similar youth as those identi /uniFB01 ed by random forest using the small predictor set or regularized regression using the full predictor set. In contrast, regularized regression and unregularized regression tended to identify the same youth. By identifying the target, need-based populations, these model comparisons operationalized the potential impact of the models for practitioners and policymakers to consider. Whereas existing studies often only prioritize prediction accuracy or predict for the sake of prediction (Yarkoni &amp; Westfall, 2017), this study offers alternative, meaningful ways to evaluate and compare predictive models that may have been overlooked in child welfare research but is critical for supporting human decision-makers in making better decisions that can lead to better outcomes.

Comparisons of most important predictors and their similarities across models provided yet another way to evaluate predictive models. In this study, likely due to the sheer number of predictors, there was little to no similarity in the top 10 most important predictors across models, whether between or within ML models and traditional regularized regression. However, a closer examination of the types of important predictors showed commonalities across the predictive models -youth ' s age, number of prior placement changes, and prior ratings in the Emotional/Behavioral Needs domain of the CANS assessment -that are consistent with the characteristics of the target population of the program per policies and procedures. Substantively, this means ML models such as random forest in our use case, without compromising prediction accuracy, have comparable interpretability relative to conventional regularized regression. Thus, this study provides evidence that counters the often pejorative assumption that ML models are ' black boxes ' or dif /uniFB01 cult to interpret (Church &amp; Fairchild, 2017; Drake et al., 2020; Yarkoni &amp; Westfall, 2017), especially when important predictors categorically converge with those in the presumably ' more interpretable ' unregularized regression models (Janczewski &amp; Nitkowski, 2023).

Most importantly, skeptics and critics of ML predictive models that in /uniFB02 uence child welfare decisions, especially those informing child removal (Chouldechova et al., 2018), share concerns about perpetuation of institutional bias (e.g., systemic racism) and inequity (e.g., resource allocation) visà-vis these models (Drake et al., 2020; Glaberson, 2019). These concerns, however, are not endemic to ML predictive models but to any predictive models, including the more socially accepted traditional regression. To explore fairness and equity, this study provided a transparent view of each model ' s TPR and TPR disparity across youth ' s attributes such that the degree of fairness and equity could be quanti /uniFB01 ed, compared, and used to inform model selection and re /uniFB01 nement (Ahn et al., 2021). In this study, because the models found higher TPRs for White youth than for Black youth in the most recent testing cohort as of 7/2/19 (Appendix 4), the models were generally more sensitive to detecting the program need of White youth than Black youth among the top 100 highest-need youth. In other words, among all youth, by race, who were referred to the program in the next 6 months (i.e., actual need), the percent of whom a model also predicted to receive the program in the next 6 months (i.e., true positive) was higher for White youth than for Black youth. This /uniFB01 nding suggested racial inequity that was agnostic to model types and to the sample racial distributions (Appendix 1) as TPR by race used a race-speci /uniFB01 c denominator. This /uniFB01 nding also suggests potential empirical strategies for reducing inequity, for example, by increasing the target capacity (e.g., top 200, 500 youth). Though beyond the scope of this study, these strategies can illuminate tradeoffs between broadening the pool of identi /uniFB01 ed youth who could more equitably bene /uniFB01 t from preventive placement stabilization and the capacity of caseworkers to respond to these youth ' s needs in a timely manner. Relative TPR disparities in other youth attributes such as type of prior allegation, number of prior child welfare spells, and administrative region responsible for a youth ' s case might re /uniFB02 ect inequitable child protection and casework practice rather than inherent youth characteristics that lead to inequitable uses of the program. While the general rule of thumb of aiming for a TPR of 1.00 to reach equity applies to most situations (Chor et al., 2023), in some circumstances TPR disparities ' favoring ' underserved subgroups in the target population or setting different TPR disparity benchmarks for different subgroups might be appropriate when the system ' s status quo disadvantages these subgroups relative to the majority, privileged group (Rodolfa et al., 2020).

## Practice Implications

This study has direct implications on casework practice. In addition to exploring inequity associated with ML prediction, we did not /uniFB01 nd inequitable administration of the CANS by caseworkers regarding the needs of speci /uniFB01 c youth, by gender or by race, being undetected due to

<!-- image -->

missing the CANS, which was consistently at 7 -8% (Appendix 5). However, the state child welfare system in question struggles with caseworker shortage as evidenced by nearly a 25% vacancy and high caseloads as evidenced by consistently falling short of capping the average caseload of no more than 12 -15 new cases per month as required by a consent decree (State of Illinois Of /uniFB01 ce of the Auditor General, 2022). These systemic challenges trickle down to individual caseworkers who have more information they can process and more case management decisions they must make for more youth assigned to their caseloads. Considering placement disruption is a hallmark predictor of prolonging youth ' s length of stay in the child welfare system and delaying their exit to permanency (Stott &amp; Gustavsson, 2010), equipping caseworkers with tools and empirical information about youth with a high need for placement stabilization has potential to improve both the ef /uniFB01 ciency and quality of casework decision-making. For example, within a 6-month window (i.e., the study ' s prediction timeframe), caseworkers could prioritize youth with a high ' need ' score and preempt placement disruption by proactively coordinating community-based services and treatments that address individual youth ' s need factors, whether that be age-speci /uniFB01 c services or evidence-based treatments that target speci /uniFB01 c emotional/behavioral needs. This practice shift from the way the program is currently deployed, at times ' too late ' or reactive when youth might have already experienced precipitating placement disruption, could alleviate the time, personnel, and resource constraints placed on caseworkers. This type of ML predictive model-supported preventive service planning for youth who are in foster care is understudied but sorely needed (Chor et al., 2023; Chor et al., 2022) when its potential is often overshadowed by the /uniFB01 eld ' s focus on the perils of predictive model-assisted punitive decision-making such as screening in a maltreatment report for a child protective service investigation (Chouldechova et al., 2018) or removing a child from their home (Centre for Social Data Analytics, 2019). It also presents opportunities for caseworkers to conduct mid-course corrections to the systemic disparities inherited from decisions made upstream (i.e., to screen in an investigation or to remove a child) by allocating services and resources to overrepresented but underserved youth in the child welfare system.

## Policy Implications

As state child welfare agencies implement the Family First Prevention Services Act (P. L. 115-123) to invest in family-based settings and reduce the inappropriate or unnecessary use of residential care, they can leverage and integrate ML and non-ML predictive models into system reforms, including tailoring services and treatment

<!-- image -->

resources to high-need youth, streamlining needs-based placement in family-based settings, and adaptive case assignment to caseworkers not only in terms of quantity but also complexity of the cases. The state child welfare agency in this study offers speci /uniFB01 c placement stabilization services. Eligibility, referral, and allocation for these services can be anchored to the predictive models ' identi /uniFB01 -cation of high-need youth. Similarly, specialized or therapeutic foster home capacity (supply) can be calibrated with the volume of these high-need youth (demand) who are likely to experience placement disruption and step-up to residential care (Chor et al., 2023). Based on the cost-bene /uniFB01 t of therapeutic foster care compared to residential care in this state (Chor &amp; Oltmans, 2023), our predictive models can facilitate identi /uniFB01 cation of appropriate candidates for therapeutic foster care. Finally, to address systemic challenges in recruitment and retention of the child welfare workforce, it is possible to use predictive models such as those in this study to inform case assignment. Rather than capping caseloads by the absolute number of cases, it might be more meaningful to ensure equitable distribution of case needs (derived from the models) across caseworkers. In doing so, some caseworkers might have more cases on their caseloads made up of lower-need cases while others might have fewer cases on their caseloads made up of higher-need cases. In both scenarios, the cases assigned are need-adjusted or weighted despite the unequal number of cases assigned. This kind of systemic workforce change, however, is best tested in a pilot or a feasibility study /uniFB01 rst before taking it to scale statewide.

Ongoing monitoring and evaluation of predictive models as well as the downstream actions taken using it is necessary to inform their long-term implementation in the child welfare system. Although a selected model might meet a state child welfare agency ' s standards for accuracy, fairness, equity, and interpretability for deployment, it must be evaluated for its usage by caseworkers and impact on youth ' s outcomes. How, when, and why caseworkers use the model (e.g., innovation adoption, human-computer interactions) must be studied to examine the value-add of the model to the existing case work /uniFB02 ow. How, if, and for whom the model impact outcomes relevant to the model use (e.g., placement stability, time until permanency) must also be evaluated to understand how the target population (e.g., youth at risk of placement disruption) might bene /uniFB01 t from the model. The Centre for Social Data Analytics (2019) demonstrates how a predictive model can be rigorously evaluated for its use by child protection workers and supervisors, and its impact on key outcomes such as substantiation of an investigation and repeat maltreatment report. This evaluation framework can be applied to other types of predictive models and use cases such as ours.

## Limitations

This study has several limitations. First, administrative data from one state child welfare agency might not be generalizable to other states. Inherent /uniFB02 aws and concerns about data quality and objectivity are liabilities of all predictive models, ML or not, as much as we carefully vetted the data to operationalize relevant predictors based on existing studies. Second, because we operationalized the predicted outcome as youth ' s program receipt in the next 6 months, we assumed existing practice for offering the program, though proactively, should be replicated and that offering the program was a proxy indicator for current placement needs and future placement disruption or entry to residential care. Although the robust implementation of the program for over 10 years suggests that these assumptions were justi /uniFB01 ed, they should be evaluated in an impact or outcome study. Similarly, our de /uniFB01 nitions of fairness and equity using model recall or true positive rate (TPR) assumed the program as implemented should reach all youth who need it and equitably across youth subgroups at all times, which were aspirations that might not be realistic. Third, because this was a proof-of-concept study on the potential of MLaided decision-making, it did not evaluate downstream model usage or outcomes. Further, while this study operationalized transparency in model accuracy, fairness, equity, and interpretability to enhance service coordination and case management, the carefully designed and curated models could still be vulnerable to institutionalizing existing systemic and practice bias (Glaberson, 2019). In other words, an accurate, fair, equitable, and interpretable model does not mean implementation of the model will necessarily be accurate, fair, equitable, and interpretable. Relatedly, while we highlighted TPR disparities across youth attributes, we did not attempt bias reduction strategies (e.g., using different need score thresholds to balance bias across subgroups) since these decisions are better discussed during model deployment.

## Directions for Future Research

Future studies can build on this study. First, child welfare agencies, even with limited or targeted administrative data, should feel emboldened to explore the potential utility of predictive ML in supporting in-care decision-making, especially when there are methods such as counterfactual prediction that addresses unaccounted confounding due to data scarcity (Kiani et al., 2023). Given a high-performing ML model is only as good as it is implemented as a decision support tool, a thorough /uniFB01 eld trial -building a child welfare agency ' s internal capacity to use, maintain, and update ML, training caseworkers to respond to youth ' s placement needs, and addressing ethical considerations for deploying ML - will deepen the understanding of ML ' s impact on casework practice. Whether this practice change will impact youth ' s outcomes (e.g., placement stability, returning home) can be evaluated in a randomized controlled trial or a rigorous quasiexperimental study. Future studies can also test ML models at each casework decision-making point preceding, during, and following placement disruption for a more holistic exploration of ML applications. Finally, equity-driven child welfare agencies might focus on known disparities of interest (e.g., demographic characteristics, child welfare history, clinical needs) in the program ' s implementation to test the effect of speci /uniFB01 c bias reduction strategies (e.g., targeted training, peer consultation). Further examination of caseworker characteristics (e.g., experience, case assignment duration, caseload) might shed light on caseworkers ' differential program referral behavior depending on youth ' s race.

## Code Availability

The code used in the analysis is available upon request.

Supplementary information The online version contains supplementary material available at https://doi.org/10.1007/s10826-024-02993-x.

Author Contributions Ka Ho Brian Chor led the study conception and design. Material preparation, data collection, and analysis were performed by Zhidi Luo and Ka Ho Brian Chor. The /uniFB01 rst draft of the manuscript was written by Ka Ho Brian Chor and all authors commented on previous versions of the manuscript. All authors read and approved the /uniFB01 nal manuscript.

Funding The work was supported by the University of ChicagoChapin Hall Joint Research Fund.

## Compliance with Ethical Standards

Con /uniFB02 ict of Interest The authors declare no competing interests.

Ethics Approval The authors received ethics approval from the University of Chicago Crown Family School of Social Work, Policy, and Practice and Chapin Hall Institutional Review Board (IRB), and the Illinois Department of Children and Family Services (DCFS) IRB.

Informed Consent A waiver of informed consent was obtained and approved by the IRBs for the use of secondary data collected by DCFS for youth in DCFS care.

## References

Ackermann, K., Walsh, J., Unánue, A. D., Naveed, H., Rivera, A. N., Lee, S.-J., Bennett, J., Defoe, M., Cody, C., Haynes, L., &amp; Ghani, R. (2018). Deploying machine learning models for public policy: A framework Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, London, United Kingdom. https://doi.org/10.1145/3219819. 3219911.

Ahn, E., Gil, Y., &amp; Putnam-Hornstein, E. (2021). Predicting youth at high risk of aging out of foster care using machine learning

<!-- image -->

- methods. Child Abuse &amp; Neglect , 117 , 105059. https://doi.org/10. 1016/j.chiabu.2021.105059.
- Allegheny County Department of Human Services. (2017). Developing predictive risk models to support child maltreatment hotline screening decisions . Allegheny County Department of Human Services.
- American Academy of Pediatrics, &amp; Chapin Hall at the University of Chicago. (2023). Family First implementation: A one-year review of state progress in reforming congregate care . American Academy of Pediatrics &amp; Chapin Hall at the University of Chicago.
- Annie, E. Casey Foundation. (2002). Team decision-making involving the family and community in child welfare decisions part two: Building community partnership in child welfare . Annie E. Casey Foundation.
- Archer, K. J., &amp; Kimes, R. V. (2008). Empirical characterization of random forest variable importance measures. Computational Statistics &amp; Data Analysis , 52 (4), 2249 -2260. https://doi.org/10. 1016/j.csda.2007.08.015.
- Breiman, L. (2001). Random forests. Machine Learning , 45 (1), 5 -32. https://doi.org/10.1023/A:1010933404324.
- Centre for Social Data Analytics. (2019). Implementing a child welfare decision aide in Douglas County: Methodology report . Centre for Social Data Analytics.

Chambers, R. M., Crutch /uniFB01 eld, R. M., Willis, T. Y., Cuza, H. A., Otero, A., Goddu Harper, S. G., &amp; Carmichael, H. (2018). ' It ' s just not right to move a kid that many times: ' A qualitative study of how foster care alumni perceive placement moves. Children and Youth Services Review , 86 , 76 -83. https://doi.org/10.1016/j.childyouth. 2018.01.028.

- Child Welfare Information Gateway. (2020). Determining the best interests of the child . U.S. Department of Health and Human Services, Administration for Children and Families, Children ' s Bureau.
- Chor, K. H. B., Epstein, R. A., &amp; Luo, Z. (2023). Developing and validating a predictive risk model for youth placement in residential care to support decision-making under the Family First Prevention Services Act. Residential Treatment for Children &amp; Youth , 40 (3), 324 -347. https://doi.org/10.1080/0886571X.2022. 2111018.

Chor, K. H. B., Luo, Z., Dworsky, A., Raman, R., Courtney, M. E., &amp; Epstein, R. A. (2022). Development and validation of a predictive risk model for runaway among youth in child welfare. Children and Youth Services Review , 143 , 106689. https://doi.org/10.1016/ j.childyouth.2022.106689.

- Chor, K. H. B., McClelland, G. M., Weiner, D. A., Jordan, N., &amp; Lyons, J. S. (2015). Out-of-home placement decision-making and outcomes in child welfare: A longitudinal study. Administration and Policy in Mental Health and Mental Health Services Research , 42 (1), 70 -86. https://doi.org/10.1007/s10488-0140545-5.

Chor, K. H. B., &amp; Oltmans, C. (2023). Cost -bene /uniFB01 t of Treatment Foster Care Oregon (TFCO) versus residential care in Illinois. Research on Social Work Practice , 10497315231206752. https:// doi.org/10.1177/10497315231206752.

Chor, K. H. B., Rodolfa, K. T., &amp; Ghani, R. (2022). A conceptual framework for using machine learning to support child welfare decisions. arXiv:2207.05855. Retrieved February 1, 2024, from https://doi.org/10.48550/arXiv.2207.05855.

- Chouldechova, A., Putnam-Hornstein, E., Benavides-Prado, D., Fialko, O., &amp; Vaithianathan, R. (2018). A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions Proceedings of the 1st Conference on Fairness, Accountability and Transparency, Proceedings of Machine Learning Research. http://proceedings.mlr.press.
- Church, C. E., &amp; Fairchild, A. J. (2017). In search of a silver bullet: Child welfare ' s embrace of predictive analytics. Juvenile and Family Court Journal , 68 (1), 67 -81. https://doi.org/10.1111/jfcj. 12086.
- Courtney, M. E., &amp; Zinn, A. (2009). Predictors of running away from out-of-home care. Children and Youth Services Review , 31 (12), 1298 -1306. https://doi.org/10.1016/j.childyouth.2009.06.003.
- Cuccaro-Alamin, S., Foust, R., Vaithianathan, R., &amp; Putnam-Hornstein, E. (2017). Risk assessment and decision making in child protective services: Predictive risk modeling in context. Children and Youth Services Review , 79 , 291 -298. https://doi.org/10.1016/ j.childyouth.2017.06.027.
- Drake, B., Jonson-Reid, M., Ocampo, M. G., Morrison, M., &amp; Dvalishvili, D. (2020). A practical framework for considering the use of predictive risk modeling in child welfare. The ANNALS of the American Academy of Political and Social Science , 692 (1), 162 -181. https://doi.org/10.1177/0002716220978200.
- Epstein, Jr, R. A. (2004). Inpatient and residential treatment effects for children and adolescents: a review and critique. Child and Adolescent Psychiatric Clinics , 13 (2), 411 -428. https://doi.org/10. 1016/S1056-4993(03)00126-3.
- Epstein, R. A., Schlueter, D., Gracey, K. A., Chandrasekhar, R., &amp; Cull, M. J. (2015). Examining placement disruption in child welfare. Residential Treatment for Children &amp; Youth , 32 (3), 224 -232. https://doi.org/10.1080/0886571X.2015.1102484.
- Ghani, R., &amp; Schierholz, M. (2020). Machine learning. In I. Foster, R. Ghani, R. S. Jarmin, F. Kreuter, &amp; J. Lane (Eds.), Big data and social science: Data science methods and tools for research and practice (2nd ed., pp. 143 -191). CRC Press.
- Glaberson, S. K. (2019). Coding over the cracks: Predictive analytics and child protection. Fordham Urban Law Journal , 46 (2), 307 -363.
- Hall, S. F., Sage, M., Scott, C. F., &amp; Joseph, K. (2023). A systematic review of sophisticated predictive and prescriptive analytics in child welfare: Accuracy, equity, and bias. Child and Adolescent Social Work Journal . https://doi.org/10.1007/s10560-023-00931-2.
- Hardt, M., Price, E., &amp; Srebro, N. (2016). Equality of opportunity in supervised learning. In Advances in Neural Information Processing Systems (pp. 3315 -3323). https://ui.adsabs.harvard.edu/abs/ 2016arXiv161002413H.
- Havlicek, J. (2011). Lives in motion: A review of former foster youth in the context of their experiences in the child welfare system. Children and Youth Services Review , 33 (7), 1090 -1100. https:// doi.org/10.1016/j.childyouth.2011.02.007.
- Hyndman, R. J., &amp; Athanasopoulos, G. (2018). Forecasting: Principles and Practice (2nd ed.). OTexts.
- Illinois Department of Children and Family Services. (2013). Policy Guide 2013.03: Clinical Intervention for Placement Preservation (CIPP) . Illinois Department of Children and Family Services.
- Jackson, D., &amp; Marx, G. (2017). Data mining program designed to predict child abuse proves unreliable, DCFS says. Chicago Tribune. https://www.chicagotribune.com/investigations/ct-dcfseckerd-met-20171206-story.html.
- James, S. (2004). Why do foster care placements disrupt? An investigation of reasons for placement change in foster care. Social Service Review , 78 (4), 601 -627. https://doi.org/10.1086/424546.
- Janczewski, C. E., &amp; Nitkowski, J. (2023). Predicting mental and behavioral health service utilization among child welfareinvolved caregivers: A machine learning approach. Children and Youth Services Review , 155 , 107150. https://doi.org/10.1016/ j.childyouth.2023.107150.
- Jenkins, B. Q. (2021). Measuring the equity of risk assessment instruments used in child protection. Children and Youth Services Review , 131 , 106266. https://doi.org/10.1016/j.childyouth.2021. 106266.

<!-- image -->

- Keddell, E. (2019). Algorithmic justice in child protection: Statistical fairness, social justice and the implications for practice. Social Sciences , 8 (10), 281. https://doi.org/10.3390/socsci8100281.
- Kiani, S., Barton, J., Sushinsky, J., Heimbach, L., &amp; Luo, B. (2023). Counterfactual prediction under selective confounding. Frontiers in Arti /uniFB01 cial Intelligence and Applications , 372 , 1256 -1263. https://doi.org/10.3233/FAIA230403.
- Kim, H., &amp; Kao, D. (2014). A meta-analysis of turnover intention predictors among U.S. child welfare workers. Children and Youth Services Review , 47 , 214 -223. https://doi.org/10.1016/j. childyouth.2014.09.015.
- Lee, S. I., Lee, H., Abbeel, P., &amp; Ng, A. (2006). Ef /uniFB01 cient L1 regularized logistic regression. American Association for Arti /uniFB01 cial Intelligence (AAAI) , 6 , 401 -408.
- Liaw, A., &amp; Wiener, M. (2002). Classi /uniFB01 cation and regression by randomForest. R News , 2 (3), 18 -22.
- Lyons, J. S. (2009). Communimetrics: A communication theory of measurement in human service settings . Springer.
- McCurdy, B. L., &amp; McIntyre, E. K. (2004). ' And what about residential … ? ' Re-conceptualizing residential treatment as a stop-gap service for youth with emotional and behavioral disorders. Behavioral Interventions , 19 (3), 137 -158. https://doi.org/10.1002/bin.151.
- Myles, A. J., Feudale, R. N., Liu, Y., Woody, N. A., &amp; Brown, S. D. (2004). An introduction to decision tree modeling. Journal of Chemometrics , 18 (6), 275 -285. https://doi.org/10.1002/cem.873.
- Negriff, S., Dilkina, B., Matai, L., &amp; Rice, E. (2022). Using machine learning to determine the shared and unique risk factors for marijuana use among child-welfare versus community adolescents. PLOS ONE , 17 (9), e0274998. https://doi.org/10.1371/ journal.pone.0274998.
- Pecora, P. J., Whittaker, J. K., Barth, R. P., Borja, S., &amp; Vesneski, W. (2018). The child welfare challenge: Policy, practice, and research (4th, Ed.). Routledge.
- Python Software Foundation. (2020). Python 3.7 release . Python Software Foundation.
- Rodolfa, K. T., Saleiro, P., &amp; Ghani, R. (2020). Machine learning. In I. Foster, R. Ghani, R. S. Jarmin, F. Kreuter, &amp; J. Lane (Eds.), Big data and social science: Data science methods and tools for research and practice (2nd ed.). CRC Press.
- Rodolfa, K. T., Salomon, E., Haynes, L., Mendieta, I. H., Larson, J., &amp; Ghani, R. (2020). Case study: Predictive fairness to reduce misdemeanor recidivism through social service interventions Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, Barcelona, Spain. https://doi.org/10.1145/ 3351095.3372863.
- Russell, J. (2015). Predictive analytics and child protection: Constraints and opportunities. Child Abuse &amp; Neglect , 46 , 182 -189. https://doi.org/10.1016/j.chiabu.2015.05.022.
- Saxena, D., Badillo-Urquiola, K. A., Wisniewski, P., &amp; Guha, S. (2020). A human-centered review of algorithms used within the U.S. child welfare system. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems.
- Schwartz, I. M., York, P., Nowakowski-Sims, E., &amp; Ramos-Hernandez, A. (2017). Predictive and prescriptive analytics, machine learning and child welfare risk assessment: The Broward County experience. Children and Youth Services Review , 81 , 309 -320. https://doi.org/10.1016/j.childyouth.2017.08.020.
- State of Illinois Of /uniFB01 ce of the Auditor General. (2022). Performance audit of the Department of Children and Family Services child safety and well-being. State of Illinois Of /uniFB01 ce of the Auditor General.
- Stott, T., &amp; Gustavsson, N. (2010). Balancing permanency and stability for youth in foster care. Children and Youth Services Review , 32 (4), 619 -625. https://doi.org/10.1016/j.childyouth. 2009.12.009.
- The R Foundation for Statistical Computing. (2020). R version 4.0.3 . The R Foundation for Statistical Computing.
- Wisconsin Department of Children and Families. (2014). The P.S. program: Using predictive analytics in program implementation . Wisconsin Department of Children and Families.
- Yarkoni, T., &amp; Westfall, J. (2017). Choosing prediction over explanation in psychology: Lessons from machine learning. Perspectives on Psychological Science , 12 (6), 1100 -1122. https://doi.org/ 10.1177/1745691617693393.
- Ye, T., Johnson, R., Fu, S., Copeny, J., Donnelly, B., Freeman, A., Lima, M., Walsh, J., &amp; Ghani, R. (2019). Using machine learning to help vulnerable tenants in New York City Proceedings of the 2nd ACM SIGCAS Conference on Computing and Sustainable Societies, Accra, Ghana. https://doi.org/10.1145/3314344. 3332484.

Publisher ' s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional af /uniFB01 liations.

Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.

<!-- image -->