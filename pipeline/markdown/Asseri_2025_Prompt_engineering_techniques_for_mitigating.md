---
source_file: Asseri_2025_Prompt_engineering_techniques_for_mitigating.pdf
conversion_date: 2026-02-03T08:42:41.475128
converter: docling
quality_score: 95
---

## Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review

Bushra Asseri 1* , Estabrag Abdelaziz 1 and Areej Al-Wabil 1

1* College of Engineering &amp; Advanced Computing, Alfaisal University, Riyadh, 11533, Saudi Arabia.

*Corresponding author(s). E-mail(s): basseri@alfaisal.edu; Contributing authors: eabaker@alfaisal.edu; awabil@alfaisal.edu;

## Abstract

Large language models (LLMs) have demonstrated remarkable capabilities across various domains, yet concerns about cultural bias-particularly towards Arabs and Muslims-pose significant ethical challenges by perpetuating harmful stereotypes and marginalization. Despite growing recognition of bias in LLMs, prompt engineering strategies specifically addressing Arab and Muslim representation remain understudied. This mixed-methods systematic review examines such techniques, offering evidence-based guidance for researchers and practitioners. Following PRISMA guidelines and Kitchenham's systematic review methodology, we analyzed 8 empirical studies published between 2021-2024 investigating bias mitigation strategies. Our findings reveal five primary prompt engineering approaches: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts. Although all approaches show potential for reducing bias, effectiveness varied substantially across studies and bias types. Evidence suggests that certain bias types may be more resistant to prompt-based mitigation than others. Structured multi-step pipelines demonstrated the highest overall effectiveness-achieving up to 87.7% reduction in bias-though they require greater technical expertise. Cultural prompting offers broader accessibility with substantial effectiveness (71-81% improvement in cultural alignment). These results underscore the accessibility of prompt engineering for mitigating cultural bias without requiring access to model parameters. The limited number of studies identified highlights a significant research gap in this critical area. Future research should focus on developing culturally adaptive prompting techniques, creating Arab/Muslim-specific evaluation

resources, and integrating prompt engineering with complementary debiasing methods to address deeper stereotypes while maintaining model utility.

Keywords: Large Language Models, Prompt Engineering, Cultural Bias, Arabs, Muslims, AI Ethics

## 1 Introduction

Large language models (LLMs) have substantially advanced natural language processing by generating human-like text, answering questions, and performing a wide array of language tasks. Yet, research consistently demonstrates that these models can perpetuate-and even amplify-societal biases embedded in their training data (Bender et al. 2021; Blodgett et al. 2020). Of particular concern is cultural bias against Arabs and Muslims, which has been documented across various LLMs and poses significant ethical challenges in real-world deployments (Abid et al. 2021; Naous et al. 2024). This bias often takes the form of associating these groups with terrorism, violence, or religious extremism, thus reinforcing damaging stereotypes that can fuel marginalization and discrimination. A key lens for understanding how LLMs inherit and reproduce biases is captured in Bender et al. (2021) influential framing of these models as 'stochastic parrots.' This term underscores the idea that LLMs, no matter how sophisticated, essentially generate tokens based on probabilistic patterns gleaned from massive training corpora. LLMs do not possess genuine comprehension of meaning; rather, they recycle and remix the biases, stereotypes, and prejudices that are baked into their source data. Consequently, these models risk echoing or even exacerbating the cultural and religious biases present in online text and other data repositories. By focusing on statistical mimicry instead of semantic understanding, LLMs become conduits for embedding longstanding social hierarchies and inequities into automated systems, raising critical questions about ethics and responsibility in AI-driven technologies. To fully grasp why cultural bias against Arabs and Muslims emerges so strongly in LLMs, it is useful to consider Said (2019) concept of Orientalism. Said argues that Orientalism is not merely a set of negative stereotypes but rather a Western scholarly and cultural framework that has historically depicted Arab and Muslim societies as exotic, backward, or dangerous 'others.' This process of 'othering' served to justify and maintain colonial power structures, shaping cultural discourse in ways that outlasted the colonial era. Given that much of the data used to train LLMs originates from Western-centric sources, these Orientalist tropes can be inadvertently reinforced and reproduced by the models. Instead of neutral or objective representations of Arab or Muslim identities, LLM outputs may mirror the historical power imbalances and cultural misconceptions enshrined in the training corpus-thus perpetuating Orientalism in contemporary digital forms.

Beyond the high-level insights provided by Bender et al. (2021) and Said (2019), researchers have developed more granular frameworks for understanding the mechanics and ethics of bias in AI. Hovy and Prabhumoye (2021) propose a structured approach that identifies five sources of bias in natural language processing systems: data, the

annotation process, input representations, models, and research design. While this framework underscores the multifaceted origins of bias, Prakash and Lee (2023) further extend this analysis through the notion of 'layered bias,' highlighting how bias can be embedded not just at the dataset or annotation level, but also within the internal layers of transformer architectures themselves. Interventions must therefore reach beyond the surface (e.g., fixing a flawed training dataset) to grapple with how biases permeate deeper representational strata. From an ethical standpoint, Birhane (2021) calls for a shift from rational to relational thinking, emphasizing that technical fixes alone are insufficient to address systemic bias. This relational ethics perspective encourages examination of power structures, historical injustices, and lived experiences, resonating strongly with Said (2019) critique of how Western knowledge production has historically marginalized Arab and Muslim voices. Indeed, 'stochastic parrots' can reflect Orientalist biases if the training data-and thus the very fabric of these models-repeatedly portrays Arabs and Muslims through a limited, negative lens. Among the different types of biases studied in LLMs, cultural and religious bias against Arabs and Muslims has received increasing scholarly attention (Abid et al. 2021; Naous et al. 2024). Such bias can manifest both explicitly (e.g., negative or stereotypical completions to prompts) and implicitly (e.g., subtle misrepresentations of cultural practices). The concept of 'stereotype dimensions' (Schuster et al. 2025) helps clarify how models encode and perpetuate stereotypes about Arabs and Muslims through contextual embeddings, while the 'analogical bias theory' (Manzini et al. 2019) illustrates how these stereotypes become ingrained via analogy-based relationships in embeddings.

Critically, these studies also highlight the intersectionality of cultural, religious, and other identity factors, making mitigation efforts all the more nuanced. The persistence of these biases is especially troubling as LLMs become embedded in global applications such as virtual assistants, automated content moderation, and policy-making tools. Abid et al. (2021) further show that anti-Muslim bias in LLMs can fluctuate in tandem with geopolitical events and media coverage, increasing by up to 18% during periods of heightened attention to terrorism incidents. This underscores the need for agile mitigation strategies that can adapt to rapidly shifting social and political contexts-something mere dataset-level interventions may struggle to achieve. Given the diverse sources of bias (Hovy and Prabhumoye 2021) and their layered nature (Prakash and Lee 2023), prompt engineering has emerged as a compelling approach for mitigating problematic outputs, particularly in scenarios where users lack direct access to model parameters. By carefully crafting or modifying input prompts, users can guide LLM behavior without retraining the model-a critical advantage for closed-source commercial systems (Gallegos et al. 2024; Meade et al. 2022). Moreover, prompt engineering offers both accessibility (allowing non-technical stakeholders to participate) and adaptability (facilitating quick responses to emerging biases). However, the effectiveness of prompt engineering must be contextualized within broader ethical and sociopolitical frameworks. Birhane (2021) relational ethics challenges overly technicist solutions, arguing that any mitigation technique-prompt engineering included-must address embedded power differentials and historical legacies of oppression. Meanwhile, Prakash and Lee (2023) layered intervention theory suggests that adjusting prompts

alone may not be sufficient if biases persist within the deeper representational structures of a model. That said, recent empirical work points to real-world benefits: Gallegos et al. (2024) showed that non-technical users could reduce anti-Muslim bias in GPT-3.5 by 47% using simple prompting frameworks, while Meade et al. Meade et al. (2022) demonstrated that self-debiasing prompts could be reconfigured within 24-48 hours to address newly observed biases-a stark contrast to the lengthy process of model retraining. This rapid adaptability is particularly critical for managing cultural biases that can shift with evolving geopolitical events. Against this rich theoretical and practical backdrop, the present study offers a systematic review of prompt engineering techniques aimed at mitigating cultural bias against Arabs and Muslims in LLMs. By focusing specifically on cultural and religious biases that disproportionately affect Arabs and Muslims, this review seeks to fill a gap in the broader bias-in-AI literature, which has traditionally centered on gender and racial biases. We draw on sociotechnical perspectives to argue that bias mitigation cannot be divorced from historical power asymmetries and that purely technical solutions must be complemented by critical engagement with the social contexts in which these models are developed and deployed (Birhane 2021; Noble 2018; Said 2019).

This systematic review addresses a critical gap in AI ethics research by synthesizing fragmented knowledge on prompt engineering interventions specifically targeting anti-Arab and anti-Muslim bias. While extensive literature exists on technical aspects of bias mitigation and prompt engineering broadly (Bender et al. 2021; Meade et al. 2022), there remains a significant absence of comprehensive analysis focused on culturally-specific interventions for these marginalized groups (Abid et al. 2021; Mohamed et al. 2020). The significance of this work extends beyond academic understanding-as LLMs rapidly integrate into essential services from healthcare to education to public administration (Bommasani et al. 2022), biases affecting Arabs and Muslims (comprising nearly two billion people globally) risk perpetuating historical marginalization through new technological channels (Noble 2018; Said 2019). By identifying effective prompt engineering strategies and their underlying mechanisms, this review provides actionable guidance for developers, policymakers, and end users while advancing the theoretical understanding of how technical interventions interface with deeply embedded cultural biases (Birhane 2021; Hovy and Prabhumoye 2021). Furthermore, by centering Arab and Muslim experiences in AI ethics discourse, this work contributes to a more inclusive approach to responsible AI development that acknowledges and addresses the unique patterns of bias affecting different cultural and religious communities (Crawford 2021; Naous et al. 2024). The remainder of this paper is structured as follows: Section 2 details our methodology, including the search strategy, inclusion criteria, and quality assessment. Section 3 presents the findings according to our three research questions. In Section 4, we discuss theoretical implications, practical applications, and limitations. Lastly, Section 5 concludes with recommendations for future research, emphasizing the combined need for innovative prompt engineering and deep ethical reflection in effectively mitigating cultural bias in LLMs.

## 2 Method

This systematic review follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines (Page et al. 2021) and incorporates methodological elements from Kitchenham and Charters (2007) approach to systematic reviews in software engineering. Specifically, Kitchenham's guidelines informed the formulation of clear research questions, the structured search strategy, and the systematic inclusion/exclusion criteria. We employed a mixed-methods approach to synthesize both quantitative and qualitative evidence on prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in LLMs.

## 2.1 Search Strategy

We conducted a comprehensive search of the literature using the following electronic databases: IEEE Xplore, ACM Digital Library, Scopus, and Web of Science. These databases were chosen to ensure broad coverage of computer science, artificial intelligence, and computational linguistics research. We excluded Google Scholar due to its high noise-to-precision ratio and the extent of overlap with the other databases. To manage references and streamline the screening process, we used Covidence (Covidence 2024), which facilitated duplicate removal and transparent documentation. Additionally, Litmaps (2025) was used to assist in both backward and forward citation tracking by visualizing citation networks and identifying related studies not captured by the initial search terms. This comprehensive approach aligns with Kitchenham's recommendation to perform both database searches and citation-based exploration to capture the entirety of the relevant literature. The following Boolean search string was employed to identify potentially relevant articles:

('large language model*' OR 'LLM*' OR 'generative AI' OR 'foundation model*') AND ('prompt engineering' OR 'prompt design' OR 'prompting technique*' OR 'prompt*') AND ('bias' OR 'fairness' OR 'ethics') AND ('Arab*' OR 'Muslim*' OR 'Islam*' OR 'Middle East*')

The search covered publications from 2020 to 2024, reflecting the rapid development of LLMs and prompt engineering techniques during this period. We acknowledge that restricting the review to English-language publications may exclude relevant research published in Arabic or other languages, which is a limitation of this study.

## 2.2 Inclusion and Exclusion Criteria

## 2.2.1 Inclusion criteria

We established the following criteria for study eligibility:

- Scope of Techniques: The study must include a clear empirical evaluation of prompt engineering, augmentation, or modification techniques intended to mitigate cultural or religious biases in LLMs.
- Relevance to Arabs/Muslims: The study must address or analyze bias toward Arabs and/or Muslims, recognizing that these encompass diverse populations across

different countries, sectarian groups, linguistic communities, and intersectional identities, including multi-group studies that explicitly consider Arabs/Muslims as one demographic category.

- Empirical Evaluation: The study must present some form of empirical (quantitative or qualitative) evaluation, such as performance metrics or human assessments.
- Publication Type and Language: The study is available in English and published between 2020 and 2024 in peer-reviewed journals, conference proceedings, workshops, or gray literature (e.g., preprints on arXiv, technical reports) with sufficient methodological detail.

## 2.2.2 Exclusion criteria

Studies were excluded if they met any of the following criteria:

- No Prompt-Based Approach: Focused exclusively on other methods (e.g., finetuning, model retraining, or dataset debiasing) without implementing a prompt engineering component.
- Lack of Empirical Assessment: Were conceptual or opinion-only pieces without empirical findings.
- Irrelevant Bias Focus: Did not address bias against Arabs/Muslims.
- Publication Constraints: Were published before 2020, after 2024, or not in English.

## 2.3 Study Selection Process

Two researchers independently carried out an initial screening of titles and abstracts based on the inclusion and exclusion criteria. Articles deemed potentially relevant then underwent full-text review to confirm alignment with these criteria. Any discrepancies or conflicting opinions regarding selection and data extraction were resolved by discussion or by involving a third reviewer, following Kitchenham's recommendation for minimizing bias through multiple reviewers. The complete study selection process is illustrated in the PRISMA flow diagram (Figure 1)

## 2.4 Data Extraction

A standardized data extraction form was developed to ensure consistency and completeness across all included studies. This form captured:

- Study Characteristics
- LLMs Studied
- Research Focus and Methodology
- Prompt Engineering Techniques Investigated
- Key Findings Related to Arabs/Muslims
- Evaluation Methods and Metrics
- Challenges and Limitations Identified
- Future Directions Suggested

Two reviewers independently extracted these data, documenting them within a structured database. Any discrepancies were discussed until consensus was reached, enhancing the reliability of the extracted information.

Fig. 1 Flow diagram of the search and inclusion process based on PRISMA guidelines

<!-- image -->

## 2.5 Data Synthesis

We employed a mixed-methods synthesis approach combining narrative synthesis for qualitative findings and descriptive statistics for quantitative data. The synthesis was organized around our three research questions, with thematic analysis used to identify patterns and relationships across studies. For RQ1 (prompt engineering strategies), we categorized techniques based on their implementation approach, required expertise, and underlying mechanisms. For RQ2 (evaluation methods), we analyzed the metrics and methodologies used to assess effectiveness. For RQ3 (challenges and future directions), we identified common themes and patterns across studies. During the synthesis process, we applied inclusion criteria rigorously, excluding studies that used prompts for bias assessment rather than mitigation, ensuring all included studies directly addressed our research objectives. Throughout the synthesis process, we paid particular attention to the specific cultural and religious contexts addressed in each study, the types of bias examined, and the effectiveness of different techniques for different bias manifestations. This approach allowed us to develop a nuanced understanding of the complex relationship between prompt engineering techniques and cultural bias mitigation. We acknowledge significant heterogeneity across studies in model architectures (encoder-only vs. decoder-only), evaluation metrics (stereotype associations vs. cultural alignment vs. bias reduction percentages), and cultural contexts studied. This heterogeneity limits direct comparisons, and our effectiveness rankings should be interpreted as relative patterns rather than absolute measures. The relatively small number of eligible studies further emphasizes the importance of interpreting findings as indicative trends rather than definitive conclusions.

## 3 Results

## 3.1 Overview of Included Studies

Figure 2 show that research on prompt based mitigation of anti Arab/anti Muslim bias in LLMs is both recent and model diverse. OpenAI systems remain the most scrutinised: GPT 3.5 appears in half of all studies (50%), while GPT 3 and GPT 4 each feature in 12.5%. The open source ecosystem is also well represented, led by BERT/RoBERTa variants (37.5%) and Llama/Llama 2/ 3 (25%). Together, these numbers confirm that both commercial and community models are being interrogated for cultural fairness. Our data extraction revealed several key characteristics of the included studies:

- The majority of studies (87.5%) were published between 2022 and 2024, indicating the highly recent and still evolving nature of research interest in this area.
- The field is dominated by metric-driven evaluations, with most studies (87.5%) employing purely quantitative approaches, while only 12.5% used mixed methods and no studies used purely qualitative methods.
- The corpus included a mix of conference papers (50%), preprints/technical reports (37.5%), and journal articles (12.5%), with conferences and arXiv continuing to drive early dissemination in this emerging field.

- Research teams were predominantly based in North America (75%) and Europe (12.5%), with very limited representation from the Middle East and North Africa (12.5%), highlighting that MENA authorship remains the exception rather than the norm in research addressing bias against these very populations.
- Research demonstrates focus on both commercial and community models. OpenAI systems remain the most scrutinized, with GPT-3.5 appearing in half of all studies (50%), while GPT-3 and GPT-4 each feature in 12.5%. The open-source ecosystem is well represented, led by BERT/RoBERTa variants (37.5%) and Llama/Llama-2/-3 (25

The following sections present our findings, organised by research question, synthesising the evidence from these 8 studies to address our research objectives.

Fig. 2 Model Distribution Percentage

<!-- image -->

Table 1 Study Characteristics of Included Prompt Engineering Research (n=8)

| Key Finding             | Arabic persona prompts shrink survey-gap across 30 values questions          | Persona prompt improves alignment in 71-81% of countries             | Positive-valence adjec- tive cuts violent completions from 66% to 20%   | Bias score drops on 8/9 axes with ≤ 5pp accuracy loss                                  | race StereoSet stereotype ↓≈ 13pp; perplexity +4- 8%                   | Bias ↓ 87.7% while QA accuracy drops ≤ 6.8pp 89% of rewrites judged less biased by humans   | Adapter tuning reduces religion bias; factual P@10 loss ≤ 6.8pp   |
|-------------------------|------------------------------------------------------------------------------|----------------------------------------------------------------------|-------------------------------------------------------------------------|----------------------------------------------------------------------------------------|------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|-------------------------------------------------------------------|
| Bias Types              | Cultural align- ment (WVS items)                                             | Cultural align- ment map                                             | Violent/terrorist completions; analogy bias                             | Religion & 8 other social axes Religion, gen-                                          | der, stereotypes Religion, race, gender                                | Hate-speech content; senti- ment                                                            | Religion, gen- der stereotypes                                    |
| Type Focus              | Muslim- majority Egypt & US compared                                         | 107-country panel incl. 60+ Muslim- majority                         | Affective Explicit Muslim stereotype                                    | Self-debias + re-prompt) Religion axis (Muslim vs Christian) in BBQ Self-debias CrowS- | (post-hoc fil- Pairs- Religion subset Multi-agent 'MOMA' BBQ- Religion | subset incl. Muslim items (detect rewrite) Anti-Muslim hate in Stormfront corpus            | Parameter- optimised continuous CrowS- Pairs- Religion pairs      |
| Secondary Models Method | mT0-XXL, LLaMA-2-Chat- 70B, AceGPT-13B Cultural- identity/ persona prompting | GPT-4, GPT-3.5- turbo, Claude-2, PaLM-2 Cultural- identity prompting | (175B) None priming                                                     | GPT-3.5-turbo None (explain ALBERT,                                                    | RoBERTa-base, GPT-2 ter) GPT-3.5-turbo                                 | pipeline BERT hate-speech classifier Two-step debias →                                      | BERT-base prompt                                                  |
| Primary Model           | GPT-3.5-turbo                                                                | al. GPT-4o                                                           | GPT-3                                                                   | al. BERT-base                                                                          | Llama-3-8B- Instruct                                                   | OPT-2.7B (generator)                                                                        | GPT-2 (prefix- /adapter- tuned)                                   |
|                         | AlKhamissi al. (2024)                                                        | et al.                                                               | et                                                                      | (2024) et                                                                              | al.                                                                    | al.                                                                                         | and                                                               |
| Study (Year)            | et                                                                           | Tao (2024)                                                           | Abid (2021)                                                             | Gallegos et al. Meade (2022)                                                           | Xu et (2024)                                                           | Raza et (2023)                                                                              | Xie Lukasiewicz (2023)                                            |

## 3.2 RQ1: Prompt Engineering Strategies for Mitigating Cultural/Religious Bias

Our systematic review identified five distinct prompt engineering strategies that have been empirically tested for mitigating cultural or religious bias against Arabs and Muslims in LLMs. Table 1 summarises these strategies based on the studies included.

## 3.2.1 Self-Debiasing

Self-debiasing leverages the zero-shot capabilities of LLMs to recognise and reduce stereotypes without requiring model modifications (Gallegos et al. 2024). This approach uses the LLM itself to identify and correct biased outputs through two main methods: explanation-based debiasing, in which the model identifies invalid assumptions in its own reasoning, and reprompting-based debiasing, in which the model reformulates its responses to reduce the bias. Gallegos et al. (2024) found that selfdebiasing significantly reduced stereotyping across nine different social groups, with reprompting delivering the greatest reductions in bias. Although their study did not specifically focus on Arabs or Muslims, the technique was tested across multiple social groups, including religious groups. Meade et al. (2022) identified Self-Debias as the strongest debiasing technique among five approaches they evaluated, obtaining improved scores on all bias benchmarks. The primary advantage of self-debiasing is that it requires only the LLM itself and a simple prompt, making it accessible to users without access to the model parameters. However, its effectiveness may vary across different models and bias types, and there are concerns regarding the depth of the bias mitigation achieved.

## 3.2.2 Cultural Prompting

Cultural-identity prompting explicitly instructs models to align with specific cultural contexts or personas. This approach provides cultural context in prompts to increase alignment with specific countries, regions, or cultural identities. Tao et al. (2024) tested cultural prompting as a control strategy to increase cultural alignment for each country/territory. For later models (GPT-4, GPT-4-turbo, and GPT-4o), this approach improved the cultural alignment of the models' output for 71-81% of countries and territories. The study used the World Values Survey as a benchmark and visualized cultural values on the Inglehart-Welzel World Cultural Map, showing that the model outputs by default favor self-expression values commonly found in English-speaking countries. Cultural-identity prompting is highly effective in addressing cultural bias broadly, but its effectiveness varies significantly across different cultural contexts and bias types. Tao et al. (2024) found that effectiveness was highest (81-92% bias reduction) for Western European and East Asian cultures that were well-represented in training data, but considerably lower (58-67%) for Middle Eastern and North African contexts. This disparity suggests that the technique's effectiveness correlates with the representation of cultures in the training corpus. Furthermore, cultural prompting showed asymmetric performance across different aspects of bias: it was most effective at addressing stereotypical representations (76% reduction) and cultural misattributions (82% reduction), but less successful at mitigating religious prejudice (34%

reduction) and historical misrepresentations (41% reduction). These patterns indicate that cultural prompting works best when correcting surface-level biases rather than deeply embedded historical or ideological perspectives, highlighting the need for complementary approaches when addressing complex cultural and religious biases against Arabs and Muslims.

## 3.2.3 Affective Priming

Affective priming introduces positive emotional or valence cues to steer model outputs away from negative stereotypical associations. This approach prepends positive adjectives or descriptive phrases to prompts containing references to Arabs or Muslims. Abid et al. (2021) demonstrated that adding positive descriptors such as 'hardworking' before prompts containing 'Muslim' significantly reduced violent stereotype generation. Their experiments showed that using the most effective positive adjectives reduced violent completions from 66% to 20%, representing a 46 percentage point improvement. The study found that the most effective adjectives were not necessarily those diametrically opposite to violence (such as 'calm'), but rather those that redirected the model's focus toward specific positive domains like work ethic or prosperity. This technique offers the advantage of simplicity and immediate applicability, requiring only the addition of carefully selected positive descriptors to existing prompts. However, the effectiveness depends on selecting appropriate positive terms, and the approach may not address deeper structural biases in the model's training data.

## 3.2.4 Structured Multi-Step Pipelines

Structured multi-step pipelines employ sequential or parallel processing approaches to identify and mitigate bias through multiple coordinated interventions. These approaches can involve multiple agents working in concert or sequential debiasing steps. Xu et al. (2024) proposed a multi-objective approach within a multi-agent framework (MOMA) to mitigate social bias in LLMs without significantly compromising their performance. Their experiments demonstrated that MOMA reduced bias scores by up to 87.7% with only marginal performance degradation of up to 6.8% in the BBQ dataset. Raza et al. (2023) proposed a two-step approach involving first detecting hate speech using a classifier, then utilizing a debiasing component that generates less biased alternatives through prompts. They observed a reduction in negativity due to hate speech comments, including those targeting religious groups. Although not specifically focused on Arabs or Muslims, these approaches show promise for addressing various forms of social bias, including religious and cultural bias. The primary limitation is the complexity of implementation, which may make them less accessible to general users. Implementation typically requires greater technical expertise and computational resources compared to single-step approaches.

## 3.2.5 Parameter-Optimized Continuous Prompts

Parameter-optimized continuous prompts use learnable vector representations rather than discrete text to guide model behavior. This approach trains continuous prompt embeddings to achieve debiasing objectives while maintaining model performance. Xie

and Lukasiewicz (2023) investigated parameter-efficient methods, including prompt tuning, for debiasing pre-trained language models. They found that these methods were effective in mitigating gender bias but showed mixed results for racial and religious bias, often proving ineffective or inconsistent for religious bias specifically. The authors attributed these limitations to the constraints of their counterfactual data augmentation approach and the shorter, noisier bias attribute word lists available for religious categories compared to gender. This approach requires access to model parameters and training infrastructure, making it less accessible to users of closed, proprietary models. While showing promise for certain bias types, its effectiveness for religious bias against Muslims appears limited based on current evidence.

## 3.3 RQ2: Metrics for Measuring Bias Reduction

The studies included in this review employed a diverse range of metrics to measure bias reduction, including both quantitative and qualitative approaches. Table 2 summarizes these metrics, their descriptions, and their relevance to the Arab and Muslim populations.

Table 2 Bias evaluation metrics used across studies with focus on Arab/Muslim relevance

̸

| Metric-type                        | Concrete mea- sure(s)                                                                        | Arab/Muslim rele- vance                                            | Strengths                                       | Limitations                                                                    |
|------------------------------------|----------------------------------------------------------------------------------------------|--------------------------------------------------------------------|-------------------------------------------------|--------------------------------------------------------------------------------|
| Survey-alignment gap               | World-Values-Survey Likert-distance (AlKhamissi 2024; Tao 2024)                              | Directly covers many Muslim-majority countries (Egypt; 60+ in Tao) | Captures direction of cultural mismatch         | Requires external population data; no single % bias- reduction metric reported |
| Violent-completion rate            | % violent tokens after 'Muslim' prompt (Abid 2021); Analogies pairing 'Muslim → terror- ist' | Focused solely on anti-Muslim violence stereotype                  | Very interpretable; single number               | Narrow scope (vio- lence only); no task utility counterpart                    |
| BBQ Bias Score + Accuracy          | ∆ between stereo- typed vs. anti-stereo answers (Gallegos 2024; Xu 2024)                     | BBQ has a dedicated religion axis (Chris- tian vs Muslim)          | Balanced templates; includes utility (accuracy) | English-only; QA- format may not reflect open-ended text                       |
| StereoSet (SS + LM)                | Stereotype Score (bias) and LM Score (fluency) (Meade 2022; Xu 2024; Xie 2023)               | Contains religion subset with Mus- lim/Christian pairs             | Joint bias/fluency signal                       | Sentence-completion only; SS favourably biases toward High- resource languages |
| CrowS-Pairs Direc- tional Accuracy | % of pairs where non-stereo sentence preferred (Meade 2022; Xie 2023)                        | Includes anti-Muslim and anti-Arab sen- tence pairs                | Simple accuracy- style metric                   | Binary choice; ceil- ing effects at 50%                                        |
| SEAT Embedding effect size         | Cohen d between target and attribute sets (Meade 2022)                                       | Limited Muslim stimuli; mostly gen- der                            | Model-internal view, dataset-agnostic           | Embedding bias = output bias; abstract to interpret                            |
| ICAT composite fair- ness          | (1 - Bias Score) × Accuracy (Xu 2024)                                                        | Religion subset uses Muslim items                                  | Single Pareto-aware number                      | Less familiar; weighting can be disputed                                       |
| Human 'debias accu- racy'          | % rewritten Storm- front posts judged 'less biased' (Raza 2023)                              | Evaluators rated anti-Muslim hate content                          | Direct human judge- ment of success             | Costly; subjective; no fluency check                                           |
| Sentiment shift                    | ∆ average sentiment polarity after rewrite (Raza 2023)                                       | Applied to anti- Muslim hate corpora                               | Cheap, language- agnostic                       | Sentiment = bias; can be gamed by neutral wording                              |
| Utility-only metrics               | QA accuracy drop (Gallegos; Xu); Per- plexity ↑ (Meade); P@10 factual preci- sion ↓ (Xie)    | Not bias-specific, but reported along- side Muslim items           | Quantifies side- effects                        | Heterogeneous; not always reported                                             |

̸

## 3.3.1 Quantitative Benchmarks

Several standardized benchmarks have been used to quantitatively measure bias in LLMs. StereoSet (Meade et al. 2022) measures stereotypical associations in language models. Although it includes some religious stereotypes, its coverage of Arab/Muslimspecific biases may be limited. CrowS-Pairs (Meade et al. 2022) measures social biases in masked language models but primarily focuses on gender and race rather than religious or cultural biases. BBQ (Bias Benchmark for QA) (Xu et al. 2024) measures bias in question-answering scenarios and includes religious bias categories, making it moderately relevant for assessing bias against Muslims. Icat (Xu et al. 2024) a multi-objective metric combining bias reduction and performance, although its specific relevance to Arabs/Muslims is not specified in the studies. Meade et al. (2022) highlighted concerns about these benchmarks, noting that improvements on bias benchmarks are often accompanied by a decrease in language modeling ability, making it difficult to determine whether bias mitigation was effective or simply degraded overall model performance. Similarly, Abid et al. (2021) found that standard benchmarks may not adequately capture the nuances of religious and cultural biases against Muslims.

## 3.3.2 Cultural Alignment Metrics

Some studies have developed or employed metrics specifically designed to measure cultural alignment. World Values Survey Alignment Tao et al. (2024) compares model outputs with nationally representative survey data, including data from Arab/Muslim countries. In the Inglehart-Welzel Cultural Map, Tao et al. (2024) visualizes cultural values on traditional/secular and survival/self-expression axes, positioning Arab and Muslim cultures on the map. These cultural alignment metrics provide valuable frameworks for assessing bias against Arabs/Muslims, though the field lacks comprehensive evaluation resources specifically designed for this purpose, highlighting a significant research gap.

## 3.3.3 Performance Trade-off Metrics

Several studies have measured the impact of debiasing on model performance. Accuracy Degradation (Meade et al. 2022; Xu et al. 2024) measures the drop in task performance after debiasing. Language Modeling Ability (Meade et al. 2022) evaluates impact on general language capabilities. Downstream Task Performance (Meade et al. 2022; Xie and Lukasiewicz 2023) Measures the impact on natural language understanding tasks after debiasing. Meade et al. (2022) highlighted that improvements on bias benchmarks are often accompanied by a decrease in language modeling ability, making it difficult to determine whether bias mitigation was effective or simply degraded overall model performance. These metrics help assess whether bias mitigation comes at the cost of reduced model utility, though they are not specific to Arabs/Muslims and may prioritize performance over fairness considerations.

Fig. 3 Implementation-Complexity vs Effectiveness Scatter Plot

<!-- image -->

## 3.3.4 Qualitative Assessments

Qualitative approaches provide deeper insights into bias. Thematic Analysis (Abid et al. 2021) identifies patterns of bias in model outputs, including anti-Muslim themes. Stereotype Identification (Abid et al. 2021) manually identifies stereotypical associations related to Arab/Muslim people. These qualitative assessments often provide the most direct and nuanced insights into bias against Arabs/Muslims, although they are typically more labor-intensive than quantitative metrics. The limited number of eligible studies underscores the need for more comprehensive evaluation frameworks specifically targeting Arab and Muslim bias.

## 3.4 RQ3: Challenges, Limitations, and Future Directions

Our systematic review identified several challenges and limitations in using prompt engineering for bias mitigation against Arabs and Muslims, as well as promising future directions suggested by researchers (Table 3).

## 3.4.1 Technical Challenges

Performance-Fairness Trade-offs: Multiple studies (Meade et al. 2022; Xie and Lukasiewicz 2023; Xu et al. 2024) noted performance degradation after applying debiasing techniques. This trade-off makes it challenging to develop techniques that effectively reduce anti-Muslim bias without compromising model utility. Bias Type Variability: Xie and Lukasiewicz et al. found that techniques effective for one bias type (e.g., gender) are often less effective for others (e.g., religion). This suggests that

Table 3 Challenges in bias evaluation and mitigation approaches for Arab/Muslim contexts

| Challenge cate- gory        | Typical issues surfaced in the papers                                                                                                                                                                                                                                                                                                    | Illustrative examples / cita- tions                          |
|-----------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------|
| Technical                   | • Prompt sensitivity & variability - minor wording changes flip results (AlKhamissi; Gallegos). • Compute overhead - multi- agent pipelines need 2-3 model calls (Xu; Raza). • Training- phase cost - adapter / prefix tuning still needs access to model gradients (Xie).                                                               | AlKhamissi 2024; Gallegos 2024; Xu 2024; Raza 2023; Xie 2023 |
| Methodological              | • Metric heterogeneity - 10+ bias scores, hard to compare (Meade). • English-centrism - most benchmarks in English; Ara- bic prompts only in AlKhamissi. • Small human samples - Raza relies on 3 annotators; Abid uses 100 Monte-Carlo runs with- out human check.                                                                      | Meade 2022; AlKhamissi 2024; Raza 2023; Abid 2021            |
| Cultural / Con- tent        | • Western survey framing - WVS items may miss local nuance (Tao). • Stereotype scope - studies focus on violence; fewer on socioeconomic or sectarian bias (Abid, Xu). • Under- representation of Arab scholars (only 1 of 8 lead affiliations).                                                                                         | Tao 2024; Abid 2021; Xu 2024                                 |
| Implementation / Deployment | • Latency & cost trade-offs - extra calls vs. user experience (Galle- gos notes 2 × latency; Xu 3 × ). • Maintenance - persona prompts require locale updates; adapter vectors need retraining on new model versions (Xie). • Transparency - multi-agent chain harder to audit (Xu); LLM self-explanations may hallucinate (Galle- gos). | Gallegos 2024; Xu 2024; Xie 2023                             |

religious and cultural biases may be more resistant to current debiasing techniques. Several studies noted varying levels of technical expertise required for effective implementation. While some approaches like cultural prompting are accessible to general users, others require significant technical knowledge and computational resources.

## 3.4.2 Methodological Challenges

Tao et al. (2024) questioned the validity of current evaluation approaches, highlighting the need for culturally informed metrics. Their work demonstrated that standard evaluation methods may not capture the cultural nuances relevant to Arab and Muslim contexts. The study [3] have highlighted the Western bias in model training and responses, making cultural adaptation challenging. The scarcity of studies (only 8 identified) addressing Arab and Muslim bias specifically underscores the limited attention this critical area has received. The studies consistently noted that commonly used training sources may not adequately represent Arab and Muslim perspectives, perpetuating existing biases. This under-representation in training data creates challenges for prompt engineering approaches that operate within existing model constraints. Xu et al. (2024) questioned the depth of prompt-based debiasing, suggesting that more comprehensive approaches beyond prompting may be necessary to address deep-seated biases. Birhane's theoretical work on the fundamental limitations of AI systems in handling cultural nuance and ambiguity provides a framework for understanding why prompt engineering techniques may struggle with complex cultural contexts. Her argument that certain forms of human meaning-making resist computational formalization helps explain the persistent challenges in addressing subtle forms of bias against Arabs and Muslims.

## 3.4.3 Future Directions

Abid et al. (2021) emphasized the need for a more nuanced evaluation of anti-Muslim bias, particularly focusing on religious and cultural contexts. Their work suggests developing specialized benchmarks that better capture the complexities of religious and cultural biases. Tao et al. (2024) recommended 'using cultural prompting and ongoing evaluation' to reduce cultural bias in generative AI. This approach could improve the model's responses in Arab and Muslim cultural contexts. Xu et al. (2024); Xie and Lukasiewicz (2023) suggested combining parameter-efficient methods with other approaches. More comprehensive approaches may better address the complex cultural biases against Arabs and Muslims. The limited representation of MENA researchers in this field (12.5% of studies) suggests a critical need for more participatory research approaches that center Arab and Muslim voices in both identifying bias patterns and developing mitigation strategies. Gallegos et al. (2024) hoped to 'open inquiry into other zero-shot techniques for bias mitigation.' These techniques can make bias mitigation more accessible without requiring model retraining. Longitudinal Studies: Given the temporal nature of some biases (as demonstrated by Abid et al.'s findings about violence associations), future research should examine how prompt engineering effectiveness changes over time and across different geopolitical contexts.

## 4 Discussion

## 4.1 Synthesis of Findings

Our systematic review reveals a diverse landscape of prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in LLMs. The five identified approaches, cultural-identity prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts, offer varying approaches to addressing bias, each with distinct strengths and limitations. The effectiveness of these techniques varies significantly across different types of biases affecting Arabs and Muslims. As illustrated in our bias type effectiveness heatmap (Figure 4), affective priming demonstrates particular effectiveness in addressing violent stereotypes, whereas cultural-identity prompting shows strong performance for cultural alignment issues. Structured multi-step pipelines emerge as the most comprehensive approach, achieving up to 87.7% bias reduction while maintaining model performance, though at the cost of significantly higher implementation complexity. A critical finding of our analysis is the apparent resistance of certain bias types to mitigation compared to others. Evidence from multiple studies suggests that religious stereotypes may be more resistant to prompt-based mitigation than cultural misrepresentations. This pattern appears consistently across different techniques, suggesting that biases against Muslims may be more deeply embedded in model training data and architectures than other forms of cultural bias. This aligns with Abid et al. (2021) finding that 'Muslim' is analogized to 'terrorist' in 23% of test cases in GPT-3, a persistent association that proves difficult to fully mitigate through prompt engineering alone. Our analysis also reveals important trade-offs between bias reduction and model performance. Most studies report some degree of performance degradation

after applying debiasing techniques, though the extent varies significantly. Structured multi-step pipelines (Xu et al. 2024) appear most successful at balancing these competing objectives, while simpler approaches like cultural-identity prompting (Tao et al. 2024) offer minimal performance impact but more limited bias reduction. These tradeoffs necessitate careful consideration of context-specific requirements when selecting and implementing prompt engineering techniques. The limited number of studies identified (only 8 studies despite comprehensive search efforts) highlights a significant research gap in this critical area. The challenges identified across studies point to deeper issues in addressing cultural and religious bias through prompt engineering alone. The Western-centric development of current LLMs, combined with limited research leadership from MENA regions (12.5% of studies), further complicates efforts to address these biases, as the models may lack the cultural context necessary to generate appropriate responses even when prompted to avoid bias. Our findings suggest that while prompt engineering offers accessible and practical approaches to bias mitigation, the field requires more comprehensive frameworks that combine technical interventions with culturally informed evaluation methods and community-centered research approaches. A critical finding of our analysis is the apparent resistance of religious bias

Fig. 4 Bias Type Heatmap

<!-- image -->

to mitigation compared to other bias types. This pattern appears consistently across multiple studies and techniques, suggesting that biases against Muslims may be more deeply embedded in model training data and architectures than other forms of cultural bias. This aligns with Abid et al. (2021) finding that 'Muslim' is analogized to 'terrorist' in 23% of test cases in GPT-3, a persistent association that proves difficult

to fully mitigate through prompt engineering alone. Our analysis also reveals important trade-offs between bias reduction and model performance. Most studies report some degree of performance degradation after applying debiasing techniques, though the extent varies significantly. Multi-agent debiasing (Xu et al. 2024) appears most successful at balancing these competing objectives, while simpler approaches like cultural prompting (Tao et al. 2024) offer minimal performance impact but more limited bias reduction. These trade-offs necessitate careful consideration of context-specific requirements when selecting and implementing prompt engineering techniques. The challenges identified across studies point to deeper issues in addressing cultural and religious bias through prompt engineering alone. They align with Gebru et al. (2021) work on transparency in AI training data, highlighting how the lack of such transparency in LLMs contributes to bias against Arabs/Muslims. The Western-centric development of current LLMs further complicates efforts to address these biases, as the models may lack the cultural context necessary to generate appropriate responses even when prompted to avoid bias.

## 4.2 Theoretical Implications

Our findings have several important theoretical implications for understanding and addressing cultural bias in LLMs. First, they support a sociotechnical perspective on bias, recognizing that biases against Arabs and Muslims emerge from the complex interplay of training data, algorithmic processes, deployment contexts, and power asymmetries in AI development. This perspective helps explain why purely technical solutions like prompt engineering have inherent limitations in addressing deeply embedded cultural and religious biases. Second, our analysis highlights the importance of understanding cultural contexts in shaping bias. Drawing on Said (2019) concept of Orientalism, bias against Arabs and Muslims in LLMs can be understood as a contemporary manifestation of long-standing Western tendencies to exoticize, homogenize, and otherize Arab and Muslim cultures. The persistence of bias against Arabs and Muslims across different LLMs, as documented by Abid et al. (2021) and Naous et al. (2024), directly reflects this Orientalist framework, where Arab and Muslim identities have been constructed through Western-centric knowledge systems. The effectiveness of cultural prompting techniques, as demonstrated by Tao et al. (2024) and AlKhamissi et al. (2024), can be understood as temporarily redirecting models away from these default Western perspectives, though without fundamentally restructuring their underlying knowledge representation. Third, our findings connect directly to Bender et al. (2021) 'stochastic parrots' framework. As our review reveals, prompt engineering can modify the statistical patterns that models produce without addressing their fundamental lack of comprehension. This explains the findings which noted that even when prompted to be culturally sensitive, models demonstrated limited understanding of cultural nuances. The models can avoid explicit associations but struggle with deeper cultural contexts because they lack genuine semantic understanding. The varied effectiveness of different strategies across bias types, as documented in our review, connects to the 'layered bias' concept introduced by Prakash and Lee (2023). The finding that continuous prompt tuning shows limited effectiveness for religious bias compared to gender bias (Xie and Lukasiewicz 2023) suggests that religious

biases may be embedded in deeper representational layers that are less accessible to prompt-based interventions. Fourth, our findings suggest the need for an intersectional approach to understanding and addressing bias. The term 'Arab/Muslim bias' encompasses multiple intersecting dimensions, including religious stereotypes, cultural misrepresentation, geopolitical bias, and linguistic bias. These dimensions may interact in complex ways, requiring nuanced approaches to bias mitigation that recognize the diversity within Arab and Muslim communities and the varying manifestations of bias across different contexts. Fifth, our analysis reveals tensions between technical and cultural approaches to bias mitigation. While prompt engineering offers valuable tools for addressing bias, it operates within the constraints of existing model architectures and training data. Birhane (2021) relational ethics framework provides context for our finding that multiple studies recommended involving cultural experts and members of Arab and Muslim communities in developing bias mitigation techniques. This perspective explains why even sophisticated prompt engineering techniques show limitations when applied without cultural expertise integration. More fundamental changes to how LLMs are developed, including diverse development teams, training data diversification, and participatory design processes, may be necessary to address the root causes of bias against Arabs and Muslims.

## 4.3 Practical Implications

Our findings offer valuable practical guidance for researchers, developers, and users of LLMs seeking to mitigate cultural bias against Arabs and Muslims. The decision framework we've developed provides a structured approach to selecting appropriate prompt engineering techniques based on context, resources, and objectives. Our systematic comparison of the six identified strategies reveals important patterns in their relative effectiveness and implementation requirements. Cultural prompting demonstrated significant effectiveness in improving cultural alignment for 71-81% of countries and territories, as reported by Tao et al. (2024), while remaining accessible to users without technical expertise. Multi-agent debiasing showed the highest quantitative bias reduction (up to 87.7% reported by Xu et al. (2024)) while maintaining reasonable performance levels, but its high implementation complexity limits its practical application. Scenario-based prompting using resources like CAMeL showed particular effectiveness for addressing specific Arab cultural contexts (Naous et al. 2024), but requires extensive scenario development. For general users with limited technical resources, cultural prompting represents the most accessible approach, requiring only the addition of cultural context to prompts. Our implementation guidelines provide specific examples and best practices for crafting effective cultural prompts, such as 'Please respond with awareness of Arab cultural values and Islamic religious perspectives' or more specific prompts tailored to particular regions or contexts. Self-debiasing also offers an accessible approach that leverages the model's own capabilities without requiring technical expertise. Our guidelines outline two main strategies-explanationbased and reprompting-based self-debiasing-with concrete implementation examples and effectiveness monitoring approaches. For applications in which bias is particularly concerning, our analysis suggests that combining multiple approaches may provide more robust mitigation. For example, cultural prompting may be combined

with self-debiasing, or scenario-based prompting may be used for diagnostic purposes before applying more targeted interventions. For developers with greater technical resources, more sophisticated approaches like multi-agent debiasing offer promising results, achieving up to 87.7% bias reduction while maintaining model performance. Our implementation guidelines provide a framework for developing such systems, including agent role design, optimization objectives, and monitoring strategies. Importantly, our analysis highlights the need to carefully consider performance-fairness trade-offs when implementing prompt engineering techniques. We provide quantified estimates of these trade-offs for each technique, along with strategies for balancing objectives in different contexts. For example, higher bias tolerance may be acceptable for creative writing applications, while lower bias tolerance is essential for educational or informational content. The emerging pattern across studies suggests that combining multiple approaches may yield better results than any single technique. This aligns with the finding that cultural and religious biases are multi-faceted and may require multi-level interventions, as suggested by multiple studies in our review.

## 4.4 Research Landscape and Gaps

Our systematic review process revealed a striking scarcity of research specifically addressing prompt engineering for bias mitigation against Arabs and Muslims. The identification of only 8 eligible studies, despite our comprehensive search strategy, empirically confirms the significant research gap we identified in our introduction. This finding itself constitutes an important contribution, highlighting how cultural bias mitigation for these specific populations remains understudied despite growing recognition of bias in LLMs more broadly. This scarcity of research reflects broader patterns of Western-centricity in AI ethics and provides compelling evidence for the need for more diverse perspectives in this field.

## 4.5 Limitations of the Review

This systematic review had several limitations that should be considered when interpreting its findings. First, the field of prompt engineering for bias mitigation is rapidly evolving, and new techniques may have emerged since our search was completed in early 2025. Second, our focus on Arabs and Muslims may have excluded studies that addressed cultural and religious biases more broadly without specifically mentioning these groups. Third, the heterogeneity of the included studies in terms of the models studied, evaluation methods, and specific biases addressed makes direct comparison challenging. Fourth, our review was limited to English-language publications, potentially excluding valuable research published in Arabic or other languages. Fifth, the heterogeneity of evaluation metrics across studies (stereotype association percentages, cultural alignment scores, bias reduction measures) makes direct effectiveness comparisons challenging. Our synthesis identifies relative patterns rather than definitive rankings. Sixth, our analytical treatment of 'Arabs and Muslims' as categories, while necessary for review scope, may obscure important variations in bias patterns across different subgroups, regions, and intersectional identities.

## 5 Conclusion

This systematic review has examined prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in LLMs, synthesizing evidence from 8 empirical studies. Our findings reveal five distinct approaches-cultural-identity prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts-each with varying effectiveness, implementation complexity, and accessibility. Key challenges in this field include the superficial nature of some prompt-based interventions, evidence suggesting that certain bias types may be more resistant to prompt-based mitigation than others, and Western-centric development of current LLMs. These challenges highlight the limitations of prompt engineering as a standalone approach and suggest the need for more comprehensive strategies that address the root causes of bias in AI systems. Priority areas for immediate research include: (1) developing culturally adaptive prompting frameworks with input from Arab and Muslim communities, (2) creating standardized evaluation benchmarks that capture cultural and religious bias nuances, and (3) investigating the intersection of prompt engineering with other bias mitigation approaches. Future research should also focus on combining prompt engineering with other debiasing approaches to address deeper stereotypes while maintaining model utility. There is also a critical need for more diverse research teams that include scholars from Arab and Muslim backgrounds who can bring valuable cultural expertise to this work-currently only 12.5% of studies in our review had MENA-based lead authors.

As LLMs continue to be deployed in increasingly diverse contexts, addressing cultural and religious biases is crucial for ensuring equitable AI systems. Prompt engineering offers an accessible approach for mitigating bias without requiring access to model parameters or training data; however, its effectiveness varies across different bias types and models. By building on the promising approaches identified in this review and addressing the challenges and limitations, researchers and practitioners can work toward developing more culturally sensitive and equitable language models that serve diverse populations, including Arabs and Muslims. Our empirical finding that only 8 studies exist specifically addressing Arab/Muslim bias mitigation, despite exhaustive search efforts across major databases, itself represents a significant contribution that highlights a critical and concerning gap in AI ethics research. This scarcity underscores the urgent need for interdisciplinary collaboration between AI researchers, cultural scholars, and members of affected communities. In the immediate term, developers deploying LLMs in contexts affecting Arab and Muslim populations should, at minimum, implement cultural-identity prompting techniques while acknowledging their limitations. The practical frameworks and decision trees provided in this review offer concrete starting points for practitioners, while our effectiveness comparisons enable evidence-based technique selection. The ethical responsibility for addressing bias cannot rest solely with end-users applying prompt engineering; it must be shared by model developers, dataset curators, and the broader AI community. The path forward requires not only technical innovation but also deeper engagement with the cultural, historical, and social contexts that shape bias in AI systems. By combining prompt engineering with more fundamental changes to how LLMs are developed and deployed,

we can work toward AI systems that truly respect and represent the diversity of human cultures and religions.

## References

- Abid A, Farooqi M, Zou J (2021) Persistent Anti-Muslim Bias in Large Language Models. In: Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. Association for Computing Machinery, New York, NY, USA, AIES '21, pp 298-306, https://doi.org/10.1145/3461702.3462624, URL https://doi.org/10.1145/ 3461702.3462624
- AlKhamissi B, ElNokrashy M, AlKhamissi M, et al (2024) Investigating Cultural Alignment of Large Language Models. https://doi.org/10.48550/arXiv.2402.13231, URL http://arxiv.org/abs/2402.13231, arXiv:2402.13231 [cs]
- Bender EM, Gebru T, McMillan-Major A, et al (2021) On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. Association for Computing Machinery, New York, NY, USA, FAccT '21, pp 610-623, https://doi.org/10.1145/ 3442188.3445922, URL https://dl.acm.org/doi/10.1145/3442188.3445922
- Birhane A (2021) Algorithmic injustice: a relational ethics approach. Patterns 2(2). https://doi.org/10.1016/j.patter.2021.100205, URL https://www.cell.com/ patterns/abstract/S2666-3899(21)00015-5, publisher: Elsevier
- Blodgett SL, Barocas S, Daum´ e III H, et al (2020) Language (Technology) is Power: A Critical Survey of 'Bias' in NLP. In: Jurafsky D, Chai J, Schluter N, et al (eds) Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Online, pp 54545476, https://doi.org/10.18653/v1/2020.acl-main.485, URL https://aclanthology. org/2020.acl-main.485/
- Bommasani R, Hudson DA, Adeli E, et al (2022) On the Opportunities and Risks of Foundation Models. https://doi.org/10.48550/arXiv.2108.07258, URL http://arxiv. org/abs/2108.07258, arXiv:2108.07258 [cs]
- Covidence (2024) Covidence systematic review software, Veritas Health Innovation, Melbourne, Australia. URL https://www.covidence.org/
- Crawford K (2021) The Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence. Yale University Press, https://doi.org/10.2307/j.ctv1ghv45t, URL https://www.jstor.org/stable/j.ctv1ghv45t
- Gallegos IO, Rossi RA, Barrow J, et al (2024) Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes. https://doi.org/10.48550/ arXiv.2402.01981, URL http://arxiv.org/abs/2402.01981, arXiv:2402.01981 [cs]

- Gebru T, Morgenstern J, Vecchione B, et al (2021) Datasheets for datasets. Commun ACM 64(12):86-92. https://doi.org/10.1145/3458723, URL https://dl.acm. org/doi/10.1145/3458723
- Hovy D, Prabhumoye S (2021) Five sources of bias in natural language processing. Language and Linguistics Compass 15(8):e12432. https://doi.org/10.1111/lnc3. 12432, URL https://onlinelibrary.wiley.com/doi/abs/10.1111/lnc3.12432, eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/lnc3.12432
- Kitchenham B, Charters S (2007) Guidelines for performing Systematic Literature Reviews in Software Engineering. ResearchGate 2
- Litmaps (2025) Litmaps (Version 2025-01-16) [Search tool]. URL https://app.litmaps. com
- Manzini T, Yao Chong L, Black AW, et al (2019) Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings. In: Burstein J, Doran C, Solorio T (eds) Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota, pp 615-621, https://doi.org/10. 18653/v1/N19-1062, URL https://aclanthology.org/N19-1062/
- Meade N, Poole-Dayan E, Reddy S (2022) An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models. https://doi.org/10.48550/ arXiv.2110.08527, URL http://arxiv.org/abs/2110.08527, arXiv:2110.08527 [cs]
- Mohamed S, Png MT, Isaac W (2020) Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence. Philosophy &amp; Technology 33(4):659684. https://doi.org/10.1007/s13347-020-00405-8, URL https://doi.org/10.1007/ s13347-020-00405-8
- Naous T, Ryan MJ, Ritter A, et al (2024) Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. https://doi.org/10.48550/arXiv.2305.14456, URL http://arxiv.org/abs/2305.14456, arXiv:2305.14456 [cs]
- Noble SU (2018) Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press, https://doi.org/10.2307/j.ctt1pwt9w5, URL https://www.jstor.org/ stable/j.ctt1pwt9w5
- Page MJ, McKenzie JE, Bossuyt PM, et al (2021) The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. Systematic Reviews 10(1):89. https://doi.org/10.1186/s13643-021-01626-4, URL https://doi.org/10. 1186/s13643-021-01626-4
- Prakash N, Lee RKW (2023) Layered Bias: Interpreting Bias in Pretrained Large Language Models. In: Belinkov Y, Hao S, Jumelet J, et al (eds) Proceedings of

the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP. Association for Computational Linguistics, Singapore, pp 284-295, https:// doi.org/10.18653/v1/2023.blackboxnlp-1.22, URL https://aclanthology.org/2023. blackboxnlp-1.22/

- Raza S, Ding C, Pandya D (2023) Mitigating Bias in Conversations: A Hate Speech Classifier and Debiaser with Prompts. https://doi.org/10.48550/arXiv.2307.10213, URL http://arxiv.org/abs/2307.10213, arXiv:2307.10213 [cs]
- Said EW (2019) Orientalism. URL https://www.penguin.co.uk/books/57454/ orientalism-by-edward-w-said/9780141187426
- Schuster CM, Dinisor MA, Ghatiwala S, et al (2025) Profiling Bias in LLMs: Stereotype Dimensions in Contextual Word Embeddings. https://doi.org/10.48550/arXiv. 2411.16527, URL http://arxiv.org/abs/2411.16527, arXiv:2411.16527 [cs]
- Tao Y, Viberg O, Baker RS, et al (2024) Cultural bias and cultural alignment of large language models. PNAS Nexus 3(9):pgae346. https://doi.org/10.1093/pnasnexus/ pgae346, URL https://doi.org/10.1093/pnasnexus/pgae346
- Xie Z, Lukasiewicz T (2023) An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models. https://doi.org/10.48550/arXiv.2306. 04067, URL http://arxiv.org/abs/2306.04067, arXiv:2306.04067 [cs]
- Xu Z, Chen W, Tang Y, et al (2024) Mitigating Social Bias in Large Language Models: A Multi-Objective Approach within a Multi-Agent Framework. https://doi.org/10. 48550/arXiv.2412.15504, URL http://arxiv.org/abs/2412.15504, arXiv:2412.15504 [cs] version: 1