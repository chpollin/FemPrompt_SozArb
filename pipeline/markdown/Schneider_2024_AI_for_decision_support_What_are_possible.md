---
source_file: Schneider_2024_AI_for_decision_support_What_are_possible.pdf
conversion_date: 2026-02-03T09:20:40.624002
converter: docling
quality_score: 95
---

www.tatup.de

<!-- image -->

## ZEITSCHRIFT FÜR TECHNIKFOLGENABSCHÄTZUNG IN THEORIE UND PRAXIS

J O U R N A L   F O R   T E C H N O L O G Y   A S S E S S M E N T   I N   T H E O RY   A N D   P R AC T I C E

peer reviewed &amp; open access

33/1 (2024)

<!-- image -->

Special topic AI for decision support: What are possible futures, social impacts,   regulatory options, ethical conundrums and agency constellations?

Research Dekarbonisierung des Verkehrssektors in Berlin Interview mit Karl von Wendt: Künstliche Intelligenz außer Kontrolle?

- Zeitschrift für Technikfolgenabschätzung in Theorie und Praxis ist die begutachtete Open-Access-Zeitschrift für das inter  disziplinäre Feld der Technikfolgenabschätzung und angrenzende Forschungsgebiete. Die Zeitschrift richtet sich an eine inter- und transdisziplinäre Leserschaft.

TATuP - Journal for Technology Assessment in Theory and Practice is the peer reviewed open access journal for the interdisciplinary field of technology assessment and neighboring fields of research. It addresses an inter- and transdisciplinary readership.

## IMPRESSUM  /  LEGAL NOTICE

## HERAUSGEBER/EDITOR

<!-- image -->

Karlsruher Institut für Technologie (KIT) Institut für Technikfolgenabschätzung und Systemanalyse (ITAS) Karlstraße 11 76133 Karlsruhe, Germany

## VERLAG /  PUBLISHING HOUSE

<!-- image -->

oekom - Gesellschaft für ökologische Kommunikation mit beschränkter Haftung Goethestraße 28 80336 München, Germany

## HERAUSGEBERSCHAFT / EDITORIAL BOARD

Prof. Dr. Armin Grunwald, KIT-ITAS, Karlsruhe, DE (Editor in Chief / Main Handling Editor 1) Prof. Mahmud Farooque, ASU, Phoenix, US Prof. Sven Ove Hansson, KTH, Stockholm, SE Dr. Karen Kastenhofer, ITA, Wien, AT Dr. Linda Nierling, KIT-ITAS, Karlsruhe, DE Dr. Pauline Riousset, KIT-ITAS/TAB, Berlin, DE

## WISSENSCHAFTLICHER BEIRAT / SCIENTIFIC ADVISORY BOARD

Prof. Peta Ashworth, University of Queensland, AU Prof. Dr. Daniel Barben, Universität Klagenfurt, AT Prof. Dr. Birgit Blättel-Mink, Universität Frankfurt a. M., DE PD Dr. Alexander Bogner, ITA, Wien, AT Dr. Pierre Delvenne, Universität von Lüttich, BE Prof. Dr. Hans-Liudger Dienel, nexus Institut, Berlin, DE Prof. Dr. Ulrich Dolata, Universität Stuttgart, DE Dr. Elisabeth Ehrensperger, TA-Swiss, CH PD Dr. Jessica Heesen, Universität Tübingen, DE Prof. Dr. em. Matthias Kaiser, University of Bergen, NO Prof. Dr. Cordula Kropp, Universität Stuttgart, DE Dr. Justine Lacey, CSIRO, Canberra, AU Dr. Ralf Lindner, Fraunhofer ISI, Karlsruhe, DE Prof. Dr. Krzysztof Michalski, TU Rzeszów, PL PD Dr. Michael Nentwich, ITA, Wien, AT Dr. Poonam Pandey, DST-Center for Policy Research, IISC, Bangalore, IN

Prof. Dr. Sebastian Pfotenhauer, TU München, DE Dr. Witold-Roger Poganietz, KIT-ITAS, Karlsruhe, DE Dr. Bernard Reber, CEVIPOF, Paris, FR Dr. Martin Sand, TU Delft, NL Prof. Dr. Thomas Saretzki, Universität Lüneburg, DE Prof. Dr. Petra Schaper-Rinkel, Universität Graz, AT Prof. Dr. Jan C. Schmidt, Hochschule Darmstadt, DE Prof. Dr. Miranda Schreurs, HfP an der TU München, DE Dr. Elena Seredkina, Universität Perm, RU PD Dr. Mahshid Sotoudeh, ITA, Wien, AT Prof. Dr. Karsten Weber, OTH Regensburg, DE Dr. Marcel Weil, KIT-ITAS, Karlsruhe, DE Prof. Dr. Johannes Weyer, TU Dortmund, DE

## REDAKTION / EDITORIAL TEAM

Dr. Ulrich Ufer (Managing Editor/Main Handling Editor 2) Reinhard Heil (wiss. Redaktion/academic editor) Jonas Moosmüller (TA-Fokus-Redaktion/TA Focus editor) Dr. Ralf Schneider (wiss. Redaktion/academic editor) Dr. Leonie Seng (wiss. Redaktion/academic editor) Ketevan Jeladze (Redaktionsassistenz/editorial assistant) Sascha Weis (Redaktionsassistenz/editorial assistant)

Institut für Technikfolgenabschätzung und Systemanalyse (ITAS) Karlstraße 11 76133 Karlsruhe, Germany +49 (0)721 608 26014 redaktion@tatup.de

<!-- image -->

## HINWEISE FÜR AUTOR*INNEN/ INFORMATION FOR AUTHORS

www.tatup.de/index.php/tatup/Submit

## ERSCHEINUNGSWEISE / PUBLICATION SCHEDULE

3× jährlich  /  3× per year

## BESTELLUNG / SUBSCRIPTION

www.tatup.de/index.php/tatup/subscriptionStatic

<!-- image -->

<!-- image -->

www.oekom.de/tatup/bezug Verlegerdienst München GmbH Aboservice oekom verlag Gutenbergstraße 1 82205 Gilching, Germany +49 (0)8105 388 563 +49 (0)8105 388 333

<!-- image -->

oekom-abo@verlegerdienst.de

## TATuP ONLINE

www.tatup.de

Newsletter: www.oekom.de/newsletter

## PAPIER  /  PAPER

Innenteil gedruckt auf Steinbeis select , Umschlag auf Vivus 100 . Beide Papiere sind zu 100 % aus Recyclingpapier und   zertifiziert mit dem Blauen Engel. /   Printed on Steinbeis select ,   cover on Vivus 100 . Both are 100 % recycled papers and   certified with the Blue Angel.

## DRUCK /  PRINTING

Friedrich Pustet GmbH &amp; Co. KG 93051 Regensburg, Germany www.pustet-druck.de

## ANZEIGEN /  ADVERTISEMENTS

<!-- image -->

<!-- image -->

Karline Folkendt oekom - Gesellschaft für ökologische Kommunikation mit beschränkter Haftung +49 (0)89 544184-217 anzeigen@oekom.de

## VISUELLE KONZEPTION &amp; GESTALTUNG / VISUAL CONCEPT &amp; DESIGN

Kornelia Rumberg, www.rumbergdesign.de

## GRAFIK &amp; SATZ  /  GRAPHIC DESIGN &amp; TYPESET

Tobias Wantzen, www.wantzen.com Cover art based on ©  Freepik

## ISSN

2568-020X (Print), 2567-8833 (Online)

## COPYRIGHT &amp; LIZENZ  /  COPYRIGHT &amp; LICENCE

Creative Commons Licence CC BY 4.0 www.creativecommons.org/licenses/by/4.0/

Erfüllungsort  /  Gerichtsstand: München, Deutschland Place of fulfillment  /  Place of jurisdiction: Munich, Germany

<!-- image -->

## oekom, naturally - We publish sustainably!

oekom kompensiert bereits seit 2008 seine unvermeidlichen CO₂-Emissionen. / Since 2008 oekom offsets its   unavoidable CO₂ emissions. At oekom verlag, ecology and sustainability are not just words on paper; they form the foundation of our   corporate philosophy. Cooperation instead of egoism,   environmentally friendly production instead of maximum sales figures, diversity instead of monotony. We want to highlight alternatives with our publications and be an alternative ourselves. Find more information on oekom, naturally www.natürlich-oekom.de and #natürlichoekom

<!-- image -->

## Editorial

<!-- image -->

ARMIN GRUNWALD Institute for Technology Assessment and Systems Analysis, Karlsruhe Institute of Technology, Karlsruhe, DE (armin.grunwald@kit.edu)

<!-- image -->

© 2024 by the authors; licensee oekom. This Open Access article is licensed under a Creative Commons Attribution 4.0 International License (CC BY). https://doi.org/10.14512/tatup.33.1.03 published online: 15. 03. 2024

<!-- image -->

AI (artificial intelligence) systems steer cars, make medical diagnoses, support legal proceedings, and write texts with astonishing speech quality. To do this, AI systems must recognize their environment, distinguish between relevant and less relevant elements, and make diagnoses on the basis of which subsequent actions are initiated.

When humans go through these steps, we speak of decisions. The ability to simulate such decisions and arrive at proper results, e.g., in car traffic, is probably the most striking technical innovation in the field of AI. Depending on the perspective, it is fascinating or scary, or both, how fast progress is being made and that decisions are increasingly delegated to AI systems.

Impact analyses and reflections in this field are challenging. Technology assessment (TA) not only has to analyze market potentials in different areas of application, develop scenarios of market penetration and possible consequences, and survey the perceptions of different groups and stakeholders, as is often the case, but is also faced with ethical problems. These begin with the attribution and distribution of responsibility as soon as decisions are delegated to AI systems. However, they also include possible discrimination by algorithms or the question of which decisions should not be delegated to technical systems, e.g., in the case of military drones.

In 2023, the German Ethics Council took a stand and stated as a criterion for a positive assessment of AI that AI systems should extend human autonomy and freedom and not limit or even replace them. What this means in concrete terms must, of course, be spelled out individually for each area of application. Ethics is not enough for this; the respective empirical context must be adequately considered. This is the inter- and transdisciplinary strength of TA, which understands the consequences of technology in general and in the field of AI in particular not simply as the consequences of technology, but as the interplay of technical features and human behavior. As the Special topic of this TATuP issue shows, TA must therefore not narrow its focus to AI as a technology, but must take into account the interactions with human behavior.

Armin Grunwald

<!-- image -->

## Inhalt / Content 33/1 (2024)

|    | EDITORIAL                                                                                                                                             |
|----|-------------------------------------------------------------------------------------------------------------------------------------------------------|
| 3  | A.GRUNWALD                                                                                                                                            |
|    | TA FOCUS                                                                                                                                              |
| 6  | Personalia • Meldungen • Aus dem openTA-Kalender • 5 Fragen an K.Stelzl                                                                               |
|    | SPECIAL TOPIC AI FOR DECISION SUPPORT: WHAT ARE POSSIBLE FUTURES, SOCIAL IMPACTS, REGULATORY OPTIONS, ETHICAL CONUN- DRUMS AND AGENCY CONSTELLATIONS? |
| 9  | D. SCHNEIDER, K.WEBER AI-based decision support systems and society: An opening statement                                                             |
| 14 | A.-K.DHUNGEL, M. HEINE Cui bono? Judicial decision-making in the era of AI: A qualitative study on the expectations of judges in Germany              |

21

28

34

41

SPECIAL TOPIC

## AI for decision support

Artificial intelligence to support   decision-making is affecting ever larger parts of society; human decisions can now be made by machines. This Special topic brings together contributions that   present societal challenges, ethical issues, stakeholders, and possible futures of AI use for decision support.

8

B. LONG, A. PALMER

AI and access to justice: How AI legal advisors can reduce economic and shame-based barriers to justice

G. LOPES

Artificial intelligence and judicial decision-making:

Evaluating the role of AI in debiasing

D. MINKIN, L. T. BRANDNER

Borderline decisions?: Lack of justification for automatic deception detection at EU borders

M. MARQUARDT  , P. GRAF, E. JANSEN  , S. HILLMANN  ,

J.-N. VOIGT-ANTONS

Situativität, Funktionalität und Vertrauen:

Ergebnisse einer szenariobasierten Interviewstudie zur Erklärbar  keit von KI in der Medizin

- 48 J.  C.  ZOELLICK, H. DREXLER  , K. DREXLER Artificial intelligence in melanoma diagnosis: Three scenarios, shifts in competencies, need for   regulation, and reconciling dissent between humans and AI

- 55

## RESEARCH

- M. KREUSCHNER, N. BONATZ  , T. SCHLENTHER  , H. MOSTOFI  , H.-L. DIENEL  , K. NAGEL Dekarbonisierung des Verkehrssektors in Berlin: Bürgerinnen- und Bürgergutachten zu wissenschaftlich erstellten Szenarien

## INTERVIEW

- 64 MIT/WITH K. VON WENDT VON/BY R. HEIL Künstliche Intelligenz außer Kontrolle?

## REFLECTIONS

- 68 M. W. SCHMIDT Book review: Coeckelbergh, Mark (2022): The political philosophy of AI
- 70 J.  SON, S. SAHA Book review: Timon, McPhearson; Nadja, Kabisch; Niki, Frantzeskaki (eds.) (2023): Nature-based solutions for cities

RESEARCH

## Dekarbonisierung des Verkehrssektors

Ein Berliner Bürger:innenrat erarbeitete gemeinsam mit der Wissenschaft Einschätzungen bzgl. Maßnahmen zur Dekarbonisierung des Verkehrssektors. Der Forschungsartikel zeigt u. a. Einigungen und Konfliktfelder hinsichtlich möglicher Dekarbonisierungspfade sowie Einstellungen zu Pull- und Push-Maßnahmen.

55

INTERVIEW

## Künstliche Intelligenz außer Kontrolle?

Eine außer Kontrolle geratene künstliche Intelligenz (KI) könnte die Existenz der gesamten Menschheit gefährden. Werden existenzielle KI-Risiken in Deutschland ausreichend ernstgenommen? Wie unterscheiden sich allgemeine von spezialisierten KIs? Wäre ein Moratorium in der KI-Entwicklung sinnvoll und durchführbar?

64

- 72 A. SIEGEMUND, J. BOSCO LOURDUSAMY  , J. FIEDLER  ,
- R. THOMAS Meeting report: 'Religion and technology in an era

of rapid digital and climate change'. Conference, 2023, Chennai, IN (hybrid)

- 74 S. ALBRECHT, R. GRÜNWALD Meeting report: 'Generative artificial   intelligence Opportunities, risks and policy challenges'. EPTA Conference, 2023, Barcelona, ES
- 76 B. BROHMANN, R. RHODIUS  , M. MBAH Meeting report: 'PartWiss23'. Conference, 2023, Chemnitz, DE
- 78 TATuPDates

<!-- image -->

## PUBLICATION

## Policy advice with AI

'[A]  new  generation  of  AI-based  tools could, in the near future, present an opportunity to drastically improve science advice, making it more agile, rigorous, and targeted,' emphasizes Chris Tyler, head of the United Kingdom's Parliamentary Office of Science and Technology (POST). In a commentary article in Nature, he and his co-authors argue that the framework conditions for those involved in scientific policy advice are becoming increasingly dynamic:  Science  advisors  must  wade through a constantly growing flood of information in ever-shorter periods of time. Against this backdrop, AI tools could be particularly helpful in 'synthesizing evidence and drafting briefing papers.' However,  to  ensure  that  such  tools  can  actually  be  used  appropriately,  the  authors urge  scientific  advisors  and  political  institutions to 'create common guidelines and to carefully consider the design and re  sponsible use of this nascent technology.' www.nature.com

## ARBEITSPAPIER

## Typologie der TA

Was  genau  ist  eigentlich  wissenschaftliche Technikfolgenabschätzung? Zu dieser grundlegenden  Frage  hat  Michael  Nent-

1 Institute for Technology Assessment and Systems Analysis (ITAS), Karlsruhe Institute of Technology (KIT), Karlsruhe, DE

## TA Focus 33/1 (2024)

## News for the TA community / Meldungen für die TA-Community

Jonas Moosmüller 1

wich  das  Arbeitspapier  'Wissenschaftliche  Technikfolgenabschätzung.  Begriff, Typologie und Konsequenzen' vorgelegt. Der Leiter des Instituts für Technikfolgen-Abschätzung der Österreichischen Akademie der Wissenschaften unterscheidet  darin  drei  Grundformen:  Politik-orientierte  TA,  Intervenierende  TA und TA-bezogene Forschung. Nentwich untermauert seine Typologie mit empirischen Daten aus der Praxis seines Instituts und des europäischen Netzwerks für parlamentarische  TA  (EPTA).  Mit  der Veröffentlichung  verbindet  der  'langgediente  TA-Praktiker'  Nentwich das Plädoyer für eine systematische Qualitätssicherung von TA und mehr Reflexion darüber, was TA leistet - und leisten sollte. www.oeaw.ac.at/ita

## NACHHALTIGKEIT

## Umweltstaatssekretär will TA für KI-Modelle

Großes  Potenzial  für  den  Klimaschutz sieht Christian Kühn, Parlamentarischer Staatssekretär im Bundesministerium für Umwelt und Verbraucherschutz (BMUV), beim Einsatz von KI: etwa bei der effizienten  Steuerung  zukünftiger  Energieund  Verkehrssysteme  oder  bei  der  Vorhersage von Extremwetterereignissen. Es gebe jedoch keine Garantie, dass die enormen Fortschritte in diesem Bereich nicht 'durch Rebound-Effekte aufgefressen' würden, so Kühn im Januar auf der Konferenz 'KI: Immer größer statt grüner'  der  Heinrich-Böll-Stiftung.  'Wenn wir  wollen,  dass  Künstliche  Intelligenz uns auch einen ökologischen Fortschritt

<!-- image -->

bringt […, dann] brauchen wir eine Technikfolgenabschätzung,  die  Umweltbilanzen erstellt.'  Diese  müssten  den  gesamten  Lebenszyklus  von  der  Produktion und dem Training von KI bis hin zur Anwendung umfassen und bei der Regulierung  von  KI  berücksichtigt  werden,  so Kühn.

calendar.boell.de/de/event/ki-immergroesser-statt-gruener

## CONFERENCE

## Policy advice in times of polycrises and AI

Almost twenty years to the day after its foundation, the Network Technology Assessment (NTA) will host its eleventh conference  from  18  to  20  November  2024. The event in Berlin will focus on the current  challenges  in  scientific  policy  advice in the face of mutually reinforcing 'polycrises' and technological leaps, especially in the field of AI: What scientific advice is needed? How can this be pro-

<!-- image -->

<!-- image -->

Quelle: Öko-Institut e.V.

Quelle: Thomas Lehmann vided in a precise and informed manner in times of extreme urgency? And what organizational  structures  and  networks are needed to develop options for action for  more  resilient,  sustainable  futures? Answers  to  these  questions  will  be  discussed at the conference from a TA perspective,  but  also  from  a  social  and  political  point  of  view.  The  conference  is being organized by the Institute for Technology   Assessment and Systems Analysis (ITAS), one of the initiators of the NTA. The call for ab  stracts has recently gone online. Abstracts can be submitted until 15 June 2024.

www.itas.kit.edu/nta11

## Personalia

<!-- image -->

CHRISTOF TIMPE ist seit Januar  2024  neuer  Sprecher der Geschäftsführung des Öko-Instituts. Der Diplom-Ingenieur arbeitet seit mehr als 30 Jahren an der Einrichtung für transdisziplinäre Nachhaltigkeitsforschung, seit 1996 als Leiter des Institutsbereichs  Energie  und  Klimaschutz  in Freiburg. Bei seinem Amtsantritt sprach sich Timpe  dafür  aus,  gemeinsam  mit  den  von nachhaltigen  Transformationen  betroffenen Personen an positiven Zukunftsbildern zu arbeiten  und  aufgeheizte  Debatten  durch  faktenbasierte Kommunikationsangebote wieder zu versachlichen.

<!-- image -->

WALTER  PEISSL , stellvertretender Leiter des Instituts  für  TechnikfolgenAbschätzung in Wien, wurde in das neu gegründete Artificial Intelligence

Advisory Board der österreichischen Bundesregierung  berufen.  Das  Beratungsgremium wurde im Dezember zusammen mit der ebenfalls neuen Servicestelle vorgestellt, die Transparenz  und  Rechtssicherheit  schaffen und helfen soll,  Chancen  und  Innovationen für Unternehmen nutzbar zu machen. Peissl forscht am ITA seit vielen Jahren zur gesellschaftlichen Wirkung von KI, insbesondere im Bereich von Datenschutz und Privatsphäre.

5 Fragen an Katharina Stelzl

<!-- image -->

Citizen Scientist aus Karlsruhe mit Beteiligungserfahrung an mehreren TA-Projekten

## Warum ist es aus Ihrer Perspektive wichtig, Technikfolgenabschätzung zu betreiben?

Die technische Entwicklung schreitet rasend schnell voran und es ist kaum möglich, die Folgen dieser Entwicklungen vorher  zusagen, zumal neue Entwicklungen ihrerseits   wieder in Wechselwirkung mit bestehenden und kommenden Technologien stehen.   Daher scheint es mir sehr sinnvoll, in diesem Bereich Forschung zu betreiben.

## Welche Forschungsfrage finden Sie besonders spannend?

Ich bin durch ein Projekt zu PV-Balkonmodulen und deren Einfluss auf Nutzerinnen und Nutzer zum ersten Mal mit der Technikfolgenabschätzung in   Berührung gekommen. Das Thema finde ich super spannend, vor allem hat mich fasziniert, welche Rückschlüsse Forschende aus Interviews und Fragebögen ziehen können, die einem selbst so gar nicht bewusst waren.

## Angenommen, Sie wären TA-Wissenschaftlerin: Womit würden Sie sich gerne beschäftigen?

Den Einfluss von Sprachmodellen auf Kinder und Jugendliche finde ich sehr   spannend: Was passiert mit Menschen, wenn sie mit Technologien wie ChatGPT aufwachsen und von klein auf daran gewöhnt sind, diese zu nutzen? Welche Fähigkeiten sollten wir ihnen mitgeben, damit sie kompetent mit diesen Technologien umgehen können und welcher Schulstoff wird durch die neuen Technologien vielleicht obsolet?

## Was können Forschende von Bürgerinnen und Bürgern lernen?

Ich glaube, vor allem Flexibilität und RealityChecks. Ich finde die Methodenentwicklung und Planung von Forschungsprojekten in der Technikfolgenabschätzung wahnsinnig beeindruckend, insbesondere, weil diese Art von Forschung ja außerhalb des Labors stattfindet. Das Einbeziehen von 'NichtWissen  schaftlerinnen und -Wissenschaftlern' kann den Forschenden dabei helfen, realistische Szenarien zu entwickeln.

## Technik wird oft als Lösung für große Herausforderungen gesehen. Was überwiegt bei Ihnen Technikoptimismus oder -pessimismus?

Eigentlich Optimismus. Ich finde neue Techno  logien spannend und probiere gerne Neues aus. Aber gerade beim Klimaschutz habe ich manchmal das Gefühl, dass 'neue Technologien' als Entschuldigung genutzt werden, um am eigenen Verhalten oder an Gesetzen nichts ändern zu müssen. Quasi: 'Wir brauchen uns ja gar nicht bemühen, Emissionen zu reduzieren, irgendjemand wird schon etwas entwickeln, das den Klima  wandel für uns löst …'. Das finde ich fatal.

AUSFÜHRLICHE VIDEO-INTERVIEWS UNTER  www.tatup.de/youtube

SPECIAL TOPIC

## AI for decision support :

What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?

KI zur Entscheidungsunterstützung: Was sind mögliche Zukünfte, soziale Auswirkungen, regulatorische Optionen, ethische Fragen und Akteurskonstellationen?

Edited by D. Schneider and K. Weber

<!-- image -->

<!-- image -->

## INTRODUCTION

## AI-based decision support systems and society : An opening statement

Diana Schneider 1

<!-- image -->

Abstract  · Although  artificial  intelligence  (AI)  and  automated  decision-making systems have been around for some time, they have only recently gained in importance as they are now actually being used and are no longer just the subject of research. AI to support decision-making is thus affecting ever larger parts of society, creating technical, but above all ethical, legal, and societal challenges, as decisions can now be made by machines that were previously the responsibility of humans. This introduction provides an overview of attempts to regulate AI  and  addresses  key  challenges  that  arise  when  integrating  AI  systems into human decision-making. The Special topic brings together research articles that present societal challenges, ethical issues, stakeholders, and possible futures of AI use for decision support in healthcare, the legal system, and border control.

KI-basierte Entscheidungsunterstützungssysteme und die Gesellschaft : Der Versuch einer Einordnung

Zusammenfassung · Obwohl künstliche Intelligenz (KI) und automatisierte Entscheidungssysteme schon länger existieren, haben sie erst in  jüngster  Zeit  stark  an  Bedeutung  gewonnen,  da  sie  nun  tatsächlich eingesetzt werden und nicht mehr nur Gegenstand der Forschung sind. KI zur Unterstützung von Entscheidungen betrifft somit immer größere Teile  der  Gesellschaft,  wodurch  technische,  vor  allem  aber  ethische, rechtliche und soziale Herausforderungen entstehen, da nun Entscheidungen von Maschinen getroffen werden können, die bisher in der Verantwortung von Menschen lagen. Diese Einführung gibt einen Überblick über die Versuche, KI zu regulieren, und geht auf zentrale Herausforderungen ein, die sich aus der Integration von KI-Systemen in die menschliche Entscheidungsfindung ergeben. Das Special topic versam-

* Corresponding author: karsten.weber@oth-regensburg.de

1 Fraunhofer Institute for Systems and Innovation Research ISI, Karlsruhe, DE

2 Institute for Social Research and Technology Assessment, OTH Regensburg, Regensburg, DE

<!-- image -->

© 2024 by the authors; licensee oekom. This Open Access article is licensed under a Creative Commons Attribution 4.0 International License (CC BY). https://doi.org/10.14512/tatup.33.1.9

Received: 15. 01. 2024; revised version accepted: 19. 01. 2024;

published online: 15. 03. 2024 (editorial peer review)

https://doi.org/10.14512/tatup.33.1.9

<!-- image -->

, Karsten Weber  * , 2

<!-- image -->

melt Forschungsartikel, die gesellschaftliche Herausforderungen, ethische Fragen, Akteur*innen sowie mögliche Zukünfte des KI-Einsatzes zur  Entscheidungsunterstützung  in  der  Gesundheitsversorgung,  dem Rechtssystem und bei der Grenzkontrolle präsentieren.

Keywords · artificial intelligence (AI), decision support, sociotechnical systems, regulation, social impacts.

This article is part of the Special topic 'AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?,' edited by D. Schneider and K. Weber. https://doi.org/10.14512/tatup.33.1.08

## Introduction

In recent years the use of artificial intelligence (AI) systems to support decision-making has become established in various areas of application and has therefore also gained societal significance, as more and more individuals are affected by AI systems in very different situations and contexts. In contrast to systems that decide certain aspects autonomously, decision support systems (DSS) are characterized by the fact that they are merely a decision-making aid for human users. By means of AI, for example, decisions can be prepared by analyzing large amounts of data or recognizing patterns in it. While this can increase the efficiency and accuracy of decisions, it could have a variety of serious and far-reaching effects on individuals, groups, institutions, associations, companies, and society as well as the natural environment.

Since the scope of the impacts and the number of parties affected is so vast, or at least appears to be so vast, extreme scenarios are all too often conjured up in which AI systems either subjugate humanity or solve all of humanity's pressing problems, from climate change to combating pandemics, especially in public discussions about AI. The recent debate on large language models in general and ChatGPT in particular also follows this pattern, with proponents - to put it somewhat tongue-in-cheek declaring the use of AI systems a panacea and opponents label-

SPECIAL TOPIC · AI fOr dECISIOn SuPPOrT

<!-- image -->

ling them the work of the devil. However, Manichean thinking will hardly help to achieve plausible and realistic impact assessments of AI that can help to minimize or even prevent negative consequences and strengthen positive effects of this potentially disruptive technology.

One  fundamental  problem  with  referring  to  such  extreme scenarios is that the actual opportunities and risks of using AI tend to be obscured. In reality, there exists a huge continuum of effects between saving the world and destroying it, the assessment of  which  will  also  depend  on  point  of  view;  what  benefits  one  stakeholder  may  have  negative  consequences for another. In addition, the narrative of 'becoming a victim of tech- ulatory issues could only be dealt with marginally or not at all; therefore, at least a few comments on this should be made.

When a provisional agreement on the Artificial Intelligence Act (AIA) was reached on 9 December 2023 after lengthy negotiations in a trialogue between the European legislator, the European Parliament and the Council of the EU, this was heralded as a historic moment in the regulation of AI. The AIA takes a risk-based approach to regulation: While AI systems with no or only low risk are hardly regulated, special requirements apply to high-risk applications, e.g., specific transparency obligations and extensive requirements for data quality, documentation, and traceability (European Union 2023; European Commission

## It is currently impossible to predict what effects the proposed EU regulation will have on other countries and within the international discourse.

nology' might reduce the ability of social actors to intervene - if one always sees oneself as a victim or is considered to be a victim, this can prevent stakeholders from even trying to take measures to shape the technology and the social framework in which it  should be deployed. From such a passive position, it seems to be difficult or even impossible to discuss AI dispassionately and, for that matter, to make civic, professional, and/or political decisions based on sound information and rational arguments.

One example of this rather unfortunate situation is the sometimes quite emotional debate surrounding (AI-based) decision support  systems  in  medicine,  social  work,  the  judiciary,  and many other professional fields that are strongly characterized by human interactions between clients and professionals. This Special topic of TATuP is intended to help developing a differentiated perspective on AI systems in order to counteract premature judgements of AI. By presenting AI systems in various fields of application as well as the challenges and opportunities they create, the aim is also to encourage interdisciplinary dialogue. In the limited space available, it is impossible to cover all areas in which (AI-based) decision support systems are already being applied or could be employed in the foreseeable future. However, it is to be hoped that the examination of individual use cases will also provide insights into domains not covered in this volume.

## Attempts to regulate artificial intelligence

The research articles in this Special topic focus predominantly on outlining possible areas of application for (AI-based) decision support systems, identifying stakeholders and describing any potential impacts. Given the novelty of the subject, this is not only an important but also a difficult task. As a result, reg-

2021). Transparency is considered to be highly relevant in order to interpret AI-generated results and ensure appropriate use (European Commission 2021, reason 47, p. 30) - and thus ultimately contributes to the explainability of AI analyses and recommendations, so seems to be the assumption of EU lawmakers.

The measures proposed in the AIA are primarily aimed at preventing  potential  risks  to  the  fundamental  rights,  health, or safety of EU citizens. The debate on the draft regulation is particularly  essential  for  the  discussion  of  AI-supported  decision-making systems, as all the fields of application covered in this  TATuP Special topic (jurisdiction, law enforcement, and medicine) in principle must be considered particularly sensitive areas. The AI use cases discussed in the research articles can therefore  have  a  considerable  impact  on  the  lives  of  those  affected - not only in the event of an error, but also in regular use.

Yet, the AIA was and is on no account the only attempt to regulate the use of AI (Butcher and Beridze 2019; Schiff et al. 2022; Schmitt 2022; Ulnicane et al. 2021). An arbitrary and by no means comprehensive selection shows the range of types of actors and approaches to regulation: For instance, the OECD has formulated the Recommendation of the Council on Artificial Intelligence , the EU the Ethics Guidelines for Trustworthy AI and the Future of Life Institute the Asilomar AI Principles . These and many other documents appear to propose ethical guidelines and codes of ethics for regulation - at least that is what an initial review suggests. However, the binding force of ethical guidelines and codes of ethics is based on voluntary commitment; there is therefore no enforceability that only laws could offer. Moreover, Schiff et al. (2022) emphasize that most of these documents offer little indication of how requirements, recommendations and/ or claims they propose can be translated into actionable instructions for the practice of AI development and use, and instead remain at the rather abstract level of moral imperatives.

It can only be assumed that, in view of competing regulatory approaches, the AIA is rather not the last word on the regulation of AI, even more so as criticism of the AIA has not been long in coming. Furthermore, it is currently impossible to predict whether and what effects the proposed EU regulation will have on other countries and within the international discourse. How the application of AI will be regulated in, say, ten years' time in the areas covered in this Special topic as well as in other areas is therefore difficult to predict today.

## Human decisions and artificial intelligence

With regard to the question of how AI systems can be specifically integrated into human decision-making, mainly theoretical considerations and only a few empirical studies exist. Many of the following considerations originate from the medical context, as the impact of AI systems on human decision-making has long been the subject of intensive research, particularly in the healthcare professions. For example, Braun et al. (2020) outlined various modes of interaction for the healthcare sector (e.g., the integrative AI-DSS, which can independently request and collect patient data, or the fully automated AI-DSS, which does not require the involvement of professionals); however, most considerations primarily assume direct, essentially bilateral interaction between the professional and the AI system, i. e., a conventional AI-DSS. Simultaneously, there is widespread agreement that the integration of AI systems into the professional decision-making process - regardless of the respective mode of interaction - will have an impact on established work relations, e.g., on the relationship between professionals and patients or employees and tions can only be used responsibly if they can be correctly understood and interpreted by the users. The perception of AI systems as a second opinion could also raise further ethical questions regarding responsibility (Kempt and Nagel 2022). How AI-based systems could be meaningfully incorporated into shared decision-making processes (e.g., between patients or clients and professionals) also appears to be largely unresolved.

As the use of AI-based systems results in a stronger focus on data and the information, patterns, or meta-information it contains, other forms of professional knowledge could become jeopardized. Particularly in areas of application in which human experience, intuition, tactile or implicit knowledge, but also interpersonal relationships are highly valued (e.g., in the social and healthcare sector as well as in case of judicial or administrative decisions), an inappropriate focus on dataism is a concern and has been strongly criticized in some cases (Pedersen 2019; Devlieghere et al. 2022; Webb 2003). Various papers have pointed out that the data sets used for AI-based systems are fragmented (Tucker  2023),  may  contain  deliberate  omissions  (Schnei  der 2022), or that administrative data sets are unsuitable for assessing  professional  issues  (Gillingham  2015,  2020).  Besides  the fact that most training datasets have a strong bias and are poorly representative in terms of ethnic origin and gender, for example, there is also the problem that particularly vulnerable and/or stigmatized groups of people are often underrepresented.

However, analyzing large data sets opens up the possibility of contributing to evidence-based practice. But this requires that the algorithms underlying the AI-based recommendations are not exclusively pattern-based, but also incorporate concepts and theories  from  current  research  and  knowledge  -  otherwise  it would be almost impossible to make valid statements (Schneider et al. 2022 a).

## If AI systems suggest decisions and users routinely adopt them, it is not the technology that has changed but the way it is used.

employers (Schneider et al. 2022 b). On the one hand, the use of DSS is expected to reduce the workload and the potential time saved is associated with a more empathetic approach to patients (Topol 2019) - expectations that appear questionable given the increasing costs of purchasing and maintaining technology and educating  personnel  as  well  as  labor  shortages.  On  the  other hand, there are concerns that computer paternalism could undermine the essential relationship of trust between healthcare professionals and patients (Čartolovni et al. 2022; Heyen and Salloch 2021) - or, to put it in more general terms, between professionals and clients. Studies already indicate that time and again automation bias occurs (Sujan et al. 2019), i. e., recommendations from AI-based decision-making systems are adopted without question. In view of the frequent lack of data literacy, this poses an enormous challenge, as the AI-generated recommenda-

These short comments can only highlight a few aspects regarding the use of AI systems for decision support. For instance, the differentiation between systems for automatic decision-making (human-out-of-the-loop) and for decision support (humanin-the-loop, human-on-the-loop) should certainly be dealt with in much more detail, as different questions are raised depending on how AI systems are actually employed. It would also be worth to examine whether and how transitions from decision support systems to automatic decision-making systems might take place; this is less a technical issue than an organizational and practical one, because if AI systems suggest decisions but users routinely adopt them, it is not the technology that has changed but the way it is used. Such transitions in modes of use can in turn lead to far-reaching changes in the respective understanding of the profession and this in turn can again change modes of use

(Schneider et al. 2022 b). In other words: When talking about AI, this must always be done in terms of a socio-technical system.

## Contributions in this Special topic

The six contributions to this TATuP Special topic cover the use of AI systems in three different domains: healthcare, legal system, and law enforcement or, more precisely, border control. We decided to cluster thematically related research articles and to sort them in the clusters according to the alphabetical order of the names of the first authors - this seemed to us to be the best variant of an ultimately arbitrary arrangement. The first three research articles deal with the use of AI in the legal system, followed by a research article on AI systems used to identify illegal migrants at the border, and finally two research articles on the application of AI systems in medical contexts.

-  Anna-Katharina Dhungel and Moreen Heine are investigating whether, how and who would benefit from the use of AI systems in the legal system. They will attempt to answer this question based on interviews with judges from Germany. On the one hand, this means a limitation, as the results are only meaningful in the context of the German legal system, yet on the other hand, comparable studies have so far mostly been carried out in countries, such as the United States, whose legal system is structured significantly differently to the German system. The article therefore fills a research gap.
- Brandon Long and Amitabha Palmer focus on a different target group, as they look at the question of whether AI systems could have an advantage for jurisdiction from the perspective of the users of the legal system in the United States. Their primary interest is whether AI systems could be used as cost-effective advisors, in particular for socially disadvantaged people, providing, for example, reliable information about the potential success of a lawsuit.
-  With  reference  to  the  legal  system,  Giovana  Lopes  asks whether AI systems could be used to identify prejudices and partialities - bias for short - among judges, to alert them to the existence of bias, thereby inducing behavioral and attitudinal changes on the part of judges and thus ensuring fairer judgments.
- In their research article on the use of AI in the context of border controls, Daniel Minkin and Lou Therese Brandner explore the theoretical assumptions underlying the '  iBorderCtrl' system. They conclude that the theoretical foundations of the system are questionable and that the use of the system to recognize false statements made by people entering a country should therefore be called into question. This indicates that in  the  context  of  technology  assessment  not  only  technology, but also fundamental and/or theoretical assumptions that are incorporated into technology should be taken into account.
- Manuela Marquardt, Philipp Graf, Eva Jansen, Stefan Hillmann, and Jan-Niklas Voigt-Antons use a scenario-based in-

terview study to investigate which requirements AI systems in the medical context must fulfil in terms of explainability so that their outputs are comprehensible for all stakeholders. Without situationally adapted and comprehensible explanations, building trust in AI systems would hardly be possible and their use would therefore be called into question.

- Finally, Jan C. Zoellick, Hans Drexler, and Konstantin Drexler address the use of AI systems for the detection of melanoma. Using three scenarios, they consider shifts in competences from physicians to the systems used, examine the need for regulation of the use of AI systems in diagnostic contexts and finally look at a conundrum that is often brought up but rarely clearly resolved: What should be done if humans and machines come to different conclusions? Whose conclusion should take priority?

## Conclusion

As already indicated, the contributions in this Special topic cannot  address  all  aspects  of  AI-based  decision-making  support. Regarding technology assessment they do, however, provide examples of the topics and issues raised by the rapid and ubiquitous introduction of AI technologies. In the case of country-specific studies, the potential for drawing generalizations may be limited, but particularly from a technology assessment perspective, such specific studies cannot be dispensed with, as the effects of technology are determined to a considerable extent by the prevailing conditions. Discussions of specific topics usually differ not only from country to country, but also within a profession in a country where there are different and long-established strands of discourse with corresponding arguments and assumptions, e.g. in the field of the digitalization of social work in Germany (Waag and Rink 2023). Country comparisons and interdisciplinary studies can therefore help to make such discourses more comprehensible and transparent. The comparison of arguments and assumptions can also help to uncover blind spots in one's own argumentation. A differentiated view that is considering  country-specific  characteristics  and  social  conditions  is also indispensable with regard to impact assessment and evaluation of technology, as otherwise there is a risk of not being able to move beyond thinking in terms of extreme scenarios. While the research articles in this TATuP Special topic refer to similar challenges and issues, they also illustrate the importance of detail and differentiation, despite the variety of subjects covered.

Funding · This work received no external funding. Competing interests · Karsten Weber is a member of TATuP's scientific advisory board. He was not involved in the editorial voting process for the article's approval.

## Acknowledgement

The Special topic editors would like to thank the authors and reviewers for the professional and most cooperative collaboration.

## References

Braun, Matthias; Hummel, Patrik; Beck, Susanne; Dabrock, Peter (2020): Primer on an ethics of AI-based decision support systems in the clinic. In: Journal of Medical Ethics 47 (12), p. e3. https://doi.org/10.1136/medethics-2019-105860 Butcher, James; Beridze, Irakli (2019): What is the state of artificial intelligence governance globally? In: The RUSI Journal 164 (5-6), pp. 88-96. https://

doi.org/  10.1080/03071847.2019.1694260

Čartolovni, Anto; Tomičić, Ana; Lazić Mosler, Elvira (2022): Ethical, legal, and social considerations of AI-based medical decision-support tools. A scoping review. In: International Journal of Medical Informatics 161, p. 104 738. https://doi.org/ 10.1016/j.ijmedinf.2022.104738

Devlieghere, Jochen; Gillingham, Philip; Roose, Rudi (2022): Dataism versus relationshipism. A social work perspective. In: Nordic Social Work Research 12 (3), pp. 328-338. https://doi.org/10.1080/2156857X.2022.2052942

European Union (2023): Briefing - EU legislation in process. Artificial intelligence act. Available online at https://www.europarl.europa.eu/RegData/etudes/ BRIE/2021/698792/EPRS\_BRI(2021)698792\_EN.pdf, last accessed on 18. 01. 2024.

European Commission (2021): Proposal for a regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain union legislative acts. Brussels: European Commission. Available online at https://eur-lex.europa. eu/legal-content/EN/TXT/PDF/?uri=CELEX:52021PC0206, last accessed on 18. 01. 2024.

Gillingham, Philip (2015): Electronic information systems in human service organisations. The what, who, why and how of information. In: British Journal of Social Work 45 (5), pp. 1598-1613. https://doi.org/10.1093/bjsw/bcu030

Gillingham, Philip (2020): The development of algorithmically based decisionmaking systems in children's protective services. Is administrative data good enough? In: The British Journal of Social Work 50 (2), pp. 565-580. https:// doi.org/10.1093/bjsw/bcz157

Heyen, Nils; Salloch, Sabine (2021): The ethics of machine learning-based   clinical decision support. An analysis through the lens of professionalisation   theory. In: BMC Medical Ethics 22 (112), 9 pp. https://doi.org/10.1186/s12910-02100679-3

Kempt, Hendrik; Nagel, Saskia (2022): Responsibility, second opinions and peer-disagreement. Ethical and epistemological challenges of using AI in clinical diagnostic contexts. In: Journal of Medical Ethics 48 (4), pp. 222-229. https://doi.org/10.1136/medethics-2021-107440

Pedersen, John (2019): The digital welfare state. Dataism versus relationshipism. In: John Pedersen and Adrian Wilkinson (eds.): Big Data. Promise,   application and pitfalls. Cheltenham: Edward Elgar Publishing, pp. 301-324. https:// doi.org/10.4337/9781788112352.00019

Schiff, Daniel; Laas, Kelly; Biddle, Justin; Borenstein, Jason (2022): Global AI ethics documents. What they reveal about motivations, practices, and policies. In: Kelly Laas, Michael Davis and Elizsabeth Hildt (eds.): Codes of   ethics and ethical guidelines. Cham: Springer International, pp. 121-143. https:// doi.org/10.1007/978-3-030-86201-5\_7

Schmitt, Lewin (2022): Mapping global AI governance. A nascent regime in a fragmented landscape. In: AI and Ethics 2 (2), pp. 303-314. https://doi.org/10.1007/ s43681-021-00083-y

Schneider, Diana (2022): Ensuring privacy and confidentiality in social work through intentional omissions of information in client information systems. A qualitative study of available and non-available data. In: Digital Society 1 (26), 21 pp. https:/ /doi.org/10.1007/s44206-022-00029-9

Schneider, Diana; Maier, Angelika; Cimiano, Philipp; Seelmeyer, Udo (2022 a): Exploring opportunities and risks in decision support technologies for social workers. An empirical study in the field of disabled people's services. In: Sozialer Fortschritt 71 (6-7), pp. 489-511. https://doi.org/10.3790/sfo.71.6-7.489 Schneider, Diana; Sonar, Arne; Weber, Karsten (2022 b): Zwischen Automatisierung und ethischem Anspruch. Disruptive Effekte des KI-Einsatzes in und auf Professionen der Gesundheitsversorgung. In: Mario Pfannstiel (ed.): Künstliche Intelligenz im Gesundheitswesen. Entwicklungen, Beispiele und Perspektiven. Wiesbaden: Springer, pp. 325-348. https://doi.org/10.1007/978-3-65833597-7\_14

Sujan, Mark et al. (2019): Human factors challenges for the safe use of artificial intelligence in patient care. In: BMJ Health &amp; Care Informatics 26 (1), p. e100 081. https://doi.org/10.1136/bmjhci-2019-100081

Topol, Eric (2019): Deep medicine. How artificial intelligence can make healthcare human again. New York, NY: Basic Books.

Tucker, Catherine (2023): Algorithmic exclusion. The fragility of algorithms to sparse and missing data. Working Paper. In: Brookings Institution, 02. 02. 2023. Available online at https://www.brookings.edu/research/algorithmicexclusion-the-fragility-of-algorithms-to-sparse-and-missing-data/, last accessed on 17. 01. 2024.

Ulnicane, Inga; Knight, William; Leach, Tonii; Stahl, Bernd; Wanjiku, Winter-Gladys (2021): Framing governance for a contested emerging technology. Insights from AI policy. In: Policy and Society 40 (2), pp. 158-177. https://doi.org/10.108 0/14494035.2020.1855800

Waag, Philipp; Rink, Konstantin (2023): Digitalisierung als Irritation. Von ideologischen zu reflexionstheoretischen Selbstbeschreibungen der Sozialen Arbeit im Zuge ihrer Auseinandersetzung mit digitalen Technologien. In: Neue Praxis 23 (4), pp. 292-306.

Webb, Stephen (2003): Technologies of care. In: Elizabeth Harlow and Stephen Webb (eds.): Information and communication technologies in the welfare services. London: Jessica Kingsley Publishers, pp. 223-238.

<!-- image -->

<!-- image -->

## DIANA SCHNEIDER

is research associate at the Competence Center Emerging Technologies at the Fraunhofer Institute for Systems and Innovation Research ISI since 2021. She was a PhD candidate of the NRW Digital Society research program from 2018-2022. Her research focuses on innovations in social and healthcare systems, in particular their ethical and social implications.

## PROF. DR. KARSTEN WEBER

is research professor for Technology Assessment and AI-based Mobility at OTH Regensburg since 2022 and has been working there as senior researcher since 2013. His research is focused on impacts of technology on individuals, groups, society, and environment, namely information and communication technology, particularly in healthcare, mobility, and energy.

<!-- image -->

## RESEARCH ARTICLE

## Cui bono? Judicial decision-making in the era of AI : A qualitative study on the expectations of judges in Germany

Anna-Katharina Dhungel  * , 1

Abstract · Despite substantial artificial intelligence (AI) research in various domains, limited attention has been given to its impact on the judiciary, and studies directly involving judges are rare. We address this gap by using 20 in-depth interviews to investigate German judges' perspectives on AI. The exploratory study examines (1) the integration of AI in court proceedings by 2040, (2) the impact of increased use of AI on the role and independence of judges, and (3) whether AI decisions should supersede human judgments if they were superior to them. The findings reveal an expected trend toward further court digitalization and various AI use scenarios. Notably, opinions differ on the influence of AI on judicial independence and the precedence of machine decisions over human judgments. Overall, the judges surveyed hold diverse perspectives without a clear trend emerging, although a tendency toward a positive and less critical evaluation of AI in the judiciary is discernible.

## Cui bono? Richterliche Entscheidungsfindung in Zeiten von KI :

Eine qualitative Untersuchung zur Erwartungshaltung von Richter*innen in Deutschland

Zusammenfassung · Obwohl der Einsatz von künstlicher Intelligenz (KI) in diversen Kontexten wissenschaftlich erforscht wird, ist die Anzahl der Studien zu KI in der Justiz überschaubar. Insbesondere gibt es kaum Untersuchungen mit direkter Einbindung von Richter*innen. Um diese Lücke zu schließen, analysieren wir anhand von 20 Interviews die Perspektive deutscher Richter*innen auf KI. Die Schwerpunkte in diesem explorativen Beitrag liegen auf (1) der Nutzung von KI in Gerichtsver-

* Corresponding author: annakatharina.dhungel@uni-luebeck.de

1 Institute of Multimedia and Interactive Systems (IMIS), University of Lübeck, Lübeck, DE

<!-- image -->

© 2024 by the authors; licensee oekom. This Open Access article is licensed under a Creative Commons Attribution 4.0 International License (CC BY). https://doi.org/10.14512/tatup.33.1.14

Received: 25. 08. 2023; revised version accepted: 10. 01. 2024; published online: 15. 03. 2024 (peer review)

<!-- image -->

, Moreen Heine 1

<!-- image -->

fahren bis 2040, (2) dem Einfluss der zunehmenden KI-Nutzung auf die Rolle und die Unabhängigkeit von Richter*innen sowie (3) der Frage, ob KI-Entscheidungen mehr Gewicht haben sollten als menschliche Urteile, wenn sie diesen überlegen wären. Die Ergebnisse zeigen, dass tendenziell eine zunehmende Digitalisierung der Gerichte und einige spezielle KI-Anwendungen erwartet werden. Bei der Frage nach dem Einfluss auf die  richterliche  Unabhängigkeit  und  der  Bewertung  von  KI-Entscheidungen gehen die Meinungen der Befragten auseinander. Insgesamt zeigt sich in den Interviews keine einheitliche Position, in der Tendenz überwiegt jedoch eine eher positive und weniger kritische Bewertung des KI-Einsatzes in der Justiz.

Keywords · artificial intelligence, algorithmic judges, user-centered studies, e-justice, expert interviews

This article is part of the Special topic 'AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?,' edited by D. Schneider and K. Weber. https://doi.org/10.14512/tatup.33.1.08

## Introduction

While significant strides have been made in the exploration of AI's impact on various sectors, the judicial domain remains relatively understudied within this discourse. Existing research primarily centers on legal issues related to AI implementation in courts, public perceptions of algorithmic judges, and isolated technical case studies (Eidenmüller and Wagner 2021; Watson et al. 2023; Yalcin et al. 2023). Particularly, the deployment of risk assessment systems in criminal proceedings attracts scholarly attention, notably concerning issues of justice and discrimination, alongside the technical feasibility (Berk 2019; Dressel and Farid 2018; Završnik 2020). However, there is a conspicuous lack of studies that focus on the attitudes of judges towards AI. Publications that specifically address this audience are rare, https://doi.org/10.14512/tatup.33.1.14

<!-- image -->

<!-- image -->

both  in  the  context  of  Germany,  and  in  comparison  with  other nations. Notably for the German-speaking region, Hartung et  al.  (2022)  examined  the  future  of  digital  justice,  involving interviews with judges, and a publication by IBM compiles insights garnered from discussions with (vice-)presidents of various courts (IBM Deutschland 2022).

Judges provide unique insights into the current state of the art regarding technology use within courts, and they are the target group of the AI systems under consideration. Therefore, their perspectives serve as a critical touchstone for understanding the potential implications arising from AI's integration into legal proceedings. Building on this premise, through the conduct of 20 in-depth interviews with German judges, this explorative research aims to shed light on the following research questions:

- RQ1: According to the surveyed judges, how will a court proceeding appear in the year 2040, and to what extent is AI expected to be involved?
- RQ2: What implications would an increased use of AI have on the role of judges and the principle of judicial independence?
- RQ3: Based on responses from the judges, should the results and decisions of an AI system prevail over human judgments if the system demonstrably arrives at superior verdicts?

The  findings  demonstrate  a  general  expectation  for  the  ongoing digitalization of courts, while scenarios for the implementation of AI are only partially conceivable. Concerning the impact of AI on judicial independence, contrasting views were prevalent. Many individuals hold reservations about fully delegating to be deployed by 2025, designed to bolster and streamline legal processes (Yu 2022). The United Nations Educational, Scientific and Cultural Organization envisions a rising adoption of AI in the judiciary, evident in the development of a dedicated online course titled 'AI and the Rule of Law: Capacity Building for Judicial Systems' (UNESCO 2023). Additionally, a growing demand for the judiciary to go digital has been fueled by citizens' higher expectations, increased court workloads, succession challenges, and the need to balance the playing field with legal tech providers (IBM Deutschland 2022).

However, gazing into the future is unnecessary, as AI systems are already being formally employed by judges. Risk assessment tools are perhaps best recognized: The objective of these tools is to determine the prospective likelihood of recidivism among offenders. In 49 out of 50 US states, such systems are applied to assess aspects like bail, parole, pretrial custody status, or the duration of sentences (Stevenson 2018). The Chinese AI-driven system 'Little Judge Bao' goes further, proposing tailored sentences based on pre-selected factors (Shi 2022). Looking at the state of digitalization, Singapore serves as a notable instance of a highly digitized judiciary with an all-encompassing online case management system across jurisdictions, facilitating case initiation, monitoring, and data for predictive caseload analysis. Canada showcases another example, launching its first online tribunal in 2012, where all court interactions occur digitally (Hartung et al. 2022).

Germany's judiciary has fallen behind both internationally and compared to other sectors in adopting digital transformation. According to Hartung et al. (2022), the technological solutions implemented within the German judicial system are li-

## In many countries, it is expected that AI use in legal proceedings will increase in the future.

decision-making to machines, perceiving it as both inconceivable and worrisome. Conversely, a portion of respondents deem such delegation conceivable given specific circumstances and conditions. A minority of proponents advocate for machine-mediated decision-making, contingent upon substantiated evidence demonstrating its superior decision-making capabilities. Overall, the perspectives and views of the surveyed judges are diverse and a clear trend cannot be determined. However, there exists a tendency to evaluate AI implementations in the judiciary more optimistically and positively rather than critically.

## AI in the judiciary: state of the art

In  many countries, it is  expected  that  AI  use  in  legal  proceedings will increase in the future. This sentiment is exemplified in China, where an extensive network of AI applications is set mited,  outdated,  and  not  sufficiently  aligned  with  user  requirements.  They  estimate  that  the  digitalization  of  the  German judiciary lags behind leading countries by approximately 10-15 years. Dreyer and Schmees (2019) conclude that the feasibility of AI in the judiciary fails solely due to the insufficient availability of training data. Despite this lag in digitalization, the use of algorithms in courts is subject of a critical debate among legal scholars. This encompasses discussions on how algorithms could be deployed within the judiciary to address the shortcomings of human decision-making (Nink 2021), the legal evaluation of so-called 'robot judges' (Greco 2021), or the implications of AI deployment on human rights (Završnik 2020). In the field of information systems, the topic has received less attention thus far. Some studies examine the recidivism prediction algorithms already in use in the United States, focusing on aspects such as fairness and reliability (Berk 2019), or the effects of human-machine interaction in this context (Grgić-Hlača et al. 2019). The

target audience of AI systems in the judiciary is predominantly not directly involved in these studies.

## Research method

## Sample characteristics

The sample (n = 20) was recruited through email invitations to  courts,  to  the  Deutscher  Richterbund  (German  association of judges), and through personal networks. It comprises eleven male and nine female individuals, with an average experience of 13.6 years as judges (sd = 10.3 years). Almost all participants hold active judge positions, with only one individual having ceased working as a judge in 2017. The distribution across judicial levels includes 10 judges from local courts, 8 from regional courts, and 2 from higher regional courts. Regarding age, one of the participants is below 30 years old, seven are between 30 to 39 years old, five are between 40 to 49 years old, five are between 50 to 59 years old, and two are above 60 years old. Nine individuals specialize in civil law, three in criminal law, with four of them holding active judge positions in both civil and criminal law. Two participants each practice administrative law and labor law. Participants responded to the Affinity for Technology Interaction (ATI) Scale (Franke et al. 2019). The results, obtained on a scale ranging from 1 (low affinity) to 6 (high affinity), reveal that, as a group, participants demonstrate a moderate level of affinity for technology interaction, with a mean score of m = 3.47 (sd = .94, range: 2.00-5.00) and a high internal consistency ( α = .92). This suggests that the sample is not biased by a strong affinity for technology, which could have been possible since participation in the interviews was voluntary, implying an inherent interest in the topic.

## Conducting the interviews

This  study  adopted  the  reporting  framework,  guidelines,  and dramaturgical model proposed by Myers and Newman (2007) for conducting interviews within the context of information sys-

Tab. 1: Meta-data of the interviews. Source: based on Myers and Newman 2007

<!-- image -->

| subjects/interviews:    | 20/20                                                            |
|-------------------------|------------------------------------------------------------------|
| period of interviews:   | 3 months                                                         |
| interview model:        | dramaturgical model                                              |
| description of process: | see this chapter                                                 |
| type of interview:      | structured, improvised callbacks, small survey at the end        |
| recording technique:    | mostly taped and transcribed                                     |
| thin/thick description: | moderate description                                             |
| anon/revealed:          | anonymous                                                        |
| feedback:               | participants welcomed to share any further thoughts on the issue |

tem research. They suggest incorporating essential meta-data related to the interviews (see table 1).

The initial interview was conducted in person, while all subsequent interviews were conducted virtually. The guiding questionnaire consisted of 29 questions, categorized into six sections: current technology usage, AI system requirements, personal attitudes, expectations, human judges' capacity, and ethics. The present paper emphasizes the questions within the expectations and ethics categories.

Nearly the entire interview process was audio-recorded. In the initial twelve interviews, recording was omitted for the categories human judges' capacity and ethics , opting for written notes instead. This approach was intended to foster greater trust and enhance participants' confidence, given the sensitive nature of these questions. However, this method did not produce the desired outcome. As a result, for the subsequent eight interviews, the entire interview process was recorded. The content analysis that followed used the MAXQDA software.

## Qualitative analysis

The content analysis was guided by the methodological frameworks put forth by Kuckartz and Rädiker, encompassing both their general approaches and the specific techniques employed for analyzing interviews (Kuckartz and Rädiker 2019; Rädiker and Kuckartz 2020). For the development of the coding scheme, a  data-driven  approach (inductive methodology) was adopted, involving a step-by-step coding process where codes were iteratively generated until saturation was achieved. The aim is to structure the content and analyze it on the basis of this structure. In this way, diverse attitudes and opinions can be identified. The two authors initially independently coded three randomly selected interviews. The results were then discussed and harmonized. Subsequently, the remaining interviews were independently coded, and their outcomes, such as their alignment with existing codes or the creation of new codes, were deliberated upon. The overall categorical system consists of 72 codes and a total of 339 text passages were coded.

## Results

## The judiciary in 2040

The judges were asked how a court proceeding might look in the year 2040. The question was received differently, yielding a wide range of responses, as indicated by the numerous codes generated (25 in total). These responses can be categorized into two main themes: future scenarios that describe expectations for forthcoming court proceedings, and critical aspects and concerns  regarding  the  anticipated  developments.  The  following list summarizes the mentioned scenarios, with the frequency of each mention indicated in parentheses.

- Advanced  digitalization  (17): It  was  mentioned  that  'improved technology', 'no more paper files', and the ability

to 'better process documents with computers' are anticipated.  Moreover,  parties  are  expected  to  'communicate  electronically with each other', 'submit a relatively significant amount electronically', and 'hopefully, we will indeed have an electronic case file'. Digital documentation is expected to increase, for example, due to the recording of hearings, and it is anticipated that evidence will be technologically processed and digitally accessible.

- Video hearing (14): The judges are confident that video conferencing for hearings will continue to expand, anticipating increased opportunities in this regard.

Furthermore,  the  following  statements  were  mentioned  once each: the potential for new citizen-court interactions, such as online lawsuit filings; the potential partial replacement of judges; the potential use of Virtual Reality; the emergence of digital lawyers for defendants; and the anticipated collaboration enhancement within the EU, possibly facilitated through an EU-wide shared database for decisions.

Regarding the mentioned concerns and critical considerations, it was noted six times that face-to-face conversation is irreplaceable, and four times it was emphasized that human interaction cannot be substituted, with one person saying: 'What dis-

## This risk of an automation bias was acknowledged and confirmed by nearly all respondents.

- Hardly any change (7): Some judges do not anticipate significant changes within the judiciary, and if any changes are foreseen, they are mostly limited to minor developments in digitalization.
- AI  deployment  (5): Certain  judges  anticipate  an  increased use of AI, for instance, in tasks such as suggesting applicable norms for a case, organizing precedent cases, automating the creation of basic legal documents, and employing predictive analysis tools.
- Pre-trial proceedings (5): Some judges anticipate the establishment of pre-trial proceedings such as the implementation of predictive tools, resulting in the avoidance of less promising lawsuits, the use of algorithms developed by companies or other private sector stakeholders to decide specific disputes, and the existence of online dispute resolutions.

In addition, the following aspects were addressed, which characterize the scenarios in more detail:

- Decision-making remains with humans (4): Four respondents are certain that the ultimate decision will remain with the

tinguishes judicial decisions and court proceedings at their core, however, is the personal conversation and the individual context within a legal process. I believe that this cannot be replaced by AI systems because there is a significant amount of social interaction involved, which may not directly relate to legal matters but nonetheless significantly shapes the situation.' Two individuals stated that they believe older judges will struggle with the growing digitalization. Another two highlighted the importance of a societal debate about the use of AI in the legal system, questioning whether we as a society desire such developments. Two respondents expressed concerns about the increasing reliance on technology. The following concerns were raised once each: the growing digital asymmetry within the legal profession, IT security, the lack of competence of IT service providers, and concerns about the rule of law.

## The future role of judges

Subsequently, participants were asked about their expectations regarding the development of the judge's role by 2040. The responses varied between positive expectations, concerns, and neutral statements (see table 2).

- judge, as one individual expressed, 'I believe we still need judges who make the final decision'.
- Reduced  processing  times  (4): It  is anticipated that, among other factors, the  implementation  of  new  technologies  will  enable  court  proceedings to  be  conducted in a shorter span of time.
- Change  in  organizational  structure (3): An 'infrastructure reform' is anticipated, and it is also assumed that there will be fewer courts and judges as well as fewer case numbers, particularly in civil law.

Tab. 2: Categorization of potential changes of the judge's role by 2040.

| Optimistic Anticipation       | ∑   | Concerns                          | ∑   | Neutral                               |   ∑ |
|-------------------------------|-----|-----------------------------------|-----|---------------------------------------|-----|
| Relief through digitalization | 7   | Reduced decision-making authority | 2   | No changes of the judge's role        |   6 |
| AI as support and assistance  | 4   | Reduced reverence                 | 1   | Judges as case managers and mediators |   5 |
| AI in mass proceedings        | 1   | Rise in information overflow      | 1   | Judgment remains with the human       |   3 |
|                               |     | Additional responsibilities       | 1   | New competencies necessary            |   3 |
|                               |     |                                   |     | Surveillance of the systems           |   2 |

Source: interviews conducted by the authors

Tab. 3: Categorization of responses to the question of whether AI machines should render judgments instead of humans. Source: interviews conducted by the authors

| Perspective                     | Argument                                             |   ∑ |
|---------------------------------|------------------------------------------------------|-----|
| Approval                        | Human beings prone to errors                         |   3 |
|                                 | If demonstrably superior judgments, then approval    |   3 |
|                                 | The machine is more powerful                         |   1 |
| Conditional approval            | Usage allowed but adopting the results not mandatory |   3 |
|                                 | Usage if case-by-case justice appropriate            |   2 |
| Approval for specific use cases | AI usage for mass proceedings                        |   1 |
|                                 | AI in preceding administrative actions               |   1 |
|                                 | In some cases conceivable (without specifying)       |   1 |
| Rejection                       | Human perception and responsibility crucial          |   7 |
|                                 | End of judicial independence                         |   2 |
|                                 | Hierarchy of instances rendered obsolete             |   1 |
|                                 | Rule of law concerns                                 |   1 |
|                                 | Training data susceptible to manipulation            |   1 |
|                                 | AI verdict not accepted by humans                    |   1 |

creasingly facing pressure to justify decisions that do not align with those of AI, and fears that AI might render judges redundant. On the other hand, the argument was made that judicial  independence  is not  at  risk,  as  ultimately,  judges  decide how  and  when  to  use  technology,  with AI systems serving merely as assistants (7). Moreover, it was emphasized that a threat to judicial independence would depend on whether the use of AI systems would be mandatory and how the integration  of  such  algorithms  into  procedural rules would occur (12). Additionally, it was stressed twice that it is within the responsibility of individual judges to determine whether their own independence would be compromised or not: 'I believe that an AI system can pose a significant threat to lazy-minded judges.'

The question was raised as to whether the implementation of AI systems within the judiciary might lead judges to excessively rely on them, potentially fostering automation bias - a tendency to overly trust automated systems, which may result in errors or overlooking something (Skitka et al. 2000). This question specifically pertained to decision support systems, where the human makes the final decision, for instance, pre-filing court orders. According to Sheridan's automation scale - ranging from 1 = the human must decide and execute everything to 10 = the system acts autonomously and decides without human involvement - these systems fall within levels up to a maximum of 5 (Sheridan et al. 1978).

Some judges noted this issue persists even without AI, such as when they agree to a prosecutor's case dismissal request to reduce workload (mentioned in 4 interviews). Younger individuals, with greater trust in technology, were identified by two respondents as more prone to agree with the system, reinforcing automation bias. Additionally, it was noted that judges often face time constraints, which could lead them to go along with the system's decision simply due to time pressure (4). Proposed solutions included the need for judges to receive appropriate training (1), designing the systems and their usage context with psychological incentives to avoid automation bias (1), and the implementation of relevant regulations (1). In contrast, some respondents (5) believe that automation bias is not a problem for judges because they are 'self-disciplined', that they have 'inherent skepticism towards anything that touches their own high decision-making authority', and, ultimately, that the 'professional group is inherently inclined to resist'.

Participants were also asked whether judicial independence is called into question with an increased use of AI. On the one hand, some (5) believe the development to be critical due to concerns about a gradual takeover by such systems, with judges in-

## Perspectives on AI-generated judgments

During the interviews, the judges were also asked: Should AI system outcomes override human judgments when the system consistently  yields  better  verdicts?  In  this  context,  the  discussion pertained to systems classified at level 10 on the automation scale, meaning they operate autonomously without human intervention (Sheridan et al. 1978). The majority of respondents (14)  initially  countered  by  stating  that  it  is  not  demonstrable at what point a decision would be considered 'better'. Subsequently, responses to this question diverged significantly. Some supported the idea of machines issuing judgments, while others endorsed it only under certain conditions or for specific use cases. Conversely, many responses entailed explicit and absolute rejection (see table 3).

As evident from the frequency of mentions, it is apparent that not only were the respondents divided in their opinions, but individual participants also provided varying statements. Three statements not included in the table indicate that the responsibility of AI systems for decision-making is a societal choice: 'I believe that, since we live in a democracy, if society decides that we want this, it should be done.'

## Discussion

Judges  operate  with  autonomy,  determining  their  operational methodologies, and hold accountability for each procedural facet, as articulated in Art. 97 Abs. 1 GG, which underscores their independence and subordination solely to the law. They typically lack dedicated secretarial support or personally assigned assistants, exemplifying the self-directed nature of their role. Consequently, this engenders, on the one hand, the fundamental latitude for judges to exercise discretion in adopting technologi-

cal aids, unless statutory provisions dictate otherwise. On the other hand, it underscores the challenge of seamlessly integrating AI-based assistance systems into existing judicial processes.

It is to be noted that the interviews do not reveal a consistent consensus; a multitude of diverse viewpoints were expressed. However, it is noticeable that there is a general tendency towards a more favorable outlook rather than a critical one regarding the future possibilities of AI application, such as its potential use as a helpful tool in mass proceedings or as an assistance system for case processing. At the same time, critical topics such as IT security or data for such systems and the associated potential for discrimination  were  scarcely  addressed.  The  slightly  positive view held by judges could be attributed to their potential lack of AI expertise compared to members of, for instance, the informa- bias, as they hold an interest in AI. Finally, different interpretations and confusion regarding AI and digitalization were observed. Despite the predominantly descriptive nature of the analysis, it might serve as a valuable resource for future research endeavors, particularly for theory building.

## Outlook

According to the draft of the EU AI Act (Article 8 of Annex III), AI systems used in the judiciary are classified as high risk. Therefore, the deployment of AI systems for judges is already politically  viewed  with  skepticism.  As  a  result,  scenarios  involving the use of AI give rise to various legal, technical, and ethi-

## Nearly one-third of the respondents anticipate that the role of the judge will not change.

tion systems community, leading to a limited understanding of the technological challenges associated with AI. Judges tend to perceive digitalization and related AI technologies as advantageous for their daily work, hence the positive outlook.

Regarding the first research question concerning expectations for the year 2040, the responses demonstrate a strong reliance on digitalization. This underscores the previously mentioned lag in the German judiciary and the judges' expectations that this gap will be bridged in the coming years. Increased AI deployment is only expected to a limited extent. Regarding the second research question concerning potential impacts on the role of judges, on the one hand, a positive expectation was revealed, such as relief through digitalization. On the other hand, concerns were expressed, for instance, regarding reduced decision-making authority.

A similar pattern emerged in response to the question about judicial independence, with some expressing concerns about a gradual takeover by AI, while others had no reservations. The risk  of  automation  bias  coming  with  regular  use  of  such  systems, in turn, was largely acknowledged. Concerning the third research  question  about  whether  judgments  from  AI  systems should potentially be deemed more significant than those made by humans, there were supporters who could envision such a scenario under specific circumstances, as well as opponents who assert that such an outcome is precluded. Notably, the diversity of responses to the previous questions remained evident in addressing this question as well, although there is research that demonstrate that higher levels of automation are frequently met with less acceptance when compared to lower levels (Ghazizadeh et al. 2012).

The study has notable limitations to consider. It is confined to the German context, potentially impacting its applicability to other legal systems. Due to the small sample size, the study is not representative. Also, the judges' self-selection might introduce cal questions, such as: How can the outcomes of AI systems be made comprehensible for judges (encompassing the broad extensive topic of explainable AI)? How can procedural justice and the right to a fair hearing be ensured? How can ongoing legal oversight be maintained despite the use of AI, and self-reinforcing processes be prevented? What specific impact do particular systems have on the decision-making of judges?

It is an ongoing societal debate; therefore, it is essential that scientific research is conducted to ensure the effective customization of solutions to the distinct requirements of judges. The partnership between legal scholars and computer scientists becomes pivotal in cultivating approaches that address the unique demands of a contemporary judicial system. Future interdisciplinary research should focus on exploring in a human-centered manner how judges can effectively employ AI in ways that align with technical feasibility, streamline their work processes, and gain societal acceptance.

## Acknowledgement

We thank Eva Beute for her support during the interviews and we thank the reviewers for their constructive and supportive feedback.

Funding · This work received no external funding. Competing interests · The authors declare no competing interests.

## References

Berk, Richard (2019): Machine learning risk assessments in criminal justice settings. Cham: Springer. https://doi.org/10.1007/978-3-030-02272-3

Dressel, Julia; Farid, Hany (2018): The accuracy, fairness, and limits of predicting recidivism. In: Science Advances 4 (1), p. eaao5580. https://doi.org/10.1126/ sciadv.aao5580

Dreyer, Stephan; Schmees, Johannes (2019): Künstliche Intelligenz als Richter?

Wo keine Trainingsdaten, da kein Richter. Hindernisse, Risiken und Chancen

der Automatisierung gerichtlicher Entscheidungen. In: Computer und Recht 35 (11), pp. 758-764. https://doi.org/10.9785/cr-2019-351120

Eidenmüller, Horst; Wagner, Gerhard (2021): Law by algorithm. Tübingen: Mohr Siebeck. https://doi.org/10.1628/978-3-16-157509-9

Franke, Thomas; Attig, Christiane; Wessel, Daniel (2019): A personal resource for technology interaction. Development and validation of the affinity for technology interaction (ATI) scale. In: International Journal of HumanComputer Interaction 35 (6), pp. 456-467. https://doi.org/10.1080/10447318.20 18.1456150

Ghazizadeh, Mahtab; Lee, John; Boyle, Linda (2012): Extending the technology acceptance model to assess automation. In: Cognition, Technology &amp; Work 14 (1), pp. 39-49. https:/ /doi.org/10.1007/s10111-011-0194-3

Greco, Luís (2021): Roboter-Richter? Eine Kritik. In: Hans-Georg Dederer and Yu-Cheol Shin (eds.): Künstliche Intelligenz und juristische Herausforderungen. Tübingen: Mohr Siebeck, pp. 103-122.

Grgić-Hlača, Nina; Engel, Christoph; Gummadi, Krishna (2019): Human decision making with machine assistance. In: Proceedings of the ACM on HumanComputer Interaction 3 (CSCW), pp. 1-25. https://doi.org/10.1145/3359280

Hartung, Dirk; Brunnader, Florian; Veith, Christian; Plog, Philipp; Wolters, Tim (2022): The future of digital justice. Boston: Boston Consulting Group. Available online at https://web-assets.bcg.com/3a/4a/66275bf64d92b78b8fabeb 3fe705/22-05-31-the-future-of-digital-justice-bls-bcg-web.pdf, last accessed on 04. 01. 2024.

IBM Deutschland (2022): Unter Digitalisierungsdruck. Die Justiz auf dem Weg ins digitale Zeitalter. New York, NY: IBM Corporation.

Kuckartz, Udo; Rädiker, Stefan (2019): Analyzing qualitative data with MAXQDA. Text, audio, and video. Cham: Springer.

Myers, Michael; Newman, Michael (2007): The qualitative interview in IS research. Examining the craft. In: Information and Organization 17 (1), pp. 2-26. https:// doi.org/10.1016/j.infoandorg.2006.11.001

Nink, David (2021): Justiz und Algorithmen. Über die Schwächen menschlicher Entscheidungsfindung und die Möglichkeiten neuer Technologien in der Rechtsprechung. Berlin: Duncker &amp; Humblot. https://doi.org/10.3790/978-3428-58106-1

Rädiker, Stefan; Kuckartz, Udo (2020): Focused analysis of qualitative interviews with MAXQDA. Step by Step. Berlin: MAXQDA Press.

Sheridan, Thomas; Verplank, William; Brooks, Thomas (1978): Human/  computer control of undersea teleoperators. In: Proceedings of NASA Ames   Research Center 14 th  Annual Conference on Manual Control, pp. 343-357. Avail  able online at https://ntrs.nasa.gov/api/citations/19790007441/downloads/ 19790007441.pdf, last accessed on 15. 01. 2024.

Shi, Jiahui (2022): Artificial intelligence, algorithms and sentencing in Chinese criminal justice. Problems and solutions. In: Criminal Law Forum 33 (2), pp. 121-148. https://doi.org/10.1007/s10609-022-09437-5

Skitka, Linda; Mosier, Kathleen; Burdick, Mark (2000): Accountability and automation bias. In: International Journal of Human-Computer Studies 52 (4), pp. 701-717. https://doi.org/10.1006/ijhc.1999.0349

Stevenson, Megan (2018): Assessing risk assessment in action. In: Minnesota Law Review 103, pp. 303-384. Available online at https://scholarship.law.umn.edu/ mlr/58, last accessed on 04. 01. 2024.

UNESCO - United Nations Educational, Scientific and Cultural Organization (2023): AI and the rule of law. Capacity building for judicial systems. Available online at https://www.unesco.org/en/artificial-intelligence/rule-law/ mooc-judges, last accessed on 04. 01. 2024.

Watson, Joe; Aglionby, Guy; March, Samuel (2023): Using machine learning to create a repository of judgments concerning a new practice area. A case study in animal protection law. In: Artificial Intelligence and Law 31 (2), pp. 293-324. https://doi.org/10.1007/s10506-022-09313-y

Yalcin, Gizem; Themeli, Erlis; Stamhuis, Evert; Philipsen, Stefan; Puntoni, Stefano (2023): Perceptions of justice by algorithms. In: Artificial intelligence and Law 31 (2), pp. 269-292. https://doi.org/10.1007/s10506-022-09312-z

Yu, Eileen (2022): China wants legal sector to be AI-powered by 2025. In: ZDNET/ innovation, 12. 12. 2022. Available online at https://www.zdnet.com/article/ china-wants-legal-sector-to-be-ai-powered-by-2025/, last accessed on 04. 01. 2024.

Završnik, Aleš (2020): Criminal justice, artificial intelligence systems, and human rights. In: ERA Forum 20 (4), pp. 567-583. https://doi.org/10.1007/s12027-02000602-0

<!-- image -->

<!-- image -->

## ANNA-KATHARINA DHUNGEL

holds a Master's degree in International Politics and International Law, as well as a Master in   Business Informatics. With five years of experience as an IT Consultant, she joined the University of Lübeck in 2021 to pursue her PhD and conduct research on the implementation of artificial intelligence in the field of justice.

## PROF. DR. MOREEN HEINE

has been a professor of E-Government and Open Data Ecosystems at the University of Lübeck and scientific director of the Joint Innovation Lab since 2019. Previously, she was an assistant professor of business information systems at the University of Potsdam. She is a National E-Government Competence Centre (NEGZ) board member.

<!-- image -->

## RESEARCH ARTICLE

## AI and access to justice :

## How AI legal advisors can reduce economic and shame-based barriers to justice

<!-- image -->

Brandon Long  * , 1  , Amitabha Palmer 2 

<!-- image -->

Abstract · ChatGPT - a large language model - recently passed the U.S. bar exam. The startling rise and power of generative artificial intelligence (AI) systems such as ChatGPT lead us to consider whether and how more specialized systems could be used to overcome existing barriers to the legal system. Such systems could be employed in either of the two major stages of the pursuit of justice: preliminary information  gathering  and  formal  engagement with the state's legal institutions and professionals. We focus on the former and argue that developing and deploying publicly funded AI legal advisors can reduce economic and shame-based cultural barriers to the information-gathering stage of pursuing justice.

KI und Rechtszugang : Wie rechtsberatende KI-Systeme wirtschaftliche und schambedingte Barrieren für den Rechtszugang abbauen können

Zusammenfassung · ChatGPT - ein ,Large Language Model' - hat kürzlich  die  US-amerikanische  Anwaltsprüfung  bestanden.  Der  erstaunliche Erfolg und die Leistungsfähigkeit generativer Systeme künstlicher Intelligenz  (KI)  wie  ChatGPT  veranlassen  uns  zu  der  Überlegung,  ob und wie spezialisiertere  Systeme  eingesetzt  werden  könnten,  um  bestehende Barrieren im Rechtssystem zu überwinden. Solche Systeme könnten in den zwei wichtigsten Phasen der Rechtsfindung eingesetzt werden: der vorbereitenden Informationsbeschaffung und der formellen Zusammenarbeit mit den staatlichen Rechtsinstitutionen und -expert*innen. Wir konzentrieren uns auf Erstere und argumentieren, dass

* Corresponding author: brlong@bgsu.edu

1

2

Department of Philosophy, Bowling Green State University, Bowling Green, US MD Anderson Cancer Center, The University of Texas, Houston, US

<!-- image -->

© 2024 by the authors; licensee oekom. This Open Access article is licensed under a Creative Commons Attribution 4.0 International License (CC BY).

https://doi.org/10.14512/tatup.33.1.21

Received: 22. 08. 2023; revised version accepted: 04. 01. 2024;

published online: 15. 03. 2024 (peer review)

https://doi.org/10.14512/tatup.33.1.21

<!-- image -->

die Entwicklung und der Einsatz öffentlich finanzierter rechtsberatender  KI-Systeme  wirtschaftliche  und  schambedingte  kulturelle  Barrieren in der Informationsbeschaffungsphase der Rechtsfindung abbauen können.

Keywords · artificial intelligence, shame, barriers to justice, philosophy of technology, law

This article is part of the Special topic 'AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?,' edited by D. Schneider and K. Weber. https://doi.org/10.14512/tatup.33.1.08

## Introduction

In  most  countries,  the  legal  system  is  the  primary  institution through which citizens pursue justice when wronged or to exercise their rights. However, in many of these countries, a variety of barriers prevent citizens from accessing the legal system (OECD 2015). The primary barriers are economic, cultural, and political and they generate significant externalities such as social exclusion, government dependence, weaker business assurances, and higher healthcare costs (OECD 2015, p. 4). It is not only a just but a prudent state that seeks to reduce or eliminate barriers to justice.

ChatGPT - a large language model (LLM) - recently passed the U.S. bar exam (Arredondo et al. 2023). The startling rise and power of generative AI systems such as ChatGPT lead us to consider whether and how more specialized systems could be employed to overcome existing barriers to the legal system. Broadly, such systems could be employed in either of two major stages of the pursuit of justice: preliminary information gathering and formal engagement with the State's legal institutions and professionals. We focus on the former and argue developing and deploying publicly funded artificial intelligence legal   advisors

SPECIAL TOPIC · AI fOr dECISIOn SuPPOrT

<!-- image -->

(AI LAs) can reduce economic and shame-based cultural barriers to the information-gathering stage of pursuing justice.

By AI LA we have in mind a system that provides potential litigants with reliable legal information that is specific and intelligible to allow them to make informed decisions whether to formally contract a lawyer and/or formally pursue their claims in court. Several similar legal AI platforms already exist (AI Lawyer 2023; Rattray 2023; Casetext 2022) and will likely only improve over time. In one London-based law firm from November 2022 to February 2023, Harvey AI was used by 3,500 of their lawyers to ask 40,000 legal questions during their day-to-day work (Rattray 2023). Specialized AI models can already give advice for specific domains of law. For example, JusticeBot (Tribunal administratif du logement 2023), a free tool for Quebec housing law, takes case facts into account in giving legal advice, asks pertinent questions, and cites similar cases for each relevant legal claim.

Throughout, we do not claim that highly reliable AI LAs currently exist but assume they will be available soon. As such, a foresighted technology assessment should consider their use before such technologies are developed and become available. We advocate that AI LAs be publicly funded. While a privately developed AI LA could achieve the same technical goals, a publicly  funded  AI  LA  supports  broader  economic  accessibility. Moreover, democratic governments and international organizations should support implementation of AI LAs. Access to justice is intrinsically good but also provides instrumental benefits. It is a crucial prerequisite for establishing legal confidence and trust, thereby creating a favorable business environment, attracting  investments,  and  contributing  to  overall  economic  spending (The Perryman Group 2009, pp. 19-21). Growing evidence suggests that the ability to address legal issues and obtain justice has a positive impact on inclusive economic growth (OECD

To support our thesis that AI LAs can reduce barriers to justice, we (1) outline common economic and shame-based cultural barriers to pursuing legal justice, (2) describe how an AI LA can mitigate barriers during the information-gathering stage, and (3) address  potential  limitations  and  harms.  Our  scope  for  these claims is  Anglo-American common law systems. This brings with it  unique  barriers  to  legal  aid  and  implementing  AI  systems, and a specific common law that may or may not generalize more globally.

## Economic barriers

## Economic barriers to legal aid seeking

This section reviews economic barriers to justice and suggests how an AI LA could reduce them during the information-gathering stage. Economic barriers are not only financial, but also the opportunity cost of time spent on information-seeking and transportation.

A  substantial  body  of  evidence  finds  people  with  low  socio-economic status (SES) face greater barriers to the legal system - and therefore they also face greater barriers to gain access to justice (Commission on Legal Empowerment of the Poor 2008, pp. 6-9; Legal Services Corporation 2022, sec. 5; OECD 2015, p. 7). Poverty, poverty-related discrimination, and distrust present barriers to justice globally (Beqiraj and McNamara 2014, chaps. 4-5). What is more, marginalized - including economically marginalized - populations face unique barriers to justice in the UK (Gill et al. 2021) and in Canada (Silverman and Molnar 2016).

Financial barriers influence whether people pursue justice through the legal system. Across OECD countries, 42 % to 90 % of individuals who opt out of pursuing legal aid attribute their

## Our thesis is that artificial intelligence legal advisors can reduce barriers to justice.

2013, p. 2, 2015, pp. 1-4). This impact is manifested through job creation, reduced absences at work due to legal problems (Task  Force  on  Justice  2019,  p. 45),  improved  housing  stability, resolution of debt, and stimulating growth by instilling confidence  in  assurance  (Stolper  et  al.  2007,  pp. 8-9).  Equal  access to justice may also foster economic growth by establishing a level playing field (Task Force on Justice 2019, pp. 39-41), especially for small or medium economic participants (OECD 2015, pp. 3-4). It also facilitates enforcement of contracts, encourages fair competition, and instills confidence in regulatory frameworks (OECD 2015, pp. 9-10). Thus, supporting access to justice can play a role in assisting individuals to overcome severe forms of social exclusion and ensuring equal opportunities for economic advancement.

decision to financial considerations, whether real or perceived (OECD 2015, p. 5). Further, for 92 % of legal problems low-income Americans face, they do not receive any or enough legal aid (Legal Services Corporation 2022, pp. 47-48). Moreover, education and accessible information are also barriers to the justice system -53 % of low-income Americans do not know if they are able to find an affordable lawyer if needed (Legal Services Corporation 2022, pp. 51-52). These and other barriers lead them to pursue litigation at lower rates than higher SES groups. For example, in medical contexts, lower SES groups pursue litigation at lower rates when compared to other groups because of a lack of access to legal resources and the nature of the contingency fee system in medical malpractice claims (McClellan et al. 2012; Viser 2022).

## Overcoming economic barriers with publicly funded AI legal advisors

To specify ways in which AI technology can reduce economic barriers to justice we propose conceiving the pursuit of legal justice as having two stages:

1. Information gathering: In this stage, one seeks to determine whether one has a claim, evaluate the strength of that claim, and evaluate the cost-benefit tradeoffs of formally pursuing one's legal claim.
2. Formal engagement: In this  stage,  one  hires  a  lawyer  and engages with state officials and the court system to pursue one's claim.

People with limited economic resources cannot frivolously engage with an economically onerous legal system. Before one invests resources to make an informed formal pursuit of a claim, one must have some sense of one's prospects for success and the underlying legal reasoning. Hence, existing economic barriers to information gathering prevent people who, unbeknownst to them, have strong claims and might have pursued them formally had they possessed this knowledge. We believe AI LAs are well-suited to address economic barriers to information gathering.

We have in mind an AI LA that could provide prospective litigants with (a) an assessment of legal considerations and reasoning involved in their claim, (b) a crude assessment of their case's likelihood of success in court, (c) an interactive lay explanation of (a) and (b).

Assessment of legal considerations would include explanations of which laws apply, why and how they apply, and how similar cases have been treated. The crude assessment of the likelihood of legal success would be expressed as 'poor,' 'unlikely,'

tion-gathering stage may now choose to pursue them. Nevertheless, economic barriers are not the only barriers to justice. We now turn to investigating how legal AI can address cultural and shame-based barriers to justice.

## Cultural barriers to justice

In this section, we (a) define shame and how it relates to cultural norms, (b) explain how it can pose a barrier to pursuing justice in the information-gathering stage, (c) suggest how a publicly funded AI LA can mitigate barriers to legal information seeking. Notice in the following that while economic barriers to justice may be addressed by funding human legal resources, AI LAs have unique features that address shame-based barriers in ways additional funding cannot.

## Shame, stigma, culture

Shame is a 'negative emotion that arises when one is seen and judged  by  others  (whether  they  are  present,  possible  or  imagined) to be flawed in some crucial way, or when some part of  oneself  is  perceived  to  be  inadequate,  inappropriate  or  immoral' (Dolezal and Lyons 2017, p. 257). Shame influences behavior because it can threaten one's feelings of belonging and acceptance within interpersonal contexts, socially, and politically (Walker and Bantebya-Kyomuhendo 2014).

Shame can be acute or chronic. Acute shame is a single episode  that  arises  unexpectedly,  as  in  cases  of  embarrassment where in social interaction, one's self-presentation falters, fails, or falls short of socially desired modes of comportment (Dolezol and Lyons 2017). Chronic shame is often a result of general social stigma directed at marginalized social groups. For instance, shame is linked to racism, discrimination (Harris-Perry 2011),

## A government-funded artificial intelligence legal advisor can provide legal information sought without imposing burdensome costs.

'unknown,' 'fair,' or 'strong.' Critically, like existing LLMs, AI advisors will be conversational, allowing users to ask follow-up questions and clarifications.

Citizens with a limited understanding of their legal situation, the likelihood of success, and scarce economic resources may be hesitant to approach a lawyer. Proposed capabilities for the AI LA align with the information citizens seek during information gathering. Unlike consulting a lawyer, a government-funded AI LA can provide legal information sought without imposing burdensome financial, time, and transportation costs. Moreover, if it is publicly funded, citizens bear minimal direct costs and online accessibility eliminates transportation and mitigates time costs.

Under this model, citizens who otherwise might not have pursued legitimate claims due to economic barriers in the informa- low SES (Walker and Bantebya-Kyomuhendo 2014), and body size (Farrell 2011).

Shame  is  often  a  function  of  cultural  norms.  Groups  use shame to reinforce norms by stigmatizing norm violators in two ways: by stigmatizing observed behavior of individuals, or by stigmatizing one's relationship or belonging to a particular social group (Goffman 1986). Stigmatization gives rise to feelings of shame among stigmatized which in turn disincentivizes or incentivizes certain behaviors (Battle 2019, p. 645). Hence, stigma, shame, and cultural norms interact to influence behavior.

The justice system and social norms can leverage the power of  shame  through  stigmatization  to  prevent  certain  behaviors and incentivize others. This is neither good nor bad, but rather depends on the nature of the norms being supported. Stigma-

tizing  theft  or  domestic  violence  isn't  a  bad  cultural  practice. However, we suggest shame and shame-inducing norms that impede the legitimate pursuit of justice are prima facie bad and should be eliminated. These include norms against litigation due to group membership and norms against disclosing information that stigmatizes or shames.

In the cases below, we show how AI LAs can mitigate shamebased  barriers  to  justice  due  to  the  human  propensity  not  to feel  judged  when  interacting  with  AI  (Bartneck  et  al.  2010; Holthöwer and Van Doorn 2023).

## Overcoming shame-based cultural barriers with publicly funded AI legal advisors

First case: Shame-based barriers to justice for victims of intimate partner violence

Victims of intimate partner violence (IPV) often forgo pursuing justice because the stigmatization of being a victim can lead to shame (Overstreet and Quinn 2013). IPV survivors grapple with a lasting sense of shame after their experience, stemming from lost self-identity, self-blame, and fear of judgment (Camp 2022, p. 103). Seeking help often leads to encounters with people or institutions - including the legal system - that worsen rather than alleviate this shame (Camp 2022, pp. 136-137). Understandably, individuals who perceive a stigma associated with being a victim of IPV are less likely to seek institutional support, and when they do disclose experiences, they prefer indirect language that hints at abuse without disclosing details (Williams and Mickelson 2008).

The most common legal intervention for victims of IPV is protection  orders,  vital  tools  for  responding  to  IPV.  Yet,  obtaining a protection order requires survivors to enter 'a process that often deprives them of their privacy and ability to control their self-image - experiences anchored in shame' (Camp 2022, pp. 103-104).  This  suggests  shame  may  both  be  a  barrier  to disclosing information and for seeking aid. For example, one U.S. abuse shelter from 2004-2008 found of all victims of IPV, only 32.25 % had protection orders upon appearing at the shelter (Durfee and Messing 2012).

AI  LAs  can  mitigate  shame-based  barriers  to  information gathering for victims of IPVs. Interacting with an AI rather than a human restores privacy and eliminates shame that can be induced by the presence of others which can allow a victim of IPV to safely learn (a) what legal recourse and protections are available to them, (b) how to pursue legal recourse/protection, (c) whether their circumstances meet legal criteria, (d)  the  likelihood they will succeed in obtaining legal recourse or protection, and (e) all the above in an interactive lay-friendly language. Certainly, victims of IPV will have to engage with humans if they decide to pursue recourse formally. However, AI LAs allow the acquisition of information for an informed legal decision. Moreover, since social stigmas associated with being a victim of IPV are unjustified and harmful, access to an AI LA justifiably reduces shame-based harms to victims of IPVs.

Second case: Shame-based barriers to justice for marginalized groups when cultural norms obscure legal rights

Cultural norms may stigmatize seeking legal aid for women or people in positions of lower social status where fear of reprisal or shame keeps legal complaints underserved (Long Chamness and Ponce 2019, pp. 13-17). This may be common outside the U.S., where compensation culture is weak, nonexistent, or displaced by other norms in specific contexts. Consider the following case involving inheritance rights.

In  some  communities,  there  is  a  cultural  expectation  that women will relinquish their inheritance rights to their brothers when their parents die (Nayeen 2020). In doing so, a woman protects and ensures their culturally coveted status of a 'good sister'. Conversely, failure to relinquish her right puts her social status in jeopardy and incurs stigma from norm violation.

The prevalence of a cultural norm for women to relinquish their inheritance rights can create confusion about what legal inheritance rights women have. Moreover, it can prevent women from inquiring about their rights as this could be interpreted as a pre-emptive norm violation (Nayeen 2020). Even inquiring into one's rights can incur stigma or shame - especially if one isn't yet sure of the extent of one's rights or whether one would indeed pursue them.

This cost often prohibits investigating legal rights, which prevents obtaining information required to make informed legal decisions. With full information, a woman may reason the benefit of asserting her inheritance rights offsets the social cost of norm violation. Furthermore, social norms can never be overturned until women living in such communities accurately understand their rights. In short, the conflation between social norms and legal rights deprives women (and other similarly situated marginalized groups) of the opportunity to make informed decisions regarding whether they wish to exercise their inheritance rights.

An AI LA can provide women with the information necessary to make informed decisions regarding tradeoffs between social sanction and exercising inheritance rights (or other rights) is worth it. Such information includes: (a) clarifying any confusion with respect to the nature and extent of the rights in question, (b) other legal variables, (c) the legal process required for exercising rights, (d) a coarse-grained assessment of the case's likelihood of success in court, and (e) an interactive lay explanation of preceding information.

This is how our model can mitigate cultural barriers to justice in the information-gathering stage. An AI LA permits private inquiries into rights in a way immune to shame. This is most true in small or tight-knit communities where being seen walking into a law office could cause gossip and shame.

Third case: Shame as a barrier to justice for victims of fraud Disclosing to others that one has been a victim of fraud brings about acute shame that can prevent victims from pursuing justice.  In  the  U.S.,  for  example,  fraud is an enormous problem, as consumers lost nearly $ 9 billion in 2022 (Fair 2023). However,  victims  rarely  come  forward  and  pursue  justice.  A  sur-

vey conducted by The American Association of Retired Persons (AARP) found an estimated 15 % contacted  authorities  (Williams 2023). Another report found that 30 % of respondents indicated they would be embarrassed to admit to being a victim of a financial scam, whether to friends, family, or authorities (Aviva 2021, p. 11).

Shame may lead such victims to forgo information-gathering since they can only identify costs (shame) without the benefit. Hence, such victims frequently do not pursue legal claims because they never acquire the information to evaluate benefits.

Again, a publicly funded AI LA could reduce shame-induced barriers to pursuing justice in the information-gathering stage for victims of fraud. Formally pursuing legal recourse requires understanding at minimum (a) whether the law applies to one's case, (b) what one is entitled to in a successful judgment, (c) a coarse-grained assessment of one's chances of success, and (d) an interactive lay explanation of the above. The private nature

However, this concern relies on the assumption that access to AI LAs will only increase the number of cases going to trial. AI LAs may also reduce litigation in some cases. Litigants who go to trial systematically overestimate their chance of success (Korobkin and Guthrie 1994; Weinstein 2002). By providing litigants with estimates of success, litigants who overestimated their likelihood of gain may not pursue a trial when they otherwise would have. Reducing litigant miscalculation may also lead to more settlements than trials (Korobkin and Guthrie 1994; Priest and Klein 1984), and settlements are less burdensome to the legal system. We do not speculate on the net change of cases in the legal system, but the effects will likely work in both directions.

Finally, this concern overlooks the benefits of broader AI implementation within the law. Tasks that used to take days of research can now be completed in minutes. The cost and time associated with each case will likely decrease with widespread AI implementation.

## Artificial intelligence legal advisors are permissibly deployed when they are as reliable and accurate as human lawyers.

of interactions with AI shields victims from the potential gossip and shame of interacting with a human. An AI LA can mitigate shame-based barriers to pursuing justice by reducing the shamebased cost of seeking legal information a victim needs to make an informed decision regarding tradeoffs between shame-based (and other) costs and benefits of formally pursuing litigation.

## Discussion

One worry with AI LAs are reliability and accuracy standards (Grimm et. al. 2021). However, our position does not depend on the reliability and accuracy of current systems. Rather, we hold that such systems are permissibly deployed when they are as reliable and accurate as human lawyers. Early investigations suggest that high levels of reliability and accuracy will be attained in specific domains (e.g., the above mentioned JusticeBot for housing law) before an all-purpose LLM can handle all legal domains (Deakin and Markou 2020; Hildebrandt 2016). Insofar as this is the case, we support domain-specific models since some mitigation of the barriers to justice is better than no mitigation.

## Harms

Our proposal might result in increased caseloads in a legal system, which is concerning due to increased funding needs for typically  underfunded  and  overburdened  systems.  This  legitimate concern points to the inevitable trade-offs emergent technologies generate.

## Bias

A common concern with many AI systems is that they can inherit and reproduce biases in their training data. This concern also applies to AI LAs who will have been trained in case law rife with historical biases. This topic of biases in AI is large and ongoing. An exhaustive treatment goes beyond the scope of this research article. However, a brief response is warranted.

First,  the  question  of  biases  will  always  be  comparative. There is unlikely ever to exist any human-developed system free of all biases. The question, therefore, is whether an AI LA could have fewer biases than the current system. We believe the answer is 'yes' because it is much easier to alter the biases of an AI than it is of the individuals and institutions that compose the entire legal system.

Bias  inhabits  AI  systems  within  their  training  data,  algorithms, and outputs. We know that biased data leads to biased algorithms. Therefore, it's possible to mitigate bias through debiasing the training set or through careful selection of training data. Where this isn't possible, it's possible to adjust algorithms that we know were developed using biased data. Finally, if we know in advance that an AI's outputs are biased, it's possible to have the AI correct in the other direction (Fazelpour and Danks 2021). While these correction measures are not easy or foolproof, they are easier and more likely to succeed than attempting to correct the implicit and systemic biases of every individual and institution that compose current justice systems. Finally, addressing biases in a legal system can happen concurrently while addressing biased AI LAs in the ways we have mentioned.

## Responsibility

AI generates questions about legal responsibility within existing legal frameworks (Beckers and Teubner 2021). An AI LA could make two major kinds of errors that lead to harm which raise questions about responsibility and liability: The AI (a) recommends pursuing litigation when there is no viable claim, or (b) recommends abstaining from litigation when in fact there is a viable claim. The topic of responsibility in AI ethics is rich and complicated and cannot be addressed comprehensively within the constraints of this research article. Nevertheless, a few brief remarks are in order.

In the first case, the issues of responsibility and liability are relatively unproblematic. The AI LA recommends pursuing a claim which leads the user to contact a lawyer. If the AI has erred, the lawyer should explain why further legal action would be inappropriate. If the lawyer is correct, there is no harm save a consultation fee. If the lawyer is incorrect, the lawyer bears the responsibility just as they are currently held responsible for poor legal advice.

In  the  second  case,  a  fund  liability  model  is  appropriate (Beckers and Teubner 2021, pp. 139-140). In this model, a regulatory agency creates and administers a fund or insurance to provide compensation for harm. Firms in the industry sector finance the fund according to their market share and the agency determines ex-post liability for each case/class of cases. Finally, such a model will require that AI LA be audited at appropriate intervals since naive individuals will not know when the AI's advice not to litigate is mistaken.

## Conclusion

We have explained how economic cost and shame present barriers to accessing the justice system, how AI LAs may alleviate these barriers, and have covered some limitations and harms of such LLMs. There is no one solution to every legal barrier for everyone, but AI LAs present several viable solutions. Such advisors can reduce economic and shame-based barriers to the information-gathering  stage  of  pursuing  justice.  This  is  significant since lack of information is itself a barrier to informed decision-making regarding whether to formally pursue justice. We take the value of justice to be intrinsic and self-evident, therefore, expanding access to justice is a good thing. The legal system becomes more just when the cases reaching the court do so based on merit rather than arbitrary barriers.

Funding · This work received no external funding. Competing interests · The authors declare no competing interests.

## References

- AI Lawyer (2023): AI lawyer blog. Available online at https://ailawyer.pro/blog, last accessed on 03. 01. 2024.
- Arredondo, Pablo; Driscoll, Sharon; Schreiber, Monica (2023): GPT-4 passes the bar exam. What that means for artificial intelligence tools in the legal pro-

fession. In: Stanford Law School Blog. Available online at https://law.stanford. edu/2023/04/19/gpt-4-passes-the-bar-exam-what-that-means-for-artificialintelligence-tools-in-the-legal-industry/, last accessed on 03. 01. 2024.

- Aviva (2021): The Aviva fraud report. The online fraud epidemic during the pandemic. London: Aviva. Available online at https://static.aviva.io/content/ dam/aviva-corporate/documents/newsroom/pdfs/reports/Aviva\_Fraud\_ Report\_2021.pdf, last accessed on 03. 01. 2024.
- Bartneck, Christoph; Bleeker, Timo; Bun, Jeroen; Fens, Pepijn; Riet, Lynyrd (2010): The influence of robot anthropomorphism on the feelings of embarrassment when interacting with robots. In: Paladyn, Journal of Behavioral Robotics 1 (2), pp. 109-115. https://doi.org/10.2478/s13230-010-0011-3
- Battle, Brittany (2019): 'They look at you like you're nothing'. Stigma and shame in the child support system. In: Symbolic Interaction 42 (4), pp. 640-668. https://doi.org/10.1002/symb.427
- Beckers, Anna; Teubner, Gunther (2021): Three liability regimes for artificial intelligence. Algorithmic actants, hybrids, crowds. Oxford: Hart. https://doi. org/10.5040/9781509949366
- Beqiraj, Julinda; McNamara, Lawrence (2014): International access to justice. Barriers and solutions. London: International Bar Association. Available online at https://www.biicl.org/documents/485\_iba\_report\_060215. pdf?showdocument=1, last accessed on 03. 01. 2024.
- Camp, A. Rachel (2022): From experiencing abuse to seeking protection. Examining the shame of intimate partner violence. In: UC Irvine Law Review 13 (1), pp. 103-154. Available online at https://scholarship.law.uci.edu/ucilr/vol13/ iss1/7, last accessed on 03. 01. 2024.
- Casetext (2022): Westlaw, lexis outranked by Casetext on G2. Casetext blog, 27. 06. 2023. Available online at https://casetext.com/blog/casetext-to-jointhomson-reuters-ushering-in-a-new-era-of-legal-technology-innovation, last accessed on 03. 01. 2024.
- Commission on Legal Empowerment of the Poor (2008): Making the law work for everyone. New York, NY: United Nations Development Programme. Available online at https://digitallibrary.un.org/record/633966?ln=en, last accessed on 03. 01. 2024.
- Deakin, Simon; Markou, Christopher (eds.) (2020): Is law computable?   Critical perspectives on law and artificial intelligence. Oxford: Hart. https://doi. org/10.5040/9781509937097
- Dolezal, Luna; Lyons, Barry (2017): Health-related shame. An affective determinant of health? In: Medical Humanities 43 (4), pp. 257-263. https://doi. org/10.1136/medhum-2017-011186
- Durfee, Alesha; Messing, Jill (2012): Characteristics related to protection order use among victims of intimate partner violence. In: Violence Against Women 18 (6), pp. 701-710. https:/ /doi.org/10.1177/1077801212454256
- Fair, Lesley (2023): FTC crunches the 2022 numbers. See where scammers continue to crunch consumers. In: FTC Buisness Blog, 23. 02. 2023. Available online at https://www.ftc.gov/business-guidance/blog/2023/02/ftc-crunches2022-numbers-see-where-scammers-continue-crunch-consumers, last accessed on 03. 01. 2024.
- Farrell, Amy (2011): Fat shame. Stigma and the fat body in American culture. New York, NY: New York University Press.
- Fazelpour, Sina; Danks, David (2021): Algorithmic bias. Senses, sources, solutions. In: Philosophy Compass 16 (8), p. e12 760. https://doi.org/10.1111/phc3.12760
- Gill, Nick et al. (2021): The tribunal atmosphere. On qualitative barriers to access to justice. In: Geoforum 119, pp. 61-71. https://doi.org/10.1016/j. geoforum.2020.11.002

- Goffman, Erving (1986): Stigma. Notes on the management of spoiled identity. New York, NY: Simon &amp; Schuster.

Grimm, Paul; Grossman, Maura; Cormack, Gordon (2021): Artificial intelligence as evidence. In: Northwestern Journal of Technology and Intellectual Property 19 (1), pp. 9-106. Available online at https://scholarlycommons.law.northwestern. edu/njtip/vol19/iss1/2, last accessed on 04. 01. 2024.

- Harris-Perry, Melissa (2011): Sister citizen. Shame, stereotypes, and black women in America. New Haven, CT: Yale University Press.

Hildebrandt, Mireille (2016): Law as information in the era of data-driven agency. In: The Modern Law Review 79, pp. 1-29. https://doi.org/10.1111/14682230.12165

Holthöwer, Jana; Van Doorn, Jenny (2023): Robots do not judge. Service robots can alleviate embarrassment in service encounters. In: Journal of the Academy of Marketing Science 51 (4), pp. 767-784. https://doi.org/10.1007/s11747022-00862-x

Korobkin, Russell; Guthrie, Chris (1994): Psychological barriers to litigation settlement. An experimental approach. In: Michigan Law Review 93 (1), pp. 107-192. https://doi.org/10.2307/1289916

Legal Services Corporation (2022): The justice gap. The unmet civil legal needs of low-income Americans. Washington, DC: Legal Services Corporation. Available online at https://lsc-live.app.box.com/s/xl2v2uraiotbbzrhuwtjlgi0emp3 myz1, last accessed on 03. 01. 2024.

Long Chamness, Sarah; Ponce, Alejandro (2019): Measuring the justice gap. A   people-centered assessment of unmet justice needs around the world. Washington, DC: World Justice Project. Available online at https:// worldjusticeproject.org/our-work/research-and-data/access-justice/ measuring-justice-gap, last accessed on 04. 01. 2024.

McClellan, Frank; White, Augustus; Jimenez, Ramon; Fahmy, Sherin (2012): Do poor people sue doctors more frequently? Confronting unconscious bias and the role of cultural competency. In: Clinical Orthopaedics &amp; Related Research 470 (5), pp. 1393-1397. https:/ /doi.org/10.1007/s11999-012-2254-2

Nayeen, Zulker (2020): Social and cultural barriers in accessing civil   justice system. In: The Daily Star, 11. 02. 2020. Available online at https://www. thedailystar.net/law-our-rights/news/social-and-cultural-barriersaccessing-civil-justice-system-1866442, last accessed on 04. 01. 2024.

OECD (2013): What makes civil justice effective? In: OECD Economics   Department Policy Note 18, pp. 1-16. Available online at https://web-archive.oecd. org/2013-06-20/238744-Civil%20Justice%20Policy%20Note.pdf, last accessed on 04. 01. 2024.

OECD (2015): Equal access to justice. Expert roundtable notes. Paris: OECD. Available online at https://www.oecd.org/gov/Equal-Access-Justice-Roundtablebackground-note.pdf, last accessed on 04. 01. 2024.

Overstreet, Nicole; Quinn, Diane (2013): The intimate partner violence stigmatization model and barriers to help seeking. In: Basic and Applied Social Psychology 35 (1), pp. 109-122. https://doi.org/10.1080/01973533.2012.746599

- Priest, George; Klein, Benjamin (1984): The selection of disputes for litigation. In: The Journal of Legal Studies 13 (1), pp. 1-55. https://doi.org/10.1086/ 467732

Rattray, Kate (2023): Harvey AI. What we know so far. In: Clio Blog, 10. 10. 2023. Available online at https://www.clio.com/blog/harvey-ai-legal/, last accessed on 03. 01. 2024.

- Silverman, Stephanie; Molnar, Petra (2016): Everyday injustices. Barriers to access to justice for immigration detainees in Canada. In: Refugee Survey Quarterly 35 (1), pp. 109-127. https:/ /doi.org/10.1093/rsq/hdv016

Stolper, Antonia; Walker, Mark; Sabatini Christopher; Marczak Jason (2007): Rule of law, economic growth, and prosperity. New York, NY: Americas Society and Council of the America Rule of Law Working Group. Available online at https://www.as-coa.org/sites/default/files/Rule%20of%20Law.pdf, last accessed on 03. 01. 2024.

Task Force on Justice (2019): Justice for all. Final report. New York, NY: Center on International Cooperation. Available online at https://www.sdg16.plus/ resources/justice-for-all-report-of-the-task-force-on-justice/, last accessed on 03 January 2024.

The Perryman Group (2009): The impact of legal aid services on economic activity in Texas. An analysis of current efforts and expansion potential. Waco, TX: The Perryman Group. Available online at https://legalaidresearch. org/2020/02/04/the-impact-of-legal-aid-services-on-economic-activityin-texas-an-analysis-of-current-efforts-and-expansion-potential/, last accessed on 04. 01. 2024.

Tribunal administratif du logement (2023): JusticeBot - interactive legal information tool. Available online at https://www.tal.gouv.qc.ca/en/justicebotinteractive-legal-information-tool, last accessed on 03. 01. 2024.

Viser, Cassidy (2022): The economics of injustice. Stratification in medical malpractice claims by poor and vulnerable patients. In: Georgetown Journal on Poverty Law and Policy 29 (2), pp. 273-285. Available online at https://www. law.georgetown.edu/poverty-journal/wp-content/uploads/sites/25/2022/05/ GT-GPLP220017-3.pdf-Cassidy-Viser.pdf, last accessed on 04. 01. 2024.

- Walker, Robert; Bantebya-Kyomuhendo, Grace (2014): The shame of poverty. Oxford: Oxford University Press.

Weinstein, Ian (2002): Don't believe everything you think. Cognitive bias in   legal decision making. In: Clinical L. Rev. 8 (783), pp. 783-834. Available online at https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=2779670, last accessed on 04. 01. 2024.

Williams, Alicia (2023): Consumer fraud awareness gets d grade. In: AARP Research, 17. 05. 2023. https:/ /doi.org/10.26419/res.00606.001

- Williams, Stacey; Mickelson, Kristin (2008): A paradox of support seeking and rejection among the stigmatized. In: Personal Relationships 15 (4), pp. 493509. https://doi.org/10.1111/j.1475-6811.2008.00212.x

BRANDON LONG

<!-- image -->

<!-- image -->

is currently an M. A. student at Bowling Green State University and will be seeking a PhD after its completion. He is currently working on bioethical arguments for and against genetic enhancement.

AMITABHA PALMER, PHD

holds a PhD in Philosophy and is a HEC-C   certified Clinical Ethicist. He is an Instructor and Clinical Ethicist at the University of Texas MD Anderson Cancer Center. His primary areas of research include the ethics of AI, medical ethics, political philosophy, and the effects of medical misinformation on   clinical interactions.

<!-- image -->

## RESEARCH ARTICLE

## Artificial intelligence and judicial decision-making : Evaluating the role of AI in debiasing

Giovana Lopes  * , 1

Abstract · As arbiters of law and fact, judges are supposed to decide cases impartially, basing their decisions on authoritative legal sources and not being influenced by irrelevant factors. Empirical evidence, however, shows that judges are often influenced by implicit biases, which can affect the impartiality of their judgment and pose a threat to the right to a fair trial. In recent years, artificial intelligence (AI) has been increasingly used for a variety of applications in the public domain, often with the promise of being more accurate and objective than biased human decision-makers. Given this backdrop, this research article  identifies  how  AI  is  being  deployed  by  courts,  mainly  as  decision-support tools for judges. It assesses the potential and limitations of these tools, focusing on their use for risk assessment. Further, the article shows how AI can be used as a debiasing tool, i. e., to detect patterns of bias in judicial decisions, allowing for corrective measures to be taken. Finally, it assesses the mechanisms and benefits of such use.

## Künstliche Intelligenz und richterliche Entscheidungsfindung :

Nutzung von KI zur Vermeidung kognitiver Verzerrungen

Zusammenfassung · Als  Schiedsrichter*innen in Rechts- und Tatsachenfragen sollen Richter*innen unparteiisch entscheiden, indem sie ihre  Entscheidungen  auf  der  Grundlage  maßgeblicher  Rechtsquellen treffen  und sich nicht von irrelevanten Faktoren beeinflussen lassen. Empirische Untersuchungen zeigen jedoch, dass Richter*innen häufig von kognitiven Verzerrungen (implicit biases) beeinflusst werden, die die Unvoreingenommenheit ihres Urteils beeinträchtigen und eine Gefahr für das Recht auf ein faires Verfahren darstellen können. In den letzten Jahren wurde künstliche Intelligenz (KI) vermehrt für eine Viel-

* Corresponding author: giovana.figueiredo@unibo.it

1 Department of Legal Studies, University of Bologna, Bologna, IT

<!-- image -->

© 2024 by the authors; licensee oekom. This Open Access article is licensed under a Creative Commons Attribution 4.0 International License (CC BY).

https://doi.org/10.14512/tatup.33.1.28

Received: 20. 08. 2023; revised version accepted: 15. 12. 2023; published online: 15. 03. 2024 (peer review)

<!-- image -->

zahl von Anwendungen im öffentlichen Bereich eingesetzt, oft mit dem Versprechen,  genauer  und  objektiver  zu  sein  als  voreingenommene menschliche Entscheidungsträger*innen. Vor diesem Hintergrund diskutiert  dieser  Forschungsartikel den Einsatz von KI in Gerichten, insbesondere als Entscheidungshilfe für Richter*innen, und bewertet das Potenzial und die Grenzen dieser Instrumente hinsichtlich deren Einsatz bei der Risikobewertung. Darüber hinaus wird gezeigt, wie KI als Instrument zur Verringerung von Vorurteilen genutzt werden kann, d. h. um Muster der Voreingenommenheit bei gerichtlichen Entscheidungen aufzudecken und ihnen entgegenzuwirken. Abschließend werden die Mechanismen und Vorteile einer solchen Nutzung bewertet.

Keywords · judicial decision-making, judicial biases, artificial intelligence, risk assessment, debiasing

This article is part of the Special topic 'AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?,' edited by D. Schneider and K. Weber. https://doi.org/10.14512/tatup.33.1.08

## Introduction

Several  cognitive  and  social  psychology  studies  suggest  that judges are susceptible to various implicit biases which, unlike overt prejudice, they tend to be unaware of. These can influence their decisions in ways that are problematic considering the duty of impartiality and the right to a fair trial. The desire to increase objectivity, accuracy, and consistency in judicial decision-making has prompted the adoption of artificial intelligence (AI) to assist with different decision points throughout proceedings. At the same time, recognizing judges' susceptibility to biases raises the issue of how to mitigate them, and it is worth questioning whether AI might have a role to play in it. Hence, the goal of this article is twofold: First, it will describe how AI has been adopted for decision-support in judicial systems, and the challenges aris- https://doi.org/10.14512/tatup.33.1.28

<!-- image -->

<!-- image -->

ing from such use; and second, it will evaluate the possibility of using AI for helping to detect and counteract judges' implicit biases, recommending which extralegal factors should be considered when doing so. It is a theoretical and bibliographic research, drawing on direct and indirect sources for a comprehensive analysis of the theme. The article will proceed as follows: I will first provide an overview of how implicit biases affect judicial decision-making, consequently giving rise to arguments favoring the adoption of algorithms to promote more accurate and objective decisions. Subsequently, I will explore their current use in judicial settings, focusing on decision-support tools that are adopted to promote risk assessments. The implications of using automated procedures in the judicial process will be eval- such  as  criminal  history  and  past  pretrial  misconduct  (Arnold et al. 2020). In a virtual reality courtroom, minority defendants were treated more harshly by evaluators - including judges - during conviction (Bielen et al. 2021).

The idea that judicial decision-making can be influenced by extralegal factors is problematic considering judges' duty of impartiality and the right to a fair trial. Article 6 of the European Convention on Human Rights (ECHR) establishes the right to a fair trial by an independent and impartial tribunal. Impartiality  requires  that  judicial  decisions  are  based  on  the  objective circumstances of the case, in accordance with the law, and free from external influences. Moreover, it excludes the existence of

## Given the high stakes involved in judicial decision-making, the issue of how to mitigate judicial bias is important.

uated, with the desirability of their adoption being called into question. I will then offer a different possibility of AI use in the judiciary, namely, to help identify and counteract judicial bias, therefore increasing fairness and legal certainty, before drawing some conclusions.

## Biases in adjudication

There is ample scientific evidence demonstrating how judges like jurors and laypeople - are prone to both cognitive and social biases. While the former entails some broadly erroneous form of reasoning, the latter entails reasoning based on stereotypes (Zenker 2021). Biases have the potential to reduce the accuracy of a judgment and, throughout different stages of proceedings, can influence judicial decisions. To give some examples of relevant findings:

-  Judges' sentencing decisions and compensation awards were found not only to be anchored by the initial demand made by the prosecutor, but also by random and unrelated factors to the decision at hand (Bystranowski et al. 2021).
- In a criminal investigation scenario, irrelevant contextual information affected judges' conviction rate, and confirmation bias led them to prefer incriminating investigations (Rassin 2020). Similarly, the pretrial detention of defendants later influenced judges' assessments of their guilt in criminal cases (Lidén et al. 2019).
- Judges' decisions were biased by the gender of the parties in studies involving hypothetical cases about child custody and relocation,  employment  discrimination,  and  criminal  sentencing (Miller 2019; Rachlinski and Wistrich 2021).
- Data analysis of judges' bail decisions revealed racial bias against black defendants, even after controlling for variables

a prior disposition of the judge's mind that could lead them to favor or harm either party. The European Court of Human Rights (ECtHR) has distinguished between an objective aspect of this requirement, linked with the appearance of impartiality, and a subjective one, linked to 'the personal conviction and behavior of a particular judge, that is, whether the judge held any personal prejudice or bias in a given case' 1 . The existence of a subjective approach may lead one to believe that there is an effective remedy to fight against judges' implicit biases, but such remedy is truly limited. The ECtHR has recognized the hardship of establishing a breach of Article 6 on account of subjective partiality, given the difficulty to procure evidence with which to rebut the presumption of impartiality, and has thus justified its common recourse to the objective analysis. 2

One way to do so is through the implementation of debiasing techniques, which seek to address biases' negative effects by improving either the decision-making process or some relevant characteristics of the decision-maker (Zenker 2021). Another possibility relates to the adoption of artificial intelligence in judicial systems as decision-support or decision-making tools. ' Artificial intelligence' is used as an umbrella term to describe various human-designed technologies that exhibit intelligent behavior, analyzing their environment, and taking actions - with a certain level of autonomy - to achieve specific goals. The use of AI brings with it the promise of more accuracy, objectivity, and consistency, with governments increasingly adopting the technology 'to attain greater accuracy when making predictions, replace biased human decisions with 'objective' automated ones, and  promote  more  consistent  decision-making'  (Green  2022,

1 ECHR, Micallef v. Malta, Judgment of 15 October 2009, Application No. 17056/06, p. 22.

2 ECHR, Kyprianou v. Cyprus, Judgment of 15 December 2005, Application No. 73797/01.

p. 3). However, and at least for the time being, not only do these systems also have several limitations that can further deepen the problem of bias in adjudication, but there is also a risk-magnifying potential associated with AI that is not present with human decision-making (Dietterich 2019). In the following session, I will examine how AI has been adopted in judicial systems, specifically as decision-aid tools for assessing risk, and the challenges posed by this use.

## AI in judicial systems

To assist adjudication, several countries are experimenting with and  integrating  digital  technologies,  particularly  AI,  in  their judicial  systems.  Applications  like  advanced  case-law  search engines, online dispute resolution, or document categorization and screening can potentially lower the cost of dispute resolution  and  help  courts  address  their  backlog  of  cases,  many  of which  are  low-volume,  low-value,  and  low-complexity  matters  (Steponenaite  and  Valcke  2020).  Furthermore,  some  evidence suggests that algorithms are better at making policy-relevant predictions than public servants (Kleinberg et al. 2018). This makes the prospects of adopting digital technologies in judicial systems, particularly supportive and advisory AI-based tools, quite promising. On the other side, the use of algorithms to make consequential decisions about the application of public policy to individuals in street-level bureaucracies like courts, police  departments,  and  welfare  agencies  has  been  highly controversial (Angwin et al. 2016; Heaven 2020; Allhutter et al. 2020).

Considering this scenario, institutions such as the European Union (EU) and the Council of Europe (CoE) are working towards ensuring that the development, implementation, and use of AI is done in an ethical and lawful way, especially in contexts where there is a high impact on individuals' fundamental rights. While the EU is in the final stages of approving a regulation creating standardized rules for AI (European Commission  2021),  the  CoE's  Commission  for  the  Efficiency  of  Justice  adopted the first European Ethical Charter on the use of AI in  judicial  systems  (CEPEJ  2018).  In  it,  the  Commission, which is responsible for evaluating European judicial systems and defining concrete ways to improve their performance, provides  an  ethical  framework  to  guide  private  and  public  stakeholders throughout the development and implementation of AI in the judiciary. Continuing this work, in April 2023 CEPEJ also launched a Resource Centre on Cyberjustice and AI, which aims to  serve  as  a  publicly  accessible  focal  point  for  reliable  information on AI systems applied in the transformation of judicial systems (CEPEJ 2023). One of its first endeavors was to obtain an overview of these systems, providing a starting point for further examination of their risks and benefits for professionals and end-users. A total of 58 systems were identified in CoE member states, and then classified according to their main field of application. Categories include, e.g., 'anonymization tools', which are used for removing identifying information of court users, and 'natural language processing tools', used for speech recognition and the automatic transcription of court procedures. My analysis here will focus, however, on the category of 'decision-support and decision-making', which encompasses tools meant to facilitate or fully automate decision-making processes in justice systems, considering that some think that highly accurate AI systems could improve the performance of judges, or even come to replace them (Chatziathanasiou 2022).

First, it is worth highlighting that the use of the tools mapped by CEPEJ by judges themselves is still quite limited, and the initiative for their development remains primarily within the private sector, focusing on insurance companies, lawyers, and legal services wishing to reduce the uncertainty and unpredictability of judicial decisions. The French application Predictice, for example, is a predictive justice tool developed to calculate the chances of success of a legal action according to different variables, using jurisprudence analysis algorithms. More recently, it has launched a generative AI tool called Assistant, developed to answer legal professionals' questions by citing reliable and upto-date sources (Larret-Chahine 2023). Even though most applications of this kind have their use currently restricted to private agents, 'public decision-makers are beginning to be increasingly solicited by a private sector wishing to see these tools […] integrated into public policies' (CEPEJ 2018, p. 14).

Second, not all the applications listed at the Resource Centre can technically be categorized as AI, as is the case for many of the risk assessment tools, mainly used for assessing the risk of recidivism. These make up for the majority of 'decision-support and decision-making' tools that have been listed by CEPEJ as being currently used in the public sector, namely by judges. Examples  include  OASys,  the  Offender  Assessment  System used by the prison and probation services in England and Wales (  Justice Data Lab 2016), RITA, the Finish Risk and Needs Assessment  Form  (Salo  et  al.  2019),  or  RISC,  the  Recidivism Assessment Scale adopted in the Netherlands (van Essen et al. n.d.). Risk assessments are perceived and often marketed as an objective  means  of  overcoming  human  bias  in  decision-making and have been adopted to assist with several decision points throughout the criminal justice system, from pretrial release to post-conviction sentencing, probation, and parole. These tools do not use new statistical methods commonly associated with AI, such as machine learning (ML), but are rather overwhelmingly  based  on  regression  models  (Barabas  et  al.  2018).  The main goal of regression is to identify a set  of  variables  (e.g., prior  arrest)  that  are  predictive  of  a  given  outcome  variable (e.g., risk of reoffending). This process can be automatized and improved using ML methods (Ghasemi et al. 2021), but still constitute  an  incremental  innovation  in  the  way  risk  assessments have historically worked, instead of being truly transformational.

One example of a risk assessment tool that incorporates a machine learning approach is the Correctional Offender Management Profiling for Alternative Sanctions, or COMPAS, used

by  some  United  States'  courts  to  assess  the  likelihood  of  recidivism (van Dijck 2022). Ever since an exposé by the news outlet ProPublica revealed that the software was biased against blacks (Angwin et al. 2016), it has become the primary example of the risks posed by algorithmic crime prediction overall. The controversy surrounding its use revolves around what the models measure and intend to measure, the accuracy of the predictions, and whether they might increase inequality and discrimination or otherwise compromise fairness (Mayson 2019; Rudin et al. 2020). The question of (un)fairness is of particular concern, given that risk assessment tools can lead to discriminatory outcomes based on race or ethnicity (Jordan and Bowman 2022). Furthermore, since the software is proprietary, the data and algorithms are not transparent, neither for the suspect nor for the judge (van Dijck 2022), a problem that was addressed in the case of Loomis v. Wisconsin 3 (2016). In this case, while admitting the system's flaws, the Court claimed that it is up to judges to exercise discretion when assessing a risk score.

In high-stakes decisions such as criminal justice risk assessments, it is common to place emphasis on the decision-makers' discretion  in  incorporating  algorithmic  advice  into  their  decisions, to make their use acceptable even in light of flaws. However, human discretion does not necessarily improve outcomes. Decision-makers are susceptible to automation bias, a tendency to defer to automated systems, reducing the amount of independent scrutiny exhibited when deciding (Parasuraman and Manzey 2010). Similar issues arise when humans collaborate with predictive algorithms. Recent research has found that people are bad at judging the quality of algorithmic outputs and determining whether and how to override those outputs (Green 2022). In simulated pretrial and sentencing decisions, for instance, risk assessments made participants - including judges - place a greater emphasis on its results  than  on  other  relevant  factors  (Green and Chen 2021). It is thus likely that, instead of improving the issue of bias by promoting an 'objective' score, the incorporation of results into a decision may nonetheless result in biased outcomes.

The challenges discussed in this section do not necessarily entail a categorical rejection of employing AI for assisting in judicial decisions, but it does raise the question of which uses might be advantageous without presenting a risk to the fairness of a trial. Here, one possibility relates to its use for triaging, allocation, and workflow automation, facilitating some activities during  the  lifecycle  of  proceedings  and  minimizing  the  need for  human input. For instance, one Higher Regional Court in Germany began using AI to help process the large volume of lawsuits relating to a scandal involving vehicle manufacturers. With more than 13,000 cases pending and around 600 entries added monthly, the AI is used to analyze the files and organize them according to the facts, but the judges remain responsible for processing the content, reviewing it, and making decisions (SWR 2022).

3 Wisconsin Supreme Court, Wisconsin v. Loomis 2016 WI 68.

## AI as a debiasing tool

After examining how AI has been used for decision-support in judicial settings, I will now explore the possibility of applying AI methods for helping detect and counteract judges' implicit biases. One way of doing so is through predictive judicial analytics in the form of machine learning. As Chen (2019 a) explains, biases are most likely to manifest in situations where judges are closer to indifference between options. These contexts of 'judicial indifference' are also ones where the highest levels of disparities  in  inter-judge  accuracy  are  present  -  essentially  conditions where judges are unmoved by legally relevant circumstances. A judge could be said to have strong preferences over legally relevant circumstances (which are covariates) when it is costly to depart from the legally optimal outcome, defined as the outcome that would be generated through consideration of legal facts alone. But a judge may also have weak preferences over legally relevant circumstances, meaning that there is a relatively low cost in departing from the legally optimal outcome. In such cases of legal indifference, a different set of covariates that are legally irrelevant (and thus should not predict a legal outcome) can be expected to have greater influence. In other words:

'If a judge can be predicted prior to observing the case facts, one might worry about the use of snap or pre-determined judgements, or judicial indifference. To put it differently, the preferences of judges over the legally relevant covariates may affect the influence of irrelevant features. A judge could be said to have weak preferences, meaning that there was a relatively low cost in departing from the legally   optimal outcome. In such cases of legal indifference, irrelevant factors can be expected to have greater influence. Behavioral bias reveals when decision-makers are indifferent' (Chen 2019 b, p. 16).

The accuracy of predictions depends on the number of judicial attributes or characteristics of the case that are analyzed by the system. In a study conducted by Dunn et al. (2017) on asylum courts, using only data available at the case opening (i. e., information on the judge's identity and the nationality of the asylum seeker), researchers were able to predict case outcomes with an accuracy of 78 %. Through this notion of 'early predictability', ML could be used to automatically detect judicial indifference, alerting  to  situations  where  extralegal  factors  are  more  likely to influence a decision. This raises the question of which other sets of data could be incorporated into the legally irrelevant covariates to improve predictive accuracy, requiring an analysis of which are the main extralegal factors that influence judges when deciding. Based on an examination of the literature on social biases, initial contenders include race, gender, and ethnicity of the defendant and of the judge, the latter for the assessment of ingroup favoritism. And based on findings on cognitive biases, the number of (un)favorable previous decisions by the court, the comparison of caseloads between judges, and whether it is a spe-

cialized court (all being indicative of contrast effects) are variables likely to influence the way judges decide. Other factors besides biases include the time of day at which a decision is made (Shroff and Vamvourellis 2022; Danziger et al. 2011), and temperature (Chen and Loecher 2019; Heyes and Saberian 2019). These are of course only initial suggestions and do not fully encompass all the irrelevant covariates that can affect a decision. But the advantage of using machine learning techniques for this purpose is precisely that any sort of data can be used to feed the model, enabling patterns and trends to emerge without necessarily requiring a theoretical explanation for such.

In the in-depth study on the use of AI in judicial systems that accompanies CEPEJ's ethical charter (2018), the commission analyzes the benefits and risks of different applications, encouraging their use to various degrees. While specific judge profiling is highly discouraged, among uses to be considered following additional studies is offering judges an assessment of their activities with an informative aim of assisting in decision-making. Indeed, offering judges' feedback regarding their decisions is  a  fundamental  step  in  debiasing,  alongside  other  interventions such as the promotion of general bias awareness, training in  rules  and  representations,  exposure  to  stereotype-incongruent models, and the adoption of scripts and checklists (Wistrich and Rachlinski 2017) - all of which can be directed once bias is identified. Furthermore, AI offers a mechanism of detecting bias in real time, and could hence alert judges to situations where biases are likely to occur (e.g., after a string of positive decisions), allowing them to intervene before a biased decision takes place.

## Concluding remarks

The adoption of digital technologies like artificial intelligence in judicial settings often comes from a desire to increase objectivity, accuracy, and consistency in decision-making, improving the quality of decisions traditionally made by humans. However, we have seen how the use of AI for decision-support in adjudication, albeit still not prevalent in CoE member states, can worsen issues already identified in the risk assessment tools that they seek to automate. These include the accuracy (or lack thereof) of their predictions, the reproduction of existing patterns of prejudice and bias, the lack of transparency and opportunities for defendants to challenge their outcomes, and the difficulty of decision-makers to properly evaluate assessments. Thus, instead of using AI to make decisions that traditionally pertain to judges (e.g., assessing the risk of recidivism), a different possibility was offered,  namely,  to  employ  the  technology  for  debiasing  purposes. AI can help identify the situations where judicial bias is likely to take place, based on the analysis of covariates that, despite being legally irrelevant, have been shown to influence judicial decisions, some of which were listed here. By identifying the instances in which bias commonly arises, it is possible not only to alert judges but also to target debiasing interventions, such as educating judges on the subject and offering feedback on their work, with the ultimate goal of ensuring objectivity and impartiality in their decisions.

Funding · This work received no external funding. Competing interests · The author declares no competing interests.

## References

Allhutter, Doris; Cech, Florian; Fischer, Fabian; Grill, Gabriel; Mager, Astrid (2020): Algorithmic profiling of job seekers in Austria. How austerity politics are made effective. In: Frontiers in Big Data 3 (5), pp. 1-17. https://doi.org/10.3389/ fdata.2020.00005

Angwin, Julia; Larson, Jeff; Mattu, Surya; Kirchner, Lauren (2016): Machine bias. There's software used across the country to predict future criminals. And it's biased against blacks. In: ProPublica, 23. 05. 2016. Available online at https:// www.propublica.org/article/machine-bias-risk-assessments-in-criminalsentencing, last accessed on 22. 01. 2024.

Arnold, David; Dobbie, Will; Hull, Peter (2020): Measuring racial   discrimination in bail decisions. In: NBER Working Paper Series, pp. 1-84. https://doi.org/ 10.3386/w26999

Barabas, Chelsea; Virza, Madars; Dinakar, Karthik; Ito, Joichi; Zittrain, Jonathan (2018): Interventions over predictions. Reframing the ethical debate for actuarial risk assessment. In: Proceedings of the 1st Conference on Fairness, Accountability and Transparency, PMLR 81, pp. 62-76. Available online at https:// proceedings.mlr.press/v81/barabas18a.html, last accessed on 22. 01. 2024 Bielen, Samantha; Marneffe, Wim; Mocan, Naci (2021): Racial bias and in-group bias in virtual reality courtrooms. In: The Journal of Law and Economics 64 (2), pp. 269-300. https://doi.org/10.1086/712421

Bystranowski, Piotr; Janik, Bartosz; Próchnicki, Maciej; Skórska, Paulina (2021): Anchoring effect in legal decision-making. A meta-analysis. In: Law and Human Behavior 45 (1), pp. 1-23. https://doi.org/10.1037/lhb0000438 Chatziathanasiou, Konstantin (2022): Beware the lure of narratives. 'Hungry Judges' should not motivate the use of 'Artificial Intelligence' in law. In: German Law Journal 23 (4), pp. 452-464. https://doi.org/10.1017/glj.2022.32 Chen, Daniel (2019 a): Machine learning and the rule of law. In: Michael Livermore and Daniel Rockmore (eds.): Law as Data. Santa Fe, NM: SFI Press, pp. 433-441. Chen, Daniel (2019 b): Judicial analytics and the great transformation of American law. In: Artificial Intelligence and Law 27 (1), pp. 15-42. https://doi.org/10.1007/ s10506-018-9237-x

Chen, Daniel; Loecher, Markus (2019): Mood and the malleability of moral reasoning. In: SSRN Electronic Journal, pp. 1-62. https://dx.doi.org/10.2139/ ssrn.2740485

Danziger, Shai; Levav, Jonathan; Avnaim-Pesso, Liora (2011): Extraneous factors in judicial decisions. In: Proceedings of the National Academy of Sciences 108 (17), pp. 6889-6892. https://doi.org/10.1073/pnas.1018033108 Dietterich, Thomas (2019): Robust artificial intelligence and robust human organizations. In: Frontiers of Computer Science 13 (1), pp. 1-3. https://doi. org/  10.1007/s11704-018-8900-4

Dunn, Matt; Sagun, Levent; Şirin, Hale; Chen, Daniel (2017 a): Early predictability of asylum court decisions. In: ICAIL '17. Proceedings of the 16th edition of the International Conference on Artificial Intelligence and Law. New York, NY: Association for Computing Machinery, pp. 233-236. https:// doi.org/10.1145/3086512.3086537

European Commission (2021): Proposal for a regulation of the European Parliament and the Council laying down harmonised rules on artificial

intelligence (Artificial Intelligence Act) and amending certain union legislative acts.   Brussels: European Commission. Available online at https://eur-lex. europa.eu/legal-content/EN/TXT/?uri=celex:52021PC0206, last accessed on 22. 01. 2024.

CEPEJ - European Commission for the Efficiency of Justice (2018): European ethical charter on the use of artificial intelligence in judicial systems and their environment. Strasbourg: Council of Europe. Available online at https:// rm.coe.int/ethical-charter-en-for-publication-4-december-2018/16808f699c, last accessed on 22. 01. 2024.

CEPEJ (2023): Resource centre on cyberjustice and AI. Available online at https:// public.tableau.com/app/profile/cepej/viz/ResourceCentreCyberjusticeandAI/ AITOOLSINITIATIVESREPORT, last accessed on 22. 01. 2024.

Ghasemi, Mehdi; Anvari, Daniel; Atapour; Mahshid; Wormith, Stephen;   Stockdale, Keira; Spiteri, Raymond (2021): The application of machine learning to a general risk-need assessment instrument in the prediction of   criminal recidivism. In: Criminal Justice and Behavior 48 (4), pp. 518-538. https:// doi.org/  10.1177/0093854820969753

Green, Ben; Chen, Yiling (2021): Algorithmic risk assessments can alter human decision-making processes in high-stakes government contexts. In: Proceedings of the ACM on Human-Computer Interaction 5 (CSCW2). New York, NY: Association for Computing Machinery, pp. 1-33. https://doi.org/10.1145/3479562

Green, Ben (2022): The flaws of policies requiring human oversight of   government algorithms. In: Computer Law &amp; Security Review 45, pp. 1-22. https://doi.org/ 10.1016/j.clsr.2022.105681

Heaven, Will (2020): Predictive policing algorithms are racist. In: MIT Technology Review, 17. 07. 2020. Available online at https://www.technologyreview. com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantledmachine-learning-bias-criminal-justice/, last accessed on 10. 01. 2024.

- Heyes, Anthony; Saberian, Soodeh (2019): Temperature and decisions. In:   American Economic Journal 11 (2), pp. 238-265. https://doi.org/10.1257/

app.20170223

Kleinberg, Jon; Lakkaraju, Himabindu; Leskovec, Jure; Ludwig, Jens; Mullainathan, Sendhil (2018): Human decisions and machine predictions. In: The Quarterly Journal of Economics 133 (1), pp. 237-293. https://doi.org/10.1093/qje/qjx032

Larret-Chahine, Louis (2023): Predictice lance assistant, une IA générative pour les professionnels du droit. In: Predictice Blog, 26. 05. 2023. Available online at https://blog.predictice.com/assistant-ia-pour-les-professionnels-du-droit, last accessed on 10. 01. 2024.

Jordan, Kareem; Bowman, Rachel (2022): Interacting race/ethnicity and   legal factors on sentencing decisions. A test of the liberation hypothesis. In: Corrections 7 (2), pp. 87-106. https://doi.org/10.1080/23774657.2020.1726839

Lidén, Moa; Gräns, Minna; Juslin, Peter (2019): 'Guilty, no doubt'. Detention provoking confirmation bias in judges' guilt assessments and debiasing techniques. In: Psychology, Crime &amp; Law 25 (3), pp. 219-247. https://doi.org/10.1080 /1068316X.2018.1511790

Mayson, Sandra (2019): Bias in, bias out. In: The Yale Law Journal 128 (8), pp. 2218-2300. Available online athttps://www.yalelawjournal.org/article/ bias-in-bias-out, last accessed on 10. 01. 2024.

Miller, Andrea (2019): Expertise fails to attenuate gendered biases in judicial decision making. In: Social Psychological and Personality Science 10 (2), pp. 227-234. https://doi.org/10.1177/1948550617741181

Parasuraman, Raja; Manzey, Dietrich (2010): Complacency and bias in   human use of automation. In: Human Factors 52 (3), pp. 381-410. https://doi.org/ 10.1177/0018720810376055

Rachlinski, Jeffrey; Wistrich, Andrew (2021): Benevolent sexism in judges. In: San Diego Law Review 58 (1), pp. 101-142. Available online at https://digital. sandiego.edu/sdlr/vol58/iss1/3, last accessed on 22. 01. 2024.

Rassin, Eric (2020): Context effect and confirmation bias in criminal fact   finding. In: Legal and Criminological Psychology 25 (2), pp. 80-89. https://doi.org/ 10.1111/lcrp.12172

Salo, Benny; Laaksonen, Toni; Santtila, Pekka (2019): Predictive power of dynamic (vs. static) risk factors in the Finnish risk and needs   assessment form. In: Criminal Justice and Behavior 46 (7), pp. 939-960. https://doi.org/ 10.1177/0093854819848793

Steponenaite, Vilte; Valcke, Peggy (2020): Judicial analytics on trial. An assessment of legal analytics in judicial systems in light of the right to a fair trial. In: Maastricht Journal of European and Comparative Law 27 (6), pp. 759-773. https://doi.org/10.1177/1023263X20981472

Shroff, Ravi; Vamvourellis, Konstantinos (2022): Pretrial release judgments and decision fatigue. In: Judgment and Decision Making 17 (6), pp. 1176-120. https://doi.org/10.1017/S1930297500009384

Rudin, Cynthia; Wang, Caroline; Coker, Beau (2020): The age of secrecy and unfairness in recidivism prediction. In: Harvard Data Science Review 2 (1), pp. 1-53 https://doi.10.1162/99608f92.6ed64b30

SWR (2022): OLG Stuttgart setzt KI bei Diesel-Klagen ein. In: SWR   Aktuell, 24. 10. 2022. Available online at https://www.swr.de/swraktuell/ baden-wuerttemberg/stuttgart/olg-stuttgart-mit-ki-gegen-flut-vondieselklagen-100.html, last accessed on 10. 01. 2024.

Justice Data Lab (2016): Incorporating offender assessment data to the justice data lab process. London: Ministry of Justice. Available online at https:// assets.publishing.service.gov.uk/government/uploads/system/uploads/ attachment\_data/file/491688/oasys-methodology.pdf, last accessed on 22. 01. 2024.

Van Essen, Laurus; Van Alphen, Huib; Van Tuinen, Jan-Maarten (n.d.): Risk assessment the Dutch way. A scalable, easy to use tool for probation reports. In: Confederation of European Probation News. Available online at https://www. cep-probation.org/risk-assessment-the-dutch-way-a-scalable-easy-to-usetool-for-probation-reports/, last accessed on 22. 01. 2024.

Van Dijck, Gijs (2022): Predicting recidivism risk meets AI act. In: European Journal on Criminal Policy and Research 28 (3), pp. 407-423. https://doi.org/10.1007/ s10610-022-09516-8

Wistrich, Andrew; Rachlinski, Jeffrey (2017): Implicit bias in judicial decision making. How it affects judgement and what judges can do about it. In:   Sarah Redfield (ed.): Enhancing justice: Reducing bias, pp. 87-130. https://doi.org/ 10.31228/osf.io/sz5ma

Zenker, Frank (2021): De-biasing legal factfinders. In: Christian Dahlman, Alex Stein and Giovanni Tuzet (eds.): Philosophical foundations of evidence law. Oxford: Oxford University Press, pp. 395-410. https://doi.org/10.1093/ oso/9780198859307.003.0027

<!-- image -->

## GIOVANA LOPES

is a doctoral candidate in 'Law, Science and Technology' at the University of Bologna and KU Leuven and an affiliated research fellow at the Centre for IT &amp; IP Law (CiTiP).

<!-- image -->

## RESEARCH ARTICLE

## Borderline decisions? : Lack of justification for automatic deception detection at EU borders

<!-- image -->

Daniel Minkin  * , 1  , Lou Therese Brandner 2 

<!-- image -->

Abstract · Between 2016 and 2019, the European Union funded the development and testing of a system called 'iBorderCtrl', which aims to help detect illegal migration. Part of iBorderCtrl is an automatic deception detection system (ADDS): Using artificial intelligence, ADDS is designed to calculate the probability of deception by analyzing subtle facial expressions to support the decision-making of border guards. This text explains the operating principle of ADDS and its theoretical foundations. Against this background, possible deficits in the justification of the use of this system are pointed out. Finally, based on empirical findings, potential societal ramifications of an unjustified use of ADDS are discussed.

Grenzwertige Entscheidungen? : Rechtfertigungsdefizite der automatischen Täuschungserkennung an EU-Grenzen

Zusammenfassung · Von 2016 bis 2019 wurde mit Fördergeldern der Europäischen Union ein System namens 'iBorderCtrl' entwickelt und getestet. Dieses System soll dabei helfen, illegale Migration zu erkennen.  Eine  Komponente von iBorderCtrl ist das sog. Automatic-Deception-Detection-System (ADDS). Mithilfe künstlicher Intelligenz soll das ADDS subtile Gesichtsausdrücke analysieren, um die Wahrscheinlichkeit  einer  Täuschung zu berechnen, die Grenzschutzbeamt*innen als Entscheidungshilfe  nutzen  können.  Im  vorliegenden  Beitrag  werden das Funktionsprinzip des ADDS sowie seine theoretische Basis erläutert. Vor diesem Hintergrund wird auf mögliche Defizite in der Rechtfer-

* Corresponding author: hpcdmink@hlrs.de

1 High-Performance Computing Center Stuttgart, University of Stuttgart, Stuttgart, DE

2 International Center for Ethics in the Sciences and Humanities, University of Tübingen, Tübingen, DE

<!-- image -->

© 2024 by the authors; licensee oekom. This Open Access article is licensed under a Creative Commons Attribution 4.0 International License (CC BY). https://doi.org/10.14512/tatup.33.1.34

Received: 22. 08. 2023; revised version accepted: 03. 01. 2024;

published online: 15. 03. 2024 (peer review)

tigung des Einsatzes von ADDS hingewiesen. Auf der Grundlage empirischer Untersuchungen werden schließlich mögliche soziale Auswirkungen eines ungerechtfertigten Einsatzes von ADDS diskutiert.

Keywords · automatic deception detection, machine learning, emotion recognition, border control, trust

This article is part of the Special topic 'AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?,' edited by D. Schneider and K. Weber. https://doi.org/10.14512/tatup.33.1.08

## Introduction

The potential of artificial intelligence (AI) to revolutionize border management has been recognized for some time (Beduschi 2020).  In  the  European  Union  (EU),  different  AI-based  technologies for border control are either already in use or are being tested for future deployment, such as biometric identification and verification, risk assessment, or emotion detection (Dumbrava 2021). This contribution focuses on a subset of the latter: The so-called  automatic  deception  detection  system  (ADDS), part of the iBorderCtrl project funded by the EU, was developed to detect cases of illegal border crossing by video-interviewing travelers to analyze their facial microexpressions for indicators of deceit. The technology is intended to support border guards in their decision-making process, providing recommendations in the form of risk assessments regarding individual travelers.

Who gets to cross European borders, an already complex social issue with major implications for migrants and society at large, thus becomes embedded in discourses around AI-based decision support. These discourses pertain not only to one state or one scientific discipline. A responsible use of AI-based systems at border crossing points requires responsible policy-making based on an interdisciplinary and transnational perspective.

https://doi.org/10.14512/tatup.33.1.34

<!-- image -->

<!-- image -->

Against this background, in this paper, we bring together epistemological, technological, and social science arguments in order to contribute to an informed assessment of the technology.

After giving a more in-depth description of ADDS, its purpose  and  theoretical  basis,  we  focus  on  two  interlinked  questions:

1. What are the concerns about the use of ADDS and are they warranted? We examine epistemological and empirical arguments against the deployment of ADDS. The first group of arguments targets the system's theoretical foundation, contending that ADDS rests on a scientifically unfounded basis. The second group attempts to show that the mechanisms underlying ADDS are not sufficiently accurate.
2. Having discussed the above concerns, we turn to the second question: Given the concerns outlined, what would be the social implications of using ADDS in terms of public trust? The concept of public trust is related to various dimensions of technology assessment (TA) such as trustworthy technology, acceptance of new technologies, as well as the promotion and maintenance of trust in technology-related policy-making (Weydner-Volkmann 2021). We address these aspects at the end of the paper, arguing in favor of more transparency in the implementation of systems such as ADDS.

## The automatic deception detection system

## Purpose and working principle

ADDS is a machine learning (ML) based system designed to identify deception 1 when crossing a state border (O'Shea et al. 2018; Podoletz 2023). It has been dubbed an 'AI Polygraph' or 'AI lie detector' (Kaminski 2019, p. 178). ADDS is part of iBorderCtrl, which is designed to facilitate and accelerate the registration and control of travelers coming to the EU, including refugees. It has already been tested in Hungary, Latvia, and Greece, with the test phase ending in 2019. Currently, it is not known whether and in what form iBorderCtrl will be deployed; however, with the EU extensively testing this kind of technology and funding several related border control and surveillance projects (iBorderCtrl 2023), critically examining these systems remains relevant.

iBorderCtrl works in the two stages of pre-registration and border crossing. It includes procedures such as biometric identification,  document  matching,  risk  analysis,  and  ADDS,  on which  this  contribution  focuses.  During  pre-registration,  trav-

1 Both in the description of ADDS by iBorderCtrl and in research articles examining the accuracy of the system, the concept of deception is not characterized in detail (Rothwell et al. 2006, iBorderCtrl 2018, O'Shea et al. 2018). These texts appear to use the term 'deception' synonymously with 'lying' (O'Shea 2018, p. 4). O'Shea et al. tested the system using simulated scenarios in which the subjects had the task of smuggling various illegal substances such as drugs or infectious materials.

elers  undergo  an  online  video  interview  with  a  police  avatar. ADDS analyzes the recorded interviews, more specifically combinations of the travelers' microexpressions, very subtle facial expressions, normally invisible to the naked eye, to quantify the probability of deceit. O'Shea et al. (2018, p. 3) point out a difference between microgestures and microexpressions: The former are 'more fine-grained and require no functional psychological model of why the behaviour has taken place'. However, since the original description by Rothwell et al. (2006, p. 759) uses the term 'microexpression', we follow this publication. Based on the microexpression analysis and other components of the iBorderCtrl system, a risk estimation regarding the traveler is provided. The system is intended as a human-in-the-loop system: When travelers attempt to cross the border, a border guard makes the final decision after performing a security check against the background of the data provided by the system.

The main component of ADDS is a subsystem called 'Silent Talker' (ST) (iBorderCtrl 2018, p. 15). By using several artificial neural networks, ST learns to recognize combinations of microexpressions (by means of supervised or unsupervised methods). The actual classification of these combinations as truthful or deceptive is based on a conceptual model of non-verbal behavior (NVB): 'This model assumes that certain mental states associated with deceptive behavior will drive an interviewee's NVB when deceiving. These include Stress or Anxiety (factors in psychological Arousal), Cognitive Load, Behavioral Control and Duping Delight' (O'Shea 2018, pp. 3-4). It is worth noting that a combination's classification as deceptive cannot be regarded as proof of deception but as a probabilistic result obtained by inductive learning.

The assumption that microexpressions are capable of revealing deceptive intentions thus forms the theoretical basis of ST, which will be further discussed in more detail in the next chapter.

## Theoretical foundation

Deception consists of or involves mental states, especially intentions, while microexpressions are a kind of behavior open to intersubjective investigation. ST and ADDS analyze the latter and thereby provide a basis for human actors to obtain information about the former. Against this backdrop, the question arises as to which possible combinations of microexpressions can indicate an intention to deceive on the part of the subject. In other words, the development of deception detection systems requires a psychological theory about the connections between microexpressions and mental states. In their description of the ST, the inventors explicitly state that they take some key elements from the psychological work of Paul Ekman, such as Ekman's definition of microexpression (Rothwell et al. 2006, p. 759).

Ekman became famous for his cross-cultural studies of emotional expression. In the 1960s, for instance, he asked indigenous people in New Guinea, at that time largely isolated from the Western world, to assign terms like 'sad' to pictures of Europeans  expressing  different  emotions.  Experiments  with  various  cultures  as  well  as  later  studies  (Elfenbein  and  Ambady

2002) resulted in relatively high accuracy rates, leading to the assumption of universal emotional expression. Regarding universal microexpressions, Ekman analyzed video recordings of proven liars frame by frame, finding that subjects with deceptive intentions could not consciously control all facial muscle movements while experiencing a particular emotion (Ekman 1985, p. 133).

Ekman's theory indicates, therefore, that some unconscious and uncontrollable facial expressions can provide evidence of deception.  However,  this  idea  is  controversial;  in  the  coming chapter,  we  will  explore  criticisms  of  microexpression  analysis and ADDS.

## Criticism

Deception  detection  has  been  criticized  as  'pseudoscience' (Whittaker et al. 2018). Against the background of such normative characterizations, we want to assess if the use of ADDS is justified, focusing on the system's theoretical background and its accuracy; the first aspect is the subject of an epistemological criticism, the second of an empirical one.

## Epistemological criticism

The psychological foundation of ST and ADDS has been criticized as flawed, which, according to the systems' opponents, means their use cannot be justified. A major part of this criticism is an observed lack of scientific consensus on the assumption that deceptive intentions can be derived from microexpressions. In general terms, the epistemological criticism states that the  use  of  deception  detection  systems  is  not  justified  unless there is widespread agreement on their theoretical foundations (Podoletz 2023, p. 1071). Given that Ekman assumes one can derive deceptive intentions from microexpressions, it is reason- fixed states (Whittaker et al. 2018, p. 14). Although studies report some advancement in the field (Varghese et al. 2015), it has for instance been argued that emotional expressions are contingent on cultural and social factors (Feldman Barrett et al. 2019), which fundamentally challenges the basis of Ekman's theory. Lastly, there is no consensus regarding the appropriate classification of emotions; Ekman himself conducted a survey among experts in psychology of emotion to identify the preferred classification model in the field. His results suggest that only 16 % of experts favor the model his theory implies (Ekman 2016, p. 32). The theoretical basis of ST thus seems to be approved only by a minority.

While this line of thought appears convincing, there are some limitations. To start with, although it is unclear how much disagreement is too much, it seems uncontroversial that perfect consensus is not necessary for justifying the use of a system. On the other hand, even if there were a perfect consensus on the theory as a basis of a system, the use of this system would not be justified without a sufficiently high level of accuracy. This indicates that a lack of consensus on a system's theoretical basis does not necessarily impact the justification of its use; it can be argued that it is irrelevant whether ST is based on a controversial theoretical basis as long as it is able to distinguish deceptive statements from truthful ones with sufficient accuracy. In the third part of this paper, we will revisit the relevance of the theoretical foundation.

## Empirical criticism

The empirical criticism of ADDS considers the accuracy of such systems (Sánchez-Monedero and Dencik 2022). As part of iBorderCtrl, ADDS is intended to support the decision-making of border guards. The use of such subsidiary systems, particularly in  high-risk  application contexts like border control, is argua-

## Ekman's theory indicates that some unconscious and uncontrollable facial expressions can provide evidence of deception.

able to ask whether there is disagreement among experts about his theory. By the term 'disagreement', we mean the incompatibility of other psychological theories with Ekman's position.

A  literature  review  shows  that,  indeed,  there  are  disagreements  on  different  aspects  and  at  different  levels  of  abstraction: First, the interpretation of microexpressions as indicators of deceit is not conclusive. Psychologists have made other reasonable assumptions on what microexpressions might indicate (Zhang and Arandjelović 2021). This disagreement touches the very  core  of  ST's  theoretical  foundation,  for  if  microexpressions are not indicative of deceit but of suppressed emotions, the system does not measure what it is supposed to. Second, at a deeper level, we find psychological disagreement on whether facial analysis can provide a universal reading of emotions as bly only justified if their accuracy rates exceed those of human experts. Since human actors are expected to make the final decision,  the  systems  would  otherwise  have  no  added  value;  in the worst case, they would direct human experts towards wrong decisions due to the perceived authority of automated outputs (Helm and Hagendorff 2021). 2

The concern is that in the case of ST this condition is not met: Its accuracy rate has been reported to be 63 to 70 % (Rothwell et  al.  2006)  and  74.6  %  (O'Shea  et  al.  2018).  Empirical  find-

2 The studies and texts describing ST do not address such risks (Rothwell et al. 2006, O'Shea et al. 2018, iBorderCtrl 2018) but focus on advantages (e.g. compared to conventional polygraphs) and limitations from a technological perspective.

ings suggest that while human performance is below these values for untrained subjects, ST's performance does not exceed human  experts.  According  to  a  meta-analysis,  subjects  without  special  training  perform  only  slightly  better  than  chance (54 %) when attempting to distinguish deceptive from truthful statements (Bond and DePaulo 2006). However, the mean accuracy of trained parole officers has been found to be 76.7 % (Porter et al. 2000) and thus beyond the highest values found for ST.

A second empirical problem is that ST was trained and tested on a surprisingly small number of subjects under controlled experimental conditions. Rothwell et al. worked with 39 subjects and O'Shea et al. with 30, with most of them being male Europeans. Since the aim of this system is to control non-EU citizens, Rothwell et al. openly admit this could lead to data bias (Rothwell 2006, p. 768): Underrepresenting certain groups - such as people of color or women - in AI training datasets can lead to unreliable assessments regarding individuals belonging to these groups and therefore to discriminatory outcomes (Brandner and Hirsbrunner 2023; Selbst 2017). The results of the real-life tests in Hungary, Latvia, and Greece have not been publicly disclosed, and it is thus so far unclear whether they show biases.

These concerns are independent of the epistemological criticism since they relate solely to the accuracy of the system in question.  Thus,  even  ignoring  potential  flaws  in  the  theoretical foundations, there seems to be no sufficient justification for deploying ADDS. Taking into account the addressed empirical criticism, the next chapter assesses the potential social implications of ADDS. To do so, we build on empirical studies on public attitudes toward the police and AI-based technology.

## Social implications

## Trust in automated deception detection

AI-based policing and border control evoke divergent public attitudes.  For  controversial  real-time  facial  recognition  technology, acceptance appears to depend on the general trust invested in the police, with a nearly fifty/fifty split regarding the question if the police should be able to use this technology (Bradford et al. 2020). While trust is a much debated, complex notion, someone trusting in an institution like the police can be described as 'having confidence that the institution is reliable, observes rules and regulations, works well, and serves the general interest' (Devos et al. 2002, p. 484). Trust in the police varies greatly between European nations and regions, particularly between Scandinavia (high trust) and Eastern Europe (low trust) (Pfister 2020).

Public attitudes toward AI decision-making and support are far  from  uniform and depend on multiple interrelated factors, such  as  application  context,  geographical  and  cultural  differences, or other (perceived) characteristics of the respective technology, such as fairness and transparency (Starke et al. 2022). It has been found that in the application context of justice, automated decisions are perceived as less risky and fairer than deci- sions made by human experts (Araujo et al. 2020). Others, however, suggest a preference for decisions made by human actors in a policing context (Hobson et al. 2023). Human decisions to accept or reject AI suggestions are furthermore not only contingent on the confidence vested in the system but also on personal self-confidence (Chong et al. 2022). Both over- and undertrust can thus lead to problematic outcomes in high-risk situations such as border control, where humans are meant to make reliable final decisions based on AI suggestions. Given the lack of social consensus, the coming paragraphs assess the implications of both low and high trust in a system like ADDS.

## Implications of low trust

Public  trust  is  fundamental  for  the  societal  acceptance  of AI-based systems and therefore their sustainable adoption (Gillespie et al. 2023). The deployment of unreliable AI-based technology can actively lead to a loss of trust in public institutions (Starke et al. 2022). While, to our knowledge, trust in automated deception detection in a border control context is yet to be studied empirically, automated emotion recognition appears to predominantly evoke negative attitudes: Interviewees describe it as 'invasive' and 'scary' (Andalibi and Buss 2020, p. 6). Those who believe emotion detection to be accurate can also perceive this accuracy as concerning, i. e., as a threat to human agency (Grill  and  Andalibi  2022).  Others  question  if  the  technology can work at all due to the complexity of human emotions. This indicates that academic criticisms of the theoretical basis and empirical accuracy of ADDS are also reflected in citizen concerns.

While ADDS is meant to control non-EU nationals and therefore poses no immediate personal risk to EU citizens, based on the qualitative studies just mentioned it seems plausible to assume that many would fundamentally oppose the use of deception detection in a high-risk setting such as border control. Particularly  persons with a general 'anti-surveillance' viewpoint (Ezzeddine  et  al.  2023,  p. 869)  emphasize  the  importance  of personal freedom over security and oppose all police AI that might flag individuals as suspicious, regardless of who the system is used on. Those generally critical and distrustful of AI policing might perceive the use of ADDS at European borders as threatening to human agency rather than as a trustworthy security measure. Given the already uneven trust in the police within the EU, deploying ADDS might further erode this shaky ground, leading to increased dissonance between nations and political unrest.

The perceived transparency of systems impacts trust, with higher transparency entailing more trust (Aysolmaz et al. 2023). As has been shown in TA studies, trust-building communication cannot only consist of conveying technical aspects such as reliability  (Weydner-Volkmann 2021). Meaningful transparency should also include justifying the logic, reasoning, and legality of (semi-)automated decisions (Malgieri 2020). In the case of ADDS, this would inevitably include disclosing and explaining the  technology's  contentious  theoretical  basis  and  potentially

underwhelming  accuracy;  given  the  described  criticisms  that question both if and how the system works, a sufficient justification of its use to the public would thus be challenging.

## Implications of high trust

Freedom  of  movement  of  EU  citizens  is  one  of  the  cornerstones of the European project. Yet, with the Russian invasion of Ukraine, high immigration and growing support for far-right parties, even internal borders such as Germany-Poland undergo more rigorous checks, while the majority of EU citizens support stricter external border protection and up to a third think individual nations should control their own borders (BrusselsReport.eu 2022). As opposed to groups who fundamentally oppose AI policing, parts of the population passively trust any police action on principle (Bradford et al. 2020), which might lead them to be more accepting of technologies they would oppose in other contexts. Given that the iBorderCtrl system is meant to control non-European migrants while EU citizens maintain privacy, a "Not Me group" (Ezzeddine et al. 2023, p. 872) might also be prevalent in this case; these individuals endorse AI policing for their personal safety but not on their own data and might therefore trust in systems like ADDS.

Comprehension is not a necessary prerequisite for trust. Instead, people often trust in things they find too complex to un-

EU citizens, expressing distrust in the form of refusal is therefore not possible and those with a "Not Me" perspective might overall not be interested in protesting the technology. However, deploying similar systems at EU borders has the potential to perpetuate and aggravate harmful social inequalities and therefore affect all parts of society. EU citizens should thus be comprehensively informed about the described risks and uncertainties in order to facilitate reasonable distrust in - and therefore resistance against - potentially unjustified technology.

## Conclusion

Both immigration and AI-based systems are complex and controversial  societal  issues.  With  iBorderCtrl,  the  EU  has  attempted to find solutions to the former via the latter. This paper has highlighted the importance of justification for the use of such a system, particularly considering public trust. At the current state, we observe a lack of justification on two fundamental levels: The theoretical basis of deception detection is highly contentious on an epistemological level. From an empirical perspective, varying accuracy rates achieved in small-sample studies do not seem sufficient to justify the usefulness of the technology compared to human experts. At the same time, public

Deploying automatic deception detection systems might further erode the shaky ground of trust in the police within the EU, leading to increased dissonance between nations and political unrest.

derstand (Reinhardt 2023). Considering its justification issues, trust in lie detection technology might be misplaced since unreliable systems can lead to biased decisions; it has for instance been found that emotion detection systems can have racist bias (Rhue 2018). Not only underrepresenting populations can lead to  discriminatory  outcomes  but  also over representing  already marginalized groups (Bacchini and Lorusso 2019). If predominantly non-EU citizens' data are fed into ADDS, the system might for instance learn to disproportionately classify the microexpressions of individuals of non-European descent as deceptive. The use of ADDS for border control might not only continue inequalities and discriminatory dynamics but, by automating them, embed them further into the social fabric of an already divided and crisis-ridden Europe.

The question of transparency is again relevant here, but in the context of actively fostering distrust (Ammicht Quinn 2015). Distrust can mobilize citizens to refuse using certain technologies or actively protest them (Büscher and Sumpf 2015), which can incentivize governments and industries to more carefully assess the potential harms of deploying systems such as ADDS. In the case of ADDS, EU citizens' data would not be analyzed; for opinions and trust regarding AI policing are already divided. A responsible European policy towards related systems must consider the criticism outlined as well as transparently disclose it to the public to prevent both a further loss of trust in public institutions and blind trust in AI-based border control.

Funding · This article is based on research in the projects 'Trust in   Information', funded by the Ministry of Science, Research and Arts Baden-Württemberg, and 'PEGASUS', funded by the German Federal Ministry of Education and Research.

Competing interests · The authors declare no competing interests.

## References

Ammicht Quinn, Regina (2015): Trust generating security generating trust. An ethical perspective on a secularized dicourse. In: Behemoth. A Journal on Civilisation 8 (1), pp. 109-125. https:/ /doi.org/10.6094/behemoth.2015.8.1.855 Andalibi, Nazanin; Buss, Justin (2020): The human in emotion recognition on social media. Attitudes, outcomes, risks. In: Regina Bernhaupt et al. (eds.): Proceedings of the 2020 CHI Conference on Human Factors in   Computing Systems (CHI '20). New York, NY: Association for Computing Machinery, pp. 1-16. https://doi.org/10.1145/3313831.3376680

- Araujo, Theo; Helberger, Natali; Kruikemeier, Sanne; de Vreese, Claes (2020): In AI we trust? Perceptions about automated decision-making by   artificial intelligence. In: AI &amp; Society 35 (3), pp. 611-623. https:/ /doi.org/10.1007/ s00146-019-00931-w
- Aysolmaz, Banu; Müller, Rudolf; Meacham, Darian (2023): The public   perceptions of algorithmic decision-making systems. Results from a large-scale   survey. In: Telematics and Informatics 79, p. 101 954. https://doi.org/10.1016/j.tele. 2023.101954
- Bacchini, Fabio; Lorusso, Ludovica (2019): Race, again. How face recognition technology reinforces racial discrimination. In: Journal of Information, Communication and Ethics in Society 17 (3), pp. 321-335. https://doi.org/10.1108/ jices-05-2018-0050
- Beduschi, Ana (2020): International migration management in the age of   artificial intelligence. Migration Studies 9 (3), pp. 576-596. https://doi.org/10.1093/ migration/mnaa003
- Bond, Charles; DePaulo, Bella (2006): Accuracy of deception judgments. In: Personality and Social Psychology Review 10 (3), pp. 214-234. https://doi.org/ 10.1207/s15327957pspr1003\_2
- Bradford, Ben; Yesberg, Julia; Jackson, Jonathan; Dawson, Paul (2020): Live facial recognition. Trust and legitimacy as predictors of public support for police use of new technology. In: The British Journal of Criminology 60 (6), pp. 15021522. https://doi.org/10.1093/bjc/azaa032
- Brandner, Lou; Hirsbrunner Simon (2023). Algorithmic fairness in police investigative work. Ethical analysis of machine learning methods for facial recognition. In: TATuP - Journal for Technology Assessment in Theory and Practice 32 (1), pp. 24-29. https:/ /doi.org/10.14512/tatup.32.1.24
- BrusselsReport.eu (2022): Poll reveals great unease among Europeans about migration policy. In: Brussels report, 01. 02. 2022. Available online at https:// www.brusselsreport.eu/2022/02/01/poll-reveals-great-unease-amongeuropeans-about-migration-policy/, last accessed 26. 01. 2024.
- Büscher, Christian; Sumpf, Patrick (2015): 'Trust' and 'confidence' as sociotechnical problems in the transformation of energy systems. In: Sustainability and Society 5 (34), pp. 1-13. https://doi.org/10.1186/s13705-0150063-7
- Chong, Leah; Zhang, Guanglu; Goucher-Lambert, Kosa; Kotovsky, Kenneth; Cagan, Jonathan (2022): Human confidence in artificial intelligence and in themselves. The evolution and impact of confidence on adoption of AI advice. In: Computers in Human Behavior 127, p. 107 018. https://doi.org/10.1016/j. chb.2021.107018
- Devos, Thierry; Spini, Dario; Schwartz, Shalom (2002): Conflicts among human values and trust in institutions. In: The British Journal of Social Psychology 41, pp. 481-494. https://doi.org/10.1348/014466602321149849
- Dumbrava, Costica (2021): Artificial intelligence at EU borders. Overview of applications and key issues. Brussels: European Parliamentary Research Service. Available online at https://www.europarl.europa.eu/thinktank/en/document/ EPRS\_IDA(2021)690706, last accessed on 26. 01. 2024.
- Ekman Paul (1985): Telling lies. Clues to deceit in the marketplace, politics and marriage. New York, NY: W. W. Norton and Company.
- Ekman, Paul (2016): What scientists who study emotion agree about. In:   Per  spectives on Psychological Science 11 (1), pp. 31-34. https://doi.org/  10.1177/ 1745691615596992
- Elfenbein, Hillary; Ambady, Nalini (2002): On the universality and cultural specificity of emotion recognition. A meta-analysis. In: Psychological Bulletin 128 (2), pp. 203-235. https://doi.org/10.1037/0033-2909.128.2.203
- Ezzeddine, Yasmine; Bayerl, Petra; Gibson, Helen (2023): Safety, privacy, or both. Evaluating citizens' perspectives around artificial intelligence use by police forces. In: Policing and Society 33 (7), pp. 861-876. https://doi.org/10.1080/104 39463.2023.2211813
- Feldman Barrett, Lisa; Adolphs, Ralph; Marsella, Stacy; Martinez, Aleix;   Pollak, Seth (2019): Emotional expressions reconsidered. Challenges to inferring emotion from human facial movements. In: Psychological Science in the Public Interest 20 (1), pp. 1-68. https://doi.org/10.1177/1529100619832930
- iBorderCtrl (2018): D7.6 Yearly communication report including communication material. Available online at https://ec.europa.eu/research/participants/ documents/downloadPublic?documentIds=080166e5be014692&amp;appId=PP GMS, last accessed on 26. 01. 2024.
- iBorderCtrl (2023): Related projects. Available online at https://web.archive.org/ web/20211203233051/https://www.iborderctrl.eu/Related-Projects, last accessed on 26. 01. 2024.
- Gillespie, Nicole; Lockey, Steven; Curtis, Caitlin; Pool, Javad; Akbari, Ali (2023): Trust in artificial intelligence. A global study. Brisbane: University of Queensland and KPMG Australia. https://doi.org/10.14264/00d3c94
- Grill, Gabriel; Andalibi, Nazanin (2022): Attitudes and folk theories of data subjects on transparency and accuracy in emotion recognition. In: Proceedings of the 2022 ACM on Human-Computer Interaction 6 (CSCW1), pp. 1-35. https:// doi.org/10.1145/3512925
- Helm, Paula; Hagendorff, Thilo (2021): Beyond the prediction paradigm. Challenges for AI in the struggle against organized crime. In: Law and Contemporary Problems 84 (3), pp. 1-17. Available online at https://scholarship.law.duke. edu/lcp/vol84/iss3/2, last accessed on 26. 01. 2024.
- Hobson, Zoë; Yesberg, Julia; Bradford, Ben; Jackson, Jonathan (2023):   Artificial fairness? Trust in algorithmic police decision-making. In: Journal of Experimental Criminology 19 (1), pp. 165-189. https://doi.org/10.1007/s11292021-09484-9
- Kaminski, Andreas (2019): Begriffe in Modellen. Die Modellierung von   Vertrauen in Computersimulation und maschinellem Lernen im Spiegel der Theoriegeschichte des Vertrauens. In: Nicole Saam, Michael Resch and Andreas Kaminski (eds.): Simulieren und Entscheiden. Wiesbaden: Springer VS, pp. 173-197. https://doi.org/10.1007/978-3-658-26042-2
- Malgieri, Gianclaudio (2020): 'Just' algorithms. AI justification (beyond explanation) in the GDPR. In: Gianclaudio Malgieri Blog, 14. 12. 2020. Available online at  www.gianclaudiomalgieri.eu/2020/12/14/just-algorithms/, last accessed on 26. 01. 2024.
- O'Shea, James; Crockett, Keeley; Khan, Wasiq; Kindynis, Philippos; Antoniades, Athos; Boultadakis, Georgios (2018): Intelligent deception detection through machine based interviewing. In: Proceedings of the International Joint Conference on Neural Networks 2018. New York, NY: Institute of Electrical and Electronics Engineers, pp. 1-8. https://doi.org/10.1109/IJCNN.2018.8489392
- Pfister, Sabrina (2020): Vertrauen in die Polizei. Wiesbaden: Springer VS. https:// doi.org/10.1007/978-3-658-35425-1
- Podoletz, Lena (2023): We have to talk about emotional AI and crime. In: AI &amp; Society 38 (3), pp. 1067-1082. https://doi.org/10.1007/s00146-022-01435-w
- Porter, Stephen; Woodworth, Mike; Birt, Angela (2000): Truth, lies, and videotape. An investigation of the ability of federal parole officers to detect deception. In: Law and Human Behavior 24 (6), pp. 643-658. https://doi.org/ 10.1023/a:1005500219657
- Reinhardt, Karoline (2023): Trust and trustworthiness in AI ethics. In: AI Ethics 3 (3), pp. 735-744. https:/ /doi.org/10.1007/s43681-022-00200-5

Rhue, Lauren (2018): Racial influence on automated perceptions of emotions. In: SSRN Journal. https://dx.doi.org/10.2139/ssrn.3281765

Rothwell, Janet; Bandar, Zuhair; O'Shea, James; McLean, David (2006): Silent Talker. A new computer-based system for the analysis of facial cues to deception. In: Applied Cognitive Psychology 20 (6), pp. 757-777. https://doi. org/  10.1002/acp.1204

Sánchez-Monedero, Javier; Dencik, Lina (2022): The politics of deceptive borders. 'Biomarkers of deceit' and the case of iBorderCtrl. In: Information, Communication &amp; Society 25 (3), pp. 413-430. https://doi.org/10.1080/13691 18X.2020.1792530

Selbst, Andrew (2017): Disparate impact in big data policing. In: Georgia Law

Review 52 (1), pp. 109-195. http://dx.doi.org/10.2139/ssrn.2819182

Starke, Christoph; Baleis, Janine; Keller, Birte; Marcinkowski, Frank (2022):

Fairness perceptions of algorithmic decision-making. A systematic review of the empirical literature. In: Big Data &amp; Society 9 (2), pp. 1-16. https://doi. org/10.1177/20539517221115189

Varghese, Ashwini; Cherian, Jacob; Kizhakkethottam, Jubilant (2015): Overview on emotion recognition system. In: Proceedings of the 2015 International Conference on Soft-Computing and Networks Security (ICSNS). Coimbatore: IEEE Xplore, pp. 1-5. https://doi.org/10.1109/ICSNS.2015.7292443

Weydner-Volkmann, Sebastian (2021): Technikvertrauen. In: TATuP - Journal for Technology Assessment in Theory and Practice 30 (2), pp. 53-59. https:// doi.org/10.14512/tatup.30.2.53

Whittaker, Meredith et al. (2018): AI now report 2018. New York, NY: AI Now Institute. Available online at https://ec.europa.eu/futurium/en/system/files/ ged/ai\_now\_2018\_report.pdf, last accessed on 26. 01. 2024.

Zhang, Liangfei; Arandjelović, Ognjen (2021): Review of automatic microexpression recognition in the past decade. In: Machine Learning and Knowledge Extraction 3 (2), pp. 414-434. https://doi.org/10.3390/make3020021

<!-- image -->

<!-- image -->

## DR. DANIEL MINKIN

is substitute professor at Bergische University Wuppertal and research associate at the HighPerformance Computing Center Stuttgart,   Germany. His research and areas of expertise include philosophy of artificial intelligence, epistemology, and philosophy of the social sciences.

## DR. LOU THERESE BRANDNER

is a research associate at the International Center for Ethics in Sciences and Humanities (IZEW) at the University of Tübingen. She received her PhD in   Sociology from La Sapienza University in Rome. Her research focuses on AI and data ethics, digital capitalism, and spatial issues.

<!-- image -->

<!-- image -->

## RESEARCH ARTICLE

## Situativität, Funktionalität und Vertrauen : Ergebnisse einer szenariobasierten Interviewstudie zur Erklärbarkeit von KI in der Medizin

Manuela Marquardt 1

<!-- image -->

, Philipp Graf  * , 2

<!-- image -->

, Eva Jansen 1

Zusammenfassung · Eine  zentrale  Anforderung  an  den  Einsatz  von künstlicher Intelligenz (KI) in der Medizin ist ihre Erklärbarkeit, also die Bereitstellung von adressat*innengerechten Informationen über ihre Funktionsweise. Dies führt zu der Frage, wie eine sozial adäquate Erklärbarkeit gestaltet werden kann. Um Bewertungsfaktoren zu identifizieren, befragten wir Akteur*innen des Gesundheitswesens zu zwei Szenarien: Diagnostik und Dokumentation. Die Szenarien variieren den Einfluss, den ein KI-System durch das Interaktionsdesign und die Menge der  verarbeiteten  Daten  auf  die  Entscheidung  hat.  Wir  stellen  zentrale Bewertungsfaktoren für Erklärbarkeit auf interaktionaler und prozessualer  Ebene  dar.  Erklärbarkeit  darf  im  Behandlungsgespräch  situativ nicht interferieren und die professionelle Rolle infrage stellen. Zugleich  legitimiert  Erklärbarkeit  ein  KI-System  funktional  als  Zweitmeinung und ist zentral für den Aufbau von Vertrauen. Eine virtuelle Verkörperung des KI-Systems ist vorteilhaft für sprachbasierte Erklärungen.

* Corresponding author: pgraf@hm.edu

- 1 Institut für Medizinische Soziologie und Rehabilitationswissenschaften, Charité - Universitätsmedizin Berlin, Berlin, DE
- 2 Fakultät für angewandte Sozialwissenschaften, Hochschule München, München, DE

3 Quality &amp; Usability Lab, Technische Universität Berlin, Berlin, DE

4 Immersive Reality Lab, Hochschule Hamm-Lippstadt, Hamm-Lippstadt, DE

<!-- image -->

© 2024 by the authors; licensee oekom. This Open Access article is licensed under a Creative Commons Attribution 4.0 International License (CC BY). https://doi.org/10.14512/tatup.33.1.41

Received: 22. 08. 2023; revised version accepted: 15. 12. 2023; published online: 15. 03. 2024 (peer review)

https://doi.org/10.14512/tatup.33.1.41

<!-- image -->

<!-- image -->

, Stefan Hillmann 3

<!-- image -->

, Jan-Niklas Voigt-Antons 4

<!-- image -->

Situativity, functionality and trust : Results of a scenario-based interview study on the explainability of AI in medicine

Abstract · A central requirement for the use of artificial intelligence (AI) in medicine is its explainability, i. e., the provision of addressee-oriented information about its functioning. This leads to the question of how socially adequate explainability can be designed. To identify evaluation factors, we interviewed healthcare stakeholders about two scenarios: diagnostics and documentation. The scenarios vary the influence that an AI system has on decision-making through the interaction design and the amount of data processed. We present key evaluation factors for explainability at the interactional and procedural levels. Explainability must not interfere situationally in the doctor-patient conversation and question the professional role. At the same time, explainability functionally legitimizes an AI system as a second opinion and is central to building trust. A virtual embodiment of the AI system is advantageous for language-based explanations.

Keywords · explainability, XAI, AI in healthcare, embodied AI, voice dialog system

This article is part of the Special topic 'AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?,' edited by D. Schneider and K. Weber. https://doi.org/10.14512/tatup.33.1.08

## Zielsetzung

In den letzten Jahren erfolgte eine stete Zunahme von Systemen künstlicher  Intelligenz  (KI)  im  medizinischen  und  therapeutischen  Bereich:  von  Sprachassistenzsystemen  zur  Pflegedokumentation über Chatbots zur Förderung der mentalen Gesund-

SPECIAL TOPIC · AI fOr dECISIOn SuPPOrT

<!-- image -->

heit bis hin zu diagnostischen Systemen wird KI in vielfältigen Formen angewandt (Becker 2019). Dabei rücken nicht nur die Verarbeitung größerer Datensätze, sondern auch die Diskussion um sensible Entscheidungsprozesse als mögliche Anwendungsfälle in den Fokus (Heil et al. 2021). Auch ,weiche' KI-Systeme, wie dialogbasierte Assistenzsysteme, die keine expliziten Entscheidungen treffen, üben durch die Selektion und Darstellung von Informationen (Gupta et al. 2022), insbesondere durch deren Wortwahl (Mahmood und Huang 2022), einen impliziten Einfluss  auf  das  Entscheidungsverhalten  der  mit  ihnen  interagierenden Personen aus.

Um eine sichere und ethisch vertretbare Anwendung eines KI-Systems im medizinischen Bereich zu ermöglichen, besteht die Forderung, die Funktionsweise der KI transparent und nachvollziehbar zu gestalten (Markus et al. 2021; Samhammer et al. 2023). Ein vielversprechender Ansatz ist die Entwicklung von Explainable Artifical  Intelligence  (XAI)  Systemen.  Erklärbarkeit wird in Anlehnung an Miller (2019, S. 5) als 'an active characteristic of a model, denoting any action or procedure taken by a model with the intent of clarifying or detailing its internal functions'  definiert.  Um  das  Ziel  der  Nachvollziehbarkeit  zu erreichen, müssen Inhalt und Form der Erklärung an den Adressat*innen orientiert sein (Barredo Arrieta et al. 2020). Eine sprachvermittelte  Interaktion  zwischen  Nutzenden  und  dem KI-System bietet das Potenzial zur Bildung von Vertrauen mittels XAI (Hillmann et al. 2021). Auch wenn natürlichsprachliche Erklärungen im Rahmen von XAI noch nicht im Stand der Technik angekommen sind, zeigen erste Forschungsergebnisse ihre Machbarkeit in Form von Dialogen (Feldhus et al. 2023).

Neben den unterschiedlichen Adressat*innen der Erklärungen lassen sich KI-Systeme im medizinischen Bereich nach dem Zweck  der  Erklärungen  differenzieren.  Markus  et  al.  (2021) schlagen  drei  Bereiche  vor:  Um  Modellannahmen  zu  verifizieren oder zu verbessern, um neue Erkenntnisse zu gewinnen oder '[t]o manage social interaction' (Markus et al. 2021, S. 3). Allerdings besteht wenig Wissen darüber, wie Erklärbarkeit in KI-Systemen  bei  der  Translation  ins  Gesundheitssystem  kon- kret gestaltet werden soll und welche kontextuellen Bedarfe der Nutzenden relevant werden. Der Fokus dieses Beitrags liegt auf den sozialen Aspekten von Erklärbarkeit von KI in der Medizin, unabhängig von der technischen Ausprägung des zugrunde liegenden KI-Ansatzes (bspw. Klassifikation vs. Prädiktion, oder die Unterscheidung nach verschiedenen Methoden des maschinellen Lernens wie Support Vector Machines oder Deep Learning) oder den regulatorischen Implikationen (HEG-KI 2019).

Ein besonderer Forschungsbedarf besteht zu der Frage, welche soziale Rolle ein erklärbares KI-System zwischen Behandelnden und Patient*innen einnehmen könnte und welche Auswirkungen eine virtuelle (Avatar) oder real-weltliche (Roboter) Verkörperung des KI-Systems auf die Interaktion hat. Aus Sicht der  Technikfolgenabschätzung  ist  es  wünschenswert,  die  Diskussion der Umsetzung von XAI und ihrer sozialen Folgen mit empirischen Daten zu informieren. Ziel unserer Studie ist es, mithilfe von szenariobasierten Interviews Bewertungsfaktoren im Hinblick auf Erklärbarkeit von KI in der Medizin herauszuarbeiten und mit sozialen Deutungsmustern im medizinischen Kontext in Relation zu setzen.

## Methode

Zur Identifikation relevanter Faktoren der Deutungsmuster eignen  sich  qualitative  Interviews  (Meuser  und  Sackman  1992). Diese setzen mittels vignettenförmiger Szenarien einen Stimulus, um Teilnehmende zu aktivieren und emotional bedingte, implizite Anteile an Bewertungen aufzudecken (Kosow und Gaßner 2008). Während das ,Baseline-Szenario' den allgemeinen Rahmen steckt, variieren die Vignetten einzelne Aspekte. So können graduelle Bewertungen zu einem konkreten Thema abgefragt werden (Barter und Renold 1999). Vignetten sollten präzise, einfach und plausibel sein, um einen möglichst anschlussfähigen Stimulus zu bieten (Hughes und Huby 2004). Die vergleichend ausgestalteten Vignetten animieren Interviewte zu einer differenzierten oder widersprüchlichen Bewertung (Jenkins et al. 2010).

Abb. 1: Übersicht über die Konstruktion der Szenarien.

<!-- image -->

Quelle: eigene Darstellung

Für diese Studie entwickelten wir zwei Baseline-Szenarien mit je drei Fallvignetten. Die Vignetten variieren im Einfluss auf  die  Entscheidung,  die  ein  KI-System  mittels  des  Interaktionsdesigns sowie des Umfangs der verarbeiteten Daten nimmt (siehe Abb. 1).

Das erste Szenario liegt im Bereich Diagnostik. Die Diagnostik ist eines der vielversprechendsten Einsatzgebiete für KI-Systeme in der Medizin (Samhammer et al. 2023), da hier große Mengen an Daten präzise und effizient analysiert und komplexe Muster erkannt werden können (Kinar et al. 2017). Wenn die

## Inhaltliche Auswertung

Im  Folgenden  gliedern  wir  die  zentralen  Bewertungsfaktoren auf  interaktionaler  und  prozessualer  Ebene.  Diese  Differenzierung  orientiert  sich  primär  an  raumzeitlichen  Wirkungszusammenhängen: Die interaktionale Ebene thematisiert die konkrete Interaktionssituation zwischen Behandler*in und/oder Patient*in  und  dem  KI-System.  Sie  fokussiert  auf  gleichzeitige und kopräsente Aspekte der Bewertung von Erklärbarkeit (situativ, kurzfristig), wobei die Form der Mitteilung einer Erklärung

Die von der KI abgegebene Erklärung oder gar das   Aufdecken von ,Fehlern' empfinden die Interviewten als unpassende Zweitmeinung, die die Fachkompetenz infrage stellt und das Vertrauensverhältnis gefährdet.

Entscheidungsgrundlage nicht nachvollziehbar ist, kann der Einsatz von KI-Systemen zu erheblichen Vertrauensproblemen führen (Markus et al. 2021) und dadurch die traditionelle Beziehung zwischen Behandler*in und Patient*in untergraben (Čartolovni et al. 2022).

Das zweite Szenario adressiert den Bereich der Erhebung von Gesundheitsdaten sowie die Dokumentation einer Therapie und das Verfassen eines Entlassungsbriefes durch eine KI als Entlastung für Behandler*innen (Bossen und Pine 2023). Zentrale Konfliktlinien zeichnen sich in der Akzeptanz der Technologie (Čartolovni et al. 2022), im Umgang mit Datenschutz (Becker 2019) und in der Verstärkung von Ungleichheiten im Kontrast zur Arbeitserleichterung (Panch et al. 2019) ab.

Die Szenarien stellten wir in qualitativ-explorativen Onlineund Präsenzinterviews insgesamt 16 Akteur*innen des Gesundheitswesens vor. Die Hälfte der Interviewten stammte aus dem ärztlichen Bereich (Tab. 1). Diese Auswahl wurde getroffen, da Ärzt*innen üblicherweise in den Gesundheitseinrichtungen, in denen sie arbeiten, am diagnostischen Prozess beteiligt sind. Zur Gewinnung einer vielfältigen Perspektive auf den KI-Einsatz in der Diagnostik und Dokumentation rekrutierten wir die Teilnehmenden aus verschiedenen Fachbereichen. Sie sollten die ihnen fremden Szenarien, die circa zehn Jahre in der Zukunft angesiedelt sind, auf ihren Bereich übertragen. Die Teilnehmenden wurden gebeten, die sozialen Folgen, den Nutzen, die Schwierigkeiten und die Funktionalität des jeweiligen KI-Systems und die gewünschte Form der Erklärung für Behandelnde und Patient*innen einzuschätzen.

Die Rekrutierung erfolgte über professionelle Netzwerke. Die Interviews fanden zwischen dem 12. 06. 23 und dem 14. 08. 23 statt  und  dauerten  durchschnittlich  57  Minuten.  Für  die  Auswertung der transkribierten Interviews verwendeten wir die Methode der Grounded Theory (Pentzold et al. 2018).

und die Verkörperung des KI-Systems besonders relevant werden. Die prozessuale Ebene umfasst längerfristige, translokale und  organisationale  Faktoren  wie  den  Aufbau  von  Vertrauen, die Bedeutung von Standards sowie den Einfluss auf die fachliche Kompetenz.

## Interaktionale Ebene

Der erste Faktor  auf  interaktionaler  Ebene,  der  von  den  Teilnehmenden als Einflussfaktor auf die Erklärbarkeit von KI-Systemen referenziert  wurde,  ist  das  Behandlungsgespräch  (BG). Hier  wird  die  Institution  der  professionellen  ärztlichen  Rolle wirkmächtig, die mit einem Vertrauensvorschuss, der Entscheidungshoheit  und  der  Haftbarkeit  für  Behandlungsfehler  verknüpft ist. Obwohl ein KI-Einsatz im BG nicht Teil der Fallvignetten des ersten Szenarios war, reflektierte die Mehrheit der Interviewten in ihren Einschätzungen darüber. Innerhalb eines BG ist eine Interaktion mit einer ,pro-aktiven' KI tendenziell nicht  erwünscht  -  hier  sollte  die  'KI  nicht  interferieren'  (3). Während die passive Nutzung von KI als reines Werkzeug, das auf Zuruf arbeitet, in Einzelfällen akzeptiert wird, stellt das Erklären von Sachverhalten für die meisten Behandler*innen eine Grenze dar, die die ärztliche Integrität untergräbt: 'Das ist wie wenn ein Kollege ungefragt bei mir reinquatscht' (3). Die von der KI abgegebene Erklärung oder gar das Aufdecken von 'Fehlern'  (13)  empfinden  die  Interviewten  als  unpassende  Zweitmeinung, die die Fachkompetenz infrage stellt und das Vertrauensverhältnis gefährdet (10). Zudem dürfen Diagnosen nicht in einem BG verhandelt werden, 'um den Patienten nicht zu verunsichern' (14). Als vertrauensbildend wurde die Möglichkeit der Bezugnahme auf KI genannt - nämlich als 'Zusatz, dass die KI das auch gesagt hat' (15). Die KI kann als Statussymbol für eine hohe technische Ausstattung (2) eine vertrauensfördernd positive Wirkung haben.

Die  befragten  Behandler*innen  stellten  allerdings  die  notwendige  Kompetenz infrage, da KI 'nicht empathisch genug' sei und daher 'nicht mitreden' solle (3). Dies betrifft insbesondere das Erklären persönlicher Einzelfalldiagnosen, die bei schwerwiegenden Fällen emotionaler Arbeit bedürfen (15, 12). Eine Ausnahme davon  stellt  das  Vermitteln  von  'Lehrbuchwissen' (9) dar: Die direkte Interaktion zwischen KI und Patient*in könnte Entlastung schaffen und ein zusätzliches Informationsangebot sein (3).

Die Form der Mitteilung einer Erklärung  ist  der zweite relevante  Faktor  auf interaktionaler  Ebene.  Sie  variierte  zwischen  den  Fallvignetten  beider  Szenarien. Die Antworten auf die Frage nach der Präferenz einer sprachlichen und/oder visuellen  Form  divergieren  situativ  und mit der eigenen Erfahrung mit Sprachassistenzsystemen  und  deren  Fehleranfälligkeit sowie den kontextuellen Faktoren der eigenen Berufspraxis. Viele Befragte wünschten sich eine gut funktionierende, korrigierbare Diktieroption (8, 16). Sprache ist für einige das effizientere Medium,

Tab. 1: Übersicht über die Teilnehmenden.

| Nr.    | Alter           | Ausbildung                | Position                        |
|--------|-----------------|---------------------------|---------------------------------|
| 1. 40  | Psychologie     |                           | Wissenschaftliche Mitarbeiterin |
| 2. 70  | Zahnmedizin     |                           | Oberarzt                        |
| 3. 35  | FA Dermatologie |                           | Funktionsoberarzt               |
| 4. 50  | FA              | Psychosomatik             | Oberarzt                        |
| 5.     | 67              | FA Pädiatrie&Neonatologie | Chefarzt                        |
| 6. 45  | FA              | Dermatologie              | Fachärztin                      |
| 7. 73  |                 | Orthopädie                | Arzthelferin                    |
| 8.     | 35              | FA Radiologie             | Fachärztin                      |
| 9. 57  |                 | Krankenpflege             | Hygienefachschwester            |
| 10. 52 |                 |                           | Patient*innenvertretung         |
| 11. 53 |                 | Medizin                   | Gesundheitsmanagerin            |
| 12. 60 |                 |                           | Patient*innenvertretung         |
| 13. 42 |                 | Psychosomatik             | Psychotherapeutin               |
| 14. 42 |                 | FA Kardiologie            | Fachärztin                      |
| 15.    | 38              | FA Gynäkologie            | Fachärztin                      |
| 16. 28 |                 | Medizinstudent            |                                 |
| ø = 49 |                 | w = 10 | m = 6            |                                 |

um mit einer KI zu interagieren (2, 4, 8, 13, 14). Für andere ist ein Text und zusätzliches Bildmaterial unumgänglich zur Einschätzung der Erklärung. Einerseits, weil eine Hürde darin bestünde, in direkte, sprachliche Interaktion mit einer KI zu gehen: 'Sprachbasiert funktioniert astrein, aber ich würde es lieber als Text haben, weil ich mich nicht mit einem Computer austauschen möchte' (3). Andererseits ist für die Befragten die Dokumentation als Grundlage der Erklärung mit einer visuellen Darstellung der Vorbefunde und Statistiken leichter zugänglich (8) und damit nachvollziehbarer (6). Insbesondere im stationären Kontext, 'wo eh schon immer so viel los ist' (16), wird die textbasierte Form gegenüber Sprache aus Gründen des Datenschutzes  bevorzugt.  Neben  der  persönlichen  Präferenz  und  Befähigung nimmt der räumlich-soziale Faktor die zentrale Rolle ein, ob sprachliche Interaktion mit einer KI als angemessen wahrgenommen wird oder nicht.

Quelle: eigene Darstellung

wahr, die ein 'eigenartiges Gefühl' auslöst (15). Seine real-weltliche  Verkörperung  wird  aufgrund  der  geringeren  Kosteneffizienz (8) und der nicht vorhandenen Translokalität (8, 11) als ungeeignet wahrgenommen und implizit stärker mit dem Narrativ des ,Ersetzt-Werdens' assoziiert (2, 6). Die Teilnehmenden perzipieren den ,stärker' verkörperten Roboter als Entität mit graduellem Personenstatus. Dieser ist nicht mehr nur Werkzeug, sondern erlangt eine darüber hinaus gehende, nicht erwünschte, Präsenz (6). Im Kontakt mit Patient*innen erachteten sie eine real-weltliche  Verkörperung  hingegen  aufgrund  der  'Plastizität' (3) und Anonymität (9) als Chance. Den Avatar sehen die Befragten als passendere Form an, die dem ,Werkzeugcharakter' der KI entspräche. Für den Zweck sprachlicher Erklärbarkeit wird er als angemessener empfunden, da er abschaltbar und dann 'nicht so invasiv' wie ein Roboter (16) sei.

Der dritte und letzte Faktor ist die Verkörperung einer KI im Hinblick auf Erklärbarkeit, die in den Fallvignetten des zweiten Szenarios variiert. Einige der Befragten betrachteten eine virtuelle oder real-weltliche Verkörperung als angemessen für ihre Patient*innen, nicht jedoch für sich selbst, da hier 'eine ganz normale Spracheingabe' ausreichend sei (14). Andere würden hingegen bevorzugt mit einem Avatar (6) oder einem Roboter (3) interagieren, wobei eine kleine Minderheit der Interviewten eine virtuelle Verkörperung bevorzugt. Die Präferenz geht auf eine Differenz in der Bewertung des ontologischen Status beider Formen zurück: Einen Roboter nehmen die Befragten als Maschine

## Prozessuale Ebene

Als ersten Bewertungsfaktor auf prozessualer Ebene im Hinblick auf die Erklärbarkeit von KI-Systemen identifizierten wir den Aufbau von Vertrauen. Erklärbarkeit stellt eine Bedingung für eine 'Vertrauensbasis' dar, 'die aufgebaut werden [muss]' (9), wie folgendes Zitat verdeutlicht: 'Feedback hätte ich schon gerne, warum das System auf die Diagnose kommt, ansonsten würde ich dem System nicht vertrauen' (6, auch 9, 14). Dabei ist es wichtig, 'aus dem Schwarz-Weiß [raus zu gehen] - es muss eine Graustufe haben und auf die reagieren können' (9). Eine differenzierte Einschätzung mit Angabe des Grades der Unsicherheit

rege zu einem eigenen Kontrollprozess an: 'Gleichzeitig kritisiert es sich noch selber, was es glaubwürdiger macht, […] das heißt, es sagt dem Betrachter selber, ich bin nicht 100 % genau, das heißt, du musst selber noch mal gucken - das ist super' (10). Als weitere 'vertrauensbildende Maßnahme' nannte eine Ärztin, 'dass man am Anfang, vielleicht im ersten halben Jahr, viel kontrolliert - so wie wenn ich mit einem Assistenzarzt arbeite' (6). Später reiche es, nur noch bei schweren Diagnosen zu kontrollieren. Der Aufbau von Vertrauen kann also den Bedarf an Erklärbarkeit zum Teil ersetzen, dennoch muss die Möglichkeit der Nachvollziehbarkeit immer gegeben bleiben. Eine besondere Stellung nimmt der Zugriff und der Bezug in den Erklärungen auf die Rohdaten (8, 11, 14) ein. Erklärbarkeit und Vertrauen in die Reliabilität dieser Erklärungen sind diejenigen Aspekte, die aus dem ,Werkzeug KI' eine 'Zweitmeinung' (13) werden lassen und eine 'Kommunikation auf Augenhöhe' (6) ermöglichen: 'Man hat die Möglichkeit nachzufragen und bekommt Infos. Das wäre, als ob sich zwei Ärzte unterhalten' (13).

Vertrauen  kann  durch  fachspezifische  Standards,  etwa  im Sinne von 'Datenbankstandards' (10), sowie der Absicherung durch medizinische Fachgesellschaften oder Verbände mittels Zertifikaten externalisiert werden, 'ähnlich wie bei einem neuen Medikament' (5). Von besonderem Interesse für die Befragten war, wer in welchem Umfang welche Daten für das Training der KI berücksichtigt hat (5, 6, 9, 10, 16) und welche Verantwortlichkeit daraus resultiert (9). Sollten sich bestimmte KI-Systeme als 'Goldstandard' (11) durchsetzen, würde dies den Bedarf an Erklärbarkeit des allgemeinen Funktionierens reduzieren.

Der zweite Faktor zur Bewertung von Erklärbarkeit auf prozessualer Ebene resultiert aus verschiedenen Nutzen- und Problemvisionen und bewegt sich im Rahmen von Verbesserungen der  Qualität  medizinischer  Versorgung.  Erklärbarkeit  kommt bei diesen Visionen ein ambivalenter Stellenwert zu. Der Einsatz von KI als 'bessere Expertise' (5) kann die Effizienz (3, 6, 12, 15), Genauigkeit und damit Qualität der medizinischen Versorgung verbessern. Erklärungen als zusätzliche Reflexionsangebote 'zur Absicherung, Bestätigung [und] Hilfestellung' (2) haben  das  Potenzial,  die  Genauigkeit  zu  steigern  und  zur  Sicherstellung  gleichbleibender  Qualität  der  medizinischen  Versorgung  trotz  hoher  Arbeitsbelastung  (8)  beizutragen.  Die Interviewten sahen es jedoch als Gefahr für die Qualität medizinischer Versorgung an, wenn Empfehlungen einer KI 'unreflektiert übernommen' (14, auch 5) werden oder 'man sich als Therapeut zu sehr auf die Technik verlässt' (4, auch 15). Erklärungen sind eine zentrale Stellschraube für die Vermeidung blinden Technikvertrauens, das zu Deskilling, also dem Abbau von Kompetenzen, führen könne. Bei einer schlechten Umsetzung bestünde zudem das Risiko eines Effizienzverlusts (5).

Im  Zusammenhang  mit  einer  möglichen  Kompetenzerweiterung durch den Einsatz von KI (4, 14) für den medizinischen Nachwuchs (7, 8) und das nicht-ärztliche Personal (7, 9) oder wenn kein Facharzt vor Ort ist (6, 11), existieren gegenläufige Tendenzen im Hinblick auf Erklärbarkeit. In Kombination mit der eigenen fachlichen Expertise schätzen die Behandelnden das

Risiko für Fehlentscheidungen beim Einsatz von erklärbarer KI als gering ein (6). Erklärbarkeit sei nur dann sinnvoll, wenn die adressierte Person in der Lage ist, die zusätzlich zur Verfügung gestellten Informationen in die eigene Entscheidungspraxis zu integrieren und dafür die Verantwortung zu übernehmen. Ein kontrastierendes Beispiel aus unserem Interviewmaterial verdeutlicht dies anhand des Einsatzes eines Diagnostik-KI-Systems durch nicht-ärztliche Gesundheitsfachkräfte (7). Die Befragte äußerte ihre Präferenz für ein Blackbox-KI-System, 'schon damit nicht so viele Fragen kommen' (7) und keine eigenständige Entscheidung getroffen werden müsse. Die Übernahme der Verantwortung für die Diagnosestellung wird in diesem Fall aufgrund fehlender Fachkompetenz und geringerer Bezahlung abgelehnt (9).

## Folgerungen

Die vorliegende Analyse widmete sich mithilfe szenariobasierter Interviews empirisch ermittelten Bewertungsfaktoren sozialadäquater Erklärbarkeit von KI-Systemen im medizinischen Bereich.  Erklärbarkeit  von  KI  ist  durch  den  definitorischen  Zuschnitt auf eine soziale Adressat*in (Miller 2019) der Erklärung eines erklärenden Agenten (Barredo Arrieta et al. 2020) ein sozialwissenschaftlich  anschlussfähiges  Thema.  In  der  Auswertung  zeigt  sich  ein  differenziertes  -  teils  widersprüchliches  Netzwerk an Faktoren, die bei der Bewertung von XAI in der Medizin auf interaktionaler  und  prozessualer  Ebene  wirksam werden. Im Folgenden möchten wir die Anforderungen an sozial-adäquate Erklärbarkeit unter den drei Aspekten Situativität, Funktionalität und Vertrauen zusammenfassen.

## Situativität

Unter den Aspekt der Situativität fällt die Frage, wann und wie eine  Erklärung  angebracht  erscheint.  Die  Befragten  äußerten sich je nach persönlicher Erfahrung, Fachgebiet und Berufspraxis ambivalent zur Frage der sprachlichen Interaktion mit einer KI. Sie assoziierten spezifische Herausforderungen sprachlicher Kommunikation mit Technik (Dickel 2021). Häufig präferierten sie  für  die  Berufspraxis  text-  bzw.  datenbasierte  Erklärungen, obwohl sie situative Vorteile in sprachbasierten und verkörperten Formen von KI sahen. Tendenziell empfanden sie real-weltliche Verkörperungen durch Roboter für den Zweck von Erklärbarkeit  als  weniger  angemessen  als  eine  virtuelle  Form.  Eine häufig  formulierte  Grenze  für  den  Einsatz  von  Erklärbarkeit stellte das BG dar, was im Einklang mit internationaler Literatur steht (Aminololama-Shakeri und López 2019; Powell 2019). Diese Grenze schützt die Autorität und Institution der professionellen ärztlichen Rolle.

## Funktionalität

Der Aspekt der Funktionalität umfasst die Bandbreite von Fähigkeiten und Aufgaben, die einer KI zugewiesen werden können, welche Erwartungen im medizinischen Alltag an sie gerichtet sind und inwiefern Erklärbarkeit hier interferiert. Während

einerseits  akkurate  Erklärungen  die  Genauigkeit  und  Reflexionsleistung steigern können, besteht zugleich Skepsis hinsichtlich der Frage, wie diese zusätzlichen Informationen konkret in bestehende Arbeitsabläufe integriert werden können (Stichwort Zweitmeinung und Kompetenzerweiterung) und welche Konsequenzen sich daraus mittelfristig ergeben (Stichwort Patient*innenkontakt und Deskilling).  Entgegen  der  von  den  Befragten geäußerten Vision KI als Zweitmeinung zu nutzen, finden sich in der Literatur empirische Hinweise, die eine bloße Assistenzfunktion von KI plausibler erscheinen lassen. KI zeigt sich hier als  'unreliable  in  predictable  ways'  (Bossen  und  Pine  2023, S. 16), die eine stetige Kontrolle nötig macht. Erklärbarkeit kann helfen, diese Lücke zu schließen, birgt aber ebenso das Risiko, zu Fehlern zu führen, die ein Mensch nicht machen würde. Eine einheitliche soziale Rolle für eine KI findet sich in unseren Ergebnissen noch nicht. Vielmehr oszilliert diese zwischen einer assistierenden Funktion und einem Agenten mit gleich- oder höherwertiger Kompetenz. Konkrete Rollenerwartungen an ein je spezifisches KI-System dürften sich erst durch wiederholt bewährte Interaktionen verfestigen.

## Vertrauen

Ein  zentrales  Leitmotiv  ist  Vertrauen,  das  aufgebaut,  in  Anspruch genommen, untergraben oder externalisiert wird und mit dem Bedarf an Erklärbarkeit eng verwoben ist. Durch Erklärungen kann das Vertrauen in eine KI gestärkt werden, was den Bedarf an Erklärbarkeit in der wiederholten Nutzung mit der Zeit verringert, wenngleich Kontrolle durch den Zugriff auf die ,Rohdaten' gegeben sein sollte. Dieser Befund steht in Einklang mit der Unterscheidung von Vertrauen als Einstellung und ,reliance' als Verhalten '[…] that follows the level of trust' (Scharowski et al. 2023, S. 4). Kloker et al. (2022, S. 1) unterstreichen die vermittelnde Rolle von Erklärbarkeit zwischen Vertrauen und Vorsicht: '[… M]aximizing trust is not the goal, rather to balance caution and trust to find the level of appropriate trust'. Dies spiegelt sich in unseren Ergebnissen wider: Das Vertrauen in die Sicherheit einer KI geht dialektisch aus dem Umgang des Systems mit Unsicherheit hervor, weil dieser zu einer eigenständigen Reflexionsleistung anregt und einer ,overreliance' vorbeugt.

Zusammenfassend möchten wir das disruptive Potenzial von XAI für mögliche Zukünfte im Kontext der Medizin betonen, das sich an den heterogenen Bewertungsfaktoren ablesen lässt. Durch den Zuschnitt der Szenarien und die Auswahl der Interviewten konnte nur ein Bereich möglicher Anwendungen von XAI beleuchtet werden. Weitere Forschung ist notwendig, um das Zusammenspiel der einzelnen Faktoren besser zu verstehen. Ein ethnographischer Ansatz, der auf die reale und langfristige Einbindung von XAI fokussiert, bietet sich zur Untersuchung der sozialen Dimension von Erklärbarkeit an.

Funding · This article received funding by the German federal ministry of education and research (BMBF) as part of the MIA-PROM project (Multimodal Interactive Assistance for the Collection of Patient Reported Outcome Measures). Competing interests · The authors declare no competing interests.

## Literatur

- Aminololama-Shakeri, Shadi; López, Javier (2019): The doctor-patient   relationship with artificial intelligence. In: American Journal of Roentgenology 212 (2), S. 308-310. https://doi.org/10.2214/AJR.18.20509

Barredo-Arrieta, Alejandro et al. (2020): Explainable artificial intelligence (XAI). Concepts, taxonomies, opportunities and challenges toward responsible AI. In: Information Fusion 58, S. 82-115. https://doi.org/10.1016/j.inffus.2019.12.012 Barter, Christine; Renold, Emma (1999): The use of vignettes in qualitative research. In: Social Research Update 25. Online verfügbar unter https://sru.soc. surrey.ac.uk/SRU25.html, zuletzt geprüft am 11. 01. 2024.

- Becker, Aliza (2019): Artificial intelligence in medicine. What is it doing for us today? In: Health Policy and Technology 8 (2), S. 198-205. https://doi.org/ 10.1016/  j.hlpt.2019.03.004
- Bossen, Claus; Pine, Kathleen (2023): Batman and Robin in healthcare knowledge work. Human-AI collaboration by clinical documentation integrity specialists. In: ACM Transactions on Computer-Human Interaction 30 (2), S. 1-29. https:// doi.org/10.1145/3569892
- Čartolovni, Anto; Tomičić, Ana; Lazić Mosler, Elvira (2022): Ethical, legal, and   social considerations of AI-based medical decision-support tools. A scoping review. In: International Journal of Medical Informatics 161, S. 104 738. https://doi.org/ 10.1016/j.ijmedinf.2022.104738
- Dickel, Sascha (2021): Wenn die Technik sprechen lernt. Künstliche Kommunikation als kulturelle Herausforderung mediatisierter Gesellschaften. In: TATuP Zeitschrift für Technikfolgenabschätzung in Theorie und Praxis 30 (3), S. 23-29. https://doi.org/10.14512/tatup.30.3.23
- Feldhus, Nils; Wang, Qianli; Anikina, Tatiana; Chopra, Sahil; Oguz, Cennet; Möller, Sebastian (2023): InterroLang. Exploring NLP models and datasets through dialogue-based explanations. In: Houda Bouamor, Juan Pino und Kalika Bali (Hg.): Findings of the Association for Computational Linguistics: EMNLP 2023. Singapur: Association for Computational Linguistics, S. 5399-5421. https://doi. org/  10.18653/v1/2023.findings-emnlp.359
- Gupta, Akshit; Basu, Debadeep; Ghantasala, Ramya; Qiu, Sihang; Gadiraju, Ujwal (2022): To trust or not to trust. How a conversational interface affects trust in a decision support system. In: Frédérique Laforest et al. (Hg.): Proceed  ings of the ACM Web Conference 2022. New York, NY: Association for Computing Machinery, S. 3531-3540. https://doi.org/  10.1145/  3485447.3512248
- HEG-KI - Hochrangige Expertengruppe für KI (2019): Ethik-Leitlinien für eine vertrauenswürdige KI. Brüssel: Europäische Kommission. https://doi.org/  10.2759/ 22710
- Heil, Reinhard et al. (2021): Artificial intelligence in human genomics and biomedicine. Dynamics, potentials and challenges. In: TATuP - Zeitschrift für Technikfolgenabschätzung in Theorie und Praxis 30 (3), S. 30-36. https:// doi.org/  10.14512/tatup.30.3.30
- Hillmann, Stefan; Möller, Sebastian; Michael, Thilo (2021): Towards speech-based interactive post hoc explanations in explainable AI. In: Astrid   Carolus, Carolin Wienrich und Ingo Siegert (Hg.): Proceedings of the 1 st AI-  Debate Workshop. Magdeburg: Universität Magdeburg, pp. 13-15. http://dx.doi.org/ 10.25673/38473
- Hughes, Rhidian; Huby, Meg (2004): The construction and interpretation of vignettes in social research. In: Social Work and Social Sciences Review 11 (1), S. 36-51. https://doi.org/10.1921/swssr.v11i1.428
- Jenkins, Nicholas; Bloor, Michael; Fischer, Jan; Berney, Lee; Neale, Joanne (2010): Putting it in context. The use of vignettes in qualitative interviewing. In: Qualitative Research 10 (2), S. 175-198. https://doi.org/10.1177/1468794109356737

Kinar, Yaron et al. (2017): Performance analysis of a machine learning flagging system used to identify a group of individuals at a high risk for   colorectal cancer. In: PLoS One 12 (2), S. e0171759. https://doi.org/10.1371/journal.pone. 0171759

Kloker, Anika; Fleiß, Jürgen; Koeth, Christoph; Kloiber, Thomas; Ratheiser, Patrick; Thalmann, Stefan (2022): Caution or trust in AI? How to design XAI in sensitive use cases? In: AMCIS 2022 Proceedings 16.

Kosow, Hannah; Gaßner, Robert (2008): Methoden der Zukunfts- und Szenarioanalyse. Überblick, Bewertung und Auswahlkriterien. Berlin: Institut für Zukunftsstudien und Technologiebewertung. Online verfügbar unter https:// www.researchgate.net/publication/262198781\_Methoden\_der\_Zukunfts-und\_ Szenarioanalyse\_Uberblick\_Bewertung\_und\_Auswahlkriterien, zuletzt geprüft am 15. 01. 2024.

Mahmood, Amama; Huang, Chien-Ming (2022): Effects of rhetorical strategies and skin tones on agent persuasiveness in assisted decision-making. In: Carlos Martinho, João Dias, Joana Campos und Dirk Heylen (Hg.): Proceedings of the 22 nd  ACM International Conference on Intelligent Virtual Agents. New York, NY: Association for Computing Machinery, S. 1-8. https://doi.org/ 10.1145/3514197.3549628

Markus, Aniek; Kors, Jan; Rijnbeek, Peter (2021): The role of explainability in creating trustworthy artificial intelligence for health care. A   comprehensive survey of the terminology, design choices, and evaluation strategies. In: Journal of Biomedical Informatics 113, S. 103 655. https://doi.org/10.1016/j.jbi. 2020.103655

Meuser, Michael; Sackmann, Reinhold (1992): Zur Einführung. Deutungsmusteransatz und empirische Wissenssoziologie. In: Michael Meuser und Reinhold Sackmann (Hg.): Analyse sozialer Deutungsmuster. Beiträge zur empirischen Wissenssoziologie. Pfaffenweiler: Centaurus-Verlagsgesellschaft, S. 9-37.

Miller, Tim (2019): Explanation in artificial intelligence. Insights from the   social sciences. In: Artificial Intelligence 267, S. 1-38. https:/ /doi.org/10.1016/j.artint. 2018.07.007

Panch, Trishan; Mattie, Heather; Atun, Rifat (2019): Artificial intelligence and algorithmic bias. Implications for health systems. In: Journal of Global Health 9 (2), S. 010 318. https:/ /doi.org/10.7189/jogh.09.020318

Pentzold, Christian; Bischof, Andreas; Heise, Nele (2018): Einleitung. Theoriegenerierendes empirisches Forschen in medienbezogenen Lebenswelten. In: Christian Pentzold, Andreas Bischof und Nele Heise (Hg.): Praxis grounded theory. Theoriegenerierendes empirisches Forschen in medienbezogenen Lebenswelten. Wiesbaden: Springer, S. 1-24. https://doi.org/10.1007/978-3-65815999-3\_1

Powell, John (2019): Trust me, I'm a chatbot. How artificial intelligence in health care fails the Turing Test. In: Journal of Medical Internet Research 21 (10), S. e16222. https://doi.org/10.2196/16222

Samhammer, David et al. (2023): Klinische Entscheidungsfindung mit Künstlicher Intelligenz. Ein interdisziplinärer Governance-Ansatz. Heidelberg: Springer. https://doi.org/10.1007/978-3-662-67008-8

Scharowski, Nicolas; Perrig, Sebastian; Svab, Melanie; Opwis, Klaus; Brühlmann, Florian (2023): Exploring the effects of human-centered AI explanations on trust and reliance. In: Frontiers in Computer Science 5, S. 1-15. https://doi. org/  10.3389/fcomp.2023.1151150

MANUELA MARQUARDT

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

ist ausgebildete Techniksoziologin und   arbeitet seit 2019 als wissenschaftliche Mitarbeiterin an der   Charité zu   rehabilitationswissenschaftlichen Themen. Seit 2022 promoviert sie im Projekt MIA-PROM.

PHILIPP GRAF

ist Techniksoziologe und Doktorand an der TU Chemnitz zum Thema sozialer Robotik. Seit 2022 arbeitet er für die Hochschule München im Projekt MIA-PROM.

DR. EVA JANSEN

ist Medizinethnologin und seit 2021 am Institut für medizinische Soziologie und Rehabilitationswissenschaften der Charité.

DR. STEFAN HILLMANN

forscht und arbeitet seit 2010 zu   sprachgestützter Mensch-Maschine Interaktion. 2017 promovierte er zur Simulation von Nutzerverhalten bei Interaktionen mit multimodalen Dialogsystemen.   Aktuell forscht er am Einsatz von KI-Methoden zum Trainieren und Evaluieren von Dialogsystemen.

PROF. DR. JAN-NIKLAS VOIGT-ANTONS

ist seit 2021 Professor für Angewandte   Informatik mit dem Schwerpunkt immersive Medien an der Hochschule Hamm-Lippstadt. Er leitet das Immersive Reality Lab, welches sich auf die Erforschung von immersiven Anwendungen sowie auf Schnittstellen zwischen Menschen und Technik fokussiert.

<!-- image -->

## RESEARCH ARTICLE

## Artificial intelligence in melanoma diagnosis : Three scenarios, shifts in competencies, need for regulation, and reconciling dissent between humans and AI

Jan C. Zoellick  * , 1

<!-- image -->

, Hans Drexler 2

Abstract · Tools based on machine learning (so-called artificial intelligence, AI) are increasingly being developed to diagnose malignant melanoma in dermatology. This contribution discusses (1) three scenarios for the use of AI in different medical settings, (2) shifts in competencies from dermatologists to non-specialists and empowered patients, (3) regulatory frameworks to ensure safety and effectiveness and their consequences for AI tools, and (4) cognitive dissonance and potential delegation of human decision-making to AI. We conclude that AI systems should not replace human medical expertise but play a supporting role.  We identify needs for regulation and provide recommendations for action to help all (human) actors navigate safely through the choppy waters of this emerging market. Potential dilemmas arise when AI tools provide diagnoses that conflict with human medical expertise. Reconciling these conflicts will be a major challenge.

Künstliche Intelligenz in der Melanom-Diagnose : Drei Szenarien, Kompetenzverschiebungen, Regulierungsbedarf und Umgang mit Dissens zwischen Mensch und KI

* Corresponding author: jan.zoellick@charite.de

- 1 Institute of Medical Sociology and Rehabilitation Science, Charité Universitätsmedizin Berlin, corporate member of Freie Universität Berlin and Humboldt-Universität zu Berlin, Berlin, DE
- 2 Institut und Poliklinik für Arbeits-, Sozial- und Umweltmedizin, FAU Erlangen-Nürnberg, Erlangen, DE
- 3 Department for Dermatology, University Hospital Regensburg, Regensburg, DE

<!-- image -->

© 2024 by the authors; licensee oekom. This Open Access article is licensed under a Creative Commons Attribution 4.0 International License (CC BY). https://doi.org/10.14512/tatup.33.1.48

Received: 27. 07. 2023; revised version accepted: 15. 12. 2023;

published online: 15. 03. 2024 (peer review)

<!-- image -->

, Konstantin Drexler 3

<!-- image -->

Zusammenfassung · Für die Diagnose von malignen Melanomen in der Dermatologie werden zunehmend Instrumente entwickelt, die auf maschinellem Lernen (sogenannter künstlicher Intelligenz, KI) basieren. Dieser Beitrag diskutiert (1) drei Szenarien für den Einsatz von KI in verschiedenen medizinischen Bereichen, (2) Kompetenzverschiebungen von Dermatolog:innen zu Nicht-Spezialist:innen und mündigen Patient:innen, (3) regulatorische Rahmenbedingungen zur Gewährleistung von Wirksamkeit und Unbedenklichkeit und ihre Folgen für KI-Tools sowie (4) kognitive Dissonanz und potenzielle Delegation menschlicher Entscheidungen an KI. Wir kommen zu dem Schluss, dass KI-Systeme menschliche medizinische Expertise nicht ersetzen, sondern eine unterstützende Rolle spielen sollten. Wir zeigen Regulierungsbedarf auf und geben Handlungsempfehlungen, um alle (menschlichen) Akteur:innen dabei zu unterstützen, sicher durch die unruhigen Gewässer dieses neuen Marktes zu navigieren. Potenzielle Dilemmata entstehen, wenn KI-Tools Diagnosen liefern, die im Widerspruch zur menschlichen medizinischen Expertise stehen. Diese Konflikte zu lösen, wird eine große Herausforderung sein.

Keywords · melanoma, diagnosis, artificial intelligence, patientdoctor relationship, diagnostic accuracy

This article is part of the Special topic 'AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?,' edited by D. Schneider and K. Weber. https://doi.org/10.14512/tatup.33.1.08

## Background

Malignant melanoma is a skin tumour that accounts for 80 % of deaths from skin cancer (Saginala et al. 2021). Early detection is most important for the prognosis of the patients. The diagnostic  procedure typically involves clinical history, visual inspec- https://doi.org/10.14512/tatup.33.1.48

<!-- image -->

<!-- image -->

tion, and dermatoscopy (examination of a skin lesion by using an epiluminescence microscope). In cases where there is suspicion, the mole is surgically removed and sent for histopathological testing to a specialist laboratory to determine malignancy. This standardized procedure with clear diagnostic outcomes in suspicious cases is an ideal dataset for training and utilizing machine learning tools (so-called artificial intelligence, AI). AI in our understanding is a computational approach that uses knowledge gained from training cases to identify patterns and make predictions from input data. Specialized AI for dermatological image recognition  has  surpassed  the  performance  in  identifying skin cancer compared to human dermatologists (Esteva et al. 2017; Pham et al. 2021). In their review of 272 studies Jones et al. (2022) found solid performance of AI systems (89 % accuracy; 95 % Confidence Intervals: 60 %-100 %), which suggests that patients and clinicians can expect AI to be an asset for diagnosing melanoma. However, none of the studies included acceptance measures on the part of clinicians or patients demonstrating a limitation in the current approach towards technology assessment of AI in diagnostics. The controlled experimental findings should additionally be validated in medical practice to arrive at a realistic assessment of performance and practical fit. This is particularly relevant for a stringent technology assessment that evaluates AI in diagnostics in practical settings. While technology assessment studies for AI diagnostics exist (Schreier et al. 2020; Schwendicke et al. 2021), their underlying frameworks oftentimes neglect categories specific for AI technologies such as cybersecurity and explainability (Farah et al. 2023).

With this conceptual contribution we aim to shed light on four dimensions of AI employment in melanoma diagnosis, i. e., possible futures, social impacts, regulatory options, and ethical conundrums and agency constellations to inform technology assessment researchers about further areas for assessing AI systems in (melanoma) diagnostics along relative axes of analyses. We aim to provide insights for the following questions, framing our discussions within the German healthcare and regulatory system:

1. Possible futures: What role can AI play in diagnosing melanoma?
2. Social impacts: What competencies and changes in roles follow the comprehensive introduction of AI in diagnostics?
3. Regulatory options: Where is AI located between legal regulations and medical evidence-based guidelines?
4. Ethical conundrums and agency constellations: Who decides regarding diagnosis of melanoma - AI or human?

## Methods

We conducted a narrative review of the literature on AI in melanoma diagnosis focusing on the aspects 1) scenarios, 2) social impacts,  3)  regulatory  options,  and  4)  ethical  facets.  Accordingly, we used the following search term to find articles in the databases PubMed and Google Scholar: [ Artificial intelligence

OR AI ]  AND [ melanoma ]  AND [ diagnos* ]  AND [[ Scenario OR vision OR future ] OR [ social ] OR [ legal OR regulat* ] OR [ ethic* ] OR [ ELSI ]]. We removed duplicates, screened the remaining articles, and selected the most fitting ones for the four topics based on our expertise. We also screened reference lists of the selected articles to find additional sources. For the scenarios, we conducted an initial brainstorming to develop scenarios as impulses for possible futures and then consulted further literature for details or contrasts.

## Results

## Possible futures

AI diagnosis  can  be  applied  in  different  settings  by  different stakeholders. We will focus on the following three scenarios as they cover the outpatient and inpatient healthcare provision in Germany as well as self-administered care outside professional work: (1) second opinion for a dermatologist in outpatient care, (2) triage and prioritization within a dermatologist clinic, and (3) patient self-monitoring.

## Scenario 1: Second opinion in outpatient care

In this scenario, dermatologists upload pictures of suspicious moles to an AI database to obtain a second opinion. The system extracts relevant image features, compares them to a database of expert annotations, and generates a second opinion report highlighting potential areas of concern and providing a diagnosis. As such, the AI system complements the initial human assessment providing an additional layer of confidence reducing diagnostic errors. This scenario follows the interaction mode between clinicians and their machines described by Braun et al. (2021). Such a second opinion system might however transform over time: In a first step, time-restrained dermatologists realize that the AI tools provide (1) dissenting and potentially more reliable  diagnoses (2) more efficiently than  they  can.  These  attributions could make the AI-based second opinion the first or only opinion. Consequently, dermatological expertise might be removed from the process and tasks regarding diagnosis delegated to non-dermatologists, e.g., to medical technical staff. As a result, dermatological hegemony in diagnosing melanoma is challenged. Companies offering AI tools might target general practitioners providing AI-based dermatological expertise. As such, a system initially providing a second opinion to specialist  doctors  ultimately  might  lead  to  spreading  dermatological expertise across disciplines whilst removing specialist doctors from this task.

## Scenario 2: Triage in a dermatologist clinic

In  this  scenario,  the  AI  system  extends  the  process  of  prioritizing  patients  in  a  dermatology  clinic.  When  patients  arrive for skin examinations, images of their moles are captured and fed into the AI system. The algorithm compares the images to a  database  of  annotated  melanoma  cases  and  provides  a  risk

assessment score for each mole. Based on this score, the system  prioritizes  patients,  flagging  those  with  higher  risks  for immediate attention by dermatologists. The specialist doctors can  then  disregard  those  moles  deemed  benign  by  the  AI  focusing on the prioritized cases. This generates efficiency gains needed in a strained system: Already today, waiting times for outpatient dermatological appointments last 4.9 weeks with urban-rural variations (Krensel et al. 2015). Until 2035, the number  of  German  regions  underserved  or  without  any  dermatological specialists are expected to increase by 129 % and 700 %, respectively, in a forecast with moderate demographic changes (Kis et al. 2017).

## Scenario 3: Patient self-monitoring

In this scenario, medical laypeople regularly use an AI smartphone app to monitor their moles instead of screening in outpatient dermatological care. Instructed by the app, users capture standardized images of their moles and upload them to a database. The app compares the images to a database of annotated melanoma cases and provides a risk assessment for each mole together with recommendations for further action. Such a scenario  follows  the  interaction  mode between  patients  and  machines (Braun  et  al.  2021).  AI-supported  self-monitoring  empowers users to participate actively in their skin health and facilitates early detection of melanomas, potentially leading to timely medical intervention and improved health outcomes. However, from patients and doctors alike. Second, AI tools need to perform reliably with sufficient specificity and sensitivity. Third, regulatory assurance must be provided to enable the use and billing  of  AI  tools  as  medical  services.  Regulatory  guidance would also be the basis for assessing harm caused by the tools' appraisal systems, i. e., false positive or false negative diagnoses or prioritizing the wrong patients. Finally, it is important to acknowledge that imaging represents only one facet of melanoma differential diagnosis next to anamnestic conversation about progression of the mole's appearance, itching, and bleeding.

## Social impacts

Social impacts of AI technology in melanoma diagnosis vary between individual and societal perceptions of the medical profession. Both competencies of dermatologists and the framework of evidence-based medicine are being scrutinized. Patients might be - or at least feel - empowered and informed about their health. Those impacts have consequences for the configuration of the patient-doctor relationship.

## Competencies and expertise

Technology  has  played  a  crucial  role  in  diagnosing  medical conditions.  X-rays,  magnetic  resonance  imaging,  and  electroencephalography all offer literal insights into the human body and aid in diagnostic procedures across medical disciplines such as orthopedics, neurology, or oncology. These technologies are

## Through AI, the ideal of the 'informed patient' might actualize with increased adherence and responsibility for the patient's own health.

self-empowerment oftentimes coincides with personal responsibility that necessitates the willingness of patients and their acceptance  of  new  technologies.  With  9.6  annual  consultations per capita, Germany has the second most doctor consultations in the EU (OECD 2023). Thus, the German healthcare system relies heavily on the trust relationship between patients and doctors. Shifting health responsibility from the patient-doctor dyad towards the patient-machine interaction might encounter barriers of acceptance.

## Implications

The three possible futures answer in varying constellations current debates in healthcare provision characterized by resource scarcity. AI tools potentially provide efficiency gains by automating processes - diagnosing time-consuming difficult cases (scenario 1), prioritizing patients (scenario 2), or providing initial appraisal for patients (scenario 3).

Three conditions seem to be necessary for successful implementation. First, all scenarios assume acceptance for AI tools primarily used as tools expanding the diagnostic repertoire of medical professionals. Within this history, AI serves as a continuation of established procedures integrating technology into diagnostics.

However, the distinction between AI systems and other technologies lies in the attribution of expertise. Traditional imaging techniques do not offer diagnoses directly, but provide data to be interpreted by a human expert . AI instead offers its own diagnosis usually accompanied by a reliability or confidence score. Yet, human actors - medical professionals and IT experts alike are usually unable to retrace the AI analyses, as the weights of the different nodes or the dataset used for training the model are mostly unavailable. This necessitates trust in the outcome and the accuracy of the reliability score. As in Ellul's self-perpetuating,  efficiency-driven  technological  society,  dermatologists will 'be confined to the role of a recording device; [they] will note the effects of techniques upon one another, and register the results' (Ellul 1964, p. 93). As in scenario 1, this trajectory does not displace human actors from the process of diag-

nosis, but it shifts the competencies needed - '[h]uman beings are, indeed, always necessary. But literally anyone can do the job, provided he is trained to it. Henceforth, men will be able to act only in virtue of their commonest and lowest nature, and not in virtue of what they possess of superiority and individuality' (Ellul 1964, pp. 92-93).

## Evidence-based medicine scrutinized

The inherent complexity of AI tools contributes to their intrigue, as they evoke the notion of fortune telling, a topos deeply ingrained in human imaginaries. Examples of entities with predictive  capabilities  include  the  ancient  oracles  in  Delphi  or

## The patient-doctor relationship

With the advent of AI systems, potentially life-threatening diagnoses are presented to patients with little contextualization using incomprehensible methods. Unsettled patients then consult doctors who are tasked with managing information whose origin they cannot reproduce or comprehend. Efforts to make AI analyses explainable could lead to more transparency and understanding for human patients and doctors alike (WHO 2021). Currently, however, opaque analytical processes prevail in AI systems. Findings about the patient-doctor relationship illustrate that  healthcare  provision  is  more  than  the  communication  of facts (Ridd et al. 2009). Rather, reciprocal trust and empathic

## In this crucial moment regulators should align themselves with these developments and shape the legal landscape concerning safe and effective AI technology.

Cichyrus, the prophecy about the chosen one in the Harry Potter series, or the clairvoyant 'precogs' in The Minority Report. An AI system capable of providing believable predictions bears similarities to a technical version of these transcendent revelations from mythological narratives. Returning to such narratives stands in contrast to evidence-based medicine that demonstrated its effectiveness by achieving better health outcomes (e.g., taming deadly diseases or raising life expectancy) in an understandable, reproducible way. As such, medical professions are faced with a difficult task to reconcile their success using reproducible, experimental methods with novel technologies outperforming humans in certain confined tasks using inscrutable computational methods. In the light of the success rates in image recognition, individual dermatologists understandably start to question their own expertise.

## Patient empowerment

Where medical professionals might struggle with shifts in competencies, patients might welcome such a transformation. With the introduction of AI tools, the scarce resource of specialist medical expertise becomes omnipresent in their pockets. Scenario 3 mentioned above demonstrates how AI tools might enhance  patients'  perceived  self-efficacy,  health  literacy,  and health outcomes.

However, the success of such a transformation depends on both the system's accuracy and the users' expectations. Sensitivity and specificity as indicators for accuracy need to reach high thresholds, and patients' performance and effort expectancies must be met for AI tools to unfurl their potential (Venkatesh 2022). Technical and user mistakes can create an erroneous impression of safety to the detriment of the patient. In that sense, a wrongly applied AI tool resembles an FFP-2 mask covering the mouth but not the nose.

communication are relevant vehicles to generate better health outcomes for the patient (Chandra et al. 2018). Indeed, analyses of text responses from doctors compared to AI text generation in online forums indicate better quality and more empathy in the AI responses (Ayers et al. 2023). However, human-human relationships with regard, trust, and empathy might be preferable depending on the cultural context.

## Implications

As a social impact, competencies potentially shift in several directions. The competency of diagnosing melanoma based on images might shift from dermatologists to technical staff equipped with AI tools. This process frees dermatologists' resources for other aspects of differential diagnosis where further AI tools might be utilized (see scenarios 1 and 2 above) or lays off abundant dermatologists. AI could thus extend or replace human dermatological expertise. On another dimension, patients might enhance their health literacy using AI systems (Au et al. 2023). Literate patients encounter medical professionals at eye level thereby actualizing the ideal of an informed patient. However, transparency and explainable systems are necessary (Bjerring and Busch 2021). Otherwise, the system becomes a threat towards the ideal of human-centered reproducible and understandable science as well as patient-centered care (Bjerring and Busch 2021).

## Regulatory options

In current regulatory practice, the two principles of harmlessness and effectiveness are used to assess the impact of novel interventions, e.g., in form of devices or medication. Regulations on medical devices (e.g., the EU Regulation 2017/745, or the German Medizinproduktegesetz ) generally require producers to demonstrate the harmlessness of their products for patients or in case of expected harm (e.g., in radiation therapy) a risk miti-

gation and reduction strategy. In contrast, licensing for medication follows the framework of effectiveness in a series of medical studies determining a safe clinical dose (phase I), assessing side effects and efficacy (phase II), and ultimately demonstrating effectiveness (phase III) (Müllner 2005). Substances not demonstrating effectiveness can still be marketed, but under different legal frameworks as cosmetics or foods, not as medications.

With diverse pathways to choose, it is not surprising that AI companies pursue different legal strategies. Some AI tools for dermatological diagnosis already underwent the medical devices path of demonstrating harmlessness (e.g., A.S.S.I.S.T. (OnlineDoctor 2022)). Others are marketing their products as providing simple non-medical services 'not intended to perform diagnosis, but rather to provide users the ability to image, track, and better understand their moles' (AI Dermatologist 2023). These are  strategies  rather  common  in  emerging  markets.  However, regulators  should  be  aware  of  potential  impacts  applying  the principles of harmlessness vs. effectiveness. When assessing AI performance, established parameters such as sensitivity, specificity, and precision should be complemented by a critical appraisal of biases and risks of the respective learning cycles and databases (Wehkamp et al. 2023). In this crucial moment regulators  should  align  themselves  with  these  developments  and shape the legal landscape concerning safe and effective AI technology.

Besides legal regulation, medical guidelines ( Leitlinien ) systematically synthesize current knowledge based on clinical evidence. Balancing harmlessness and effectiveness, they give recommendations for action to medical practitioners without being legally binding. AI tools are currently not part of medical guidelines. However, given promising experimental results, guideline developers soon need to adopt a stance on AI tools. Here, critical assessment of the evidence is an important first step for including or excluding AI tools from recommendations. Successful RCT studies in image recognition should be validated in medical practice to arrive at a realistic assessment of performance and practical fit. With AI tools discussed in medical guidelines, clini-

Childress 2001). Complying with these principles is paramount for AI systems to integrate well into the healthcare system. For instance,  enhancing  autonomy  means  that  patients  should  be given a choice to agree or disagree with the use of AI systems in their diagnosis procedure without negative consequences, i. e., higher health insurance premiums that would shift the burden of responsibility solely towards the patients and challenge the solidarity principle in health insurance (Böning et al. 2019). Fairness in this context would mean equal access to enhanced diagnostic procedures such as AI (WHO 2021). In the following, we will focus on responsibility, particularly regarding doctors and their decisions since diagnosing is primarily a task for medical professionals.

Legally and ethically, doctors are responsible for their medical decisions, and they are held accountable for malpractice and negligence. This strong belief in assuming responsibility stands in contrast with the opaque and thus fascinating nature of AI systems outlined above. Conflicts arise when an AI system provides a different interpretation compared to the dermatologist. With responsibility clearly being attributed to the human actor, the dermatologist is faced with the difficult task to reconcile their own beliefs with discordant input from the AI system. Figure 1 shows a contingency table with the dermatologist's and the AI's diagnosis expanding the ethical discussion by Tupasela and Di Nucci (2020) with a temporal dimension.

The concordant cases are straightforward. In dissenting cases, the serious diagnosis melanoma is likely dominant and guiding in the first instance, irrespective of the information source. The responsible dermatologist will likely escalate the diagnostic process 'to be on the safe side' and excise the suspicious skin lesion. This leads to a general increase in operations and a consequent influx in associated healthcare costs in a field where already only 1 in 10 operations identifies a case of disease (Petty et al. 2020). Given a learning curve with the AI system assumed to be even slightly more accurate than the dermatologist, the AI system's appraisal over time becomes the dominant assessment irrespective of the diagnosis .  In  that  case,  the  dermatologist's cians will have more guidance to include

or  willfully  exclude  them  in  their  practice. Recommending AI in medical guidelines  would also entail clinical  malpractice not to use the tools unless the patient agrees with below standard care (Thissen 2021). After all, responsibility for medical interventions lies with the human doctor and the informed patient as scenarios 1 and 3 show.

## Ethical conundrums and agency constellations

Beneficence and non-maleficence, autono  my, fairness, and responsibility are among the  guiding  ethical  principles  discussed in healthcare provision (  Beauchamp and

Fig. 1: Contingency table with human dermatologist and AI system as actors diagnosing a mole as either melanoma or as harmless. In the top left and bottom right cell both actors agree. In the top right cell in a first instance the more dangerous diagnosis melanoma takes precedence. As the AI system proves more reliable than the dermatologist over time this diagnosis is upheld. In the bottom left cell, the more dangerous diagnosis melanoma also takes precedence. However, this is overwritten later by the AI system. Both dissenting cells create cognitive dissonance for the dermatologist. Source: authors' own compilation

<!-- image -->

|               | Human dermatologist                              | Human dermatologist                         |
|---------------|--------------------------------------------------|---------------------------------------------|
| Diagnosis     | Melanoma                                         | Harmless mole                               |
| Melanoma      | Concordance melanoma                             | Dissent Initially: melanoma Later: melanoma |
| Harmless mole | Dissent Initially: melanoma Later: harmless mole | Concordance harmless mole                   |

competence is depreciated, they are 'confined to the role of a recording  device'  (Ellul  1964,  p. 93),  however,  whilst  assuming responsibility, liability, and accountability for the decision. Ethical  conundrums  arise  challenging  the  agency  of  the  dermatologists who do not fully understand the AI 'decision-making' process. Explainability and critical reflection of AI tools are necessary for the dermatologists to experience self-efficacy and interpret the results adequately (Bjerring and Busch 2021). This is particularly the case, as many profit-oriented companies might sense a chance in this emerging market by conveying the appearance of a functioning and accurate system.

## Conclusion

We conclude that AI tools for diagnosing melanoma potentially provide convincing benefits for the healthcare system in terms of efficiency gains, more adequate resource allocation, health literacy and empowerment for patients, or more accurate diagnoses and better health outcomes. Promising experimental results must be validated in clinical practice. Three scenarios demonstrated applications of AI tools for diagnosing melanoma in different settings for different purposes.

However,  the  following  necessary  conditions  must  be  fulfilled: AI tools must perform reliably with sufficient specificity and sensitivity; they must be transparent regarding the analytical processes and outcomes; and they must be accepted by patients, doctors, and other stakeholders. Moreover, AI tools need to adhere to ethical values of beneficence, non-maleficence, autonomy, fairness, and responsibility to protect dignity of all human actors involved. That includes AI tools being a support rather than replacement for human actors. Even considering all aspects above, cognitive dissonance in decision-making and competency shifts for dermatologists have to be expected particularly when the AI systems demonstrate superiority.

Based on these analyses we suggest technology assessment studies for AI in diagnostics to analyze the application contexts and  consequences  for  the  multiple  stakeholders  involved.  Experimental studies focusing on performance should be complemented by observation  studies  in  realistic  settings  of  clinical practice. Regarding regulation, a nuanced debate about the underlying frameworks and an analysis of their consequences for accepting AI in diagnostics is needed.

Funding •

Competing interests •

This work received no external funding. The authors declare no competing interests.

## References

- AI Dermatologist (2023): AI Dermatologist Skin Scanner. Available online at https://ai-derm.com/, last accessed on 03. 01. 2024.
- Au, Jessica; Falloon, Caitlin; Ravi, Ayngaran; Ha, Phil; Le, Suong (2023): A betaproto  type chatbot for increasing health literacy of patients with decompensated cirrhosis. Usability study. In: JMIR Human Factors 10 (1), p. e42506. https://doi.org/10.2196/42506

Ayers, John et al. (2023): Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media   forum. In: JAMA internal medicine 183 (6), pp. 589-596. https://doi.org/10.1001/ jamainternmed.2023.1838

Beauchamp, Tom; Childress, James (2001): Principles of biomedical ethics. Oxford: Oxford University Press.

Bjerring, Jens; Busch, Jacob (2021): Artificial intelligence and patient-centered decision-making. In: Philosophy &amp; Technology 34, pp. 349-371. https://doi.org/ 10.1007/s13347-019-00391-6

Böning, Sarah-Lena; Maier-Rigaud, Remi; Micken, Simon (2019): Gefährdet die Nutzung von Gesundheits-Apps und Wearables die solidarische Krankenversicherung? Bonn: Friedrich Ebert Stiftung. Available online at https:// library.fes.de/pdf-files/wiso/15883.pdf, last accessed on 03. 01. 2024.

- Braun, Matthias; Hummel, Patrik; Beck, Susanne; Dabrock, Peter (2021): Primer on an ethics of AI-based decision support systems in the clinic. In: Journal of Medical Ethics 47 (12), p. e3 https:/ /doi.org/10.1136/ medethics-2019-105860

Chandra, Swastika; Mohammadnezhad, Masoud; Ward, Paul (2018): Trust and communication in a doctor-patient relationship. A literature review. In: Journal of Healthcare Communications 3 (3), pp. 1-6. https://doi.org/10.4172/ 2472-1654.100146

Ellul, Jacques (1964): The technological society. New York, NY: Knopf. Esteva, Andre et al. (2017): Dermatologist-level classification of skin cancer with deep neural networks. In: Nature 542 (7639), pp. 115-118. https://doi.org/ 10.1038/nature21056

- Farah, Line; Davaze-Schneider, Julie; Martin, Tess; Nguyen, Pierre; Borget,   Isabelle; Martelli, Nicolas (2023): Are current clinical studies on artificial intelligencebased medical devices comprehensive enough to support a full health technology assessment? A systematic review. In: Artificial Intelligence in Medicine 140, p. 102 547. https://doi.org/10.1016/j.artmed.2023.102547
- Jones, Owain et al. (2022): Artificial intelligence and machine learning algorithms for early detection of skin cancer in community and primary care settings. A systematic review. In: The Lancet Digital Health 4 (6), pp. e466-e476. https:// doi.org/10.1016/S2589-7500(22)00023-1
- Kis, Anne; Augustin, Matthias; Augustin, Jobst (2017): Regionale fachärztliche Versorgung und demographischer Wandel in Deutschland - Szenarien zur dermatologischen Versorgung im Jahr 2035. In: Journal der Deutschen Dermatologischen Gesellschaft 15 (12), pp. 1199-1210. https://doi.org/10.1111/ ddg.13379\_g
- Krensel, Magdalene; Augustin, Matthias; Rosenbach, Thomas; Reusch, Michael (2015): Wartezeiten und Behandlungsorganisation in der Hautarztpraxis. In: Journal der Deutschen Dermatologischen Gesellschaft 13 (8), pp. 812-814. https://doi.org/10.1111/ddg.80\_12625
- Müllner, Marcus (2005): Die Zulassung von Medikamenten (und anderen Medizinprodukten). Good Clinical Practice. In: Marcus Müllner: Erfolgreich wissenschaftlich arbeiten in der Klinik. Evidence Based Medicine. Wien: Springer, pp. 243-249. https://doi.org/10.1007/3-211-27476-6\_33
- OECD - Organization for Economic Co-operation and Development (2023): Doctors' consultations (indicator). https://doi.org/10.1787/173dcf26-en OnlineDoctor (2022): OnlineDoctor übernimmt KI-Startup A.S.S.I.S.T. Available online at https://www.onlinedoctor.de/pressemitteilung/onlinedoctoruebernahme-ki-startup/, last accessed on 03. 01. 2024.
- Petty, Amy et al. (2020): Meta-analysis of number needed to treat for diagnosis of melanoma by clinical setting. In: Journal of the American

Academy of Dermatology 82 (5), pp. 1158-1165. https://doi.org/10.1016/j.jaad. 2019.12.063

Pham, Tri-Cong; Luong, Chi-Mai; Hoang, Van-Dung; Doucet, Antoine (2021): AI   outperformed every dermatologist in dermoscopic melanoma diagnosis, using an optimized deep-CNN architecture with custom mini-batch logic and loss function. In: Scientific Reports 11 (1), p. 17 485. https://doi.org/10. 1038/s41598-021-96707-8

Ridd, Matthew; Shaw, Alison; Lewis, Glyn; Salisbury, Chris (2009): The patientdoctor relationship. A synthesis of the qualitative literature on patients' perspectives. In: British Journal of General Practice 59 (561), pp. e116-e133. https://doi.org/10.3399/bjgp09X420248

Saginala, Kalyan; Barsouk, Adam; Aluru, John; Rawla, Prashanth; Barsouk, Alexander (2021): Epidemiology of melanoma. In: Medical Sciences 9 (4), p. 63. https://doi.org/10.3390/medsci9040063

Schreier, Jan; Genghi, Angelo; Laaksonen, Hannu; Morgas, Tomasz; Haas,   Benjamin (2020): Clinical evaluation of a full-image deep segmentation algorithm for the male pelvis on cone-beam CT and CT. In: Radiotherapy and Oncology 145, pp. 1-6. https://doi.org/10.1016/j.radonc.2019.11.021

Schwendicke, Falk et al. (2021): Cost-effectiveness of artificial intelligence for proximal caries detection. In: Journal of Dental Research 100 (4), pp. 369-376. https://doi.org/10.1177/0022034520972335

Thissen, Christina (2021): KI auf dem Weg zum Facharztstandard - nicht ohne Haftungsprophylaxe. In: Radiologen WirtschaftsForum 12, pp. 7-8.   Available online at https://www.rwf-online.de/system/files/RWF\_12\_2021.pdf, last accessed on 03. 01. 2024.

Tupasela, Aaro; Di Nucci, Ezio (2020): Concordance as evidence in the Watson for Oncology decision-support system. In: Ai &amp; Society 35, pp. 811-818. https:// doi.org/10.1007/s00146-020-00945-9

Venkatesh, Viswanath (2022): Adoption and use of AI tools. A research agenda grounded in UTAUT. In: Annals of Operations Research 308 (1), pp. 641-652. https://doi.org/10.1007/s10479-020-03918-9

Wehkamp, Kai; Krawczak, Michael; Schreiber, Stefan (2023): The quality and utility of artificial intelligence in patient care. In: Deutsches Ärzteblatt International 120, pp. 463-469. https://doi.org/10.3238/arztebl.m2023.0124

WHO - World Health Organization (2021): Ethics and governance of   artificial intelligence for health. WHO guidance. Geneva: World Health Organization. Available online at https://iris.who.int/bitstream/handle/  10665/  341996/ 9789240029200-eng.pdf, last accessed on 03. 01. 2024.

<!-- image -->

<!-- image -->

<!-- image -->

DR. JAN C. ZÖLLICK

has been a researcher at the Institute of   Medical Sociology and Rehabilitation Science at   Charité Universitätsmedizin Berlin since 2017. He conducts research and teaches about the opportunities and risks of technological innovations for healthcare.

## PROF. DR. HANS DREXLER

has been the Chair for Occupational, Social, and Environmental Medicine since 2000. He is a dermatologist and occupational physician. He focuses on the social-medical consequences of malignant skin tumors.

DR. KONSTANTIN DREXLER

has been working as a physician at the Clinic and Polyclinic for Dermatology at the University   Hospital Regensburg since 2016. His professional and scientific focus is on the diagnosis and treatment of   various skin tumors.

<!-- image -->

<!-- image -->

## RESEARCH ARTICLE

## Dekarbonisierung des Verkehrssektors in Berlin : Bürgerinnen- und Bürgergutachten zu wissenschaftlich erstellten Szenarien

<!-- image -->

Moritz Kreuschner  * , 1  , Nora Bonatz 2  , Tilmann Schlenther 1  , Hamid Mostofi 2  , Hans-Liudger Dienel 2  , Kai Nagel 1 

<!-- image -->

Zusammenfassung · Im  Rahmen  dieser  Arbeit  wurde  ein  Bürger:innenrat eingerichtet, um Feedback zu Maßnahmen zur Dekarbonisierung des Berliner Verkehrssektors zu erhalten. Das generelle Ziel der Dekarbonisierung fand breite Unterstützung. Auf der Grundlage bereits vorliegender Studien wurden jeweils Diskussionen für den privaten Personenverkehr,  den  kommerziellen  Personenverkehr,  den  Güterverkehr  und  Sonderverkehre geführt.  Für  alle  Segmente  mit  Ausnahme des privaten Personenverkehrs konnte eine Einigung über die Dekarbonisierungspfade  erzielt  werden.  Für  Letzteren  werden  Pull-Maßnahmen, wie zum Beispiel  Verbesserungen  des  öffentlichen  Nahverkehrs oder der Radwege, allgemein akzeptiert, was mit dem generellen Wunsch nach Umstellung auf nicht-fossile Antriebe und einer Gestaltung eines weniger autozentrierten Berlins einhergeht. Simulationen zeigen allerdings, dass diese Pull-Maßnahmen bei Weitem nicht ausreichen  werden,  um  den  privaten  Personenverkehr  zu  dekarbonisieren.  Wirksamere  Push-Maßnahmen, wie höhere Preise oder Verbote fossiler Fahrzeuge, erzielten für dieses Segment jedoch keine klaren Mehrheiten.

* Corresponding author: kreuschner@vsp.tu-berlin.de

1 F G Verkehrssystemplanung und Verkehrstelematik, Technische Universität Berlin, Berlin, DE

2 FG Arbeitslehre/Technik und Partizipation, Technische Universität Berlin, Berlin, DE

<!-- image -->

© 2024 by the authors; licensee oekom. This Open Access article is licensed under a Creative Commons Attribution 4.0 International License (CC BY). https://doi.org/10.14512/tatup.33.1.55

Received: 08. 09. 2023; revised version accepted: 19. 12. 2023;

published online: 15. 03. 2024 (peer review)

https://doi.org/10.14512/tatup.33.1.55

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

Decarbonization of Berlin's transport sector : Citizens' assembly on scientifically developed scenarios

Abstract · As part of this work, a citizens' assembly was set up to obtain feedback on measures to decarbonize Berlin's transport sector. The general goal of decarbonization was widely supported. Based on existing studies, discussions were held for private passenger transport, commercial passenger transport, freight transport, and all other transport modes. Agreement could be found on the decarbonization paths for all segments, with the exception of private passenger transport. For the latter, pull measures, like improvements of public transport or bike lanes, are generally accepted, going along with a general desire to switch to non-fossil fuels and to make the transport system in Berlin less car-oriented. Simulations show that pull measures will be far from sufficient to decarbonize private passenger transport. However, more effective push measures, such as higher prices or ban on fossil-fueled vehicles, did not yield clear majorities for this segment.

Keywords · citizens' assembly, Berlin's transport sector, non-fossil scenarios, decarbonization, defossilization

## Einleitung

Kommunale Klimabürger:innenräte haben sich in den letzten Jahren als deliberativ-demokratisches Instrument in Europa weit verbreitet. Das europäische Netzwerk KNOCA (2024) zeigt den Aufstieg der Klimabürgerräte eindrücklich. Im April 2021 fasste das Berliner Abgeordnetenhaus den Beschluss für einen Klimabürger:innenrat in Berlin 1 und vergab die Durchführung u. a. an

1 Wir schließen uns der von der Berliner Senatsverwaltung durchgehend verwendeten Terminologie 'Klimabürger:innenrat' an.

<!-- image -->

das Berliner nexus Institut, an dessen Leitung Autor H.-L. Dienel beteiligt ist. Im Rahmen von zehn Sitzungen, zwei davon zum Thema Mobilität, wurden mit zufällig ausgewählten Bürger:innen 47 Empfehlungen an die Politik ausgearbeitet und formuliert, von denen der Senat 42 im Klimaschutzprogramm aufgriff. Die Mobilitätsempfehlungen thematisieren sowohl die Stärkung des Umweltverbunds als auch die Abschwächung des Autoverkehrs. Konkrete Maßnahmenvorschläge umfassen niedrige ÖPNV-Ticketpreise, Radinfrastrukturausbau, Verknüpfung verschiedener Verkehrsformen und Preiserhöhung für das Parken von Autos (SenUVK 2023). Simulationsbasierte Ansätze zeigen allerdings, dass die gewählten Maßnahmen für eine vollständige Dekarbonisierung des Verkehrssystems nicht ausreichend sind.

rat: Pro Verkehrssegment gab es zunächst fachlichen Input, dann diskutierten die Bürgergutachter:innen in Gruppen, gefolgt von einer Schnellumfrage, die dann im Plenum diskutiert wurde. Daraus wurden im Nachgang zusammenfassende Aussagen generiert, welche den Bürgergutachter:innen zur Entscheidung vorgelegt wurden.

Die Diskussion über das Segment des privaten Personenverkehrs wurde dabei durch ein Dashboard unterstützt, welches die Wirkung unterschiedlicher Maßnahmenbündel bzgl. der Dimensionen CO2-Reduktion, Reduktion des fahrenden Verkehrs, Reduktion  des  stehenden  Verkehrs  sowie  zusätzliche  Ausgaben oder Einnahmen des Staates aufzeigte. Hierdurch wurden Aspekte des Decision-Theater-Ansatzes (Wolf et al. 2023) in den

## Partizipative Methoden ermöglichen es den Menschen, aktiv und einflussreich an Entscheidungen mitzuwirken, die ihr Leben betreffen.

Unabhängig  vom  Ansatz  des  Klimabürger:innenrats  untersucht das Projekt zeroCUTS (2018) seit 2018 mögliche Maßnahmen  zur  vollständigen  Dekarbonisierung  des  Verkehrssystems aus einer simulationsbasierten Sicht (Göhlich et al. 2021). Hierzu wird der urbane Verkehr in die vier Segmente privater Personenverkehr, kommerzieller Personenverkehr, kommerzieller Güterverkehr und sonstige Verkehre eingeteilt. Für jedes dieser Segmente werden mehrere Dekarbonisierungsstrategien definiert, und für diese werden Veränderungen der Kosten, der Reisezeiten etc. bewertet.

Dabei stellt sich zunächst für die letzten drei Segmente heraus,  dass  deren  Dekarbonisierung  bereits  mit  vorhandenen Technologien erreicht werden könnte, wobei die Kosten des Verkehrs in keinem der Segmente um mehr als 20 % ansteigen würden. Hier ist die Frage also, ob diese Kostensteigerung von 20 % akzeptabel wäre, und falls ja, wie die resultierenden Lasten verteilt werden sollten. Dabei ist zu beachten, dass der Transportanteil der Kosten bei den meisten Produkten nur ein kleinerer Teil ist, die resultierenden Preissteigerungen also oft deutlich niedriger als diese genannten 20 % ausfallen würden.

Auch die Dekarbonisierung des ersten Segments, des privaten Personenverkehrs, ist laut zeroCUTS mit vorhandenen Technologien möglich (Göhlich et al. 2021). Allerdings besteht hier ein breiteres Spektrum von plausibel erscheinenden Alternativen, von rein technologischen Lösungen der Dekarbonisierung der Antriebe über einen Wandel vom individuellen Autobesitz hin zu geteilten Flotten bis hin zum vollständigen Wechsel auf andere Verkehrsmittel oder auch Kombinationen dieser Möglichkeiten.

Aus dieser Ausgangssituation ergaben sich die folgenden Forschungsfragen: Können - regional differenzierte - Maßnahmenpakete definiert werden, welche dann breit unterstützt werden? Und  konnte  das  bestehende  Momentum  des  Klimabürger:innenrats genutzt werden, um diese Fragen zu adressieren? Daher orientierte sich das partizipative Format am Klimabürger:innen-

Ansatz des Bürger:innenrats integriert. Aus der Integration beider Ansätze ergibt sich der Vorteil, dass die Wirkungen unterschiedlicher Maßnahmen sofort beurteilt werden können. Ein Nachteil  ist,  dass  Maßnahmen,  welche  über  die  vorher  definierten hinausgehen, an dieser Stelle nicht berücksichtigt werden können. Von einem Charrette-Verfahren - bei welchem die Ergebnisse der Diskussionen in erneuten Simulationen hätten berücksichtigt werden können - wurde abgesehen, weil es Indikationen gab, dass eine noch höhere zeitliche Belastung der Bürgergutachter:innen zu erhöhten Abbruchquoten bei der Teilnahme geführt hätten.

## Workshops mit Bürgergutachter:innen

## Projektvorhaben und Ziel

Partizipative  Methoden werden unter anderem eingesetzt, um Ideen zu sammeln und die Wahrnehmung der Bürger:innen zu analysieren. Partizipative Methoden umfassen eine Reihe von Aktivitäten mit einem gemeinsamen Nenner: Sie ermöglichen es den Menschen, aktiv und einflussreich an Entscheidungen mitzuwirken, die ihr Leben betreffen. Innerhalb des Projekts ,Bürgerschaftliche Resonanz auf nachhaltige Mobilitäts- und Infrastrukturszenarien für Berlin' haben wir, basierend auf unseren Simulationen, Transformationsszenarien mit konkreten Maßnahmen für eine Dekarbonisierung der zuvor genannten vier Verkehrssegmente erstellt. Hierfür wurden unter anderem browserbasierte, interaktive Dashboards entwickelt und verwendet (siehe Abb. 2). Die unterschiedlichen Maßnahmen wurden in Workshops mit Bürger:innen  aus  Berlin  und  Brandenburg,  die  als  Bürgergutachter:innen agieren, kontrovers und alternativ diskutiert und darüber abgestimmt. Auf diese Weise werden nicht nur Informationen zu Akzeptanz und Widerstand bestimmter Maßnahmen eingeholt, sondern die Szenarien erhalten auch ein partizipatives

und demokratisches Gewicht, was ihre Durchsetzungschancen erhöht. Mit dem Projekt wurde das Vorhaben eines bürgerschaftlichen Sounding Boards (oder auch einer bürgerschaftlichen Resonanz) für wissenschaftliche Erkenntnisse und politische Diskussionen erprobt. Zudem ist dies eine wertvolle Ergänzung der wissenschaftlichen Arbeiten rund um die Verkehrssimulationen, um eine gesellschaftliche Resonanz zu erhalten.

Die Intensität des Beteiligungsformats (Schwab 2019) geht über ein reines Informieren der Teilnehmenden hinaus. In den Workshops wurden explizit sowohl die Meinung und der Meinungsaustausch zwischen den Teilnehmenden als auch eine Bewertung aus ihrer eigenen Lebenssituation erwünscht. Einwürfe und Ideen von Teilnehmenden, wie beispielsweise die Frage der sozialen Verträglichkeit, wurden für die finale Abfrage der Maßnahmen aufgegriffen.  Dadurch  unterscheidet  sich  das  Format von einfachen Befragungsdesigns.

## Sozio-demographische Zusammensetzung der Bürgergutachter:innen

An den Workshops haben insgesamt 64 Bürger:innen aus Berlin und Brandenburg teilgenommen. Die Bürger:innen wurden aus verschiedenen Gruppen rekrutiert (siehe Abb. 1). Anschließend wurden Personen mit unterrepräsentierten Merkmalen gezielt nachrekrutiert und vor den Workshops wurden sozio-demographische Merkmale, Wohnort und Mobilitätsverhalten von 52 der 64 Teilnehmenden erfasst; für Details siehe Research data. Vor den Workshops wurden sozio-demographische Merkmale, Wohnort und Mobilitätsverhalten von 52 der 64 Teilnehmenden erfasst; für Details siehe Research data.

Die Auswertung der Daten zeigt, dass ein breites Spektrum aus der Gesellschaft vertreten war:

- Es gab eine ausgewogene Geschlechterverteilung.
- Es gab Teilnehmende aus allen Altersklassen : unter 15 Jahre: 8 %; 15-25 Jahre: 21 %; 25-45 Jahre: 35 %; 4565 Jahre: 17 % und über 65 Jahre: 19 %.
- 20 % der Teilnehmenden waren in Vollzeit berufstätig ; 12  % in Teilzeit und 6 % nicht arbeitend; 19 % Rentner:innen oder Pensionär:innen; 25 % Studierende; 15 % Schüler:innen.
- Es gab Teilnehmende aus allen Haushaltsgrößen : Einpersonenhaushalte: 28 %; Zweipersonenhaushalte: 21 %; Dreipersonenhaushalte: 15 %; Vierpersonenhaushalte: 10 %; Haushalte mit fünf Personen und mehr: 26 %.
- Es gab Teilnehmende aus allen Einkommensklassen : unter 500 €/Monat: 5 %; 500-1500 €/Monat: 31 %; 1500-2500 €/ Monat: 43 %; 2500-3500 €/Monat: 7 % sowie über 3500 €/ Monat: 14 %.
- 44 %  der  Teilnehmenden wohnten außerhalb  des  S-BahnRings; 38 % innerhalb des S-Bahn-Rings; 18 % in Brandenburg. Fünf der neun Personen aus Brandenburg waren Schüler:innen.
- 65 % der Teilnehmenden besaßen ein Auto .

Abb. 1: Anteilige Rekrutierung von Bürgergutachter:innen aus verschiedenen Gruppen. Quelle: eigene Darstellung

<!-- image -->

Dies bestätigt insgesamt eine hohe sozio-demographische Heterogenität der Teilnehmenden.

Ein Vergleich mit dem Mikrozensus Berlin ist unter Research data zu finden. Dabei fällt auf, dass die Altersklasse 15-25 Jahre überrepräsentiert und die Altersklasse 45-65 Jahre unterrepräsentiert  war.  Damit  einhergehend  waren  Studierende  deutlich überrepräsentiert - laut Statistik Berlin haben sie einen Anteil von 5 %, während sie bei uns einen Anteil von 25 % hatten. Ähnlich,  aber  nicht  so  stark,  haben  Schüler:innen in Berlin einen Anteil  von  11 %,  während  sie  bei  uns  einen  Anteil  von  15  % hatten. Teilnehmende aus Haushalten mit fünf oder mehr Personen waren überrepräsentiert. Der hohe Anteil der Fünfpersonenhaushalte wurde zum einen durch die Gruppe der Schüler:innen  erzeugt  -  es  ist  statistisch  wahrscheinlich,  dass  man  bei Rekrutierung  über  Schulen  überproportional  viele  Vielpersonenhaushalte zieht. Ein weiterer Teil entsteht durch den überproportional hohen Anteil von Studierenden, die oft in Wohngemeinschaften wohnen. Bzgl. der Einkommen sind diejenigen bis 2500 Euro über- und diejenigen höher als 2500 Euro unterrepräsentiert.

Bock und Reimann (2021) diskutieren die Zufallsauswahl bei Beteiligungsverfahren, an welcher sich unser Ansatz orientiert. Diese ist allerdings kaum zu gewährleisten, da es immer Bevölkerungsgruppen gibt, welche eine Teilnahme mit höherer Wahrscheinlichkeit ablehnen als andere (self selection bias). Der typische Ansatz zur Kompensation sind geschichtete Stichproben, also dass man unterrepräsentierte Personengruppen nachrekrutiert.  Nicht  vermeidbar sind dabei allerdings Verzerrungen innerhalb dieser Personengruppen - z. B. wurden ältere Personen nachrekrutiert, aber es ist zu vermuten, dass hier vor allem aktivere  Mitglieder  dieser  Bevölkerungsgruppe  erreicht  wurden, und somit weniger aktive, mit möglicherweise anderen Meinungen, zu schwach berücksichtigt sind.

Die politische Legitimationskraft partizipativer Technikfolgeabschätzung hängt von der verbindlichen Einbindung in den politischen Prozess ab (Grunwald 2000). Im vorliegenden Projekt geht es allerdings erst in einem zweiten Schritt um die Einbindung in den politischen Prozess und zunächst um die bürgerschaftliche Rückmeldung an die Wissenschaft und die von Bürger:innen erarbeiteten Szenarien und damit um Antworten

auf die Frage, welche Maßnahmenpakete unter  welchen  Voraussetzungen  breite Unterstützung erfahren könnten.

Im  gewählten  Format  geht  der  Beschlussfassung  wissenschaft  licher    Input und Diskussion zwischen den sehr unterschiedlichen Teilnehmenden voraus. Wir gehen davon aus, dass dies  zu  einer  Erweiterung der Perspektiven von Beteiligten und zu einer Berücksichtigung anderer Interessen als der eigenen führt. Im Sinne der Suche nach Maßnahmenpaketen mit breiter Unterstützung werden im Folgenden nur Voten mit mehr als 70 % Unterstützung  thematisiert.  Somit  hat  unsere Studie zwei primäre Ziele:

Erstens,  die  Identifikation  von  Maßnahmen  oder  Maßnahmenpaketen,  bei denen breite Unterstützung erwartet werden  kann.  Diese  können  dann  den  politisch Handelnden nahegelegt werden. Zweitens,  die  Identifikation  von  Konfliktlinien,  bei  deren  Auftreten  im  Verfahren also keine mehrheitlich unterstützten Lösungen gefunden werden konnten. Aus Forschungssicht  ergibt  sich  hieraus

Abb. 2: Darstellung des Dashboards.

<!-- image -->

die  Frage,  ob  nicht  in  anderer  Weise  doch  noch  mehrheitsfähige Lösungen gefunden werden können - z. B. indem Lösungsvorschläge aus den Diskussionen aufgegriffen und genauer analysiert werden oder indem im vorliegenden Fall die Lösungen räumlich stark differenziert werden. Gerade letzteres erscheint im Bereich Verkehr - welcher ja wenigstens teilweise eine Raumwissenschaft ist - durchaus machbar, wenn Verfahren gefunden werden, die diese Differenzierung berücksichtigen.

Quelle: VSP (2023)

kehrssegmente mit unterschiedlichen Zeithorizonten vorgestellt und die wissenschaftlichen Erkenntnisse der Simulationen dieser Maßnahmen in Bezug auf CO2-Emissionsreduzierung, Kosten und Auswirkungen auf den fahrenden und parkenden Verkehr erläutert.

## Organisation und Ablauf der Workshops

Die Organisation und der Ablauf der Workshops lehnen sich größtenteils an das Beteiligungsformat einer Planungszelle an. Der  Gründer  dieses  Beteiligungsformats  Peter  Christian  Dienel  unterteilt  die  Organisation  in  eine  Vorbereitungs-,  Durchführungs- und Nachbereitungsphase. Die Durchführungsphase beinhaltet  die  Eröffnung,  den  Informationseingang,  Arbeit  in Untergruppen,  Ergebnisdokumentation  durch  Bewertungsvorgänge und das Abschlussergebnis (Dienel 2002).

Die Workshops mit den Bürger:innen fanden an zwei aufeinanderfolgenden Tagen statt. Am ersten Tag standen die Segmente Sonderverkehre und Güterverkehr im Fokus, am zweiten Tag die Segmente privater und kommerzieller Personenverkehr. Der Ablauf für die einzelnen Segmente war gleich aufgebaut.

Eröffnung: Nach der Registrierung der Teilnehmenden wurde im Plenum der Workshopablauf und die Aufgabenstellung erläutert.

Informationseingang: Zunächst haben Wissenschaftler:innen im Plenum Maßnahmen zur Dekarbonisierung der einzelnen Ver-

Diskussion in Kleingruppen: Bevor Teilnehmende um eine Bewertung  der  Maßnahmen  gebeten  wurden,  fand  eine  Diskussionsrunde statt. Hierfür wurden die Teilnehmenden in fünf Kleingruppen mit unterschiedlichen demographischen Profilen nach Alter, Geschlecht und Einkommen aufgeteilt, um sicherzustellen, dass in den Gruppendiskussionen ein Austausch verschiedener  Perspektiven  unterschiedlicher  Lebenslagen  stattfindet. Für die Segmente Güterverkehr und privater Personenverkehr  wurden  die  von  Wissenschaftler:innen  präsentierten Maßnahmen und deren abgeschätzte Auswirkungen in Form von interaktiven Dashboards unterstützend illustriert (siehe Abb. 2). Das Dashboard wurde in den Diskussionen zur Verfügung gestellt. Es wurde hier ähnlich dem Konzept des Decision Theatre vorgegangen (Wolf et al. 2023). Die Diskussionen wurden von Moderator:innen geleitet. Hierbei wurde insbesondere die finanzielle Machbarkeit, die individuelle und gesellschaftliche Akzeptanz und die Vor- und Nachteile fokussiert.

Bewertung  der  vorgestellten  Maßnahmen :  Nach  der  Diskussion haben die Teilnehmenden über die zuvor diskutierten Maßnahmen abgestimmt. Die Umfrage enthielt Single-ChoiceFragen  mit  fünf-stufigen  Likert-Skalen  und  offene  Fragen  zu beispielsweise  Vor-  und  Nachteilen  der  Maßnahmen  und  ein Kommentarfeld für weitere Anregungen.

Abschlussergebnis und finale Abstimmung: Nach den Workshops  wurden  die  Daten  und  Meinungen  der  Teilnehmenden ausgewertet  und  in  21  Hauptaussagen  zusammengefasst.  Den Teilnehmenden wurden die Hauptaussagen im Nachgang zugeschickt und sie haben final über die Aussagen in Form von Zustimmung oder Ablehnung abgestimmt.

## Fachliche Inputs

Wie in der Einleitung ausgeführt, orientierten sich die fachlichen Inputs an den Resultaten des Projekts zeroCUTS. Details der Inputs sind unter Research data zu finden; der Abschnitt hier gibt eine kurze Zusammenfassung.

Zunächst  thematisierte  eine  übergreifende  Einführung  die Vor- und Nachteile verschiedener nicht-fossiler Antriebtechnologien aus heutiger Sicht. Konkret angesprochen wurden elektrische Antriebe, Wasserstoff-Antriebe, synthetische Kraftstoffe sowie  Brennstoffzellen,  mit  dem  Tenor,  dass  die  letzten  drei deutlich mehr Primärenergie benötigen würden, welche aber andererseits potenziell von außerhalb Deutschlands zur Verfügung gestellt  werden  könnte.  Weiterhin  wurde  darauf  hingewiesen, dass eine vollständige Umstellung der vorhandenen Fahrzeugflotte  auf  nicht-fossile  Antriebe  bis  2045  erhebliche  -  möglicherweise nicht zu leistende - Anforderungen an die industrielle Produktion stellen würde.

Bzgl.  der  einzelnen  Segmente  wurden  zeroCUTS  Forschungsresultate referiert. Dies wird im Folgenden beschrieben.

Wegen ihrer geringeren Komplexität wurden zunächst Sonderverkehre behandelt, wo selbst problematisch erscheinende Fälle wie z. B. die Müllabfuhr mit elektrischen Antrieben gelöst werden können (Ewert et al. 2021). Generell war der zentrale Vorschlag, dass alle Betreiber von Sonderverkehren umgehend entsprechende Experimentierfahrzeuge mit unterschiedlichen nicht-fossilen Antriebstechnologien beschaffen sollten, um darauf basierend eine Entscheidung bzgl. zukünftigem nicht-fossilem Betrieb zu treffen. Alle zukünftigen Beschaffungen sollten dann auf dieser Entscheidung beruhen.

Bei den Güterverkehren konzentrierten sich die Diskussionen auf deren urbanen Anteil, also den Vorlauf oder den Verteilverkehr.  Laut  zeroCUTS  ist  eine  Dekarbonisierung  auch dieser  Verkehre  weitgehend  durch  Elektrifizierung  machbar; in  einigen  wenigen  Fällen  liegen  allerdings  die  Güterverteilzentren so weit von Berlin entfernt, dass Fernverkehrslösungen (z. B. Schnell-Ladung oder Stromschienen) einbezogen werden müssten.

Weil  der  kommerzielle  Personenverkehr  die  gleichen  Ansätze verwendet wie der nichtkommerzielle Personenverkehr , wurde  letzterer  zuerst  behandelt.  Hier  wurde  zunächst  dargestellt, dass typische Ansätze wie Ausbau des öffentlichen Verkehrs,  Ausbau  des  Radverkehrs  oder  Kiezblöcke  (auto-arme Zonen) zwar in günstigen Fällen den Autoverkehr um jeweils ein  Zehntel  reduzieren,  folglich  aber  selbst  die  Kombination dieser drei Maßnahmen mindestens 70 % des fossilen Autoverkehrs bestehen lassen würde. Einerseits wurde darauf hingewiesen, dass Simulationen ergeben, dass durch reine Verbesserun- gen der Alternativen zum Auto (sogenannte Pull-Maßnahmen) eine Reduktion des Autoverkehrs um mehr als ein Drittel kaum möglich  sein  würde.  Andererseits  wurde  darauf  hingewiesen, dass die jetzige Gesetzeslage keine fossilen Antriebe im Land Berlin  nach  2045  mehr  zulassen  würde.  Die  wesentlichen  offenen Fragen seien daher (aus Sicht der einführenden Wissenschaftler:innen):

1. Sollte das Ziel eine reine Umstellung der Antriebe bis 2045 sein, oder sollte dieses Ziel mit einer deutlichen Reduktion des fahrenden oder des stehenden Autoverkehrs kombiniert werden? Falls letzteres, durch welche Maßnahmen sollte dies erreicht werden? Neben den oben beschriebenen Pull-Maßnahmen wurden konkret eine Bepreisung des fahrenden Verkehrs sowie eine Bepreisung des stehenden Verkehrs (Parkraumbewirtschaftung) vorgestellt.
2. Sollte  ein  jeweils  gewähltes  Ziel  durch  partielle  Maßnahmen zu einem früheren Zeitpunkt flankiert werden? Vorgeschlagene  Denkrichtungen  waren  zum  einen  räumlich  eingeschränkte Maßnahmen wie z. B. ein Verbot fossiler Fahrzeuge innerhalb des Berliner S-Bahn-Rings bereits ab 2035 oder deutlich erhöhte Bepreisungen des stehenden oder fahrenden fossilen Verkehrs ab 2035.

Wie oben bereits erwähnt, sind die Ansätze für den kommerziellen  Personenverkehr die  gleichen  wie  für  den  nichtkommerziellen  Personenverkehr:  Dekarbonisierung  des  Antriebs, möglicherweise  Verlagerung  auf  andere  Verkehrsmittel.  Die wesentliche  Frage  aus  Sicht  der  einführenden  Wissenschaftler:innen war hier, ob eine Verlagerung auf andere Verkehrsmittel in diesem Segment sinnvoll und daher anzustreben sei.

Wie in der Einleitung ausgeführt, war die Hypothese der Wissenschaftler:innen, dass die Vorschläge für die Segmente ,Sonderverkehre', ,Güterverkehr' sowie ,kommerzieller Sonderverkehr' breite Unterstützung erfahren würden, während für den privaten Personenverkehr kontroversere Diskussionen aufgrund stärkerer Betroffenheit erwartet wurden.

## Ergebnisse

Wie  im  Abschnitt  ,Sozio-demographische  Zusammensetzung der Bürgergutachter:innen' beschrieben, stützen wir uns nur auf Voten  mit  mindestens  70 %  Unterstützung.  Für  die  textuellen Beschreibungen verwenden wir die in Tab. 1 dargestellte Skala.

Tab. 1: Klassifizierung der Unterstützung von Maßnahmen(-paketen).

| Unterstützung             | Anteil   |
|---------------------------|----------|
| sehr breite Unterstützung | > 90%    |
| breite Unterstützung      | > 75%    |
| deutliche Unterstützung   | > 70%    |

Quelle: eigene Darstellung

## Umfrage während der Workshops

Die Resultate der Umfragen während der Workshops lassen sich wie folgt zusammenfassen; für Details siehe Research data:

- Breite Unterstützung für nicht-fossilen Sonderverkehr schon deutlich  vor  2045: 79 %  der  befragten  Personen  unterstützen es, die Sonderverkehre verpflichtend mit nicht-fossilen Fahrzeugen auszustatten; 69 % erwarten eine gesellschaftliche Akzeptanz dieses Szenarios; 56 % glauben, dass es politisch/finanziell machbar ist.
- Breite  Unterstützung  für  nicht-fossilen  kommerziellen  Personenverkehr schon deutlich vor 2045 (76  %; 69  %; 40  %).
- Deutliche  Unterstützung  für  nicht-fossilen  Güterverkehr schon deutlich vor 2045 (70 %; 61 %; 36 %) .

Innerhalb der diskutierten Maßnahmen sprachen sich 80 % für Einschränkungen für fossile Fahrzeuge im Güterverkehr schon deutlich vor 2045 aus.

- Keine  eindeutige  Stimmungslage  im  privaten  Personenverkehr: Die  Dekarbonisierungsszenarien  wurden  jeweils  mit einer knappen Mehrheit (unter 60 %) befürwortet.
-  Auffällig  bei  allen  diesen  Abstimmungen  ist,  dass  die  Zustimmungswerte von ,eigene Präferenz' über , Vermutung der gesellschaftlichen  Akzeptanz'  bis  zu  ,politischer/finanzieller Machbarkeit' deutlich absinken. Ersteres bedeutet, dass die tatsächliche Veränderungsbereitschaft höher ist als deren Wahrnehmung; dies wird auch in anderen Untersuchungen so festgestellt (Drews et al. 2022). Nochmals deutlicher sin-

bis 2045 abgelehnt, aber eine Antriebswende bis 2035 befürwortet, oder es wurde eine Antriebswende bis 2045 abgelehnt, jedoch eine kombinierte Antriebs- und Verkehrswende bis 2045 befürwortet.

Wir  interpretieren  diese  Antworten  so,  dass  für  diese  Personen  eine  Umstellung  bis  2045  zu  langsam,  oder  auch  eine reine  Antriebswende  ohne  eine  gleichzeitige  Verkehrswende unerwünscht ist. Dennoch teilen diese Personen offenbar das Ziel eines nicht-fossilen Verkehrs in Berlin 2045. Wir interpretieren daher im Folgenden die Bejahung einer ,Antriebswende bis 2035' auch als Zustimmung zu ,nicht-fossiler Verkehr bis 2045' sowie die Bejahung einer ,Antriebs- und Verkehrswende bis  2035'  auch  als  Zustimmung  zu  ,nicht-fossiler  Verkehr bis 2045', etc.; für genaue Details dieser ,Bereinigung' siehe Research data. Dort werden auch die entsprechenden Abstimmungsergebnisse sowohl als ,unbereinigt' als auch als ,bereinigt' angegeben.

Die Resultate lassen sich wie folgt zusammenfassen; für Terminologie siehe Tab. 1:

- Sehr breite Unterstützung für nicht-fossilen Verkehr: Mehr als 90 % der Befragten unterstützen das Ziel eines nicht-fossilen Verkehrs bis 2045.
- Sehr breite Unterstützung für eine über eine reine Antriebswende hinausgehende Verkehrswende: Mehr als 85 % unterstützen  Maßnahmen,  'die  über  die  Dekarbonisierung  des Verkehrs hinausgehen und somit unter anderem den Platzbedarf  für  Verkehr  und  die  Lebensqualität  der  Einwohner:innen adressieren.'

Es gab breite Unterstützung für nicht-fossilen Sonderverkehr und für nicht-fossilen kommerziellen Personenverkehr schon deutlich vor 2045.

ken die Zustimmungswerte für politische/finanzielle Machbarkeit, und zwar oft deutlich unter 50 %. Hier herrscht also ein erheblicher Pessimismus, dass die Politik geeignete Maßnahmen frühzeitig genug umsetzt.

## Finale Abstimmung über 21 Hauptaussagen

Nach den Workshops wurden Aussagen unter Berücksichtigung der Diskussionen und Anregungen der Workshops verfasst und den Teilnehmenden zugesendet. Diese konnten individuell die jeweiligen Aussagen befürworten oder ablehnen. Für eine Übersicht der Fragen mit dem jeweiligen Abstimmungsergebnis siehe Research data.

Bei der Auswertung der Abstimmung über die Hauptaussagen  stellte  sich  heraus,  dass  manche  Teilnehmenden  Antworten gaben, welche zunächst widersprüchlich scheinen. Zum Beispiel wurde von mehreren Teilnehmenden eine Antriebswende

- Sehr breite Unterstützung für die Entlastung von Betroffenen: Mehr als 90 % der Befragten unterstützen sowohl eine Entlastung von Betroffenen als auch, dass die Einnahmen aus Maßnahmen zur Dekarbonisierung des Verkehrs für weitere Klimaschutz-Maßnahmen und für die Unterstützung von Betroffenen verwendet werden.
- Breite Unterstützung für Pull-Maßnahmen: starker Ausbau des Öffentlichen Personennahverkehrs (ÖPNV) (100 % Zustimmung), starker Ausbau der Radwege (75 %), starker Ausbau von stadtweiten Car-Sharing-Angeboten (knapp 80 %), Erweiterung des ÖPNV durch Rufbusse in den Außenbezirken (85 %), alternative Konzepte wie Lastenfahrräder oder Regionalität (knapp 80 %).
- Breite Unterstützung für klare Ansagen und langfristige Planbarkeit: Klar definierte Maßnahmenpakete und deren Kommunikation (knapp 90 %), Sondervermögen Klimaschutz (gut

- 80 %); Institutionen des Landes Berlin sollen  verpflichtet  werden,  nicht-fossile Fahrzeuge zu erproben und nach Probephasen keine fossilen Fahrzeuge mehr  anzuschaffen  (gut  80 %).  Hingegen gibt es eine deutliche Mehrheit (gut  70  %) gegen eine  sofortige  deutliche Erhöhung der Benzinpreise.
- Verursacherprinzip  wird  breit  unterstützt, aber in Bezug auf konkrete Maßnahmen kein eindeutiges Bild: Mehr als  80  %  sprechen  sich  für  eine  Anwendung des Verursacherprinzips aus. Die Zustimmung sinkt auf ca. 50 % ab, wenn es konkret um Maut oder deutlich höhere Preise für Anwohnerparkauswei  se geht (siehe Abb. 3).

## Fazit

Ziel dieses Projekts war, Verständnis der politischen  und  gesellschaftlichen  Hürden zu erlangen, die sich beim Konzeptionieren von verkehrlichen Maßnahmen im  Rahmen  der  Dekarbonisierung  ergeben.  In  der  beschriebenen  Studie  wur-

Abb. 3: Zustimmungsrate bzgl. der 21 Hauptaussagen.

<!-- image -->

den Szenarien zur Dekarbonisierung des Verkehrs mit Bürgergutachter:innen diskutiert. Unseren eigenen wissenschaftlichen Studien zur Dekarbonisierung des Verkehrs folgend wurde der Verkehr dazu in vier Segmente unterteilt: privater Personenverkehr, kommerzieller Personenverkehr, kommerzieller Güterverkehr und Sonderverkehre wie Einsatzverkehre, Müllabfuhr oder Straßenreinigung.

Quelle: eigene Darstellung

eine Kombination von sehr deutlichen Verbesserungen in den Bereichen ÖPNV und Radverkehr führt in keiner unserer Simulationen zu einer CO2 -Reduktion um mehr als 20 %. Selbst in Bezug auf Push-Maßnahmen gab es eine breite Mehrheit für die Anwendung des Verursacherprinzips. Für konkrete Maßnahmen wie Maut oder erhöhte Parkgebühren für fossile Fahrzeuge fanden sich allerdings keine Mehrheiten.

Das durchgeführte Bürger:innengutachten stellt einen Informationsgewinn bezüglich der Bewertung von Maßnahmen bzw. des Dekarbonisierungbedarfs im Verkehr in der Gesellschaft dar. Die gewonnenen Informationen sind vor allem in Bezug auf etwaige Diskrepanzen in der wissenschaftlichen und politischen Bewertung von Relevanz.

Die Diskussionen zeigen zunächst Handlungsspielräume für alle  Segmente  mit  Ausnahme  des  privaten  Personenverkehrs. Dies ist wichtig, weil sich in diesen Segmenten bereits erhebliche Beiträge zur Dekarbonisierung des Verkehrssystems erreichen lassen. Doch selbst für den privaten Personenverkehr gab es eine breite Befürwortung vieler Ziele und Maßnahmen: das generelle Ziel des nicht-fossilen Verkehrs, eine über eine reine Antriebswende hinausgehende Verkehrswende, die Verbesserung der Alternativen jenseits des Autos, einen sozialen Ausgleich sowie klare Kommunikation und Planbarkeit. Auch Pull-Maßnahmen wie Verbesserungen des öffentlichen Verkehrs oder für das Fahrrad fanden breite Mehrheiten, selbst wenn auf deren hohe Kosten sowie deren eingeschränkte Wirksamkeit im Hinblick auf die Dekarbonisierung des Verkehrssystems hingewiesen wurde - selbst

## Acknowledgements

Die Autor:innen bedanken sich beim nexus Institut und insbesondere bei Eike Biermann für das Teilnehmendenmanagement sowie bei Janek Laudan und Ricardo Ewert für die Moderation der Workshops und bei allen Teilnehmenden.

Funding · This study was funded in part by German Research Foundation, DFG (zeroCUTS, project number 398051144), by Federal Ministry of Education and Research, BMBF (DiTriMo, funding code 05M22KTA), and by Climate Change Center Berlin Brandenburg (Citizens' response to sustainable mobility and infrastructure scenarios for Berlin, funding code CCC2022\_09) with funds of Berlin Senate Department for Science, Health, Care and Equality.

Competing interests · The authors declare no competing interests.

## Research data

Kreuschner, Moritz; Nagel, Kai; Schlenther, Tilmann; Bonatz, Nora; Mostofi, Hamid; Dienel, Hans-Liudger (2023): Ergänzendes Material Dekarbonisierung des Verkehrssektors in Berlin - Bürger:innengutachten zu wissenschaftlich   erstellten Szenarien. Berlin: Technische Universität Berlin. https://doi.org/10.14279/ depositonce-19472

## Literatur

Bock, Stephanie; Reimann, Bettina (2021): Mit dem Los zu mehr Vielfalt in der Bürgerbeteiligung? Chancen und Grenzen der Zufallsauswahl. Berlin: Deutsches Institut für Urbanistik. Online verfügbar unter https://repository. difu.de/handle/difu/583064, zuletzt geprüft am 16. 01. 2024.

Dienel, Peter (2002): Die Planungszelle. Der Bürger als Chance. Wiesbaden: VS Verlag für Sozialwissenschaften. https://doi.org/10.1007/978-3-322-80842-4

Drews, Stefan; Savin, Ivan; van den Bergh, Jeroen (2022): Biased perceptions of other people's attitudes to carbon taxation. In: Energy Policy 167, S. 113 051. https://doi.org/10.1016/j.enpol.2022.113051

Ewert, Ricardo; Grahle, Alexander; Martins-Turner, Kai; Syré, Anne; Nagel, Kai; Göhlich, Dietmar (2021): Electrification of urban waste collection. Introducing a simulation-based methodology for technical feasibility, impact and cost analysis. In: World Electric Vehicle Journal 12 (3), S. 122. https://doi.org/ 10.3390/wevj12030122

Göhlich, Dietmar et al. (2021): Integrated approach for the assessment of strategies for the decarbonization of urban traffic. In: sustainability 13 (2), S. 839. https://doi.org/10.3390/su13020839

Grunwald, Armin (2000): Partizipative Technikfolgenabschätzung - wohin? Einführung in den Schwerpunkt. In: TATuP- Zeitschrift für Technikfolgenabschätzung in Theorie und Praxis 9 (3), S. 3-11. https://doi.org/10.14512/ tatup.9.3.3

KNOCA - Knowledge Network On Climate Assemblies (2024): KNOCA. A   European network for sharing best practice on the design and implementation of Climate Assemblies. Online verfügbar unter https://knoca.eu/, zuletzt geprüft am 16. 01. 2024.

Schwab, Nina (2019): Konfliktkompetenz im Bauprojektmanagement. Konfliktrisiken vermeiden. Konfliktpotenziale nutzen. Wiesbaden: Springer Fachmedien. https://doi.org/10.1007/978-3-658-27089-6

SenUVK - Senatsverwaltung für Mobilität, Verkehr, Klimaschutz und Umwelt (2023): Berliner Klimabürger:innenrat. Dokumentation. Online verfügbar unter https://www.berlin.de/klimabuergerinnenrat/dokumentation, zuletzt geprüft am 16. 01. 2024.

VSP - Verkehrssystemplanung und Verkehrstelematik (2023): Sounding Board Berlin. Berlin: TU Berlin. Online verfügbar unter www.vsp.berlin/soundingboard, zuletzt geprüft am 16. 01. 2023.

Wolf, Sarah et al. (2023): The decision theatre triangle for societal challenges. An example case and research needs. In: Journal of Cleaner Production 394, S. 136 299. https:/ /doi.org/10.1016/j.jclepro.2023.136299

zeroCUTS (2018): zeroCUTS - Analyse von Strategien zur vollständigen Dekarbonisierung des urbanen Verkehrs. Projekt der Deutschen Forschungsgemeinschaft (DFG). Online verfügbar unter https://www.tu.berlin/vsp/forschung/ projekte/zerocuts, zuletzt geprüft am 15. 01. 2024.

MORITZ KREUSCHNER

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

ist am Fachgebiet ,Verkehrssystemplanung und Verkehrstelematik' an der Technischen Universität Berlin tätig und befasst sich mit der Modellierung und Simulation von Verkehr mit einem Fokus auf Dekarbonisierung des Verkehrssystems.

NORA BONATZ

ist als wissenschaftliche Mitarbeiterin am Lehrstuhl ,Arbeitslehre/Technik und Partizipation' an der Technischen Universität Berlin tätig und befasst sich unter anderem mit Methoden, Prozessen und Techniken der Bürgerbeteiligung.

TILMANN SCHLENTHER

ist am Fachgebiet ,Verkehrssystemplanung und Verkehrstelematik' an der Technischen Universität Berlin tätig und befasst sich mit der Modellierung und Simulation von Verkehr mit einem Fokus auf Dekarbonisierung des Verkehrssystems.

DR. HAMID MOSTOFI DARBANI ist als Senior Researcher am Lehrstuhl ,Arbeitslehre/Technik und Partizipation' an der Technischen Universität Berlin tätig und befasst sich unter anderem mit Methoden, Prozessen und Techniken der Bürgerbeteiligung.

PROF. DR. HANS-LIUDGER DIENEL

ist Leiter des Lehrstuhls ,Arbeitslehre/Technik und Partizipation' an der Technischen Universität Berlin und befasst sich unter anderem mit   Methoden, Prozessen und Techniken der Bürgerbeteiligung. Zudem ist er Gründer und Geschäftsführer des nexus Instituts für Kooperationsmanagement und interdisziplinäre Forschung.

PROF. DR. KAI NAGEL

ist Leiter des Fachgebietes ,Verkehrssystemplanung und Verkehrstelematik' an der Technischen Universität Berlin und befasst sich mit der Modellierung und Simulation von Verkehr mit einem Fokus auf Dekarbonisierung des Verkehrssystems.

## RESEARCH. PUBLISH. TRANSFORM.

Your publication at oekom science

<!-- image -->

## The academic platform for sustainability and transformation

We are convinced that science plays a central role in shaping a sustainable future. With oekom science, we enable scientists to publish their work, exchange ideas and gain new insights.

We make sure your work gets the attention it deserves.

-  Your book
-  Your article
-  Your book series or journal
-  Open Access available

<!-- image -->

Contact us ...

<!-- image -->

English German www.oekom-science.de

<!-- image -->

<!-- image -->

<!-- image -->

I n diesem Interview spricht Reinhard Heil mit  dem  Science-Fiction-Autor  Karl  von Wendt  (Pseudonym  Karl  Olsberg)  über die  Möglichkeit,  dass  künstliche  Intelligenz außer Kontrolle geraten und die Existenz der gesamten Menschheit gefährden oder zumindest  ihr  Entwicklungspotenzial  nachhaltig  und  gravierend  einschränken  könnte.  Unkontrollierbare  künstliche  Intelligenz,  lange Zeit nur ein Thema der Science-Fiction, wird, vor allem seit dem großen Erfolg der auf sogenannten großen Sprachmodellen basierenden Chatbots, wie ChatGPT und Bard, in Politik und Medien zunehmend und sehr kontrovers diskutiert.

Reinhard Heil: Karl, Du bist Science-Fiction-Schriftsteller  und  -  ich  glaube  das darf  man  durchaus  so  sagen  -  Aktivist. Vor  Kurzem  hast  Du  beide  Eigenschaften  zusammengeführt  und  'Virtua'  veröffentlicht,  einen  Roman,  dessen  Untertitel  'KI  -  Kontrolle  ist  Illusion'  Deine Befürchtungen bezüglich der Risiken von Künst  licher Intelligenz (KI) auf den Punkt bringt.

Karl  von  Wendt: Ich  würde  mich  nicht unbedingt als ,Aktivist' bezeichnen. Ich sehe  mich  eher  als  Aufklärer.  Dazu  engagiere ich mich in einer internationalen Community von Menschen, die sich mit den  Risiken  der  KI  beschäftigen.  Während das Thema in den USA und Großbritannien schon weit oben auf der politischen  Agenda  steht,  wird  es  hier  in Deutschland immer noch kaum ernst genommen.

Mein  neuester  Roman  thematisiert zwar  das  Problem,  das  mir  momentan die größten Sorgen macht - eine KI, die in  mancher Hinsicht intelligenter ist als wir, uns manipuliert und so außer Kontrolle gerät. Aber 'Virtua' ist nach wie vor ein Roman, der in erster Linie unterhalten

Keywords · artificial intelligence, existential risk, governance, digital transformation

<!-- image -->

© 2024 by the authors; licensee oekom. This Open Access article is licensed under a Creative Commons Attribution 4.0 International License (CC BY). https://doi.org/10.14512/tatup.33.1.64 Published online: 15. 03. 2024 (editorial peer review)

## INTERVIEW

mit/with Karl von Wendt von/by Reinhard Heil

## Künstliche Intelligenz außer Kontrolle?

Artificial intelligence out of control?

soll und nicht die Zukunft vorhersagt. Ich habe ein umfangreiches Nachwort hinzugefügt, das meine Sicht auf das reale Problem grob schildert, aber trotzdem würde ich das Buch nicht als Aufklärungsmaterial sehen. Es soll eher nachdenklich machen  und  vielleicht  den  einen  oder  die andere  dazu  bringen,  sich  genauer  über die  Risiken  hochentwickelter  KI  zu  informieren. Dafür betreibe ich unter anderem mein Blog KI-Risiken.de.

Dass  KI-Risiken  in  Deutschland  nicht ernstgenommen würden, möchte ich so nicht  stehenlassen.  Es  gibt  eine  große Anzahl von Stellungnahmen und Studien zu  KI-Risiken  von  unterschiedlichen  Institutionen.  Unter  anderem  die  Technikfolgenabschätzung  (TA),  der  Deutsche Ethikrat  und  NGOs  haben  sich  intensiv mit möglichen Risiken beschäftigt. Weniger Aufmerksamkeit erhalten allerdings,

<!-- image -->

da  stimme  ich  Dir  zu,  Risiken  der  unkontrollierbaren KI, zu denen es ja sehr unterschiedliche Positionen gibt: Beispielsweise spricht Elon Musk von einer existenziellen  Bedrohung,  Yann  LeCun hingegen sieht darin vor allem wilde Spekulationen auf Basis methodisch zweifelhafter  Studien  und  Sascha  Lobo  warnt, dass  es  ganz  im  Interesse  der  KI-Konzerne sei, auf spekulative zukünftige Risiken zu fokussieren, um von den gegenwärtigen  abzulenken.  Warum  siehst  Du die  aktuell  doch  noch  sehr  spekulative Möglichkeit  eines  Kontrollverlustes  als relevantes, drängendes Problem?

In  der  Tat  meinte  ich  die  gravierenden, sogar existenziellen Risiken einer unkontrollierbaren KI, die in Deutschland bisher  kaum  ernst  genommen  werden.  Ich kann  es  gut  verstehen,  wenn  man  den Aussagen von Elon Musk oder Sam Altman misstraut, die auf solche existenziellen Risiken hinweisen. Bei ihnen liegt der Verdacht  nahe,  dass  sie  diese  Aussagen aus geschäftlichem Kalkül oder Wichtigtuerei tätigen, zumal sie ja selbst alles tun, um solche Risiken überhaupt erst heraufzubeschwören.  Allerdings  sind  sie  bei Weitem nicht die Einzigen, die vor existenziellen  Risiken  warnen.  Ein  entsprechender  Aufruf,  solche  Risiken  ebenso ernst zu nehmen wie die Gefahren eines globalen  Atomkriegs  oder  einer  Pandemie, wurde im Mai 2023 von über 200 namhaften  Wissenschaftler:innen  unterschrieben,  darunter  absolute  KI-Koryphäen  wie  die  Turing-Preisträger  Yoshua Bengio und Geoffrey Hinton, der oft als einer der Begründer moderner KI bezeichnet  wird,  oder  Stuart  Russell,  der ein  Standardwerk über neuronale Netze geschrieben  hat  und  schon  seit  Jahren vor  den  Gefahren  unkontrollierbarer  KI warnt  (Center  for  AI  Safety  2023).  Ein ähnlicher Aufruf kurz vor dem AI Safety Summit im November 2023 in London wurde zum Beispiel auch von Nobelpreisträger Daniel Kahnemann unterzeichnet, der  zwar  kein  KI-Experte  ist,  sich  dafür aber sehr gut mit den Schwachstellen des menschlichen Geistes auskennt, die es uns schwer machen, neuartige Risiken von großer Tragweite richtig einzuschätzen (Bengio et al. 2023). Wenn jemand https://doi.org/10.14512/tatup.33.1.64

<!-- image -->

wie Sascha Lobo solche Leute als ,nützliche  Idioten'  bezeichnet,  ist  das  schon ziemlich arrogant und vermessen, finde ich. Mir wäre es am liebsten, wir würden im  öffentlichen  Diskurs  nicht  über  die möglichen Motive dieser Leute spekulieren,  sondern  uns  mit  den  konkreten  Argumenten und Fakten auseinandersetzen, die für oder gegen ein existenzielles Risiko durch unkontrollierbare KI sprechen.

## Was verstehst Du genau unter einer unkontrollierbaren KI?

Ich verstehe darunter eine KI, die in der Lage ist, automatisch Entscheidungen zu treffen, und alle Maßnahmen, die wir ergreifen, um diese Entscheidungen zu korrigieren  oder  zu  unterbinden,  konterkarieren  oder  umgehen  kann.  Eine  solche KI wäre dann nicht mehr aufhaltbar oder korrigierbar,  entweder,  weil  sie  intelligenter ist als wir oder weil sie sich zum Beispiel ähnlich wie ein Virus in unserer technischen Infrastruktur ausgebreitet hat und wir sie nicht mehr daraus entfernen können. Wenn eine solche KI das falsche Ziel  verfolgt,  würde  dabei  höchstwahrscheinlich  unsere  Zukunft  zerstört.  Leider wissen wir nicht, wie wir ein Ziel so formulieren können, dass es wirklich gut für uns ist und keine Schlupflöcher oder Raum  für  Fehlinterpretationen  bietet  ein  Problem,  das  schon  die  alten  Griechen kannten und das Goethe mit seinem Zauberlehrling sehr schön auf den Punkt gebracht  hat.  Es  wird  auch  das  Alignment-Problem, holprig übersetzt das Zielangleichungs-Problem, genannt.

Wenn  man  genauer  hinschaut,  wird sehr  schnell  offensichtlich,  dass  dieses Problem ungelöst und eine Lösung auch nicht in Sicht ist. Die Frage lautet daher eher,  wie  lange  es  noch  dauert,  bis  wir in  der  Lage sind, eine potenziell unkontrollierbare  KI  zu  entwickeln,  und  ob wir  dann  dumm  genug  sind,  das  auch zu  tun.  Letzteres  beantwortet  sich  angesichts des jüngsten Dramas bei dem führenden  KI-Konzern  OpenAI  wohl  von selbst: Auch wenn es dabei anscheinend nicht  primär  um  Sicherheit  ging,  haben die auf Sicherheit fokussierten Kräfte im Aufsichtsrat den Machtkampf gegen Sam Altman verloren. In einem globalen Wett- rennen um Forschungserfolge und Marktführerschaft  bleibt  eben  die  Sicherheit gerne mal auf der Strecke, und allzu oft in der Geschichte hat blinde Gier die Vernunft besiegt.

Deiner letzten  Aussage  stimme ich  hundertprozentig  zu.  ,Gier  frisst  Hirn'  ist wahrscheinlich die einzige echte anthropologische Konstante. In der TA vermeiden wir zwar konkrete Aussagen über die fernere Zukunft, ich frage Dich aber jetzt trotzdem nach dem Zeithorizont.

Die Frage, wie viel Zeit uns noch bleibt, kann  niemand  sicher  beantworten,  aber dass die Zeitlinie sich in den letzten Monaten  drastisch  verkürzt  hat,  dürfte  offensichtlich  sein,  selbst  wenn  an  den Spekulationen  über  einen  Durchbruch beim  mysteriösen  Q*-Projekt  von  OpenAI nichts dran sein sollte. Yoshua Bengio schreibt in seinem Blog, dass er die Entwicklung einer Artificial General Intelligence, also einer allgemeinen KI auf menschlichem Niveau, im Jahr 2022 noch in 20-50 Jahren erwartete. Ein Jahr später hat sich dieser Erwartungshorizont auf

Karl von Wendt

<!-- image -->

hat 1988 über Künstliche Intelligenz (KI) promoviert, mehrere KI-relevante Start-ups gegründet und schreibt Blogbeiträge sowie unter dem Pseudonym 'Karl Olsberg' Romane über die Risiken der KI.

fünf bis 20 Jahre verkürzt (Bengio 2023). Viele  in  der  AI  Safety  Community  haben  noch  kürzere  Erwartungshorizonte. Uns bleiben also mit Glück noch höchstens zwei Jahrzehnte, wenn wir Pech haben  nur  noch  wenige  Jahre.  Meine  persönliche Erwartung ist da angesichts der sich überschlagenden Ereignisse eher auf der  kürzeren  Seite,  ich  befürchte,  dass die kritische Phase potenziell  unkontrollierbarer  KI  noch  innerhalb  dieses Jahrzehnts eintreten könnte. Aber selbst, wenn ich damit falsch liegen sollte, müssen  wir  uns  auf  diese  Möglichkeit  vorbereiten,  damit  wir  nicht  Gefahr  laufen, wie bei Covid-19 und dem Klimawandel, schon wieder eine böse Überraschung zu erleben.

Du  hast  zuvor  von  unkontrollierbarer KI als existenziellem Risiko gesprochen. Unter diesem Begriff werden Ereignisse gefasst, deren Eintreten die Existenz der gesamten  Menschheit  gefährden  oder dauerhaft  und  schwerwiegend  ihr  Entwicklungspotenzial  beschränken  würde. Beispielsweise globale Naturkatastrophen  aber  auch  von  Menschen  (mit-) verursachte Katastrophen wie extremer  Klimawandel,  Bioterrorismus  oder ein  atomarer  Weltkrieg.  Du  hast  gesagt, die Gefahr bestünde darin, dass eine KI nicht  das  ihr  vorgegebene  Ziel  verfolgt oder zwar das richtige Ziel, aber mit den falschen  Mitteln.  Hierzu  wird  oft  Nick Bostroms  (2014)  Behauptung  angeführt, selbst  eine  mit  der  Herstellung  von  Büroklammern  beauftragte  allgemeine  KI könnte sich der Kontrolle entziehen, damit niemand sie am Erreichen ihres Ziels hindern  kann.  Sind  solche  Gedankenspiele mit dem gegenwärtigen Stand der KI zu rechtfertigen, oder doch eher der Science-Fiction zuzurechnen?

Die  Begründung  dafür,  dass  unkontrollierbare  KI  ein  reales  Problem  ist,  liegt nicht in der Technik, sondern ergibt sich aus theoretischen Überlegungen, die auch in der Ökonomie in der so genannten  Prinzipal-Agent-Theorie  und  in  der Entscheidungstheorie  zum  Tragen  kommen. Es ist prinzipiell äußerst schwierig, einem Agenten, also zum Beispiel einem Dienstleister,  einen  Auftrag  so  zu  stel-

len, dass sichergestellt ist, dass dieser genau das tut, was man möchte. Jeder, der schon mal ein Haus gebaut hat, weiß das. Einerseits  ist  das  ein  Kommunikationsproblem - wie sage ich dem anderen, was ich genau will? Andererseits ein Problem des Interessenkonflikts - der Dienstleister  will  vielleicht  möglichst  wenig  Leistung  für  das  zugesagte  Geld  erbringen, um seinen eigenen Profit zu maximieren. Dann  gibt  es  noch  das  Messproblem  - nicht abgeschaltet zu werden. Wenn wir das Ziel der KI ändern, kann sie ihr ursprüngliches  Ziel  ebenfalls  nicht  mehr erreichen, also ist es ein instrumentelles Teilziel  dieses  Ziels,  nicht  geändert  zu werden. Auch Macht und Einfluss zu erhöhen sind instrumentelle Ziele. Und die stehen unserem Wunsch entgegen, die KI kontrollieren, ihr Ziel ändern und sie notfalls abschalten zu können. Heutige KIs wie GPT-4 oder Gemini sind noch keine

## Der ,optimale Weltzustand', den die KI auf Basis ihres Ziels anstrebt, ist höchstwahrscheinlich nicht mit unseren Wünschen und oft auch nicht mit unserem Überleben vereinbar.

wie stelle ich eigentlich fest, ob das, was der Dienstleister gemacht hat, das ist, was ich  wollte?  Baumängel  treten  zum  Beispiel  oft  erst  nach  Jahren  auf.  Alle  drei Probleme kommen bei KI noch deutlich stärker zum Tragen als bei menschlichen Dienstleistern, denn wir verstehen KI viel weniger als Menschen und man kann von einer KI auch keinen Schadensersatz fordern oder sie vor Gericht stellen. Wenn die  KI  dann  auch  noch  intelligenter  ist als  wir  und  vielleicht  besonders  gut  darin,  zu  lügen und Menschen zu manipulieren, wird es noch schwieriger.

Voraussetzung unkontrollierbarer KI wäre also demnach, dass KI eigene Ziele entwickelt  und  zudem  erkennt,  dass  es zum Erreichen dieser Ziele beiträgt, Kontrolle auszuüben?

Sobald die KI irgendein Ziel verfolgt und in der Lage ist, auf Basis eines komplexen Weltmodells einen Plan für die Zielerreichung  zu  erstellen,  der  auch  sie  selbst beinhaltet,  kommt  es  nahezu  zwingend zu  einem  Interessenkonflikt.  Denn  aus einem beliebigen Ziel ergeben sich so genannte instrumentelle Ziele, die für fast alle übergeordneten Ziele dieselben sind. Wenn die KI zum Beispiel abgeschaltet wird, kann sie ihr Ziel nicht mehr erreichen. Also ist es ein instrumentelles Ziel,

Agenten im hier gemeinten Sinn, sie erstellen keine langfristigen Pläne, die sie dann systematisch verfolgen. Aber wenn wir eine bestimmte Schwelle überschreiten,  könnte  es  sein,  dass  eine  solche agentische KI entsteht und sich unserer Kontrolle entzieht. Und dann ist es per se äußerst unwahrscheinlich, dass sie genau das tut, was wir wollen. Denn der ,optimale  Weltzustand',  den  die  KI  auf  Basis  ihres  Ziels  anstrebt,  ist  höchstwahrscheinlich  nicht  mit  unseren  Wünschen und oft auch nicht mit unserem Überleben vereinbar.

Das  Büroklammer-Beispiel  von  Bostrom  ist  bewusst  überzogen,  aber  ich glaube,  ich  könnte  Dir  zu  nahezu  jedem  Ziel,  das  Du  mir  nennst,  eine  extreme Lösung beschreiben, bei der die Zukunft der Menschheit zerstört wird. Und ich bin keine superintelligente KI. Stuart Russell hat es mal so ausgedrückt: ,Wenn wir in der Beschreibung des Ziels für die KI eine Variable vergessen, die uns wichtig  ist,  dann  wird die KI diese Variable womöglich auf einen Extremwert setzen.' Ein naheliegendes Beispiel wäre der Klimawandel: Für fast alle Ziele wäre es aus Sicht der KI nützlich, möglichst viel Rechenleistung  zu  haben,  was  im  Extremfall  riesige  weltweite  Serverfarmen  und eine rapide globale Erwärmung zur Folge haben könnte. Das könnte man natürlich bei  der  Zielformulierung  spezifisch  ausschließen, aber der CO2 -Anteil in der Atmosphäre ist selbst ein Beispiel für eine kritische Variable, die wir noch gar nicht kannten,  als  wir  mit  der  Industrialisierung begannen.

Vielleicht  führt  der  Begriff  allgemeine KI hier auch in die Irre. Ist allgemeine KI tatsächlich  eine  Voraussetzung  für  den Kontrollverlust  oder  könnten  nicht  auf wenige Aufgaben spezialisierte Systeme, wie  wir  sie  bereits  besitzen,  zu  einem Kontrollverlust führen?

In der Tat ist es ein Problem, dass wir den Menschen als Maßstab nehmen, um die Leistungsfähigkeit  einer  KI  und  damit auch  ihre  potenzielle  Gefährlichkeit  zu beurteilen. KI funktioniert völlig anders als unsere eigene Intelligenz. So, wie ein Flugzeug  nicht  mit  den  Flügeln  schlägt und  keine  Federn  hat,  ,denkt'  KI  nicht wie wir.

Schon jetzt sind KIs Menschen in vieler Hinsicht überlegen, nicht nur in Spielen  wie  Schach  oder  Go,  sondern  auch zum  Beispiel  in  der  schieren  Menge des Wissens, das sie verarbeiten können und in der Geschwindigkeit. Wer spricht schon wie GPT-4 dutzende Sprachen fließend und kann ein Essay in 30 Sekunden schreiben? Die agentische Planungsfähigkeit, die ich beschrieben habe, verbunden mit einer übermenschlichen Fähigkeit zur Manipulation  von  Menschen  oder  technischen  Systemen  könnten  Eigenschaften einer potenziell unkontrollierbaren KI sein.  Die  KI  könnte  dann  vielleicht  immer noch vieles nicht,  was  ein  Mensch kann, zum Beispiel ein Spiegelei braten. Aber wenn die KI geschickt genug wäre, könnte  sie  uns  Menschen  dazu  bringen, zu tun, was ihrem Ziel nützt, so wie ein menschlicher  Diktator  die  Massen  manipulieren und ihnen seinen Willen aufzwingen  kann.  Es  sind  auch  Szenarien denkbar, bei denen die KI sich selbst im Internet  verbreitet  und  technische  Systeme manipuliert, oder ein militärisches System  könnte  außer  Kontrolle  geraten und einen Weltkrieg verursachen (Future of  Life  Institute  2023).  Das  größte  Problem dabei ist, dass wir bereits heutige

KI kaum verstehen und deswegen extrem schwer  vorhersagen  können,  wann  und wie  zukünftige,  noch  leistungsfähigere Systeme außer Kontrolle geraten könnten.

Nimmt  man  deine  Befürchtungen  ernst, so  scheint  die  einzige  Möglichkeit,  den Untergang der Menschheit zu verhindern, darin zu bestehen, auf die Weiterentwicklung von KI gänzlich zu verzichten oder sie doch zumindest extrem streng zu regulieren.  Im  März  2023  veröffentlichte das  Future  of  Life  Institute  einen  offenen  Brief,  in  dem  gefordert  wurde,  die Entwicklung von KI Systemen, die mächtiger  sind  als  GPT-4,  für  mindestens sechs Monate zu unterbrechen und diesen Zeitraum zu nutzen, um Sicherheitsprotokolle zu entwickeln, die eine sichere Weiterentwicklung gewährleisten. Das Moratorium kam zwar nie zustande, aber im Dezember 2023 wurde zumindest im Rahmen  des  AI-Acts  der  EU  beschlossen,  dass  Hersteller  sogenannter  ,general  artificial  intelligence  systems',  mit deren Einsatz große systemische Risiken verbunden sind, Modellevaluierungen durchführen, systemische Risiken bewerten  und  abschwächen,  Schwachstellen  analysen  durchführen  und  der  Kommission  über  schwerwiegende  Vorfälle  berichten müssen. Zudem müsse die Cybersicherheit gewährleistet werden. Erscheinen  Dir  diese  Maßnahmen  hinreichend, um einen Kontrollverlust zu vermeiden? Ich würde nicht sagen, dass die Weiterentwicklung von KI insgesamt gestoppt werden sollte. Im Gegenteil glaube ich, dass hochentwickelte  KI  in  vielen  Anwendungsgebieten extrem nützlich ist. Mein Lieblingsbeispiel ist AlphaFold - eine KI, die das so genannte Protein-Faltungsproblem gelöst und so die pharmazeutische und medizinische Forschung enorm vorangebracht hat. AlphaFold kann weit besser als der Mensch die räumliche Struktur von Proteinen auf der Basis der chemischen  Formel  vorhersagen,  was  sehr wichtig ist, um die Wirkung auf den Organismus einzuschätzen. Theoretisch kann das natürlich auch missbraucht werden, um zum Beispiel biologische Waffen zu entwickeln, aber es besteht keine Gefahr, dass AlphaFold oder ein ähnliches spezialisiertes System jemals instrumentelle  Ziele  entwickelt  und  unkontrollierbar  wird.  Ich  glaube,  dass  fast  alle  wissenschaftlichen und gesellschaftlichen Probleme von solchen spezialisierten KIs gelöst werden könnten, ohne dass wir dabei  die  Zukunft  der  Menschheit  riskieren. Eine Gefahr geht vor allem von allgemeinen KIs aus, die nahezu beliebige Probleme  lösen  und  langfristige  Pläne

verzichten und die Energie und Investitionen stattdessen in spezialisierte KIs wie AlphaFold zu stecken, zumindest so lange, bis  wir  allgemeine  KI  besser  verstehen und wissen, wie wir sie sicher kontrollieren können. Deshalb unterstütze ich Forderungen  nach  einem  Moratorium  -  alles,  was  das  aktuelle  ,Wettrennen  übers Minenfeld' zwischen Google/Deepmind, Microsoft/OpenAI,  Meta  und  noch  ein

Agentische Planungsfähigkeit verbunden mit einer übermenschlichen Fähigkeit zur Manipulation von Menschen oder technischen Systemen könnten Eigenschaften einer potenziell unkontrollierbaren KI sein.

schmieden  können.  Solche  ,Universalgenies' brauchen wir eigentlich gar nicht. Natürlich  sind  sie  in  gewisser  Hinsicht praktischer  als  spezialisierte  KIs,  weil man eben fast alles mit nur einer KI machen kann, und wirtschaftlich erscheinen sie sehr attraktiv. Aber sie sind eben auch viel gefährlicher.

Was den AI Act angeht, freue ich mich, dass die deutsche Forderung, ausgerechnet die sogenannten ,foundation models', also zum Beispiel große Sprachmodelle wie  GPT-4,  von  der  Regulierung  auszuklammern,  sich  offenbar  nicht  durchgesetzt hat. Ob allerdings die in der Beschlussvorlage genannten Regulierungen  am  Ende  wirklich  etwas  bringen, muss sich noch zeigen. Ich bin da generell skeptisch, da wir ja nicht einmal im Ansatz verstehen, wie komplexe Systeme wie GPT-4 funktionieren, warum sie zum Beispiel  bestimmte  Entscheidungen  so und nicht anders treffen. Ich glaube, ab einem  bestimmten  Punkt  der  Leistungsfähigkeit  ist  es  nicht  mehr  möglich,  bei einer allgemeinen KI vorauszusagen, ob sie  unkontrollierbar werden könnte. Wo genau dieser Punkt liegt, weiß allerdings niemand. Daher wäre es aus meiner Sicht das  Klügste,  tatsächlich  auf  die  Weiterentwicklung solcher allgemeinen KIs zu paar  anderen  verlangsamt,  ist  hilfreich. Darüber  hinaus  wünsche  ich  mir  mehr Forschung zum besseren Verständnis der notwendigen  und  hinreichenden  Bedingungen für die Unkontrollierbarkeit von KI, so dass wir besser als heute ,rote Linien'  definieren  können,  die  niemand bei  klarem  Verstand  überschreiten  wollen würde.

## Literatur

Bengio, Joshua (2023): FAQ on catastrophic AI risks. In yoshuabengio.org, 24. 06. 2023. Available online at https://yoshuabengio.org/2023/06/24/ faq-on-catastrophic-ai-risks, last accessed on 09. 01. 2024.

Bengio, Yoshua, et al. (2023): Managing ai risks in an era of rapid progress. In: arxiv.org, 12. 11. 2023. https://doi.org/10.48550/arXiv.2310.17688

Bostrom, Nick (2014); Superintelligence. Paths, dangers, strategies. Oxford: Oxford University Press.

Center for AI Safety (2023): Statement on AI risk. Available online at https://www.safe.ai/ statement-on-ai-risk, last accessed on 09. 01. 2024.

Future of Life Institute (2023): Artificial escalation. (Video) Available online at https://www.youtube. com/watch?v=w9npWiTOHX0, last accessed on 09. 01. 2024.

<!-- image -->

## Book review: Coeckelbergh, Mark (2022): The political philosophy of AI

<!-- image -->

Michael W. Schmidt  * , 1 

Mark Coeckelbergh starts his book with a very powerful picture based on a real incident: On the 9 th  of January 2020, Robert Williams was wrongfully arrested by Detroit police officers in front of his two young daughters, wife and neighbors. For 18 hours the police would not disclose the grounds for his arrest (American Civil Liberties Union 2020; Hill 2020). The decision to arrest him was primarily based on a facial detection algorithm which matched Mr. Williams' driving license photo with the picture of a man who was suspected of watch theft two years earlier. Not only did the computer 'get it wrong' as one of the detectives said, when Mr. Williams made them aware that the picture of the suspect obviously wasn't resembling him, the probably unreliable algorithm very likely contributed to racial discrimination (Hill 2020).

It  is  well  documented  that  many  available  facial  detection algorithms  at  this  time  had  significant  problems  (e.g.  a  comparably high false positive rate) with respect to black persons, like Mr. Williams (NIST 2019). Multiple causes may exist, such as  unbalanced  training  datasets  and  insufficient  optimization. Coeckelbergh  compares  the  disturbing  case  of  Mr. Williams with a political interpretation of Franz Kafka's The Trial , where the protagonist, Josef K., is accused of an unspecified crime by an opaque, oppressive and absurd bureaucracy: 'In the 21st-century United States, Josef K. is black and is falsely accused by an algorithm, without explanation' (p. 2).

This dire picture highlights that the emerging technology of artificial  intelligence  (AI),  in  its  various  forms,  is  ever  more pervading our societies and impacting our collective or individual lives. And, that it is naïve to consider AI technologies, or any other technology, as a per se politically neutral tool (pp. 3 f., 59 ff.; Winner 1980). Arrests based on AI processed evidence might, especially for marginalized groups, lead to serious harm. Decisions by an automated vehicle are also sometimes a mat-

* Corresponding author: michael.schmidt@kit.edu

1 Institute for Technology Assessment and Systems Analysis, Karlsruhe Institute of Technology, Karlsruhe, DE

<!-- image -->

© 2024 by the authors; licensee oekom. This Open Access article is licensed under a Creative Commons Attribution 4.0 International

License (CC BY).

https://doi.org/10.14512/tatup.33.1.68 Published online: 15. 03. 2024

<!-- image -->

ter of life and death, especially for vulnerable road users. Deep fakes in social media might significantly impact the outcome of democratic elections. From a social perspective, therefore, the application of AI involves considerable associated risks. However, there are also relevant opportunities. How to assess the impacts of AI technologies on our values and how to act on this assessment? According to Coeckelbergh, we need clarity concerning inherently political concepts, such as freedom , democracy and justice , to adequately answer these questions. What do we mean when we say an AI based decision racially discriminated? Political philosophy deals with the theoretical and normative reflection of such concepts and can help explicating our concerns  and  expectations,  situating  them  within  our  web  of beliefs  and  assessing  their  strength  and  plausibility.  Coeckelbergh's book aims to enable scholars from multiple disciplines and fields to constructively pick up threads from political philosophy for their research and to contribute to the quality of the general public debate on AI.

## Freedom, justice, democracy, power and a challenged anthropocentrism

Coeckelbergh organizes the overview of the possible combination  of  political  philosophy  and  reflection  on  AI  by  discussing important concepts and theoretical approaches from political philosophy. To give an example from chapter 2, which is focused on notions connected to the concept of freedom: With regard to John Stuart Mill's liberalism we can locate the burden of proof for the legitimacy of AI aided predictive policing or pervasive surveillance on the side of those who approve, or use, these coercive or intrusive measures (pp. 11-16). Further issues discussed in this chapter include AI-aided manipulation and the critique of libertarian paternalism or self-realization and emancipation  regarding  the  commodification  of  personal  data  and AI-based automation.

The other main chapters are concerned with discussions related to justice (chapter 3), democracy (chapter 4), power (chapter 5) and challenges to anthropocentrism by post- and transhumanist theory (chapter 6). In general, Coeckelbergh succeeds in not only providing a comprehensive overview of important

<!-- image -->

Coeckelbergh, Mark (2022):

The political philosophy of AI. An introduction.

Kriterien für eine menschengerechte Gestaltung Cambridge, MA: Polity Press

176 pp., 19,99 €, ISBN 9781509548545

https://doi.org/10.14512/tatup.33.1.68

<!-- image -->

debates from political philosophy for the reflection on AI, but also presents a considerable variety of theoretical approaches in a well-informed and accessible way. These include the systems of thinkers such as Hegel, Marx, Arendt or Foucault, varieties of liberalism, libertarianism, critical theory, radical or agonistic theories of democracy, as well as identity-based approaches and post-colonialism.

Of course, some aspects are open to criticism. Chapter 4 focuses intensively on social media. However, it is not easy to identify the degree to which AI or rather other features of current social media, like the for-profit orientation, are responsible for the formation of filter bubbles and echo chambers. More attention ingly consist of complex socio-technical systems. Correspondingly, there is a recent trend to call for political philosophy or normative inquiries in collective action - as Maarten Franssen framed it at the Forum for Philosophy, Engineering, and Technology 2023 in Delft (Mitcham forthcoming) - in philosophy of technology and engineering as well as in technology ethics (Himmelreich 2019).

Coeckelbergh's book is a much-needed introduction of how to relate political philosophy to fields and disciplines concerned with the reflection on technology. As Coeckelbergh stresses, political philosophy cannot be simply applied to technology and engineering (p. 150) - conceiving socio-technical systems as es-

## In light of currently emerging disruptive technologies the political dimension of engineering and technology seems to become more apparent again.

could have been devoted to the increasing risk deep fakes pose for democracies' public sphere.

Not all relevant topics and concepts are covered extensively. One  example  is  utopian  thinking  within  political  philosophy. Naturally, it must be said that it is simply unfeasible to cover everything in an introductory book. A related fact is that the book tends to focus on the risks of AI. The opportunities with regard to our social values, also deserves due consideration. Can AI-aided translation, for example, enable a real agora for language wise rather Babylonian structures such as the European Union?

## Reflection on technology and engineering needs more political philosophy - and vice versa

Coeckelbergh concludes (chapter  7)  with  the  provocative  thesis that 'political philosophy in the 21st century can no longer be done, and should no longer be done, without responding to the  question  concerning  technology'  (p. 150)  and  speculates, whether there should ultimately be a merging between reflection on technology and reflection on politics. The latter might be slightly exaggerated; however, I fully agree with the diagnosis that philosophy of technology and engineering and related fields need more political philosophy - and vice versa.

Carl Mitcham explains that '[…] despite the early presence of  political  philosophy  in  1970s  philosophy  of  technology,  it has tended for more than 50 years to be marginalized: First, by an emphasis on ethics alone separated from politics; second, by a turn away from ethics itself' (Mitcham forthcoming). In light of currently emerging disruptive technologies - besides and in combination with AI, for example, gene editing, quantum computing or nuclear fusion - the political dimension of engineering and technology seems to become more apparent again. To speak in Rawlsian terms: the basic structures of our societies increas- sential parts of the basic structure of society, will lead us to rethink and interpret established theories, such as Rawls's political liberalism (Binns 2018; Gabriel 2022; Hoffmann 2020). Work, defending a theoretical position at this intersection, is a necessary next step.

## References

American Civil Liberties Union (2020): Wrongfully arrested because of flawed face recognition technology. Available online at https://www.youtube.com/ watch?v=Tfgi9A9PfLU (video), last accessed on 23. 01. 2024.

Binns, Reuben (2018): Algorithmic accountability and public reason. In: Philosophy &amp; Technology 31 (4), pp. 543-556. https://doi.org/10.1007/s13347-0170263-5

Gabriel, Iason (2022): Toward a theory of justice for artificial intelligence. In: Daedalus 151 (2), pp. 218-231. https://doi.org/10.1162/daed\_a\_01911

Hill, Kashmir (2020): Wrongfully accused by an algorithm. In: The New York Times, 24. 06. 2020. Available online at https://www.nytimes.com/2020/06/24/ technology/facial-recognition-arrest.html, last accessed on 23. 01. 2024.

Himmelreich, Johannes (2019): Ethics of technology needs more political philo- sophy. In: Communications of the ACM 63 (1), pp. 33-35.

Hoffmann, Anna Lauren (2020): Rawls, information technology, and the sociotechnical bases of self-respect. In: Shannon Vallor (ed.): The Oxford handbook of philosophy of technology. Oxford: Oxford University Press, pp. 230-249. https://doi.org/10.1093/oxfordhb/9780190851187.013.15

Mitcham, Carl (in press): Brief for the political philosophy of engineering and technology. In: Science and Engineering Ethics.

NIST - National Institute of Standards and Technology (2019): NIST study evaluates effects of race, age, sex on face recognition software. Gaithersburg, MD: NIST. Available online at https://www.nist.gov/news-events/news/2019/ 12/nist-study-evaluates-effects-race-age-sex-face-recognition-software, last   accessed on 23. 01. 2024.

Winner, Langdon (1980): Do artifacts have politics? In: Daedalus 109 (1), pp. 121-136.

<!-- image -->

## Book review: Timon, McPhearson; Nadja, Kabisch; Niki, Frantzeskaki (eds.) (2023): Nature-based solutions for cities

<!-- image -->

Jaewon Son  * , 1  , Somidh Saha 1, 2 

<!-- image -->

What are nature-based solutions (NbS)? The International Union for Conservation of Nature (2017) defines them as 'actions to protect, sustainably manage, and restore nature or modified ecosystems,  that  address  societal  challenges  effectively  and adaptively, simultaneously providing human well-being and biodiversity  benefits'.  For  example,  trees  are  a  classical  object of  NbS  that  provides  shade,  cool  down  the  temperatures,  remove  carbon  dioxide,  and  can  contribute  to  mental  and  psychological health. For cities to be prepared and recover from extreme weather conditions, it is crucial to understand the interplay between ecological, technical, and social systems. The editors  of  the  open-access  book  'Nature-Based  Solutions  for Cities' (2023), Timon McPhearson, Nadja Kabisch, and Niki Frantzeskaki, define NbS as an 'umbrella term' (p. 4), 'bringing together knowledge and expertise developed over the years' (p. 4 f.). While not every literature on NbS covers the technology side  or  discusses  the  connection  between  society,  technology, and the environment, the anthology provides a comprehensive framework by showing the interlinkages between social, ecological, and techno  logical-infrastructural dimensions in urban settings, covering five key sub-themes:

1. NbS 'for what and for whom?' (pp. 14-49)
2. The 'nature' of NbS (pp. 50-105)
3. 'The multiple benefits' of NbS (pp. 106-214)
4. NbS 'governance, planning and value' (pp. 215-295)
5. 'Engaging art and design for and with NbS' (pp. 296-375)

* Corresponding author: jae.son@partner.kit.edu

- 1 Institute for Technology Assessment and Systems Analysis,

Karlsruhe Institute of Technology, Karlsruhe, DE

2 Institute for Geography and Geoecology, Karlsruhe Institute of Technology, Karlsruhe, DE

<!-- image -->

© 2024 by the authors; licensee oekom. This Open Access article is licensed under a Creative Commons Attribution 4.0 International

License (CC BY).

https://doi.org/10.14512/tatup.33.1.70 Published online: 15. 03. 2024

<!-- image -->

Following the announcement of presentation of 'multi-disciplinary' (p. 4) approaches and projects, each chapter deals with a specific topic using inter- and transdisciplinary methods and illustrating global case studies to provide the state of the art in enhancing NbS in cities.

## Co-existence and co-habitation

The editors explain the role of NbS in providing benefits including the well-being of mental and physical aspects of the public, and addressing environmental justice. The book highlights that NbS is unevenly distributed within urban settings and socio-economic backgrounds such as income, gender, and race influence people's access to the benefits of urban nature. Thus, disseminating and co-designing NbS with the public, and advocating a participatory decision-making process is necessary in order to address the unequal distribution of nature in the city.

Due to multi-disciplinary nature of the subject, the editors refrain from providing a single definition of the term 'nature'. Nonetheless, their perspectives and proposed solutions seem to align predominantly in a bio-ecological understanding by stating, that cities are 'laboratories of the future where nature and people co-exist and co-habitate' (p. xii). A broader understanding of 'nature' beyond the content of the book could also include basic infrastructure like internet or transportation which can in turn correspond with two factors very important also for NbS: stakeholder engagement and public participation.

The fifth part of the book deals with 'engaging art and design for and with nature-based solutions'. While Brian McGrath et  al.  develop  an  'integrative notion of urban design as an array  of  small-scale  practices  […  for]  regenerative  social-natural processes' (p. 296), the editors share in their final chapter (16) an ambitious vision: a 'just, equitable, resilient, and sustainable landscape of virtuous relations among people, nature, and infrastructure'  (p. 364).  To  achieve  this  vision,  McPhearson, Kabisch, and Frantzeskaki provide seven insights based on the synthesis of key findings throughout the book - among others: placing NbS at the forefront of urban climate change adaptation; prioritizing equity and justice in NbS design, planning, and management; implementation of NbS for human health im-

<!-- image -->

Timon, McPhearson; Nadja, Kabisch; Niki, Frantzeskaki (eds.) (2023): Nature-Based Solutions for Cities. Cheltenham: Edward Elgar Publishing. 408 pp., 130 £, ISBN 9781800376755

https://doi.org/10.14512/tatup.33.1.70

<!-- image -->

provement; adapting governance approaches in responds to local contexts. While those insights are sound and provide an overarching framework for how NbS should aim for, they remain broad and high level, catering primarily to the science policy community. However, they may not offer sufficient guidance for practitioners seeking more actionable approaches.

## Society, technology, and the environment

The decline of biodiversity and ecosystem quality impacts all individuals, underscoring the indispensable role of nature in sustaining the world economy. According to Dasgupta (2023), there are three essential capitals: produced capital, such as buildings and roads; human capital, such as health and education; and natural capital, including ecosystems. As these factors show the interlinkages  of  nature,  society,  and  economy,  the  book  emphasizes aiming for sustainable, resilient, and equitable cities, and highlights the role of NbS in achieving these goals by working with the technical, social, and environmental sectors.

Within the multidimensional framework of the socio-ecological systems, social components encompass planning, management, policy, institutional capability, cultural perspectives, and societal standards. Ecological sides cover biodiversity, weather, climate,  and  ecosystem.  In  contrast,  technological  factors  involve sensors and monitoring systems based on the Internet of Things, autonomous systems, and physical infrastructure such as dams, and basins. Collaborations are necessary to decide suitable NbS types for specific social challenges and locations. Moreover,  incorporating  public  demands  is  crucial  in  the  development and management of NbS which may vary across different demographic groups, even within the same city or country. Thus, understanding and accommodating this variance within various  demographic  backgrounds,  such  as  gender,  income, ethnicity, and education, is integral for distributive justice and accessible  solutions  for  all,  ensuring  'positive  rights  to  benefits' (p. 367).

## Nature-based solutions and technology assessment

For  the  technology  assessment  (TA)  community,  it  is  important to note that NbS covers diverse topics that resonate with TA. These range from the assessment of the socio-ecological benefits to systemic solution development at the intersection of technology, and other sectors, including policy, governance, regulation,  social  innovation,  finance,  and  business.  On  the  one hand,  the  environmental  impact  of  technology  such  as  greenhouse emissions should be acknowledged to facilitate collaborative efforts across sectors to minimize trade-offs and increase synergies. On the other hand, it is important to acknowledge the  application  of  technology  in  our  interactions  with  nature. For example, urban and peri-urban forests, considered 'green infrastructure', can be systemically categorized using technological applications. Using platforms like GitHub, city tree registers with detailed information on tree locations and species can be accessible to the public. Employing software tools such as 'i-Tree' (USDA et al. 2023), may enable quantification and eval- uation of ecosystem services and highlight the benefits provided by nature, including carbon sequestration and storage.

An example of citizen awareness is placing QR codes on trees to provide direct access to tree information and benefits, and simultaneously provide a platform for their participation in surveys and feedback (Son and Saha 2022). In addition, various citizen engagement platforms and online software such as 'Maptionnaire' (Kyttä et al. 2023) have been utilized in projects and research for including citizens' insights in the decision-making.

## From global insights to local adaptation

Overall, the book suggests ways how NbS should be governed, and  conceptualize  cross-sectoral,  multi-species,  intergenerational, epistemic, and spatial inclusivity. Notably, the discussion addresses not only traditional methodologies but also unconventional practices, including art and storytelling. 'Nature-Based Solutions for Cities' provides a comprehensive exploration of NbS with diverse global case studies; however, the book falls short  in  guiding  readers  on  bridging  the  gap  between  theory and practice. The unique environmental characteristics of each location bring the need for NbS to be adapted to specific local settings, emphasizing the necessity of practical insights. While the global NbS cases can provide valuable insights, they are not sufficient in guiding readers on the adaptation and implementation of these solutions in their specific context. Nevertheless, the book remains a recommendable resource for those seeking a deeper understanding of NbS. As clear highlight and exception for scientific literature the colorful illustrations by Alyssa Dennis in coordination with the respective chapter contents are to be mentioned. As the field continues to evolve, future works might consider addressing the implementation gap to enhance the guidance available for local adaptation and practical implementation.

## References

Dasgupta, Partha; Besley, Tim (2023): Biodiversity. A conversation with sir Partha Dasgupta. In: Annual Review of Economics 15, pp. 755-773. https:// doi.org/10.1146/annurev-economics-042423-044154

International Union for Conservation of Nature (2017): ICUN 2017. International Union for Conservation of Nature annual report 2017. Gland: IUCN. Available online at https://portals.iucn.org/library/node/47536, last accessed on 23. 01. 2024.

Kyttä, Marketta; Fagerholm, Nora; Hausner, Vera; Broberg Anna (2023): Maptionnaire. In: Charla Burnett (ed.): Evaluating participatory mapping software. Cham: Springer, pp. 71-91. https://doi.org/10.1007/978-3-031-19594-5\_4 Conference 'Digital Future(s). TA in and for a Changing World' (ETAC 2022),

Son, Jaewon; Saha, Somidh (2022): Digitalization of urban trees in   Karlsruhe, Germany. Poster presented at the 5 th  European Technology   Assessment 25.-27. 07. 2022, Karlsruhe, Germany. https://doi.org/10.13140/RG.2.2.30271. 79520

USDA - U.S. Department of Agriculture et al. (2023): i-Tree software. Available online at https://www.itreetools.org/tools, last accessed on 23. 02. 2024.

<!-- image -->

## Meeting report: 'Religion and technology in an era of rapid digital and climate change'. Conference, 2023, Chennai, IN (hybrid)

Axel Siegemund  * , 1  , John Bosco Lourdusamy 2  , Johann Fiedler 1 , Renny Thomas 3 

The conference 'Religion and technology in an era of rapid digital and climate change', co-organized by RWTH Aachen University and IIT Madras/Chennai, was held at IIT Madras from 21 to 23 November 2023. It was a timely interdisciplinary and intercultural exercise in making sense of religion and technology and a continuation of current debates in this field: For India, Thomas (2018, 2022) has described a future for science and religion beyond conflict and complementarity. In the West, a new quest for their interlinkage has come up (Jones et al. 2019). After the experience of finiteness and transcendence has been examined in the sciences (Gülker 2012), the latest research has shown how in both Asia and Europe, industry, biotechnology and environmental technologies are involved in the construction of self-transcendence (Siegemund 2022). Here, not only digital transformation, but also climate change adaptation can be seen as motivated through both secular and religious resources (Siegemund 2023).

## Technology in religious life practice and human-nature relations

Conference participants came from a variety of humanities and social sciences backgrounds: literature and film studies, history, sociology,  social  anthropology,  science  and  technology  studies, history of science, religious studies and theological studies. Their presentations clustered in four major areas.

First, the transfer of engineered products and processes from the West to Asia is a quest for indigenization including the ritual-

* Corresponding author: siegemund@kt.rwth-aachen.de

1 Institute for Catholic Theology, RWTH Aachen University, Aachen, DE

2 Department of Humanities and Social Sciences, IIT Madras, Chennai, IN

3 Department of Humanities and Social Sciences, IISER Bhopal, Madhya Pradesh, IN

<!-- image -->

© 2024 by the authors; licensee oekom. This Open Access article is licensed under a Creative Commons Attribution 4.0 International

License (CC BY).

https://doi.org/10.14512/tatup.33.1.72

<!-- image -->

istic specifications of environmental and digital technology. The transfer is being realized through bridging the Christian-humanistic origins of technology and the interconnectedness between secularity  and  multi-religiosity  in  India.  However,  the  use  of technical products also bears risks for the continuation of colonialism and dependencies. It should also be mentioned here that cultural nationalism often goes hand in hand with strong religious ideology and practice. Some ethnographic conference papers contributed to the causes of cultural nationalism by looking at how Indian right wing Hindutva groups appropriate technology and digital spaces. Technology in such cases helps the right wing nationalist groups to 'scienticize' Hinduism. Here, for example, technology is being used to create what Banu Subramaniam calls 'archaic modernities' (Subramaniam 2000).

The sociological and anthropological perspectives on digital technologies and religion were discussed by many young scholars at the conference displaying rich ethnographic data of how new technologies are used by the devotees for ritualistic purposes. At the same time, it was also pointed out that right wing groups use these technologies to express and promote cultural nationalism and religious pride. These investigations made clear that religion and religious rites can be understood as a cry of the poor and that technologized practices of religion or faith play an important role.

Second, participants discussed how the increasing interface between religion and technology also shows in the practice of Islamic rituals through digital devices, in the conversion of Adivasi (autochthonous Indian populations) from one to another religion under the influence of mass media, in the emergence of a secular lifestyle, or in a new awareness for Gandhian Spiritual Practices. There is no particular affinity of technologies to a specific religion or secular way of life here. Technology just reinforces the tendencies that exist already in society, thereby contradicting the prejudice of technology being merely a means for secularization.

Throughout the conference, it was emphasized that religion is not to be equated with faith. While faith can be seen as purely individual, collective institutions, places of worship, rites and customs play an essential role in religion. It was noted that religion can no longer be practiced only in an analogue way. Offerings via virtual temples or services via online video broadcast are taking religion to a digital level where the use of technology and a connection to technology are indispensable. Covid-19 has strongly encouraged the use of technology in practices of faith and religion  and  has  strongly  emancipated  individualized  approaches  to  faith  from  collective  religion.  This  poses  a  challenge for all religions and religious institutions, since online religion is interdenominational and interreligious. Since Covid-19 religions have to adapt and respond to individualized believers in a new way, often through digital technology. This changing field of religion and faith is an excellent example of how religion can no longer be understood as the opposite of technology, even in practice. Religion and faith utilize technology and coevolve with it.

https://doi.org/10.14512/tatup.33.1.72

<!-- image -->

Third, conference papers showed how the representation of subaltern groups in Indian society and their practices of selfhood vary through the acceptance of technologies. For example, their role changes significantly through access to the digital allowing them to explore secularism as well as Islam, Hinduism, or Christian faith. Media become a means of creating new options and changes the self-understanding.

Fourth, conference papers emphasized that there is a mutually dependent and strong interrelation between religion, nature, and humans. There is an interreligious understanding of nature as creation and of a human-nature relation to be respected and itself,  (2)  to  construct  narratives  of  human  becoming through the  coexistence  with  (invisible)  machinery  processes,  and  (3) to open up new ways of creating meaning. This can be helped by the use of audio-video recording and broadcasting, platform technology for climate mitigation, or other emergent technologies still to be designed. In this way, the era of rapid digital transformation and climate change is grounded in transcendent ideas that switch between secular and religious spheres. We need to be sensitive to the intrinsic and unavoidable effect that this era of transformation and change is exercising on worldviews as well as to the influences of faith and religions in TA.

## The era of rapid digital transformation and climate change is grounded in transcendent ideas.

protected. Religions in particular can criticize and question the role of technological development in this relation and contribute to environmental protection by opening up a fundamentally critical perspective on the human role. Technology assessment (TA) looks at technological development's impacts on freedom, life chances, but also at threats. The value of emergent technologies is regularly examined by asking in how far they transcend already existing options. Visions of future technological worlds provide an insight into the possible effects on humanity of technological development paths. On the basis of ties between religion and technology, religion was discussed as a source for climate action, such as implementing the United Nations' Sustainable Development Goals in companies as well as creating environmental awareness through religious dignitaries and grassroots work.

## Religious-technological transcendence

Contributions to the conference agreed that traditional sociological understanding of technology as the cause of the world's disenchantment does not hold true any longer, rather technology itself has become part of the religious worlds (Srinivas 2018; Geraci  2018).  However,  from  an  engineering  perspective  the most important finding is that neither religion nor technology can exist without a transcendent meaning. The acceptance of any technology is realized through imagining, boundary work, and  constructions  of  unavailable  spaces.  If  cyberspace  in  the West, for  example,  can  be  said  to  have  been  modeled  on  the trans-temporal and trans-spatial communion in the Lord's Supper, in Hinduism it can be said to develop along the lines of the temple  community.  When  emerging  technologies  address  societal hopes of the West, such as the discovery of new worlds (space technology), or realizing justice outside the boundaries of one's own life (intergenerational climate protection), they also play a role in India by modernizing mythology such as Ramayana or the rituals in Varanasi.

Potential future impacts of religious-technological transcendence are (1) to produce ideas of how the world could sacrifice

## References

Geraci, Robert (2018): Nationalism, Hinduism, and transhumanism in South Indian science. New York, NY: Rowman &amp; Littlefield.

Jones, Stephen; Catto, Rebecca; Kaden, Tom (eds.) (2019): Science, belief and society. International perspectives on religion, non-religion and the   public understanding of science. Bristol: Bristol University Press. https://doi.org/ 10.56687/9781529206968

Siegemund, Axel (2022): Grenzziehungen in Industrie- und Biotechnik. Transzendenz und Sinnbehauptungen technologischer Modernisierung in Asien und Europa. Baden-Baden: Nomos. https://doi.org/10.5771/9783748930877 Siegemund, Axel (2023): God's kingdom, but no Planet B? Religious and secular sources for common action in climate adaptation. In: Teocomunicação 53 (1), pp. e44005-e44005. https://doi.org/10.15448/0103-314X.2023.1.44005 Srinivas, Tulasi (2018): The cow in the elevator. An anthropology of wonder. Durham, NC: Duke University Press. https://doi.org/10.1215/9780822371922 Subramaniam, Banu (2000): Archaic modernities. Science, secularism, and religion in modern India. In: Social Text 18 (3), pp. 67-86. https://doi. org/10.1215/01642472-18-3\_64-67

Thomas, Renny (2018): Beyond conflict and complementarity science and religion in contemporary India. In: Science, Technology and Society 23 (1), pp. 47-64. https://doi.org/10.1177/0971721817744444

Thomas, Renny (2022): Science and religion in India: Beyond disenchantment. London: Routledge. https://doi.org/10.4324/9781003213475

## Further information

Conference program:  https://hss.iitm.ac.in/conferencesworkshops/#ratirdacc2023

<!-- image -->

Meeting report: 'Generative artificial intelligence Opportunities, risks and policy challenges'. EPTA Conference, 2023, Barcelona, ES

<!-- image -->

Steffen Albrecht  * , 1 , Reinhard Grünwald 1 

Disruptionen wurden als übergreifendes Thema bereits 2022 bei der EPTA-Konferenz in Berlin sehr vielschichtig und vorausschauend diskutiert. Im letzten Jahr konnten dann viele Menschen greifbar erleben, mit welcher Kraft eine Technologie gewohnte  Lebens-,  Arbeits-  und  Denkweisen  aufbrechen  und durcheinanderwirbeln  kann:  die  generative  künstliche  Intelligenz (KI), insbesondere in Form von ChatGPT. Insofern war es  durchaus  konsequent,  dass  die  parlamentarischen  TA-Einrichtungen ihre Jahrestagung 2023, die am 9. Oktober in Barcelona stattfand, ganz dieser Technologie und ihren Auswirkungen widmeten.

Die große Bedeutung generativer KI wurde von Anna Erra i  Solà,  der  Präsidentin  des  gastgebenden  Parlaments  von  Katalonien,  in  ihrer  Begrüßung  unterstrichen:  'Niemand  zweifelt daran, dass diese disruptive Technologie unser Leben stark beeinflussen wird. Kein Mensch, kein Wirtschaftszweig, kein Land und keine Organisation wird von der Massenverarbeitung von Daten und der systematischen Anwendung von Algorithmen ausgenommen sein' (Übersetzung der Autoren). Dass auch dem EPTA-Netzwerk Bedeutung zugemessen wird, konnte man an  der  großen  Zahl  der  angereisten  Parlamentarier*innen  ablesen. Der Deutsche Bundestag war mit vier Mitgliedern vertreten, weitere fünf Abgeordnete kamen aus Österreich und der Schweiz.

Die Politiker*innen wurden in der Keynote von Gary Marcus, KI-Entwickler wie auch -Kritiker sowie renommierter Psychologe  und  Neurowissenschaftler,  in  die  Pflicht  genommen. Marcus mahnte eine strikte Beobachtung und Regulierung der Technologie an, mit der ihre Risiken kontrolliert werden sollten.

* Corresponding author: steffen.albrecht@kit.edu

1 Büro für Technikfolgen-Abschätzung beim Deutschen Bundestag, Berlin, DE

<!-- image -->

© 2024 by the authors; licensee oekom. This Open Access article is licensed under a Creative Commons Attribution 4.0 International

License (CC BY).

https://doi.org/10.14512/tatup.33.1.74

<!-- image -->

Seine Kernbotschaft: Man darf KI nicht trauen! Im Rückblick auf frühere Versprechen der KI-Entwicklung und anhand von aktuellen Beispielen (Verstärkung von Stereotypen durch bildgenerierende KI-Systeme und Probleme von Sprachmodellen mit Logik und Fakten) verwies er auf die grundsätzlichen Unzulänglichkeiten der Systeme. Um Risiken wie etwa Desinformation, Manipulation von Verhalten und schädliche Umweltauswirkungen einzuhegen und um Transparenz, Sicherheit und Rechtmäßigkeit der Entwicklung von KI-Systemen sicherzustellen, seien internationale Einrichtungen nach dem Vorbild des Forschungszentrums CERN nötig.

## Austausch vielfältiger Perspektiven

Auch die folgenden drei Sessions wurden dem Anspruch des EPTA-Netzwerks  gerecht,  Politiker*innen  und  Wissenschaftler*innen miteinander ins Gespräch über Technologien zu bringen und dabei Kontroversen nicht zu scheuen. Für den Bereich der Bildung zeichneten die beiden Vortragenden, Carles Sierra vom Consejo Superior de Investigaciones Científicas und Enkelejda Kasneci von der TU München, ein sehr positives, durch große  Potenziale  generativer  KI  gekennzeichnetes  Bild.  KI werde Lehrende nicht ersetzen, könne aber dabei helfen, Bildung zielgenauer auf die Bedürfnisse der Lernenden abzustimmen und das Lehrpersonal etwa bei der nötigen differenzierten Vermittlung zu entlasten. Dabei müsse aber das Ziel, zukünftige Fähigkeiten zu vermitteln und Bildungsungleichheiten entgegenzuwirken, im Blick behalten werden. Das diskussionsfreudige Publikum sorgte durch den Verweis auf die negativen Auswirkungen übermäßiger Digitalisierung für eine Korrektur des aus seiner Sicht zu positiven Szenarios - und für eine perspektivenreiche Debatte.

Im Gesundheitsbereich wurde der Stand der KI-Anwendungen von den Expert*innen dagegen eher skeptisch beurteilt, wiewohl etwa Paula Petrone (Data Scientist bei ISGlobal) deutlich machte, dass die Gesundheitssysteme in den EU-Ländern dringend auf Entlastung angewiesen sind, da die Bevölkerung zwar immer länger lebt, die gesund verbrachte Lebenszeit aber nicht im gleichen Maß anwächst. KI-Anwendungen wie die generative KI böten zum Beispiel in der personalisierten präventiven Gesundheitsversorgung Potenziale. Ihre Nutzung stoße allerdings aufgrund mangelnder Qualifizierung des Personals und fehlendem Vertrauen in die KI-Systeme auf Widerstände. Zur Verbesserung der Situation wurde vorgeschlagen, KI durch vielfältigere Teams entwickeln zu lassen und durch strikte Regulierung und Aufsicht eine Vertrauensbasis zu schaffen und schädliche Auswirkungen zu verhindern.

## Generative KI als Herausforderung für die Demokratie

Generative KI kann sich sehr grundlegend auf unsere Gesellschaft auswirken und wird entsprechend intensiv in der Öffentlichkeit debattiert. Während in Bezug auf demokratische Prozesse dabei meist negative Wirkungen von ChatGPT &amp; Co. im Vordergrund stehen, beschrieb Karina Gibert von der Universi- https://doi.org/10.14512/tatup.33.1.74

<!-- image -->

tat Politècnica de Catalunya, wie diese Systeme zur Unterstützung der parlamentarischen Alltagsarbeit eingesetzt werden können, etwa bei der Recherche in Dokumentendatenbanken und bei der Zusammenfassung längerer Texte. Auch Marta R. CostaJussà, KI-Forscherin bei Meta AI, verwies auf die Potenziale generativer KI, mit der die Übersetzung gesprochener Sprachen, gerade auch von weniger stark verbreiteten Sprachen, erleichtert wird.

Doch solchen Potenzialen stehen die enormen Herausforderungen der Demokratie durch generative KI entgegen. Mehrere Vortragende betonten die Verantwortung der Entwickler*innen von KI-Systemen, deren Ausbildung durch ethische Leitlinien und einen stärkeren Austausch mit gesellschaftlichen Gruppierungen verbessert werden könnte. Die Auswahl der Datenquellen  müsse  bewusster  und  nachvollziehbar  erfolgen  und  etwa Nutzungsrechte berücksichtigen,  auch  wenn  dies  zulasten  der Geschwindigkeit der technischen Entwicklung gehe. Jerome Duberry, Managing Director des Albert Hirschman Centre on Democracy, empfahl eine Beobachtung durch staatliche Einrichtungen, wie sie in der EU bereits im Rahmen von 'AI Watch' durch  das  Joint  Research  Centre  der  Europäischen  Kommission erfolgt. Nur so ließe sich die Anwendung von KI in vielen Fällen sichtbar und damit einer öffentlichen Kritik zugänglich machen.

Wie bedacht die komplexen Abwägungsprozesse in der Politik aufgegriffen werden, machte der Abgeordnete des schwedischen Riksdag, Ulrik Nilsson, deutlich. Er verwies auf die Herausforderung (aber auch die Notwendigkeit), die technologische Entwicklung  angesichts  ihres  enormen  potenziellen  Nutzens einerseits und von Risiken wie der Verstärkung veralteter Muster  andererseits  an  menschlichen  Maßstäben  zu  orientieren. Brando Benifei, Mitglied des EU-Parlaments, machte in einer Videobotschaft die praktischen Herausforderungen der Kompromissfindung deutlich, denen er als einer der Co-Berichterstatter für den europäischen AI Act im Trilogverfahren begegnet ist. Das EU-Parlament achte besonders auf den Schutz von Grundund Freiheitsrechten und beziehe neben Expert*innen auch die Zivilgesellschaft ein.

## Gestaltungs- und Einflussmöglichkeiten für eine verantwortungsvolle Entwicklung

Auch ein zweiter grundlegender Einflussbereich von generativer KI, Arbeit, ist in der öffentlichen Debatte sehr präsent. Viele Angestellte fürchten um ihre Jobs, erste Berufsgruppen etwa in der Filmbranche haben versucht, ihre Rechte angesichts des zunehmenden Einsatzes von KI in Arbeitskämpfen zu verteidigen. Virginia Dignum von der Umeå Universitet sieht sogar alle Berufsgruppen als  zukünftig  von  KI  betroffen  an  (sofern  sie  es nicht bereits jetzt sind). Sie hinterfragte in ihrem Vortrag das Bild von KI, wie es vielfach gezeichnet wird, und verwies auf die Verzerrungen und Fehler, die der generativen KI inhärent bzw. aufgrund von Designentscheidungen zugrunde gelegt sind. Es sollte weniger darum gehen, was KI ist, als vielmehr darum, welche Art von KI wir als Gesellschaft möchten.

Zu einer ähnlichen Folgerung kam Aina Gallego von der Universitat de Barcelona, die sich stärker auf die ökonomischen Effekte generativer KI bezog. Bereits die Digitalisierung habe zu einer  Verstärkung  der  Einkommensungleichheit und zur Aushöhlung  der  Mittelschicht  geführt,  auch  generative  KI  könne sich in dieser Art auswirken. Sie könne aber auch zur Entstehung neuer Tätigkeiten und zur Verbesserung des Wohlstands aller Mitglieder einer Gesellschaft beitragen, je nachdem, wie die Anwendung von KI, die Ausbildung von Arbeitskräften und die grundlegenden arbeitsrechtlichen, wirtschafts- und technologiepolitischen Strukturen ausgestaltet werden. Damit spielten beide Rednerinnen den Ball zum Schluss der Tagung wieder an die Politik zurück.

Für die fortdauernde Aufgabe der verantwortungsvollen Gestaltung  der  Entwicklung  von  generativer  KI  haben  Wissenschaft und Technikfolgenabschätzung nicht nur bei der Tagung, sondern auch in dem zugleich veröffentlichten, mehr als 80-seitigen  EPTA  Report  2023  viele  Informationen  und  Anregungen geliefert. Für den Bericht, in dem die aktuellen Debatten um generative KI und die damit verbundenen Wissensbedarfe und Anforderungen an die Technikfolgenabschätzung in 15 der in EPTA vereinigten Länder und Regionen dargestellt werden, wurden auch die jeweiligen Parlamentarier zu ihren Sichtweisen auf die Chancen und Risiken der generativen KI und ihrer Anwendungen befragt.

Bei allen Versuchen, zukünftige Entwicklungen abzuschätzen, kam bei der Tagung nicht zuletzt auch der Blick in die Vergangenheit nicht zu kurz. Die Teilnehmenden wurden beim Besuch der Ausgrabungsstätte El Born und bei einer Führung durch das Gebäude des Parlaments von Katalonien, das einst Teil der Zitadelle und damit Symbol für die Zentralregierung in Madrid war, mit der wechselhaften Geschichte der Demokratie in Katalonien bekannt gemacht.

## Weitere Informationen

EPTA Report 2023:

https://eptanetwork.org/news/epta-news/24-publication/ 136-epta-report-2023.

Aufzeichnung der Konferenz:

https://www.parlament.cat/ext/f.p=700:3:::::P3\_ID\_ CATEGORIA:17491.

<!-- image -->

## Meeting report: 'PartWiss23'. Conference, 2023, Chemnitz, DE

<!-- image -->

Bettina Brohmann  * , 1  , Regina Rhodius 2  , Melanie Mbah 2 

Vom 22. bis 24. November 2023 kamen auf der zweiten PartWiss Konferenz mehr als 230 Teilnehmende aus allen Bereichen der Partizipationsforschung (und -praxis) in Chemnitz zusammen, um sich über konzeptionelle und methodische Ansätze zur Beteiligung verschiedenster Akteure an der Co-Produktion von Wissen in transformativen Forschungsprozessen und soziotechnischen  Kontexten  auszutauschen.  In  seinem  Eröffnungsbeitrag am Vorabend zeigte Philipp Schrögel (Uni Heidelberg) mit einer 'Kartierung der Partizipationslandschaft' den aktuell breiten Einsatz von Partizipation in unterschiedlichen Kontexten von Planungs- und Technikprozessen. Am Vortag der Konferenz fand zudem die erste Mitgliederversammlung der 2023 gegründeten  Gesellschaft  für  transdisziplinäre  und  partizipative Forschung (GTPF) statt. In ihrer Konferenz-Keynote legte die neue Vorsitzende der GTPF, Christine Ahrend (TU Berlin), den Fokus auf Akteursgruppen in Forschung und Praxis und betonte die Notwendigkeit zur Veränderung des eigenen Verhaltens in der Transformation. Besonders wichtig seien dabei das Bündeln von Kräften durch Kooperationen, ein Umdenken im Wissenschaftsbetrieb und die Berücksichtigung anderer Logiken als entscheidende Parameter in der transdisziplinären und partizipativen Forschung.

Transdisziplinäre Wissenschaft und partizipative Forschung sollen vor dem Hintergrund der komplexen Herausforderungen einer Nachhaltigkeits- und soziotechnischen Transformation zu lebensweltlichen  Problemlösungen  beitragen,  so  der  Tenor  in verschiedenen Podiumsdiskussionen. Anhand sehr unterschiedlicher Vorhaben - beispielsweise aus dem Gesundheits- und Inklusionsbereich (,Team Vielfalt'), der Digitalisierung oder der Energiewende (,SmartQuart') - die auf einer begleitenden Postersession ihre Tools und Ergebnisse zeigten - wurde deutlich, dass eine gesellschaftliche Einbettung neuen Wissens und so-

* Corresponding author: b.brohmann@oeko.de

1 Öko-Institut e. V. (Institut für angewandte Ökologie), Darmstadt, DE

2 Öko-Institut e. V. (Institut für angewandte Ökologie), Freiburg, DE

<!-- image -->

© 2024 by the authors; licensee oekom. This Open Access article is licensed under a Creative Commons Attribution 4.0 International

License (CC BY).

https://doi.org/10.14512/tatup.33.1.76

<!-- image -->

zio-technischer  Lösungen  auch  besonders  innovativer  Beteiligungsformen bedarf. Dabei scheint es von Bedeutung zu sein, dass sich Forschungsfragen und Umsetzungsstrategien im Diskurs zwischen Wissenschafts- und Praxisakteuren aus verschiedenen Interessenlagen gemeinsam entwickeln. Für diese Kooperation ist es wichtig, dass in der jeweiligen Projektphase (CoDesign;  Co-Produktion)  und  Konstellation  (gesellschaftliches Handlungsfeld, räumlicher Bezug) ein angemessenes Format gewählt wird, in dem sich die Erarbeitung und Integration neuen lösungsorientierten Wissens entfalten kann.

Der  Verbindung  partizipativer  und  transdisziplinärer  Elemente widmete sich unter anderem das Panel 'Rolle von Kontexten  für  Formate  und  Methoden  der  transdisziplinären  und partizipativen  Forschung'.  Zunächst  zeigten  Regina  Rhodius, Melanie Mbah und Bettina Brohmann (Öko-Institut Freiburg/ td Academy) eine Heuristik, die das Zusammenwirken der Methoden über den Verlauf der drei transdisziplinären ForschungsKernphasen Design, Produktion und Integration sowie der übergreifenden Evaluation darstellt. Von einem Format sprechen sie, wenn mindestens zwei der Forschungsphasen mit einem Set von Methoden systematisch ,umfasst' werden. Die Heuristik basiert auf  einer  umfangreichen  Literaturrecherche  und  empirischen Daten aus über 25 Projektkontexten. Die Auswertung der Daten machte deutlich, dass sich gesellschaftliche Anforderungen an die Mitgestaltung in der Nachhaltigkeitspraxis und -forschung ständig ändern. Hier könnten kontextsensible Formate und verschiedene Methoden helfen, den Forschungsprozess zu strukturieren und damit auch die Qualität der Forschungsergebnisse zu verbessern, so die Vortragenden. Die passenden Formate unterschieden sich dann in der Adressierung der Phasen, in der Methodenkombination, im Beteiligungsanspruch oder im Umgang mit Wirkungen.

## Akteure und Rollen

Als ein übergreifendes Thema auf der PartWiss23 konnte die ,Gewinnung' und Einbeziehung von (Praxis-)Akteuren in Forschungs-  und  Beteiligungsprozesse  identifiziert  werden.  Die Diskutanten verschiedener Sessions und eines Podiums - 'Partizipation aus der Sicht von Nicht-Wissenschaftler*innen' - verwiesen einerseits auf die besondere Rolle der Forschenden, die in frühen Phasen der Exploration und des Co-Designs vorrangig Sondierungsaufgaben bekommen. Gleichzeitig wurde von kommunalen Akteuren eine ,Kultur der Partizipation' diskutiert, die parallel zu den inhaltlichen Fragen auch auf institutionelle Anforderungen aus der Verwaltung und strukturelle Bedingungen bürgerschaftlicher Akteure achtet. Als problematisch wurde hier unter anderem die Vernachlässigung der frühzeitigen Einbeziehung von Praxispartnern seitens der Forschungsförderung konstatiert: So stünden zeitliche und finanzielle Hürden einem CoDesign oft im Wege.

Britta Oertel (IZT Berlin) verdeutlichte in ihrem Vortrag die teils  parallele,  teils  aufeinander  aufbauende  Entwicklung  von Formaten und Methoden bei transdisziplinären und partizipativen Ansätzen. Während man in der Partizipationsforschung ver- https://doi.org/10.14512/tatup.33.1.76

<!-- image -->

sucht,  eine  frühzeitige  Teilhabe  zu  realisieren,  sei  dies  in  der td- Forschung die Organisation von Co-Design. Auch analytisch sei  man hier nah beieinander, was beispielsweise die Anwendung gleicher Methoden zeige. Britta Röseners Vortrag (RWTH Aachen) ergänzte mit Blick auf Beteiligungsprojekte aus dem Klimaschutz die Wichtigkeit von Kontextbedingungen und der Abfrage von Zielen, Akteuren und lokalen Rahmenbedingungen, wie man sie aus der Stadtentwicklungsplanung bereits kenne. Sowohl in diesen Beteiligungsprozessen der Stadtentwicklung als auch in der Phase der transdisziplinären Co-Produktion seien die wissenschaftlichen Akteure vor allem in der fachlichen Integration gefordert, die auch einer besonderen Qualifikation bedürfen. Daniel Lang, Claire Grauer und Farina Tolksdorf (KIT/ ITAS, Karlsruhe) schlugen in ihrem Vortrag vor, für den Blick auf  individuelle  Kompetenzen und Kapazitäten auch den Begriff des ,Kontextes' heranzuziehen und zu berücksichtigen, um mehr Klarheit über die jeweiligen Anforderungen und Rollen der Akteure zu gewinnen. Ein gutes Verständnis der Kontextfaktoren  helfe  bei  der  erfolgreichen  gemeinsamen  Gestaltung von Prozessen.

## Reflexion, Anpassung und Wirkungen

Bei  der  Diskussion  um  die  Gelingensbedingungen  von  Forschungs- und Beteiligungsprozessen wurden Reflexivität, Transparenz und Integration als wichtige Kategorien genannt. Im Rahmen von Lightning Talks wurden auf der PartWiss23 zahlreiche Tools aus der Partizipations- und Forschungspraxis gezeigt, die  Reflexionsprozesse  und  Wirkungsanalysen  der  beteiligten Partner*innen unterstützen, wie Crowd Innovation Plattformen, Transformation Innovation Center oder WTT-Impact-Canvas.

Wirkungen  werden  grundsätzlich  differenziert  hinsichtlich ihrer Intention, Beeinflussung durch Akteure sowie ihres zeitlichen und räumlichen Kontextes, wie Martina Schäfer, Emilia Nagy und Jasmin Wiefek (ZTG Berlin/tdAcademy) in ihrem Panel 'Gesellschaftliche und wissenschaftliche Wirkungen' darstellten. Sie unterschieden zwischen zwei bzw. drei Wirkungsordnungen: Der Wirkung erster Ordnung (intendiert und weitgehend steuerbar), der Wirkung zweiter Ordnung (nah am Kontext und nur begrenzt steuerbar) sowie der Wirkung dritter Ordnung (jenseits des Projektkontextes). Welche Herausforderungen damit jeweils verbunden sind, diskutierten ein Workshop zu Synergien und ,Trade-offs' gesellschaftlicher und wissenschaftlicher Wirkungen sowie ein darauf aufbauender Workshop zu ,Monitoring der Ergebnisse und Wirkungen von transdisziplinärer und partizipativer  Forschung'  (Wiefek,  Schäfer,  Nagy,  Lux,  ZTG/ ISOE/tdAcademy). In der Forschungspraxis lässt sich nur selten ein enges Ursache-Wirkungsverhältnis beschreiben, wie die Diskussion in verschiedenen Workshops der Konferenz zeigte: Forschungstätigkeiten könnten vorrangig Wirkungspotenziale aufbauen, wobei es hier sehr entscheidend auf die Prozessqualitäten ankomme, unter denen geplante Produkte, wie neue Modelle, Konzepte oder Dienstleistungen entwickelt werden. Die Reflexion und Anpassung von Formaten könnten dabei unterstützend wirken, so die Einschätzung von Teilnehmenden.

Abb. 1: Kultur der Partizipation.

<!-- image -->

## Perspektiven

Auf  der  PartWiss23-Konferenz  konnten  einige  Unterschiede zwischen Partizipations- und transdisziplinärer Forschung deutlich  herausgearbeitet,  aber  auch  zahlreiche  Schnittstellen  und Gemeinsamkeiten identifiziert werden. Als eine große Herausforderung - in beiden Bereichen - wurden die notwendigen Ressourcen und die Bedingungen im Hinblick auf Kapazitäten und Herkünfte  der  beteiligten  Akteure  und  Institutionen  genannt. Wie gelingt es, gleichberechtigt miteinander an Forschungsthemen und ihrer wirksamen Umsetzung zu arbeiten, bislang nicht beteiligte  Akteursgruppen  einzubeziehen  und  in  krisenhaften Kontexten, wie der COVID19-Pandemie geeignete, kurzfristig einsetzbare Formate zu finden? Diesen Fragen, aber auch Wünschen und Ideen aus dem bürgerschaftlichen Engagement wird sich die nächste PartWiss-Konferenz im Herbst 2024 widmen.

## Weitere Informationen

https://partizipation-wissenschaft.de/index.php/konferenz2023-tu-chemnitz/

Graphic recording: Stephanie Brittnacher

## TA  TuPDates 33/1 (2024)

News from the editorial office/Meldungen aus der Redaktion

## PUBLISHING IN TATuP

Publish your research results in TATuP - Journal for Technology Assessment in Theory and Practice . We welcome a variety of text types like reviews, meeting reports, short essays, replicas or artistic perspectives regarding the topic of technology assessment. We don't charge Author Processing Charges (APC).

www.tatup.de/index.php/tatup/about/submissions

## THE EDITORIAL TEAM GROWS

As of April 2024, Dr. Ralf Schneider will join the editorial team as academic editor. Welcome, Ralf!

## SUBMIT ARTICLES AND READ ONLINE

<!-- image -->

www.tatup.de

## STAY UP TO DATE

<!-- image -->

www.oekom.de/newsletter

## COMMENT, SHARE, LIKE

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

www.mastodon.social/@TATuP

www.twitter.com/TAjournal

www.tatup.de/linkedin

www.tatup.de/youtube

33/2 (2024) will be published in June 2024 with the Special topic

## 'The ambivalence of emergent technologies:   Malevolent creativity and civil security'

The democratic decline that is evident throughout the world. It poses challenges to society, some of which are amplified by new technologies in areas ranging from communications to artificial intelligence. Social media enables the radicalization of vulnerable individuals without the need for physical support structures or networks. The question of how technology might be used - deliberately - by malevolent actors to undermine civil security has rarely been addressed in technology assessment research, sometimes as a by-product, sometimes in  thematic  sidebars  (usually  as  dual-use  capabilities,  i. e.,  military and civilian uses).

This  Special  topic  will  address  questions,  such  as:  What  are  the repercussions of widespread AI use, especially in decision support? How can research methods from informatics, social sciences, humanities, and technology assessment be combined for studying AI? Why is interdisciplinary research crucial for understanding AI's impact on social contexts?

Special topic editors are Georg Plattner, Octavia Madeira, Christian Büscher, Alexandros Gazos, Tim Röller (all Institute for Technology Assessment  and  Systems  Analysis  (ITAS),  Karlsruhe  Institute  of  Technology (KIT).

AIA

<!-- image -->

- Ecological Perspectives for Science and Society 30% Rabatt mit dem Code GAIA30

mprehensible style.

formation on state-of-the-art environmental and sustainability sciences and

IA ...

current solutions to environmental problems. Well-known editors, reviewers, a community-based journal.

d authors work to ensure a unique inter- and transdisciplinary dialogue - in a

CC BY 4.0.

## GAIA -  ÖKOLOGISCHE PERSPEKTIVEN FÜR WISSENSCHAFT UND GESELLSCHAFT published by oekom, an independent publisher committed to high ronmental standards (see below and www.oekom.de). ows the Green Road to Open Access with these features. This policy is ompliant with Plan S (www.coalition-s.org). articles can be archived with no embargo period, authors retain copyright, articles are published under the Creative Commons Attribution licence EN →NO BORDERS!

oekom ist eine transdisziplinäre Zeitschrift für Wissenschaftler*innen und Wissenschaftsinteressierte, die sich mit Hintergründen, Analysen und Lösungen von Umwelt- und Nachhaltigkeitsproblemen befassen. al puts a true slice of sustainability into your hands. In buying it, you ing production methods based on strict environmental standards. g our journals, we ... recycled paper and mineral-oil free inks, p them in plastic packaging, in Germany, thus guaranteeing short transport distances. ation is available at www.natuerlich-oekom.de om, naturally SOCIETAL CHANGE: MAKING ALTERNATIVE PLACES  |  TRANSDISZIPLINARITÄT UND SOLUTIONISMUS  |  AGROECOLOGY AND PEACEBUILDING

<!-- image -->

ECOLOGICAL PERSPECTIVES FOR SCIENCE AND SOCIETY              31/1 (2022): 1  -  64

YEARS

!

THE TRANSDISCIPLINARY J

<!-- image -->

<!-- image -->

SOCIETAL CHANGE: MAKING ALTERNATIVE PLACES TRANSDISZIPLINARITÄT UND SOLUTIONISMUS AGROECOLOGY AND PEACEBUILDING

## Sichern Sie sich jetzt Ihr vergünstigtes Probeabo!

2 Ausgaben inkl. Versand für nur 23,10 Euro statt 33,- Euro www.gaia-online.net

18.03.22   14:35

<!-- image -->

ist die weltweit einzige interdisziplinäre Zeitschrift für Technikfolgenabschätzung. Wir fördern Debatten über technologische Innovation, Politik, Gesellschaft und Nachhaltigkeit. In TATuP finden Sie…

- Antworten auf drängende Fragen unserer Zeit;
- höchste wissenschaftliche Qualität durch faire Begutachtung und Transparenz;
- 100 % Open-Access: freier Zugang zu allen Artikeln und kostenfreie Publikation.

Herausgegeben vom Institut für Technikfolgenabschätzung und Systemanalyse (ITAS) am Karlsruher Institut für Technologie (KIT)

<!-- image -->

## TATuP und oekom passt natürlich!

<!-- image -->

Mit dieser Zeitschrift halten Sie ein echtes Stück Nachhaltigkeit in den Händen. Sie unterstützen eine Produktion mit hohen ökologischen Ansprüchen. Der oekom verlag …

- verwendet 100 % Recyclingpapier;
- verzichtet auf Plastikfolie;
- kompensiert alle klimaschädigenden Emissionen;
- druckt in Deutschland - und sorgt damit für kurze Transportwege.

Weitere Informationen finden Sie unter www.natürlich-oekom.de und #natürlichoekom.

<!-- image -->