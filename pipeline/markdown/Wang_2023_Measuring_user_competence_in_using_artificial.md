---
source_file: Wang_2023_Measuring_user_competence_in_using_artificial.pdf
conversion_date: 2026-02-03T19:01:15.842295
converter: docling
quality_score: 95
---

<!-- PAGE 1 -->
<!-- image -->

## Behaviour &amp; Information Technology

ISSN: 0144-929X (Print) 1362-3001 (Online) Journal homepage: www.tandfonline.com/journals/tbit20

## Measuring user competence in using artificial intelligence: validity and reliability of artificial intelligence literacy scale

Bingcheng Wang, Pei-Luen Patrick Rau &amp; Tianyi Yuan

To cite this article: Bingcheng Wang, Pei-Luen Patrick Rau &amp; Tianyi Yuan (2023) Measuring user competence in using artificial intelligence: validity and reliability of artificial intelligence literacy scale, Behaviour &amp; Information Technology, 42:9, 1324-1337, DOI: 10.1080/0144929X.2022.2072768

To link to this article:

https://doi.org/10.1080/0144929X.2022.2072768

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

Published online: 10 May 2022.

Submit your article to this journal

Article views: 34227

View related articles

View Crossmark data

Citing articles: 452 View citing articles

曲

CrossMark

<!-- image -->


<!-- PAGE 2 -->


## RESEARCH ARTICLE

<!-- image -->

<!-- image -->

## Measuring user competence in using arti /uniFB01 cial intelligence: validity and reliability of arti /uniFB01 cial intelligence literacy scale

Bingcheng Wang , Pei-Luen Patrick Rau and Tianyi Yuan

<!-- image -->

Department of Industrial Engineering, Tsinghua University, Beijing, People ' s Republic of China

## ABSTRACT

As arti /uniFB01 cial intelligence (AI) became a part of daily life, it has become important to determine user competence in using AI technology. Here, we propose the concept of AI literacy and develop a quantitative scale for obtaining accurate data regarding the AI literacy of ordinary users. We /uniFB01 rst identi /uniFB01 ed the primary core constructs of AI literacy, including awareness, use, evaluation, and ethics. Next, we generated 65 items to capture these four constructs; only 31 items were retained after a three-step content validation process. Then, we conducted a survey, and collected two samples of data. By reducing the number of items using the /uniFB01 rst sample and performing reliability and validity tests on the second sample, we obtained a 12-item instrument for the quantitative measurement of AI literacy. The results con /uniFB01 rmed that the proposed four-construct model is an adequate representation of AI literacy. Further, AI literacy is signi /uniFB01 cantly related to digital literacy, attitude towards robots, and users ' daily usage of AI. This study will not only aid researchers in understanding how user competence in using AI technology a /uniFB00 ects human -AI interactions but will also help designers develop AI applications that are aligned with the AI literacy levels of the target users.

## 1. Introduction

Our lives have changed signi /uniFB01 cantly with the emergence of arti /uniFB01 cial intelligence (AI) technology. With an increasing number of smart devices and AI-embedded applications becoming available, we are witnessing an era where ordinary users are transforming from AI immigrants into AI natives. The competence required to survive and engage in the so-called ' AI era ' (Davenport and Ronanki 2018) is increasingly becoming a necessary skill. Several researchers have suggested that there is an urgent need to improve people ' s ability to use AI in the coming era (Kandlhofer et al. 2016; Su 2018; Tarafdar, Beath, and Ross 2019). At the same time, others (Jarrahi 2018; Stembert and Harbers 2019) have highlighted the positive e /uniFB00 ects of high AI competence on human -AI interactions (HAII). Although Long and Magerko (2020) has summarised a set of core competencies in using AI technology, there neither exists a mature framework nor a practical tool for measuring it. To /uniFB01 ll this gap, we propose the concept of AI literacy to describe the competence of individuals in using AI technology. AI literacy refers to the ability to properly identify, use, and evaluate AI-related products under the premise of ethical standards. Like many other related literacies, such as computer literacy (Ho /uniFB00 man and Blake 2003; Tobin 1983) and digital literacy (Ala-

<!-- image -->

<!-- image -->

## ARTICLE HISTORY

Received 23 June 2021 Accepted 25 April 2022

## KEYWORDS

User competence; model validation; information literacy; twenty/uniFB01 rst century abilities; evaluation methodologies

Mutka 2011; Calvani et al. 2008), AI literacy does not require people to become experts in the underlying theory of and developments related to AI. Instead, a person would be viewed as being AI literate if they are capable of using AI products pro /uniFB01 ciently and reasonably.

Research on AI literacy is crucial for three main reasons. To begin with, AI literacy will shed light on the ongoing research in HAII, because one ' s literacy of a product can shape one ' s mental model of it (Brandt 2001; Grassian and Kaplowitz 2001; Rosling and Littlemore 2011); this is important as mental models play a vital role in interaction processes (Norman 2013). It may also explain the variations in people ' s behaviour when they interact with AI. Second, AI literacy can help quantify user competence in AI usage. Many previous studies on HAII (Lee and Choi 2017; Luo et al. 2019; Metelskaia et al. 2018) have examined the participants ' AI competence by measuring their prior experience and usage frequency, which cannot be considered complete or accurate measurements because they do not represent the participants ' full competence (Munro et al. 1997) with respect to AI usage. Although prior experience with AI may be closely related to AI literacy, the development of a scale for quantifying AI literacy will allow researchers and designers to obtain a better understanding of user competence in using AI.


<!-- PAGE 3 -->


Finally, AI literacy will help improve AI education by providing a comprehensive framework that can serve as an outline for curriculum design (Kandlhofer et al. 2016).

This study makes the following contributions. Firstly, we propose a theoretical framework of AI literacy consisting of four constructs. Secondly, we develop a reliable and valid self-report instrument for measuring individuals ' AI literacy using two sample sets. Thirdly, we explore the relationship between individuals ' AI literacy and their digital literacy, attitude towards robots and daily usage of AI products and applications. In the rest of the paper, we /uniFB01 rst review the relevant literature on AI literacy and provide a detailed de /uniFB01 nition and model for it based on an analogy with digital literacy. Then, we develop a tool to measure AI literacy, using the standard process proposed by Hinkin (1998); this process involves the following steps: item generation, content validation, questionnaire administration, item reduction, con /uniFB01 rmatory factor analysis, and reliability and validity testing. Next, we explore the relationships between users ' AI literacy and users ' attitude and daily usage of AI technology. Finally, we evaluate the suitability and limitations of the proposed instrument and discuss its potential for use in future research on AI.

## 2. Literature review

The term literacy originally referred to the basic skills and knowledge associated with books and printings (McMillan 1996). However, with the rapid development of computer and digital technologies, there have been many attempts to extend the conception of literacy beyond its original meaning of reading and writing (Buckingham and Burn 2007). Thus, discussions on literacy have been extended to television literacy (Buckingham 1989), information literacy (Eisenberg, Lowe, and Spitzer 2004), digital literacy (Gilster and Glister 1997), game literacy (Buckingham and Burn 2007), and media literacy (Livingstone and Van der Graaf 2008).

Arti /uniFB01 cial intelligence (AI) is a new technological science for simulating, extending, and expanding human intelligence. Now, the emergence of AI technology has made the concept of AI literacy necessary for describing people ' s competence in using this technology. Digital technology is a science and technology associated with electronic computers, which is a general concept. AI technology is a branch of computer science concentrating on the essence of intelligence. It attempts to produce a new intelligent machine that can respond in a similar way to human intelligence. On a technical level, AI technology based on digital technology. At the conceptual level, AI has a wide integration with the concepts of cognitive neuroscience and other disciplines. AI development involves many moral and ethical issues.

Nevertheless, for a user to be literate in a technology requires that they have more than a utilitarian perspective of it (Kandlhofer et al. 2016; Moore 2011). Thus, it is essential to understand the entire interaction process and establish the proper attitude and values towards AI systems. In this section, we discuss the existing de /uniFB01 nition and constructs of AI literacy based on existing research on the topic.

AI literacy is not a novel concept, and several researchers (Druga et al. 2019; Kandlhofer et al. 2016; Xiao and Bie 2019) have previously discussed methods for integrating AI education into existing educational frameworks. Kandlhofer et al. (2016) identi /uniFB01 ed seven topics in AI literacy: automata, intelligent agents, graphs and data structures, sorting, problem solving through search, classic planning, and machine learning. However, these frameworks are only applicable with respect to the cultivation of AI literacy in students within the education system. For the general population, methods for measuring user knowledge of AI and the ability to use it remain to be developed. To establish a concrete and comprehensive framework, we referred to the underlying concepts of an intimately related literacy, namely, digital literacy. The reason for choosing digital literacy was that there already exists a signi /uniFB01 cant body of literature on it, including tested theoretical frameworks, which can aid research on AI literacy. In addition, many AI applications and products rely on computer technology and information and communication technology (ICT) (Smith and Eckroth 2017), with a large number of the AI technologies available being integrated with digital applications. Being literate in digital products currently sometimes also implies the ability to use AI well.

However, we must mention that digital literacy is not a substitute for AI literacy in HAII research. Although AI belongs to the category of digital technology in a sense, AI is regarded as an interdisciplinary /uniFB01 eld involving computer science, information science, mathematics, psychology, sociology, linguistics, philosophy, and many other subjects (Russell and Norvig 2002). The intersection of these disciplines naturally indicates the di /uniFB00 erences between AI literacy and digital literacy. Besides, from users ' perspective, AI usually has more biological and social attributes than general digital technology (Minsky 2007; Poria et al. 2017; Tao and Tan 2005). For example, when users face a robot, they often interact with it through social logic instead of the logic to interact with the machine (Bruce,


<!-- PAGE 4 -->


<!-- image -->

Nourbakhsh, and Simmons 2002; Vossen, Ham, and Midden 2010). Such di /uniFB00 erences in interactions re /uniFB02 ect the di /uniFB00 erence in the mental models of users dealing with AI and digital technology. These di /uniFB00 erences would lead users to have di /uniFB00 erent criteria for evaluating AI products than for evaluating digital products. AI literacy is not the same as digital literacy. It is inappropriate to directly describe AI Literacy by using digital literacy related content. For example, a high school student who has not been exposed to the concept of AI but is skilled in using electronic devices has good digital literacy but may have poor AI literacy in some aspects. Therefore, the instruments for digital literacy are not appropriate for measuring users ' AI literacy, but the framework used to build digital literacy can inform the establishment of AI literacy. In the following section, we will review some important concepts in digital literacy for a clearer insight into AI literacy.

There are many overlaps between AI literacy and digital literacy. AI literacy and digital literacy share many similarities at the user level. The way people interact with computers is relatively consistent. Therefore, digital literacy can still provide us with many references. Digital literacy comprises the complex and integrated subdisciplines of skill, knowledge, ethics, and creative outputs in the digital environment (Calvani et al. 2008). Eshet (2004) has suggested that digital literacy refers not only to the ability to use digital devices and software but also to the cognitive, motor, sociological, and emotional skills required to work e /uniFB03 ciently in a digital environment. Eshet (2004) proposed /uniFB01 ve dimensions for digital literacy: photo-visual literacy, reproduction literacy, branching literacy, informational literacy, and socioemotional literacy. Gapski (2007) has argued that digital literacy includes the following three areas of ability: interpreting information, selecting information, and articulating information. In turn, these capabilities are suggestive of technological, cognitive, and ethical pro /uniFB01 ciencies, which, according to Calvani, Fini, and Ranieri (2009), are the three primary aspects of digital literacy. Ferrari (2012) believes that digital literacy lies at the intersection of Internet literacy, information communication technology literacy, media literacy, and information literacy. Wilson, Scalise, and Gochyyev (2015) proposed an ICT framework called KSAVE, which is an acronym for knowledge (K), skills (S), attitudes (A), values (V), and ethics (E). In KSAVE, knowledge refers to the speci /uniFB01 c requirements for understanding, such as declarative knowledge and facts; skills are the abilities and processes required to employ and deploy this knowledge. Further, attitudes, values, and ethics together constitute the behaviours and a /uniFB00 ective aspects that a person exhibits with respect to knowledge and skills. Given these de /uniFB01 nitions, KSAVE can be viewed as a general framework that can also be applied to AI literacy. In fact, considering the close relationship between digital technology and AI technology, the above-described models lay the foundation for AI literacy.

The meaning of digital literacy has been extended signi /uniFB01 cantly in the past decades, as various user devices have become ubiquitous (Ho /uniFB00 man and Blake 2003). The concepts of literacy must be further integrated with this new technology and must also include the broader use and social importance of computer technology. Nowadays, AI is also experiencing a similar boom, and its connotations and applications are expanding constantly. Therefore, it is di /uniFB03 cult to summarise the speci /uniFB01 c skills that an AI-literate person should have. In other words, AI literacy ' should not be thought of as an isolated set of skills but as an essential strategy for most learning objects ' (Moore 2011). In light of this idea, the technological -cognitive -ethical model and KSAVE model are more appropriate for AI literacy because they are general frameworks and do not involve speci /uniFB01 c digital skills. Based on these models, we de /uniFB01 ne AI literacy as the ability to be aware of and comprehend AI technology in practical applications; to be able to apply and exploit AI technology for accomplishing tasks pro /uniFB01 ciently; and to be able to analyze, select, and critically evaluate the data and information provided by AI, while fostering awareness of one ' s own personal responsibilities and respect for reciprocal rights and obligations. As per this de /uniFB01 nition, the following four constructs can be identi /uniFB01 ed for AI literacy: awareness, usage, evaluation, and ethics. In addition, many literacies have suggested close relationships between users ' digital literacy and their attitude towards digital technology (Comunello et al. 2015; Eshet 2004; Prior et al. 2016) and their behaviours when using digital technology (Neves, Amaro, and Fonseca 2013; Noh 2017; Park 2013). Thus, we can reasonably hypothesise that AI literacy will also be positively related to their attitude towards AI and daily usage of AI applications and products.

H1a: Users ' AI literacy will be positively correlated with their digital literacy.

H1b: Users ' AI literacy will be positively correlated with their attitude towards AI.

H1c: Users ' AI literacy will be positively correlated with their daily usage of AI applications and products.

Awareness refers to the ability to identify and comprehend AI technology during the use of AI-related applications. Many researchers (Calvani, Fini, and Ranieri


<!-- PAGE 5 -->


2009; Hallaq 2016; Katz 2007; Martin and Grudziecki 2006) have introduced awareness as a cognitive process that occurs before one uses a particular technology. Hallaq (2016) introduced awareness as one of the /uniFB01 ve core constructs of media literacy. Calvani, Fini, and Ranieri (2009) have indicated that the cognitive dimension is one of the three critical dimensions of digital competence. In the digital literacy framework called DigEuLit (Martin and Grudziecki 2006), awareness includes statement and identi /uniFB01 cation, which are the /uniFB01 rst two steps of a 13-step process for digital literacy. Another information literacy framework, called iSkills (Katz 2007), also contains a ' de /uniFB01 ne ' dimension, which refers to the ability to understand and articulate the scope of an information problem. The above-mentioned concepts attempt to measure the degree of awareness of the users and understand their respective literacy levels. Given these facts, we believe that awareness is also an important factor with respect to AI literacy. Some research (Kim 2013; Weisberg 2011) indicated strong relationship between awareness and attitude. Donat, Brandtweiner, and Kerschbaum (2009) also argued that a positive attitude would contribute to people ' s willingness to use digital products. Thus, we can hypothesise:

H2a: Users ' awareness in AI literacy will be positively correlated with their attitude towards AI.

H2b: Users ' awareness in AI literacy will be positively correlated with their daily usage of AI applications and products.

Usage refers to the ability to apply and exploit AI technology to accomplish tasks pro /uniFB01 ciently. This construct focuses on operational levels, including easy access to AI applications and tools, pro /uniFB01 ciency in the operation of AI applications and tools, and capable integration of di /uniFB00 erent types of AI applications and tools. Usage belongs to the technological dimension in the model of Calvani, Fini, and Ranieri (2009) and is common to other related literacies as well (Balfe, Sharples, and Wilson 2018; Katz 2007; Leahy and Dolan 2010). For example, the European Computer Driver License refers to 13 speci /uniFB01 c skills needed for computer literacy (Leahy and Dolan 2010), such as word processing, spreadsheets, and presentation. iSkills (Katz 2007) also refers to management and information integration for regular usage. In addition, KSAVE (Wilson, Scalise, and Gochyyev 2015) also contains skills as an essential aspect of ICT literacy. A few information and media literacy frameworks (Hair et al. 1998; Katz 2007) also include access, which refers to the skills and competencies needed to locate information and media content using the available technologies and associated software. The above-described concepts measure the user ' s literacy level from the perspective of use and operation. That is to say, usage in AI literacy will be closely related to users ' daily usage of AI products and applications. Also, some researchers (Donat, Brandtweiner, and Kerschbaum 2009; Porter and Donthu 2006) have found that users who use digital products pro /uniFB01 ciently would hold a positive attitude towards digital technology, which implies that usage in AI literacy will be positively related to users ' attitude.

H3a: Users ' usage in AI literacy will be positively correlated with their attitude towards AI.

H3b: Users ' usage in AI literacy will be positively correlated with their daily usage of AI applications and products.

Evaluation refers to the ability to analyze, select, and critically evaluate AI applications and their outcomes. Owing to the complexity and black-box nature of AI (Mueller et al. 2019), the results produced by it require careful consideration and evaluation. Thus, evaluation is an essential competency with respect to AI literacy. Evaluation also plays a critical role in other related literacies (Hallaq 2016; Katz 2007; Martin and Grudziecki 2006). For example, DigEuLit (Martin and Grudziecki 2006) includes analysis, evaluation, and interpretation. Further, both iSkills (Katz 2007) and digital online media literacy (Hallaq 2016) have evaluation in their models as well. In these models, evaluation often refers to the evaluation of information and results; however, in the case of AI literacy, evaluation also requires a user to form accurate opinions regarding AI applications and product. In addition, users who are able to evaluate an AI applications or product usually implies rich experience of using AI applications or products. Thus, we can hypothesise:

H4a: Users ' evaluation in AI literacy will be positively correlated with their attitude towards AI.

H4b: Users ' evaluation in AI literacy will be positively correlated with their daily usage of AI applications and products.

Ethics refers to the ability to be aware of the responsibilities and risks associated with the use of AI technology. In the /uniFB01 eld of AI, ethical issues have always been a concern of the public. While AI technology brings convenience, it also leads people to think carefully and explicitly about intelligence and ethics within it (Gunkel 2012). Therefore, an AI-literate person must be able to correctly understand and judge ethical issues to ensure that AI technology is used correctly and appropriately. In the case of other related literacies as well, many researchers view ethics as one of the critical components in their frameworks (Calvani et al. 2008; Hallaq 2016;


<!-- PAGE 6 -->


Wilson and Daugherty 2018). In the digital competence assessment framework (Calvani et al. 2008; Calvani, Fini, and Ranieri 2009), the ethical dimension is one of the three core dimensions of digital literacy. Hallaq (2016) also introduced ethical awareness in his framework of media literacy. Wilson, Scalise, and Gochyyev (2015) regarded attitudes, values, and ethics together as the key behaviours and a /uniFB00 ective aspects related to ICT literacy. Thus, it is reasonable to hypothesise that users ' ethics in AI literacy will be positively related to their attitude towards AI. For daily usage, abuse of the digital technology usually means low awareness of ethical issues, but some research (Bartsch and Dienlin 2016; Dinev and Hart 2004) indicates that people who use more digital products are more likely to be aware of the issues because they are probably the victims of the abuse. Therefore, we hypothesise:

H5a: Users ' ethics in AI literacy will be positively correlated with their attitude towards AI.

H5b: Users ' ethics in AI literacy will be positively correlated with their daily usage of AI applications and products.

After synthesising and analogising several related literacies, we propose a framework for AI literacy with four constructs. These constructs lay the foundation for the development of the AI Literacy Scale (AILS). In the sections that follow, we will evaluate whether the proposed four-construct framework is the most suitable one for AI literacy and test the reliability and validity of the proposed instrument. We will then test our hypotheses to explore the relationships between user ' s AI literacy, attitude, and daily usage.

## 3. Method

One of the purposes of this study was to develop a valid and reliable scale to measure people ' s AI literacy for future HAII research. After receiving approval from the ethics review board of the authors ' university, we developed a standard scale to measure AI literacy. AILS was developed following the recommendations of Hinkin (1998), and its development included the following steps: item generation, content validation, questionnaire administration, item reduction, con /uniFB01 rmatory factor analysis, and reliability and validity testing.

## 3.1. Item generation

To capture the four constructs of AI literacy, we generated self-report items through various methods, such as heuristic association, brainstorm, and card sorting. The self-report items in AILS were structured as statements that addressed the behaviour, ability, or level of comfort when performing speci /uniFB01 c AI-involving tasks and allowed the respondent to show their degree of agreement with them. This format has been shown to be the most e /uniFB00 ective one because it allows for the possibility that a respondent may be able to perform a certain task but chooses not to participate in it (Hallaq 2016). To be included, an item had to explicitly capture (at least) one of the four constructs: awareness, usage, evaluation, and ethics. The items were also designed to re /uniFB02 ect the literature and to be easy to understand for ordinary people. Besides, to prevent the questionnaire from becoming obsolete in a short time due to the rapid development of AI technology, the items were narrated from a more general perspective rather than asking speci /uniFB01 c applications. Then the items were compared for theoretical de /uniFB01 ciency and redundancy within each construct. The items that did not meet the above criteria were eliminated in this step. Finally, we obtained a total of 65 items with the number of items per dimension ranging from 10 to 24.

## 3.2. Content validation

To measure awareness, usage, evaluation, and ethics, 24, 16, 15, and 10 items were evaluated, respectively. Five subject matter experts (SMEs) were recruited for this part of the study. Three of the SMEs were PhD candidates in the /uniFB01 eld of human -computer interactions, who had prior experience in scale development and research concerning AI or robotics. The other two SMEs were PhD candidates in computer science and automation. It should be noted that using a small sample of students for content validity assessment was considered appropriate, since this was a cognitive task that did not require an understanding of the phenomena under examination (Anderson and Gerbing 1991; Hinkin 1998; Schriesheim et al. 1993). The de /uniFB01 nitions of the four AI literacy constructs were personally explained to all the SMEs to ensure they understood the meaning of each construct. Subsequently, content validation was performed in the following three steps.

Following the approach proposed by Schriesheim et al. (1993), the SMEs were /uniFB01 rst asked to classify each item into one of the four constructs. They were also provided with an ' unclassi /uniFB01 ed ' category for items that were deemed to not /uniFB01 t any of the constructs. An item was considered to clearly address a construct if at least four of the /uniFB01 ve SMEs classi /uniFB01 ed it in a similar manner. A total of 42 items matched this criterion, with 15 items being unclassi /uniFB01 ed or classi /uniFB01 ed incorrectly by one SME. Further, 23 items were unclassi /uniFB01 ed or classi /uniFB01 ed incorrectly by more than one SME; these items were excluded from the rest of the steps.


<!-- PAGE 7 -->


Next, we used the approach proposed by Hinkin (1985). The SMEs were asked to review the items selected in the /uniFB01 rst step and rate them on a three-point Likert scale on the extent to which each item corresponded to the construct: 1 = no /uniFB01 t, 2 = moderate /uniFB01 t, 3 = good /uniFB01 t. An item was accepted if at least three SMEs thought it was a ' good /uniFB01 t ' and no SME thought it was ' no /uniFB01 t ' . Using this criterion, 10 items were eliminated, while 31 items were selected for the remaining steps.

Finally, three of the SMEs were asked to participate in a focus group to supplement the items (i.e. questions) and improve their wording and format. The other two SMEs, who were unable to join the focus group, were interviewed separately by the authors. After the completion of the focus group discussion and interviews, two items were eliminated, and 14 items were rephrased. We also included an additional item that had been proposed and approved by all the SMEs. In this manner, a 31-item scale was obtained, wherein nine items are related to AI awareness, nine items to AI usage, six items to AI evaluation, and seven items to AI ethics. We ensured that each item was structured in a manner conducive to obtaining accurate responses.

## 3.3. Questionnaire administration

AILS is formatted in the form of a seven-point Likert scale. Though both Likert (1932) and Hinkin (1998) have recommended that new items be scaled using a /uniFB01 ve-point Likert scale, other researchers (Cox 1980; Pett, Lackey, and Sullivan 2003) have shown that scales with /uniFB01 ve to seven levels are also appropriate. Moreover, a recent study (Finstad 2010) suggested that seven-point scales are more suitable for electronically distributed usability surveys. Given that our questionnaire is to be distributed online, we considered a seven-point Likert scale to be more appropriate.

Internet. However, some researchers (Beach 1989; Pettit 2002) have pointed out that computer-based surveys are more likely to result in random responses and nonresponses as compared to the paper-and-pencil method. To counter this disadvantage of electronic surveys, we incorporated an attentiveness question to help the researchers distinguish those respondents who may have answered the survey questions in a random manner. This technique has been employed frequently by many researchers (Hallaq 2016; Hargittai 2005) to ensure that they only consider those respondents who have paid close attention to the survey.

We collected two sets of samples, Sample 1, and Sample 2, through two di /uniFB00 erent methods. Sample 1 was to be used for item reduction, while Sample 2 was to be used for model validation and con /uniFB01 rmation. The participants for Sample 1 were recruited through social media and a professional survey company. Of the 684 participants who completed the online survey, 83 were removed for not answering the attentiveness question correctly. The /uniFB01 nal sample size was 601, and the sample was 55.91% female and 44.09% male. The age of the participants ranged from 16 to 57, with the mean age being 26.55 ( SD = 6.29). Sample 2 was collected by the professional survey company. Of the 363 participants who completed the online survey, 38 were removed for not answering the attentiveness question correctly. The /uniFB01 nal sample size was 325, and the sample was 55.69% female and 44.31% male. Further, the age of the participants ranged from 17 to 65, with the mean age being 29.67 ( SD = 7.33). As for education, most of the participants in Sample 1 (72.21%) and Sample 2 (84.92%) had a bachelor ' s degree. The details of the participants are summarised in Table 1.

AILS was developed to be presented in an electronic format on computers or smartphones so that it can be transmitted and distributed readily through the

In Sample 2, we incorporated three additional measures into the questionnaire for examining criterion validity and test the hypotheses. The /uniFB01 rst measure was a popular digital literacy scale developed by Ng (2012) to measure people ' s literacy in information and communication technology. The scale included six questions for

Table 1. Demographic statistics of the participants in Sample 1 and Sample 2.

|           |                             | Sample 1   | Sample 1       | Sample 2   | Sample 2       |
|-----------|-----------------------------|------------|----------------|------------|----------------|
| Parameter | Value/speci /uniFB01 cation | Frequency  | Percentage (%) | Frequency  | Percentage (%) |
| Gender    | Female                      | 336        | 55.91          | 181        | 55.69          |
|           | Male                        | 265        | 44.09          | 144        | 44.31          |
| Age       | <20                         | 56         | 9.32           | 16         | 4.92           |
|           | 20 ∼ 30                     | 372        | 61.90          | 148        | 45.54          |
|           | 30 ∼ 40                     | 157        | 26.12          | 133        | 40.92          |
|           | ≥ 40                        | 16         | 2.66           | 28         | 8.62           |
| Education | Middle school               | 6          | 1.00           | 1          | .31            |
|           | High school                 | 54         | 8.99           | 10         | 3.08           |
|           | Bachelor                    | 434        | 72.21          | 276        | 84.92          |
|           | Master                      | 97         | 16.14          | 36         | 11.08          |
|           | Ph. D                       | 10         | 1.66           | 2          | .62            |


<!-- PAGE 8 -->


<!-- image -->

the technical dimension, two for the cognitive dimension, and two for the social-emotional dimension. For attitude, we incorporated Negative Attitude towards Robots Scale (NARS), which measured people ' s attitudes towards communication robots or agents in daily life (Nomura, Kanda, and Suzuki 2006) from the negative dimension. The reason for choosing NARS was that the scale was one of the most popular instruments in recent HAII or human robot interaction research /uniFB01 eld. NARS also consisted of three constructs: attitudes towards situations of interaction with robots (S1), attitudes towards social in /uniFB02 uence of robots (S2), and attitudes towards emotions in interaction with robots (S3). In the third measure, we included /uniFB01 ve questions based on the system usage model of Burton-Jones (2005) to measure the people ' s daily usage of AI technology. The /uniFB01 ve questions were derived from /uniFB01 ve aspects: extent of use, variety of use, frequency of use, proportion of use, and duration of use. These three measures would not only serve as criteria for criterion validity analysis but also helped us understand the relationship between AI literacy and other related factors.

## 4. Results and analysis

## 4.1. Item reduction

The item reduction process aimed to ensure that a fair number of items representing the four AILS constructs was retained, thus providing evidence of construct validity (Hinkin 1998). Item reduction can also reduce participants ' cognitive load and thus increase the usability of the scale. We limited the number of items to around 10 in our /uniFB01 nal scale, making it palatable for AI

researchers to include, especially when they want to use it for pre-screening or post-task evaluation. For this purpose, we performed an exploratory factor analysis (EFA). EFA is a technology used to /uniFB01 nd out the essential structure of multivariate observation variables and deal with dimensionality reduction. Therefore, EFA can synthesise the variables with complex relationships into a few core factors. We calculated the item-total correlations to select the suitable items for the four constructs. All the analyses related to item reduction were performed on Sample 1.

We /uniFB01 rst checked the suitability of the data for factor analysis through Kaiser-Meyer-Olkin test, which con /uniFB01 rmed that the sample was suitable for EFA with coe /uniFB03 cient value of 0.95. The scree plot and parallel analysis results as well as the number of eigenvalues greater than 1.00 suggested a four-factor solution, shown in Figure 1.

Thus, we ran a four-factor EFA with the diagonal rotation and minimum residual estimation. In this step, we wanted to ensure that the items matched the theoretical structure and to identify any items that were cross loaded in more than one construct. The results of the EFA are shown in Table 2.

Next, we used con /uniFB01 rmation factor analysis (CFA). Di /uniFB00 erent from EFA, CFA is a process of using sample data to verify the factor structure hypothesis made according to some theories and a priori knowledge. In con /uniFB01 rmatory factor analysis, the factor structure established according to the existing theory can form an estimated covariance matrix. We performed CFA for each subscale to check the factor loadings for their respective constructs. We also determined the item-total correlations to check if any items on the scale were inconsistent with the average behaviour of the others. The item

Figure 1. Scree plot.

<!-- image -->


<!-- PAGE 9 -->


Table 2. Results of exploratory factor analysis.

| Item   | Awa.   | Usa.   |   Eva. |   Eth. | Item   | Awa.   | Usa.   | Eva.   | Eth.   |
|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| AW_1   | . 65   | .24    |   0.17 |   0.27 | US_8   | .36    | .40    | .16    | .19    |
| AW_2   | .54    | .11    |   0.15 |   0.18 | US_9   | .26    | .65    | .13    | .16    |
| AW_3   | .45    | .20    |   0.23 |   0.15 | EV_1   | .21    | .13    | .61    | .11    |
| AW_4   | .57    | .27    |   0.18 |   0.17 | EV_2   | .07    | .14    | . 73   | .21    |
| AW_5   | .17    | -.06   |   0.16 |   0.17 | EV_3   | .21    | .11    | . 64   | .21    |
| AW_6   | .45    | .19    |   0.2  |   0.34 | EV_4   | .15    | .16    | .61    | .21    |
| AW_7   | .44    | .26    |   0.23 |   0.29 | EV_5   | .21    | .14    | .29    | .28    |
| AW_8 . | 58     | .23    |   0.15 |   0.23 | EV_6   | .23    | .15    | . 72   | .14    |
| AW_9 . | 65     | .20    |   0.19 |   0.24 | ET_1   | .27    | .27    | .15    | . 72   |
| US_1   | .14    | . 74   |   0.1  |   0.19 | ET_2   | .30    | .24    | .25    | . 64   |
| US_2   | .12    | .62    |   0.19 |   0.29 | ET_3   | .29    | .20    | .19    | .46    |
| US_3   | .21    | . 71   |   0.09 |   0.2  | ET_4   | .03    | .06    | .06    | .19    |
| US_4   | .38    | .38    |   0.02 |   0    | ET_5   | .30    | .28    | .17    | . 62   |
| US_5   | .13    | . 70   |   0.08 |   0.22 | ET_6   | .08    | .13    | .12    | .44    |
| US_6   | .15    | .68    |   0.23 |   0.22 | ET_7   | .30    | .16    | .19    | .48    |
| US_7   | .31    | 0.48   |   0.12 |   0.08 |        |        |        |        |        |

Note. Items in boldface are included in the /uniFB01 nal scale. Factor loadings less than .30 are suppressed. Awa. = Awareness, Usa. = Usage, Eva. = Evaluation, Eth. = Ethics. χ 2 (601) = 483.36, p &lt; .0001, Tucker-Lewis index of factoring reliability is .98, root mean square error of approximation (RMSEA) is .03, root mean square of the residuals (RSMR) is .02.

loadings and item-total correlation results for each of the four constructs are listed in Table 3. We selected the following items: (a) those with factor loadings higher than 0.30 (Costello and Osborne 2005); (b) those with item-total correlation coe /uniFB03 cients higher than 0.60 (Hair et al. 1998); (c) those with the highest three item-total correlation coe /uniFB03 cients; and (d) those that did not exhibit redundancy with respect to the other high-loading and high-item-total-correlation items. Eventually, twelve items emerged from the 31 items with three items for each construct.

## 4.2. Model validation and comparison

After /uniFB01 nalising the items in the scale, we con /uniFB01 rmed whether the theoretical model used was the best one for measuring AI literacy. For this, we conducted a series of CFAs on Sample 2. The results are shown in

Table 3. Item pool factor loadings and corrected item-total correlations.

| Item   |   Loading |   Item-total correlation | Item   | Loading   | Item-total correlation   |
|--------|-----------|--------------------------|--------|-----------|--------------------------|
| AW_1   |      0.77 |                     0.77 | US_8   | 0.54      | 0.62                     |
| AW_2   |      0.58 |                     0.66 | US_9   | 0.72      | 0.75                     |
| AW_3   |      0.55 |                     0.65 | EV_1   | 0.66      | 0.73                     |
| AW_4   |      0.66 |                     0.69 | EV_2   | 0.76      | 0.79                     |
| AW_5   |      0.22 |                     0.37 | EV_3   | 0.71      | 0.77                     |
| AW_6   |      0.61 |                     0.66 | EV_4   | 0.67      | 0.73                     |
| AW_7   |      0.62 |                     0.68 | EV_5   | 0.41      | 0.56                     |
| AW_8   |      0.7  |                     0.72 | EV_6   | 0.79      | 0.81                     |
| AW_9   |      0.76 |                     0.76 | ET_1   | 0.82      | 0.8                      |
| US_1   |      0.77 |                     0.78 | ET_2   | 0.79      | 0.79                     |
| US_2   |      0.71 |                     0.72 | ET_3   | 0.59      | 0.66                     |
| US_3   |      0.77 |                     0.79 | ET_4   | 0.21      | 0.45                     |
| US_4   |      0.46 |                     0.59 | ET_5   | 0.77      | 0.78                     |
| US_5   |      0.75 |                     0.76 | ET_6   | 0.47      | 0.56                     |
| US_6   |      0.75 |                     0.76 | ET_7   | 0.53      | 0.6                      |
| US_7   |      0.56 |                     0.65 |        |           |                          |

Note. Items in boldface are included in the /uniFB01 nal scale.

Table 4. We found that the theoretical model used for AI literacy was an acceptable one (CFI = 0.99, TLI = 0.99, GFI = 0.98, RMSEA=0.01, SRMR=0.03). In addition, we also performed CFAs to test four competing, theoretically meaningful models: a single-factor model, where all 12 items are loaded onto one factor; a two-factor model, where the items measuring awareness and ethics (cognitive level) are loaded onto one factor while those for usage and evaluation (technological level) are loaded onto the other factor; a three-factor model, where the items measuring awareness (cognitive level) are loaded onto one factor, those measuring usage and evaluation (technological level) are loaded onto another factor, and those measuring ethics (ethical level) are loaded onto the third factor; and a four-factor model, where the awareness, usage, evaluation, and ethics items are loaded onto four individual factors. The /uniFB01 t statistics for the four models are listed in Table 4. The results suggest that the four-factor model exhibited better /uniFB01 t statistics than the other models, which indicates that the theoretical model based on four distinct constructs was the most appropriate for conceptualizing AI literacy. The descriptions and loadings of the twelve items for the /uniFB01 nalised scale are shown in Table 5.

Table 4. Model /uniFB01 t statistics.

| Factor    |    c 2 |   df | p     |   CFI |   TLI |   GFI |   RMSEA |   SRMR |
|-----------|--------|------|-------|-------|-------|-------|---------|--------|
| 1 factor  | 397.09 |   54 | < .01 |  0.69 |  0.62 |  0.81 |    0.14 |   0.1  |
| 2 factors | 344.76 |   53 | < .01 |  0.74 |  0.68 |  0.82 |    0.13 |   0.09 |
| 3 factors | 218.11 |   51 | < .01 |  0.85 |  0.81 |  0.87 |    0.1  |   0.07 |
| 4 factors |  49.62 |   48 | .41   |  0.99 |  0.99 |  0.98 |    0.01 |   0.03 |

Note. The four-factor model is the theoretical model. CFI = comparative /uniFB01 t index; TLI = Tucker-Lewis index; GFI = goodness of /uniFB01 t index; RMSEA = root mean square error of approximation; SRMR = standardised root mean square residual. TLI, CFI and GFI values greater than .900 and RMSEA values less than .050 suggest adequate model /uniFB01 t.


<!-- PAGE 10 -->


Table 5. Descriptions and loadings on a standardised scale.

R indicates that item is in reverse form.

| Item   | Description                                                                                                 | Construct   |   Loading |
|--------|-------------------------------------------------------------------------------------------------------------|-------------|-----------|
| AW_1   | I can distinguish between smart devices and non-smart devices.                                              | Awareness   |      0.72 |
| AW_8   | I do not know how AI technology can help me. R                                                              | Awareness   |      0.64 |
| AW_9   | I can identify the AI technology employed in the applications and products I use.                           | Awareness   |      0.7  |
| US_1   | I can skilfully use AI applications or products to help me with my daily work.                              | Usage       |      0.72 |
| US_3   | It is usually hard for me to learn to use a new AI application or product. R                                | Usage       |      0.66 |
| US_5   | I can use AI applications or products to improve my work e /uniFB03 ciency.                                 | Usage       |      0.72 |
| EV_2   | I can evaluate the capabilities and limitations of an AI application or product after using it for a while. | Evaluation  |      0.71 |
| EV_3   | I can choose a proper solution from various solutions provided by a smart agent.                            | Evaluation  |      0.72 |
| EV_6   | I can choose the most appropriate AI application or product from a variety for a particular task.           | Evaluation  |      0.78 |
| ET_1   | I always comply with ethical principles when using AI applications or products.                             | Ethics      |      0.76 |
| ET_2   | I am never alert to privacy and information security issues when using AI applications or products. R       | Ethics      |      0.6  |
| ET_5   | I am always alert to the abuse of AI technology.                                                            | Ethics      |      0.73 |

## 4.3. Reliability and construct validity

The reliability and validity of AILS were determined using Cronbach ' s alpha as well as the composite reliability (CR), average variance extracted (AVE), and heterotrait-monotrait ratio of the correlations (HTMT). The results are presented in Table 6. Cronbach ' s alpha is an index used to measure reliability, and a value greater than 0.7 is considered acceptable (DeVellis 2016). The value of Cronbach ' s alpha for AILS was 0.83, whereas the alpha values for the four constructs were 0.73, 0.75, 0.78, and 0.73, respectively. Though all four constructs exhibited reliabilities of more than 0.70, the instrument itself scored higher than 0.80, which indicated that the instrument in its entirety is more reliable than the separate constructs.

According to the criterion established by Fornell and Larcker (1981), CR and AVE were used to assess the scale ' s convergent validity. CR is a less biased estimate of reliability than Cronbach ' s alpha, and CR values of

Table 6. Results of reliability and validity analyses of AILS.

|            |      |     |      |     |     |     | Correlation & HTMT   | Correlation & HTMT   | Correlation & HTMT   | Correlation & HTMT   |
|------------|------|-----|------|-----|-----|-----|----------------------|----------------------|----------------------|----------------------|
| Construct  | Mean | SD  | PV   | α   | CR  | AVE | Awareness            | Usage                | Evaluation           | Ethics               |
| Awareness  | 5.93 | .81 | 0.08 | .73 | .73 | .48 | 1                    | .66                  | .53                  | .49                  |
| Usage      | 5.76 | .77 | 0.11 | .75 | .75 | .49 | .49                  | 1                    | .44                  | .41                  |
| Evaluation | 5.62 | .85 | 0.10 | .78 | .78 | .55 | .40                  | .34                  | 1                    | .41                  |
| Ethics     | 5.76 | .80 | 0.09 | .73 | .73 | .49 | .36                  | .30                  | .31                  | 1                    |
| AILS       | 5.77 | .58 |      | .83 | .88 | .50 | .78                  | .72                  | .72                  | .68                  |

Note. PV = proportion variance, α = Cronbach ' s alpha, CR = composite reliability, AVE = average variance extracted, HTMT = heterotrait-monotrait ratio of the correlations. The HTMT results are displayed above the diagonal, and the correlation results are below the diagonal.

0.70 and higher are considered acceptable (Hair et al. 1998). AVE measures the level of variance captured by a construct versus that attributable to measurement error. Values greater than 0.5 indicate adequate convergence (Hair et al. 1998); thus, the average factor loadings should be larger than 0.72 to ensure AVE values greater than 0.5. However, in some cases, and especially for a new instrument, factor loadings higher than 0.5 (i.e. AVE &gt; 0.25) could be considered acceptable (e.g. Hsu and Wu 2013; Hu 2013; Mayerl 2016). In our scale, CR values were higher than 0.7, and AVE values were close to 0.5, which indicated acceptable convergence. Discriminant validity was measured based on the HTMT approach proposed by Henseler, Ringle, and Sarstedt (2015). It suggested that the HTMT should not be higher than 0.85 for high discriminant validity (Clark and Watson 1995; Kline 2015). In the case of the proposed scale, the results showed that the HTMT values of all four constructs were lower than the threshold of 0.85, indicating acceptable discriminant validity for the instrument. In addition, almost all correlations among the four constructs are between .15 and .50, which were also considered ideal according to Clark and Watson (2016).

## 4.4. Hypothesis test results

The correlations for hypothesis testing are presented in Table 7. The result partly proved the Hypothesis H1a. As we mentioned, AI literacy and digital literacy might share some overlapping parts. Hypothesis H1b suggested that the participants ' AI literacy would be positively correlated with their attitude towards AI. Results suggested that AI literacy was signi /uniFB01 cantly and negatively correlated with NARS, S1, S2, and S3, thus supporting H1b. Hypothesis H1c was partly supported as AI literacy was only signi /uniFB01 cantly and positively correlated with the extent, variety, and frequency of use of AI technology in daily life. Results supported Hypothesis H2a, /uniFB01 nding that the awareness was signi /uniFB01 cantly and negatively correlated with NARS and NARS S1. Hypothesis H2b was also partly supported, as the awareness in AILS was signi /uniFB01 cantly and positively correlated


<!-- PAGE 11 -->


Table 7. Correlation between AILS and criterion factors.

|                   | AILS      | Awareness   | Usage     | Evaluation   | Ethics    |
|-------------------|-----------|-------------|-----------|--------------|-----------|
| Digital Literacy  | 0 . 76*   | 0 . 55*     | 0 . 59*   | 0 . 57*      | 0 . 49*   |
| NARS              | - 0 . 35* | - 0 . 29*   | - 0 . 24* | - 0 . 29*    | - 0 . 19* |
| NARS S1           | - 0 . 40* | - 0 . 36*   | - 0 . 29* | - 0 . 27*    | - 0 . 23* |
| NARS S2           | - 0 . 22* | - 0.17      | - 0.17    | - 0 . 22*    | - 0.09    |
| NARS S3           | - 0 . 23* | - 0.16      | - 0.11    | - 0 . 21*    | - 0.17    |
| Extent of Use     | 0 . 35*   | 0 . 33*     | 0 . 38*   | 0 . 23*      | 0.07      |
| Variety of Use    | 0 . 41*   | 0 . 32*     | 0 . 44*   | 0 . 30*      | 0.15      |
| Frequency of Use  | 0 . 31*   | 0 . 24*     | 0 . 38*   | 0 . 24*      | 0.05      |
| Proportion of Use | 0.18      | 0.11        | 0 . 28*   | 0.17         | - 0.03    |
| Duration of Use   | 0.15      | 0.08        | 0 . 23*   | 0.13         | - 0.01    |

Note. NARS = Negative Attitude towards Robots Scale. S1 = attitude towards situations of interaction with robots. S2 = attitude towards social in /uniFB02 uence of robots. S3 = attitude towards emotions in interaction with robots. * represents p -value &lt; .05.

with the extent, variety, and frequency of use of AI technology in daily life. Hypotheses H3a and H3b suggested that the usage in AI literacy would be positively correlated with attitude and daily use of AI technology, respectively, and the results supported both hypotheses. The results indicated that the usage was signi /uniFB01 cantly and negatively correlated with NARS and NARS S1. Results also indicated that the usage in AI literacy was signi /uniFB01 cantly and positively with the extent, variety, frequency, proportion, and duration of use of AI technology in daily life. Hypothesis H4a was fully supported as the evaluation was signi /uniFB01 cantly and negatively correlated with NARS, S1, S2, and S3. Results partly supported Hypothesis H4b, /uniFB01 nding that the evaluation was signi /uniFB01 cant and positively correlated with the extent, variety, and frequency of use of AI technology in daily life. For hypotheses H5a and H5b, the results only supported H5a, /uniFB01 nding that the ethics was signi /uniFB01 cantly and negatively correlated with NARS and NARS S1. The ethics was found to have nothing to do with daily usage. In addition, the results indicated a close relationship between AI literacy and digital literacy as digital literacy was signi /uniFB01 cantly and positively with AI literacy, awareness, usage, evaluation, and ethics.

Table 8. Summary of multiple regression results.

|            | Digital literacy   | Digital literacy   | Digital literacy   | NARS           | NARS           | NARS           | NARS S1          | NARS S1          | NARS S1          | NARS S2           | NARS S2           | NARS S2           | NARS S3         | NARS S3         | NARS S3         |
|------------|--------------------|--------------------|--------------------|----------------|----------------|----------------|------------------|------------------|------------------|-------------------|-------------------|-------------------|-----------------|-----------------|-----------------|
| Scale      | β                  | R 2                | p                  | β              | R 2            | p              | β                | R 2              | p                | β                 | R 2               | p                 | β               | R 2             | p               |
| Awareness  | .16                |                    | < .001 *           | -.17           |                | .02 *          | -.27             |                  | < .001 *         | -.09              |                   | .35               | -.09            |                 | .34             |
| Usage      | .30                |                    | < .001 *           | -.11           |                | .15            | -.13             |                  | .07 +            | -.13              |                   | .19               | .00             |                 | .99             |
| Evaluation | .27                |                    | < .001 *           | -.19           |                | .01 *          | -.13             |                  | .04 *            | -.24              |                   | < .001 *          | -.22            |                 | .01 *           |
| Ethics     | .21                |                    | < .001 *           | -.06           |                | .32            | -.09             |                  | .16              | .02               |                   | .83               | -.14            |                 | .10             |
| AILS       |                    | .58                | < .001 *           |                | .13            | < .001 *       |                  | .17              | < .001 *         |                   | .06               | < .001 *          |                 | .06 <           | .001 *          |
| Scale      | Extent of Use      | Extent of Use      | Extent of Use      | Variety of Use | Variety of Use | Variety of Use | Frequency of Use | Frequency of Use | Frequency of Use | Proportion of Use | Proportion of Use | Proportion of Use | Duration of Use | Duration of Use | Duration of Use |
|            | β                  | R 2                | p                  | β              | R 2            | p              | β                | R 2              | p                | β                 | R 2               | p                 | β               | R 2             | p               |
| Awareness  | .35                |                    | < .001 *           | .18            |                | .10            | .12              |                  | .31              | -.06              |                   | .68               | .10             |                 | .52             |
| Usage      | .57                |                    | < .001 *           | .68            |                | < .001         | .68              |                  | < .001 *         | .65               |                   | < .001 *          | .62             |                 | < .001 *        |
| Evaluation | .16                |                    | .12                | .28            |                | .01 *          | .24              |                  | .02 *            | .25               |                   | .04 *             | .21             |                 | .12             |
| Ethics     | -.21               |                    | .05 *              | -.08           |                | .45            | -.23             |                  | .03 *            | -.32              |                   | .01 *             | -.23            |                 | .10             |
| AILS       |                    | .19                | < .001 *           |                | .23            | < .001 *       |                  | .17              | < .001 *         |                   | .10               | < .001 *          |                 | .07             | < .001 *        |

Note. NARS = Negative Attitudes towards Robots Scale. S1 = attitudes towards situations of interaction with robots. S2 = attitudes towards social in /uniFB02 uence of robots. S3 = attitudes towards emotions in interaction with robots. * represents p -value &lt; 0.05, and + represents p -value &lt; 0.10.

To demonstrate the predictive power of the four constructs of AILS, we conducted a multiple regression of participants ' AI literacy on their digital literacy, negative attitude towards robots, and daily usage of AI technology. The results of the regression are presented in Table 8. The Cronbach ' s alpha of digital literacy scale was 0.83, but the alpha of the two subscales (cognitive dimension and social-emotional dimension) were far below 0.70 (0.39 and 0.39, respectively). Thus, we only used the digital literacy scale in its entirety for predictive criterion. Results suggested that the four constructs of AILS combined explained 58% of the variance in reported digital literacy. All four constructs turned out to be signi /uniFB01 cant predictors of digital literacy. For NARS and its subscales, the values of Cronbach ' s alpha were 0.88, 0.80, 0.80, and 0.80. The results suggested that the four constructs of AILS combined explained 13%, 17%, 6%, and 6%, respectively, of the variance in NARS and its subscales. Awareness and evaluation were signi /uniFB01 cant negative predictors for both NARS and NARS S1 (attitudes towards situations of interaction with robots). But for NARS S2 (attitudes towards social in /uniFB02 uence of robots) and S3 (attitudes towards emotions in interaction with robots), evaluation was the only factor predicting the results.

In terms of users ' daily usage of AI technology, we explored its relationship with AILS from /uniFB01 ve aspects. The four constructs of AILS combined explained 19% of the variance in the extent of use, while awareness, usage, and ethics were signi /uniFB01 cant predictors. The four constructs of AILS combined explained 23% of the variance for the variety of use, while usage and evaluation emerged as signi /uniFB01 cant predictors. Both frequency of use and proportion of use were determined to have signi /uniFB01 cant relationship with usage, evaluation, and ethics with explained variances of 17% and 10%. For the duration of usage, usage was the only signi /uniFB01 cant predictor, and the four constructs of AILS explained 7% of its


<!-- PAGE 12 -->


<!-- image -->

variance. Across the /uniFB01 ve dimensions of daily use, the construct usage emerged as the most signi /uniFB01 cant predictor while other constructs, which share enough explained variance, were below signi /uniFB01 cance as unique predictors.

## 5. Discussion and conclusions

This study aimed to improve our understanding of AI literacy by developing and validating a novel measure of AI literacy. The proposed scale, namely, AILS, is based on the conceptually similar idea of digital literacy suggested by Balfe, Sharples, and Wilson (2018) and Calvani et al. (2008). The theoretical basis for their approach to digital literacy led us to believe that a similar framework may be applicable in the case of AI literacy as well. Using a six-step approach, we were able to demonstrate the reliability and validity of AILS. The results of factor analyses indicated that the theoretical model based on four distinct constructs is the most appropriate conceptualization model for AI literacy. Although the AVE values of some constructs were slightly lower than the threshold value of 0.50, the whole scale proved to have su /uniFB03 cient convergence validity. The other indicators, such as CR and HTMT, also suggested su /uniFB03 cient construct validity.

In the hypotheses testing, most of the hypotheses have been veri /uniFB01 ed. AI literacy and the four constructs were found to be negatively correlated with negative attitude towards robots (NARS) and negative attitude towards situations of interaction with robots (NARS S1). The extent, variety, and frequency of use of AI technology were found to be correlated with AI literacy and the four constructs except ethics. The results of the correlation and the regression showed that digital literacy and AILS were closely related, which could be intuitively explained: digital literacy is a prerequisite for AI literacy since most of today ' s AI technology is embedded in digital products. It might come from the section based on the digital literacy. The regression results also indicated that awareness and evaluation in AILS played an important role in predicting users ' negative attitudes towards robots. This could be attributed to the fact that awareness and evaluation represented users ' cognition and judgement of AI, which would a /uniFB00 ect their attitudes towards robots in the end. In terms of users ' daily AI usage, the results that the construct of usage in AILS showed a signi /uniFB01 cant relationship with user ' s daily behaviour in using AI technology were not surprising. The /uniFB01 ndings indicated that AILS is an important predictor of user ' s attitude and behaviour towards AI technology usage. In general, these results indicated that AILS is suitable to measure AI literacy.

Several points need to be noted when applying AILS to use in practice. The /uniFB01 rst one is that the constructs alone are less reliable than the instrument as a composite. Though all four constructs exhibited reliabilities of more than 0.70, the instrument itself scored higher than 0.80. The recommendation then is to use the instrument in its entirety rather than the individual constructs. It should also be noted the correlation between AILS and digital literacy was high, which does not mean that AI literacy and digital literacy can be used interchangeably. As discussed in the literature review, the de /uniFB01 nitions of AI and ICT are di /uniFB00 erent, and users have di /uniFB00 erent perceptions of the two technologies, which would lead to di /uniFB00 erent attitudes and use of the two kinds of products. Thus, it is recommended for researchers and practitioners to use the AILS for user research involving AI technology. It is worth noting that users are likely to mistake AI literacy for competence of using speci /uniFB01 c applications, given that AI is usually embedded in the applications. As user competence regarding di /uniFB00 erent applications varies signi /uniFB01 -cantly, it is likely to cause an inconsistency in the results. Thus, we recommend using AILS to measure users ' general AI competence rather than the ability to use speci /uniFB01 c AI applications. Finally, AI literacy might relate to the digital literacy, we recommend using the outcome to explore the association between AI literacy and digital literacy.

This study has certain limitations. First, this study does not address all concerns regarding AI literacy, and much work remains to be done on this topic. For instance, future studies should examine other theoretically meaningful structures. It is also worth noting that both samples scored highly on AILS, meaning that either the respondents possessed high levels of literacy or that the attitude-based nature of the scale, wherein the respondents report their levels of agreement with the statements regarding AI, fails to accurately assess their understanding of AI. It is also possible that the respondents who completed the survey online have more experience in using AI applications. Hence, the relationship between digital literacy and AI literacy is worth investigating in the future. In addition, we must recognise the limitations of the SMEs. Although they had training in the relevant analysis methods and work in related /uniFB01 elds, they are PhD students. Future studies should examine whether AI researchers have di /uniFB00 erent conceptualizations of the content and items included in the measure.

That said, this study lays the foundation for future research on the e /uniFB00 ects of AI literacy on various outcomes. We have provided a comprehensive de /uniFB01 nition of AI literacy and created a well-de /uniFB01 ned four-factor


<!-- PAGE 13 -->


model. Further, we have developed a short 12-item measure to provide researchers and practitioners with a tool for assessing the self-report competence of users in using AI. By de /uniFB01 ning the AI literacy domain and providing e /uniFB00 ective measurement tools, we hope to improve and encourage future research in this area. For example, one domain that may bene /uniFB01 t signi /uniFB01 cantly from this work is explainable AI. The explanations provided by intelligent systems or agents also improve users ' literacy, leading to a better collaboration with the systems or the agents (Zhou, Itoh, and Kitazaki 2021). Moreover, the conceptualization of AI literacy and the development of suitable tools for measuring it will help designers to portray accurate user model and subsequently design appropriate explainable AI systems based on these models. Finally, because the proposed model and the scale based on it de /uniFB01 ne and capture the constructs of AI literacy, researchers will be able to discern more nuanced relationships between AI literacy and user experience in HAII research.

## Disclosure statement

No potential con /uniFB02 ict of interest was reported by the author(s).

## Funding

This work was supported by National Key R&amp;D Program of China (2018AAA0101702).

## ORCID

Bingcheng Wang http://orcid.org/0000-0003-0996-6824 Pei-Luen Patrick http://orcid.org/0000-0002-5713-8612 Tianyi Yuan http://orcid.org/0000-0002-7134-130X

## References

- Ala-Mutka, K. 2011. ' Mapping Digital Competence: Towards a Conceptual Understanding. ' Sevilla: Institute for Prospective Technological Studies .
- Anderson, J. C., and D. W. Gerbing. 1991. ' Predicting the Performance of Measures in a Con /uniFB01 rmatory Factor Analysis with a Pretest Assessment of Their Substantive Validities. ' Journal of Applied Psychology 76 (5): 732.
- Balfe, N., S. Sharples, and J. R. Wilson. 2018. ' Understanding Is Key: An Analysis of Factors Pertaining to Trust in a RealWorld Automation System. ' Human Factors 60 (4): 477 -495. doi:10.1177/0018720818761256.
- Bartsch, M., and T. Dienlin. 2016. ' Control Your Facebook: An Analysis of Online Privacy Literacy. ' Computers in Human Behavior 56: 147 -154.
- Beach, D. A. 1989. ' Identifying the Random Responder. ' The Journal of Psychology 123 (1): 101 -103.
- Brandt, D. S. 2001. Information Technology Literacy: Task Knowledge and Mental Models.
- Bruce, A., I. Nourbakhsh, and R. Simmons. 2002. ' The Role of Expressiveness and Attention in Human-Robot Interaction. ' Paper Presented at the Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No. 02CH37292).
- Buckingham, D. 1989. ' Television Literacy: A Critique. ' Radical Philosophy 51: 12 -25.
- Buckingham, D., and A. Burn. 2007. ' Game Literacy in Theory and Practice. ' Journal of Educational Multimedia and Hypermedia 16 (3): 323 -349.
- Burton-Jones, A. 2005. New Perspectives on the System Usage Construct.
- Calvani, A., A. Cartelli, A. Fini, and M. Ranieri. 2008. ' Models and Instruments for Assessing Digital Competence at School. ' Journal of E-Learning and Knowledge Society 4 (3): 183 -193.
- Calvani, A., A. Fini, and M. Ranieri. 2009. ' Assessing Digital Competence in Secondary Education. Issues, Models and Instruments. ' Issues in Information and Media Literacy: Education, Practice and Pedagogy , 153 -172.
- Clark, L. A., and D. Watson. 1995. ' Constructing Validity: Basic Issues in Objective Scale Development. ' Psychological Assessment 7 (3): 309 -319.
- Clark, L. A., and D. Watson. 2016. Constructing Validity: Basic Issues in Objective Scale Development.
- Comunello, F., S. Mulargia, F. Belotti, and M. FernándezArdèvol. 2015. ' Older People ' s Attitude Towards Mobile Communication in Everyday Life: Digital Literacy and Domestication Processes. ' Paper presented at the International Conference on Human Aspects of it for the Aged Population.
- Costello, A., and J. Osborne. 2005. ' Best Practices in Exploratory Factor Analysis: Four Recommendations for Getting the Most from Your Analysis. ' Pract Assess, Res Eval.
- Cox III, E. P. 1980. ' The Optimal Number of Response Alternatives for a Scale: A Review. ' Journal of Marketing Research 17 (4): 407 -422.
- Davenport, T. H., and R. Ronanki. 2018. ' Arti /uniFB01 cial Intelligence for the Real World. ' Harvard Business Review 96 (1): 108 -116.
- DeVellis, R. F. 2016. Scale Development: Theory and Applications . Vol. 21. Sage publications.
- Dinev, T., and P. Hart. 2004. ' Internet Privacy, Social Awareness, and Internet Technical Literacy. An Exploratory Investigation. ' BLED 2004 Proceedings , 24.
- Donat, E., R. Brandtweiner, and J. Kerschbaum. 2009. ' Attitudes and the Digital Divide: Attitude Measurement as Instrument to Predict Internet Usage. ' Informing Science: The International Journal of an Emerging Transdiscipline 12: 37 -56.
- Druga, S., S. T. Vu, E. Likhith, and T. Qiu. 2019. ' Inclusive AI Literacy for Kids Around the World. ' Paper Presented at the Proceedings of FabLearn 2019.
- Eisenberg, M. B., C. A. Lowe, and K. L. Spitzer. 2004. Information Literacy: Essential Skills for the Information Age . ERIC.
- Eshet, Y. 2004. ' Digital Literacy: A Conceptual Framework for Survival Skills in the Digital era. ' Journal of Educational Multimedia and Hypermedia 13 (1): 93 -106.
- Ferrari, A. 2012. ' Digital Competence in Practice: An Analysis of Frameworks. ' In: Luxembourg: Publication o /uniFB03 ce of the EU. Research Report by the Joint … .


<!-- PAGE 14 -->


<!-- image -->

- Finstad, K. 2010. ' Response Interpolation and Scale Sensitivity: Evidence Against 5-Point Scales. ' Journal of Usability Studies 5 (3): 104 -110.
- Fornell, C., and D. F. Larcker. 1981. ' Evaluating Structural Equation Models with Unobservable Variables and Measurement Error. ' Journal of Marketing Research 18 (1): 39 -50.
- Gapski, H. 2007. ' Some Re /uniFB02 ections on Digital Literacy. ' Paper Presented at the Proceedings of the 3rd International Workshop on Digital Literacy.
- Gilster, P., and P. Glister. 1997. Digital Literacy . New York: Wiley Computer Pub.
- Grassian, E. S., and J. R. Kaplowitz. 2001. ' Information Literacy Instruction. ' Neal-Schuman, New York, NY .
- Gunkel, D. J. 2012. The Machine Question: Critical Perspectives on AI, Robots, and Ethics . MIT Press.
- Hair, J. F., W. C. Black, B. J. Babin, R. E. Anderson, and R. L. Tatham. 1998. Multivariate Data Analysis . 5 vols. Upper Saddle River, NJ: Prentice hall.
- Hallaq, T. 2016. ' Evaluating Online Media Literacy in Higher Education: Validity and Reliability of the Digital Online Media Literacy Assessment (DOMLA). ' Journal of Media Literacy Education 8 (1): 62 -84.
- Hargittai, E. 2005. ' Survey Measures of Web-Oriented Digital Literacy. ' Social Science Computer Review 23 (3): 371 -379.
- Henseler, J., C. M. Ringle, and M. Sarstedt. 2015. ' A New Criterion for Assessing Discriminant Validity in Variance-Based Structural Equation Modeling. ' Journal of the Academy of Marketing Science 43 (1): 115 -135.
- Hinkin, T. R. 1985. ' Development and Application of New Social Power Measures in Superior-Subordinate Relationships. ' University of Florida.
- Hinkin, T. R. 1998. ' A Brief Tutorial on the Development of Measures for Use in Survey Questionnaires. ' Organizational Research Methods 1 (1): 104 -121.
- Ho /uniFB00 man, M., and J. Blake. 2003. ' Computer Literacy: Today and Tomorrow. ' Journal of Computing Sciences in Colleges 18 (5): 221 -233.
- Hsu, L., and P. Wu. 2013. ' Electronic-Tablet-Based Menu in a Full Service Restaurant and Customer Satisfaction-a Structural Equation Model. ' International Journal of Business, Humanities and Technology 3 (2): 61 -71.
- Hu, C. 2013. ' A New Measure for Health Consciousness: Development of A Health Consciousness Conceptual Model. ' Paper presented at the Unpublished paper presented at National Communication Association 99th Annual Convention, Washington, DC, November 2013.
- Jarrahi, M. H. 2018. ' Arti /uniFB01 cial Intelligence and the Future of Work: Human-AI Symbiosis in Organizational Decision Making. ' Business Horizons 61 (4): 577 -586.
- Kandlhofer, M., G. Steinbauer, S. Hirschmugl-Gaisch, and P. Huber. 2016. ' Arti /uniFB01 cial Intelligence and Computer Science in Education: From Kindergarten to University. ' Paper Presented at the 2016 IEEE Frontiers in Education Conference (FIE).
- Katz, I. R. 2007. ' Testing Information Literacy in Digital Environments: ETS ' s ISkills Assessment. ' Information Technology and Libraries 26 (3): 3 -12.
- Kim, Y. 2013. ' A Study of Primary School Teachers ' Awareness of Digital Textbooks and Their Acceptance of Digital Textbooks Based on the Technology Acceptance Model. ' Journal of Digital Convergence 11 (2): 9 -18.
- Kline, R. B. 2015. Principles and Practice of Structural Equation Modeling . Guilford publications.
- Leahy, D., and D. Dolan. 2010. ' Digital Literacy: A Vital Competence for 2010? ' Paper Presented at the IFIP International Conference on Key Competencies in the Knowledge Society.
- Lee, S., and J. Choi. 2017. ' Enhancing User Experience with Conversational Agent for Movie Recommendation: E /uniFB00 ects of Self-Disclosure and Reciprocity. ' International Journal of Human-Computer Studies 103: 95 -105.
- Likert, R. 1932. ' A Technique for the Measurement of Attitudes. ' Archives of Psychology .
- Livingstone, S., and S. Van der Graaf. 2008. ' Media Literacy. ' The International Encyclopedia of Communication .
- Long, D., and B. Magerko. 2020. ' What is AI Literacy? Competencies and Design Considerations. ' Paper Presented at the CHI ' 20: CHI Conference on Human Factors in Computing Systems, April 21.
- Luo, X., S. Tong, Z. Fang, and Z. Qu. 2019. ' Machines Versus Humans: The Impact of AI Chatbot Disclosure on Customer Purchases. ' Luo, X, Tong S, Fang Z, Qu (2019).
- Martin, A., and J. Grudziecki. 2006. ' DigEuLit: Concepts and Tools for Digital Literacy Development. ' Innovation in Teaching and Learning in Information and Computer Sciences 5 (4): 249 -267.
- Mayerl, J. 2016. ' Environmental Concern in Cross-National Comparison: Methodological Threats and Measurement Equivalence. ' In Green European , 210 -232. Routledge.
- McMillan, S. 1996. ' Literacy and Computer Literacy: De /uniFB01 nitions and Comparisons. ' Computers &amp; Education 27 (3-4): 161 -170.
- Metelskaia, I., O. Ignatyeva, S. Denef, and T. Samsonowa. 2018. ' A Business Model Template for AI Solutions. ' Paper Presented at the Proceedings of the International Conference on Intelligent Science and Technology.
- Minsky, M. 2007. The Emotion Machine: Commonsense Thinking, Arti /uniFB01 cial Intelligence, and the Future of the Human Mind: Simon and Schuster.
- Moore, D. R. 2011. ' Technology Literacy: The Extension of Cognition. ' International Journal of Technology and Design Education 21 (2): 185 -193.
- Mueller, S. T., R. R. Ho /uniFB00 man, W. Clancey, A. Emrey, and G. Klein. 2019. ' Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI. ' arXiv preprint arXiv:1902.01876 .
- Munro, M. C., S. L. Hu /uniFB00 , B. L. Marcolin, and D. R. Compeau. 1997. ' Understanding and Measuring User Competence. ' Information &amp; Management 33 (1): 45 -57.
- Neves, B. B., F. Amaro, and J. R. Fonseca. 2013. ' Coming of (Old) Age in the Digital age: ICT Usage and Non-Usage Among Older Adults. ' Sociological Research Online 18 (2): 22 -35.
- Ng, W. 2012. ' Can we Teach Digital Natives Digital Literacy? ' Computers &amp; Education 59 (3): 1065 -1078.
- Noh, Y. 2017. ' A Study on the E /uniFB00 ect of Digital Literacy on Information Use Behavior. ' Journal of Librarianship and Information Science 49 (1): 26 -56.
- Nomura, T., T. Kanda, and T. Suzuki. 2006. ' Experimental Investigation Into In /uniFB02 uence of Negative Attitudes Toward Robots on Human -Robot Interaction. ' Ai &amp; Society 20 (2): 138 -150. doi:10.1007/s00146-005-0012-7.


<!-- PAGE 15 -->


- Norman, D. 2013. The Design of Everyday Things: Revised and Expanded Edition . Basic books.
- Park, Y. J. 2013. ' Digital Literacy and Privacy Behavior Online. ' Communication Research 40 (2): 215 -236.
- Pett, M. A., N. R. Lackey, and J. J. Sullivan. 2003. Making Sense of Factor Analysis: The Use of Factor Analysis for Instrument Development in Health Care Research . Sage.
- Pettit, F. A. 2002. ' A Comparison of World-Wide Web and Paper-and-Pencil Personality Questionnaires. ' Behavior Research Methods, Instruments, &amp; Computers 34 (1): 50 -54.
- Poria, S., E. Cambria, R. Bajpai, and A. Hussain. 2017. ' A Review of A /uniFB00 ective Computing: From Unimodal Analysis to Multimodal Fusion. ' Information Fusion 37: 98 -125.
- Porter, C. E., and N. Donthu. 2006. ' Using the Technology Acceptance Model to Explain how Attitudes Determine Internet Usage: The Role of Perceived Access Barriers and Demographics. ' Journal of Business Research 59 (9): 999 -1007.
- Prior, D. D., J. Mazanov, D. Meacheam, G. Heaslip, and J. Hanson. 2016. ' Attitude, Digital Literacy and Self E /uniFB03 cacy: Flow-on E /uniFB00 ects for Online Learning Behavior. ' The Internet and Higher Education 29: 91 -97.
- Rosling, A., and K. Littlemore. 2011. ' Improving Student Mental Models in a New University Information Setting. ' In Digitisation Perspectives , 89 -101. Cham, Switzerland: Springer.
- Russell, S., and P. Norvig. 2002. Arti /uniFB01 cial Intelligence: A Modern Approach.
- Schriesheim, C. A., K. J. Powers, T. A. Scandura, C. C. Gardiner, and M. J. Lankau. 1993. ' Improving Construct Measurement in Management Research: Comments and a Quantitative Approach for Assessing the Theoretical Content Adequacy of Paper-and-Pencil Survey-Type Instruments. ' Journal of Management 19 (2): 385 -417.
- Smith, R. G., and J. Eckroth. 2017. ' Building AI Applications: Yesterday, Today, and Tomorrow. ' AI Magazine 38 (1): 6 -22.
- Stembert, N., and M. Harbers. 2019. ' Accounting for the Human When Designing with AI: Challenges Identi /uniFB01 ed. ' CHI ' 19-Extended Abstracts, Glasgow, Scotland Uk -May 04-09 , 2019.
- Su, G. 2018. ' Unemployment in the AI Age. ' AI Matters 3 (4): 35 -43.
- Tao, J., and T. Tan. 2005. ' A /uniFB00 ective Computing: A Review. ' Paper presented at the International Conference on A /uniFB00 ective Computing and Intelligent Interaction.
- Tarafdar, M., C. M. Beath, and J. W. Ross. 2019. ' Using AI to Enhance Business Operations. ' MIT Sloan Management Review 60 (4): 37 -44.
- Tobin, C. D. 1983. ' Developing Computer Literacy. ' Arithmetic Teacher 30 (6): 22 -60.
- Vossen, S., J. Ham, and C. Midden. 2010. ' What Makes Social Feedback from a Robot Work? Disentangling the E /uniFB00 ect of Speech, Physical Appearance and Evaluation. ' Paper presented at the International Conference on Persuasive Technology.
- Weisberg, M. 2011. ' Student Attitudes and Behaviors Towards Digital Textbooks. ' Publishing Research Quarterly 27 (2): 188 -196.
- Wilson, H. J., and P. R. Daugherty. 2018. ' Collaborative Intelligence: Humans and AI Are Joining Forces. ' Harvard Business Review .
- Wilson, M., K. Scalise, and P. Gochyyev. 2015. ' Rethinking ICT Literacy: From Computer Skills to Social Network Settings. ' Thinking Skills and Creativity 18: 65 -80.
- Xiao, W., and M. Bie. 2019. ' The Reform and Practice of Educational Technology Major in the Age of Arti /uniFB01 cial Intelligence 2.0. ' Paper presented at the IOP Conference Series: Materials Science and Engineering.
- Zhou, H., M. Itoh, and S. Kitazaki. 2021. ' How Does Explanation-Based Knowledge In /uniFB02 uence Driver TakeOver in Conditional Driving Automation? ' IEEE Transactions on Human-Machine Systems .