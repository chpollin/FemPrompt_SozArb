---
source_file: Creswell Báez_2025_Clinical_Social_Workers’_Perceptions_of_Large.pdf
conversion_date: 2026-02-03T08:46:01.003942
converter: docling
quality_score: 95
---

<!-- image -->

## Journal of Evidence-Based Social Work

ISSN: 2640-8066 (Print) 2640-8074 (Online) Journal homepage: www.tandfonline.com/journals/webs22

## Clinical Social Workers' Perceptions of Large Language Models in Practice: Resistance to Automation and Prospects for Integration

## Johanna Creswell Báez, Eunhye Ahn, Aubrey Tamietti, Bryan G. Victor &amp; Lauri Goldkind

To cite this article: Johanna Creswell Báez, Eunhye Ahn, Aubrey Tamietti, Bryan G. Victor &amp; Lauri Goldkind (2026) Clinical Social Workers' Perceptions of Large Language Models in Practice: Resistance to Automation and Prospects for Integration, Journal of Evidence-Based Social Work, 23:1, 42-63, DOI: 10.1080/26408066.2025.2542450

To link to this article: https://doi.org/10.1080/26408066.2025.2542450

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

Published online: 01 Aug 2025.

Submit your article to this journal

Article views: 288

View related articles

View Crossmark data

曲

CrossMark

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

Large language models (LLMs) hold significant promise for improving the practice of mental health  treatment  and  clinical  social  work  in  particular.  LLMs  are  AI  systems  trained  on extensive text data to statistically model patterns of words and ideas, enabling them to generate context-sensitive text and adapt to new tasks without additional coding, and have evolved into

<!-- image -->

<!-- image -->

<!-- image -->

models capable of solving complex, multi-step problems (Brown et al., 2020; Zhao et al., 2023). Generic LLMs such as ChatGPT, Claude, and Gemini offer broad capabilities in generating text-  based  documents  such  as  treatment  plans,  psychoeducational  materials  and  clinical documentation  (Schueller  &amp;  Morris,  2023;  Stade  et  al.,  2024).  These  models  also  have  the capacity  to  generate  illustrations,  images  and  computer  code,  opening  new  possibilities  for social workers to market and promote their practices. This major technological advancement has prompted urgent questions within the profession about how LLMs might reshape clinical social work practice and challenge our disciplinary boundaries (Victor &amp; Goldkind, 2025).

The  ability  of  LLMs  to  quickly  generate  human-like  text  responses  to  plain  language queries makes them useful tools for efficiently supporting decision-making and assessment. For  instance,  researchers  have  demonstrated  that  LLMs  can  be  effective  in  helping  to generate  clinical  case  notes,  doing  so  with  accuracy  and  high-quality  organization  (Ali et al., 2024). LLMs have also shown promise for enhancing the ability of mental health care providers to make differential diagnoses (McDuff et al., 2025) and for enhancing peer-topeer mental health support through AI-assisted feedback systems that help peer supporters write more empathic responses (Sharma et al., 2023).

While professionals are considering the ethics, utility, and effectiveness of LLM-based tools simultaneously, consumers of mental health services are also adopting LLM technologies for personal and therapeutic purposes (Raile, 2024). Research suggests that clients are using LLM tools to supplant traditional face to face psychotherapy, and that there is some evidence that clients prefer services delivered by LLMs (Benda et al., 2024; Zao-Sanders, 2025). In some cases, individuals are turning to LLM-powered chatbots for mental health support rather than human practitioners with AI chatbots being recommended for mild to moderate psychological issues (Kuhail et al., 2025; J. Liu et al., 2023). For example, LGBTQ individuals have sought mental health support from LLMs when faced with limited care options tailored to their unique needs (Ma et al., 2024). In a similar study, McNally et al. (2024) found that autistic individuals used ChatGPT to help with tasks in their personal and professional lives, and in so doing reduced their sense of stress and anxiety.

This study employs a reflexive thematic analysis to co-construct practitioners' experiences of  LLMs  through  the  theoretical  lens  of  the  Technology  Acceptance  Model  (TAM)  that focuses  on  perceived  usefulness  and  emerging  concerns.  Reflexive  thematic  analysis  is a qualitative research method that emphasizes the researcher's active, reflexive engagement in  an  iterative  process  of  coding  and  theme  development  to  identify  patterns  of  meaning making  (Braun  &amp;  Clarke,  2022,  2023,  2024,  2025).  The  TAM  suggests  that  if  individuals perceive new digital technologies to be both useful as well as easy to implement, they will adopt them at greater rates. In individual interviews, participants in this study discussed their acceptance of LLMs in a professional context and shared their concerns. Below we present the findings from these interviews as well as recommendations for future research to inform the mental health workforce and allied health professions on AI use in clinical practice.

## Literature review

## LLM use in client-involved practice

LLMs have demonstrated utility across several key functions relevant to clinical social work, although  most  existing  examples  come  from  health  care  and  mental  health  rather  than

<!-- image -->

social work itself (Bedi et al., 2024; Hua et al., 2025; Victor &amp; Goldkind, 2025). For example, in  care  coordination,  LLM-assisted tools have been shown to enhance the reliability and completeness  of  care  plans  (Dağci  et  al.,  2024)  and  to  improve  the  appropriateness  of emergency referrals (Barash et al., 2023). In psychoeducational support, LLMs have produced client-friendly materials that may increase understanding of coping strategies and legal rights (Babayiğit et al., 2023). LLMs have also been investigated as decision-support tools,  generating  personalized,  evidence-based  treatment  recommendations  aligned  with established frameworks (Perlis et al., 2024; Wilhelm et al., 2023). In clinical documentation, real-time drafting of session notes can reduce administrative time without compromising accuracy (Kassab et al., 2024). Voice recognition tools that automatically transcribe conversations  between  clinicians  and  clients,  such  as  Nuance's  Dragon  Copilot,  can  reduce documentation  time  by  creating  after-visit  summaries  and  referral  letters  (Bundy  et  al., 2024), yet T. L. Liu et al. (2024) found no significant time savings in a longitudinal study.

AI-driven LLM tools also have been applied to a range of client-facing mental health interventions,  each  leveraging  different  formats  and  evidence-based  interventions.  For instance, text-based chatbots engage users in conversational counseling, offering emotional support  and  relationship  guidance  in  a  low-barrier  format  that  can  be  accessed  anytime Vowels et al. (2024); Limpanopparat et al. (2024). Platforms such as Woebot, Wysa, Youper, and  Elomia  deliver  structured  psychoeducation,  mood  monitoring,  and  cognitivebehavioral  exercises  via  self-guided  chat  interfaces.  Hybrid  models  like  Digital  Clinic combine automated dialogue with licensed human clinician coaching Macrynikola et al. (2023). Beyond text, immersive LLM-supported interventions, such asVR-based exposure therapy and journaling assistants, have been developed for anxiety, depression, and ADHD management (Berrezueta-Guzman et al., 2025; Spiegel et al., 2024). Complementing these are  positive  psychology  interventions  delivered  via  LLM-powered  platforms,  such  as Resonance,  have  incorporated  reframing  exercises  and  gratitude  journaling  prompts, which have been associated with increases in self-reported well-being (Zulfikar et al., 2025).

## Unique considerations for social work client support

Although LLM-powered tools offer potential benefits, such as automating documentation and providing psychoeducational support, clinical social work has been cautious in adopting them, reflecting not only resource constraints but also a commitment to core professional  values  (Victor  &amp;  Goldkind,  2025;  Zorn  et  al.,  2011).  Under-resourced  agencies, ethical and legal considerations, and limited training opportunities require careful evaluation to ensure client dignity and equity (Goldkind et al., 2016). The emphasis on face-toface relationships in social work practice also leads some practitioners to question whether algorithms can capture the nuances of human interaction (Wassal et al., 2024).

Moreover, there are concerns that AI chatbots could displace human social workers or create a two-tiered service model, where clients from high-income backgrounds continue receiving in-person support, while individuals from marginalized communities with lower incomes rely on automated, algorithm-driven care, risking a widening of existing inequalities Victor and Goldkind (2025); Lai et al. (2024). At the same time, there are unresolved questions about how LLMs handle sensitive client information, who holds liability if an AIgenerated recommendation leads to harm, and whether automated interactions can sustain the nuanced, trust-based relationship central to social work practice Victor and Goldkind

45

(2025).  Even  when  AI  tools  offer  clear  benefits,  clinicians  may  hesitate  to  adopt  them without endorsements from agency leaders, professional organizations, or respected colleagues  (Wassal  et  al.,  2024).  A  recent  survey  of  German  social  work  students  indicates growing interest in LLMs among both technical and non-technical users but also highlights concerns  about  whether  these  tools  will  deliver  on  promised  benefits  (Meinhardt-Injac et al., 2025). Despite this interest, there is still a significant gap in empirical data on how clinical social workers perceive and engage with LLMs in their own practice contexts remain scarce.

## Technology acceptance model and LLM use

Understanding how social work practitioners are discerning whether to adopt LLM tools as well  as  how  they  perceive  the  benefits  posed  by  generative  AI  is  aided  by  a  theoretical framework that captures how individuals think about and engage with emerging technologies  within  professional  contexts.  The  Technology  Acceptance  Model  (TAM)  provides such a lens, offering insight into how individuals evaluate and ultimately accept or reject these  technologies  (Davis  et  al.,  1989;  Greener,  2022).  Although  originally  developed  for organizational settings, TAM has been used to examine the adoption of new technologies in professional  practice,  including  in  health  and  human  services  (AlQudah  et  al.,  2021; Mastour  et  al.,  2025).  As  early  as  2007,  Zhang  and  Gutierrez  (2007)  employed  a  TAMorientated  analysis  to  explore  the  use  of  homeless  services  management  information systems; while more recently, Barrera-Algarín et al. (2023) assessed social workers' attitudes toward  digital  innovations  across  thirteen  countries,  showing  high  acceptance  of  digital tools, particularly among experienced practitioners.

TAM  emerged  in  the  1980s  as  a  foundational  theory  for  understanding  individual adoption  of  information  technologies  in  organizational  settings  (Davis  et  al.,  1989; Greener,  2022).  TAM  theorizes  that  technology  acceptance  is  primarily  determined  by two  key  beliefs:  perceived  usefulness,  which  reflects  an  individual's  assessment  of  how a system will enhance job performance, and perceived ease of use, which captures perceptions of the effort required to use the system (Davis et al., 1989). The model posits that these beliefs mediate the effects of external variables on behavioral intentions and actual system usage, with perceived ease of use also influencing perceived usefulness directly.

Venkatesh and Davis (2000) extended the original TAM to TAM2 by identifying two broad  categories  of  determinants  that  shape  perceived  usefulness  and  intentions  to  use: social influence processes and cognitive instrumental processes. Social influence processes refer to the ways that professional norms, peer expectations, and organizational pressures affect  individual decision-making around new technologies. These influences include the subjective norm, or the perceived expectations of colleagues, supervisors, and professional organizations; image, or the belief that using a particular technology may enhance one's professional  status;  and  voluntariness,  or  the  degree  to  which  individuals  feel  free  or obligated to adopt a technology (Holden &amp; Karsh, 2010).

Cognitive instrumental processes, by contrast, reflect the user's more pragmatic evaluation of how well a technology fits into the actual demands of their work. These processes involve  assessments  of  job  relevance,  or  how  applicable  a  technology  is  to  one's  clinical tasks; output quality, or whether the system produces accurate, reliable, and useful results; and result demonstrability, or the extent to which the benefits of using the technology are

<!-- image -->

visible and measurable in real-world settings. Together with perceived ease of use and social influence processes, these cognitive factors shape whether individuals view technologies as acceptable tools for use in their work (Venkatesh &amp; Davis, 2000).

Through the theoretical lens of TAM2, LLM technologies represent a digital innovation with the potential to disrupt traditional practice norms while also offering new capabilities and  efficiencies.  Unlike  prior  technologies  of  relevance  to  clinical  social  work  such  as electronic  health  record  systems  or  telehealth  platforms,  LLMs  actively  generate  content and can simulate human interactions. These novel abilities raise unique questions related to perceived  usefulness  and  ease  of  use,  particularly  for  a  profession  grounded  in  human relationships, ethics, and the person-in-environment perspective (Schueller &amp; Morris, 2023; Victor &amp; Goldkind, 2025). In clinical social work settings, where norms are often shaped through  peer  consultation,  supervision,  and  professional  discourse  (Kourgiantakis  et  al., 2019), social influence processes are likely to play a significant role in shaping practitioners' perceptions  of  the  challenges  and  opportunities  associated  with  LLMs  (Barrera-Algarín et al., 2023). These influences may either support adoption, when colleagues or supervisors endorse LLMs as useful tools, or contribute to resistance, particularly when professional colleagues' express skepticism about the ethical implications of LLM use or the threats to professional jurisdiction posed by the use of these models.

## Current study's contribution

This study offers insights into how social work practitioners are responding to the arrival of easy-to-access  LLMs.  Since  the  release  of  Open  AI's  ChatGPT  2  in  2022,  new  clinically focused  digital  platforms,  powered  by  LLMs,  have  been  released  and  marketed  to  social workers and their clients.  For  example,  automated  clinical  note  taking  services  offer  the possibility  of  reducing  burnout  and  freeing  practitioner  time  to  expand  the  number  of clients served. Similarly, clients are engaging with these models in ways that may supplement and potentially supplant treatment. This research documents how a group of clinical social workers are engaging with these new generative AI tools. To our knowledge, this is among the first studies to document practitioners' actual use of LLMs, describing how they engage with these tools and in what contexts, building upon our research group's earlier work examining practitioners' emotional readiness to adopt LLMs (Báez et al., 2025). Using the TAM as a conceptual framework, we explore the perceived usefulness of LLMs, the ease of  which  they  are  adoptable,  and  the  types  of  support  practitioners  need  to  engage  with them effectively.

## Materials and methods

## Study design

A  reflexive  thematic  analysis  method  was  used  to  better  understand  the  experiences of  clinical  social  workers  and  their  work  in  terms  of  the  introduction  and  rapid expansion  of  LLMs  (Braun  &amp;  Clarke,  2022,  2023,  2024,  2025).  Semi-structured  oneon-one  zoom  interviews  lasting  30-45 minutes  were  conducted  in  2024  to  explore the  research  question: How  do  clinical  social  workers  perceive  and  describe  their experiences  of  using  LLMs  in  professional  practice? The  interviewers  (authors  1,  5,

and  a  research  assistant)  also  randomly  selected  participants  for  a  brief  follow-up interview  lasting  15-20 minutes  to  explore  how  their  perspectives  evolved  over  time. The interviewers took a collaborative approach in demonstrating the use of LLMs by inviting  participants  to  provide  a  case  dilemma  from  their  own  practice.  The dilemma  was  entered  into  ChatGPT  (without  any  identifying  information)  to demonstrate  how  the  tool  might  generate  relevant  suggestions.  Interviewers  then showed  participants  a  video  of  a  client  discussing  their  experience  with  LLMs  to support  mental  health.  Through  these  interactions,  participants  and  interviewers negotiated  ways  of  engaging  with  LLMs  and  reflected  on  their  evolving  perspectives. This  study  is  part  of  a  larger  project,  with  previous  findings  on  beliefs  and  emotional  responses already reported (Báez et al., 2025; see for further description of the interview  structure).  The  research  was  approved  by  the  university  human  subjects review  committee.

## Sample

Participants were 21 social work practitioners in private practice, with eight participating in the follow-up interview. Among the participants, 57% ( n = 12) had tried an LLM, while 43% ( n = 9)  had  never  used  one.  The  majority  were  female  ( n = 16),  and  the  median  age  was 46-55, with the majority of participants being white ( n = 14). Further, the majority were from  the  Northeast  and  West  of  the  United  States  with  the  majority  having  a  terminal degree  of  master's  in  social  work  (MSW; n = 18).  Recruitment  was  conducted  through e-mails  sent  to  individuals  who  had  attended  a  training  on  LLMs  through  the  National Association  of  Social  Workers  and  had  expressed  interest  in  research  participation. Purposive  sampling  was  used  to  collect  rich,  detailed  accounts  from  practitioners  with varied levels of LLM experience. Following Braun and Clarke's (2021) reflexive thematic approach, we did not aim for thematic 'saturation' but instead focused on ensuring that the sample provided sufficient depth and relevance to address our specific research question.

## Researcher positionality

The three interviewers were all social work scholars with advanced practice in using LLMs. Two of the interviewers  were  social  work  professors  and  one  was  a  social  work  student working as a research assistant. The researchers used a constructivist-pragmatic stance that acknowledges  knowledge  as  co-constructed  through  researcher  -  participant  interaction and prioritizes the practical applicability of insights for informing real-world social work practice  (Lincoln  &amp;  Guba,  1985;  Morgan,  2014).  The  researchers  recognized  that,  as facilitators,  our  own  expertise  with  LLMs  shaped  the  conversations  and  influenced  how participants  made  meaning  of  their  experiences.  Reflexivity  was  central  as  researchers critically considered how their experiences, beliefs, and familiarity with LLMs intersected with the participants' interviews. As a research team, we discussed moments of frustration when participants were really resistant to LLM use which we balanced with compassion for their  concerns.  We  also  found  ourselves  feeling  encouraged  when  participants  explored innovative applications or raised ethical concerns.

## Data analysis

Interviews  were  transcribed,  anonymized  and  analyzed  using  Dedoose  version  9.0 (Dedoose, 2024) as an organizational tool through a six-phase reflexive thematic analysis process  (Braun  &amp;  Clarke,  2022).  The  researchers  immersed  themselves  in  the  data  by reading  transcripts  and  re-watching  video  interviews,  then  collaborated  on  the  coding process  to  enhance  understanding  and  interpretation  of  the  participant  interviews. Throughout this process, the researchers recorded memos on novel insights and reflexive thoughts. The research team met regularly to discuss the codes and created themes based on shared ideas and meaning. The themes were created based on the patterns in the data and then applied to the TAM as a framework for understanding the findings. Themes were then reviewed against the full dataset to ensure coherence and meaningfulness, refined accordingly, and incorporated into the final write-up (Braun &amp; Clarke, 2023).

## Results

The findings reveal a dynamic landscape in which social work practitioners are navigating the  perceived usefulness of integrating LLMs into clinical practice. The participants who had used LLMs previously in their practice were more likely to state that these tools are useful, whereas those who had not used LLMs had more concerns. For practitioners who reported using LLMs, they described utilizing them to support treatment planning, streamline documentation, create psychoeducational content, and engage clients between sessions. LLMs were viewed as a 'useful tool' (Participant #6) for helping to manage heavy workloads, alleviate burnout, and enhance efficiency within high-demand practice settings.

Participants  also  expressed  hesitation  in  terms  of  usefulness  and  ease  of  use  with incorporating LLMs into their practice, expressing 'a lot of concern about artificial intelligence' (Participant #17) and raised questions about the potential for bias in LLMs, risks to client confidentiality, and uncertainty around data security. Others questioned the ethical implications  of  its  use.  One  practitioner  captured  this  uncertainty,  explaining,  'With NASW, our code of ethics  is  so  ingrained,  and  I  don't  know  ethically  how  it  sits  using this  undefined  technology'  (Participant  #10).  Another  participant  echoed  this  sentiment stating, 'The  reason  why  I've been  hesitant is because  of ethical implications' (Participant #18).

Social  workers'  experiences  of  LLMs  in  social  work  practice  coalesced  around  two overarching  themes:  (1)  factors  that  increased  perceived  usefulness  of  LLMs  in  clinical social work practice and (2) factors that diminished perceptions of usefulness (see Table 1). Each factor is explored in detail in the following sections, illustrated with participant quotes. The TAM constructs perceived ease of use , job  relevance , output quality , result  demonstrability , and subjective norms are italicized and threaded throughout the analysis, explicitly linking each construct to the factors that heighten or lessen perceived usefulness.

## Factors increasing perceived usefulness of LLMs in clinical social work

## Applications in daily practice

Efficiency and productivity. Participants described discovering new ways to integrate LLMs into  their  daily  practice,  with  many  highlighting  the  tools'  ability  to  save  time  and  boost

<!-- image -->

Table 1. Social workers' perceptions of large language model usefulness in clinical practice.

| Factors Increasing Perceived Usefulness                                                                                                                                                                                                                             | Factors Decreasing Perceived Usefulness                                                                                                                                                           |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Applications in Daily Practice Efficiency and Productivity Synthesis of Complex Information Treatment Planning Resource Development Expanding Access to Information and Brainstorm Partner Client Engagement with LLMs Professional Decision-Making and Supervision | Ethical Considerations Confidentiality and Data Security Bias and Equity Quality of Care Impact on Therapeutic Relationship Misinformation Risks Workforce Implications Threat of Job Replacement |

output, which reflected a strong perception of their usefulness in enhancing job performance. LLMs  were  especially  valued  for  their  'time-saving'  advantage  (Participant  #1),  enabling practitioners  to  'speed  up'  (Participant  #2)  content  generation  and  administrative  work. One practitioner shared:

Honestly,  the  productivity  is  incredible.  I'm  the  executive  director  of  this  group  private practice, and it's just like I have got so much to do, that if I can just input something to send a really good email to 40-something people, it's so much easier than having to sit and construct it, when I just have a lot to do. So, productivity is my favorite thing about it. (Participant #3)

Participants also emphasized how LLMs helped streamline their workflow by organizing thoughts  and  directing  focus.  Essential  administrative  tasks  such  as  documentation, e-mails,  and  scheduling,  often  detracted  from  more  meaningful  clinical  engagement. LLMs offered a way to structure this work more effectively, making these tasks not only faster  but  cognitively  easier  to  manage.  As  one  practitioner  explained,  'It  is  helpful  in focusing my thoughts and directing them in a very specific way rather than just having things  floating  around  . . .  ChatGPT  is  the  super  brain  that  holds  it  all  together' (Participant  #4).  Collectively,  these  accounts  underscore  the  perceived  usefulness  of LLMs in streamlining workflows and supporting the cognitive demands of clinical social work.

These insights reflect the growing perception that LLMs not only reduce cognitive load but also effectively offload routine tasks, aligning with core TAM constructs like perceived ease of use and job relevance .  As a result, social workers could reallocate time and mental energy toward client care and clinical decision-making. Several participants noted that this increased efficiency helped them manage high caseloads while also mitigating stress and burnout.

Synthesis  of  complex  information. In  addition  to  improving  productivity,  LLMs  were recognized  for  their  ability  to  help  synthesize  and  organize  complex  information.  This function was especially useful when practitioners needed to quickly grasp large volumes of material or enter unfamiliar areas of practice. This aligns with participants' perceptions of the models' output quality and their capacity to deliver relevant, job-specific support. One participant described this potential, 'I would love ChatGPT to synthesize that data, pull out themes for me and recommendations' (Participant #5). Another elaborated on how LLMs support clear communication and concise thinking:

I think just how quickly it can provide some feedback, templates, how quickly it can synthesize all of the information that maybe I have trouble being very concise, and very straightforward with.

<!-- image -->

I find it extremely helpful with that, with being able to just take a whole bunch of information, and then just making it very concise, and I think that's very helpful. (Participant #6)

By  summarizing  and  contextualizing  information  quickly,  LLMs  enabled  participants  to make more informed decisions in less time, a critical advantage in high-pressure clinical environments.  This  ability  to  extract  meaning  from  complexity  with  speed  and  clarity highlighted  the output  quality and result  demonstrability of  these  tools,  which  are  key features that made their impact tangible in clinical practice.

Treatment planning. Treatment planning emerged as a key area where participants found LLMs  useful  for  helping  to  'generate  useful  ideas  for  treatment'  (Participant  #7). Practitioners described using these tools to create starting points that could be tailored to each  client's  unique  needs.  As  one  participant  explained,  'Occasionally  I'll  use  it  for treatment planning, but I'll just say, 'For a 30-ish year old woman with dysthymic depression, what are some treatment goals to start with?'' (Participant #8). In this way, LLMs were viewed  as  offering  structured,  clinically  relevant  suggestions  that  helped  streamline  the planning process. These structured outputs reflected a high level of output quality ,  giving clinicians a reliable foundation on which to build individualized plans. Another practitioner emphasized  the  potential  for  LLMs  to  inform  practical  decisions  about  timelines  and interventions:

If  we  have  these  problems,  what  would  be  an  ideal  treatment  plan?  What  would  be  a  good amount of time to possibly even meet with them? A lot of those things are things that we talk about constantly at work. And so, I feel like it would be really useful to have an AI system that maybe can gather all of the data as to what is known to be effective, efficient, and be able to have it be more time limited. (Participant #6)

These reflections highlight the perceived value of LLMs in enhancing the treatment planning process by providing evidence-informed suggestions that can be quickly adapted to meet client needs, enhancing the demonstrability of results in both clinical logic and workflow efficiency.

Resource development. In addition to streamlining treatment planning, LLMs were utilized to develop workshops and resources. One practitioner highlighted its effectiveness, stating, 'I used it to create a questionnaire for social workers across New Mexico, trying to just get feedback about important issues for advocacy purposes, and it was extremely helpful with that' (Participant #4). This case illustrates how LLMs demonstrated strong output quality , enabling practitioners to generate high-quality, purpose-driven content for outreach and advocacy. By leveraging LLMs for creating resources, participants reported that these tools helped them organize their thoughts more effectively, allowing them to efficiently develop relevant, informative programs tailored to community needs. They also noted that LLMs streamlined data collection efforts and significantly cut down the time required to complete such tasks,  allowing  them  to  spend  that  time  'towards  thinking  about  cases  and  clinical work with the administrative support' (Participant #3).

Expanding  access  to  information  and  brainstorm  partner. Participants  described  how LLMs can serve as  valuable  tools  for  broadening  their  knowledge  base  and  acting  as brainstorming  partners  when  developing  treatment  plans,  designing  workshops,  or

crafting  client  interventions.  For  many  practitioners,  LLMs  were  identified  as  tools to  support  'thinking  of  things  outside  of  the  box'  (Participant  #9)  by  offering  fresh ideas  and  generating  prompts  to  inspire  new  directions  of  thinking.  As  one  practitioner  explained,  'I  love  the  idea  that  it  can  help  refine,  or  flesh  out  maybe a  concept  that  you  have,  but  it  might  help  you  broaden  what  you  were  thinking about'  (Participant  #10).  This  theme  of  refining  initial  ideas  and  generating  new ones  appeared  throughout  the  interviews.  One  social  worker  shared,  'It  gave  me  all these  other  pointers  that  I  hadn't  thought  about'  (Participant  #11),  highlighting  the capacity  of  LLMs  to  introduce  alternative  perspectives  and  surface  previously  overlooked  possibilities.

Participants  also  described  LLMs  as  tools  that  could  enhance  inclusive,  culturally responsive practice. As one practitioner noted, 'It might be helpful around diversity, like working with 90-year-old men or 25-year-old men - it helps bridge those gaps and stay culturally sensitive' (Participant #12). In these cases, LLMs were seen as aligning well with the  culturally  attuned  aspects  of  social  work  by  demonstrating  high job  relevance in supporting  equity-informed  decision-making.  By  providing  insight  into  a  range  of  age groups, identities, and cultural contexts, LLMs offered participants opportunities to tailor their  interventions  more  thoughtfully.  Another  practitioner  highlighted  how  these  tools contributed to richer conversations with clients and families, explaining, 'It gives you so many more ideas as to what to say to clients about certain subjects, so it really opens up the dialogue  for  things  that  I  may  not  have  considered'  (Participant  #11).  These  outcomes reinforced a sense of result demonstrability , as LLMs were credited with directly enriching practitioner-client interactions. Practitioners' reflections emphasized the potential of LLMs to shape their thinking and enhance more responsive, equity-informed decision-making.

Practitioners also noted that LLMs served as a useful, on-demand knowledge resource, especially when working in complex or specialized areas of practice. They noted that these tools  could  help  clarify  difficult  concepts  and  guide  them  in  'the  right  direction' (Participant #13) when navigating unfamiliar topics. As one practitioner described:

What is exciting is just the really vast amount of knowledge that it has access to. And so I've had it explore or give me a little bit of a primer on some pretty arcane concepts and psychoanalytic concepts that I run across every once in a while in my reading that are pretty. . . So what I found with it is it does require a little bit more coaching. In other words, it's like you walk it through within the psychoanalytic idea relations. 'Can you tell me about . . . ' I can't remember what I did all that recently and it does a really nice job of explaining things. And so actually I've been able to do some learning on my own that has been. . .I take it with a grain of salt to a certain extent in the sense that it's like, 'Okay, that tracks with what I've been reading in this book,' where the book is kind of at a higher level and then I'm asking ChatGPT to catch me up on the particular concept. (Participant #14)

While participants acknowledged the importance of approaching LLM generated information  with  caution,  many  found  that  LLMs  were  effective  in  bridging  learning  gaps  by simplifying and consolidating complex material. This capacity to make technical information accessible contributed to perceptions of both output quality and result demonstrability . Whether used to deepen clinical knowledge, enhance creative thinking, or promote culturally  responsive  care,  LLMs  were  consistently  described  as  a  valuable  tool  for  increasing access to information and supporting idea generation within social work practice.

<!-- image -->

## Client engagement with LLMs

Practitioners reported that clients were already using LLMs independently, with one sharing, 'I have lots of clients who love ChatGPT and use it daily and multiple times a day' (Participant #9). LLMs were seen as a helpful adjunct to therapy, offering support between sessions,  especially  for  mood  regulation  and  real-time  coping  strategies.  LLMs  were described  as  a  tool  in  helping  clients  access  resources  to  better  regulate  emotions  and develop  healthier  coping  strategies.  Practitioners  listed  ways  that  clients  are  using ChatGPT to ask questions such as, 'What are some other things that  I can do  to  boost my  mood  right  now?'  (Participant  #8)  when  they're  feeling  stuck  and  want  real  time suggestions. For those who may not have access to traditional therapy, LLMs were noted as an alternative support system. One practitioner highlighted this accessibility, stating, 'I think that people that don't have good insurance or don't have the time can go in and put in, 'How do I deal with my husband leaving me, and I'm depressed.' And then, you get these 10 bullets  as  to  what  to  do'  (Participant  #11).  LLMs  were  seen  as  effective  in  providing practical suggestions that could help clients adopt healthier behaviors and quickly access coping strategies during moments of distress. This ability to deliver actionable, emotionally relevant suggestions was seen as a marker of strong output quality ,  especially in contexts where professional support was limited.

Practitioners also viewed LLMs as a resource to expand client access to mental health resources,  particularly  for  individuals  facing  financial,  systemic,  or  emotional  barriers  to seeking traditional support. One social worker emphasized this benefit, stating:

I also think that it offers a great opportunity for accessible research and accessible resources for a therapist to share with clients. That's real time. It's free. I worry as a social worker a lot about making sure that  my  clients  have  access  to  the  resources  and  supports  that  they  need,  and sometimes they don't have the privilege to be able to buy a book, or go to a library, or have access to certain things, and I think this is a great opportunity for folks to be able to use it (Participant #8).

By offering on-demand information and self-help strategies, LLMs were perceived as tools that  could  empower  clients  to  take  greater  accountability  of  their  well-being,  helping  to bridge gaps in access to mental health resources while complementing the support received through traditional therapy.

Additionally,  practitioners  noted  that  LLMs  have  the  potential  to  support  continued learning  outside  of  clinical  sessions.  As  one  practitioner  explained,  'If  a  client  leaves a  session  and  maybe  didn't  feel  safe  asking  about  self-care,  they  can  go  home  and  look that up and start to incorporate it' (Participant #10). This level of accessibility allows clients to take greater ownership of their healing process while promoting independence and selfdirected growth. As another practitioner noted, 'So I think it would add to their feelings of autonomy, add feeling that they can actually have impact and figure things out themselves with a little bit of help, increase their knowledge, increase their sensitivity' (Participant #2). By  providing  on-demand  resources  and  guidance,  participants  shared  that  LLMs  could support clients in building confidence, expanding their understanding, and engaging more proactively in their well-being.

Practitioners also shared that LLMs showed potential for supporting clients who experience challenges with interpersonal interactions, such as individuals on the autism spectrum. One practitioner reflected:

It really did demonstrate to me that there's certainly potential here for it to be a useful tool for people  who  struggle  with  interpersonal  interactions.  I  guess  it  all  depends  on  what  you're feeding into it, and so that would, I can see that there would be ways in which folks would have to be trained to how to make the most of it and how to use it effectively. But I think actually it has a lot of potential, especially I think the autism example is really probably an immediate one where it's of immediate use to people. (Participant #14)

Practitioners noted that for individuals who experience challenges with social cues, conversation skills, or emotional expression, LLMs could serve as a helpful tool for practicing communication and preparing for conversations. They observed that engaging with these tools  could  boost  a  clients'  confidence  and  support  the  development  of  strategies  for navigating social situations.

## Professional decision-making and supervision

LLMs were  also  recognized  as  a  valuable  resource  for  supporting  professional  decisionmaking  and  supervision  in  social  work.  Practitioners  described  it  as  functioning  like a 'second set of eyes' (Participant #4), offering timely insights and guidance when direct supervision  or  peer  consultation  is  not  readily  available.  This  acceptance  appears  to  be influenced by practitioners' perceptions of the technology's job relevance and output quality in their professional context. It was identified that this function is particularly beneficial in fast-paced or isolated settings such as schools, hospitals, and community-based agencies, where social  workers  may  not  always  have  access  to  colleagues  for  consultation.  As  one practitioner shared,

Maybe I don't have access to a colleague to kind of run something by with them. It might be easier to just hop on Chat and just ask a few questions. So then obviously not just default off of whatever  it  says,  but  just  to  kind  of  help  guide  thought  processes  and  decision  making. (Participant #6)

In these situations, participants shared that LLMs could serve as a virtual sounding board to help navigate complex or uncertain cases while encouraging critical thinking and informed practice. Another practitioner echoed these sentiments explaining,

So it's not going to be perfect the responses that I find from ChatGPT, but I think it's such a great sounding board and this is why I love using it in supervision . . . we'll just prompt it right into ChatGPT and then discuss what we agree, disagree, if something's missing, or maybe we haven't thought about taking this approach with the students. (Participant #1)

The tool's ability to generate thoughtful, discussion-worthy content was noted as a sign of output quality , particularly when used to enrich professional dialogue. Practitioners identified  that  LLMs  have  the  potential  to  enhance  collaborative  discussions,  not  by  replacing supervision,  but  by  adding  new  perspectives  and  strategies  for  consideration.  This  LLM generated  content,  especially  when  considered  as  a  group,  also  reflects  changing  norms around using technology in supervision and suggests a shift in what is considered helpful preparation. This practice was seen as a way to foster deeper analysis and support more comprehensive clinical reasoning among both practitioners and supervisees. The demonstration  of  tangible  results  in  supervision  sessions  appeared  to  reinforce  practitioners' positive attitudes toward the technology.

Practitioners  viewed  LLMs  as  valuable  tools  for  supporting  both  early-career  social workers  and  those  feeling  uncertain  or  'stuck'  (Participant  #15)  in  their  clinical  work.

<!-- image -->

Beyond offering direction and generating new ideas, participants emphasized the role of LLMs  in  enhancing  reflective  and  critical  thinking,  particularly  within  the  context  of supervision.  One  practitioner  shared,  'I've  used  it  before  to  support  a  supervisee  on exploring  self-disclosure.  And  it  came  up  with  some  really  good  reflective  prompts  . . . thinking about self-disclosure and questions to ask' (Participant #1). These prompts were seen as useful for encouraging ethical reflection, deepening consideration of client perspectives, and clarifying professional boundaries. Participants recognized the potential of LLMs as accessible, thought-provoking companions that foster practitioner confidence, promote critical analysis, and support ongoing professional development.

## Factors decreasing perceived usefulness of LLMs in social work

## Ethical considerations

Confidentiality and data security. Practitioners noted that they engage with highly sensitive client information and expressed that confidentiality and data security are fundamental to ethical practice. As LLMs become more accessible in clinical settings, many participants weighed their potential benefits against the risks they may pose to client privacy. Several participants  shared  concerns  about  the  ethical  complexities  of  integrating  LLMs  into documentation  and  treatment  planning.  Many  practitioners  voiced  uncertainty  about what types of clinical information are ethically and legally appropriate to enter into LLMs and questioned whether these tools can truly 'guarantee the safety of the data they store about a client' (Participant #4). One practitioner explained, 'There's just some hesitancy about something that's so new and making sure we are protecting our students and clients,' (Participant #6).

This hesitancy reflects concerns not only about security but also about subjective norms , meaning the lack of clear professional consensus or institutional guidance on what constitutes  ethical  use.  Participants  questioned  whether  using  LLMs  introduces  new  vulnerabilities  and  whether  current  digital  tools  can  offer  more  protection  than  traditional methods. One practitioner noted, 'What is better? A piece of paper in a file cabinet? . . . Just send me a fax . . . And what's more secure? Who knows anymore with AI' (Participant #15).  This  sense  of  ambiguity  suggests  a  need  for  system-level  leadership  to  help  shape evolving norms around data handling in the context of AI.

Some participants emphasized the measures they take to minimize risk, such as using vague  language  in  notes  and  limiting  the  amount  of  identifying  information  they  share when using LLMs for documentation and notes. As one practitioner explained, 'I'm being careful about how to use whatever I put into a chat, knowing that I don't really know where that information is going' (Participant #14). These individual workarounds reflected both the  absence  of  clear  institutional  standards  and  a  concern  for  maintaining  professional image, which is another social influence factor, as practitioners balance innovation with risk aversion.

Ultimately,  these  reflections  highlighted  the  need  for  clear  professional  guidelines and  institutional  safeguards  for  the  use  of  LLMs,  particularly  with  notes  and  documentation.  As  one  practitioner  underscored  this  concern  about  digital  vulnerability, 'There's  huge  vulnerabilities  now  that  didn't  exist  when  all  of  my  case  files  were  in a  metal  box  in  my  office.  Nobody's  going  to  break  and  steal  my  metal  box.  It's  not worthwhile'  (Participant  #17).  While  LLMs  may  offer  efficiencies,  participants'

continued  caution  shows  that  trust  in  the  system's  data  security,  and  by  extension  its compatibility  with  disciplinary norms ,  remains  limited  until  formal  guardrails  are  in place.  Practitioners  emphasized  that  protecting  sensitive  information  and  maintaining confidentiality  remains  a  primary  concern  when  determining  whether  to  use  these tools.

Bias and Equity. Participants expressed significant concerns about the biases that could be  embedded  within  LLMs  and  questioned  the  extent  to  which  these  technologies  are equitable and inclusive. LLMs are trained on vast datasets drawn from preexisting sources, which led participants to question if these tools could inadvertently reproduce and exacerbate systemic inequities. One practitioner asked, 'Who's teaching the AI, and what biases are  being  embedded?'  (Participant  #18).  Another  added,  'We  know  that  [AI]  can  be inherently racist and biased. And so language models are based on the data information that's given to them. Is it encompassing all viewpoints and of all ethnicities, race, language?' (Participant #1). These concerns reflect skepticism about the output quality of LLMs for certain uses, specifically whether the content they generate can meet the ethical and cultural standards required in social work practice. These concerns also highlighted that the effectiveness and inclusiveness of LLMs is limited and unethical if they fail to reflect the diverse realities and identities of the people social workers serve.

Participants' beliefs on the implications of these biases also extended to broader issues of access  and  equity.  Several  participants  noted  concern  over  the  persistent  'digital  divide' (Participant #1) and questioned who would have access to LLMs. As one participant noted, 'Is this just going to be for wealthy people? . . . If there are positive services, who gets them and how are those being delivered?' (Participant #16). These reflections raised questions not  only  about  distributional  fairness  but  also  about result  demonstrability ,  specifically whether the positive impacts of LLMs can be equitably experienced across communities.

## Quality of care

Impact on therapeutic relationship. A key concern among social workers was that reliance on  LLMs  for  therapy  could  compromise  the  therapeutic  relationship.  Many  therapists worried  that  LLMs  might  never  replicate  the  empathic  attunement  at  the  heart  of  the therapeutic alliance. As one practitioner put it:

Within therapy is just the capacity of having someone who shares, who reflects back with them, empathy for their situation and helps them to create empathy for themselves. So my question is do they feel that empathy from ChatGPT? Do they get all the information . . . accuracy that it's normal grief, how to sit in their grief, how to get used to it, how to do some other things, how to be grateful? A part of what they're also looking for sometimes is a restorative connection with another being who might help them. Just that relationship also helps them. (Participant #15)

Participants noted concern around the potential for LLMs to lack emotional nuance and empathy.  This  limitation  was  perceived  as  a  critical  gap  in output  quality ,  particularly when  compared  to  the  complex  relational  work  at  the  core  of  human  therapy. Participants voiced unease that LLMs might erode the empathic core of psychotherapy. As  Participant  #19  admitted,  'it  is  slightly  terrifying  to  think  of  a  machine  doing  the work that's  done  in  psychotherapy  . . .  that  makes  me  quite  nervous.'  Such  statements reflect broader concerns about maintaining professional image and integrity, particularly in  light  of  growing  automation.  Together,  these  reflections  underscore  participants'

<!-- image -->

concern  that  automating  therapeutic  interactions  risks  losing  the  human  touch  and emotional depth that clients need.

Overall, participants expressed a similar concern that clients would only need to talk with LLMs about their therapeutic concerns and miss the healing potential of the therapeutic relationship with a human:

Concerns? I guess, my concern would be that they might think, Oh, I don't need therapy. I'll just talk to chatGPT.' And then, potentially miss out on the benefits of a human relationship . . . there's a million other benefits of talking to a human who can interact with you in real time. So, I guess my concern would be that people might think that there isn't a place for therapy or it's not needed or it's old fashioned . . . I guess the concern would be that people would think that old fashioned approaches to dealing with the kinds of things that get dealt with in therapy are no longer needed. (Participant #19)

Such concerns reflected a broader fear that, in normalizing AI as a substitute for therapeutic dialogue,  the  deeper  relational  work  essential  to  healing  could  be  undervalued  or  lost altogether.

Misinformation  risks. Practitioners  voiced  concerns  about  the  potential  for  LLMs  to generate inaccurate information, questioning 'who's monitoring it' (Participant #18) and whether the content produced can be trusted. They highlighted there is 'always a risk of misinformation' (Participant #8) and that if clients are relying on LLMs instead of seeking professional support, misinformation could have serious consequences for their well-being. One practitioner captured this concern, cautioning, 'It's concerning that clients might feel worse or be sent down a harmful path' (Participant #19). This blurring of lines between credible guidance and oversimplified or incorrect content raised concerns that LLMs could unintentionally  reinforce  harmful  misconceptions  or  pathologize  normal  behavior  if  not used  carefully.  These  concerns  speak  directly  to  the  unpredictability  of output  quality , particularly when LLMs are used outside professional oversight.

Participants also voiced concern about the larger data sets training LLMs. They questioned whether commercial influence and user manipulation might compromise the integrity of LLM outputs. As one practitioner asked,

If something like that starts to ruin a crowdsourced experience like Reddit, what does that do with  ChatGPT? Does it get  influenced  by  the  wrong  information?  Do  corporations  start  to come in and like, 'Hey, you should mention Tide as the perfect detergent in more conversations?.' (Participant #12)

Practitioners reflected a growing unease about misinformation and manipulation in LLM tools  for  clients  seeking  help.  Their  sentiments  reflected  that  if  not  carefully  monitored, LLM generated content could contain inaccurate information and reflect the agendas of powerful  influencing  parties  rather  than  serving  as  neutral,  evidence-based  support  for clients and clinicians.

## Workforce implications

Threat of job replacement. Practitioners  voiced  serious  'concern  about  artificial  intelligence  in  the  therapy  business'  (Participant  #17),  particularly  regarding  the  perceived threat  of  being  replaced.  Some  practitioners worried that they could 'become obsolete' (Participant  #3).  One  practitioner  captured  this,  stating,  'I  could  also  see  clients  not

coming  to  therapy  anymore,  just  using  the  AI.  As  we  see  the  robots  taking  over  the world,  there's  also  the  idea  that  they'll  take  over  the  therapy  too'  (Participant  #20). Others expressed fears that LLMs could be utilized as a cost-cutting measure to reduce the  role  of  human  practitioners  in  favor  of  automated  services.  As  one  participant cautioned,

Yeah.  I  mean,  I  think  it's  kind  of  scary  as  to  what  could  happen.  I  would  think  that  these therapy, these Better Helps and all the places, along with AI, are hurting our profession. And that there's that relationship that you build with a client, that it's becoming more factory-run, with the telehealth stuff and AI. (Participant #11)

These  concerns  reflect  a  tension  between  economic  pressures  to  adopt  LLMs  and  the professional subjective  norm that  values  human  connection  as  core  to  therapeutic  work. Several participants raised concerns about the loss of essential relational elements when inperson human interactions are replaced with digital interfaces.  Practitioners  emphasized the importance of the therapeutic relationship, noting that the depth of these relationships cannot be replicated by machines. As one practitioner highlighted,

There's a dynamic in therapy, and one of my clinical supervisors taught me best, when you have a client come in, and you suddenly are feeling very different than you did before they walked in, you're probably picking up on their feelings, and it's a great thing to explore. ChatGPT can't do that, right? That human to human interaction that can occur in therapy, that's lost, and there's value in that. (Participant #10)

This perceived irreplaceability of relational work points to what many clinical social workers  saw  as  the  limitations  of  current output quality ,  particularly  in  replicating  emotional nuance  and  embodied  presence.  Another  practitioner  explained,  'I  think  taking  analog interactions  between  human  beings  and  piping  it  through  electronic  media  and  digital media removes content that is missed, and I have a concern about that' (Participant #14). These reflections highlight how concerns about relational loss, emotional nuance, and the limits  of  human  connection  significantly  diminished  practitioners'  perceptions  of  LLMs' usefulness in clinical social work.

## Discussion

This study explored clinical social workers' reflections on the perceived usefulness of LLMs within  clinical  social  work  practice.  Our  findings  suggest  that  practitioners  are  simultaneously embracing the productive potential of LLMs while expressing concerns about their impact  on  therapeutic  relationships,  professional  discretion,  and  client  wellbeing.  From a technology acceptance framework, participant reflections indicate that patterns of LLM acceptance do not rest solely on technical dimensions such as job relevance, ease of use and output quality. Clinicians are also heavily influenced by values-based decisions around the extent to which LLMs promote or inhibit human connection and ethical practice.

While  many  of  the  perceived  benefits  of  LLMs  align  with  traditional  TAM constructs  such  as  job  relevance,  ease  of  use,  and  output  quality,  social  work practitioners  also  filtered  these  evaluations  through  the  lens  of  professional  ethics and  relational  values.  Specifically,  participant  reflections  revealed  that  their  willingness  to  adopt  LLMs  was  not  only  shaped  by  the  tools'  efficiency  or  technical capabilities  but  also  by  their  perceived  alignment  with  the  humanistic  commitments

<!-- image -->

of  the  profession.  Values  such  as  empathy,  client  autonomy,  cultural  responsiveness, and  ethical  accountability  influenced  how  participants  assessed  and  engaged  with these  technologies.  These  responses  suggest  that  social  influence  components,  such as  institutional  norms  or  peer  use,  may  not  fully  account  for  the  cautious,  valuedriven  stance  that  many  practitioners  expressed.

Rather  than  simply  conforming  to  what  colleagues  might  endorse,  practitioners appeared to rely on deeper internalized standards rooted in the ethics and culture of the  profession.  Even  when  LLMs  demonstrated  high  output  quality  or  increased efficiency,  practitioners  questioned  their  appropriateness  if  the  tools  appeared  to compromise  relational  depth  or  client  safety.  These  findings  suggest  that  full  integration  of  LLMs  into  clinical  social  work  practice  or  formal  endorsement  of  LLMbased  chatbots  for  therapeutic  use  appears  unlikely  unless  that  ethical  congruence  is achieved  through  further  technological  development.

At  the  same  time,  findings  point  to  the  possibility  of  more  targeted,  ethically  aligned applications  of  LLMs  that  enhance  rather  than  supplant  relational  practice.  Participants were most optimistic about uses that complemented rather than replaced human judgment and  connection,  such  as  brainstorming,  psychoeducation,  documentation  support,  and supervision.  In  these  domains,  LLMs  were  framed  less  as  autonomous  decision-makers and  more  as  'sounding  boards'  or  cognitive  partners,  helping  practitioners  clarify  their thinking or structure their work.

These patterns suggest a preference for augmentation over automation, a vision of AI as  a  tool  that  strengthens  professional  insight  rather  than  substitutes  for  it.  Crucially, this  orientation  preserves  the  primacy  of  the  therapeutic  relationship  while  allowing LLMs to offer  practical  support  in  other  areas.  Such  uses  may  align  more  closely  with the  profession's  values,  provided  that  safeguards  for  data  privacy,  cultural  responsiveness,  and  ethical  integrity  are  in  place.  This  nuanced  stance  reflects  a  form  of  conditional  openness:  social  workers  are  not  wholesale  rejecting  LLMs  but  are  instead selectively  integrating  them  in  ways  that  align  with  professional  priorities  and  clientcentered care.

The recent passage of House Bill 1806 by the Illinois legislature provides a policy example that reflects the concerns around LLM use identified in our study. The bill restricts AI use by licensed professionals and bars unlicensed entities, including unregulated AI systems, from providing therapy services. The legislation also prohibits the use of AI for engaging in direct therapeutic communication with clients, making independent therapeutic decisions, or interpreting clients' emotions. AI-generated treatment plans may only be used once they have been reviewed and approved by a licensed professional. Under the bill, AI may be used to assist licensed professionals with administrative tasks, such as scheduling and billing, and with support functions like notetaking and data analysis. Licensed providers must maintain responsibility  for  all  AI  outputs  and  secure  informed  consent  when  AI  involves  session recordings or transcription. While the legislation may be interpreted by some clinicians as a blanket restriction on AI, it is more accurately understood as an effort to promote ethical LLM integration in mental health services, regulating standalone LLM-based therapy apps while preserving space for the supervised use of LLMs by licensed professionals. This overall approach  closely  mirrors  participants'  concerns  about  the  limitations  of  AI  in  clinical relationships  and  their  conditional  openness  to  its  use  in  administrative  and  ideagenerating capacities that assist, rather than replace, human judgment.

## Limitations

This  study  attempts  to  understand  how  social  workers  in  private  practice  are  adopting LLM technologies. Our goal was to understand how they are adopting these new tools in practice.  Like  all  research,  this  study  has  several  limitations.  First,  the  data  represents a single point in time during a rapidly evolving technological landscape of 2024. New AI tools,  regulatory  frameworks,  and  platform  capabilities  are  being  introduced  at  a  fast pace,  and  some  developments  may  have  emerged  after  data  collection  was  completed. Additionally, we focus here on social workers in private practice, who often have greater autonomy  in  selecting,  experimenting  and  implementing  new  technologies.  Agencybased  practitioners,  particularly  those  working  in  government  or  nonprofit  settings, may face more constraints  related  to  procurement  policies,  HIPAA  compliance,  supervisory  oversight,  and  institutional  risk  tolerance  (Ranerup  &amp;  Henriksen,  2022).  The insights  presented  here  may  not  fully  reflect  the  broader  profession's  engagement  with LLMs.

Lastly,  our  findings  offer  early  insight  into  how  LLMs  are  being  used  in  social  work practice. These impressions should not substitute for rigorous evaluations of ethical risks, equity concerns, or unintended consequences at scale. Further research is needed to assess how these tools affect therapeutic relationships, professionals' decision-making processes, and client outcomes in diverse settings (Benjamin, 2019; Eubanks, 2018). Nonetheless, we believe the perspectives offered by this sample of participants provide insight into the kinds of  professional  development,  ethical  guidelines,  and  continuing  education  that  may  be necessary to evaluate and integrate LLMs responsibly in clinical social work. Their experiences highlight an urgent need for social work education to address digital competency, critical algorithmic literacy, and the broader sociotechnical context of AI tools (Ahn et al., 2025; Parrott &amp; Madoc-Jones, 2008; Rodriguez et al., 2024).

Finally, the Technology Acceptance Model may not fully capture the ethical dimensions central to clinical social work practice (Holden &amp; Karsh, 2010). In this field, decisions about adopting new technologies are influenced as much by professional values such as empathy, client autonomy, and ethical accountability as by perceptions of usefulness and ease of use.

## Conclusion

Clinical social workers' perceptions of LLMs reflect both cautious interest and substantial concern, shaped as much by ethical commitments as by technical capabilities. While LLMs offer  clear  advantages  in  efficiency  and  support,  their  acceptance  remains  contingent  on alignment with the relational and values-driven foundations of the profession. Although traditional  TAM  constructs  remain  relevant  for  understanding  clinical  social  workers' perceptions of emerging technologies' usefulness, a comprehensive assessment must also account  for  factors  like  empathy,  trust,  and  data  security.  As  LLMs  evolve,  their  role  in clinical practice is likely to hinge not just on what they can accomplish, but on how well they embody the values that define the field.

## Acknowledgments

We would like to thank our research assistant that helped with interviews, Camille Dysart, MSW.

<!-- image -->

## Disclosure statement

No potential conflict of interest was reported by the author(s).

## Funding

The author(s) reported there is no funding associated with the work featured in this article.

## ORCID

Johanna Creswell Báez

http://orcid.org/0000-0003-3217-5502

Eunhye Ahn http://orcid.org/0000-0003-3777-4213

Bryan G. Victor

http://orcid.org/0000-0002-2092-912X

Lauri Goldkind

http://orcid.org/0000-0002-0967-3960

## References

- Ahn, E., Choi, M., Fowler, P., &amp; Song, I. H. (2025). Artificial intelligence (AI) literacy for social work: Implications  for  core  competencies. Journal  of  the  Society  for  Social  Work  and  Research , 16 (1), 9-26. https://doi.org/10.1086/735187
- Ali,  A.,  Kumar, R. P., Polavarapu, H., Lavadi, R. S., Mahavadi, A., Legarreta, A. D., Hudson, J. S., Shah,  M.,  Paul,  D.,  Mooney,  J.,  Dietz,  N.,  Fields,  D.  P.,  Hamilton,  D.  K.,  Agarwal,  N.  (2024). Bridging  the  gap:  Can  large  language  models  match  human  expertise  in  writing  neurosurgical operative notes? World Neurosurgery , 192 , e34-e41. https://doi.org/10.1016/j.wneu.2024.08.062
- AlQudah,  A.  A.,  Al-Emran,  M.,  &amp;  Shaalan,  K.  (2021).  Technology  acceptance  in  healthcare: A systematic review. Applied Sciences , 11 (22), 10537. https://doi.org/10.3390/app112210537
- Babayiğit, O., Eroglu, Z. T., Sen, D. O., &amp; Yarkac, F. U. (2023). Potential use of ChatGPT for patient information in periodontology: A descriptive pilot study. Cureus , 15 (11), e48518. https://doi.org/ 10.7759/cureus.48518
- Báez, J. C., Victor, B. G., Dysart, C., &amp; Goldkind, L. (2025). 'I don't understand it, but okay': An empirical study of mental health practitioners' readiness to use large language models. Journal of Technology in Human Services , 1-19. https://doi.org/10.1080/15228835.2025.2479475
- Barash, Y., Klang, E., Konen, E., Sorin, V. (2023). ChatGPT-4 assistance in optimizing emergency department radiology referrals and imaging selection. Journal of the American College of Radiology , 20 (10), 998-1003. https://doi.org/10.1016/j.jacr.2023.06.009
- Barrera-Algarín, E., Sarasola-Sánchez-Serrano, J. L., &amp; Sarasola-Fernández, A. (2023). Social work in the face of emerging technologies: A technological acceptance study in 13 countries. International Social Work , 66 (4), 1149-1166. https://doi.org/10.1177/00208728211041672
- Bedi,  S.,  Liu,  Y.,  Orr-Ewing,  L.,  Dash,  D.,  Koyejo,  S.,  Callahan,  A.,  Fries,  J.  A.,  Wornow,  M., Swaminathan,  A.,  Lehmann,  L.  S.,  Hong,  H.  J.,  Kashyap,  M.,  Chaurasia,  A.  R.,  Shah,  N.  R., Singh, K., Tazbaz, T., Milstein, A., Pfeffer, M. A., &amp; Shah, N. H. (2024). A systematic review of testing  and  evaluation  of  healthcare  applications  of  large  language  models  (LLMs). MedRxiv . https://doi.org/10.1101/2024.04.15.24305869
- Benda, N., Desai, P., Reza, Z., Zheng, A., Kumar, S., Harkins, S., Hermann, A., Zhang, Y., Joly, R., Kim, J., Pathak, J., &amp; Reading Turchioe, M. (2024). Patient perspectives on AI for mental health care: Cross-sectional survey study. JMIR Mental Health , 11 , e58462. https://doi.org/10.2196/58462 Benjamin, R. (2019). Race after technology: abolitionist tools for the new Jim code . Polity Press.
- Berrezueta-Guzman, S., Kandil, M., &amp; Wagner, S. (2025). Integrating AI into ADHD therapy: Insights from ChatGPT-4o and robotic assistants. Human-Centric Intelligent Systems , 5 (2), 1-16. https:// doi.org/10.1007/s44230-025-00099-1

<!-- image -->

- Braun, V., Clarke, V. (2021). To saturate or not to saturate? Questioning data saturation as a useful concept for thematic analysis and sample-size rationales. Qualitative Research in Sport, Exercise and Health , 13 (2), 201-216. https://doi.org/10.1080/2159676X.2019.1704846
- Braun,  V.,  &amp;  Clarke,  V.  (2022). Thematic analysis: a  practical  guide .  Sage  Publications.  https://us. sagepub.com/en-us/nam/thematic-analysis/book248481
- Braun, V., Clarke, V. (2023). Is thematic analysis used well in health psychology? A critical review of published research, with recommendations for quality practice and reporting. Health Psychology Review , 17 (4), 695-718. https://doi.org/10.1080/17437199.2022.2161594
- Braun,  V.,  Clarke,  V.  (2024).  Supporting  best  practice  in  reflexive  thematic  analysis  reporting  in palliative  medicine:  A  review  of  published  research  and  introduction  to  the  reflexive  thematic analysis  reporting  guidelines  (RTARG). Palliative  Medicine , 38 (6),  608-616.  https://doi.org/10. 1177/02692163241234800
- Braun,  V.,  &amp;  Clarke,  V.  (2025).  Reporting  guidelines  for  qualitative  research:  A  values-based approach. Qualitative  Research in  Psychology , 22 (2),  399-438.  https://doi.org/10.1080/14780887. 2024.2382244
- Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,  G.,  Askell,  A.,  Agarwal,  S.,  Herbert-Voss,  A.,  Krueger,  G.,  Henighan,  T.,  Child,  R., Ramesh, A., Ziegler,  D.  M.,  Wu,  J.,  Winter,  C.  ,  and  Amodei,  D.  (2020).  Language  models  are few-shot  learners. Proceedings  of  the  34th  International  Conference  on  Neural  Information Processing  Systems ,  Vancouver,  Canada  (pp.  1877-1901).  https://proceedings.neurips.cc/paper\_ files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf
- Bundy,  H.,  Gerhart,  J.,  Baek,  S.,  Connor,  C.  D.,  Isreal,  M.,  Dharod,  A.,  Stephens,  C.,  Liu,  T.  L., Heatherington, T., &amp; Cleveland, J. (2024). Can the administrative loads of physicians be alleviated by AI-facilitated clinical documentation? Journal of General Internal Medicine , 39 (15), 2995-3000. https://doi.org/10.1007/s11606-024-08870-z
- Dağci,  M.,  Çam,  F.,  Dost,  A.  (2024).  Reliability  and  quality  of  the  nursing  care  planning  texts generated  by  ChatGPT. Nurse  Educator , 49 (3),  E109-E114.  https://doi.org/10.1097/NNE. 0000000000001566
- Davis,  F.  D.,  Bagozzi,  R.  P.,  Warshaw,  P.  R.  (1989).  User  acceptance  of  computer  technology: A comparison of two theoretical models. Management Science , 35 (8), 982-1003. https://doi.org/ 10.1287/mnsc.35.8.982
- Dedoose. (2024). Dedoose (version 9.0) [Computer software]. SocioCultural Research Consultants. https://www.dedoose.com ,
- Eubanks, V. (2018). Automating inequality: how high-tech tools profile, police, and punish the poor . St. Martin's Press.
- Goldkind, L., Wolf, L., Jones, J. (2016). Late adapters? How social workers acquire knowledge and skills about technology tools. Journal of Technology in Human Services , 34 (4), 338-358. https://doi. org/10.1080/15228835.2016.1250027
- Greener, S. (2022). Digging for acceptance theory. Interactive Learning Environments , 30 (4), 587-588. https://doi.org/10.1080/10494820.2022.2062170
- Holden, R. J., Karsh, B. T. (2010). The technology acceptance model: Its past and its future in health care. Journal of Biomedical Informatics , 43 (1), 159-172. https://doi.org/10.1016/j.jbi.2009.07.002
- Hua, Y., Na, H., Li, Z.,  Liu,  F.,  Fang,  X.,  Clifton,  D.,  Torous,  J.  (2025).  A  scoping  review  of  large language models for generative tasks in mental health care. NPJ Digital Medicine , 8 (1), 230. https:// doi.org/10.1038/s41746-025-01611-4
- Kassab, J., El Hajjar, A. H., Wardrop, R. M., III, &amp; Brateanu, A. (2024). Accuracy of online artificial intelligence models in primary care settings. The American Journal of Preventive Medicine , 66 (6), 1054-1059. https://doi.org/10.1016/j.amepre.2024.02.006
- Kourgiantakis, T., Sewell, K. M., &amp; Bogo, M. (2019). The importance of feedback in preparing social work students for field education. Clinical Social Work Journal , 47 (1), 124-133. https://doi.org/10. 1007/s10615-018-0671-8
- Kuhail,  M.  A.,  Alturki,  N.,  Thomas,  J.,  Alkhalifa,  A.  K.,  Alshardan,  A.  (2025).  Human-human  vs human-AI therapy: An empirical study. International Journal of Human-Computer Interaction , 41 (11), 6841-6852. https://doi.org/10.1080/10447318.2024.2385001

<!-- image -->

- Lai, C. Y., He, L., &amp; Lau, S. M. (2024). The precarious helping hand-a systematic review of precarity faced by social workers. https://doi.org/10.21203/rs.3.rs-5399479/v1
- Limpanopparat, S., Gibson, E., &amp; Harris, A. (2024). User engagement, attitudes, and the effectiveness of chatbots as a mental health intervention: A systematic review. Computers in Human Behavior Artificial Humans , 2 (2), 100081. https://doi.org/10.1016/j.chbah.2024.100081
- Lincoln, Y. S., &amp; Guba, E. G. (1985). Naturalistic inquiry . Sage.
- Liu, J., Li, D., Cao, H., Ren, T., Liao, Z., &amp; Wu, J. (2023). ChatCounselor: A large language models for mental health support. ArXiv. https://doi.org/10.48550/arXiv.2309.15461
- Liu,  T.  L.,  Hetherington,  T.  C.,  Dharod,  A.,  Carroll,  T.,  Bundy,  R.,  Nguyen,  H.,  Bundy,  H.  E., Isreal,  M.,  McWilliams, A., &amp; Cleveland, J. A. (2024). Does AI-powered clinical documentation enhance clinician efficiency? A longitudinal study. NEJM , 1 (12), AIoa2400275. https://doi.org/10. 1056/AIoa2400659
- Ma, Z., Mei, Y., Long, Y., Su, Z., &amp; Gajos, K. Z. (2024). Evaluating the experience of LGBTQ+ people using large language model based chatbots for mental health support. Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems , Honolulu, Hawaii (pp. 1-15). https://doi.org/ 10.1145/3613904.3642482
- Macrynikola, N., Nguyen, N., Lane, E., Yen, S., &amp; Torous, J. (2023). The digital clinic: An innovative mental  health  care  delivery  model  utilizing  hybrid  synchronous  and  asynchronous  treatment. NEJM Catalyst Innovations in Care Delivery , 4 (9). https://doi.org/10.1056/CAT.23.0100
- Mastour, H., Yousefi, R., &amp; Niroumand, S. (2025). Exploring the acceptance of e-Learning in health professions education in Iran based on the technology acceptance model (TAM). Scientific Reports , 15 (1), 8178. https://doi.org/10.1038/s41598-025-90742-5
- McDuff, D., Schaekermann, M., Tu, T., Palepu, A., Wang, A., Garrison, J., Singhal, K., Sharma, Y., Azizi, S., Kulkarni, K., Gottweis, H., Karthikesalingam, A., Webster, D., Corrado, G. S., Matias, Y., Kohane,  I.,  Liu,  Y.,  Patel,  S.Hammel,  N.  . . .  Karthikesalingam,  A.  (2025).  Towards  accurate differential diagnosis with large language models. Nature . 642 (8067), 451-457. Advanced online publication. https://doi.org/10.1038/s41586-025-08869-4
- McNally, K., Wright, K., Goldkind, L., Kattari, S. K., &amp; Victor, B. G. (2024). Disability expertise and large  language  models:  A  qualitative  study  of  autistic  TikTok  creators'  use  of  ChatGPT. Social Media + Society , 10 (3), 1-10. https://doi.org/10.1177/20563051241279549
- Meinhardt-Injac, B., Späte, J., Siemer, L., &amp; Witter, S. (2025). Acceptance of ChatGPT in social work students. Forum for Education Studies , 3 (1), 1812. https://doi.org/10.59400/fes1812
- Morgan,  D.  L.  (2014).  Pragmatism  as  a  paradigm  for  social  research. Qualitative  Inquiry , 20 (8), 1045-1053. https://doi.org/10.1177/1077800413513733
- Parrott,  L.,  Madoc-Jones,  I.  (2008).  Reclaiming  information  and  communication  technologies  for empowering social work practice. Journal of Social Work , 8 (2), 181-197. https://doi.org/10.1177/ 1468017307084739
- Perlis,  R.  H.,  Goldberg,  J.  F.,  Ostacher,  M.  J.,  Schneck,  C.  D.  (2024).  Clinical  decision  support  for bipolar  depression  using  large  language  models. Neuropsychopharmacology  Reviews , 49 (9), 1412-1416. https://doi.org/10.1038/s41386-024-01841-2
- Raile, P. (2024). The usefulness of ChatGPT for psychotherapists and patients. Humanities and Social Sciences Communications , 11 (47), 1-8. https://doi.org/10.1057/s41599-023-02567-0
- Ranerup, A., Henriksen, H. Z. (2022). Digital discretion: Unpacking human and technological agency in automated decision making in Sweden's social services. Social Science Computer Review , 40 (2), 445-461. https://doi.org/10.1177/0894439320980434
- Rodriguez, M. Y., Goldkind, L., Victor, B. G., Hiltz, B., Perron, B. E. (2024). Introducing generative artificial intelligence into the MSW curriculum: A proposal for the 2029 educational policy and accreditation standards. Journal of Social Work Education , 60 (2), 174-182. https://doi.org/10.1080/ 10437797.2024.2340931
- Schueller, S. M., Morris, R. R. (2023). Clinical science and practice in the age of large language models and  generative  artificial  intelligence. Journal  of  Consulting  and  Clinical  Psychology , 91 (10), 559-561. https://doi.org/10.1037/ccp0000848

<!-- image -->

- Sharma, A., Lin, I. W., Miner, A. S., Atkins, D. C., &amp; Althoff, T. (2023). Human-AI collaboration enables  more  empathic  conversations  in  text-based  peer-to-peer  mental  health  support. Nature Machine Intelligence , 5 (1), 46-57. https://doi.org/10.1038/s42256-022-00593-2
- Spiegel, B. M. R., Liran, O., Clark, A., Samaan, J. S., Khalil, C., Chernoff, R., Reddy, K., Mehra, M. (2024). Feasibility of combining spatial computing and AI for mental health support in anxiety and depression. NPJ Digital Medicine , 7 (1), 22. https://doi.org/10.1038/s41746-024-01011-0
- Stade,  E.  C.,  Stirman,  S.  W.,  Ungar,  L.  H.,  Boland,  C.  L.,  Schwartz,  H.  A.,  Yaden,  D.  B.,  &amp; Eichstaedt, J. C. (2024). Large language models could change the future of behavioral healthcare: A proposal for responsible  development  and  evaluation. NPJ Mental Health Research , 3 (1),  12. https://doi.org/10.1038/s44184-024-00056-z
- Venkatesh, V., Davis, F. D. (2000). A theoretical extension of the technology acceptance model: Four longitudinal field studies. Management Science , 46 (2), 186-204. https://doi.org/10.1287/mnsc.46.2. 186.11926
- Victor, B. G., &amp; Goldkind, L. (2025). The therapist in the machine: Confronting AI's challenge to clinical  social  work. Journal  of  Technology  in  Human  Services ,  1-9.  https://doi.org/10.1080/ 15228835.2025.2500827
- Vowels,  L.  M.,  Francois-Walcott,  R.  R.  R.,  &amp;  Darwiche,  J.  (2024).  AI  in  relationship  counselling: Evaluating  ChatGPT's  therapeutic  capabilities  in  providing  relationship  advice. Computers  in Human Behavior Artificial Humans , 2 (2), 100078. https://doi.org/10.1016/j.chbah.2024.100078
- Wassal, K., Ashurst, C., Hron, J., &amp; Zilka, M. (2024). Reimagining AI in social work: Practitioner perspectives on incorporating technology in their practice. arXiv. https://doi.org/10.48550/arXiv. 2407.10244
- Wilhelm, T. I., Roos, J., Kaczmarczyk, R. (2023). Large language models for therapy recommendations  across  3  clinical  specialties:  Comparative  study. Journal  of  Medical  Internet  Research , 25 , e49324. https://doi.org/10.2196/49324
- Zao-Sanders,  M.  (2025,  April  9).  How  people  are  really  using  gen  AI  in  2025. Harvard  Business Review . https://hbr.org/2025/04/how-people-are-really-using-gen-ai-in-2025
- Zhang, W., Gutierrez, O. (2007, July). Information technology acceptance in the social services sector context: An exploration. Social Work , 52 (3), 221-231. https://doi.org/10.1093/sw/52.3.221
- Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X.Liu, Z. . . . Wen, J. R. (2023). A survey of large language models. arXiv. https://doi.org/10.48550/arXiv.2303.18223
- Zorn, T. E., Flanagin, A. J., Shoham, M. D. (2011). Institutional and noninstitutional influences on information  and  communication  technology  adoption  and  use  among  nonprofit  organizations. Human Communication Research , 37 (1), 1-33. https://doi.org/10.1111/j.1468-2958.2010.01387.x
- Zulfikar,  W.,  Chiaravalloti,  T.,  Shen,  J.,  Picard,  R.,  &amp;  Maes,  P.  (2025).  Resonance:  Drawing  from memories to imagine positive futures through AI-augmented journaling. arXiv. https://doi.org/10. 48550/arXiv.2503.24145