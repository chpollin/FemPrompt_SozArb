---
source_file: Benlian_2025_The_AI_literacy_development_canvas_Assessing_and.pdf
conversion_date: 2026-02-03T08:43:15.890701
converter: docling
quality_score: 95
---

## Journal Pre-proof

The AI literacy development canvas: Assessing and building AI literacy in organizations

Alexander Benlian, Marc Pinski

PII:

S0007-6813(25)00167-3

DOI:

https://doi.org/10.1016/j.bushor.2025.10.001

Reference:

BUSHOR 2100

To appear in:

Business Horizons

Please cite this article as: Benlian A. &amp; Pinski M., The AI literacy development canvas: Assessing and building AI literacy in organizations, Business Horizons , https://doi.org/10.1016/j.bushor.2025.10.001.

This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, and formatting for readability, but it is not yet the definitive version of record. This version will undergo additional copyediting, typesetting and review before it is published in its final form, but we are providing this version to give early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.

© 2025 Kelley School of Business, Indiana University. Published by Elsevier Inc. All rights are reserved, including those for text and data mining, AI training, and similar technologies.

<!-- image -->

## The AI literacy development canvas: Assessing and building AI literacy in organizations

Alexander Benlian * Technical University of Darmstadt Hochschulstr. 1 64287 Darmstadt Germany benlian@ise.tu-darmstadt.de

Marc Pinski Boston Consulting Group Bockenheimer Landstraße 47 60325 Frankfurt am Main pinski.marc@bcg.com * Corresponding author Journal Pre-proof

## ABSTRACT

As artificial intelligence (AI) technologies become increasingly integral to business operations, organizations must ensure their workforce possesses the necessary AI literacy to harness these advancements effectively. AI literacy, encompassing conceptual, ethical, and practical competencies, is essential for employees at all levels-executives, middle managers, and non-IT staff-to make informed decisions, integrate AI into workflows, and navigate ethical considerations. This article presents a structured framework for assessing and developing AI literacy within organizations. First, we introduce an AI Literacy Assessment Matrix tailored to different managerial levels, enabling firms to evaluate AI-related competencies across the workforce. Second, we propose the AI Literacy Development Canvas, a strategic planning tool, guiding organizations on when, where, and how to invest in AI training and capability building. An illustrative case study in the pharmaceutical sector showcases the practical application of these tools and offers key lessons learned. The article contributes to academic research by advancing the understanding of AI literacy as a multidimensional organizational capability. For practitioners, it offers a conceptual and practical toolset that supports managers in the systematic assessment and strategic development of AI literacy upskilling initiatives across all levels of the organization. KEYWORDS : AI literacy; Workforce capability development; Upskilling; Digital transformation Journal Pre-proof

## The AI literacy development canvas: Assessing and building AI literacy in organizations

## 1. The case for AI literacy development in organizations

Artificial Intelligence (AI) is no longer a futuristic ambition-it is fundamentally reshaping industries today. Yet most organizations are alarmingly unprepared to harness its power or manage its risks. Consider the recent lawsuit against Workday, where its AI-based applicant recommendation system was alleged to have perpetuated race, age, and disability discrimination-claims severe enough for a court to certify a collective action alleging algorithmic bias (Brenner et al., 2025). Incidents like these expose a deeper, systemic challenge facing organizations today: a lack of workforce preparedness to engage with AI responsibly, productively, and compliantly. This capability gap, rooted in insufficient AI literacy, is fast becoming a critical barrier to effective and ethical AI integration.

At its core, AI literacy refers to the cognitive and behavioral capabilities that enable individuals at all organizational levels to critically understand, evaluate, and interact with AI systems in ways that align with business goals and ethical standards (Ng et al., 2021). From strategic decision-making to everyday operations, every role in the organization-from the C-suite to the front line-requires a tailored form of AI literacy to engage with AI-powered tools effectively. Industry studies underscore this urgency: 86% of business leaders call for more training in responsible AI use, yet over half say their organizations fall short in educating staff on AI ethics (StiboSystems, 2024). Unlike earlier digital literacy, which focused on using tools and information, AI literacy involves engaging with opaque, adaptive, and often autonomous systems. It requires understanding algorithmic reasoning under uncertainty, navigating ethical dilemmas, and managing compliance risks. With the EU AI Act imposing fines of up to 7% of global revenue for violations involving high-risk AI systems (Sunyaev et al., 2025), AI literacy is now a non-negotiable strategic, ethical, and legal priority for organizations. Journal Pre-proof

Yet despite this urgency, most organizations remain ill-equipped to act. There is a widening gap between AI ambition and execution, fueled not by technology limitations, but by human capability shortfalls. Studies show firms may see returns of $3.70 for every $1 invested in AI (Microsoft, 2025)-but only when teams know how to apply AI productively. Without that literacy, AI projects stall, misuse proliferates, and opportunities are missed (Rajaram &amp; Tinguely, 2024). Business transformation remains elusive not because of a lack of tools, but because employees lack the confidence, insight, or ethical awareness to deploy them effectively (Holmström, 2022). Companies that proactively develop AI literacy are positioning their people to work alongside intelligent machines effectively, which is increasingly key to productivity and growth (Tursunbayeva &amp; Chalutz-Ben Gal, 2024).

Despite growing attention to AI literacy in research and regulations (EU AI Act, 2024; Laupichler et al., 2022), practical frameworks for assessing and building AI competencies within organizations remain scarce. While existing apporaches to define AI literacy span digital literacy and ethical perspectives, they are typically geared toward technical (e.g., developers) or educational (e.g., students) user groups (Almatrafi et al., 2024; Long &amp; Magerko, 2020; Stolpe &amp; Hallström, 2024). What is missing are frameworks that support organizations in assessing and developing AI competencies among different workforce populations and align training with strategy. Yet addressing this gap is crucial: organizations require a systematic approach for identifying AI literacy needs and developing targeted upskilling programs (Retkowsky et al.,

2024). Without such guidance, AI adoption risks remaining fragmented and ineffective, undermining its potential to sustain value creation.

In this article, we aim to offer organizations a clear, actionable roadmap to transform AI literacy from a recognized gap into an actionable capability. Drawing on academic research and corporate insights, we introduce two complementary tools: the AI Literacy Assessment Matrix and the AI Literacy Development Canvas. These frameworks help diagnose AI skill gaps, align training initiatives with business strategy, and implement AI literacy at scale across different organizational layers. As the speed of AI innovation increasingly outpaces organizational readiness, our approach addresses the pressing need for adaptive, scalable, and role-specific AI literacy tools that bridge strategic vision with operational action (Pinski, Tarafdar, et al., 2024).

The article proceeds as follows: Section 2 examines AI literacy across executives, middle managers, and non-IT employees, detailing their distinct competency needs for AI adoption. Section 3 introduces the AI Literacy Assessment Matrix, a framework for evaluating AI competencies within organizations. Section 4 presents the AI Literacy Development Canvas, which serves as a strategic tool for guiding AI capability investments. Section 5 explores the real-world application of these tools through a pharmaceutical industry case study. Finally, section 6 concludes with practical and research implications. 2. AI literacy needs across organizatonal levels AI literacy is not one-size-fits-all; different organizational roles have distinct learning needs. In successful organizations, AI literacy is cultivated at all levels, from entry-level staff to the CEO, ensuring that each group develops the necessary capabilities to drive AI-powered transformation effectively. A useful way to frame this is by considering the spectrum from top management to middle management to front-line employees. While AI literacy is needed across all levels, its scope and emphasis differ based on the nature of one's decisions and tasks (Relyea et al., 2024). Journal Pre-proof

For executives and top managers, AI literacy is essential to make informed strategic decisions, evaluate the business (model) implications of AI projects, and anticipate and mitigate potential risks. While they do not need to code, they must understand AI's business potential, limitations, and ethical considerations (Bartsch, Nguyen, et al., 2025). This includes identifying where AI can drive value, such as in customer personalization or supply chain optimization, and assessing AI investments in terms of feasibility and return on investment. AI-literate leaders also need to establish governance structures to ensure responsible AI use, considering risks like bias, transparency, and regulatory compliance. Many forward-thinking organizations are proactively training senior leaders, with some executives attending formal AI courses at academic institutions. This reflects a growing recognition that in an AI-driven world, maintaining AI literacy at the executive level is an ongoing requirement rather than a one-time initiative.

Middle managers play a pivotal role in AI adoption by translating high-level strategy into daily operations. Their AI literacy should emphasize operational integration, team leadership, and data-driven decision-making (Koponen et al., 2025). They need a solid grasp of AI tools relevant to their functions, such as sales analytics, recruitment screening, or predictive maintenance. Just as Toyota trained middle managers extensively in lean production techniques in the 1980s,

today's middle managers must develop AI literacy to guide their teams through digital transformation. They also serve as AI advocates, coaching employees on AI tools, addressing concerns about job displacement, and promoting a culture of responsible AI use. Even when senior leaders champion AI, a gap at the middle-management level can stall momentum. In otherwise AI-ready firms, 'AI-skeptic' middle managers can halve the number of AI initiatives that succeed (Malmsheimer, 2024). Therefore, companies should treat middle managers as a priority group for AI upskilling.

Non-IT employees (General Staff), the broad base of employees from analysts and marketers to customer service reps and beyond, also need AI literacy, albeit with a very practical, taskoriented emphasis (Cetindamar et al., 2024). They increasingly rely on AI tools in their daily work and require practical AI literacy to use them effectively and ethically. This includes knowing how to interact with AI-driven applications, such as generative AI for drafting reports or AI-enhanced customer service platforms, and critically evaluating AI-generated outputs (Pinski &amp; Benlian, 2024). Employees should be trained to recognize AI limitations, such as bias in automated decision-making, and understand when human oversight is necessary. A recent LinkedIn survey found that while 61% of desk workers want AI training, less than half of companies have begun formal education efforts, highlighting an urgent skills gap (Walsh, 2024). Organizations like IKEA and Mastercard are already implementing company-wide AI literacy programs, offering AI fundamentals training and ethical AI modules to ensure employees understand responsible AI use. Companies should provide accessible training formats, AI support forums, and role-specific learning to empower employees to work alongside AI confidently and responsibly (Rajaram &amp; Tinguely, 2024). 3. The AI literacy assessment matrix for organizations Journal Pre-proof

To systematically develop AI literacy in the workforce, organizations need a clear and actionable understanding of what AI literacy entails. However, existing research offers a range of perspectives. Some studies define AI literacy narrowly as technical proficiency, emphasizing programming skills or the ability to build AI systems (e.g., Stolpe &amp; Hallström, 2024)definitions more suitable for developers or data scientists. Other scholars, particularly in educational research, frame AI literacy as a subset of digital literacy, focusing on conceptual understanding and the ability to interact with AI responsibly (e.g., Almatrafi et al., 2024). Still others, such as Long and Magerko (2020), highlight critical thinking, ethics, and collaboration with AI as key competencies. These diverse perspectives have advanced the discourse, yet they often lack operational clarity for organizations seeking to upskill broad, non-technical workforces. Our approach responds to this gap by adopting a holistic, user-centric definition of AI literacy that builds on and extends the definitions proposed by Ng et al. (2021) and Pinski and Benlian (2024), emphasizing the knowledge, skills, and attitudes necessary to understand, apply, and evaluate AI responsibly in organizational contexts. Building on this foundation, we introduce the AI Literacy Assessment Matrix, a business-oriented adaptation of holistic AI literacy frameworks. It is tailored to workforce segments ranging from frontline employees to executives, with an emphasis on usage, interpretation, and governance rather than technical development.

The matrix is grounded in the tripartite AI competence model developed by Knoth et al. (2024), which categorizes AI-related competencies into cognitive knowledge, behavioral skills, and

attitudes across key AI domains, including generic AI concepts, domain-specific applications, and ethics.. This tripartite structure, building on the ABC model of competence (Attitude, Behavior, Cognition), is widely used in educational research for its emphasis on problem-solving flexibility. Rooted in Franz E. Weinert's conceptualization of competence (Weinert, 2001), this framework defines competencies as the knowledge (Cognition) and skills (Behavior) individuals develop to solve problems, along with the motivation, volition, and social dispositions (Attitude) necessary for responsible application in varying contexts.

Building on this conceptual foundation and previous research in organizational settings (Pinski, Hofmann, et al., 2024), we adapted the ABC model into an organizational AI Literacy Assessment Matrix, tailored to the distinct needs of executives, middle managers, and non-IT employees. The matrix distinguishes three key dimensions of AI literacy:

Conceptual, ethical, and practical AI literacy are mutually reinforcing: without ethical insight, technical knowledge risks misuse; without practical skills, even ethical awareness may fail to drive adoption. The AI Literacy Assessment Matrix enables employees to self-assess their strengths and blind spots, providing clarity on where to grow. By aggregating results, organizations can pinpoint capability gaps across levels or departments. For example, R&amp;D may require support translating insights into action, while operations staff may benefit from greater awareness of ethical risks in everyday AI use. The matrix not only guides individual upskilling but also serves as a strategic workforce planning tool aligned with real-world AI responsibilities. Table 1 presents sample self-assessment statements for each literacy dimension and role, offering a structured approach to measuring AI literacy.

- Conceptual AI literacy (Cognition/Knowledge): The ability to understand core AI principles, such as how algorithms, data, and models function, and how they are applied in business settings. Conceptual literacy enables employees to engage meaningfully with AI systems, even if they are not building them. For example, an AI-literate employee should understand how an algorithm reaches a decision, or how input data quality affects the output. · Ethical AI literacy (Attitudes): The capacity to anticipate and evaluate the societal, legal, and organizational risks associated with AI use. This includes understanding AI-related challenges such as bias, privacy, and accountability. An ethically AI-literate manager, for example, would recognize the risks of deploying a biased hiring algorithm or ensure compliance with data protection regulations in AI applications. · Practical AI literacy (Behavior/Skills): The ability to apply AI purposefully in daily work to augment human decisions, improve processes, and communicate outcomes clearly. A practically AI-literate professional can draw on AI's capabilities to improve productivity and outcome quality, and communicate more effectively across teams and levels. Journal Pre-proof

## [Insert Table 1 About Here]

The AI Literacy Assessment Matrix can support onboarding, leadership development, and ongoing training initiatives, particularly within HR. However, its value is significantly enhanced when used not merely for assessment, but as a strategic diagnostic instrument that uncovers AI

capability gaps across the organization. These diagnostic insights form the foundation for the AI Literacy Development Canvas, which operationalizes the findings into structured development plans and strategic alignment. The next section introduces the Canvas as a complementary tool that builds directly on the Matrix to support integrated, organization-wide AI literacy efforts.

## 4. The AI literacy development canvas

Drawing on interviews with managers across various industries and a systematic review of the AI literacy literature (Almatrafi et al., 2024), we identified nine key elements that guide managers in designing and implementing AI literacy initiatives. We organized these elements into three categories that constitute the main building blocks of the AI Literacy Development Canvas (see Figure 1):

1. Strategy &amp; Goals (WHY?) , which includes the strategic rationale for AI literacy initiatives, the expected outcomes, and the key AI use cases that literacy efforts should support. 2. Assessment &amp; Training (WHAT?) , which focuses on evaluating the current state of AI literacy, identifying knowledge gaps, and designing appropriate training interventions. 3. Execution &amp; Measurement (HOW?) , which ensures that AI literacy programs are wellresourced, systematically implemented, and continuously measured for effectiveness. [Insert Figure 1 About Here] The AI Literacy Development Canvas is a strategic framework designed to help companies systematically build AI competencies across all employee levels while aligning with broader business goals. By offering a structured approach, it ensures that AI literacy becomes an integral part of the organization's capability development strategy, balancing technical proficiency with ethical awareness. The following sub-sections outline its three building blocks and nine key elements, with a quick-reference summary available in Appendix 1. Journal Pre-proof

## 4.1. Strategy &amp; Goals (The WHY)

The first building block of the AI Literacy Development Canvas (Why do it?) defines the strategic intent behind AI literacy initiatives and ensures that they are directly linked to an organization's broader digital transformation and business strategy.

## 4.1.1. Business objectives ('Why are we doing this?')

AI literacy initiatives must align with an organization's broader digital and business strategy. They should address concrete goals, such as boosting operational efficiency, driving AI-based innovation, or improving customer experience. Objectives may stem from internal planning or external pressures like regulatory compliance (e.g., the EU AI Act) or competitive dynamics. Clearly defined goals ensure leadership support and position AI literacy as a solution to business challenges, not a standalone training effort. For example, a firm seeking better customer engagement might train marketing managers in AI-driven analytics, while a bank could upskill compliance teams to use AI-powered fraud detection models.

## 4.1.2. Target outcomes ('What does success look like?')

While business objectives define why an organization is investing in AI literacy, this section defines what success looks like in terms of AI competencies across the workforce. Target outcomes should be measurable and role-specific. For example, within 12 months, a company might aim for: 80% of non-IT employees scoring at least 'intermediate' in AI tool usage, all executives completing an 'AI for Leaders' program and improving literacy scores from an average of 2/5 to 4/5, and the formation of an AI Center of Excellence, where advanced AIliterate employees champion AI adoption. By defining these goals, for example in a SMART (Specific, Measurable, Achievable, Relevant, Time-bound) way, organizations can ensure accountability and better evaluate the success of AI literacy initiatives.

4.1.3. AI use cases &amp; applications ('Where is the greatest need for AI use cases?') To maximize business value, AI literacy must be tied to high-impact use cases that align with strategic goals, address operational challenges, and drive efficiency and innovation. Companies should assess AI adoption levels, analyze workflows, and pinpoint where AI can optimize processes, automate tasks, or enhance decision-making. Effective AI literacy training requires focusing on both current applications and emerging opportunities, ensuring employees at all levels engage with AI in meaningful ways (Berthon et al., 2024). By continuously refining AI use cases, organizations can maintain practical, results-driven AI adoption that evolves with technological advancements. 4.2. Assessment &amp; Training (The WHAT) The second building block of the AI Literacy Development Canvas focuses on assessing the current state of AI literacy, identifying gaps, and designing effective training interventions. Just as a doctor assesses a patient's current condition, determines an optimal health state, and prescribes the right interventions, the AI Literacy Assessment Matrix (see Table 1) may guide organizations through three key steps: diagnosing the current AI literacy levels, defining the target skill levels, and implementing targeted programs and interventions to bridge the gap and achieve AI readiness. Journal Pre-proof

## 4.2.1. Current AI literacy levels ('Where are we now?')

A clear assessment of AI literacy levels ensures that training efforts address the most relevant skill gaps. AI proficiency varies by role: Executives understand AI's strategic impact but often lack hands-on experience; middle managers show uneven literacy across departments; and nonIT employees typically have limited exposure, using AI tools passively. Before determining where to go and how to get there, organizations must first establish a clear starting point by assessing their current AI literacy levels. Just as a map requires a known location before plotting a route, companies need to understand the existing ('as-is') AI competencies within their workforce before designing effective training initiatives. The AI Literacy Assessment Matrix may serve as a diagnostic tool to evaluate employees across conceptual knowledge, ethical awareness, and practical skills.

## 4.2.2. Goals and gap analysis ('Where do we want to go and what's missing?')

At this stage, companies must define their AI literacy goals and identify the gaps preventing them from reaching it. The AI Literacy Assessment Matrix may again support this process by

helping organizations determine the required ('to-be') conceptual, ethical, and procedural competencies across executives, middle managers, and employees. By systematically comparing current and desired literacy levels, businesses can pinpoint key 'from-to' shifts and prioritize areas with the greatest development impact.

## 4.2.3. Training &amp; Development ('How will we improve?')

At this stage, organizations need to design targeted AI training to bridge literacy gaps across all levels. Executives may benefit from AI leadership bootcamps, strategy workshops, and casebased learning to enhance governance and investment decisions. Middle managers may require AI integration labs, cross-functional training, and change leadership programs to operationalize AI adoption. Non-IT employees may need hands-on AI tool training, ethical AI awareness workshops, and job-specific AI literacy programs to confidently use AI in daily tasks. AI champions within teams help sustain learning, while mentorship programs, online courses, and interactive AI labs reinforce practical skills. This structured, role-aligned approach embeds AI literacy into everyday work operations.

## 4.3.2. Implementation plan ('How will we roll this out and manage it?')

4.3. Execution &amp; Measurement (The How) The third building block ensures that AI literacy programs are adequately resourced, effectively implemented, and continuously evaluated. 4.3.1. Resources &amp; Partnerships ('What resources do we need, and who will help us?') Implementing a successful AI literacy program requires financial, technological, and human resources to ensure scalability and long-term impact. Companies must allocate budgets for AI education, including external courses and certifications, while forming strategic partnerships with universities, AI training firms, and tech companies for specialized expertise. Appointing internal AI champions fosters continuous knowledge-sharing and integration into workflows. To sustain these efforts, organizations should establish an AI literacy task force involving HR, IT, and business leaders, with larger firms potentially appointing a Chief AI Officer or AI Learning Director. With the right resources and partnerships, companies can scale AI literacy effectively and embed it as a core business capability. Journal Pre-proof

A well-executed AI literacy program requires a phased rollout to ensure gradual adoption, scalability, and long-term integration. Companies should start with a pilot phase, training one business unit as a test case to refine content and gather feedback. Once successful, the scaling phase expands the initiative across departments, ensuring that AI competencies reach a critical mass within the organization. Finally, the embedding phase integrates AI literacy into onboarding programs, leadership development, and performance management, making AI skills a permanent part of corporate learning. To drive engagement, organizations should incorporate incentives such as certifications, gamified learning, and internal marketing efforts (e.g., 'AI Literacy Month' or success story campaigns). A structured implementation ensures that AI literacy is not a one-time initiative but a sustainable capability embedded into company culture.

## 4.3.3. Metrics and follow-up ('How will we measure progress and success?')

To ensure AI literacy programs drive meaningful business impact, organizations must track progress, measure effectiveness, and refine training efforts based on key performance indicators

(KPIs). These metrics should assess participation, skill development, practical application, and overall business value. Participation metrics track employee engagement, including the percentage of staff completing AI training and total hours invested in learning. Skill development metrics measure improvements in AI literacy, reflected in self-assessments and formal evaluations before and after training. Practical application metrics focus on AI adoption, such as the number of AI-driven projects initiated, increased usage of AI tools, and managers' confidence in applying AI insights. Finally, business impact metrics evaluate tangible improvements, including error reduction, efficiency gains, compliance adherence, and improved decision-making. Organizations should also assess AI readiness, monitoring employee sentiment toward AI through surveys and feedback mechanisms (Holmström, 2022). By implementing regular reviews and feedback loops, companies can ensure that AI literacy programs remain relevant, adaptive, and aligned with evolving technologies and business priorities. This ongoing evaluation process enables organizations to scale successful initiatives, refine underperforming areas, and sustain AI capability development across all levels of the workforce.

This stepwise structure reinforces the canvas' role as a dynamic coordination tool, akin to a business model canvas (Osterwalder &amp; Pigneur, 2010). It supports ongoing dialogue, enables rapid iteration, and ensures that AI literacy development remains tightly coupled to evolving strategic priorities. In sum, the canvas makes AI upskilling not only visible and actionable, but also scalable and adaptable across different organizational settings. It helps answer key questions before launching into action and keeps all stakeholders on the same page about how the company will build the critical capacity of AI literacy in its workforce.

4.4. How to use the AI literacy development canvas The AI Literacy Development Canvas is designed to guide organizations through a structured, participatory process of workforce upskilling. Its single-page format promotes shared understanding by making it easy to visualize how business objectives are linked to training initiatives and how resources are aligned with implementation. The canvas is best used in a cross-functional workshop setting, where stakeholders collaboratively complete each section, guided by prompting questions as suggested in Appendix 2. The following step-by-step approach (Table 2) outlines a recommended process for applying the canvas in practice. [Insert Table 2 About Here] Journal Pre-proof

## 5. An exemplary application at Pharmaco and lessons learned

To illustrate the usefulness of both the AI Literacy Assessment Matrix and the AI Literacy Development Canvas, we present the results from multiple interactions via workshops and interviews with PharmaCo, a partner organization in the pharma industry. PharmaCo, a global pharmaceutical company, has been integrating AI into its drug discovery, supply chain optimization, and patient engagement efforts. However, the leadership team identified a significant barrier: many employees lacked the necessary AI literacy to effectively leverage AIpowered tools and processes. While AI adoption was accelerating, middle managers struggled to integrate AI into operations, and frontline employees were unsure how to interact with AI-driven insights. PharmaCo conducted an AI literacy assessment across three key workforce segments (i.e., executives, middle managers, and non-IT employees in business units) using the AI

Literacy Assessment Matrix (see Table 3). These results highlighted the specific literacy gaps at different levels of the organization. Executives required strategic AI literacy to assess business model implications and investments, middle managers needed operational AI fluency to integrate AI into workflows, and non-IT employees needed foundational AI knowledge to interact effectively with AI-powered tools (e.g., GenAI). These insights were instrumental in defining targeted training and development efforts.

## [Insert Table 3 About Here]

PharmaCo applied the AI Literacy Development Canvas (see Table 4) to operationalize its AI literacy initiative as a core driver of digital transformation. Grounded in the findings from the AI Literacy Assessment Matrix, the company systematically identified key gaps across conceptual, ethical, and practical AI literacy for executives, middle managers, and non-IT employees. The canvas enabled PharmaCo to align AI training programs with strategic business objectives, such as accelerating drug discovery, enhancing supply chain forecasting, and ensuring regulatory compliance, by defining clear target outcomes for each workforce segment. The gap analysis informed tailored development priorities, including executive briefings on strategic AI use in R&amp;D and market forecasting, middle manager trainings on AI in operations and clinical workflows, and AI tool-specific tutorials for frontline staff. By embedding these efforts in a phased implementation plan starting with a pilot unit, and supported by cross-departmental resources and partnerships, PharmaCo ensured efficient rollout and ongoing adaptability. Moreover, the canvas established measurable success indicators, allowing the company to track participation, skill acquisition, and AI impact on operations. In doing so, PharmaCo moved beyond isolated training events, embedding AI literacy as a scalable, organization-wide capability that supports innovation, mitigates risk, and fosters a data-driven culture. [Insert Table 4 About Here] 5.1. Lessons for practice Journal Pre-proof

The PharmaCo case revealed several practical lessons for effectively implementing the AI Literacy tools. First, securing executive sponsorship was essential to elevate AI literacy as a strategic priority and to mobilize resources. Second, by embedding the Canvas into ongoing strategic planning cycles, PharmaCo ensured that literacy development became a sustained organizational practice rather than a one-off initiative. Third, recognizing the risk of overwhelm, PharmaCo began with a lean, high-level draft focused on priority areas most relevant to its maturity level, and refined it iteratively. This flexible, evolving use of the Canvas enabled the organization to remain responsive to shifting needs. To avoid stagnation, the Canvas was treated as dynamic tool and revisited regularly. Moreover, anticipated barriers such as staff resistance and outcome measurement challenges were mitigated through clear expectation setting and alignment with existing performance metrics. Together, these lessons reinforce the importance of organizational readiness-leadership, planning, and communication-alongside the practical benefits of iterative tool use. When combined, the Matrix and Canvas provide a robust, adaptable framework for scaling AI literacy across diverse organizational contexts.

## 6. Conclusion

AI literacy is no longer optional. It is essential for organizations seeking to harness AI responsibly and effectively (Mikalef et al., 2025). This article presented a structured and actionable approach to AI capability-building through two core tools: the AI Literacy Assessment Matrix and the AI Literacy Development Canvas. Together, these tools help companies identify literacy gaps across three dimensions-conceptual, ethical, and practicaland translate those insights into role-specific development strategies for executives, managers, and employees. Like a diagnostic and treatment plan, this approach supports continuous capability-building, aligning learning with business needs and strategic goals.

We showed that AI literacy must be multidimensional and role-sensitive: without conceptual understanding, teams risk misuse; without ethical awareness, they may violate trust or compliance; and without practical skills, even the best AI systems may fail to deliver impact. Furthermore, AI literacy is required at all levels-but in different ways. Executives need a strategic grasp of AI's value, risks, and governance implications. Middle managers must develop the operational know-how to integrate AI into day-to-day processes and lead teams through AIdriven change. Non-IT employees require hands-on proficiency with AI tools and the critical thinking needed to use them responsibly. A role-specific, multidimensional approach to AI literacy ensures that training efforts are relevant, actionable, and aligned with organizational goals.

While this article offers a comprehensive and practical framework for developing AI literacy, several limitations warrant consideration. The AI Literacy Development Canvas is designed to be industry-agnostic, yet AI literacy needs may vary widely across sectors. Future research should examine its applicability in specific contexts such as healthcare, finance, or manufacturing, where domain-specific challenges may arise (Bartsch, Behn, et al., 2025). Although our framework is grounded in our prior research and a real-world case study, further empirical validation, especially through longitudinal studies, is needed to assess its long-term impact on outcomes like innovation, operational efficiency, and AI adoption. Moreover, while large firms may have the capacity to roll out structured literacy programs, small and mediumsized enterprises (SMEs) often face budgetary and capability constraints. Research should therefore explore scalable, accessible literacy models suited to SMEs. AI literacy is also a moving target; as technologies like generative AI and autonomous systems evolve, so too must literacy frameworks. Understanding how these advances reshape ethical, practical, and governance requirements is essential. Lastly, given the variation in organizational AI maturity, future work could examine how literacy initiatives align with different stages of AI readiness to maximize strategic fit. Journal Pre-proof

AI education should not be seen as a one-off initiative but as a continuous, strategic effort to build long-term organizational capabilities. No matter their starting point, all companies must recognize the multidimensional and evolving nature of AI literacy. The frameworks introduced in this article-the AI Literacy Assessment Matrix and the AI Literacy Development Canvasoffer actionable guidance to evaluate current competencies, define future literacy goals, design targeted training, and track progress over time. By adopting this structured approach, organizations can empower their workforce, elevate decision-making, and ensure that AI becomes a catalyst for lasting innovation.

[Insert Appendices 1 &amp; 2 Here]

Journal Pre-proof

## References

Almatrafi, O., Johri, A., &amp; Lee, H. (2024). A systematic review of AI literacy conceptualization, constructs, and implementation and assessment efforts (2019-2023). Computers and Education Open , 6 , 100173. https://doi.org/https://doi.org/10.1016/j.caeo.2024.100173

Bartsch, S., Behn, O., Benlian, A., Brownsword, R., Bücker, S., Düwell, M., Formánek, N., Jungtäubl, M., Leyer, M., Richter, A., Schmidt, J.-H., &amp; Will-Zocholl, M. (2025). Governance of High-Risk AI Systems in Healthcare and Credit Scoring. Business &amp; Information Systems Engineering . https://doi.org/10.1007/s12599-025-00944-4

Bartsch, S., Nguyen, L. H., Schmidt, J.-H., Adam, M., Sunyaev, A., &amp; Benlian, A. (2025). The Present and Future of Accountability for AI Systems: A Bibliometric Analysis. Information Systems Frontiers , forthcoming.

Holmström, J. (2022). From AI to digital transformation: The AI readiness framework. Business Horizons , 65 (3), 329-339. https://doi.org/https://doi.org/10.1016/j.bushor.2021.03.006

Berthon, P., Yalcin, T., Pehlivan, E., &amp; Rabinovich, T. (2024). Trajectories of AI technologies: Insights for managers. Business Horizons , 67 (5), 461-470. https://doi.org/https://doi.org/10.1016/j.bushor.2024.03.002 Brenner, G., Slowik, J., &amp; Morrison, D. (2025). AI Bias Lawsuit Against Workday Reaches Next Stage as Court Grants Conditional Certification of ADEA Claim . Retrieved 07/23 from https://www.lawandtheworkplace.com/2025/06/ai-bias-lawsuit-against-workday-reaches-nextstage-as-court-grants-conditional-certification-of-adea-claim/?utm\_source=chatgpt.com Cetindamar, D., Kitto, K., Wu, M., Zhang, Y., Abedin, B., &amp; Knight, S. (2024). Explicating AI Literacy of Employees at Digital Workplaces. IEEE Transactions on Engineering Management , 71 , 810-823. https://doi.org/10.1109/TEM.2021.3138503 EU AI Act. (2024). Regulation (EU) 2024/1689 of the European Parliament, Article 4 (AI Literacy) . http://data.europa.eu/eli/reg/2024/1689/oj Journal Pre-proof

Knoth, N., Decker, M., Laupichler, M. C., Pinski, M., Buchholtz, N., Bata, K., &amp; Schultz, B. (2024). Developing a holistic AI literacy assessment matrix - Bridging generic, domain-specific, and ethical competencies. Computers and Education Open , 6 , 100177. https://doi.org/https://doi.org/10.1016/j.caeo.2024.100177

Koponen, J., Julkunen, S., Laajalahti, A., Turunen, M., &amp; Spitzberg, B. (2025). Work Characteristics Needed by Middle Managers When Leading AI-Integrated Service Teams. Journal of Service Research , 28 (1), 168-185. https://doi.org/10.1177/10946705231220462

Laupichler, M. C., Aster, A., Schirch, J., &amp; Raupach, T. (2022). Artificial intelligence literacy in higher and adult education: A scoping literature review. Computers and Education: Artificial Intelligence , 3 , 100101. https://doi.org/https://doi.org/10.1016/j.caeai.2022.100101

Long, D., &amp; Magerko, B. (2020). What is AI literacy? Competencies and design considerations. Proceedings of the 2020 CHI conference on human factors in computing systems,

Malmsheimer, T. (2024). The AI Proficiency Report: Most organizations are not ready to deploy AI . https://www.sectionschool.com/ai/the-ai-proficiency-report

Mikalef, P., Benlian, A., Conboy, K., &amp; Tarafdar, M. (2025). Responsible AI starts with the artifact: Challenging the concept of responsible AI in IS research. European Journal of Information Systems , 1-8. https://doi.org/10.1080/0960085X.2025.2506875

Ng, D. T. K., Leung, J. K. L., Chu, S. K. W., &amp; Qiao, M. S. (2021). Conceptualizing AI literacy: An exploratory review. Computers and Education: Artificial Intelligence , 2 , 100041. https://doi.org/https://doi.org/10.1016/j.caeai.2021.100041

Osterwalder, A., &amp; Pigneur, Y. (2010). Business model generation: a handbook for visionaries, game changers, and challengers . John Wiley &amp; Sons.

Pinski, M., &amp; Benlian, A. (2024). AI literacy for users - A comprehensive review and future research directions of learning methods, components, and effects. Computers in Human Behavior: Artificial Humans , 2 (1), 1-22.

https://doi.org/https://doi.org/10.1016/j.chbah.2024.100062

Pinski, M., Hofmann, T., &amp; Benlian, A. (2024). AI Literacy for the top management: An upper echelons perspective on corporate AI orientation and implementation ability. Electronic Markets , 34 (1), 24. https://doi.org/10.1007/s12525-024-00707-1

Pinski, M., Tarafdar, M., &amp; Benlian, A. (2024). Why Executives Can't Get Comfortable With AI. MIT Sloan Management Review .

Rajaram, K., &amp; Tinguely, P. N. (2024). Generative artificial intelligence in small and medium enterprises: Navigating its promises and challenges. Business Horizons , 67 (5), 629-648. https://doi.org/https://doi.org/10.1016/j.bushor.2024.05.008

Journal Pre-proof

Relyea, C., Maor, D., &amp; Durth, S. (2024). Gen AI adoption: The next inflection point: From employee experimentation to organizational transformation .

Retkowsky, J., Hafermalz, E., &amp; Huysman, M. (2024). Managing a ChatGPT-empowered workforce: Understanding its affordances and side effects. Business Horizons , 67 (5), 511-523. https://doi.org/https://doi.org/10.1016/j.bushor.2024.04.009

StiboSystems. (2024). AI: The High-Stakes Gamble for Enterprises: The Business Leaders' View .

Stolpe, K., &amp; Hallström, J. (2024). Artificial intelligence literacy for technology education. Computers and Education Open , 6 , 100159.

https://doi.org/https://doi.org/10.1016/j.caeo.2024.100159

Sunyaev, A., Benlian, A., Pfeiffer, J., Jussupow, E., Thiebes, S., Maedche, A., &amp; Gawlitza, J. (2025). High-Risk Artificial Intelligence. Business &amp; Information Systems Engineering . https://doi.org/10.1007/s12599-025-00942-6

Tursunbayeva, A., &amp; Chalutz-Ben Gal, H. (2024). Adoption of artificial intelligence: A TOP framework-based checklist for digital leaders. Business Horizons , 67 (4), 357-368. https://doi.org/https://doi.org/10.1016/j.bushor.2024.04.006

Walsh, M. (2024). Why these big companies are expanding AI training to all employees . https://www.raconteur.net/technology/ai-training-employees

Weinert, F. E. (2001). Concept of competence: A conceptual clarification. In D. S. Rychen &amp; L. H. Salganik (Eds.), Defining and Selecting Key Competencies (pp. 45-65). Hogrefe.

Journal Pre-proof

## Table 1. The AI literacy assessment matrix

| Role → Dimension ↓                                                                                     | Executives / Top Managers (Strategic competencies)                                                                                                                                                                                                                                                                                                                                 | Middle Managers (Management competencies)                                                                                                                                                                                                                                                                                                                             | General Employees (Non- IT) (Operational competencies)                                                                                                                                                                                                                                                                                     |
|--------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Conceptual AI Literacy Understanding how AI systems work and how they apply to business contexts.      | • 'I understand core AI principles-such as how algorithms process data and make predictions-and can assess their relevance for strategic business opportunities.' • 'I can realistically evaluate the feasibility, resource needs, and ROI of proposed AI initiatives.' • 'I am aware of the infrastructure and data requirements for AI deployment at scale in our organization.' | • 'I understand how AI models generate outputs and can evaluate whether a specific AI solution fits the operational needs of my team.' • 'I can identify relevant use cases for AI in my area and understand how they function at a high level.' • 'I understand enough about the AI systems my team uses to support troubleshooting or escalate issues when needed.' | • 'I understand the basic logic behind the AI tools I use and how the quality of input data influences their outputs.' • 'I have a basic understanding of how the AI tools I use operate and what outputs to expect.' • 'I can identify aspects of my job where AI could be helpful and describe potential use cases.'                     |
| Ethical AI Literacy Recognizing and managing AI's ethical and risk implications.                       | • 'I evaluate the ethical risks of AI systems, such as fairness, transparency, and accountability, when making strategic decisions or approving projects.' • 'I stay informed about data privacy and AI regulations and integrate them into strategic decision-making.' • 'I actively promote responsible AI use and support ethics training across the organization.' Journal     | • 'I am aware of AI-related ethical risks in my team's tools and ensure we have processes in place to mitigate bias and comply with data regulations.' • 'I can identify and address potential biases or unfair outcomes in AI tools used by my team.' • 'I am comfortable discussing AI risks and limitations with both my team and senior management.' Pre-proof    | • 'I understand that AI systems can produce biased or incorrect outputs and always apply human judgment before acting on them.' • 'I am aware that AI tools can produce biased or incorrect outputs and take care to double-check results.' • 'I understand when AI outputs require human review, especially when ethical concerns arise.' |
| Practical AI Literacy Applying AI tools effectively in daily tasks and communicating insights clearly. | • 'I can clearly explain the business value, limitations, and strategic role of AI to diverse stakeholders.' • 'I interpret AI-generated analytics and use them to support executive decision- making.' • 'I can clearly communicate the strategic benefits, risks, and limitations of our AI initiatives to stakeholders such as the board, regulators, and investors.'           | • 'I know when and how to adjust existing workflows to incorporate AI tools that enhance productivity or accuracy.' • 'I regularly use AI- generated insights (e.g., forecasts, classifications, scores) to inform team decisions.' • 'I can explain the rationale behind an AI-supported decision to both my team and upper management.'                             | • 'I can use AI-generated recommendations to improve how I prioritize tasks or make routine decisions in my role.' • 'I have adapted at least one part of my workflow to make better use of the AI tools available to me.' • 'I can clearly explain to a colleague how an AI tool I use works and how it benefits my day-to-day tasks.'    |
| Note : Rate each self-assessment statement from 1 = Strongly disagree to 5 = Strongly agree            | Note : Rate each self-assessment statement from 1 = Strongly disagree to 5 = Strongly agree                                                                                                                                                                                                                                                                                        | Note : Rate each self-assessment statement from 1 = Strongly disagree to 5 = Strongly agree                                                                                                                                                                                                                                                                           | Note : Rate each self-assessment statement from 1 = Strongly disagree to 5 = Strongly agree                                                                                                                                                                                                                                                |

Table 2. A step-by-step approach for using the AI literacy development canvas

| Step                                                                                                                                                          | Key Activity                                   | Primary Stakeholders                                                               | Outcomes                                    | Check off   |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|------------------------------------------------------------------------------------|---------------------------------------------|-------------|
| Conduct AI literacy assessment using the Matrix to establish a baseline                                                                                       | HR, Learning& Development, Business Unit Leads | Clear overview of current capabilities and skill gaps                              | □                                           | 1. Identify |
| Organize and facilitate a workshop introducing the Canvas and its purpose                                                                                     | Senior Leadership, HR, IT, Strategy            | Strategic alignment and shared ownership of goals                                  | □                                           | 2. Initiate |
| 3. Ideate Co-create the Canvas in a sequence that reflects organizational priorities across Strategy &Goals, Assessment &Training, and Execution &Measurement | Cross-functional Working Group                 | actionable                                                                         | A coherent and literacy development roadmap | □           |
| 4. Implement Launch development initiatives, assign tasks, monitor activities and outcomes                                                                    | Project Owners, Team Leads, Trainers           | Coordinated execution and tracking of progress                                     |                                             | □           |
| 5. Iterate Review and adapt the Matrix and Canvas regularly to reflect changing conditions                                                                    | Steering Committee or Workshop Leads           | Sustained relevance and iterative refinement of the AI literacy strategy Pre-proof |                                             | □           |

Journal Pre-proof

Table 3. AI literacy assessment matrix results at PharmaCo

|                        | Executives / Top Managers (Strategic competencies)                                                                                                                                                                                                                                                                                                                              | Middle Managers (Management competencies)                                                                                                                                                                                                                                                                                                                                                                                  | General Employees (Non-IT) (Operational competencies)                                                                                                                                                                                                                                                                                                                                                                                              |
|------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Conceptual AI Literacy | Gaps: Limited understanding of AI's role in drug discovery, clinical trial optimization, and pharma market forecasting; unclear how AI creates value across the pharma value chain. Training Need: Executive briefings on AI-enabled R&D, strategic applications across the pharma lifecycle, and scenario-based sessions on evaluating AI investment and innovation potential. | Gaps: Limited understanding of how AI supports operational decisions in areas like demand forecasting, inventory optimization, and trial site selection; unclear on how AI models function within pharma workflows. Training Need: Applied training on AI use cases in supply chain, manufacturing, and clinical ops; role-based sessions on how AI models are built, validated, and deployed in pharma contexts.          | Gaps: Limited understanding of how AI supports regulatory documentation, automates lab tasks, or enhances patient data workflows; unclear how AI tools generate outputs. Training Need: Introductory sessions on how GenAI, NLP, and machine learning are used in pharma operations; role-specific tutorials on AI basics applied to everyday tasks.                                                                                               |
| Ethical AI Literacy    | Gaps: Unclear governance regarding AI-generated insights in drug approval decisions and patient safety regulations. Training Need: AI ethics and compliance workshops on AI bias in clinical trials, regulatory impact of AI- assisted diagnostics, and FDA/EMA                                                                                                                 | Gaps: Limited awareness of AI bias and ethical risks in patient recruitment for clinical trials, adverse event monitoring, and trial protocol automation. Training Need: Targeted training on bias mitigation in AI-driven eligibility screening, ethical handling of real-world data, and compliance-aligned decision frameworks in clinical operations. Pre-proof                                                        | Gaps: Limited awareness about AI- generated biases in drug efficacy analysis or automated patient diagnostics. Training Need: Responsible AI training focused on data privacy in AI-driven diagnostics, ethical considerations in automated prescription AI, and AI transparency in patient communication.                                                                                                                                         |
| Practical AI Literacy  | Gaps: Difficulty operationalizing AI insights in strategic planning, and limited ability to translate AI-driven R&D or supply chain results into compelling executive narratives. Training Need: Executive training on applying AI insights in board-level decisions, investor briefings, and regulatory communication strategies. Journal                                      | Gaps: Challenges in translating AI- generated outputs (e.g., biomarker predictions, patient risk scores, and production forecasts) into actionable insights for cross- funcational teams. Training Need: Targeted workshops on integrating AI insights into operational decisions (e.g., trial enrollment, batch release), paired with simulation-based exercises for explaining AI outputs in cross- functional meetings. | Gaps: Limited ability to use efficient prompt engineering techniques and apply AI-generated outputs (e.g., lab results, patient risk profiles) in daily tasks, and uncertainty in interpreting AI- assisted regulatory insights or escalating questionable outputs. Training Need: Scenario-based exercises on integrating AI into lab workflows and documentation, communication training for conveying AI findings to clinicians and regulators. |

Table 4. AI literacy development canvas implementation at PharmaCo

| Building Block                  | Element                             | PharmaCo Implementation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
|---------------------------------|-------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| TheWHY (Strategy& Goals)        | Business Objectives                 | • Reduce AI-related errors in R&D and supply chain predictions by 30%. • Accelerate clinical trial timelines by 20% and reduce supply chain disruptions by 15% through targeted AI literacy improvements.                                                                                                                                                                                                                                                                                                                                                                                    |
| TheWHY (Strategy& Goals)        | Target Outcomes (AI Literacy Goals) | • Executives: 80% of leaders proficient in conceptual and ethical AI literacy to guide AI investments and regulatory decisions. • Middle Managers: 70% of managers able to apply and explain AI outputs in clinical and supply chain workflows. • Non-IT Employees: 90% trained to use GenAI tools, interpret AI diagnostics, and follow ethical AI practices in documentation.                                                                                                                                                                                                              |
| TheWHY (Strategy& Goals)        | AI Use Cases& Application           | • Drug Discovery and R&D: AI for molecule prediction, candidate selection, and trial design. • Supply Chain Optimization: AI for forecasting, inventory control, and predictive maintenance. • Patient Stratification and Recruitment: AI to segment and match trial participants using real-world data. • Regulatory Compliance: AI to automate documentation, review submissions, and assess approval risks. Pre-proof                                                                                                                                                                     |
| TheWHAT (Assessment& Training)  | Current AI Literacy Levels          | • Executives: Strong strategic orientation but limited understanding of AI's conceptual foundations and ethical implications. • Middle Managers: Operational familiarity with AI tools but limited ability to interpret or communicate AI insights effectively. • Non-IT Employees: Minimal exposure to AI systems, low confidence in using AI tools, limited awareness of AI-related ethical issues.                                                                                                                                                                                        |
| TheWHAT (Assessment& Training)  | Goals &Gap Analysis                 | • Executives: Develop strong conceptual and ethical AI literacy to guide strategic decisions and ensure responsible AI governance. • Middle Managers: Strengthen practical and ethical AI literacy to integrate AI into workflows and lead teams confidently. • Non-IT Employees: Achieve basic conceptual and practical AI literacy for effective use of AI tools in daily tasks and compliance. Journal                                                                                                                                                                                    |
| TheWHAT (Assessment& Training)  | Training& Development Priorities    | • Executives: Briefings on strategic AI use in R&D and market forecasting; ethics training on AI bias and regulatory risks; sessions on applying AI insights in planning and communication. • Middle Managers: Training on AI in operations and clinical workflows; ethics modules on data use in trials; workshops on interpreting and presenting AI outputs. • Non-IT Employees: Tutorials on AI tools for daily tasks; training on responsible AI use; practical exercises on applying (e.g., advanced prompting techniques) and communicating AI results in lab and compliance contexts. |
| TheHOW (Execution& Measurement) | Resources& Partnerships             | • Secure Dedicated Budget: Allocate (cross-departmental) targeted funding to design, acquire, and scale role-specific AI literacy programs. • Leverage Tech& Educational Partnerships: Partner with pharma-focused AI vendors (e.g., IBM Watson Health, Tempus, Owkin) and universities to provide access to cutting-edge tools, tailored AI literacy programs, and expert-led training. • Integrate Regulatory &Ethical Expertise: Involve FDA, EMA, and ethics experts to ensure that all training aligns with compliance standards and promotes responsible, bias-aware AI adoption.      |
| TheHOW (Execution& Measurement) | High-Level Implementation Plan      | • Stage 1 - Pilot &Assessment: Launch a cross-level AI literacy pilot in one business unit (e.g., clinical operations), using the AI Literacy Assessment Matrix to baseline conceptual, ethical, and practical competencies. Gather feedback to refine content and delivery formats. • Stage 2 - Scaled Role-Based Rollout: Expand training company-wide with tailored learning paths for executives, middle managers, and non-IT staff. Secure partnerships with educational providers and AI vendors; allocate budget for platform access and expert facilitation.                         |

|                     | • Stage 3 - Embedding &Continuous Learning: Establish AI champions in key functions, integrate literacy into onboarding and L&D systems, and run ongoing workshops and simulations. Use follow-up assessments and KPIs to measure progress and adjust programs.                                                                                                                                                                |
|---------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Metrics& Evaluation | • Track AI literacy improvements via pre- and post-training assessments (e.g., improvement in AI self-assessment results after training). • Monitor AI-driven operational efficiency gains in R&D, regulatory compliance, and supply chain (e.g., reduction in AI-related errors in drug discovery or clinical trials). • Assess the impact of AI literacy on reducing compliance risks and improving drug approval timelines. |

Journal Pre-proof

Figure 1. The AI literacy development canvas

<!-- image -->

Appendix 1. Overview of the three building blocks and nine elements of the AI literacy development canvas

| Building Block                  | Element                    | Brief Description                                                                       |
|---------------------------------|----------------------------|-----------------------------------------------------------------------------------------|
| TheWHY (Strategy &Goals)        | Business Objectives        | Align AI literacy with strategic goals, such as efficiency, innovation, or compliance.  |
| TheWHY (Strategy &Goals)        | Target Outcomes            | Define measurable, role-specific goals to track literacy progress and impact.           |
| TheWHY (Strategy &Goals)        | AI Use Cases& Applications | Link literacy efforts to relevant, high-impact AI applications in the organization.     |
| TheWHAT (Assessment& Training)  | Current AI Literacy Levels | Assess existing competencies across roles to identify starting points.                  |
| TheWHAT (Assessment& Training)  | Goals and Gap Analysis     | Compare current to target states to define skill gaps and priorities.                   |
| TheWHAT (Assessment& Training)  | Training &Development      | Design tailored interventions for executives, managers, and staff.                      |
| TheHOW (Execution& Measurement) | Resources &Partnerships    | Allocate internal and external support to scale and sustain literacy efforts. Pre-proof |
| TheHOW (Execution& Measurement) | Implementation Plan        | Plan and execute phased rollout, from pilots to enterprise- wide integration.           |
| TheHOW (Execution& Measurement) | Metrics and Follow-Up      | Track participation, skill gains, adoption, and business impact over time.              |

Appendix 2. Prompting questions for completing the AI literacy development canvas

| Building Block                             | Element                          | Key Questions                                                                                                                                                                                                                                          | Notes / Inputs                                                                    |
|--------------------------------------------|----------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|
| TheWHY Strategy& Goals (Why do it?)        | Business Objectives              | Why are we doing this? - What are the strategic goals tied to AI literacy? - What business problems will this address? - What are the key performance goals (e.g., AI- driven product features, efficiency improvements, compliance readiness)?        | ____________________ ____________________ __________________ ____________________ |
| TheWHY Strategy& Goals (Why do it?)        | Target Outcomes (Literacy Goals) | What does success look like? - Which competency areas are most crucial for us? - What are the desired competency levels? - What measurable improvements are expected (e.g., assessment scores, AI adoption rates)?                                     | ____________________ ____________________ __________________ ____________________ |
| TheWHY Strategy& Goals (Why do it?)        | AI Use Cases & Application Areas | Where and how will AI be applied in our organization? - What are the key areas where AI can create most value (e.g., customer service, marketing, supply chain, finance)? - What are the top AI-powered use cases that align with business objectives? | ____________________ ____________________ __________________ ____________________ |
| TheWHAT Assessment& Training (What to do?) | Current AI Literacy Levels       | Where are we now? - What is the existing AI knowledge across different roles? (Executives, Managers, Employees) - What AI-related training/resources are available? - What are the strengths and weaknesses in AI literacy?                            | ____________________ ____________________ __________________ ____________________ |

Journal Pre-proof

|                                               | Goals &Gap Analysis              | Where do we want to go? - What is the gap between where we are and where we need to be? - What are the key knowledge and skills gaps? - Are there organizational barriers (e.g., lack of collaboration, governance issues)? - What are the specific goals/gaps by role? ____________________ ____________________ __________________ ____________________                                               |
|-----------------------------------------------|----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                               | Training& Development Activities | How will we close the gap? - What key learning interventions are needed? - What role-specific training approaches (Executives, Managers, Employees) are required? - What delivery methods will be used (workshops, e-learning, mentoring)? - Will AI Champions or mentorship programs be implemented? ____________________ ____________________ __________________ ____________________                 |
| TheHOW Execution& Measurement (How to do it?) | Resources& Partnerships          | What resources and support are needed? - What budget allocations are required? - Are external partnerships (e.g., universities, tech firms, industry consortia) necessary? - Who are the internal stakeholders (HR, IT, Business Units)? - What tools/software are required (e.g., AI sandboxes, learning platforms)? ____________________ ____________________ __________________ ____________________ |
| TheHOW Execution& Measurement (How to do it?) | Implemen- tation Plan            | How will we roll this out and manage it? - What is the rollout strategy (pilot vs. full-scale launch)? - What are the key phases (awareness, training, application, embedding into HR processes)? - What incentives and engagement strategies will be used? - How will change management be handled? ____________________ ____________________ __________________ ____________________ Pre-proof        |
| TheHOW Execution& Measurement (How to do it?) | Metrics& Follow-up               | How will we measure success? - What key performance indicators (KPIs) will be tracked? - How will training completion metrics be evaluated? - How will skill development be measured? - What business impact measures will be used? - How will follow-up and continuous improvement be ensured? ____________________ ____________________ __________________ ____________________                       |

Journal Pre-proof