---
source_file: Alam_2025_Social_work_in_the_age_of_artificial_intelligence.pdf
conversion_date: 2026-02-03T18:19:35.604683
converter: docling
quality_score: 95
---

<!-- PAGE 1 -->
<!-- image -->

## Journal of Evidence-Based Social Work

ISSN: 2640-8066 (Print) 2640-8074 (Online) Journal homepage: www.tandfonline.com/journals/webs22

## Social Work in the Age of Artificial Intelligence: A rights-Based Framework for evidence-Based Practice Through Social Psychology, Group Dynamics, and Institutional Analysis

## Nafees Alam

To cite this article: Nafees Alam (2026) Social Work in the Age of Artificial Intelligence: A rights-Based Framework for evidence-Based Practice Through Social Psychology, Group Dynamics, and Institutional Analysis, Journal of Evidence-Based Social Work, 23:1, 123-134, DOI: 10.1080/26408066.2025.2547219

To link to this article:

https://doi.org/10.1080/26408066.2025.2547219

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

Published online: 13 Aug 2025.

Submit your article to this journal

Article views: 418

View related articles

View Crossmark data

曲

CrossMark

<!-- image -->


<!-- PAGE 2 -->


<!-- image -->

<!-- image -->

## Social Work in the Age of Artificial Intelligence: A rights-Based Framework for evidence-Based Practice Through Social Psychology, Group Dynamics, and Institutional Analysis

Nafees Alam

Social Work, University of Nebraska, Kearney, NE, USA

## ABSTRACT

Purpose: This  theoretical  analysis  aims  to  develop  a  comprehensive rights-based  framework  for  navigating  artificial  intelligence  integration in social work practice while addressing the ethical implications of AI deployment across micro, meso, and macro practice levels. Materials  and  Methods: The  study  synthesized  interdisciplinary research  drawing  on  social  psychology,  group  dynamics  theory,  and institutional  analysis.  The  conceptual  framework  integrated  the  I-C-E (Ingroup Identification, Cohesion, Entitativity) model with socioecological systems theory. Analysis was conducted on existing literature and documented  case  examples  to  examine  how  AI  systems  mediate interpersonal  relationships  and  construct  meaning  in  social  work contexts.

Results: The analysis demonstrated that AI systems profoundly impact vulnerable populations by mediating interpersonal relationships and constructing  meaning  in  AI-mediated  environments.  The  developed framework successfully bridged social work theory with interdisciplinary  insights  to  provide  evidence-based  guidance  for  AI  implementation in social services.

Discussion: The  proposed  framework  offers  concrete  strategies  for social work education and provides research methodologies that center community voices. The analysis reveals how AI integration can be guided by evidence-based practice while maintaining focus on vulnerable population needs and democratic governance principles in social services.

Conclusion: This  work  provides  evidence-based guidance for practitioners to harness AI's potential while safeguarding social work's core values  of  human  dignity,  self-determination,  and  social  justice.  The framework includes policy recommendations for democratic governance of AI in social services and establishes a foundation for ethical AI deployment across all levels of social work practice.

## Introduction

The integration of artificial intelligence AI, defined as the simulation of human intelligence processes  by  machines,  especially  computer  systems  that  can  perform  tasks  requiring human-like perception, reasoning, and decision-making) into social work practice represents  a  paradigmatic  shift  that  challenges  fundamental  assumptions  about  professional judgment,  client  relationships,  and  social  justice  (Montag  and  Ali  2024;  F.  G.  Reamer,

<!-- image -->

<!-- image -->

## KEYWORDS

Social work practice; artificial intelligence ethics; evidencebased practice; human rights; democratic governance; algorithmic justice


<!-- PAGE 3 -->


<!-- image -->

2013.  As  AI  systems  increasingly  mediate  service  delivery,  assessment  processes,  and resource  allocation  across  child  welfare,  mental  health,  housing,  and  income  support programs,  social  workers  confront  unprecedented  ethical  dilemmas  that  existing  frameworks  inadequately  address  (Goldkind  &amp;  Wolf,  2015;  Heinlein  &amp;  Huchler,  2024).  The profession's historical commitment to human dignity, self-determination, and social justice demands  a  comprehensive  response  that  neither  uncritically  embraces  nor  reflexively rejects technological innovation (Alsaleh, 2024; F. Reamer, 2018).

This theoretical analysis synthesizes cutting-edge interdisciplinary research to propose an evidence-based framework that empowers social work practitioners, educators, and researchers  to  navigate  AI's  complexities  while  upholding  democratic  values  and  client  dignity. Drawing on social psychology (i.e., the scientific study of how individuals' thoughts, feelings, and behaviors are influenced by the actual, imagined, or implied presence of others), group dynamics theory (i.e., the study of processes that occur within and between groups, including formation,  structure,  interactions,  and  performance),  and  institutional  analysis  (i.e.,  the examination  of  how  formal  and  informal  institutions  shape  social  behavior,  organizational structures,  and  societal  outcomes),  we  develop  a  multidimensional  approach  that  addresses micro-level  practice  concerns,  meso-level  organizational  challenges,  and  macro-level  policy implications (Abrams &amp; der Pütten, 2020; Abulkassova et al., 2025; Montag &amp; Ali, 2024).

The  urgency  of  this  undertaking  cannot  be  overstated.  Algorithmic  systems  already determine  which  families  receive  child  protective  services  scrutiny,  which  individuals qualify for housing assistance, and which communities are targeted for intensive policing (Alsaleh,  2024;  Eubanks,  2018).  These  decisions  disproportionately  affect  marginalized populations who have historically experienced discrimination within social service systems (Heinlein &amp; Huchler, 2024; Longworth, 2021). Without proactive engagement from social work professionals grounded in evidence-based practice and human rights principles, AI risks amplifying existing inequities while obscuring them behind a veneer of technological neutrality (Montag &amp; Ali, 2024; Noble, 2018).

## Background: social work ethics and values in technological contexts

Social  work's  ethical  foundation,  codified  in  the  National  Association  of  Social  Workers (NASW) Code of Ethics, establishes six core values: service, social justice, dignity and worth of  the  person,  importance  of  human  relationships,  integrity,  and  competence  (National Association of Social Workers [NASW], 2021). These values provide crucial guidance for evaluating  AI  implementations,  yet  their  application  in  technological  contexts  requires careful interpretation and expansion (Goldkind et al., 2018; F. G. Reamer, 2013).

The  principle  of  human  dignity  demands  that  AI  systems  preserve  clients'  inherent worth and avoid reducing individuals to algorithmic categories or risk scores (F. Reamer, 2018). Self-determination requires that clients maintain meaningful control over decisions affecting  their  lives,  even  when  AI  systems  provide  recommendations  or  assessments (Garkisch  &amp;  Goldkind,  2025).  Social  justice  mandates  that  AI  implementations  reduce rather than perpetuate structural inequalities, requiring ongoing evaluation of differential impacts across communities (Nuwasiima et al., 2024).

The  value  of  human  relationships  takes  on  new  significance  in  AI-mediated  practice environments, where technology may either augment or undermine the therapeutic alliance central  to  social  work  practice  (Wretman  &amp;  Macy,  2016).  Competence  requires


<!-- PAGE 4 -->


<!-- image -->

practitioners to develop new skills in algorithmic literacy while maintaining their distinctive relational  expertise  (Haider,  2024).  Integrity  demands  honest  communication  about AI's limitations  and  potential  biases,  resisting  organizational  pressures  to  present  algorithmic assessments as neutral or infallible (F. G. Reamer, 2013).

## Method

This conceptual analysis employs literature review methodology to synthesize interdisciplinary  research  on  AI,  social  work  practice,  and  related  fields.  We  searched  multiple databases including Social Work Abstracts, PsycINFO, JSTOR, and ACM Digital Library using  terms  related  to  artificial  intelligence,  social  work,  ethics,  algorithmic  bias,  and human-computer interaction. The review encompassed peer-reviewed articles, books, and reports published between 2015 and 2025, with particular attention to social work-specific literature  and  interdisciplinary  research  relevant  to  social  service  contexts.  The  analysis integrates theoretical frameworks from social psychology (technology acceptance models), group dynamics (the I-C-E framework), and institutional analysis (socioecological systems theory)  to  develop  a  comprehensive  understanding  of  AI's  implications  for  social  work practice.  Case  examples  are  drawn  from  documented  implementations  in  child  welfare, mental health, and other social service domains to illustrate key concepts and challenges.

## Findings: theoretical foundations for AI-mediated social work practice

## Social psychology of human-AI interaction in social work contexts

Understanding how social workers and clients interact with AI systems requires examining the  psychological  mechanisms  that  shape  technology  acceptance,  trust  formation,  and decision-making  in  professional  contexts.  The  Technology  Acceptance  Model  (TAM), which  posits  that  perceived  usefulness  (i.e.,  the  degree  to  which  a  person  believes  that using a particular system would enhance their job performance) and perceived ease of use (i.e., the degree to which a person believes that using a particular system would be free of effort)  determine  an  individual's  intention  to  use  a  system,  and  its  extensions  provide crucial  insights  into  how  attitudes,  subjective  norms,  and  perceived  behavioral  control influence  social  workers'  willingness  to  integrate  technology  tools  into  practice  (Davis, 1989; Montag &amp; Ali, 2024). Research indicates that social workers' acceptance of technology depends  not  only  on  perceived  usefulness  and  ease  of  use  but  also  on  alignment  with professional values and ethical frameworks (Abulkassova et al., 2025; Berzin et al., 2015).

Cognitive  biases  significantly  impact  how  practitioners  interpret  and  act  upon  AIgenerated recommendations. Automation bias - the tendency to over-rely on automated systems - poses particular risks in high-stakes decisions about child removal or involuntary psychiatric  commitment  (Abrams  &amp;  der  Pütten,  2020;  Parasuraman  &amp;  Riley,  1997). A  documented  case  from  Allegheny  County's  child  welfare  system  supports  that  social workers  initially  showed  high  deference  to  the  Allegheny  Family  Screening  Tool's  risk scores, requiring additional training to maintain critical evaluation of algorithmic recommendations  (Vaithianathan  et  al.,  2019).  Conversely,  algorithm  aversion  may  lead  some practitioners to dismiss potentially valuable insights from data analysis, particularly when recommendations  conflict  with  clinical  intuition  (Dietvorst  et  al.,  2015;  Montag  &amp;  Ali,


<!-- PAGE 5 -->


<!-- image -->

2024).  These  biases  interact  with  professional  socialization  processes  that  traditionally emphasize  relationship-based  practice  and  holistic  assessment  (Berzin  et  al.,  2015; Heinlein &amp; Huchler, 2024).

The  concept  of  epistemic  injustice  illuminates  how  AI  systems  may  systematically discount certain forms of knowledge, particularly experiential wisdom from marginalized communities (Alsaleh, 2024; Fricker, 2007). When algorithms privilege quantifiable variables over narrative accounts, they risk perpetuating what Miranda Fricker terms testimonial injustice - the systematic discrediting of speakers from oppressed groups (Abulkassova et  al.,  2025;  Brito,  2021).  Social  workers must recognize how their own epistemic frameworks  shape  their  interpretation  of  AI  outputs  and  actively  resist  tendencies  to  treat algorithmic  assessments  as  more  objective  than  client  narratives  (Leroux  et  al.,  2025; Montag &amp; Ali, 2024).

## Group dynamics in multidisciplinary AI implementation teams

The  implementation  of  AI  in  social  service  organizations  involves  complex  negotiations  among  diverse  stakeholders  with  potentially  conflicting  interests  and  epistemologies. The I-C-E framework (i.e., Ingroup Identification, Cohesion, Entitativity)  offers  valuable  insights  into  how  implementation  teams  function  and dysfunction  (Abrams  &amp;  der  Pütten,  2020).  High  identification  with  professional ingroups  -  whether  social  workers,  IT  specialists,  or  administrators  -  can  create communication  barriers  that  impede  effective  collaboration  (Friedkin  &amp;  Johnsen, 2011;  Tajfel  et  al.,  2001).  Social  workers  may  perceive  AI  advocates  as  threatening professional  autonomy,  while  technologists  may  view  practitioner  concerns  as  resistance  to  progress  (Garkisch  &amp;  Goldkind,  2025;  Iacopini  et  al.,  2024;  Nuwasiima et  al.,  2024).

Team  cohesion,  while  generally  beneficial  for  group  performance,  can  paradoxically undermine ethical AI implementation when it suppresses dissenting voices. Research on groupthink in technology adoption reveals how pressure for consensus can lead teams to minimize risks, overlook biases, and dismiss community concerns (De Dreu et al., 2024; Janis &amp; Janis, 1982). In one documented case, a child welfare agency's AI steering committee, dominated by administrators focused on efficiency metrics, implemented a risk assessment tool despite social workers' warnings about cultural bias in its variables (Abulkassova et  al.,  2025).  The  resulting  system  flagged  poverty-related  factors  as  abuse  indicators, leading  to  disproportionate  investigations  of  low-income  families  (Eubanks,  2018; Heinlein &amp; Huchler, 2024).

Entitativity  -  the  perception  of  a  group  as  a  coherent  unit  -  influences  how  external stakeholders engage with AI implementation efforts (Abrams &amp; der Pütten, 2020; Campbell, 1958).  When  community  members  perceive  implementation  teams  as  unified  and impermeable,  they  may  feel  excluded  from  meaningful  participation  (Alsaleh,  2024; Birhane, 2021). Conversely, visible disagreements within teams can undermine stakeholder confidence in the implementation process (Friedkin &amp; Johnsen, 2011). Successful implementations require carefully managed transparency that acknowledges diverse perspectives while maintaining sufficient cohesion for effective action (Iacopini et al., 2024; Spinuzzi, 2005).


<!-- PAGE 6 -->


## Institutional structures and algorithmic governance

The deployment of AI in social work often occurs within institutional contexts characterized by resource constraints, regulatory requirements, and entrenched organizational cultures  that  shape  technology  adoption  patterns  (Abulkassova  et  al.,  2025;  DiMaggio  &amp; Powell,  1983).  Historical  analysis  reveals  how  path  dependencies  -  self-reinforcing sequences of decisions and investments - create institutional momentum that can override ethical concerns (Alsaleh, 2024; Pierson, 2000). Once agencies invest substantial resources in AI systems, switching costs and organizational inertia make course corrections difficult, even when problems emerge (Arthur, 1994; Heinlein &amp; Huchler, 2024).

Neoliberal  governance  logics  that  prioritize  efficiency,  standardization,  and  cost reduction  create  institutional  pressures  favoring  AI  adoption  regardless  of  impacts on service quality or client wellbeing (Harvey, 2007; Montag  &amp;  Ali, 2024). Performance  management  regimes  that  emphasize  quantifiable  outputs  incentivize agencies  to  adopt  systems  promising  improved  metrics,  even  when  these  metrics poorly  capture  social  work's  relational  and  transformative  dimensions  (Bevan  &amp; Hood,  2006;  Friedkin  &amp;  Johnsen,  2011).  The  contradiction  between  social  work's person-in-environment  perspective  and  AI's  tendency  toward  decontextualized  categorization  creates  ongoing  tensions  in  institutional  practice  (Greene,  2017;  Iacopini et  al.,  2024).

## Analysis: core components of a rights-based framework

Based on the theoretical foundations and documented challenges identified in the literature, we  propose  a  comprehensive  rights-based  framework  consisting  of  four  interconnected components:  (1)  Ethical  AI  Literacy  for  Practitioners,  (2)  Participatory  Governance Mechanisms, (3) Continuous Impact Assessment, and (4)  Community-Centered Advocacy. Each component addresses specific aspects of AI integration while reinforcing the others through feedback loops and shared principles.

## Component 1: ethical AI literacy for practitioners

Ethical  AI  literacy,  defined  as  the  ability  to  understand,  evaluate,  and  ethically  use  AI technologies  (Long  &amp;  Magerko,  2020),  encompasses  both  technical  understanding  and critical  consciousness  about  AI's  social  implications  (Haider,  2024;  Long  &amp;  Magerko, 2020). This competency includes the ability to evaluate AI systems' design, training data, and potential biases while  maintaining appropriate skepticism about algorithmic recommendations  (Raji  et  al.,  2020;  F.  G.  Reamer,  2013).  Practitioners  must  develop  skills  in 'algorithmic translation' - explaining complex systems in ways clients can understand and engage with critically (Alsaleh, 2024; Barocas et al., 2020). This component aligns with the NASW core values of integrity and competence.

Research on AI education in social work settings demonstrates the importance of integrating critical perspectives alongside technical knowledge (Haider, 2024). A pilot program incorporated case-based learning where students analyzed real-world examples of technology applications  in  social  services,  developing  both  analytical  skills  and  ethical  reasoning

<!-- image -->


<!-- PAGE 7 -->


<!-- image -->

capacities  (Berzin  et  al.,  2015).  Participants  showed  improved  ability  to  identify  potential harms and advocate for more equitable implementations (Berzin et al., 2015; Du et al., 2024).

## Component 2: participatory governance mechanisms

Meaningful  community  participation  in  AI  governance  requires  moving  beyond consultation  toward  genuine  power-sharing  in  decision-making  processes  (Birhane, 2021;  Costanza-Chock,  2020).  This  includes  community  representation  on  AI  steering  committees,  accessible  mechanisms  for  ongoing  feedback,  and  transparent  processes  for  addressing  concerns  (De  Dreu  et  al.,  2024;  Winner,  2017).  Participatory design methodologies from human-computer interaction provide valuable models for inclusive  technology  development  (Sanders  &amp;  Stappers,  2008;  Spinuzzi,  2005).  This component  aligns  with  the  NASW  core  values  of  dignity  and  worth  of  the  person and  social  justice.

The  algorithmic  accountability  movement  offers  concrete  examples  of  communitydriven  oversight  mechanisms.  In  New  York  City,  community  organizations  successfully advocated for the Automated Decision Systems Task Force, which required city agencies to publish  information  about  their  use  of  algorithmic  tools  (Engstrom  &amp;  Ho,  2020).  While implementation  faced  challenges,  the  effort  demonstrated  the  potential  for  democratic governance of AI systems.

## Component 3: continuous impact assessment

Traditional  evaluation  approaches  prove  insufficient  for  assessing  AI  systems'  complex,  evolving  impacts  on  individuals  and  communities  (Iacopini  et  al.,  2024;  Selbst et  al.,  2019).  Continuous  impact  assessment  requires  ongoing  monitoring  of  both intended  outcomes  and  emergent  effects,  with  particular  attention  to  differential impacts  across  social  groups  (Barocas  et  al.,  2020;  De  Dreu  et  al.,  2024).  This includes  tracking  how  AI  systems  shape  worker-client  relationships,  decisionmaking  processes,  and  broader  community  dynamics.  This  component  aligns  with the  NASW  core  values  of  service  and  social  justice.

Participatory action research (PAR) methodologies offer promising approaches for centering  affected  communities  in  evaluation  processes  (Gaventa  et  al.,  1991; Heinlein &amp; Huchler, 2024). A participatory action research (PAR) study of predictive analytics  in  child  welfare  involved  parents  with  prior  child  protective  services experience  as  co-researchers,  revealing  impacts  on  family  dynamics  and  selfperception  that  traditional  evaluation  methods  missed  (Roberts,  2022).  These  insider perspectives  proved  invaluable  for  understanding  AI's  phenomenological  and  relational  effects.

## Component 4: community-centered advocacy

Effective  advocacy  for  algorithmic  justice  requires  coalition-building  between  social workers,  affected  communities,  and  allied  professionals  (Alsaleh,  2024;  CostanzaChock,  2020).  This  involves  both  defensive  actions  to  prevent  harmful  implementations  and  proactive  efforts  to  shape  AI  development  toward  more  equitable  ends


<!-- PAGE 8 -->


<!-- image -->

(Longworth,  2021;  Montag  &amp;  Ali,  2024).  Social  workers'  professional  positioning provides  unique  opportunities  to  bridge  technical  and  community  perspectives  in advocacy  efforts.  This  component  aligns  with  the  NASW  core  values  of  service  and social  justice.

The  Stop  LAPD  Spying  Coalition  exemplifies  successful  community-centered  advocacy around AI systems. The coalition brought together affected residents, social workers,  lawyers,  and  technologists  to  challenge  the  Los  Angeles  Police  Department's predictive policing algorithms, ultimately forcing greater transparency and accountability  (Brayne,  2017).  Social  workers  contributed  clinical  insights  about  trauma  and  community dynamics that strengthened the coalition's arguments against surveillance-based approaches.

## Discussion: implications for social work practice, education, and research

## Practice implications

The proposed framework has significant implications for direct practice, requiring social workers to develop new competencies while maintaining their distinctive relational expertise.  Practitioners  must  learn  to  critically  evaluate  AI  recommendations,  understanding both technical limitations and potential biases while preserving space for clinical judgment and client voice (Goldkind et al., 2018; F. G. Reamer, 2013). This requires moving beyond binary  acceptance  or  rejection  of  AI  toward  nuanced  integration  that  preserves  human agency.

The concept of 'calibrated trust' becomes crucial in AI-mediated practice environments (Lee &amp; See, 2004). Social workers must develop appropriate skepticism that neither overrelies  on  algorithmic  assessments  nor  dismisses  potentially  valuable  insights  from  data analysis.  This  requires  ongoing  training  and  supervision  focused  on  ethical  decisionmaking in technological contexts (Berzin et al., 2015; Wretman &amp; Macy, 2016).

## Educational transformation

Social work education must undergo substantial transformation to prepare practitioners for AI-mediated  practice  environments  while  maintaining  the  profession's  core  values  and relational  focus  (Alsaleh,  2024;  Haider,  2024).  This  requires  more  than  adding  technical courses; it demands integrating critical AI literacy throughout the curriculum and fostering capacities  for  ongoing  adaptation  in  rapidly  changing  technological  landscapes  (Berzin et al., 2015; Montag &amp; Ali, 2024).

Foundational AI literacy should be integrated into human behavior, policy, and practice courses  rather  than  segregated  into  specialized  electives  (Abrams  &amp;  der  Pütten,  2020; Goldkind  &amp;  Wolf,  2015).  In  human  behavior  courses,  students  should  examine  how  AI systems  shape  identity  formation,  social  relationships,  and  cognitive  processes  (ArndCaddigan,  2015;  Friedkin  &amp;  Johnsen,  2011).  Policy  courses  must  address  algorithmic governance, digital  rights,  and  the  political  economy  of  AI  development  (Iacopini  et  al., 2024;  Winner,  2017).  Practice  courses  should  include  simulations  and  case  studies  that develop skills in critical evaluation of AI recommendations and ethical decision-making in technology-mediated contexts (De Dreu et al., 2024; F. Reamer, 2018).


<!-- PAGE 9 -->


<!-- image -->

Table 1. Recommendations for Artificial Intelligence in Social Work

<!-- image -->

| Area      | Recommendations                                                                                                                                                                                                                                                                                                                                                                                                                           |
|-----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Practice  | - Develop standardized protocols for evaluating AI recommendations that preserve space for clinical judgment and client voice - Implement regular supervision focused on ethical decision-making in AI-mediated environments - Create mechanisms for ongoing feedback from clients about their experiences with AI systems - Establish clear guidelines for when human override of algorithmic recommendations is appropriate             |
| Education | - Integrate AI literacy across core curriculum areas rather than treating it as a specialized elective - Develop case-based learning materials that address real-world examples of algorithmic bias in social services - Create field placement opportunities that expose students to both exemplary and problematic AI implementations - Establish partnerships with computer science programs to develop interdisciplinary competencies |
| Research  | - Prioritize participatory methodologies that center affected communities in AI evaluation - Develop longitudinal studies that track AI systems' evolving impacts over time - Create collaborative relationships with computer scientists to develop social work-informed fairness metrics - Establish ongoing monitoring systems for AI implementations in social service settings                                                       |
| Policy    | - Advocate for algorithmic accountability measures that require transparency and community participation - Support public alternatives to corporate AI systems in social services - Push for regulatory frameworks that address AI's broader social impacts beyond narrow technical requirements - Link AI governance to broader struggles for economic and racial justice                                                                |

## Research methodologies for algorithmic justice

Advancing  evidence-based  practice  in  AI-mediated  social  work  requires  developing research  methodologies  that  can  capture  the  complex,  multidimensional  impacts  of algorithmic  systems  on  individuals,  communities,  and  society  (Iacopini  et  al.,  2024; Selbst  et  al.,  2019).  Traditional  outcome  studies,  while  necessary,  prove  insufficient for  understanding  how  AI  transforms  power  relations,  shapes  subjectivities,  and reconfigures  the  possibilities  for  human  agency  and  social  change  (Leroux,  2025; De  Dreu  et  al.,  2024).

Algorithmic  auditing  methodologies  must  move  beyond  technical  metrics  of accuracy and efficiency to examine  differential impacts across social groups (Alsaleh,  2024;  Raji  et  al.,  2020).  This  requires  accessing  not  only  aggregate  performance  data  but  also  granular  information  about  how  algorithms  treat  different communities  (Barocas  et  al.,  2020;  Montag  &amp;  Ali,  2024).  Researchers  should  collaborate  with  computer  scientists  to  develop  'fairness  metrics'  that  reflect  social  work values,  recognizing  that  technical  definitions  of  fairness  often  conflict  and  require ethical  judgment  about  trade-offs  (Abulkassova  et  al.,  2025;  Selbst  et  al.,  2019). Future  research  should  focus  on  developing  and  testing  specific  tools  and  protocols for  implementing  the  proposed  framework.  This  includes  creating  validated  instruments  for  assessing  AI  literacy  among  social  workers,  developing  metrics  for  evaluating  community  participation  in  AI  governance,  and  establishing  evidence-based guidelines  for  ethical  AI  integration  in  various  practice  settings.  Additionally,  comparative research across different cultural and political contexts could illuminate how the  framework  might  be  adapted  for  diverse  global  settings.

Based  on  this  analysis,  we  propose  specific  recommendations  for  social  work practice,  education,  research,  and  policy.  These  recommendations  are  summarized in  Table  1.


<!-- PAGE 10 -->


## Limitations

This theoretical analysis has several limitations that suggest directions for future research. First, the framework proposed here requires empirical testing in real-world implementation contexts to assess its effectiveness and feasibility. Second, the analysis draws primarily on literature  from  Western  contexts,  limiting  its  applicability  to  diverse  global  settings  with different cultural values and governance structures. Third, the rapid pace of AI development means that specific technological capabilities and challenges may evolve faster than theoretical frameworks can accommodate.

## Conclusion

The integration of AI into social work practice presents both existential challenges and transformative  opportunities  for  the  profession.  This  analysis  has  demonstrated  how evidence-based approaches grounded in social psychology, group dynamics, and institutional  analysis  can  guide  social  workers  in  navigating  this  complex  terrain  while maintaining  commitment  to  human  rights  and  social  justice.  The  proposed  rightsbased framework offers concrete guidance for practitioners, educators, and researchers to  engage  proactively  with  AI  while  preserving  social  work's  essential  humanistic values.

The path forward requires neither uncritical acceptance nor wholesale rejection of AI, but rather engaged critique and creative adaptation that centers human dignity and community voice. Social workers at all levels - from direct practitioners to policy advocates have crucial roles to play in ensuring that AI serves human flourishing rather than replacing human judgment and relationships. The profession's  commitment  to  starting  where  the client  is  must  now  encompass  understanding  where the algorithm is  and  advocating for technologies that meet people and communities in their full complexity.

As we stand at this historical inflection point, social work's response to AI will shape not only the profession's future but also the kind of society we create together. By grounding our approach in evidence, ethics, and unwavering commitment to human dignity, social workers  can  help  steer  AI  toward  more  just  and  life-affirming  ends.  The  challenge  is immense, but so is the opportunity to demonstrate social work's essential contribution to human welfare in the digital age.

## Disclosure statement

No potential conflict of interest was reported by the author(s).

## Funding

The author(s) reported there is no funding associated with the work featured in this article.

## References

Abrams, A. M., &amp; der Pütten, A. M. R. V. (2020). I-C-E framework: Concepts for group dynamics research  in  human-robot  interaction:  Revisiting  theory  from  social  psychology  on  ingroup


<!-- PAGE 11 -->


<!-- image -->

- identification (I), cohesion (C) and entitativity (E). International Journal of Social Robotics , 12 (6), 1213-1229. https://doi.org/10.1007/s12369-020-00642-z
- Abulkassova,  D.,  Muldasheva,  G.,  Nurtazin,  M.,  Tleukhanov,  N.,  &amp;  Kuspanova,  A.  (2025).  The phenomenon  of  artificial  intelligence  in  modern  transformational  socio-cultural  processes: Socio-philosophical analysis. AI &amp; Society , 1-11. https://doi.org/10.1007/s00146-025-02195-z
- Alsaleh,  A.  (2024).  The  impact  of  technological  advancement  on  culture  and  society. Scientific Reports , 14 (1), 32140. https://doi.org/10.1038/s41598-024-83995-z
- Arnd-Caddigan, M. (2015). Sherry Turkle: Alone together: Why we expect more from technology and less from each other . Basic Books.
- Arthur, W. B. (1994). Increasing returns and path dependence in the economy . University of Michigan Press.
- Barocas,  S.,  Hardt,  M.,  &amp;  Narayanan,  A.  (2020).  Fairness  and  machine  learning. Recommender Systems Handbook , 1 , 453-459. https://fairmlbook.org/pdf/fairmlbook.pdf
- Berzin, S. C., Singer, J., &amp; Chan, C. (2015). Practice innovation through technology in the digital age: A grand challenge for social work. American Academy of Social Work &amp; Social Welfare , 12 , 3-21. https://grandchallengesforsocialwork.org/wp-content/uploads/2015/12/WP12-with-cover. pdf
- Bevan, G., &amp; Hood, C. (2006). What's measured is what matters: Targets and gaming in the English public health care system. Public Administration , 84 (3), 517-538. https://doi.org/10.1111/j.14679299.2006.00600.x
- Birhane, A. (2021). Algorithmic injustice: A relational ethics approach. Patterns , 2 (2), 100205. https:// doi.org/10.1016/j.patter.2021.100205
- Brayne, S. (2017). Big data surveillance: The case of policing. American Sociological Review , 82 (5), 977-1008. https://doi.org/10.1177/0003122417725865
- Brito,  M.  (2021).  The  routledge  handbook  of  epistemic  injustice. Essays  in  Philosophy , 22 (1/2), 147-150. https://doi.org/10.5840/eip2021221/214
- Campbell, D. T. (1958). Common fate, similarity, and other indices of the status of aggregates of persons as social entities. Behavioral Science , 3 (1), 14. https://doi.org/10.1002/bs.3830030103
- Costanza-Chock, S. (2020). Design justice: Community-led practices to build the worlds we need . The MIT Press.
- Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS Quarterly , 13 (3), 319-340. https://doi.org/10.2307/249008
- De Dreu, C. K., Gross, J., &amp; Romano, A. (2024). Group formation and the evolution of human social organization. Perspectives  on  Psychological  Science , 19 (2),  320-334.  https://doi.org/10.1177/ 17456916231179156
- Dietvorst, B. J., Simmons, J. P., &amp; Massey, C. (2015). Algorithm aversion: People erroneously avoid algorithms after seeing them err. Journal of Experimental Psychology General , 144 (1), 114. https:// doi.org/10.1037/xge0000033
- DiMaggio, P.  J.,  &amp;  Powell,  W.  W.  (1983).  The  iron  cage  revisited:  Institutional  isomorphism  and collective rationality in organizational fields. American Sociological Review , 48 (2), 147-160. https:// doi.org/10.2307/2095101
- Du, H., Sun, Y., Jiang, H., Islam, A. Y. M., &amp; Gu, X. (2024). Exploring the effects of AI literacy in teacher  learning:  An  empirical  study. Humanities  and  Social  Sciences  Communications , 11 (1), 1-10. https://doi.org/10.1057/s41599-024-03101-6
- Engstrom,  D.  F.,  &amp;  Ho,  D.  E.  (2020).  Algorithmic  accountability  in  the  administrative  state. Yale Journal  on  Regulation , 37 ,  800.  https://heinonline.org/HOL/Page?handle=hein.journals/yjor37&amp; d i v = 2 2 &amp; g \_ s e n t = 1 &amp; c a s a \_ t o k e n = p 6 k W 4 b C m 7 m A A A A A A : l V \_ xpHpIYkLwxPxYonwv9MYPMOMiYN1qiM4T4dUu82Qzs9JJcuFXjLqGivvzsn1vC5soJeVoTQ&amp;collection=journals
- Eubanks, V. (2018). Automating inequality: How high-tech tools profile, police, and punish the poor . St. Martin's Press.
- Fricker, M. (2007). Epistemic injustice: Power and the ethics of knowing . Oxford University Press.
- Friedkin, N. E., &amp; Johnsen, E. C. (2011). Social influence network theory: A sociological examination of small group dynamics (Vol. 33). Cambridge University Press.


<!-- PAGE 12 -->


<!-- image -->

- Garkisch, M., &amp; Goldkind, L. (2025). Considering a unified model of artificial intelligence enhanced social  work:  A  systematic  review:  M.  Garkisch  and  L.  Goldkind. Journal  of  Human  Rights  and Social Work , 10 (1), 23-42. https://doi.org/10.1007/s41134-024-00326-y
- Gaventa,  J.,  Fals-Borda,  O.,  &amp;  Rahman,  M.  D.  A.  (1991). Action  and  knowledge:  Breaking  the monopoly with participatory action research .
- Goldkind, L., &amp; Wolf, L. (2015). A digital environment approach: Four technologies that will disrupt social work practice. Social Work , 60 (1), 85-87. https://doi.org/10.1093/sw/swu045
- Goldkind, L., Wolf, L., &amp; Freddolino, P. P. (Eds.). (2018). Digital social work: Tools for practice with individuals, organizations, and communities . Oxford University Press.
- Greene, R. (2017). Human behavior theory and social work practice . Routledge.
- Haider,  S.  (2024).  Exploring  opportunities  and  challenges  of  artificial  intelligence  in  social  work education. The Routledge International Handbook of Social Work Teaching ,  46-62. https://www. taylorfrancis.com/chapters/edit/10.4324/9781003422402-5/exploring-opportunities-challengesartificial-intelligence-social-work-education-sharif-haider
- Harvey, D. (2007). A brief history of neoliberalism . Oxford University Press.
- Heinlein, M., &amp; Huchler, N. (2024). Artificial intelligence in society . Springer.
- Iacopini,  I.,  Karsai,  M.,  &amp;  Barrat,  A.  (2024).  The  temporal  dynamics  of  group  interactions  in higher-order  social  networks. Nature  Communications , 15 (1),  7391.  https://doi.org/10.1038/ s41467-024-50918-5
- Janis, I. L., &amp; Janis, I. L. (1982). Groupthink: Psychological studies of policy decisions and fiascoes (Vol. 349). Houghton Mifflin.
- Lee,  J.  D.,  &amp;  See,  K.  A.  (2004).  Trust  in  automation:  Designing  for  appropriate  reliance. Human Factors , 46 (1), 50-80. https://doi.org/10.1518/hfes.46.1.50.30392
- Leroux, J., &amp; D'Ignazio, C., &amp; Klein, L. F. (2025). (2020) data feminism . MIT Press.
- Long, D., &amp; Magerko, B. (2020, April). What is AI literacy? Competencies and design considerations. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems , 1-16. https://dl. a c m . o r g / d o i / p d f / 1 0 . 1 1 4 5 / 3 3 1 3 8 3 1 . 3 3 7 6 7 2 7 ? c a s a \_ t o k e n = CYblY6xMLDgAAAAA:2EwV1lbT94xDZVbhuykOGre3ANA0OR0PTbFpnKIEnEzpKhAVS6l4z gfdg-G-9PGptxx83dwUK2E-Kw
- Longworth, J. (2021). Benjamin Ruha (2019) race after technology: Abolitionist tools for the New Jim code. Medford: Polity Press. 172 pages eISBN: 9781509526437. Science and Technology Studies , 34 (2), 92-94. https://doi.org/10.23987/sts.102639
- Montag, C., &amp; Ali, R. (2024). The impact of artificial intelligence on societies . Springer. https://doi.org/ 10.1007/978-3-031-70355-3
- National Association of Social Workers. (2021). NASW code of ethics . NASW Press.
- Noble, S. U. (2018). Algorithms of oppression: How search engines reinforce racism. In Algorithms of oppression . New York University Press. https://www.academia.edu/43472000/Big\_data\_et\_la\_num %C3%A9risation\_de\_l\_exclusion\_sociale\_Review\_of\_the\_book\_Algorithms\_of\_Oppression\_ How\_Search\_Engines\_Reinforce\_Racism
- Nuwasiima,  M.,  Ahonon,  M.  P.,  &amp;  Kadiri,  C.  (2024).  The  role  of  artificial  intelligence  (AI)  and machine learning in social work practice. World Journal of Advanced Research &amp; Reviews , 24 (1), 80-97. https://doi.org/10.30574/wjarr.2024.24.1.2998
- Parasuraman, R., &amp; Riley, V. (1997). Humans and automation: Use, misuse, disuse, abuse. Human Factors , 39 (2), 230-253. https://doi.org/10.1518/001872097778543886
- Pierson,  P.  (2000).  Increasing  returns,  path  dependence,  and  the  study  of  politics. The  American Political Science Review , 94 (2), 251-267. https://doi.org/10.2307/2586011
- Raji,  I.  D.,  Smart,  A.,  White,  R.  N.,  Mitchell,  M.,  Gebru,  T.,  Hutchinson,  B.,  &amp;  Barnes,  P.  (2020, January).  Closing  the  AI  accountability  gap:  Defining  an  end-to-end  framework  for  internal algorithmic  auditing. Proceedings  of  the  2020  Conference  on  Fairness,  Accountability,  and Transparency , 33-44. https://dl.acm.org/doi/pdf/10.1145/3351095.3372873
- Reamer, F. (2018). Social work values and ethics . Columbia University Press.
- Reamer, F. G. (2013). Social work in a digital age: Ethical and risk management challenges. Social Work , 58 (2), 163-172. https://doi.org/10.1093/sw/swt003


<!-- PAGE 13 -->


<!-- image -->

- Roberts, D. (2022). Torn apart: How the child welfare system destroys Black families-and how abolition can build a safer world . Basic Books.
- Sanders, E. B. N., &amp; Stappers, P. J. (2008). Co-creation and the new landscapes of design. Co-Design , 4 (1), 5-18. https://doi.org/10.1080/15710880701875068
- Selbst, A. D., Boyd, D., Friedler, S. A., Venkatasubramanian, S., &amp; Vertesi, J. (2019, January). Fairness and abstraction in sociotechnical systems. Proceedings of the Conference on Fairness, Accountability, and Transparency , 59-68. https://dl.acm.org/doi/pdf/10.1145/3287560.3287598
- Spinuzzi,  C.  (2005).  The  methodology  of  participatory  design. Technical  Communication , 52 (2), 163-174.
- Tajfel,  H.,  Turner,  J.,  Austin,  W.  G.,  &amp;  Worchel,  S.  (2001).  An  integrative  theory  of  intergroup conflict. Intergroup  Relations:  Essential  Readings ,  94-109.  https://books.google.com/books?hl= en&amp;lr=&amp;id=uk8AFpo3BnYC&amp;oi=fnd&amp;pg=PA94&amp;dq=tajfel+turner+austic+2001+an+integrati ve&amp;ots=qNI60UhxAe&amp;sig=V1AawhHMWEoEagG8giCvtXvPaUM#v=onepage&amp;q=tajfel% 20turner%20austic%202001%20an%20integrative&amp;f=false
- Vaithianathan, R., Kulick, E., Putnam-Hornstein, E., &amp; Benavides-Prado, D. (2019). Allegheny family screening  tool:  Methodology,  version  2. Center  for  Social  Data  Analytics ,  1-22.  https://www. alleghenycountyanalytics.us/wp-content/uploads/2019/05/Methodology-V2-from-16-ACDHS-26\_PredictiveRisk\_Package\_050119\_FINAL-7.pdf
- Winner, L. (2017). Do artifacts have politics? In Computer ethics (pp. 177-192). Routledge. https:// nparikh.org/assets/pdf/sipa6545/week6-rights-responsible/digital-rights/artifacts-politics.pdf
- Wretman, C. J.,  &amp;  Macy, R.  J.  (2016).  Technology  in  social  work  education:  A  systematic  review. Journal of Social Work Education , 52 (4), 409-421. https://doi.org/10.1080/10437797.2016.1198293