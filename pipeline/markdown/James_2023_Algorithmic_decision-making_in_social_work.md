---
source_file: James_2023_Algorithmic_decision-making_in_social_work.pdf
conversion_date: 2026-02-03T18:31:14.405439
converter: docling
quality_score: 95
---

<!-- PAGE 1 -->
<!-- image -->

## Social Work Education

The International Journal

ISSN: 0261-5479 (Print) 1470-1227 (Online) Journal homepage: www.tandfonline.com/journals/cswe20

## Algorithmic decision-making in social work practice and pedagogy: confronting the competency/critique dilemma

Paul James, Jason Lal, Ashley Liao, Liam Magee &amp; Karen Soldatic

To cite this article: Paul James, Jason Lal, Ashley Liao, Liam Magee &amp; Karen Soldatic (2024) Algorithmic decision-making in social work practice and pedagogy: confronting the competency/critique dilemma, Social Work Education, 43:6, 1552-1569, DOI: 10.1080/02615479.2023.2195425

To link to this article:

https://doi.org/10.1080/02615479.2023.2195425

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

© 2023 The Author(s). Published by Informa UK Limited, trading as Taylor &amp; Francis Group.

Published online: 27 Mar 2023.

Submit your article to this journal

Article views: 4278

- [x] View related articles

View Crossmark data

Citing articles: 7 View citing articles

曲

CrossMark

<!-- image -->


<!-- PAGE 2 -->


<!-- image -->

<!-- image -->

<!-- image -->

## Introduction

Face-to-face  engagement has long been fundamental to social work practice. Not dissimilar to the habeas corpus principle in law, being present 'in the room' with a person who needs support has been important to positive interpretatively based care. However, across the course of this century, the use of mediating technologies and techniques have increasingly  confronted  social  workers  with  questions  concerning  communication, engagement, privacy, risk-analysis, decision-making and pedagogy. The dilemma here concerns the enhanced capacity of mediating technologies to provide time-saving support and the way it comes to replace the unmediated engagement of persons who do not have sufficient time. Debates about cyber-counseling and digital engagement, for example,  began  in  the  1990s  with  concerns  about  the  interactive  space  being  filled  by

CONTACT

Paul James

<!-- image -->

Sydney, Sydney, Australia

© 2023 The Author(s). Published by Informa UK Limited, trading as Taylor &amp; Francis Group.

This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial-NoDerivatives License (http://creativecommons.org/licenses/by-nc-nd/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited, and is not altered, transformed, or built upon in any way. The terms on which this article has been published allow the posting of the Accepted Manuscript in a repository by the author(s) or with their consent.

paul.james@westernsydney.edu.au

<!-- image -->

Institute of Culture and Society, University of Western


<!-- PAGE 3 -->


<!-- image -->

'hardware' (Robson &amp; Robson, 1998). Around the same time, predictive risk-assessment techniques began to be added to the repertoire of social work tools, and debates have abounded since about their efficacy (Cuccaro-Alamin et al., 2017). As the subsequent literature has explored at length, mediating technologies can have a range of effects, from being  used  sensitively  to  enhance  the  interpretative  frame  of  existing  engagement  (La Mendola,  2010)  to  being  deployed  in  ways  that  diminish  meaningful  connection  by substantially replacing face-to-face engagement or bypassing the immediacy of involved interpretation in risk-assessment or decision-making.

All of this has intensified with the introduction of algorithmic decision-making (ADM) enhanced by artificial intelligence (AI)-the subject of this article. Based upon (sometimes presumptive) claims that difficulties and errors can be created in human welfare through individual professional decision-making about prioritization of needs and identification of risks, government welfare services around the world are in the early stages of designing and implementing sophisticated automated assessment regimes. These systems aim, firstly, to determine the level of need  or risk;  second,  to  rank  individual  needs  or  risk-levels,  and third, to prioritize the release of scarce resources to address the most critical needs or to recommend the most appropriate interventions to mitigate risk.

Algorithmic decision-making encompasses both simple rule-based systems and complex Artificial Intelligence (AI) systems. A simple system will provide a recommended or mandated course of action given known variables: for example, if a family has an income below a certain threshold, they are unable to pay rent, and therefore may not be able to provide  a  safe  environment  for  a  child,  child-protection  might  be  recommended. Complex  AI  systems,  trained  on  large  data  sets,  tend  to  make  more  probabilistic inferences based on multiple variables. In the same example of child-safety, the outcome might be determined by a much wider range of influences, including potential community support, schooling standards, availability of public health services-all of which can be  included  in  training  sets  to  produce  apparently  more  'accurate'  determinations  of issues like child-safety.

Social work options have long included a problematic 'by-the-numbers' proceduralism, but the balance is shifting more strongly in that direction. As will be documented in the case studies below, unlike earlier decision-making systems, ADM, by being based on big  data  and  compounding  variables  weighted  by  AI  established  values,  has  become a  large  black  box  that  engenders  increasing  automaticity.  This  shift  has  itself  been ushered in by a series of contradictory pressures: the neoliberal imperative to streamline services, excitement about the new communications platforms and other technological possibilities, and burgeoning demand for social support in a world of increasing inequality and precarity. In some cases, not unlike telehealth, algorithmic automation is (ironically) promoted as a way of avoiding the limits of prior forms of technical or institutional mediation (Cuccaro-Alamin et al., 2017).

This shift has major consequences for the presence/mediation tension. It means for example that for all the care that a theorist such as Walter La Mendola (2010) takes in extending the notion of 'social presence' to include mediated communications, the ADM and  AI  has  returned  us  quickly  to  reasserting  the  importance  of  embodied  presence. LaMendola could not have anticipated AI-generated voice, print, and video simulations of human presence. ADM brings new dilemmas of similar kind. On the one hand, much social work pedagogy and practice has sought to foreground the rights and capacities of


<!-- PAGE 4 -->


<!-- image -->

individuals or to be self-determining and making decisions affecting their future. This is sometimes called client-centered practice. (Here, it should be noted that we use the term 'client'  advisedly-for  all  its  weaknesses,  it  at  least  gives  a  neutral  sense  of  a  person seeking or being offered professional support). On the other hand, the introduction of partly  or  fully  automated  decision-making  has  contributed  to  a  reframing  and  a  recentering  of  who  and  where  decisions  are  made  in  social  work  contexts.  Systems frequently mediate between social workers and clients either to make decisions, set the terms for those decisions, or provide background evidence to influence those decisions

Without adequate training and retraining, social workers are being caught between unanticipated tensions. On the one hand, as earlier studies on technology-adoption have found, sometimes embodied and relatively unmediated engagement is prioritized for the wrong  reasons  (Ramsey  &amp;  Montgomery,  2014).  For  example,  limited  understanding about the technical and technological complexities of different systems or easy negative presumptions about the consequences of mediating technologies in practice have been too quickly added to well-founded concerns about the inter-relational, legal and ethical challenges associated with remote or mediated social work. On the other hand, because algorithmic technologies facilitate more regularized decision-making (a potentially positive outcome) and because the current generation of social workers have grown up with widening  forms  of  technological  meditation  such  as  social  media,  the  propensity  of technologies to supplant the interpretative and face-to-face care functions of the social worker (a deeply problematic outcome), is bypassed or seen as part of the 'progressive' course of things.

The pedagogical anthology Digital  Social  Work provides a telling but unintentional example of this normalizing account. The editors acknowledge some of the important debates and long-held concerns about the technologizing process, while also accepting that it is necessary: 'In this book you will find real world examples of how technology can enhance the familiar components of social world practice' (Goldkind et al., 2019, p. 1). And, indeed, the authors of the present article face the same dilemma, although we are less  sanguine  about  resolving  them  in  the  field  without  supportive  pedagogies.  The tensions between embodied and mediated care remain abiding; they require continuing negotiation,  and  they  require  a  renewed  sensitivity  to  critically  engaged  and  practiceembedded teaching methods. Hence, the case study method in this article gives examples that show that the tension is not resolvable just through enhanced guidelines for social work practice.

Some commentators have rightly moved to elaborate a series of principles beyond the usual technocratic 'standards' and 'thresholds' that might enhance the positive potential of ADM (Gillingham, 2019). This is an important step, but we suggest that is only part of a  comprehensive  response.  Social  work  education,  we  argue,  increasingly  needs  to grapple with the tensions between, on the one hand, training future social workers for a technologically driven professional environment, and, on the other, engaging students in  critical  theories  that  acknowledge  the  unequal  distribution  of  social  power  that technological change both emerges from and reinscribes at a more abstract level. Even the  conventional  notion  of  'digital  literacy'  as  the  capacity  to  understand  and  operate digitally needs to be questioned (Bawden, 2008). Is it competency in technology or the capacity to adequately critique its framing use, or both and more? Should social work training  also  provide  adequate  tools  for  either  pushing  back  against  the  intrusion  of


<!-- PAGE 5 -->


<!-- image -->

algorithmic  decision-making  or  re-negotiating  its  framing?  Should  it  teach  a  form  of critical pragmatics that allows social workers to support their clients both through and in tension with this technological framing? Here we use the concept of 'digital literacy' in its fullest sense to include technical competency and critical understanding of the technical implications and social  form  of  such  digital  tools  as  they  contribute  to  making  and remaking our world.

Thus, in addition to the presence/mediation tension, the discipline of social work is also  faced with this technique/critique tension. Is social work education providing the conditions for both digital literacy and the critical capacity to overcome challenges that occur  through  effectively  mandated  technology-use?  Earlier  versions  of  this  question about  whether  social  work  students  are  properly  equipped  to  perform  their  role  in a  changing  world  are  long-standing  (Parton,  2008),  but  it  has  recently  taken  on  new dimensions such as dealing with the everyday use of social media (Dumitrescu, 2020). On top of that, the weaving of a critical recognition of the technique/critique tension into the social work curriculum faces barriers that parallel the terms of that tension. For example, professional  academic  resistance  to  teaching  techniques  for  uncritically  working  with these  technologies  is  sometimes  exacerbated  by  the  increasing  mandate  within  statefunded programs to use technological mediation-a mandate accelerated by restrictions on face-to-face engagement during COVID lockdowns. At the same time, professional limits of technical knowledge (in terms of both use and critical assessment) are likely to be more pronounced in academics trained in critical social work methods prior to the normalization of the new social service technologies.

The field of social-work education is now faced with the daunting task of adaptation, as technology-use in many places becomes mandatory as a directive of funding, both for the services in which social workers are employed and to determine access and resourcing for individual clients. For all that the field of social work is characterized by frequent innovation in field practice to meet the problems faced by society, these dilemmas go to the heart of discipline itself. Thus, the challenges of digital literacy go much further than Ana Maria Dumitrescu's (2020) argument that social work education needs to come to terms with a world of social media and digital communication.

This article explores the ways that algorithmic decision-making has become increasingly embedded in social work practice. In the first part of the article, through three case studies  we  show  the  varied  ways  that  automated  systems  operate  across  areas  that intersect  with  social  work,  including  health,  policing,  corrections  and  welfare.  The ADM systems that  we  explore  include  the  Allegheny  Family  Screening  Tool  (AFST), Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), and the Australian National Disability Insurance Scheme (NDIS). Through an examination of these  case  studies,  we  aim  to  provide  an  understanding  of  how  ADM  might  work positively by being treated as a technical tool, not a replacement for critical interpretation. Then in the second part of the article we turn to show how social work curriculum might be enhanced by attending reflexively to the two problem-areas we have identified: the  tensions  between  technical  training  and  critical  interpretative  education;  and  the tensions between the mediation of technologies and the presence of social work practitioners in the field. Thus far, social work pedagogy has largely failed to integrate the kinds of  training  required  to  technically  understand,  use, and critique ADM systems. While acknowledging  the  difficulties  associated  with  adding  more  to  an  already  overloaded


<!-- PAGE 6 -->


<!-- image -->

undergraduate  curriculum,  we  argue  that  the  training  of  social  workers  in  both  the technical and critical disciplines is essential to ensure future social work practice, with all its limits, can adequately address diverse client needs. Here two further case studies are used here to show how the pedagogical principles might be developed: the Hierarchical Ensembling-based Agent which plans for Effective Reduction in HIV Spread (HEALER), and Heat List (or Strategic Subjects List used in policing).

In summary, this article argues that in order to address dilemmas of presence/mediation and technique/critique, social work education must aim to educate students about the  ethical,  practical  and  technical  challenges  of  access,  privacy,  competencies,  differences in capacities, changing cultural orientations, and the creation of risk, particularly for vulnerable populations-all of which can be exacerbated through the technologization of social provisioning and social protection systems. In each part of the argument case studies are important. The first part of this article works through a series of case studies  to  underline  the  developing  complexities,  and  the  second  part  turns  to  case studies as useful for social work pedagogy. We argue that the introduction of criticaltechnical educational changes needs to not only meet the digital competency mandate within  relevant  social  work  associations,  but  also  ensure  that  graduate  social  workers have,  through  exposure  to  such  fields  as  the  sociology  of  science  and  technology, developed  capacities  to  critically  understand  the  social  implications  of  technological mediation.

## Algorithmic decision-making in practice?

We do not  want  to  overstate  the  generality  of  the  use  of  these  algorithmic  decisionmaking  processes.  At  this  stage  there  are  mostly  state  or  locally  based  examples  of ongoing implementation of these systems in social welfare. One national system designed to  facilitate  automatic  recovery  of  welfare  overpayments,  formally  called  'Online Compliance  Intervention',  was  a  tragic  failure,  ruining  lives.  'Robodebt',  as  it  became called, led to a massive class action forcing the Australia Government to pay back over $800  million  to  394,000  welfare  recipients.  We  are  leaving  this  out  of  our  discussion because it is too easily criticized.

We  proceed  instead  with  some  cases  which  suggest  that  drawing  upon  extensive algorithmically mediated data has mixed consequences, positive and negative. Carefully administered as a support tool rather than framing system, some writers have suggested, for example, that ADM can temper the over-influence of negative forms of relational or face-to-face  engagement  (Glaberson,  2019).  Here  the  general  notion  of  'relational engagement' refers to the use by a practitioner of interpersonal skills and beliefs about structures  and  contexts  to  create  a  human  connection  between  the  practitioner,  the situation and the client (Healy &amp; Hampshire, 2002). Relational engagement at its best brings together diagnostic and empirical knowledge with care and personal knowledge. It can give the practitioner, and the client, negotiated decision-making agency (Esser et al., 2016). However, it can also contribute to further marginalization or create barriers to the effective engagement of certain groups of people accessing social services. For example, social  workers may enable or restrict children's choices based on their perspectives of certain children's needs (Jensen, 2020). As a group, the case studies below suggest that ADM, operating in a qualified and qualifying way ,  can make some positive differences,


<!-- PAGE 7 -->


<!-- image -->

for example drawing attention to professional bias against marginalized groups. That is, while  there  is  no  assurance  that  ADM  will  remove  practices  of  racism,  disablism, heterosexism, and misogyny in the allocation of resources or assessment of risk (in fact it  can  introduce  new  structural  biases),  used  reflexively  it  can  draw  attention  to  such biases. At least that is the theory when the process is used with transparency and care. As some writers suggest, a 'thin agency' could be created with digital discretion for clients that, in some contexts, allows good decisions to be made (Klocker, 2007; Zouridis et al., 2020).  An  example  of  this  can  be  seen  through  the  Allegheny  Family  Screening  Tool (AFST).

## The Allegheny Family Screening Tool (AFST)

The Allegheny County Department of Human Services (DHS) in the United States has been using the AFST since 2016 as a way of informing their decision-making process in child-protection contexts. The AFST is a predictive risk-modeling tool based on specially designed algorithms that analyses large sets of data from different sources of information housed in the DHS Data Warehouse. When social services receive information about a possible situation of precarity or mistreatment, the algorithm calculates the risk against a specified index in relation to the specificity of the case by cross-analysis of more than a hundred parameters such as criminal history, drug-use, mental illness and any previous history of child abuse of or by parents, guardians or people who live in the child's home. These tools are used for risk assessments that indicate the likelihood of future maltreatment and also used for evaluation of services-estimating the outcomes of interventions (James et al., 2019; Russell, 2015).

The  calculation  of  risk,  based  on  the  cross-analysis  of  data,  produces  a  'Family Screening Score', which is an indicator of the long-term probability of future involvement of  the  child  within  the  child  welfare  system.  Scores  in  high  thresholds  result  in a  mandatory interpretative screening of the data raised. In the instances of lower-risk scores,  the  information  gathered  to  calculate  the  score  does  not  substitute  for  the judgment  of  child-protection  practitioners  but  instead  adds  further  information  to inform the decision-making process. The combination of the data associated with the Family  Screening  Score  with  other  information  classically  used  in  child-protection settings  leads  to  a  more  comprehensive  understanding  of  the  risk  posed  to  the  child. The evidence to emerge from studies suggests that it provides greater 'accuracy' in case detection .

Support through broader and combined data can also be beneficial in instances where social workers find themselves in complex environments in which they have insufficient experience. An example of this can be seen through practitioners working with children with a disability in child-protection settings. Taylor et al. (2015) found practitioners can tolerate a higher range of what might be classified as 'abuse' when working with children with a disability compared to non-disabled children. This is suggested to occur because practitioners, firstly, have difficulty in identifying abuse because they had limited experience  of  children  with  disabilities.  Second,  practitioners  do  not  necessarily  understand disability  family-care  arrangements. And third, communication barriers with the children lead to some practitioners over-relying on the accounts of parents during the riskassessment process (Dowse et al., 2013). Some practitioners lean toward parents being


<!-- PAGE 8 -->


<!-- image -->

incapable  of  deliberate  perpetration  of  abuse  and  believe  that  the  parents  of  disabled children  probably  know  best  their  child's  impairment  and  associated  needs.  The researchers infer that, too often, social work professionals in these circumstances overempathize with the parents of children with a disability, leaving too many children in high-risk situations of abuse and neglect.

However,  though  it  is  true  that  relational  engagement  is  open  to  misjudgment, unqualified  technologization  of  the  process  decision-making  is  equally  problematic, even  when  such  decisions  are  only  used  in  'screening'  applications  that  still  depend upon human intervention and review. Experienced social workers can be severely limited in their ability to advocate and work alongside their clients due to the restricted context that ADM creates. The thin agency of ADM-directed decision-making aims to reduce bias, but it has been found to also remove social workers' discretionary power and ability to mediate aspects of the relationship between citizens to the state and services (Lindgren et al., 2019; Lipsky, 2010).

Research has also revealed serious risks created by the over-reach of ADM systems. The removal of social workers' discretionary power also removes the elements of positive relational  engagement  that  occur  through  the  interaction  between  social  workers  and their clients. Continuing relational engagement between social workers and clients is vital for social workers to understand a client's circumstances, thus building on the sourced data. Continuing engagement assists in professional decision-making by adding context to the data being used to form inferences. It is important, for example, that, within the Allegheny Family Screening Tool (AFST) system, data links between specific risk factors such as a family getting behind on the rent can be analyzed and triangulated with other data  such  as  school  attendance  to  indicate  the  level  of  risk  posed  to  the  child  (Eaton, 2019). It should also be said that the overall calculated score is not necessarily representative of the threshold of risk suggested. This is because the algorithmic processes use the data  without  the  consideration  of  circumstances  leading  to,  for  example,  absences  in a school attendance report or late rent repayments (Eaton, 2019). The context of data is made  much  clearer  in  professional  relationships  between  clients  and  social  workers. Advocates of ADM systems might here sustainably argue for the limited scope of their use  in  screening  applications  where  social  workers  are  still  required  to  intervene. However, as further discussed below, even the flagging of potential risks is not without consequence for families and children, in terms of stress, fear and stigmatization.

In  summary, used well, algorithmic decision-making can qualify social worker bias and help navigate through common challenges as discussed by Taylor et al. (2015). As seen through the AFST system, the data provided by the data-sourcing process aided in the  social  workers'  decision-making  because  it  provided  additional  information  to further  inform  the  case  conceptualization  process.  This  reduced  the  need  to  rely  only on relational engagement to fill the gaps, essentially lowering the dependence on subjective  data  as  a  primary  source.  Furthermore,  the  requirement  for  mandatory  interpretative intervention when a high-risk score was produced, limited the potential of the risk  posed to a child from being underplayed as a result of misjudgment. This can be beneficial in complex situations such as working with children living with a disability in child-protections settings as it reduces the risk of abuse being seen as disability-related accidents  and  not  investigated  further.  However,  research  also  showed  how  critical thinking  is  required  to  get  around  potential  ADM  bias.  It  is  crucial  for  future  social


<!-- PAGE 9 -->


<!-- image -->

workers to learn about this complexity of ADM decision-making in a pedagogical context before going into the field.

Unfortunately, too many examples have emerged where a combination of the removal of  relational  engagement  and  tight  quantifying  of  client  characteristics  through  ADM systems has led to human rights issues, unjust decisions, violations of ethical codes and norms,  constraints  on  the  ability  to  advocate,  and  challenges  in  triage  and  resource allocation.  ADM  systems  were  designed  as  ways  to  reduce  bias  in  decision-making. However, they have been found to accentuate or introduce new biases (Eubanks, 2018; Pasquale, 2017). Even the AFST, used to determine the risk posed to children in childprotection settings, has been found to threaten the human right of nondiscrimination. This  occurs  when  the  algorithm  creates  imprecise  risk-predictions  based  on  invalid correlations  (Keddell,  2019).  The  right  for  clients  to  be  treated  as  individuals  is  also endangered because the calculation of the statistical risk that the AFST produces is based on assumed group characteristics of different categorizations (Keddell, 2019).

When a score or outcome is produced by many current ADM systems, there is limited understanding provided about how the algorithm came to a particular decision, leading to  these  types  of  systems  to  be  referred  to  as  'black-box  AI'  (Krol  et  al.,  2017).  When clients  and  social  workers  are  unable  to  receive  a  clear  reason  for  the  decision,  their ability to challenge or rectify the decision is removed (Liu et al., 2019). When there is a low degree of transparency about the algorithmic systems and legal protections not only is  the original fairness of the automated decision threatened but people are denied the fundamental  right  of  understanding  and  challenging  a  decision.  This  creates  'moral crumple  zones'  for  social  workers  (Keddell,  2019,  p.  14),  that  is,  practitioners  being held  accountable  for  decisions  despite  the  decision  being  partly  or  predominantly determined by an algorithmic system. This raises significant questions about the need to prepare social work professionals to enable them to work effectively and fairly with these systems to ensure that clients are granted the necessary resources.

## COMPAS: algorithmic decision-making in correctional services

As  seen  in  the  AFST  case,  the  representation  of  the  client  is ideally not  simply a calculation based on a dataset but includes an understanding of the data in a decisionmaking context. This context is best treated as an ecological panorama that makes sense of the data and how it interacts with the life and situation of the client. The alternative can be  significantly  impactful  for  clients  with  complex  needs  or  situations  (Hughes,  2017; McQuillan, 2015). This need for an interpretative relation is further highlighted through examining the decision-support tool used in some US justice and correctional service jurisdictions to assess recidivism risk for the purposes of bail, parole, sentencing and/or prisoner  management  (Hartmann  &amp;  Wenzelburger,  2021).  The  tool,  Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), is used by judges in the context of determining non-parole periods in sentencing. Controversy has been raised about the black-box nature and minimized legal protections in the calculation of scores, though its use has been upheld in Wisconsin (Bennett Moses, 2019). Testing of the tool in one community found that African-American populations had a higher falsenegative rate compared to white populations (Angwin et al., 2016). The company that designed  COMPAS  claims  the  likelihood  of  a  person  re-offending  who  has  received


<!-- PAGE 10 -->


<!-- image -->

a  high  risk,  is  similar  for  both  racial  groups  (Bennett  Moses,  2019;  Hartmann  &amp; Wenzelburger,  2021).  Yet,  research  has  clearly  indicated  that  this  is  not  the  case. Similarly in social work, black-box AI, legal protections of algorithms, and removal of elements of positive relational engagement would create a limited range of viable options (Lipsky, 2010). This raises significant questions about the need to prepare the social work students to advocate for clients, including promoting alternatives to incarceration.

## National Disability Insurance Scheme (NDIS)

AFST and the COMPAS discussed above provide examples of the importance for social work of understanding the logic and variables behind the ADM outcome, countering the removal  of  the  ability  to  challenge  a  decision,  and  providing  pathways  for  advocacy (Keddell, 2019). However, as both AFST and COMPAS are used to assess risk, it is also important to understand the impacts of these issues on the area of needs assessment and triage. The proposed use of ADM by the Australian National Disability Insurance Agency (NDIA)  in  eligibility  determination  and  resource  allocation  extends  the  range  of  our examples to these areas.

Under NDIS, confirmed by the new Australian government in 2022, decision-making authority will shift from human evaluation based on subjective assessment of applicantprovided medical reports to objective scores using incapacity tools. Through the use of ADM,  the  Agency  says  that  they  aim  to  reduce  subjective  inequalities  found  in  the amount  of  funding  allocated  to  participants  created  through  relational  engagement. However, this NDIA replacement of tailor-made personalized planning for a 'template' plan appears to be a cost-saving measure defeating the purpose of the National Disability Insurance Scheme (Carney, 2021). Without the interpretative social understanding used in human decision-making, it is likely that the impact of a disability may be overlooked or understated by the NDIA score. To the extent that the variable individualized impact of a disability may be overlooked by a score, clients will tend not receive the adequate support they need. Being unable to challenge the decision, raises the question of how social workers will prioritize services to ensure clients get the services they need, including through triaging alternative services that match the needs of the client. Social workers are often required to triage between multiple services. Triaging is viewed as an approach to  allocating  scarce  resources,  prioritizing  limited  resources  in  the  most  efficient  way possible (Beauchamp &amp; Childress, 2001).

The authority that social workers once held to make decisions and to advocate for clients  will  no  longer  have  the  same  weight  in  this  ADM  setting,  placing  the  role  of a social worker in a position of less power to shape the potential outcome for their clients relative to the ADM process (Halliday et al., 2009). This raises the overall consideration of  how  social  workers  will  meet  the  needs  of  their  clients  when  service  access  is determined by ADM systems.

## Implications for social work education

The increasing reliance on ADM systems in everyday social work means that to ensure optimal outcomes social work, practitioners will have to develop the professional capabilities  to  engage  effectively  with  such  technologies.  For  social  work  education  to


<!-- PAGE 11 -->


<!-- image -->

prepare students for a world where decisions affecting clients' long-term life-chances are increasingly based on AI and algorithmic formulae, students will need the opportunity to develop an algorithmic literacy that centers on understanding the often-critical limitations of such technologies and systems.

While adding social work subjects on ADM is crucial, they alone are an insufficient basis  for  sensitizing  future  social  workers  for  asking  the  right  questions  and  understanding what is at stake with the normalization of ADM (Baumann et al., 2011; Bennett Moses, 2019). As social workers will be increasingly working with ADM systems, they will not only be required by the work to understand the systems and the context to which they are applied, but ideally also  how social  work theory intersects with how the data produced, how the data is used, and in what context. For example, social workers in the child-protection system will best understand the AFST algorithm and its consequences by not only understanding the way the tool makes inferences, but also understanding the child-protection  system  itself  and  the  various  criteria  used  to  form  decisions  about interventions in general (Bennett Moses, 2019).

Moreover, there are many concerns about what data is used to inform the development  of  ADM  models.  Gillingham  (2019)  notes  concerns  about  the  accuracy  and completeness of case-data recorded in the information system, such as significant delays of recording (up to a few months), and the missing rationale for decisions already made. In  addition,  existing  data  in  the  system  only  represent  children  who  are  under  the attention of services, while there are more children who experience maltreatment that is not known by services (Stoltenborgh et al., 2015). Hence the data selected to train the ADM models  can  be  biased  and  only  partially  reflect  on  the  whole  situation,  which creates blind spots for social workers who rely on these models to provide intervention. Students need to understand these limitations.

As  ADM  continues  to  be  further  developed  and  implemented  widely,  social  work students will clearly not be able to cover directly all the contexts in which ADM is used. Nevertheless,  case  studies,  examples,  and  explanations  of  theory  incorporated  beyond a single subject area would facilitate better in-context learning and transferable skill-sets. This requires an environment that assists students in connecting their understanding of data practices (technique) with their knowledge of social work theory and practice, and societal change as a whole (critique). Integrated learning units taking this brief as their broad frame would allow students to be exposed to a common theme running across the curriculum. This does not just mean the addition of intermittent lectures on ADM and AI to each unit or subject. Even the introduction of cross-disciplinary content would not be  sufficient  in  itself  to  ensure  that  all  relevant  topics  are  brought  into  the  same conversation  and  to  the  level  necessary  for  students  to  build  a  strong  understanding (Bennett Moses, 2019; Bronstein, 2003). In other words, a holistic and critical incorporation of these areas is more likely to allow students to work effectively with such systems than the possibly disjointed understanding students might get from having the topics of ADM and AI being added here and there across the curriculum.

The strongest arguments in the area of cross-disciplinary unit-design suggest that the curriculum should be practice-embedded rather than technology-led (Bullock &amp; Colvin, 2015). To achieve this, the design of the unit needs to start with social work theory and practice and not ADM (Hill &amp; Shaw, 2011). This is important if social work theory is not either to be reduced to various perspectives on possible responsive adjustments or taught


<!-- PAGE 12 -->


<!-- image -->

in  such  a  way  that  treats  ADM as  a  given .  The  parallel  here  to  economic  theory  is instructive.  Just  because  almost  without  exception  the  practitioners  of  contemporary capitalism use a particular implicit theory of practice-neo-classical liberal economics, with a tendency to emphasize monetarism-it does not mean that this theory should be taught as if it is the only relevant theory to economic practice. By a double hermeneutic, the dominance of this approach thus becomes self-confirming. By the same logic, social work theory and skills should not be seen as little more than mediation tools for working with and responding to problems caused by ADM (Healy et al., 2011). Put positively and most generally, social work theories about good practice in intersectional contexts ideally should be seen as framing the use of ADM rather than the other way around. ADM used as an assistive and sensitizing tool to aid social work practice is very different from a tool that frames practice. Such a theory-informed-practice approach will thus ensure that the design  of  course  work,  assessment,  and  unit  learning  continue  to  be  aligned  with developmental guidelines outlined by the relevant social work associations.

Case studies are a good way to work through the differences between an ADM-focused curriculum  and  a  critical,  theory-informed,  practice-led  approach  that  treats  ADM systems as tools. We have been using the case-study method in our own undergraduate teaching, and it is illustrative that two of the present authors of this article are social work students in one of those courses. Part of the brief to those students is to find case studies involving  algorithmic  decision-making  and  work  through  the  strengths  and  problems with these cases. Taking this further, we draw in the students as interns into our research projects where they act as paid research assistants, taking part in our long debates about research methods and what is working and what is not.

Here,  instead  of  going  over  earlier  described  case  studies,  or  drawing  immediate conclusions, the article turns to two additional case studies to illustrate possible pathways through a critical social work pedagogy that engages the world of examples: the Heat List and HEALER algorithms.

The HEALER algorithm was created by the Center of Artificial Intelligence in Society, a team of social workers, to assist in reducing HIV infection for homelessness youth in the  Los  Angeles  area.  They  aimed  to  shift  behavior  in  the  homeless  youth  population toward safer practices such as regular HIV testing (Yadav et al., 2017). The algorithm was implemented through a Facebook application that connected users and tracked who was the  most  influential  persons  within  the  group.  This  information  was  used  by  social workers at the homeless shelters to train the identified members from the network as peer leaders. The training allowed the homeless shelters to create a peer-based prevention model that teaches strategies to help the peer leaders to facilitate discussions with their peers  about  safer  practices.  The  program  resulted  in  66%  of  homeless  youth  in  the network  receiving  information  from  peers.  The  social  workers  also  found  that  there was  a  25%  self-reported  increase  in  the  number  of  homeless  youths  regularly  getting tested for HIV (Yadav et al., 2017).

In  this  case  study,  our  students  concluded  that  the  success  of  the  program  was potentially  not  due  to  the  algorithmic  process  itself  but  possibly  to  the  way  in  which the  process  was  used,  including  the  face-to-face  training.  This  conclusion  was  then interrogated  through  the  following  sets  of  questions.  What  is  the  basis  for,  and  are there  potential  social  biases  built  into,  the  original  algorithmic  measure?  In  this  case, the status of peer leader was indicated by the algorithm based on how well a person was


<!-- PAGE 13 -->


connected  digitally  within  the  group.  However,  it  was  recognized  that  a  participant 'recommended  by'  algorithm  might  not  be  a  suitable  fit  for  a  peer-based  prevention model. Thus, the second set of questions. What is the outcome that the algorithm seeks, and how does it accord with existing evidence-based conclusions? Here students could explore  what  the  social  context  means  for  peer-based  prevention  models,  and  the challenge  of  designing  for  a  high-risk  community  of  adolescents  such  as  homeless youth (Dodge et al., 2006). This is a pedagogical topic in itself. Models that only focus on high-risk youth have the potential to enhance negative outcomes through what has come to be known as 'deviancy training' (Gifford-Smith et al., 2005; Snyder et al., 2005).

Hence,  we  arrive  at  much  broader  considerations  concerning  how  the  algorithm works and a third set of questions. What are the intersectionalities at play in the area covered by the algorithm? It has been suggested by some writers that, to ensure that this risk of deviancy training is significantly reduced, peer-based models need a combination of  participants  who  are  low-risk  and  high-risk  (Rice  et  al.,  2011;  Snyder  et  al.,  2005). However,  the  consequence  of  this  understanding  is  itself  contextual  and  requires  the social  worker's  knowledge  about  deep  intersectionalities:  that  is,  about  the  persons involved, the nature of peer-based prevention models, what it means to use a platform such as Facebook with all its complications, how data can be used to inform or distort intervention strategies, and the limitations and strengths of ADM systems.

In summary, using such an interrogative method applied to life-world cases has the effect  of  opening  training  to  critical  thinking,  without  giving  answers  as  such.  The method  becomes  the  key.  In  our  experience,  such  a  process  allows  students  to  track algorithmic  processes  in  relation  to  a  range  of  various  topics  and  disciplines  simultaneously. The best of those units that worked on intersecting themes could work through patterns of critical issues (critique), while asking questions about practical pathways to good practice (technique).

While we have not answered the question of what would be considered an 'ideal' unit design to achieve a holistic understanding of the ends of social work, as well as the means (technique), such case-studies do illustrate the nature of the critical method. All of the case studies introduced earlier could be used to understand intersectionality, but another useful example of complex determinations is the Heat List algorithm. This case introduced  another  set  of  questions  around  the  effect  of  the  knowledge  produced  by  the algorithm on the persons the knowledge was intended to support. Heat List is used by the Chicago police department in partnership with local social workers to lower gun-violence (Stroud, 2021). The algorithm produces calculations based on proximity to and relationships with known shooters and shooting casualties. It produces data indicating a specific person being at risk of gun-violence-though without presuming that the person would be a perpetrator or victim. A team consisting of police officers and social workers then visits the residence of the person indicated by the algorithm and informs them that they are at risk. Attending police officers issue the individual with a warning, while the social workers provide the opportunity for the individual to seek the help of various community services (Stroud, 2021).

On the face of it, introduced without intersectional context, this sounds workable and appropriate in negotiating the presence-mediation tension. However, applications of the Heat List algorithm also caused problems such as the over-policing of African-American communities and the placement of some individuals at risk of being ostracized by their


<!-- PAGE 14 -->


<!-- image -->

community, in part because they were seen talking to the police. In comparison to the HEALER algorithm, significant differences can be seen in the design and the approach used by the Heat List algorithm. The question, 'What are these differences?', could be the subject of critical pedagogy and student interrogation.

More  than  that,  it  allows  for  students  to  be  taken  through  principles  of relational engagement and  ADM  deployment  so  that  the  'lessons'  of  these  cases  are  extended beyond point-by-point comparison. Here, for example, critical race theory meets knowledge-transfer  theory,  each  with  their  own  presumptions  that  need  to  be  elaborated. Practice-embedded  critical  pedagogy  is  as  much  about  testing  the  theories  as  about testing  the  procedures.  Social  workers  will  continue  to  find  themselves  in  situations faced with limited capacities to advocate, with reduced social trust as public institutions lose legitimacy, with challenges in resource allocation, and facing a whole new range of ethical dilemmas (McDonald, 2006). These challenges can potentially shape how ADM is viewed by students in a practice-led ADM-relevant unit. While social work frameworks, practice skills, theory, procedures and tools can help to inform good ADM use, they can also create the possibility of appropriately reducing, modifying or questioning the use of algorithmic processes (Brown et al., 2014; Calder, 2004).

## Conclusion: social work in an age of automated decision-making

Social work is a helping profession with expertise in addressing human needs through working across tensions in the world and in its own practice. At its best, critical social work successfully bridges these tensions without dissolving them. It maintains embodied engagement  with  those  who  find  themselves  in  relatively  precarious  positions  while recognizing that this world, and its own practice of engagement, are becoming increasingly mediated by the very technological systems that sometimes contribute to continuing  this  precarity  (the  presence/mediation  tension).  Positive  social  work  maintains critical distance from these technical processes, including ADM, while becoming proficient enough in technologized service delivery to understand how and when to use it well -and perhaps when to not use it at all (the technique/critique tension).

Maintaining  this  changing  and  delicate  balance,  particularly  in  the  context  of  the increasing use of ADM, requires a demanding technical-critical pedagogy. Questions of technique and processes of mediation have a tendency become dominant in practice, and automatic decision-making presents us with a further leap in this direction. At the same time,  ADM  does  not  resolve  longer-term  issues  as  its  proponents  suggest.  Current evidence suggests that with ADM systems, groups already experiencing higher levels of discrimination or social-economic disadvantage are more likely to experience negative human rights impacts, while groups that experience more social advantages are more likely to benefit from it (Farthing et al., 2019). Hence, the social work profession faces a new variation on an issue that it has always faced. How is it possible to support those who live difficult lives to move beyond precarity in a world when the tools of support (and control) tend to be framed by the limits of that very world, and where precarity is too often sustained by welfare dependency?

Social work is a profession that works with both individuals and systems bridging the  tensions  between  social  change,  social  control  and  social  provision.  Social  work education  needs  to  be  able  to  support  students  to  understand  their  roles  in  the


<!-- PAGE 15 -->


<!-- image -->

emerging  world  of  ADM  application.  Educators  need  to  be  able  to  ask  a  series  of questions that begin with the general. What are the purposes of integrating ADM into our work? Is it because of cuts in government spending on welfare and services? Is it as part of a program of improving the accuracy of the information collected to better understand  clients'  needs?  Can  it  be  used  to  monitor  the  efficacy  of  social  work interventions?  Can  it  identify  the  key  problems  of  individuals  or  groups  to  inform better  prevention  strategies?  Can  it  sensitively  inform  the  allocation  of  resources  to achieve  better  social  justice?  Are  these  different  purposes  in  line  with  each  other  or otherwise?

Beyond those general framing questions, the questions we have suggested that might be used to interrogate the strengths of problems of various case studies can be summarized in the following classical form for all policy work:

- Grounding assumptions : What is the basis for, and are there potential social biases built into, the original algorithmic measure?
- Contexutalizing complexities : What are the social intersectionalities-such as identity,  capacity  and  socio-economic  standing-at  play  in  the  area  covered  by  the algorithm?
- Intended consequences : What is the outcome that the algorithm seeks, and how does it accord with human rights issues and existing research conclusions?
- Unintended  consequences :  Given  the  context  and  social  intersectionalities,  what effect will this knowledge and its implementation have on the people that we seek to support?
- Application tensions : Given a satisfactory working of the above considerations, what are the intended and unintended consequences of the chosen forms of 'relational engagement'-face-to-face and mediated.

Posing  such  questions  as  a  summary  list  may  sound  as  though  the  problems  underpinning ADM lie beyond the current paucity of work on ADM and AI in social work curriculum. A reflexive teaching-and-learning will need to confront that question too. What are the limits of pedagogy? Strengthening algorithmic literacy and social critique aims,  ultimately,  to  embolden  those  most  affected  by  technological  determinationsthose in the field and those supported by welfare-to take hold of what they identify as possibly empowering and disempowering technological processes, in this case potentially enabled in positive ways by sensitively used, constrained, algorithmic support.

In  the  emerging  world  of  ADM  where  human  decision-making  is  being  reframed, social  workers play a pivotal role in negotiating and influencing the development and application  of  such  algorithmic  systems.  This  requires  social  work  education  to  equip students with the digital literacy and awareness of opportunities and risks associated with ADM.  This  can  partly  be  achieved  through  innovations  in  curriculum  design  and academic development, continually reassessed in practice. We must aim to ensure that social workers have capacities for a practice that will always be beset by tensions: to be active advocates for individuals, groups or communities that are already or potentially being negatively impacted by technological change; to use technologies and techniques wisely; and to be change-agents in an unjust system where disadvantages can be amplified by automated processes, especially for the already most precarious.


<!-- PAGE 16 -->


<!-- image -->

## Disclosure statement

No potential conflict of interest was reported by the author(s).

## Funding

The work was supported by the Australian Research Council [LP190100099].

## Notes on contributors

Paul James is  Professor of Globalization and Cultural Diversity in the Institute for Culture and Society  at  Western  Sydney  University.  He  is  author  or  editor  of  over  30  books,  including Globalization Matters: Engaging the Global in Unsettled Times (with Manfred Steger, Cambridge University Press, 2019).

Jason Lai is a social work student at the University of Western Sydney.

Ashley Liao is a social work student at the University of Western Sydney.

Liam  Magee is  Associate  Professor  in  the  Institute  for  Culture  and  Society,  Western  Sydney University.  Liam's  principal  research  interests  focus  on  the  application  of  social  methods  and information  technology  to  social  development  and  sustainability.  He  is  author  of  Towards  A Semantic Web: Connecting Knowledge in Academic Research (Chandos, 2011).

Karen  Soldatic is  Professor  of  Social  Work  in  the  School  of  Social  Sciences,  Western  Sydney University, and an ICS Institute Fellow. Karen's research on welfare regimes builds upon her 20 years'  experience  as  an  international,  national  and  state-based  senior  policy  analyst  and  practitioner. Among other books, she is author of Human Rights and Social Work (with Jim Ife and Linda Briskman, Cambridge University Press, 2022).

## ORCID

<!-- image -->

Paul James

http://orcid.org/0000-0002-8591-4594

Liam Magee

http://orcid.org/0000-0003-2696-1064

Karen Soldatic

http://orcid.org/0000-0001-8139-2912

## References

- Angwin, J., Larson, J., Mattu, S., &amp; Kirchner, L. (2016). Machine bias: There's software used across the country to predict future criminals. And it's biased against blacks. Propublica . https://www. propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing
- Baumann, D., Fluke, J., Dalgleish, L., &amp; Kern, H. (2011). The decision-making ecology of placing a child in foster care. Child Abuse &amp; Neglect , 49 ,  12-23. https://doi.org/10.1016/j.chiabu.2015. 02.020
- Bawden, D. (2008). Origins and concepts of digital literacy. In C. Lankshear &amp; M. Knobel (Eds.), Digital literacies: Concepts, policies and practices (pp. 17-32). Peter Lang.
- Beauchamp, T., &amp; Childress, J. (2001). Principles of biomedical ethics . Oxford University Press.

Bennett Moses, L. (2019). Helping future citizens navigate an automated, datafied world. UNSW Law Research Paper , 19-28. https://doi.org/10.2139/ssrn.3370016

- Bronstein,  L.  (2003).  A  model  for  interdisciplinary  collaboration. Social  Work , 48 (3),  297-306. https://doi.org/10.1093/sw/48.3.297


<!-- PAGE 17 -->


<!-- image -->

- Brown,  L.,  Moore,  S.,  &amp;  Turney,  D.  (2014). Analysis  and  critical  thinking  in  assessment .  www. researchinpractice.org.uk/children/publications/2014/july/analysis-and-critical-thinking-in -assessment-resource-pack-2014/
- Bullock, A., &amp; Colvin, A. (2015). Communication technology integration into social work practice. Advances in Social Work , 16 (1), 1-14. https://doi.org/10.18060/18259
- Calder, M. (2004). Out of the frying pan into the fire? A critical analysis of the integrated children's system. Child Care in Practice , 10 (3), 225-240. https://doi.org/10.1080/1357527042000244365
- Carney,  T.  (2021). Automated  decision-making  in  welfare .  Economic  justice  Australia.  https:// www.ejaustralia.org.au/wp/social-security-rights-review/automated-decision-making-inwelfare/
- Cuccaro-Alamin, S., Fousta, R., Rhema Vaithianathanc, V., &amp; Putnam-Hornsteina, E. (2017). Risk assessment  and  decision  making  in  child  protective  services:  Predictive  risk  modeling  in context. Children  and  Youth  Services  Review , 79 ,  291-298.  https://doi.org/10.1016/j.child youth.2017.06.027
- Dodge,  K.,  Dishion,  T.,  &amp;  Lansford,  J.  (2006). Deviant  peer  influences  in  programs  for  youth: Problems and solutions . Guilford Press.
- Dowse,  L.,  Soldatic,  K.,  Didi,  A.,  Frohmader,  C.,  &amp;  van  Toorn,  G.  (2013). Stop  the  violence: Addressing  violence  against  women  and  girls  with  disabilities  in  Australia .  https://wwda.org. au/publication/stop-the-violence-addressing-violence-against-women-and-girls-with-disabil ities-in-australia-background-paper/
- Dumitrescu, A. M. (2020). Social work education in the digital Era. Revista de Asistenţă Socială , 19 (3), 37-44. http://www.swreview.ro/index.pl/social\_work\_education\_in\_the\_digital\_era
- Eaton, L. (2019). Is it right to use AI to identify children at risk of harm? Guardian . https://www. theguardian.com/society/2019/nov/18/child-protection-ai-predict-prevent-risks
- Esser, F., Baader, M. S., &amp; Betz, T. (2016). Reconceptualising agency and childhood: New perspectives in childhood studies . Routledge.
- Eubanks, V. (2018). Automating inequality: How high-tech tools profile, police, and punish the poor . St. Martin's Press.
- Farthing, S., Howell, J., Lecchi, K., Paleologos, Z., Saintilan, P., &amp; Santow, E. (2019). Human rights and technology: Discussion paper .  Australian Human Rights Commission. https://tech.human rights.gov.au/sites/default/files/2019-12/TechRights2019\_DiscussionPaper.pdf
- Gifford-Smith, M., Dodge, K., Dishion, T., &amp; McCord, J. (2005). Peer influence in children and adolescents:  Crossing  the  bridge  from  developmental  to  intervention  science. Journal  of Abnormal Child Psychology , 33 (3), 255-265. https://doi.org/10.1007/s10802-005-3563-7
- Gillingham, P. (2019). Decision support systems, social justice and algorithmic accountability in social work: A new challenge. Practice , 31 (4), 277-290. https://doi.org/10.1080/09503153.2019. 1575954
- Glaberson, S.  K.  (2019).  Coding  over  the  cracks:  Predictive  analytics  and  child  protection. The Fordham Urban Law Journal , 46 (2), 307-363. https://ir.lawnet.fordham.edu/ulj/vol46/iss2/3
- Goldkind,  L.,  Wolf,  L.,  &amp;  Freddolino,  P.  P.  (2019). Digital  social  work:  Tools  for  practice  with individuals, organizations and communities . Oxford University Press.
- Halliday,  S.,  Burns,  N.,  Hutton,  N.,  McNeill,  F.,  &amp;  Tata,  C.  (2009).  Street -level  bureaucracy, interprofessional relations, and coping mechanisms: A study of criminal justice social workers in  the  sentencing process. Law &amp; Policy , 31 (4),  405-428. https://doi.org/10.1111/j.1467-9930. 2009.00306.x
- Hartmann, K., &amp; Wenzelburger, G. (2021). Uncertainty, risk and the use of algorithms in policy decisions: A case study on criminal justice in the USA. Policy Sciences , 54 (2), 269-287. https:// doi.org/10.1007/s11077-020-09414-y
- Healy, K., Darlington, Y., &amp; Feeney, J. (2011). Parents' participation in child protection practice: Toward respect and inclusion. Families in Society , 92 (3), 282-288. https://doi.org/10.1606/10443894.4126
- Healy, K., &amp; Hampshire, A. (2002). Social capital: A useful concept for social work? Australian Social Work , 55 (3), 227-238. https://doi.org/10.1080/03124070208410978
- Hill, A., &amp; Shaw, I. (2011). Social work and ICT . Sage Publications.


<!-- PAGE 18 -->


<!-- image -->

- Hughes, T. (2017). Prediction and social investment. In J. Boston &amp; D. Gill (Eds.), Social investment: A New Zealand policy experiment (pp. 179-202). Bridget Williams Books.
- James, A., McLeod, J., Hendy, S., Marks, K., Rusu, D., Nik, S., &amp; Plank, M. (2019). Using family network data in child protection services. Plos One , 14 (10), e0224554. https://doi.org/10.1371/ journal.pone.0224554
- Jensen,  I.  (2020).  What  are  the  perspectives  of  children  in  child  protection  work  among  social workers in Norway and Chile? Children and Youth Services Review , 118 (1). https://doi.org/10. 1016/j.childyouth.2020.105410
- Keddell, E. (2019). Algorithmic justice in child protection: Statistical fairness, social justice and the implications for practice. Social Sciences , 8 (10), 281. https://doi.org/10.3390/socsci8100281
- Klocker, N. (2007). An example of 'thin' agency: Child domestic workers in Tanzania. In R. Panelli, S. Punch, &amp; E. Robson (Eds.), Global perspectives on rural childhood and youth: Young rural lives (pp. 83-94). Routledge.
- Krol, J., Huey, J., Barocas, S., Felten, E., Reidenberg, J., Robinson, D., &amp; Yu, H. (2017). Accountable algorithms. University of Pennsylvania Law Review , 165 (3), 633-705.
- La Mendola, W. (2010). Social work and social presence in an online world. Journal of Technology in Human Services , 28 (1-2), 108-119. https://doi.org/10.1080/15228831003759562
- Lindgren, I., Madsen, C. Ø., Hofmann, S., &amp; Melin, U. (2019). Close encounters of the digital kind: A research agenda for the digitalization of public services. Government Information Quarterly , 36 (3), 427-436. https://doi.org/10.1016/j.giq.2019.03.002
- Lipsky, M. (2010). Street-level bureaucracy: Dilemmas of the individual in public services .  Russell Sage Foundation.
- Liu,  H.,  Lin,  C.,  &amp;  Chen,  Y.  (2019).  Beyond  State  v  Loomis:  Artificial  intelligence,  government algorithmization and accountability. International Journal of Law and Information Technology , 27 (2), 122-141. https://doi.org/10.1093/ijlit/eaz001
- McDonald, C. (2006). Challenging social work: The institutional context of practice . Bloomsbury. McQuillan,  D.  (2015).  Algorithmic  states  of  exception. European Journal of  Cultural  Studies , 18 (4-5),  564-576.  https://doi.org/10.1177/1367549415577389
- Parton,  N.  (2008).  Changes  in  the  form  of  knowledge  in  social  work:  From  the  'social'  to  the 'Informational'? British  Journal  of  Social  Work , 38 (2),  253-269.  https://doi.org/10.1093/bjsw/ bcl337
- Pasquale, F. (2017). Toward a fourth law of robotics: Preserving attribution, responsibility, and explainability in an algorithmic society. Ohio State Law Journal , 78 . https://ssrn.com/abstract= 3002546
- Ramsey, A. T., &amp; Montgomery, K. (2014). Technology-based interventions in social work practice: A systematic review of mental health interventions. Social Work in Health Care , 53 (9), 883-899. https://doi.org/10.1080/00981389.2014.925531
- Rice, E., Milburn, N., &amp; Monro, W. (2011). Social networking technology, social network composition, and reductions in substance use among homeless adolescents. Prevention Science , 12 (1), 80-88. https://doi.org/10.1007/s11121-010-0191-4
- Robson,  D.,  &amp;  Robson,  M.  (1998).  Intimacy  and  computer  communication. British  Journal  of Guidance &amp; Counselling , 26 (1), 33-41. https://doi.org/10.1080/03069889800760041
- Russell, J. (2015). Predictive analytics and child protection: Constraints and opportunities. Child Abuse &amp; Neglect , 46 , 182-189. https://doi.org/10.1016/j.chiabu.2015.05.022
- Snyder, J., Schrepferman, L., Oeser, J., Patterson, G., Stoolmiller, M., Johnson, K., &amp; Snyder, A. (2005). Deviancy training and association with deviant peers in young children: Occurrence and contribution  to  early-onset  conduct  problems. Development  and  Psychopathology , 17 (2), 397-413. https://doi.org/10.1017/S0954579405050194
- Stoltenborgh, M., Bakermans -kranenburg, M. J., Alink, L. R., &amp; van IJzendoorn, M. H. (2015). The prevalence of child maltreatment across the globe: Review of a series of meta -analyses. Child Abuse Review , 24 (1), 37-50. https://doi.org/10.1002/car.2353
- Stroud,  M.  (2021). Heat listed .  The  Verge.  www.theverge.com/22444020/chicago-pd-predictivepolicing-heat-list


<!-- PAGE 19 -->


<!-- image -->

- Taylor,  J.,  Stalker,  K.,  &amp;  Stewart,  A.  (2015).  Disabled  children  and  the  child  protection  system: A cause for concern. Child Abuse Review , 25 (1), 60-73. https://doi.org/10.1002/car.2386
- Yadav, A., Chan, H., Jiang, A. X., Xu, H., Rice, E., &amp; Tambe, M. (2017). Maximizing awareness about HIV in social networks of homeless youth with limited information. Proceedings of the Twenty-Sixth International  Joint  Conference  on  Artificial  Intelligence ,  4,  959-963.  https://doi. org/10.24963/ijcai.2017/702
- Zouridis, S., Van Eck, M., &amp; Bovens, M. (2020). Automated discretion. In T. Evans &amp; P. Hupe (Eds.), Discretion and the quest for controlled freedom (pp. 313-329). Palgrave Macmillan.