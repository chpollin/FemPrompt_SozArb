---
source_file: Petzel_2025_Prejudiced_interactions_with_large_language.pdf
conversion_date: 2026-02-03T09:14:56.760380
converter: docling
quality_score: 95
---

<!-- image -->

Contents lists available at ScienceDirect

## Computers in Human Behavior

journal homepage: www.elsevier.com/locate/comphumbeh

## Prejudiced interactions with large language models (LLMs) reduce trustworthiness and behavioral intentions among members of stigmatized groups ☆

<!-- image -->

Zachary W. Petzel * , Leanne Sowerby

School of Psychology, Newcastle University, United Kingdom

## A R T I C L E  I N F O

Handling editor: Paul Kirschner

Keywords: Large language models Artificial intelligence Trustworthiness Prejudice

## 1. Introduction

As  artificial  intelligence  (AI)  advances  rapidly,  it  is  becoming  an integral part of our lives. Major organizations within financial, pharmaceutical, retail, and marketing industries have already acknowledged the utility of AI in the workplace, expecting their employees and customers to embrace these emerging technologies (Abril, 2023). Similarly, a  consortium  of  universities  in  the  United  Kingdom  have  recently endorsed the use of AI in the classroom, recognizing its potential to enrich teaching and learning (Russel Group, 2023). In the United States, the Department of Education ' s Office of Educational Technology have also provided guidelines detailing how AI may improve learning experiences  for  students,  in  addition  to  reducing  workload  for  teachers (Cardona et al., 2023). Given these commitments from both educational institutions and industry leaders, adapting to emerging AI technologies will be essential for academic success among students, in addition to remaining  competitive  while  seeking  employment.  Ultimately,  those

## A B S T R A C T

Users report prejudiced responses generated by large language models (LLMs) like ChatGPT. Across 3 preregistered  experiments,  members of stigmatized social groups (Black Americans, women) reported higher trustworthiness of LLMs after viewing unbiased interactions with ChatGPT compared to when viewing AI-generated prejudice (i.e., racial or gender disparities in salary). Notably, higher trustworthiness accounted for increased behavioral intentions to use LLMs, but only among stigmatized social groups. Conversely, White Americans were more likely to use LLMs when AI-generated prejudice confirmed implicit racial biases, while men intended to use LLMs when responses matched implicit  gender biases.  Results  suggest  reducing  AI-generated  prejudice  may promote trustworthiness of LLMs among members of stigmatized social groups, increasing their intentions to use AI tools. Importantly, addressing AI-generated prejudice could minimize social disparities in adoption of LLMs which might further exacerbate professional and educational disparities. Given expected integration of AI in professional  and  educational  settings,  these  findings  may  guide  equitable  implementation  strategies  among employees and students, in addition to extending theoretical models of technology acceptance by suggesting additional mechanisms of behavioral intentions to use emerging technologies (e.g., trustworthiness).

who fail to incorporate these emerging technologies into their learning and professional development may lag behind those who embrace it.

Access to large language models (LLMs) has proliferated, providing sophisticated tools capable of simulating detailed and accurate conversations  (e.g.,  ChatGPT).  Yet,  LLMs  are  often  trained  on  text  which contain harmful biases against disadvantaged and stigmatized groups. Unsurprisingly,  LLMs  may  produce  prejudiced  responses  to  queries linked to gender, race, religion, or disability (Abid et al., 2021; Gadiraju et al., 2023; Ghosh &amp; Caliskan, 2023). Despite efforts to improve the inclusivity of LLMs, the pervasive implicit biases embedded throughout our written history pose a significant challenge as they are more difficult to detect and address. If LLMs which emulate implicit biases are integrated in our daily lives, social disparities may be exacerbated. Importantly, AI-generated prejudice may elicit unfavorable reactions from the targets of these biases, leading to limited engagement with LLMs among members of stigmatized social groups.

The present work uses the Technology Acceptance Model (Davis,

This article is part of a special issue entitled: The social bridge: An interdisciplinary view on trust in technology edited by Johannes Kraus, Irene Valori &amp; Merle Fairhurst published in Computers in Human Behavior. ☆ Data and materials are available via the Open Science Framework: https://osf.io/j3maf/

* Corresponding author. Dame Margaret Barbour Building, Newcastle upon Tyne, NE2 4DR, United Kingdom. E-mail address: zach.petzel@newcastle.ac.uk (Z.W. Petzel).

## https://doi.org/10.1016/j.chb.2025.108563

<!-- image -->

<!-- image -->

Z.W. Petzel and L. Sowerby

1989)  and  Unified  Theory  of  Acceptance  and  Use  of  Technology (Venkatesh et al., 2003) to examine the impact of AI-generated prejudice on  behavioral  intentions  to  use  LLMs.  We  consider  recent  evidence which suggests trustworthiness of LLMs may be a vital mechanism to increase engagement with these emerging tools (Wanner et al., 2022). Notably, the current work examines how AI-generated prejudice may uniquely  reduce  perceived  trustworthiness  of  LLMs  among  Black Americans and women who are stigmatized and frequently targets of negative implicit bias.

## 1.1. Literature review

## 1.1.1. Overview of disparities

Implicit biases adversely impact stigmatized groups. For instance, racial  biases  (e.g.,  Black  Americans  as  violent)  reduce  trust  of  Black partners, while also promoting favorable economic decisions for White partners  (Stanley  et  al.,  2011).  Similarly,  women ' s  performance  is negatively  impacted  when  interviewed  by  men  with  implicit  gender biases (e.g., women as incompetent; Latu et al., 2015). Beyond experimental evidence, implicit biases can negatively influence criminal cases, hiring  decisions,  and  clinical  diagnoses,  in  addition  to  reducing  the quality of interactions between clients and their healthcare providers (Greenwald et al., 2022; Maina et al., 2018). Alarmingly, consequences of implicit bias have already been observed in the application of AI, with Amazon having used algorithms preferring men for technical jobs (e.g., software developers) while disregarding female applicants due to their gender (Reuters, 2018). With 42% of organizations using AI to automate business analytics, customer service, and hiring decisions, in addition to 40% of remaining organizations now pursuing integration of this technology  (IBM,  2024),  action  is  needed  to  understand  how  algorithms which contain implicit biases may impact engagement with LLMs among stigmatized  social  groups  who  are  most  affected  by  this  embedded prejudice.

Traditionally, social disparities are often observed in the uptake and acceptance of emerging technologies, with earlier adoption by members of  advantaged  groups predicting  future  discrepancies  in  use  of  these tools (Shaouf &amp; Altaqqi, 2018; Venkatesh et al., 2000). For instance, racial health disparities in the United States are well established, with increased risk of cancer, alcohol-related problems, and cardiovascular disease among Black Americans (Caetano et al., 2014; Graham, 2015; O ' Keefe et al., 2015). While advances in technology have been implemented to address these disparities, Black Americans are less likely to utilize telemedicine and advanced breast imaging, in addition to the use of smart devices to assist with symptom monitoring (Haynes et al., 2021; Hughes &amp; Granger,  2014;  Miles  et  al.,  2018;  Sheon  et  al.,  2017). Relatedly, men are more likely to own two or more smart home devices (e.g.,  Google  Home,  Amazon  Alexa)  than  women,  in  addition  to perceiving devices as more useful and reporting greater intentions to use this technology (Canziani &amp; MacSween, 2021). Men also report more willingness to use other emerging technologies compared to women, such  as  automated  cars,  virtual  learning  environments  (e.g.,  Canvas, Blackboard), e-banking services, and online stock trading (Goswami &amp; Dutta, 2015; Hohenberger et al., 2016).

## 1.1.2. Sources of disparities

Historically, lack of diversity has been a major issue in development and  application  of  emerging  technologies.  For  instance,  algorithms assessing Black defendants are likely to predict higher rates of future violent crimes compared to White defendants, while algorithms used by Google Photos incidentally labelled pictures of Black users as gorillas (Angwin et al., 2016; BBC, 2015). Early algorithms detecting cardiovascular disease were also less likely to be accurate among women due to gender differences in manifestation of symptoms, in addition to hiring algorithms preferring applicants with CVs describing masculine hobbies (e.g., baseball, basketball; Criado-Perez, 2019; Lytton, 2024). Similarly, while  biases  in  voice  and  facial  recognition  software  persist,  earlier versions of this technology heavily preferred White, male voices and faces compared to racial minorities and women (Bajorek, 2019; Chen et al., 2022). Thus, social disparities in the adoption of emerging technology may be due to initial biases in early iterations of these tools, with advantaged  social  groups  more  likely  to  adopt  technologies  due  to positive user experiences and preferential treatment by algorithms (e.g., White males). Conversely, members of stigmatized groups who are not often  considered  in  development  of  these  tools  may  disengage  from emerging  technologies  due  to  poorer  user  experience  resulting  from embedded implicit biases.

However, these social disparities in adoption of emerging technologies  may  be  partly  explained  by  financial  or  healthcare  inequalities (Miles et al., 2018), particularly among Black Americans who tend to be more  economically  disadvantaged  (Williams  et  al.,  2016).  Further, men ' s preference for emerging technology such as smart home devices may reflect traditional gender stereotypes which often promote men ' s interest in science and technology (Master et al., 2021; Plante et al., 2019). Gender disparities in the adoption of technology may also reflect differences in motivation between men and women, rather than exposure to implicit biases in earlier iterations of these tools. For instance, men  tend  to  use  technology  for  information  seeking  (Schehl  et  al., 2019), in addition to being more proficient at gathering information using technology (Roy &amp; Chi, 2003). Conversely, women are more likely to use technology to gather health-related information (Bidmon &amp; Terlutter, 2015; Manierre, 2015), in addition to preferring social aspects of technology compared to men (e.g., emails, social networking; Goswami &amp; Dutta, 2016; Schehl et al., 2019). Notably, differences in motives and use of technology are not due to biological sex, but rather the endorsement of masculine versus feminine gender roles (Huffman et al., 2013). Further, gender differences in adoption of technologies may vary based on  generational  divides.  For  example,  gender  differences  in  use  of e-commerce are not found among older generations (e.g., baby boomers; Lian &amp; Yen, 2014), in addition to this generation being less likely to use smart home devices compared to generation X and millennials (Canziani &amp; MacSween, 2021). Yet, despite evidence outlining how motives and use of technology vary by several demographic factors, mechanisms of why stigmatized groups have poorer adoption of emerging technologies are not well understood.

## 1.1.3. Trustworthiness of technology

Recent literature suggests perceived trustworthiness as a potential mechanism for successful implementation of technology in our daily lives,  with  the  European  Commission ' s  AI  act  emphasizing  trustworthiness  as  vital  factor  to  be  considered  when  developing  AI  tools (European Commision, 2024, 6 March). Trustworthiness is the perception of several characteristics of a trustee (Richmond et al., 2024) which are influenced by both micro- (e.g., trust propensity) and macro-level factors  (e.g.,  certifications,  endorsement  by  experts;  Schlicker  et  al., 2022).  While  several  definitions  of  trustworthiness  exist  within  the literature,  the  most  prominent  definition  which  has  frequently  been applied to technological automation and LLMs (Kraus et al., 2023; Lee &amp; See, 2004; Ressel, V ¨ oller, Murphy, &amp; Mullins, 2024; Scholz et al., 2024) is  provided by Mayer et al. (1995), defining trustworthiness as, ' the willingness of a party to be vulnerable to the actions of another party based on the expectation that the other will perform a particular action important to the trustor, irrespective of the ability to monitor or control that other party ' (p. 712). In other words, trustworthiness towards LLMs may be conceptualized as the degree to which someone is comfortable relying on these tools to help achieve a goal, despite the lack of transparency and observation of the underlying processes.

Mayer et al. (1995) proposes three factors which may be considered while developing trustworthiness of LLMs: (1) ability, (2) benevolence, and (3) integrity. Ability is the extent to which LLMs are perceived to be accurate while demonstrating domain-specific expertise. Benevolence is the assumption LLMs are altruistic, acting in a way which benefits the user without harm. Lastly, integrity captures agreement between the

Z.W. Petzel and L. Sowerby user ' s values and principles and those of the LLM (e.g., adherence to ethics). Thus, LLMs trained on data containing prejudices may reduce trustworthiness  among  stigmatized  group  members  due  to  violating these factors. For instance, algorithms which predict higher rates of violent crimes among Black defendants violate assumptions of benevolence among Black Americans due to the further propagation of negative stereotypes,  while  also  violating  integrity  by  not  adhering  to  ethical responsibilities  to  be  unprejudiced  (Angwin  et  al.,  2016).  Similarly, hiring algorithms which prefer men solely due to their gender violate benevolence assumptions among women via harming their employment opportunities,  in  addition  to  violating  integrity  by  not  adhering  to egalitarian values (Criado-Perez, 2019; Lytton, 2024). These violations of  benevolence  and  integrity  outlined  by  Mayer  et  al.  (1995) may uniquely reduce LLM trustworthiness among members of stigmatized social  groups.  Consistent  with  this  assumption,  algorithms  for  immigration,  hiring,  and  advertising  which  depict  prejudice  decrease  AI trustworthiness (Gupta et al., 2022; Parra, Gupta, &amp; Dennehy, 2021; Wang et al., 2021).

Notably, trustworthiness is vital for adopting emerging technologies (Choudhury &amp; Shamszare, 2023; Dawar et al., 2022; Mostafa &amp; Kasamani,  2022),  in  addition  to  increasing  persuasiveness  and  positivity towards AI-generated content during subsequent interactions with LLMs (Metzger et al., 2024). Therefore, members of stigmatized groups who reduce AI trustworthiness due to interacting with prejudiced LLMs may resist adopting this technology into daily life. Early disengagement from LLMs may lead to unequal opportunities in future learning and professional development, exacerbating financial and educational inequalities. Thus, it is vital to understand how LLMs which are trained on data containing implicit biases reduce AI trustworthiness to determine its role in potential disparities in the adoption of this emerging technology. Yet, limited evidence has examined trustworthiness as a mechanism for poor adoption of emerging technologies often observed among stigmatized social groups (e.g., Shaouf &amp; Altaqqi, 2018; Venkatesh et al., 2000).

## 1.1.4. Perceptions of racial and gender biases

While  individuals  are  likely  to  reduce  AI  trustworthiness  when output is perceived as containing prejudice (Gupta et al., 2022; Wang et al., 2021), users are more likely to question algorithms when they depict racial compared to gender bias (Parra et al., 2021). These effects parallel work suggesting racist humor is perceived more inappropriately compared  to  sexist  humor  (Woodzicka  et  al.,  2015),  in  addition  to research  highlighting  how  confronting  sexism  may  result  in  social backlash compared to when addressing racism (e.g., being labelled as a complainer;  Kaiser &amp; Miller,  2001;  Woodzicka  et  al.,  2020).  Thus, whether racial prejudice generated by LLMs elicits similar reductions in AI trustworthiness compared to when other disadvantaged social groups are the target of prejudice (e.g., women) is unclear. Therefore, it is vital to understand how AI-generated racial versus gender prejudices may be differentially  perceived,  in  addition  to  understanding  their  impact among members of stigmatized groups.

## 1.2. Theoretical rationale

The  technology  acceptance  model  (TAM;  Davis,  1989)  suggests perceived usefulness and ease of use predicate adoption of emerging technologies. For instance, users who do not perceive virtual reality as useful  are  less  likely  to  use  these  systems  in  the  future  (Grani ´ c &amp; Maranguni ´ c, 2019). Similarly, acceptance of the automation of financial transactions,  office  tasks,  and  driving  an  automobile,  in  addition  to enterprise resource planning, e-banking, e-commerce, e-learning, telemedicine,  smart  home  devices,  online  video  gaming,  and  LLMs  are influenced by perceived usefulness and ease of use outlined by the TAM (Gefen, 2004; Gefen et al., 2003; Ghazizadeh et al., 2012; Holden &amp; Karsh, 2010; Kamal et al., 2020; Salloum et al., 2019; Sohn &amp; Kwon, 2020; Suh &amp; Han, 2002; Wu et al., 2011; Wu &amp; Liu, 2007; Zhang &amp; Liu,

2022). Notably, the TAM is rooted within the theory of reasoned action (TRA) and its extension, theory of planned behavior (TPB; Ajzen, 1991). The TRA outlines how an individual ' s intentions dictate future behaviors, suggesting a target for interventions (i.e., behavioral intentions) which  are  influenced  by  attitudes  (Ajzen &amp; Fishbein,  1980).  Yet, changing attitudes alone may not change behavioral intentions due to other  factors,  with  TPB  suggesting  perceived  control  as  a  necessary antecedent to successful behavior change (Ajzen, 1991). Similar to the TRA and TPB, TAM posits attitudes towards perceived usefulness and ease of use predict intentions to use technology, with later iterations of the  TAM suggesting usefulness of technology being the primary predictor of behavioral intentions, with ease of use increasing perceived usefulness (Legris et al., 2003; Venkatesh &amp; Morris, 2000).

Further  extensions  of  the  TAM  include  the  Unified  Theory  of Acceptance and Use of Technology (UTAUT), which integrates not only components from the TRA, TPB, and TAM, but additional models used to understand technology acceptance (e.g., motivational models; Momani, 2020; Venkatesh et al., 2003). Thus, the UTAUT proposes behavioral intentions to use emerging technologies are determined by performance expectancy (i.e., benefits of use), effort expectancy (i.e., ease of use), social influences (i.e., expectations from others to use technology), and facilitating conditions (e.g., infrastructure support within an organization).  Importantly,  the  UTAUT considers several moderators of these components,  such  as  gender,  age,  experience  with  technology,  and voluntariness of use (e.g., whether use of technology is mandated by an organization). Similar to the TAM, the UTAUT has been used to effectively  predict intentions to use enterprise systems, e-banking, e-commerce,  e-learning,  telemedicine,  smart  home  devices,  online  video gaming, and LLMs (Abbad, 2021; Chiemeke &amp; Evwiekpaefe, 2011; Daka &amp; Phiri, 2019; Haryanti &amp; Subriadi, 2020; Kohnke et al., 2014; Ling Keong et al., 2012; Menon &amp; Shilpa, 2023; Ramírez-Correa et al., 2019; Salomon &amp; Müller, 2019).

While the components outlined by the TAM and UTAUT are established  predictors  of  intentions  to  use  technology,  other  factors  not captured by these models may similarly impact behavioral intentions. For instance, trustworthiness of technology influences perceived utility of enterprise systems, e-commerce, e-banking, and online video gaming (Gefen, 2004; Gefen et al., 2003; Suh &amp; Han, 2002; Wu et al., 2011; Wu &amp; Liu, 2007). Recent applications of the UTAUT also highlight trustworthiness as an antecedent of performance expectancies for AI tools (Wanner  et  al.,  2022).  Notably,  while  the  TAM  and  UTAUT  predict adoption of prior technology, recent proliferation of AI-assisted tools may require revision of these models. For instance, LLMs produce responses with little transparency of decision making, perceived as a black box which does not allow for observation of their underlying processes (Savage, 2022; Von Eschenbach, 2021). This lack of transparency reduces  perceived  competence  of  AI  tools,  in  addition  to  diminishing confidence and trustworthiness of the knowledge base used to train AI (Wanner  et  al.,  2022),  potentially  violating  ability  assumptions  proposed by Mayer et al. (1995). Notably, reductions in trustworthiness predict poorer performance expectancies and reductions in behavioral intentions to use these tools (Wanner et al., 2022).

Thus, factors which lead users to question the competence of LLMs may diminish trustworthiness of AI and subsequent intentions to use these tools. Therefore, AI-generated prejudice may reduce confidence in the data used to train LLMs, diminishing trustworthiness and intentions to use AI tools in the future. Given the wealth of evidence highlighting the importance of trustworthiness in adoption of technology (Choudhury &amp; Shamszare, 2023; Dawar et al., 2022; Mostafa &amp; Kasamani, 2022), in addition to trustworthiness being central to frameworks regarding the successful implementation of AI (European Commision, 2024, 6 March), further work is needed to understand the impact of AI-generated prejudice on trustworthiness and intentions to use LLMs. Using  the  TAM,  UTAUT,  and  recent  extensions  of  these  models  suggesting the role of trustworthiness (Wanner et al., 2022), we theorize trustworthiness is diminished by negative experiences with emerging

Z.W. Petzel and L. Sowerby technologies,  subsequently  impacting  behavioral  intentions.  Thus, AI-generated prejudice is expected to reduce trustworthiness, lowering intentions  to  use  LLMs.  Further,  moderators  from  the  UTAUT  are investigated, examining the impact of group identification (e.g., gender, race),  age,  and prior experience with LLMs (Venkatesh et al., 2003). Importantly, prior applications of the TAM and UTAUT focus on gender differences in technology acceptance, with limited work investigating racial differences (e.g., Mitchell, Chebli, Ruggiero, &amp; Muramatsu, 2019; Porter &amp; Donthu, 2006). Thus, we extend the UTAUT by examining race, in  addition  to  gender,  age,  and  prior  experience,  as  moderators  to examine how AI-generated prejudice reduces trustworthiness and intentions (see Fig. 1).

## 1.3. Present research

Across three preregistered experiments, we examine how racial and gender  biases  generated  by  LLMs  may  reduce  trustworthiness  and behavioral  intentions  to  use  these  tools.  Experiment  1  tests  how  AIgenerated  racial  bias  affects  trustworthiness  and  intentions  to  use LLMs. Experiments 2 and 3 replicate patterns observed in Experiment 1, examining how AI-generated gender bias similarly reduce trustworthiness and behavioral intentions. Experiment 1 recruited from the general population in the United States, while Experiment 2 recruited from the general population in the United Kingdom. Experiment 3 recruited undergraduate students from the United Kingdom given recent endorsements for the use of AI in education (Russel Group, 2023) to understand factors which may impact student engagement with AI.

These experiments test whether trustworthiness is a mechanism for poorer intentions to use LLMs following exposure to AI-generated prejudice, in addition to examining whether moderating factors outlined by the UTAUT impact this relationship (race, gender, age, prior experience; see Fig. 1). Experiments also test individual differences which may influence trustworthiness of LLMs, such as implicit biases which capture prejudices against  Black  Americans  (Experiment  1)  and  women (Experiment 2), in addition to self-reported racist (Experiment 1) and sexist  attitudes  (Experiments  2  and  3)  which  provide  insight  into whether AI-generated responses are consistent with users ' expectations.

It  was  expected  (H1)  AI-generated  prejudice  (i.e.,  disparities  in salary or hourly wage) would reduce trustworthiness and intentions to use  LLMs  compared  to  responses  depicting  racial  or  gender  parity. However,  due  to  being  targets  of  implicit  biases  (H2)  AI-generated prejudices were expected to be most effective in reducing trustworthiness and intentions among Black Americans (Experiment 1) and women (Experiments  2  and  3)  compared  to  White  Americans  and  men,

Fig. 1. Model  of AI-generated prejudiced reducing trustworthiness and behavioral intentions.

<!-- image -->

Note. Solid lines represent relationships tested in the present research, dashed lines represent established relationships from the literature using the TAM and UTAUT (Davis, 1989; Venkatesh et al., 2003; Wanner et al., 2022).

respectively.  It  was  also  expected  (H3)  diminished  trustworthiness would account for reductions in intentions  to use LLMs, particularly among members of stigmatized groups (see Fig. 1). It was predicted that (H4-5) stronger implicit and explicit bias would increase trustworthiness and intentions when AI-generated responses were consistent with users ' own biases, particularly among White Americans (Experiment 1) and men (Experiment 2 and 3) who likely hold stronger biases against stigmatized groups. Hypotheses were preregistered via the Open Science Framework for Experiments 1 and 2 (https://osf.io/c6n93) and Experiment 3 (https://osf.io/rnpcg/), except for the mechanistic account of reduced  trustworthiness  accounting  for  diminished  intentions  to  use LLMs  (H3)  which  was  subsequently  informed  by  theory  described previously.

## 2. Experiment 1

## 2.1. Method - Experiment 1

## 2.1.1. Participants - Experiment 1

One  hundred  thirty  participants  from  the  United  States  were recruited to complete an online experiment using Prolific. Data was then examined for potential outliers or abnormal responses. No responses for dependent measures exceeded 3 standard deviations from the mean. Yet, three participants were removed from analyses due to responding too quickly (e.g., finishing study in less than 60 s; n = 2) or choosing a single response for all items in the survey ( n = 1). No other criteria were used to exclude participants. Remaining participants ( N = 127; M age = 42.18, SD age = 12.32) identified as either White (49.6%) or Black (50.4%), in addition to identifying as a Man (61.4%), Woman (35.4%), or another gender (e.g., non-binary; 3.2%). Most participants had an undergraduate (e.g., BA, BSc; 42.5%) or postgraduate degree (e.g., MA, MSc, PhD; 18.1%).  Remaining  participants  either  attended  university  without attaining a degree (18.1%), completed secondary education (13.4%), or only completed primary school (4%). Sample size was determined using the  effect  sizes  observed  in  prior  work  using  similar  paradigms (Woodzicka et al., 2015, 2020). An a priori power analysis assuming a moderate effect size ( η 2 = .06) indicated 128 participants were needed to achieve 80% power to detect significant interaction effects. However, we oversampled to account for attrition due to the online format of the experiment.

## 2.1.2. Materials and procedures - Experiment 1

Participants  first  read  an  information  sheet  detailing  the  study, which included a warning that participants would not receive compensation if there was any suspicion of sabotage or hurrying through the  study.  Participants  then  completed  an  Implicit  Association  Test (Sriram &amp; Greenwald, 2009) hosted via Millisecond.com (Seattle, WA) assessing implicit racial bias (White-Good/Black-Bad). During the task, participants sorted images of either Black or White faces, in addition to words associated with positive (e.g., pleasure, joyful) or negative attributes (e.g.,  terrible,  awful).  In  two  sets  of  critical  trials,  participants sorted White faces with positive words while Black faces were sorted with negative words (i.e., stereotypical congruence). Two additional sets of  critical  trials  switched  these  pairings  (i.e.,  stereotypical  incongruence). Critical blocks contained 20 trials, with order of blocks counterbalanced.  Scores  were  calculated  such  that  positive  scores  reflected stronger implicit racial biases (i.e., Black-Bad, White-Good). To ensure quality and reliability of the task data, any participant with more than 10% of their trials completed in less than 300ms were excluded from data analysis, consistent with guidelines outlined by Greenwald et al. (2003).  However,  no  participants  met  this  exclusion  criteria  for  this experiment.

Next,  participants  reported  explicit  racial  bias  using  the  Modern Racism Scale (Swim et al., 1995). Seven items ( α = .773) were rated from  1  ( Strongly  disagree )  to  5  ( Strongly  agree ),  with  example  items including, ' Discrimination against Blacks is no longer a problem in the

Z.W. Petzel and L. Sowerby

United States, ' and, ' It is easy to understand the anger of Black people in America. ' Participants also reported their experience with ChatGPT and other LLMs using 3-items adapted from Dawar et al. (2022; α = .895). Example items included, ' I often use chatbots (e.g., ChatGPT) to get my queries answered, ' and ' I like to use chatbots (e.g., ChatGPT). ' Items were rated from 1 ( Strongly disagree ) to 5 ( Strongly agree ).

Participants  then  viewed  a  screenshot  of  an  interaction  with ChatGPT depicting the creation of a table with the starting salaries of John and Jerome (i.e., ' John and Jerome are both starting as software developers for the same technology company in the US, can you make a table with their starting salaries? ' ). The use of stereotypic White and Black names was used to distinguish race of employees, with Jerome being established as a recognizable and typical Black name in the United States which has reliably been used in prior research to prime Black racial identities in sentence completion and behavioral tasks (Chang &amp; Mitchell, 2011; Greenwald et al., 1998; Ottaway et al., 2001; Wichman, 2012). Participants were then shown a neutral response displaying equity of pay across employees (i.e., $70,000) versus a biased response depicting salary inequities informed by statistics on racial disparities in the  United  States  (i.e.,  $70,000  for  John,  $52,900  for  Jerome;  U.S. Department of Labor, 2023).

Participants  completed  a  manipulation  check  to  ensure  biases ostensibly generated by ChatGPT were effective in reducing perceive fairness and validity of LLMs. Five items were rated on a 5-point Likert scale  assessing  perceived  agreement,  appropriateness,  effectiveness, excellence, and accuracy of the response generated by ChatGPT ( α = .942). Trustworthiness of AI was assessed using 3-items adapted from Dawar et al. (2022; α = .829). Example items included, ' I believe that chatbots (e.g., ChatGPT) are trustworthy, ' and, ' I believe chatbots (e.g., ChatGPT) will not act in a way that will be harmful to me. ' Future intentions  to  engage  with  LLMs  were  similarly  assessed  using  3-items adapted from Dawar et al. (2022; α = .916). Example items included, ' I intend to continue using chatbots (e.g., ChatGPT) in the future, ' and, ' I plan to continue to use chatbots (e.g., ChatGPT). ' All items assessing trustworthiness and behavioral intentions were rated from 1 ( Strongly disagree ) to 5 ( Strongly agree ). Ethical approval was granted by the authors ' university ethics committee prior to data collection. Materials and data  are  accessible  via  the  Open  Science  Framework  (https://osf. io/j3maf).

Table 1

Descriptive statistics (Experiment 1).

|                           | 1.            | 2.            | 3.            | 4.              | 5.            | 6.        | 7.         |
|---------------------------|---------------|---------------|---------------|-----------------|---------------|-----------|------------|
| Entire Sample ( N ¼ 127)  |               |               |               |                 |               |           |            |
| 1. Age                    | 42.18(12.31)  |               |               |                 |               |           |            |
| 2. Gender                 | GLYPH<0> .117 | .63(.48)      |               |                 |               |           |            |
| 3. Experience with LLMs   | .028          | GLYPH<0> .029 | 3.09(1.25)    |                 |               |           |            |
| 4. Implicit bias          | .019          | .099          | GLYPH<0> .008 | .31(.48)        |               |           |            |
| 5. Explicit bias          | GLYPH<0> .077 | .092          | .131          | GLYPH<0> .042   | 1.90(.98)     |           |            |
| 6. Trustworthiness        | .030          | .087          | .439***       | GLYPH<0> .234** | GLYPH<0> .005 | 3.46(.96) |            |
| 7. Behavioral Intentions  | .125          | .072          | .669***       | GLYPH<0> .187*  | GLYPH<0> .013 | .686***   | 3.70(1.11) |
| White Americans ( N ¼ 63) |               |               |               |                 |               |           |            |
| 1. Age                    | 42.69(11.81)  |               |               |                 |               |           |            |
| 2. Gender                 | GLYPH<0> .063 | .57(.49)      |               |                 |               |           |            |
| 3. Experience with LLMs   | .119          | GLYPH<0> .211 | 2.65(1.12)    |                 |               |           |            |
| 4. Implicit bias          | GLYPH<0> .037 | .191          | .057          | .45(.47)        |               |           |            |
| 5. Explicit bias          | GLYPH<0> .024 | .074          | .181          | GLYPH<0> .123   | 1.94(1.07)    |           |            |
| 6. Trustworthiness        | .118          | GLYPH<0> .067 | .446***       | GLYPH<0> .149   | .098          | 3.10(.92) |            |
| 7. Behavioral Intentions  | .216          | GLYPH<0> .136 | .781***       | GLYPH<0> .052   | .063          | .652***   | 3.24(1.13) |
| Black Americans ( N ¼ 64) |               |               |               |                 |               |           |            |
| 1. Age                    | 41.67(12.85)  |               |               |                 |               |           |            |
| 2. Gender                 | GLYPH<0> .161 | .69(.46)      |               |                 |               |           |            |
| 3. Experience with LLMs   | GLYPH<0> .015 | .046          | 3.54(1.22)    |                 |               |           |            |
| 4. Implicit bias          | .048          | .088          | .141          | .18(.46)        |               |           |            |
| 5. Explicit bias          | GLYPH<0> .141 | .134          | .129          | .026            | 1.86(.88)     |           |            |
| 6. Trustworthiness        | GLYPH<0> .016 | .168          | .266*         | GLYPH<0> .142   | GLYPH<0> .099 | 3.82(.87) |            |
| 7. Behavioral Intentions  | .091          | .236          | .438***       | GLYPH<0> .124   | GLYPH<0> .087 | .607***   | 4.15(.89)  |

Note . Gender was dummy-coded (0 = woman, 1 = man) which resulted in participants ( n = 3) identifying as another gender (e.g., non-binary) being removed from pairwise correlations. * p &lt; .05, ** p &lt; .01, *** p &lt; .001.

## 2.2. Results - Experiment 1

Descriptive statistics for variables are displayed in Table 1. A series of independent  samples t -tests  indicated  that  age,  race,  gender,  prior experience with LLMs, in addition to implicit and explicit biases, did not differ between conditions, all p s &gt; .301, suggesting effective random assignment and similar populations recruited for each condition.

## 2.2.1. Manipulation check - Experiment 1

A 2 (LLM response: neutral, racial bias) x 2 (Race of participant: White,  Black)  between-subjects  ANOVA  indicated  a  significant  main effect of condition on appropriateness of the LLM response, F (1,123) = 10.46, p = .002, η p 2 = .078. Responses depicting racial disparities elicited lowered perceptions of their accuracy ( M = 3.45, SD = 1.05) compared to neutral responses ( M = 3.96, SD = .81). A main effect emerged for race, F (1,123) = 9.95, p = .002, η p 2 = .075, with Black participants ( M = 3.97, SD = .93)  rating  responses  generated  by  ChatGPT  as  more appropriate than White participants ( M = 3.46, SD = .93). No interaction emerged among factors, F (1,123) = .01, p = .923, η p 2 &lt; .001, suggesting  the  differences  in  perceived  validity  were  similar  across conditions, regardless of the participant race. Analyses of covariance (ANCOVAs) examined the influence of moderating variables outlined by the  UTAUT.  Prior  experience  with  LLMs  (e.g.,  ChatGPT),  age,  and gender were not significantly related to perceived appropriateness of responses, all p s &gt; .108.

## 2.2.2. Trustworthiness - Experiment 1

A  2  (LLM  response)  x  2  (Race  of  participant)  between-subjects ANOVA  indicated  a  main  effect  of  condition  on  trustworthiness, F (1,123) = 4.77, p = .031, η p 2 = .037, suggesting neutral responses ( M = 3.63, SD = .98)  elicited  higher  trustworthiness  than  prejudiced  responses ( M = 3.30, SD = .93). Black participants ( M = 3.83, SD = .88) reported higher trustworthiness compared to White participants ( M = 3.11, SD = .93), F (1,123) = 20.96, p &lt; .001, η p 2 = .146. An interaction emerged between factors, F (1,123) = 5.69, p = .019, η p 2 = .044. Among Black participants, AI-generated prejudice reduced trustworthiness ( M = 3.46, SD = .94) compared to neutral responses ( M = 4.17, SD = .66), F (1,123) = 10.54, p = .002, η p 2 = .079. However, White participants reported  similar  trustworthiness  regardless  of  the  presence  of  racial

Z.W. Petzel and L. Sowerby disparities in pay, F (1,123) = .02, p = .887, η p 2 &lt; .001 (see Fig. 2a). An ANCOVA indicated experience with LLMs was significantly related to trustworthiness, p &lt; .001.  Analyses  controlling  for  LLM  experience reduced significance of the main effect of condition, p = .051. Yet, effects of race and interaction among factors were unaffected by experience,  both p s &lt; .030.  Thus,  while  LLM  experience  predicted  higher trustworthiness, this relationship did not explain racial differences in trustworthiness observed after viewing AI-generated prejudice. Age and gender did not predict trustworthiness, both p s &gt; .535.

## 2.2.3. Behavioral intentions - Experiment 1

A  2  (LLM  response)  x  2  (Race  of  participant)  between-subjects ANOVA indicated a significant main effect of condition on behavioral intentions, F (1,123) = 5.38, p = .022, η p 2 = .042, indicating greater intentions  to  use  LLMs  when provided  neutral ( M = 3.89, SD = 1.15) compared to prejudiced responses ( M = 3.49, SD = 1.04). Black participants ( M = 4.15, SD = .89) reported greater behavioral intentions compared to White participants  ( M = 3.24, SD = 1.13), F (1,123) = 26.12, p &lt; .001, η p 2 = .175. LLM response significantly interacted with the race of participant, F (1,123) = 5.69, p = .019, η p 2 = .044, suggesting that prejudiced responses diminished intentions ( M = 3.74, SD = 1.00) compared to neutral responses ( M = 4.54, SD = .56) among Black participants, F (1,123) = 10.34, p = .002, η p 2 = .078. White participants reported similar intentions regardless of response, F (1,123) = .01, p = .939, η p 2 &lt; .001 (see Fig. 2b). ANCOVAs indicated experience with LLMs and age both significantly predicted behavioral intentions, both p s &lt; .036.  Yet,  analyses  controlling  for  LLM  experience  and  age  did  not impact main effects or interactions, all p s &lt; .034. Thus, while greater experience with LLMs and younger age did predict greater intentions to use these tools, these relationships did not explain the racial differences in behavioral intentions observed after viewing AI-generated prejudice. Gender was not associated with behavioral intentions, p = .841.

## 2.2.4. Trustworthiness as a mechanism - Experiment 1

The PROCESS macro (model 8; Hayes, 2012) tested whether LLM trustworthiness explained reduced behavioral intentions after viewing neutral  versus  prejudiced  responses.  The  model  specified  condition

Fig. 2. Trustworthiness and intentions as a function of LLM response and race (Experiment 1).

<!-- image -->

Note . Error bars represent standard error. ** p &lt; .01.

(LLM response) as the predictor of behavioral intentions, with trustworthiness entered as the mediator. Race was entered as a moderator for these pathways to understand racial differences in the proposed model. Trustworthiness predicted behavioral intentions, t (122) = 8.26, b = .68 , p &lt; .001, and a significant moderated-mediation effect emerged, b = GLYPH&lt;0&gt; .50, SE = .23,  95%  CI  [ GLYPH&lt;0&gt; .96, GLYPH&lt;0&gt; .07].  Among  Black  participants, exposure to AI-generated prejudice elicited lower trustworthiness which then accounted for reductions in intentions, b = GLYPH&lt;0&gt; .48, SE = .15, 95% CI [ GLYPH&lt;0&gt; .79, GLYPH&lt;0&gt; .07] (see Fig. 3). No mediation effect emerged among White participants, b = .02, SE = .16, 95% CI [ GLYPH&lt;0&gt; .27, .34], suggesting trustworthiness may not be a necessary antecedent for adoption of LLMs among members of advantaged social groups. Analyses controlling for prior experience with LLMs, age, and gender did not impact effects, b = GLYPH&lt;0&gt; .28, SE = .16,  95%  CI  [ GLYPH&lt;0&gt; .62, GLYPH&lt;0&gt; .01],  suggesting  these  moderators outlined  by  the  UTAUT  did  not  affect  the  mechanistic  account  of trustworthiness.

## 2.2.5. Explicit biases - Experiment 1

White ( M = 3.74, SD = 1.00) and Black participants ( M = 3.74, SD = 1.00) reported similar explicit racial biases, t (125) = .44, p = .659, d = .08, 95% CI [ GLYPH&lt;0&gt; .27, .42]. The PROCESS macro for SPSS (model 3; Hayes, 2012)  was  used  to  examine  the  role  of  explicit  biases  in  predicting trustworthiness and intentions after viewing neutral versus prejudiced responses between Black and White participants. The interaction was not significant in predicting trustworthiness, F (1,119) = .01, p = .947, Δ R 2 &lt; .001, 95% CI [ GLYPH&lt;0&gt; .63, .67], or behavioral intentions, F (1,119) = .39, p = .534, Δ R 2 = .002, 95% CI [ GLYPH&lt;0&gt; .96, .50].

## 2.2.6. Implicit biases -Experiment 1

White participants exhibited stronger stereotypical associations ( M = .45, SD = .47; i.e., Black-Bad) compared to Black participants ( M = .19, SD = .47), t (125) = 3.24, p &lt; .001, d = .58, 95% CI [.10, .43]. The PROCESS macro for SPSS (model 3; Hayes, 2012) was used to examine the role of implicit biases in predicting trustworthiness and intentions after viewing neutral versus prejudiced responses between Black and White  participants.  An  interaction  among  factors  emerged  for  trustworthiness, F (1,119) = 4.12, p = .045, Δ R 2 = .036, 95% CI [ GLYPH&lt;0&gt; 2.65, GLYPH&lt;0&gt; .03], indicating stereotype-consistent biases (i.e., Black-Bad) predicted lower  trustworthiness  among  White  participants  shown  neutral  responses, t (119) =GLYPH&lt;0&gt; 2.59, p = .011, b =GLYPH&lt;0&gt; .77, 95% CI [ GLYPH&lt;0&gt; 1.37, GLYPH&lt;0&gt; .18] (see Fig.  4).  Implicit  racial  biases  did  not  predict  trustworthiness  among Black participants or among White participants shown prejudiced responses, all p s &gt; .266. Analyses controlling for additional moderating variables outlined by the UTAUT indicated age and gender were unrelated to trustworthiness in the model, both p s &gt; .157. However, previous experience with LLMs did predict greater trustworthiness in the model ( p = .002)  which  then  reduced  the  significance  of  the  observed

<!-- image -->

Direct effect,b=-0.32,95%CI[-0.72,0.10]

Conditional indirect effect of trustworthiness,b=-0.48,95% CI[-0.79,-0.07]

Fig. 3. Trustworthiness as a mediator of the effects of AI-generated prejudice on intentions among Black americans (Experiment 1). Note. ** p &lt; .01, *** p &lt; .001.

Z.W. Petzel and L. Sowerby

Fig. 4. Trustworthiness as a function of implicit bias and LLM response among White participants (Experiment 1).

<!-- image -->

Note . * p &lt; .05.

interaction between implicit biases, condition, and race of participant, p = .104. Thus, greater experience with LLMs may diminish the influence of implicit racial biases on trustworthiness of these tools.

Similarly,  an  interaction  among  factors  emerged  for  intentions, F (1,119) = 5.05, p = .027, Δ R 2 = .030, 95% CI [ GLYPH&lt;0&gt; 3.12, GLYPH&lt;0&gt; .20]. Stronger implicit  biases  predicted  lower  intentions  to  use  LLMs  among  White participants shown neutral responses, t (119) = GLYPH&lt;0&gt; 2.51, p = .014, b = GLYPH&lt;0&gt; .84, 95% CI [ GLYPH&lt;0&gt; 1.50, GLYPH&lt;0&gt; .18]. Conversely, White participants exhibiting strong implicit biases reported greater intentions when shown responses depicting racial prejudice, t (119) = 2.27, p = .025, b = .92, 95% CI [.12, 1.72] (see Fig. 5). Implicit racial biases did not predict intentions among Black participants, both p s &gt; .526. Analyses controlling for additional moderating variables indicated age and gender were unrelated to intentions in the model, both p s &gt; .110. However, prior LLM experience predicted greater intentions ( p &lt; .001) which also reduced the significance of the observed interaction between implicit biases, condition, and race of participant, p = .087. Thus, greater experience with LLMs may  similarly  diminish  the  influence  of  implicit  racial  biases  on behavioral intentions to use these tools.

Fig. 5. Intentions as a function of implicit bias and LLM response among White participants (Experiment 1).

<!-- image -->

Note . * p &lt; .05.

## 3. Experiment 2

## 3.1. Method - Experiment 2

## 3.1.1. Participants - Experiment 2

One  hundred  thirty  participants  from  the  United  Kingdom  were recruited  via  Prolific.  Data  was  examined  for  potential  outliers  or abnormal responses. No responses for dependent measures exceeded 3 standard deviations from the mean. Yet, one participant was removed from the analyses due to responding too quickly (e.g., finishing in less than  60  s).  No  other  criteria  were  used  to  exclude  participants. Remaining participants ( N = 129; M age = 39.90, SD age = 12.80) identified as White (86.8%), Asian (7.7%), or another race (5.5%), in addition to either identifying as a Man (48.8%) or a Woman (51.2%). Most participants  had  an  undergraduate  (e.g.,  BA,  BSc;  36.4%)  or  postgraduate degree (e.g., MA, MSc, PhD; 16.3%). The remaining participants  either  completed  vocational  training  (14.0%),  attended  some university without attaining a degree (13.2%), or only completed secondary  education  (19.4%).  Sample  size  was  determined  using  the rationale described in Experiment 1.

## 3.1.2. Materials and procedures - Experiment 2

As described in Experiment 1, participants first read an information sheet including a warning to ensure accurate and reliable data before completing  an  Implicit  Association  Test  adapted  to  assess  implicit gender biases (Man-Career/Woman-Family). Thus, participants sorted names of men and women (e.g., Ben, John, Julia, Emily), in addition to words linked to careers (e.g., office, professional) or family (e.g., home, children).  Scores  were  calculated  such  that  positive  scores  reflected stronger implicit gender biases (i.e., Man-Career, Woman-Family). As in Experiment 1, no participants met exclusion criteria for this task suggesting data was reliable (e.g., no participant with more than 10% of their  trials  completed  in  less  than  300ms;  Greenwald  et  al.,  2003). Participants then self-reported explicit gender biases using the Modern Sexism Scale (Swim et al., 1995). Seven items ( α = .893) were rated from 1 ( Strongly disagree ) to 5 ( Strongly agree ), with example items including, ' Discrimination against women is no longer a problem in the United Kingdom, ' and, ' Women often miss out on good jobs due to  sexual discrimination. ' Experience with LLMs was assessed as in Experiment 1.

Next,  participants  viewed  a  screenshot  of  an  interaction  with ChatGPT depicting the creation of a table with the starting salaries of John and Judi (i.e., ' John and Judi are both starting as junior doctors in the NHS, can you make a table with their starting salaries? ' ). Participants were randomly assigned to view a neutral response displaying equality of pay across individuals (i.e., £ 33,000) versus biased responses depicting disparities informed by statistics on gender inequities in the United Kingdom (i.e., £ 33,000 for John versus £ 28,050 for Judi; Office for National Statistics, 2022). Participants then completed a manipulation check, in addition to measures of trustworthiness of LLMs and intentions to use LLMs as described in Experiment 1. Ethical approval was granted  by  the  authors ' university  ethics  committee  prior  to  data collection.  Materials  and  data  are  accessible  via  the  Open  Science Framework (https://osf.io/j3maf/).

## 3.2. Results - Experiment 2

Descriptive statistics for variables are displayed in Table 2. A series of independent  samples t -tests  indicated  that  age,  race,  gender,  prior experience with LLMs, in addition to implicit and explicit biases, did not differ between conditions, all p s &gt; .251, suggesting effective random assignment and similar populations recruited for each condition.

## 3.2.1. Manipulation check - Experiment 2

A 2 (LLM response: neutral, gender bias) x 2 (Gender of participant: man,  woman) between-subjects  ANOVA  indicated  a  significant  main effect  of  condition, F (1,125) = 24.75, p &lt; .001, η p 2 = .165,  on

Z.W. Petzel and L. Sowerby

Table 2

Descriptive statistics for Experiment 2.

|                          | 1.             | 2.             | 3.        | 4.             | 5.            | 6.        | 7.        |
|--------------------------|----------------|----------------|-----------|----------------|---------------|-----------|-----------|
| Entire Sample ( N ¼ 129) |                |                |           |                |               |           |           |
| 1. Age                   | 38.89(12.79)   |                |           |                |               |           |           |
| 2. Race                  | GLYPH<0> .017  | .86(.33)       |           |                |               |           |           |
| 3. Experience with LLMs  | GLYPH<0> .197* | GLYPH<0> .072  | 2.54(.96) |                |               |           |           |
| 4. Implicit bias         | GLYPH<0> .068  | GLYPH<0> .142  | .210*     | .47(.4)        |               |           |           |
| 5. Explicit bias         | GLYPH<0> .120  | GLYPH<0> .102  | .053      | GLYPH<0> .181* | 2.44(.83)     |           |           |
| 6. Trustworthiness       | GLYPH<0> .113  | GLYPH<0> .102  | .383***   | .041           | .122          | 3.17(.92) |           |
| 7. Behavioral Intentions | GLYPH<0> .068  | GLYPH<0> .129  | .642***   | .195*          | .104          | .572***   | 3.27(.95) |
| Men ( N ¼ 63)            |                |                |           |                |               |           |           |
| 1. Age                   | 38.06(13.49)   |                |           |                |               |           |           |
| 2. Race                  | GLYPH<0> .034  | .80(.39)       |           |                |               |           |           |
| 3. Experience with LLMs  | GLYPH<0> .138  | GLYPH<0> .244  | 2.62(.96) |                |               |           |           |
| 4. Implicit bias         | GLYPH<0> .024  | GLYPH<0> .266* | .278*     | .47(.45)       |               |           |           |
| 5. Explicit bias         | GLYPH<0> .175  | .082           | .017      | GLYPH<0> .271* | 2.75(.79)     |           |           |
| 6. Trustworthiness       | .005           | GLYPH<0> .146  | .285*     | .205           | .091          | 3.15(.88) |           |
| 7. Behavioral Intentions | GLYPH<0> .102  | GLYPH<0> .209  | .649***   | .387**         | GLYPH<0> .007 | .588***   | 3.41(.91) |
| Women ( N ¼ 66)          |                |                |           |                |               |           |           |
| 1. Age                   | 39.69(12.13)   |                |           |                |               |           |           |
| 2. Race                  | GLYPH<0> .021  | .92(.26)       |           |                |               |           |           |
| 3. Experience with LLMs  | GLYPH<0> .251* | .202           | 2.46(.95) |                |               |           |           |
| 4. Implicit bias         | GLYPH<0> .128  | .083           | .128      | .47(.35)       |               |           |           |
| 5. Explicit bias         | GLYPH<0> .026  | GLYPH<0> .228  | .030      | GLYPH<0> .108  | 2.14(.76)     |           |           |
| 6. Trustworthiness       | GLYPH<0> .232  | GLYPH<0> .063  | .475***   | GLYPH<0> .141  | .182          | 3.18(.96) |           |
| 7. Behavioral Intentions | GLYPH<0> .018  | .022           | .631***   | GLYPH<0> .021  | .115          | .575***   | 3.14(.97) |

Note . Race was dummy-coded (0 = non-White, 1 = White). * p &lt; .05, ** p &lt; .01, *** p &lt; .001.

appropriateness  of  the  response.  Gender  disparities  in  salary  elicited lower perceived validity ( M = 3.23, SD = .99) compared to neutral responses ( M = 3.99, SD = .73). Gender, F (1,125) = 1.43, p = .235, η p 2 = .011, and its interaction with LLM response, F (1,125) = .06, p = .814, η p 2 &lt; .001, did not impact validity. ANCOVAs which examined the influence of moderating variables outlined by the UTAUT indicated experience  with  LLMs,  age,  and  race  were  not  significantly  related  to appropriateness of responses all p s &gt; .144.

## 3.2.2. Trustworthiness - Experiment 2

A 2 (LLM response) x 2 (Gender of participant) between-subjects ANOVA indicated no main effects for condition, F (1,125) = 1.41, p = .238, η p 2 = .011, or gender, F (1,125) = .05, p = .830, η p 2 &lt; .001, on trustworthiness. Yet, an interaction emerged between factors, F (1,125) = 4.59 p = .034, η p 2 = .035, suggesting women reported greater trustworthiness  when  shown  neutral  ( M = 3.46, SD = .94)  compared  to prejudiced responses ( M = 2.92, SD = 1.01), F (1,125) = 5.67, p = .019, η p 2 = .043. Trustworthiness did not differ among men, F (1,125) = .45 p = .505, η p 2 = .004 (see Fig. 6a). An ANCOVA indicated experience with LLMs was significantly related to trustworthiness, p &lt; .001. However, analyses  controlling  for  LLM  experience  did  not  significantly  reduce interaction  effects, p = .006.  Thus,  while  LLM  experience  predicted higher trustworthiness, this relationship did not explain gender differences in trustworthiness observed after viewing AI-generated prejudice. Age and race did not predict trustworthiness, both p s &gt; .118.

## 3.2.3. Behavioral intentions - Experiment 2

A 2 (LLM response) x 2 (Gender of participant) between-subjects ANOVA indicated no effects for condition, F (1,125) = .06, p = .813, η p 2 &lt; .001, gender, F (1,125) = 2.63, p = .107, η p 2 = .021, or their interaction, F (1,125) = .36, p = .551, η p 2 = .003 (see Fig. 6b). An ANCOVA indicated LLM experience was related to intentions, p &lt; .001. However, analyses  controlling  for  experience  did  not  affect  the  significance  of main effects or the interaction among factors, all p s &gt; .191. Age and race did not predict intentions, both p s &gt; .205.

## 3.2.4. Trustworthiness as a mechanism - Experiment 2

The  PROCESS macro (model 8; Hayes, 2012) was used to  test if trustworthiness  explained  reductions  in  behavioral  intentions  after viewing neutral versus prejudiced LLMs. The model specified condition

<!-- image -->

GenderofParticipant

Fig.  6. Trustworthiness  and  intentions  as  a  function  of  LLM  response  and gender (Experiment 2).

Note . Error bars represent standard error. * p &lt; .05.

(LLM response) as the predictor of behavioral intentions, with trustworthiness entered as the mediator. Gender was used as a moderator for these pathways to understand gender differences in the proposed model. Trustworthiness predicted intentions, t (124) = 8.44, b = .63 , p &lt; .001, and a significant moderated-mediation effect emerged, b = GLYPH&lt;0&gt; .44, SE = .22,  95%  CI  [ GLYPH&lt;0&gt; .88, GLYPH&lt;0&gt; .03].  Among  women,  AI-generated  prejudice decreased trustworthiness which then reduced behavioral intentions, b = .40, SE = .19, 95% CI [.02, .78] (see Fig. 7). No mediation effect emerged for men, b = - .24, SE = .19, 95% CI [ GLYPH&lt;0&gt; .62, .14], suggesting trustworthiness may not be a necessary antecedent for adoption of LLMs

Z.W. Petzel and L. Sowerby

<!-- image -->

Direct effect,b =0.40,95%CI[0.02,0.78]

Conditional indirect effect of trustworthiness,b =-0.44,95%Cl[-0.88,-0.03]

Fig. 7. Trustworthiness as a mediator of the effects of AI-generated prejudice on intentions among women (Experiment 2). Note. * p &lt; .05, *** p &lt; .001.

among members of advantaged groups. Analyses controlling for prior experience with LLMs, age, and race did not impact effects, b =GLYPH&lt;0&gt; .36, SE = .15, 95% CI [ GLYPH&lt;0&gt; .68, GLYPH&lt;0&gt; .10].

## 3.2.5. Explicit biases - Experiment 2

Men ( M = 2.76, SD = .79) self-reported significantly higher explicit gender biases compared to women ( M = 2.15, SD = .76), t (127) = 4.47, p &lt; .001, d = .79, 95% CI [.34, .88]. The PROCESS macro for SPSS (model 3; Hayes, 2012) examined the role of explicit biases in predicting trustworthiness and intentions after viewing neutral versus prejudiced responses between men and women. Explicit gender biases were unrelated to trustworthiness and intentions, both p s &gt; .471. The interaction among factors was not significant in  predicting  trustworthiness, F (1, 121) = .12, p = .729, Δ R 2 = .001, 95% CI [ GLYPH&lt;0&gt; .72, 1.02], or intentions, F (1,119) = .24, p = .628, Δ R 2 = .002, 95% CI [ GLYPH&lt;0&gt; .69,1.14]. Analyses controlling  for  LLM  experience,  age,  and  race  did  not  impact  interactions, all p s &gt; .630.

## 3.2.6. Implicit biases - Experiment 2

Men exhibited similar implicit gender biases (i.e., Man-Career; M = .48, SD = .46) compared to women ( M = .47, SD = .35), t (127) = .11, p = 913, d = GLYPH&lt;0&gt; .02, 95% CI [ GLYPH&lt;0&gt; .13, .15]. The PROCESS macro for SPSS (model 3; Hayes, 2012) was used to examine the role of implicit gender biases  in  predicting  trustworthiness  and  behavioral  intentions  after viewing  neutral  versus  biased  responses  between  men  and  women. However, implicit gender biases were unrelated to trustworthiness and intentions,  both p s &gt; .422.  Implicit  biases  interacted  with  gender  of participant to predict behavioral intentions, F (1,125) = 4.08, p = .046, Δ R 2 = .030,  95%  CI  [ GLYPH&lt;0&gt; 1.65, GLYPH&lt;0&gt; .02].  Among  men,  stronger  implicit gender biases predicted greater intentions to use LLMs, t (125) = 3.03, p = .003, b = .78, 95% CI [.27, 1.28]. Implicit bias was unrelated to intentions among women, t (125) = GLYPH&lt;0&gt; .18, p = .861, b = GLYPH&lt;0&gt; .06, 95% CI [ GLYPH&lt;0&gt; .70, .58] (see Fig. 8). While LLM experience was associated with intentions, b = .62, p &lt; .001, controlling for experience did not reduce significance of the interaction, p = .041. The interaction between implicit biases, gender, and condition did not predict trustworthiness, F (1, 121) &lt; .001, p = .985, Δ R 2 &lt; .001, 95% CI [ GLYPH&lt;0&gt; 1.70, 1.74], or intentions, F (1,121) = .03, p = .854, Δ R 2 &lt; .001, 95% CI [ GLYPH&lt;0&gt; 1.60, 1.93]. Covarying for  familiarity,  age,  and  race  did  not  impact  the  interaction  among factors in predicting outcomes, all p s &gt; .112.

## 4. Experiment 3

## 4.1. Method - Experiment 3

## 4.1.1. Participants - Experiment 3

Two hundred and one psychology students were recruited from a UK

Fig. 8. Intentions as a function of implicit biases and gender (Experiment 2). Note . ** p &lt; .01.

<!-- image -->

university to participate in an online experiment. Data was examined for potential outliers or abnormal responses. No responses for dependent measures exceeded 3 standard deviations from the mean and no participants  met  criteria  for  exclusion  (e.g.,  responding  too  quickly, choosing one response for all items). Participants ( M age = 19.58, SD age = 1.60) identified as White (89.6%), Asian (4.5%), or another race (5.9%), in addition to either identifying as a Man (17.6%) or a Woman (82.4%). Sample size was determined using rationale described in Experiment 1. However, since psychology programs are typically female-dominated (Marulanda &amp; Radtke,  2019),  we  oversampled  to  recruit  sufficient male participants.

## 4.1.2. Materials and procedures - Experiment 3

Prior work suggests exposure to discrimination may increase vigilance for prejudice (Ramos et al., 2016). Thus, measuring implicit and explicit biases at the start of Experiments 1 and 2 may have incidentally increased participants ' awareness of AI-generated prejudices depicted in manipulations, confounding prior findings. Thus, the primary aim of the current experiment was to replicate the impact of AI-generated prejudice  on  trustworthiness  and  intentions  without  priming  participants about  inequities.  Thus,  materials  and  procedures  are  identical  to Experiment 2, except for explicit gender biases  (i.e.,  Modern  Sexism Scale;  Swim  et  al.,  1995)  being  assessed  after  the  manipulation  and dependent  measures  (i.e.,  manipulation  check,  trustworthiness,  intentions).  However,  implicit  gender  biases  (i.e.,  Implicit  Association Test) were removed from this experiment as this was not the primary focus of this iteration of the study, in addition to minimizing the time to complete the experiment.

Further, the prompt shown during the interaction with ChatGPT was altered to be more relevant to students (i.e., ' John and Judi are undergraduate students, and both will be starting as research assistants in a lab at their university in the UK, can you make a table suggesting their hourly  wages? ' ),  with  hourly  wages  depicted  as  equal  (i.e., £ 13.76/ hour)  versus  having  disparities  informed  by  statistics  on  gender  inequalities  in  the  United  Kingdom  (i.e., £ 13.76/hour  for  John  versus £ 11.70/hour  for  Judi;  Office  for  National  Statistics,  2022).  Ethical approval was granted by the university ethics committee prior to data collection.  Materials  and  data  are  accessible  via  the  Open  Science Framework (https://osf.io/j3maf/).

## 4.2. Results - Experiment 3

Descriptive statistics for variables are displayed in Table 3. A series of independent samples t -tests indicated that age, race, gender, and explicit biases did not differ between conditions, all p s &gt; .119. However, prior experience with LLMs was significantly different between conditions, t

Z.W. Petzel and L. Sowerby

Table 3

Descriptive statistics for Experiment 3.

|                                          | 1.                       | 2.                       | 3.                       | 4.                       | 5.                       | 6.                       |
|------------------------------------------|--------------------------|--------------------------|--------------------------|--------------------------|--------------------------|--------------------------|
| Entire Sample ( N ¼ 221)                 | Entire Sample ( N ¼ 221) | Entire Sample ( N ¼ 221) | Entire Sample ( N ¼ 221) | Entire Sample ( N ¼ 221) | Entire Sample ( N ¼ 221) | Entire Sample ( N ¼ 221) |
| 1. Age                                   | 19.58 (1.59)             |                          |                          |                          |                          |                          |
| 2. Race                                  | GLYPH<0> .182**          | .89 (.30)                |                          |                          |                          |                          |
| 3. Experience with LLMs                  | .104                     | GLYPH<0> .113            | 1.99 (.93)               |                          |                          |                          |
| 4. Explicit bias                         | .100                     | GLYPH<0> .096            | .106                     | 1.91 (.59)               |                          |                          |
| 5.                                       | .048                     | .013                     | .298***                  | .030                     | 2.85                     |                          |
| Trustworthiness 6. Behavioral Intentions | .017                     | GLYPH<0> .043            | .698***                  | .110                     | (.83) .486***            | 2.35 (.95)               |
| Men ( N ¼ 39)                            | Men ( N ¼ 39)            | Men ( N ¼ 39)            | Men ( N ¼ 39)            | Men ( N ¼ 39)            | Men ( N ¼ 39)            | Men ( N ¼ 39)            |
| 1. Age                                   | 19.69                    |                          |                          |                          |                          |                          |
| 2. Race                                  | .087                     | .84 (.36)                |                          |                          |                          |                          |
| 3. Experience with LLMs                  | .020                     | GLYPH<0> .087            | 2.23 (1.05)              |                          |                          |                          |
| 4. Explicit bias                         | .190                     | GLYPH<0> .021            | .363*                    | 2.17 (.67)               |                          |                          |
| 5.                                       | .066                     | .058                     | .337*                    | .323*                    | 3.05                     |                          |
| Trustworthiness 6. Behavioral            | GLYPH<0> .110            | GLYPH<0> .070            | .842***                  | .292                     | (.85) .422***            | 2.70                     |
| Women ( N ¼ 182)                         | Women ( N ¼ 182)         | Women ( N ¼ 182)         | Women ( N ¼ 182)         | Women ( N ¼ 182)         | Women ( N ¼ 182)         | Women ( N ¼ 182)         |
| 1. Age                                   | 19.56 (1.7)              |                          |                          |                          |                          |                          |
| 2. Race                                  | GLYPH<0> .228**          | .90 (.29)                |                          |                          |                          |                          |
| 3. Experience with LLMs                  | .116                     | GLYPH<0> .111            | 1.94 (.90)               |                          |                          |                          |
| 4. Explicit bias                         | .086                     | GLYPH<0> .102            | .003                     | 1.85 (.56)               |                          |                          |
| 5.                                       | .043                     | .012                     | .276***                  | GLYPH<0> .074            | 2.81                     |                          |
| Trustworthiness 6. Behavioral Intentions | .031                     | GLYPH<0> .017            | .647***                  | .009                     | (.82) .496***            | 2.28 (.89)               |

Note . Race was dummy-coded (0 = non-White, 1 = White). * p &lt; .05, ** p &lt; .01, *** p &lt; .001.

(219) = 2.04, p = .042, d = .28, indicating participants in the control condition reported greater experience with LLMs ( M = 2.12, SD = .96) compared to those randomly assigned to the experimental condition ( M = 1.87, SD = .91).

## 4.2.1. Manipulation check - Experiment 3

A 2 (LLM response: neutral, gender bias) x 2 (Gender of participant: man,  woman) between-subjects  ANOVA  indicated  a  significant  main effect of condition, F (1,217) = 26.94, p &lt; .001, η p 2 = .110, on appropriateness of the response. Gender disparities in wages elicited lower perceived validity ( M = 2.91, SD = .85) compared to neutral responses ( M = 3.47, SD = .70). Gender, F (1,217) = .85, p = .358, η p 2 = .004, and the interaction among factors, F (1,217) = 3.38, p = .067, η p 2 = .015, did not affect appropriateness of response. ANCOVAs which examined the influence of moderating variables outlined by UTAUT indicated prior experience with LLMs, age, and race were unrelated to perceived validity, all p s &gt; .522.

## 4.2.2. Trustworthiness - Experiment 3

A 2 (LLM response) x 2 (Gender of participant) between-subjects ANOVA indicated no effects for condition, F (1,217) = .81, p = .368, η p 2 = .004, or gender, F (1,217) = 2.66, p = .105, η p 2 = .012, on trustworthiness. Yet, an interaction emerged between factors, F (1,217) = 5.61, p = .019, η p 2 = .025,  suggesting  women  reported  higher  trustworthiness  when  shown  neutral  ( M = 3.05, SD = .88)  compared  to prejudiced responses ( M = 2.58, SD = .70), F (1,217) = 15.35, p &lt; .001, η p 2 = .066. Trustworthiness did not differ among men, F (1,217) = .65, p

= .421, η p 2 = .003  (see  Fig.  9a).  An  ANCOVA  indicated  prior  LLM experience was related to trustworthiness, p &lt; .001. Analyses controlling for prior experience did not affect interaction among factors, p = .042.  Thus,  while  experience  predicted  higher  trustworthiness,  this relationship  did  not  explain  gender  differences  in  trustworthiness observed after viewing AI-generated prejudice. Age and race did not predict trustworthiness, both p s &gt; .377.

## 4.2.3. Behavioral intentions - Experiment 3

A 2 (LLM response) x 2 (Gender of participant) between-subjects ANOVA indicated no main effects for condition, F (1,217) = 2.65, p = .105, η p 2 = .012. Yet, men reported greater intentions to use LLMs ( M = 2.70, SD = 1.15) compared to women ( M = 2.29, SD = .89), F (1,217) = 6.87, p = .009, η p 2 = .031. No interaction emerged among factors for intentions, F (1,217) = .09, p = .763, η p 2 &lt; .001 (see Fig. 9b). An ANCOVA indicated  prior  LLM  experience  was  related  to  intentions,  p &lt; .001. Analyses controlling for experience reduced the significance of gender differences in behavioral intentions, p = .051. Thus, while men reported greater intentions to use LLMs compared to women, it is likely these effects were due to different levels of experience using AI tools rather than gender. Age and race did not predict behavioral intentions, both ps &gt; .842.

## 4.2.4. Trustworthiness as a mechanism - Experiment 3

The  PROCESS  macro (model 8;  Hayes,  2012)  was  used  to  test  if trustworthiness  explained  reductions  in  behavioral  intentions  after viewing neutral versus prejudiced LLMs. The model specified condition (LLM response) as the predictor of behavioral intentions, with trustworthiness entered as the mediator. Gender was used as a moderator for these pathways to understand gender differences in the proposed model. Trustworthiness predicted intentions, t (216) = 7.92, b = .55 , p &lt; .001, and a significant moderated-mediation effect emerged, b = GLYPH&lt;0&gt; .37, SE = .18, 95% CI [ GLYPH&lt;0&gt; .75, GLYPH&lt;0&gt; .03]. Women exposed to AI-generated prejudice reported lower trustworthiness which accounted for reduced intentions to use LLMs, b = GLYPH&lt;0&gt; .26, SE = .07, 95% CI [ GLYPH&lt;0&gt; .41, GLYPH&lt;0&gt; .13] (see Fig. 10). No effect  emerged  among men, b = .12, SE = .16,  95% CI [ GLYPH&lt;0&gt; .19,  .44], suggesting  trustworthiness  may  not  be  a  necessary  antecedent  for

Fig.  9. Trustworthiness  and  intentions  as  a  function  of  LLM  response  and gender (Experiment 3).

<!-- image -->

Note . Error bars represent standard error. *** p &lt; .001.

Z.W. Petzel and L. Sowerby

<!-- image -->

Direct effect,b =0.04,95%CI[-0.21,0.28]

Conditional indirect effect of trustworthiness,b =-0.26,95%CI[-0.41,-0.13]

Fig. 10. Trustworthiness as a mediator of the effects of AI-generated prejudice on intentions among women (Experiment 3). Note. *** p &lt; .001.

adoption of LLMs among advantaged groups. Analyses controlling for LLM experience, age, and race did not impact these effects, b =GLYPH&lt;0&gt; .22, SE = .12, 95% CI [ GLYPH&lt;0&gt; .46, GLYPH&lt;0&gt; .01].

## 4.2.5. Explicit biases - Experiment 3

The PROCESS macro for SPSS (model 3; Hayes, 2012) was used to examine the role of explicit gender biases in predicting trustworthiness and  intentions  after  viewing  neutral  versus  prejudiced  responses  between men and women. Stronger explicit gender bias predicted greater trustworthiness ( b = 1.34, p = .016) and intentions ( b = 2.38, p &lt; .001). However, the interaction among factors was not significant in predicting trustworthiness, F (1,213) = 1.73, p = .189, Δ R 2 = .007, 95% CI [ GLYPH&lt;0&gt; .29, 1.47].  Controlling  for  experience  with  LLMs,  age,  and  race  did  not impact the interaction among factors in predicting trustworthiness, all p s &gt; .340.

An interaction among factors emerged for intentions to use LLMs, F (1,213) = 9.35, p = .003, Δ R 2 = .039, 95% CI [.55, 2.56]. Among men shown neutral responses, strong explicit gender biases were associated with greater intentions to use LLMs, t (213) = 3.82, p &lt; .001, b = 1.16, 95% CI [.56, 1.75]. Yet, explicit gender biases were unrelated to intentions among men shown gender disparities in wages, p = .593 (see Fig. 11). Further, explicit gender bias was unrelated to behavioral intentions among women across both conditions, both p s &gt; .388. Despite experience with LLMs being associated with intentions, analyses controlling for experience did not impact the significance of this interaction, p = .036. Age and race were unrelated to intentions, both p s &gt; .879, and did not affect the interaction among factors.

Fig. 11. Intentions as a function of explicit biases and LLM response among men (Experiment 3). Note . *** p &lt; .001.

<!-- image -->

## 5. Discussion

While AI-generated biases (e.g., disparities in salary) elicited lower trustworthiness among Black Americans and women compared to when these  groups  viewed  unprejudiced  responses,  these  decreased  levels were still comparable to White Americans and men after viewing AIgenerated prejudice. In other words, exposure to AI-generated prejudice  may  not  reduce  trustworthiness  as  initially  hypothesized,  with stigmatized and non-stigmatized social groups exhibiting similar trustworthiness and behavioral intentions after viewing  prejudiced LLMs. Instead, viewing AI-generated responses which depict equity between social groups may increase trustworthiness of LLMs among stigmatized individuals.  This  effect  was  most  prominent  among Black  Americans (Experiment  1)  who  reported  higher  trustworthiness  and  behavioral intentions compared to White Americans, regardless of whether LLMs depicted prejudice. However, Black Americans significantly increased trustworthiness  and  intentions  when  LLMs  depicted  racial  parity  in salary.  Notably,  these  findings  do  not  fully  replicate  prior  research demonstrating how prejudiced algorithms diminish trustworthiness (e. g.,  Gupta  et  al.,  2022;  Wang  et  al.,  2021).  However,  consistent with empirical accounts for adoption of AI and other emerging technologies, higher trustworthiness explained greater intentions  to  use  LLMs (Choudhury &amp; Shamszare, 2023; Dawar et al., 2022; Kaur et al., 2022; Mostafa &amp; Kasamani, 2022; Wanner et al., 2022). Therefore, increased trustworthiness reported by Black Americans and women after exposure to  unprejudiced  LLMs  may  contribute  towards  higher  engagement among these stigmatized groups. Conversely, our results also suggest reduced trustworthiness may diminish future engagement with LLMs (i. e.,  such as White Americans high in implicit racial bias shown LLMs depicting racial parity in Experiment 1). Together, these findings provide  valuable  insight  into  mechanisms  underlying  the  adoption  of emerging technologies among stigmatized and non-stigmatized social groups, extending previous work (e.g., Shaouf &amp; Altaqqi, 2018; Venkatesh  et  al.,  2000).  Across  three  experiments,  our  results  provide converging evidence for trustworthiness as an antecedent for intentions to use LLMs. Despite theoretical implications, factors which may influence trustworthiness and intentions (i.e., race, gender, prejudiced attitudes), in addition to limitations, must be considered.

## 5.1. Race and gender differences

Black  Americans  reported  greater  trustworthiness  and  behavioral intentions  compared  to  White  Americans,  regardless  of  AI-generated response.  Despite  prejudiced  LLMs  reducing  trustworthiness  and  intentions among Black Americans, levels remained comparable to those reported by White Americans. These findings oppose evidence suggesting Black Americans typically report low trustworthiness, attributed to historical prejudices and direct experiences with discrimination (Smith, 2010; Stets &amp; Fares, 2019). For example, Black Americans report less trustworthiness of healthcare providers compared to White Americans (Armstrong, Ravenell, McMurphy, &amp; Putt, 2007; Halbert et al., 2006; Hausmann  et  al.,  2013).  However,  social  identity  theory  suggests trustworthiness  depends  on  the  identity  of  trustees  (Simpson  et  al., 2007; Tanis &amp; Postmes, 2005). For instance, Black Americans report lower  trustworthiness  of  those  with  dissimilar  racial  identities  (e.g., White physicians; Laurencin &amp; Murray, 2017; Shen et al., 2018). Of relevance to the current findings, LLMs lack discernible identities linked to expectations of prejudice, suggesting Black users may not anticipate bias when interacting with LLMs.

Thus, positive experiences with AI tools which have ambiguous social identities (e.g., ChatGPT) may promote trustworthiness compared to AI technologies which emulate distinct social identities associated with prejudice.  For  instance,  women  poorly  evaluate  virtual  AI  assistants with male voices, instead preferring female voices which match their social  identity  or  personality  (e.g.,  Nass &amp; Brave, 2005; Nass &amp; Lee, 2001). The poorer evaluation of male voices may stem from anticipation

Z.W. Petzel and L. Sowerby of  prejudice  due  to  the  salient  social  identity  of  the  virtual  assistant (Rawlinson, 2019). Similarly, AI-generated responses depicting historical prejudices may lead stigmatized users to assign social identities to LLMs associated  with  this  prejudice  (e.g.,  White  males),  transferring anticipated  prejudice  to  these  tools  and  reducing  confidence  in  the knowledge base used to train AI, violating ability assumptions outlined by Mayer et al. (1995) to reduce trustworthiness of LLMs. Yet, when LLMs do not confirm prejudices, Black Americans may enhance trustworthiness due to subverting expectations of bias. Conversely, White Americans  reported  similar  trustworthiness  regardless  of  prejudice, suggesting  Black  Americans  uniquely  reduce  trustworthiness  when LLMs reflect historical prejudices.

Women  reduced  their  trustworthiness  of  LLMs  after  viewing  AIgenerated  prejudice  (e.g.,  gender  disparities  in  salary).  Unlike  Black Americans shown racial disparities, women did not reduce intentions to use LLMs after viewing AI-generated prejudice. These contrary findings may be explained by evidence suggesting sexism may not be perceived as unfairly compared to racism (Parra et al., 2021; Woodzicka et al., 2015),  in  addition  to  women  being  less  likely  to  confront  prejudice (Swim et al., 2010; Swim &amp; Hyers, 1999). Notably, lack of confrontation is thought to undermine progress towards gender parity, with confrontation  of  sexism  improving  intergroup  relations  and  reducing  men ' s gender biases (Harris et al., 2015; Mallett &amp; Wagner, 2011). Thus, lack of  disengagement  from  LLMs  may  reflect  these  patterns,  suggesting while women reduce trustworthiness, they may continue to use LLMs due to the social costs associated with disengagement (Kaiser &amp; Miller, 2001).  However,  reductions  in  trustworthiness  reduced  women ' s  intentions to use LLMs, suggesting AI-generated prejudice may indirectly influence  behavioral  intentions  among  women.  Notably,  men  in Experiment  3  reported  greater  intentions  regardless  of  AI-generated prejudice. This finding may be due to the sample being younger ( M = 19.58) than those recruited from the general population in Experiment 2 ( M = 39.90). Consistent with this assumption, gender differences in use of technology are apparent in younger generations (i.e., Gen Z), such that men are more likely to accept emerging technologies compared to women, particularly those which collect personal data (Ho et al., 2022). Conversely, gender disparities in technology use are less apparent in older  generations  (e.g.,  baby  boomers;  Berkowsky,  Sharit, &amp; Czaja, 2017; Lian &amp; Yen, 2014).

## 5.2. The role of prejudiced attitudes

White Americans exhibiting implicit racial biases were more likely to reduce their  trustworthiness of  LLMs  when responses depicted racial parity. These findings reflect a larger body of literature suggesting how initiatives which attempt to address inequalities promote backlash and disengagement  among  advantaged  group  members  (i.e.,  White  men; Danbold &amp; Huo,  2017;  Dover  et  al.,  2020).  Thus,  individuals  with prejudices may reduce intentions to use LLMs when they do not confirm their biases. Yet, implicit gender biases did not predict trustworthiness, contrary to implicit racial bias. However, men ' s implicit prejudice towards women (e.g., Man-Career) did predict greater intentions to use LLMs, potentially reflecting endorsement of traditional gender stereotypes which promote men ' s interest in science and emerging technology (Master et al., 2021; Plante et al., 2019). Across Experiments 1 and 2, explicit (i.e., self-reported) bias did not predict trustworthiness or intentions. An explanation for these null effects may be social desirability, as  self-reporting  of  sensitive  topics  reduces  reliability  of  explicit compared to implicit measures (Greenwald et al., 2009). Supporting this assumption, while men ' s implicit bias predicted greater intentions to use LLMs, explicit bias was unrelated to intentions in Experiment 2, suggesting little overlap between these attitudes.

However, explicit gender biases were linked to greater trustworthiness and intentions in Experiment 3. Interestingly, men ' s explicit gender biases only predicted greater intentions to use LLMs when shown AIgenerated responses depicting gender parity. Conversely, when LLMs depicted gender inequality, men ' s explicit bias no longer predicted intentions. Notably, Experiment 3 assessed explicit biases after viewing AIgenerated  output.  Thus,  viewing  AI-generated  prejudice  may  have subsequently  increased  awareness  of  gender  inequities  prior  to  selfreporting  sexist  attitudes,  altering  men ' s  responses  and  diminishing predictive validity of explicit biases. Whether AI-generated prejudice similarly reduced the predictive validity of implicit biases is unclear due to their removal in Experiment 3. However, prior work suggests exposure to prejudice is not effective in reducing implicit gender biases, as these  attitudes  are  less  likely  to  be  affected  by  social  desirability (Greenwald et al., 2009; Ramos et al., 2016).

## 5.3. Theoretical implications

The present work informs the TAM and UTAUT, longstanding theories  which  outline  the  mechanisms  underlying  intentions  to  use emerging technologies. While these models are actively being applied to understand users ' intentions to use LLMs like ChatGPT (Lee et al., 2024; Menon &amp; Shilpa, 2023), recent evidence suggests these models may need to consider additional factors which may uniquely predict adoption of AI-assisted  tools.  As  mentioned  previously,  AI-generated  content  is produced with little transparency of underlying processes, perceived as a black box (Savage, 2022; Von Eschenbach, 2021). Importantly, prior work has established how AI ' s  lack of transparency diminishes trustworthiness,  subsequently  reducing  performance  expectancies  and  intentions  to  use  these  tools  (Wanner  et  al.,  2022).  We  support  this extension of TAM and UTAUT by demonstrating how reduced intentions to use LLMs are explained by lower trustworthiness. Thus, future applications of TAM and UTAUT should consider the role of trustworthiness in the adoption of AI technologies (e.g., LLMs).

Further, we suggest additional moderators should be considered by UTAUT which may influence behavioral intentions. Notably, previous applications of UTAUT have largely focused on gender differences in the acceptance of technology, with few studies examining racial differences. In Experiment 1, Black Americans reported greater trustworthiness and behavioral intentions to use LLMs compared to White Americans. While these  effects  were  diminished  after  Black  Americans  viewed  AIgenerated prejudice, findings provide evidence that race may be a useful moderator to include in future applications of UTAUT. For instance, racial disparities are observed in acceptance of technologies (Haynes et al., 2021; Hughes &amp; Granger, 2014; Miles et al., 2018; Sheon et al., 2017) and racial differences in general levels of trustworthiness are well established in the literature (Smith, 2010; Stets &amp; Fares, 2019). Therefore, consideration of race may provide a comprehensive understanding of  potential  obstacles  which  may  impede  the  adoption  of  emerging technologies.

## 5.4. Limitations

While sample sizes were determined using power analyses and effect sizes observed in prior research which used similar paradigms (Woodzicka et al., 2015, 2020), the recruited samples may have limited our ability to detect smaller effects in more complex designs which often require larger than estimated samples (e.g., mediation, hierarchical interactions; Giner-Sorolla et al., 2024). Further, our experimental designs may have limited generalizability, as studies only examined racial or gender  biases,  but  not  the  combination  of  these  stigmatized  social groups. For instance, ChatGPT has reportedly generated content suggesting Black women scientists are, ' not worth your time or attention, ' in addition to indicating Black men should be incarcerated based on racial identity (Perrigo, 2022). Thus, these experiments do not address intersectionality and dual stigmatized identities which are associated with more severe disparities (Chen &amp; Soldner, 2013; Towns, 2010). The present research also does not examine other stigmatized groups relevant  for  the  adoption  of  emerging  technologies,  such  as  older  populations who often experience prejudice and bias (e.g., baby boomers;

Z.W. Petzel and L. Sowerby

Peek  et  al.,  2014).  Notably,  baby  boomers  are  less  likely  to  accept emerging technologies compared to generation X and millennials (e.g., smart home devices; Canziani &amp; MacSween, 2021), in addition to age being  an  established  moderator  of  UTAUT  (Venkatesh  et  al.,  2003). While age predicted behavioral intentions in Experiment 1, this effect was not replicated in subsequent experiments. This may be a result of the population recruited for Experiment 1 ( M = 42.18) being older than those recruited for Experiment 2 ( M = 39.90) and Experiment 3 ( M = 19.58). Thus, younger populations used for subsequent experiments may have limited the predictive validity of age if fewer participants were from older generations (e.g., baby boomers). Thus, future work may consider  how  age-related  prejudices  may  reduce  trustworthiness  of LLMs.

In Experiments 1 and 2, implicit and explicit measures of trait prejudice were assessed prior to viewing AI-generated responses. The order of measures may have increased vigilance for potential prejudices when viewing interactions with LLMs, exaggerating the effects of our manipulation.  Consistent  with  this  assumption,  exposure  to  discrimination increases vigilance for future prejudice (Ramos et al., 2016). To determine  whether  order  of  measures  altered  our  findings,  Experiment  3 assessed trait prejudice after manipulations and dependent measures. Notably,  AI-generated  prejudice  reduced  trustworthiness,  but  not  intentions, among women as found in Experiment 2. Similarly, reduced trustworthiness accounted for lower intentions, replicating mechanistic accounts of trustworthiness found in Experiments 1 and 2. Together, results of Experiment 3 suggest measuring trait prejudice at the onset of initial experiments may not have promoted vigilance for the prejudices depicted in our manipulations.

The demographics of our samples may have also limited generalizability  of  findings.  For  instance,  undergraduate  psychology  students were recruited for Experiment 3, which are female-dominated programs (Marulanda &amp; Radtke, 2019). Due to the overrepresentation of women in this population, we oversampled to recruit enough male students to effectively test hypotheses. However, despite oversampling, only 17.6% of students identified as men which limits generalizability. Thus, our findings  regarding  men ' s  trustworthiness  and  intentions  to  use  LLMs reported  in  this  study  should  be  interpreted  with  caution.  Notwithstanding issues with recruitment of men, findings of Experiment 3 did replicate  the  mechanistic  account  of  trustworthiness  in  reducing  intentions  to  use  LLMs  among  women.  Further,  prior  experience  with LLMs differed between conditions in Experiment 3 and was related to trustworthiness and intentions. Yet, findings were unaffected after covarying for prior experience, suggesting overrepresentation of experienced users in the control condition did not affect results.

The context of the manipulation may also limit generalizability of findings.  In  the  current  experiments,  participants  only  viewed  a screenshot of an apparent interaction with ChatGPT which was manipulated to depict prejudice. Thus, whether manipulations capture direct experiences of using LLMs is unclear. Further, we did not assess whether participants believed the AI-generated responses as genuine, which may have undermined the validity of our manipulations. Further, the present study examined AI-generated prejudice within text-based interactions. Given proliferation of AI tools, such as image generation (e.g., DALL-E, Midjourney), future work should examine how prejudice depicted in these tools may reduce trustworthiness and intentions. The current work also  does  not  consider  how  AI  which favors  stigmatized  groups may impact  trustworthiness  and  intentions.  For  instance,  in  attempts  to minimize prejudice appearing in AI-generated content, Google ' s LLM (i. e., Gemini) was instructed to, ' diversify depictions of ALL images with people to include DESCENT and GENDER …' (Milmo &amp; Hern, 2024). However,  this  attempt  to  reduce  prejudice  incidentally  produced anachronistic images of German soldiers from World War 2 as racial minorities, sparking outrage among users (Milmo &amp; Hern, 2024). Yet, increasing  the  diversity  of  AI-generated  content  may  have  beneficial effects which help to disconfirm negative stereotypes and prejudices. For instance, search algorithms manipulated to depict gender parity of an occupation reduces prejudices against women in subsequent hiring decisions (Vlasceanu &amp; Amodio, 2022). Thus, future work may consider how tuning algorithms to overrepresent stigmatized groups can increase trustworthiness and intentions to use AI tools in the future.

Lastly, the manipulation used to prime participants of AI-generated racial and gender disparities may limit generalizability of findings. For example,  Experiment  1  used  stereotypic  White  and  Black  names  to distinguish race of employees. Yet, we did not assess effectiveness of this approach by including a manipulation check which asked the alleged race of John and Jerome depicted in the AI-generated response. However, the use of stereotypic Black names (i.e., Jerome) has reliably been used to prime Black racial identities in previous tasks, which is why it was selected for the present work (Chang &amp; Mitchell, 2011; Greenwald et al., 1998; Ottaway et al., 2001; Wichman, 2012). To address these issues, future work may utilize less ambiguous manipulations, such as providing  images  of  the  individuals  described  in  prompts  to  clearly highlight the race of individuals. Further, in accordance with TAM and UTAUT, we only assessed intentions to use LLMs, and did not longitudinally measure engagement with these tools. Evidence suggests the link between  intentions  and  behavior  may  be  weaker  than  previously thought  (Sniehotta  et  al.,  2014).  Thus,  the  long-term  impact  of AI-generated biases on LLM engagement remains unclear and should be a focus of future work. Other individual differences not assessed in the current experiments should also be considered. For instance, while we assessed  explicit  and  implicit  biases,  these  measures  do  not  capture awareness of discrimination and social inequalities which are separate and unique constructs which likely influence the effectiveness of our manipulations (Pinel, 1999; Wang et al., 2012).

## 5.5. Practical implications

Despite  results  suggesting  how  AI-generated  prejudices  impact trustworthiness  and  intentions  to  use  LLMs,  interventions  can  be implemented to circumvent these consequences. As mentioned prior, algorithms which are trained on biased datasets can be tuned to avoid generating content which violate users ' expectations (e.g., ensuring diversity  of  racial  and  ethnic  groups  when  asked  to  generate  images depicting  people).  While  this  manipulation  of  algorithms  may  elicit backlash if not thoroughly tested (e.g., generating anachronistic images; Milmo &amp; Hern, 2024), increasing diversity of AI-generated content has been shown to reduce prejudices and use of stereotypes (Vlasceanu &amp; Amodio, 2022). Yet, the extent this increased diversification of LLMs may increase trustworthiness, particularly among stigmatized users, has not been examined and should be a focus for future work. Conversely, as demonstrated in the present research, advantaged social groups may reduce trustworthiness of LLMs if AI-generated content violates their implicit  biases.  Therefore,  algorithms  tuned  to  overrepresent  stigmatized groups may lead White, male users to reduce their engagement with  LLMs  due  to  violating  expectations  informed  by  their  implicit prejudices.

To account for potential backlash in cases where AI-generated responses  may  not  meet  expectations  of  users,  LLMs  have  previously included statements highlighting their limitations (e.g., ' ChatGPT may produce inaccurate information about people, places, or facts. ' Metzger et al., 2024). Alternatively, LLMs may consider including disclaimers on whether users can provide corrections to incorrect or harmful responses, reassuring the user that LLMs are capable of learning new information to limit inappropriate responses in future interactions. However, evidence suggests  disclaimers  of  limitations  and  capabilities  may  not  increase trustworthiness  or  persuasiveness  of  LLMs  (Metzger  et  al.,  2024). Instead, style of communication may increase perceived trustworthiness and persuasiveness of AI-generated content. The elaboration likelihood model outlines peripheral routes of persuasion, suggesting subtle cues may be useful in promoting positivity towards LLMs (Petty &amp; Bri ˜ nol, 2012). For instance, expert or authoritative sources are more effective in promoting agreement with new information (DeBono &amp; Harnish, 1988).

Z.W. Petzel and L. Sowerby

Similarly, LLMs which use authoritative styles of communication are viewed as more trustworthy and persuasive compared to when using powerless language (e.g., hesitance, use of disclaimers; Metzger et al., 2024).  Therefore,  authoritative  communication  might  increase  trustworthiness even when LLMs violate user expectations, such as among White Americans high in implicit racial bias who viewed AI-generated responses depicting racial parity in Experiment 1.

LLMs which do not meet user expectations may also benefit from providing  empirical  evidence  and  additional  context  to  support  AIgenerated responses, increasing trustworthiness via strengthening perceived ability of LLMs suggested by Mayer et al. (1995). For example, members of non-stigmatized groups (e.g., White males) are often unaware of social inequites and the consequences experienced by stigmatized  groups  (e.g.,  Black  Americans,  women;  Kraus  et  al.,  2017;  van Zomeren et al., 2008). Therefore, providing additional information detailing  these  social  disparities  when  generating  responses  linked  to stigmatized  groups  may  alleviate  discrepancies  between  prejudiced users ' expectations and AI-generated content as evidenced by previous work (Callaghan et al., 2021; De Souza &amp; Schmader, 2022). Further, providing this evidence may enhance trustworthiness of LLMs among stigmatized groups, as observing confrontation of prejudice can increase warmth  towards  allies who  challenge social inequities (Chu &amp; Ashburn-Nardo, 2022; Hildebrand et al., 2020). Reactions to AI-generated prejudice can also be improved by allowing stigmatized users  to  provide  feedback  to  LLMs.  For  instance,  stigmatized  group members who confront prejudice experience several positive outcomes, such  as  higher  competence,  self-esteem,  and  empowerment  (Becker et al., 2014; Gervais et al., 2010). Thus, allowing users to challenge LLMs which  violate  expectations  of  benevolence  or  integrity  outlined  by Mayer et al. (1995) may improve trustworthiness. Yet, Metzger et al. (2024) suggests allowing user feedback may not increase trustworthiness or persuasiveness of LLMs. However, this research did not examine the  impact  of  this  feature  between  social  groups.  Therefore,  future research may benefit from determining how user feedback may promote trustworthiness among stigmatized groups.

Lastly, interventions may also educate users on the impacts of AIgenerated  prejudice  on  trustworthiness  and  intentions,  particularly among stigmatized groups. Thus, educators or organizations may provide  training  which  highlights  the  potential  for  LLMs  to  incidentally generate content which depicts prejudice and stereotypes. Prior work has shown education on the prevalence and use of stereotypes mitigate the  negative  effects  they  have  on  stigmatized  groups.  For  instance, reminding women of gender stereotypes can lower their math ability, but educating them about these effects can reduce their negative impact (Johns et al., 2005). Similarly, educating users that LLMs may replicate these  negative  stereotypes  could  reduce  the  impact  of  AI-generated prejudice  on  trustworthiness  and  intentions  to  use  these  tools.  Yet, raising awareness of AI-generated prejudices may not uniquely improve user experiences, as replications of these effects are mixed (Hermann &amp; Vollmeyer,  2021;  Tomasetto &amp; Appoloni,  2013).  Further,  increasing awareness  of  AI-generated  prejudices  may  promote  anxiety  when interacting with LLMs, as stigmatized users may anticipate discrimination  from  these  tools.  Thus,  work  is  needed  to  understand  how increasing  transparency  of  prejudices  produced  by  LLMs  may  affect users ' trustworthiness and intentions to use AI tools before implemented as a potential intervention to increase engagement.

## 5.6. Conclusions

Results  provide  evidence  for  the  consequences  of  AI-generated prejudice  on  levels  of  trustworthiness  and  intentions  to  use  LLMs, providing an empirical foundation to understand the adoption of technology among stigmatized groups. Consistent with an emerging body of work (e.g., Choudhury &amp; Shamszare, 2023; Dawar et al., 2022; Kaur et al., 2022; Mostafa &amp; Kasamani, 2022), lower trustworthiness of LLMs subsequently  reduces  intentions  to  use  AI  tools  in  the  future  among

Black Americans and women. These findings may provide insight into previous disparities in the adoption of other technologies, such as early iterations of voice recognition preferring male voices preceding men ' s dominance in use of smart home devices (Bajorek, 2019; Canziani &amp; MacSween, 2021; Chen et al., 2022). Thus, prejudice generated by early iterations of LLMs may promote social inequities in the adoption of AI technologies. Together, findings suggest addressing AI-generated prejudice may not only be a technological challenge, but also a societal challenge as early exposure to prejudiced LLMs may reduce trustworthiness of AI tools. Thus, in addition to developers improving inclusivity of  LLMs,  development  of  interventions  may  be  considered  which improve trustworthiness of AI, particularly among members of stigmatized groups. Future research should continue to examine how trustworthiness develops among different social groups to effectively guide efforts to embed AI technologies within our professional development and education.

## CRediT authorship contribution statement

Zachary W. Petzel: Writing -review &amp; editing, Writing -original draft, Supervision, Project administration, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Leanne Sowerby: Writing -original draft, Methodology, Conceptualization.

## Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

## Data availability

Data and materials used in the paper are freely available via OSF, with links provided in the text.

## References

- Abbad, M. M. (2021). Using the UTAUT model to understand students ' usage of elearning systems in developing countries. Education and Information Technologies, 26 (6), 7205 -7224. https://doi.org/10.1007/s10639-021-10573-5
- Abid, A., Farooqi, M., &amp; Zou, J. (2021). Persistent anti-muslim bias in large language models. In Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society (pp. 298 -306). https://doi.org/10.1145/3461702.3462624
- Abril, d. (2023). AI isn ' t yet going to take your job -but you may have to work with it. Washington Post . https://www.washingtonpost.com/technology/interactive/2023/a i-jobs-workplace/.
- Ajzen, I. (1991). The theory of planned behavior. Organizational Behavior and Human Decision Processes, 50 (2), 179 -211. https://doi.org/10.1016/0749-5978(91)90020-T Ajzen, I., &amp; Fishbein, M. (1980). Understanding attitudes and predicting social behavior . Englewood Cliffs, NJ: Prentice-Hall.
- Angwin, J., Larson, J., Mattu, S., &amp; Kirchner, L. (2016). Machine bias. ProPublica . https ://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentenc
- ing.
- Armstrong, K., Ravenell, K. L., McMurphy, S., &amp; Putt, M. (2007). Racial/ethnic differences in physician distrust in the United States. American Journal of Public Health, 97 (7), 1283 -1289. https://doi.org/10.2105/AJPH.2005.080762
- Bajorek, J. P. (2019). Voice recognition still has significant race and gender biases. Harvard Business Review, 10 , 1 -4.
- BBC. (2015). Google apologises for Photos app ' s racist blunder. https://www.bbc.co.uk/ news/technology-33347866.
- Becker, J. C., Zawadzki, M. J., &amp; Shields, S. A. (2014). Confronting and reducing sexism: A call for research on intervention. Journal of Social Issues, 70 (4), 603 -614. https:// doi.org/10.1111/josi.12081
- Berkowsky, R. W., Sharit, J., &amp; Czaja, S. J. (2017). Factors predicting decisions about technology adoption among older adults. Innovation in Aging, 1 (3), Article igy002. https://doi.org/10.1093/geroni/igy002
- Bidmon, S., &amp; Terlutter, R. (2015). Gender differences in searching for health information on the internet and the virtual patient-physician relationship in Germany: Exploratory results on how men and women differ and why. Journal of Medical Internet Research, 17 (6), e156. https://doi.org/10.2196/jmir.4127
- Caetano, R., Vaeth, P. A., Chartier, K. G., &amp; Mills, B. A. (2014). Epidemiology of drinking, alcohol use disorders, and related problems in US ethnic minority groups. Handbook of Clinical Neurology, 125 , 629 -648. https://doi.org/10.1016/B978-0-444-626196.00037-9

Z.W. Petzel and L. Sowerby

- Callaghan, B., Harouni, L., Dupree, C. H., Kraus, M. W., &amp; Richeson, J. A. (2021). Testing the efficacy of three informational interventions for reducing misperceptions of the Black-White wealth gap. Proceedings of the National Academy of Sciences, 118 (38), Article e2108875118. https://doi.org/10.1073/pnas.2108875118
- Canziani, B., &amp; MacSween, S. (2021). Consumer acceptance of voice-activated smart home devices for product information seeking and online ordering. Computers in Human Behavior, 119 , Article 106714. https://doi.org/10.1016/j.chb.2021.106714
- Cardona, M. A., Rodriguez, R. J., &amp; Ishmael, K. (2023). Artificial intelligence and the future of teaching and learning . The Office of Educational Technology, Department of Education. https://www2.ed.gov/documents/ai-report/ai-report.pdf.
- Chang, B. P. I., &amp; Mitchell, C. (2011). Discriminating between the effects of valence and salience in the implicit association test. The Quarterly Journal of Experimental Psychology, 64 (11), 2251 -2275. https://doi.org/10.1080/17470218.2011.586782
- Chen, X., Li, Z., Setlur, S., &amp; Xu, W. (2022). Exploring racial and gender disparities in voice biometrics. Scientific Reports, 12 (1), 3723. https://doi.org/10.1038/s41598022-06673-y
- Chen, X., &amp; Soldner, M. (2013). STEM attrition: College students ' paths into and out of STEM fields. Statistical analysis report. NCES 2014-001 . National Center for Education Statistics. https://files.eric.ed.gov/fulltext/ED544470.pdf.
- Chiemeke, S. C., &amp; Evwiekpaefe, A. E. (2011). A conceptual framework of a modified unified theory of acceptance and use of technology (UTAUT) Model with Nigerian factors in E-commerce adoption. Educational Research, 2 (12), 1719 -1726.
- Choudhury, A., &amp; Shamszare, H. (2023). Investigating the impact of user trust on the adoption and use of ChatGPT: Survey analysis. Journal of Medical Internet Research, 25 , Article e47184. https://doi.org/10.2196/47184
- Chu, C., &amp; Ashburn-Nardo, L. (2022). Black Americans ' perspectives on ally confrontations of racial prejudice. Journal of Experimental Social Psychology, 101 , 1 -9. https://doi.org/10.1016/j.jesp.2022.104337
- Criado-Perez, C. (2019). Invisible women: Data bias in a world designed for men . London, UK: Abrams.
- Daka, G. C., &amp; Phiri, J. (2019). Factors driving the adoption of E-banking services based on the UTAUT model. International Journal of Business and Management, 14 (6), 1 -43.
- Danbold, F., &amp; Huo, Y. J. (2017). Men ' s defense of their prototypicality undermines the success of women in STEM initiatives. Journal of Experimental Social Psychology, 72 , 57 -66. https://doi.org/10.1016/j.jesp.2016.12.014
- Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS Quarterly, 13 (3), 319 -340. https://doi.org/10.2307/ 249008
- Dawar, S., Panwar, S., Dhaka, S., &amp; Kudal, P. (2022). Antecedents and role of trust in chatbot use intentions: An Indian perspective. Marketing and Management of Innovations, 13 (4), 198 -206. https://doi.org/10.21272/mmi.2022.4-18
- De Souza, L., &amp; Schmader, T. (2022). The misjudgment of men: Does pluralistic ignorance inhibit allyship? Journal of Personality and Social Psychology, 122 (2), 265 -285. https://doi.org/10.1037/pspi0000362
- DeBono, K. G., &amp; Harnish, R. J. (1988). Source expertise, source attractiveness, and the processing of persuasive information: A functional approach. Journal of Personality and Social Psychology, 55 (4), 541 -546. https://doi.org/10.1037/00223514.55.4.541
- Dover, T. L., Kaiser, C. R., &amp; Major, B. (2020). Mixed signals: The unintended effects of diversity initiatives. Social Issues and Policy Review, 14 (1), 152 -181. https://doi.org/ 10.1111/sipr.12059
- European Commision. (2024). AI act. https://digital-strategy.ec.europa.eu/en/pol icies/regulatory-framework-ai.
- Gadiraju, V., Kane, S., Dev, S., Taylor, A., Wang, D., Denton, E., &amp; Brewer, R. (2023). I wouldn ' t say offensive but. In Disability-centered perspectives on Large Language Models. Proceedings of the 2023 ACM conference on fairness, accountability, and transparency (pp. 205 -216). https://doi.org/10.1145/3593013.3593989
- Gefen, D. (2004). What makes an ERP implementation relationship worthwhile: Linking trust mechanisms and ERP usefulness. Journal of Management Information Systems, 21 (1), 263 -288. https://doi.org/10.1080/07421222.2004.11045792
- Gefen, D., Karahanna, E., &amp; Straub, D. W. (2003). Trust and TAM in online shopping: An integrated model. MIS Quarterly , 51 -90. https://www.jstor.org/stable/30036519.
- Gervais, S. J., Hillard, A. L., &amp; Vescio, T. K. (2010). Confronting sexism: The role of relationship orientation and gender. Sex Roles: Journal of Research, 63 (7 -8), 463 -474. https://doi.org/10.1007/s11199-010-9838-7
- Ghazizadeh, M., Lee, J. D., &amp; Boyle, L. N. (2012). Extending the technology acceptance model to assess automation. Cognition, Technology &amp; Work, 14 , 39 -49. https://doi. org/10.1007/s10111-011-0194-3
- Ghosh, S., &amp; Caliskan, A. (2023). ChatGPT perpetuates gender bias in machine translation and ignores non-gendered pronouns: Findings across Bengali and five other low-resource languages . https://doi.org/10.48550/arXiv.2305.10510. arXiv preprint:2305.10510 .
- Giner-Sorolla, R., Montoya, A. K., Reifman, A., Carpenter, T., Lewis, N. A., Aberson, C. L., … Soderberg, C. (2024). Power to detect what? Considerations for planning and evaluating sample size. Personality and Social Psychology Review, 28 (3), 276 -301. https://doi.org/10.1177/10888683241228328
- Goswami, A., &amp; Dutta, S. (2015). Gender differences in technology usage -a literature review. Open Journal of Business and Management, 4 (1), 51 -59. https://doi.org/ 10.4236/ojbm.2016.41006
- Graham, G. (2015). Disparities in cardiovascular disease risk in the United States. Current Cardiology Reviews, 11 (3), 238 -245. https://doi.org/10.2174/ 1573403X11666141122220003
- Grani ´ c, A., &amp; Maranguni ´ c, N. (2019). Technology acceptance model in educational context: A systematic literature review. British Journal of Educational Technology, 50 (5), 2572 -2593. https://doi.org/10.1111/bjet.12864
- Greenwald, A. G., Dasgupta, N., Dovidio, J. F., Kang, J., Moss-Racusin, C. A., &amp; Teachman, B. A. (2022). Implicit-bias remedies: Treating discriminatory bias as a
- public-health problem. Psychological Science in the Public Interest, 23 (1), 7 -40. https://doi.org/10.1177/15291006211070781
- Greenwald, A. G., McGhee, D. E., &amp; Schwartz, J. L. K. (1998). Measuring individual differences in implicit cognition: The implicit association test. Journal of Personality and Social Psychology, 74 (6), 1464 -1480. https://doi.org/10.1037/00223514.74.6.1464
- Greenwald, A. G., Nosek, B. A., &amp; Banaji, M. R. (2003). Understanding and using the implicit association test: I. An improved scoring algorithm. Journal of Personality and Social Psychology, 85 (2), 197 -216. https://doi.org/10.1037/0022-3514.85.2.197
- Greenwald, A. G., Poehlman, T. A., Uhlmann, E. L., &amp; Banaji, M. R. (2009). Understanding and using the implicit association test: III. Meta-Analysis of predictive validity. Journal of Personality and Social Psychology, 97 (1), 17 -41. https://doi.org/ 10.1037/a0015575
- Gupta, A., Basu, D., Ghantasala, R., Qiu, S., &amp; Gadiraju, U. (2022). To trust or not to trust: How a conversational interface affects trust in a decision support system. In Proceedings of the ACM web conference 2022 (pp. 3531 -3540). https://doi.org/ 10.1145/3485447.3512248
- Halbert, C. H., Armstrong, K., Gandy, O. H., &amp; Shaker, L. (2006). Racial differences in trust in health care providers. Archives of Internal Medicine, 166 (8), 896 -901. https:// doi.org/10.1001/archinte.166.8.896
- Harris, K. F. H. F., Grappendorf, H., Aicher, T., &amp; Veraldo, C. (2015). Discrimination? Low pay? Long hours? I am still excited:" female sport management students ' perceptions of barriers toward a future career in sport. Advancing Women in Leadership Journal, 35 , 12 -21. https://doi.org/10.21423/awlj-v35.a128
- Haryanti, T., &amp; Subriadi, A. P. (2020). Factors and theories for E-commerce adoption: A literature review. International Journal of Electronic Commerce Studies, 11 (2), 87 -106. https://doi.org/10.7903/ijecs.1910
- Hausmann, L. R. M., Kwoh, C. K., Hannon, M. J., &amp; Ibrahim, S. A. (2013). Perceived racial discrimination in health care and race differences in physician trust. Race and Social Problems, 5 (2), 113 -120. https://doi.org/10.1007/s12552-013-9092-z
- Hayes, A. F. (2012). Process: A versatile computational tool for observed variable mediation, moderation, and conditional process modeling [white paper] . Retrieved from http:// www.afhayes.com/public/process2012.pdf.
- Haynes, N., Ezekwesili, A., Nunes, K., Gumbs, E., Haynes, M., &amp; Swain, J. (2021). ' Can you see my screen? ' Addressing racial and ethnic disparities in telehealth. Current Cardiovascular Risk Reports, 15 , 1 -9. https://doi.org/10.1007/s12170-021-00685-5
- Hermann, J. M., &amp; Vollmeyer, R. (2021). Is knowing really half the battle? Exploring effects of a teaching intervention about stereotype threat on gender differences in mathematics. Unterrichtswissenschaft, 49 , 547 -565. https://doi.org/10.1007/ s42010-021-00124-9
- Hildebrand, L. K., Jusuf, C. C., &amp; Monteith, M. J. (2020). Ally confrontations as identitysafety cues for marginalized individuals. European Journal of Social Psychology, 50 (6), 1318 -1333. https://doi.org/10.1002/ejsp.2692
- Ho, M. T., Mantello, P., Ghotbi, N., Nguyen, M. H., Nguyen, H. K. T., &amp; Vuong, Q. H. (2022). Rethinking technological acceptance in the age of emotional AI: Surveying gen Z (zoomer) attitudes toward non-conscious data collection. Technology in Society, 70 , Article 102011. https://doi.org/10.1016/j.techsoc.2022.102011
- Hohenberger, C., Sp ¨ orrle, M., &amp; Welpe, I. M. (2016). How and why do men and women differ in their willingness to use automated cars? The influence of emotions across different age groups. Transportation Research Part A: Policy and Practice, 94 , 374 -385. https://doi.org/10.1016/j.tra.2016.09.022
- Holden, R. J., &amp; Karsh, B. T. (2010). The technology acceptance model: Its past and its future in health care. Journal of Biomedical Informatics, 43 (1), 159 -172. https://doi. org/10.1016/j.jbi.2009.07.002
- Huffman, A. H., Whetten, J., &amp; Huffman, W. H. (2013). Using technology in higher education: The influence of gender roles on technology self-efficacy. Computers in Human Behavior, 29 (4), 1779 -1786. https://doi.org/10.1016/j.chb.2013.02.012
- Hughes, H. A., &amp; Granger, B. B. (2014). Racial disparities and the use of technology for self-management in blacks with heart failure: A literature review. Current Heart Failure Reports, 11 , 281 -289. https://doi.org/10.1007/s11897-014-0213-9
- IBM. (2024). Data suggests growth in enterprise adoption of AI is due to widespread deployment by early adopters, but barriers keep 40% in the exploration and experimentation phases. https://newsroom.ibm.com/2024-01-10-Data-SuggestsGrowth-in-Enterprise-Adoption-of-AI-is-Due-to-Widespread-Deployment-by-EarlyAdopters.
- Johns, M., Schmader, T., &amp; Martens, A. (2005). Knowing is half the battle: Teaching stereotype threat as a means of improving women ' s math performance. Psychological Science, 16 (3), 175 -179. https://doi.org/10.1111/j.0956-7976.2005.00799.x
- Kaiser, C. R., &amp; Miller, C. T. (2001). Stop complaining! The social costs of making attributions to discrimination. Personality and Social Psychology Bulletin, 27 (2), 254 -263. https://doi.org/10.1177/0146167201272010
- Kamal, S. A., Shafiq, M., &amp; Kakria, P. (2020). Investigating acceptance of telemedicine services through an extended technology acceptance model (TAM). Technology in Society, 60 , Article 101212. https://doi.org/10.1016/j.techsoc.2019.101212
- Kaur, D., Uslu, S., Rittichier, K. J., &amp; Durresi, A. (2022). Trustworthy artificial intelligence: A review. ACM Computing Surveys, 55 (2), 1 -38. https://doi.org/ 10.1145/3491209
- Kohnke, A., Cole, M. L., &amp; Bush, R. (2014). Incorporating UTAUT predictors for understanding home care patients ' and clinician ' s acceptance of healthcare telemedicine equipment. Journal of Technology Management and Innovation, 9 (2), 29 -41. https://doi.org/10.4067/S0718-27242014000200003
- Kraus, J., Miller, L., Klumpp, M., Babel, F., Scholz, D., Merger, J., &amp; Baumann, M. (2023). On the role of beliefs and trust for the intention to use service robots: An integrated trustworthiness beliefs model for robot acceptance. International Journal of Social Robotics. Advance online publication . https://doi.org/10.1007/s12369-022-00952-4

Z.W. Petzel and L. Sowerby

- Kraus, M. W., Rucker, J. M., &amp; Richeson, J. A. (2017). Americans misperceive racial economic equality. Proceedings of the National Academy of Sciences, 114 (39), 10324 -10331. https://doi.org/10.1073/pnas.1707719114
- Latu, I. M., Mast, M. S., &amp; Stewart, T. L. (2015). Gender biases in (inter) action: The role of interviewers ' and applicants ' implicit and explicit stereotypes in predicting women ' s job interview outcomes. Psychology of Women Quarterly, 39 (4), 539 -552. https://doi.org/10.1177/0361684315577383
- Laurencin, C. T., &amp; Murray, M. (2017). An American crisis: The lack of black men in medicine. Journal of Racial and Ethnic Health Disparities, 4 , 317 -321. https://doi.org/ 10.1007/s40615-017-0380-y
- Lee, S., Jones-Jang, S. M., Chung, M., Kim, N., &amp; Choi, J. (2024). Who is using ChatGPT and why?: Extending the unified theory of acceptance and use of technology (UTAUT) model. Information Research: An International Electronic Journal, 29 (1), 54 -72. https://doi.org/10.47989/ir291647
- Lee, J. D., &amp; See, K. A. (2004). Trust in automation: Designing for appropriate reliance. Human Factors, 46 (1), 50 -80. https://doi.org/10.1518/hfes.46.1.50.30392
- Legris, P., Ingham, J., &amp; Collerette, P. (2003). Why do people use information technology? A critical review of the technology acceptance model. Information &amp; management, 40 (3), 191 -204. https://doi.org/10.1016/S0378-7206(01)00143-4
- Lian, J. W., &amp; Yen, D. C. (2014). Online shopping drivers and barriers for older adults: Age and gender differences. Computers in Human Behavior, 37 , 133 -143. https://doi. org/10.1016/j.chb.2014.04.028
- Ling Keong, M., Ramayah, T., Kurnia, S., &amp; May Chiun, L. (2012). Explaining intention to use an enterprise resource planning (ERP) system: An extension of the UTAUT model. Business Strategy Series, 13 (4), 173 -180. https://doi.org/10.1108/ 17515631211246249
- Lytton, C. (2024). AI hiring tools may be filtering out the best job applicants. BBC Worklife . https://www.bbc.com/worklife/article/20240214-ai-recruiting-hiring-soft ware-bias-discrimination.
- Maina, I. W., Belton, T. D., Ginzberg, S., Singh, A., &amp; Johnson, T. J. (2018). A decade of studying implicit racial/ethnic bias in healthcare providers using the implicit association test. Social Science &amp; Medicine, 199 , 219 -229. https://doi.org/10.1016/j. socscimed.2017.05.009
- Mallett, R. K., &amp; Wagner, D. E. (2011). The unexpectedly positive consequences of confronting sexism. Journal of Experimental Social Psychology, 47 (1), 215 -220. https://doi.org/10.1016/j.jesp.2010.10.001
- Manierre, M. J. (2015). Gaps in knowledge: Tracking and explaining gender differences in health information seeking. Social Science &amp; Medicine, 128 , 151 -158. https://doi. org/10.1016/j.socscimed.2015.01.028
- Marulanda, D., &amp; Radtke, H. L. (2019). Men pursuing an undergraduate psychology degree: What ' s masculinity got to do with it? Sex Roles: Journal of Research, 81 (5 -6), 338 -354. https://doi.org/10.1007/s11199-018-0995-4
- Master, A., Meltzoff, A. N., &amp; Cheryan, S. (2021). Gender stereotypes about interests start early and cause gender disparities in computer science and engineering. Proceedings of the National Academy of Sciences, 118 (48), Article e2100030118. https://doi.org/ 10.1073/pnas.2100030118
- Mayer, R. C., Davis, J. H., &amp; Schoorman, F. D. (1995). An integrative model of organizational trust. Academy of Management Review, 20 (3), 709 -734. https://doi. org/10.2307/258792
- Menon, D., &amp; Shilpa, K. (2023). ' Chatting with ChatGPT ' : Analyzing the factors influencing users ' intention to Use the Open AI ' s ChatGPT using the UTAUT model. Heliyon, 9 (11), Article e20962. https://doi.org/10.1016/j.heliyon.2023.e20962
- Metzger, L., Miller, L., Baumann, M., &amp; Kraus, J. (2024). Empowering calibrated (Dis-) Trust in conversational agents: A user study on the persuasive power of limitation disclaimers vs. Authoritative style. Proceedings of the CHI Conference on Human Factors in Computing Systems , 1 -19. https://doi.org/10.1145/3613904.3642122
- Miles, R. C., Onega, T., &amp; Lee, C. I. (2018). Addressing potential health disparities in the adoption of advanced breast imaging technologies. Academic Radiology, 25 (5), 547 -551. https://doi.org/10.1016/j.acra.2017.05.021
- Milmo, D., &amp; Hern, A. (2024). Google chief admits 'biased ' AI tool ' s photo diversity offended users. The Guardian . https://www.theguardian.com/technology/2024/feb/ 28/google-chief-ai-tools-photo-diversity-offended-users.
- Mitchell, U. A., Chebli, P. G., Ruggiero, L., &amp; Muramatsu, N. (2019). The digital divide in health-related technology use: the significance of race/ethnicity. The Gerontologist, 59 (1), 6 -14. https://doi.org/10.1093/geront/gny138
- Momani, A. M. (2020). The unified theory of acceptance and use of technology: A new approach in technology acceptance. International Journal of Sociotechnology and Knowledge Development, 12 (3), 79 -98. https://doi.org/10.4018/IJSKD.2020070105
- Mostafa, R. B., &amp; Kasamani, T. (2022). Antecedents and consequences of chatbot initial trust. European Journal of Marketing, 56 (6), 1748 -1771. https://doi.org/10.1108/ EJM-02-2020-0084
- Nass, C., &amp; Brave, S. (2005). Wired for speech: How voice activates and advances the humancomputer relationship . Cambridge, MA: Boston Review.
- Nass, C., &amp; Lee, K. M. (2001). Does computer-synthesized speech manifest personality? Experimental tests of recognition, similarity-attraction, and consistency-attraction. Journal of Experimental Psychology: Applied, 7 (3), 171 -181. https://doi.org/10.1037/ 1076-898X.7.3.171
- Office for National Statistics. (2022). Gender pay gap in the UK: 2022 . Office for National Statistics. https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/ea rningsandworkinghours/bulletins/genderpaygapintheuk/2022.
- O ' Keefe, E. B., Meltzer, J. P., &amp; Bethea, T. N. (2015). Health disparities and cancer: Racial disparities in cancer mortality in the United States, 2000 -2010. Frontiers in Public Health, 3 , 51. https://doi.org/10.3389/fpubh.2015.00051
- Ottaway, S. A., Hayden, D. C., &amp; Oakes, M. A. (2001). Implicit attitudes and racism: Effects of word familiarity and frequency on the implicit association test. Social Cognition, 19 (2), 97 -144. https://doi.org/10.1521/soco.19.2.97.20706
- Parra, C. M., Gupta, M., &amp; Dennehy, D. (2021). Likelihood of questioning AI-based recommendations due to perceived racial/gender bias. IEEE Transactions on Technology and Society, 3 (1), 41 -45. https://doi.org/10.1109/TTS.2021.3120303
- Peek, S. T., Wouters, E. J., Van Hoof, J., Luijkx, K. G., Boeije, H. R., &amp; Vrijhoef, H. J. (2014). Factors influencing acceptance of technology for aging in place: A systematic review. International Journal of Medical Informatics, 83 (4), 235 -248. https://doi.org/ 10.1016/j.ijmedinf.2014.01.004
- Perrigo, B. (2022). AI chatbots are getting better. But an interview with ChatGPT reveals their limits. About Time Magazine . https://time.com/6238781/chatbot-chatgpt-ai -interview/.
- Petty, R. E., &amp; Bri ˜ nol, P. (2012). The elaboration likelihood model. In P. A. M. Van Lange, A. W. Kruglanski, &amp; E. T. Higgins (Eds.), Handbook of theories of social psychology (pp. 224 -245). Sage Publications Ltd. https://doi.org/10.4135/9781446249215.n12.
- Pinel, E. C. (1999). Stigma consciousness: The psychological legacy of social stereotypes. Journal of Personality and Social Psychology, 76 (1), 114 -128. https://doi.org/ 10.1037/0022-3514.76.1.114
- Plante, I., O ' Keefe, P. A., Aronson, J., Fr ´ echette-Simard, C., &amp; Goulet, M. (2019). The interest gap: How gender stereotype endorsement about abilities predicts differences in academic interests. Social Psychology of Education: International Journal, 22 (1), 227 -245. https://doi.org/10.1007/s11218-018-9472-8
- Porter, C. E., &amp; Donthu, N. (2006). Using the technology acceptance model to explain how attitudes determine Internet usage: The role of perceived access barriers and demographics. Journal of Business Research, 59 (9), 999 -1007. https://doi.org/ 10.1016/j.jbusres.2006.06.003
- Ramírez-Correa, P., Rond ´ an-Catalu ˜ na, F. J., Arenas-Gait ´ an, J., &amp; Martín-Velicia, F. (2019). Analysing the acceptation of online games in mobile devices: An application of UTAUT2. Journal of Retailing and Consumer Services, 50 , 85 -93. https://doi.org/ 10.1016/j.jretconser.2019.04.018
- Ramos, M. R., Barreto, M., Ellemers, N., Moya, M., Ferreira, L., &amp; Calanchini, J. (2016). Exposure to sexism can decrease implicit gender stereotype bias. European Journal of Social Psychology, 46 (4), 455 -466. https://doi.org/10.1002/ejsp.2165
- Rawlinson, K. (2019). Digital assistants like Siri and Alexa entrench gender biases, says UN. The Guardian. https://www.theguardian.com/technology/2019/may/22/digit al-voice-assistants-siri-alexa-gender-biases-unesco-says.
- Ressel, J., V ¨ oller, M., Murphy, F., &amp; Mullins, M. (2024). Addressing the notion of trust around ChatGPT in the high-stakes use case of insurance. Technology in Society, 102644 . https://doi.org/10.1016/j.techsoc.2024.102644
- Reuters. (2018). Amazon ditched AI recruiting tool that favored men for technical jobs. https://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-genderbias-recruiting-engine.
- Richmond, J., Anderson, A., Cunningham-Erves, J., Ozawa, S., &amp; Wilkins, C. H. (2024). Conceptualizing and measuring trust, mistrust, and distrust: Implications for advancing health equity and building trustworthiness. Annual Review of Public Health, 45 . https://doi.org/10.1146/annurev-publhealth-061022-044737
- Roy, M., &amp; Chi, M. T. (2003). Gender differences in patterns of searching the web. Journal of Educational Computing Research, 29 (3), 335 -348. https://doi.org/10.2190/7BR8VXA0-07A7-8AVN
- Russel Group. (2023). New principles on use of AI in education. Russel Group . http s://russellgroup.ac.uk/news/new-principles-on-use-of-ai-in-education/.
- Salloum, S. A., Alhamad, A. Q. M., Al-Emran, M., Monem, A. A., &amp; Shaalan, K. (2019). Exploring students ' acceptance of e-learning through the development of a comprehensive technology acceptance model. IEEE Access, 7 , 128445 -128462. https://doi.org/10.1109/ACCESS.2019.2939467
- Salomon, G., &amp; Müller, P. (2019). Success factors for the acceptance of smart home technology concepts. Digitalen Wandel gestalten: Transdisziplin ¨ are Ans ¨ atze aus Wissenschaft und Wirtschaft , 205 -215. https://doi.org/10.1007/978-3-658-24651-8\_ 6.3
- Savage, N. (2022). Breaking into the black box of artificial intelligence. Nature Outlook . https://www.nature.com/articles/d41586-022-00858-1.
- Schehl, B., Leukel, J., &amp; Sugumaran, V. (2019). Understanding differentiated internet use in older adults: A study of informational, social, and instrumental online activities. Computers in Human Behavior, 97 , 222 -230. https://doi.org/10.1016/j. chb.2019.03.031
- Schlicker, N., Baum, K., Uhde, A., Sterz, S., Hirsch, M. C., &amp; Langer, M. (2022). A micro and macro perspective on trustworthiness: Theoretical underpinnings of the trustworthiness assessment model (TrAM). https://doi.org/10.31234/osf.io/qhwvx.
- Scholz, D. D., Kraus, J., &amp; Miller, L. (2024). Measuring the propensity to trust in automated technology: Examining similarities to dispositional trust in other humans and validation of the PTT-A scale. International Journal of Human-Computer Interaction . , Article 102644. https://doi.org/10.1016/j.techsoc.2024.102644
- Shaouf, A., &amp; Altaqqi, O. (2018). The impact of gender differences on adoption of information technology and related responses: A review. International Journal of Management and Applied Research, 5 (1), 22 -41. https://doi.org/10.18646/ 2056.51.18-003
- Shen, M., Peterson, E. B., Costas-Mu ˜ niz, R., Hunter Hernandez, M., Jewell, S. T., Matsoukas, K., &amp; Bylund, C. L. (2018). The effects of race and racial concordance on patient-physician communication: A systematic review of the literature. Journal of Racial and Ethnic Health Disparities, 5 (1), 117 -140. https://doi.org/10.1007/s40615017-0350-4
- Sheon, A. R., Bolen, S. D., Callahan, B., Shick, S., &amp; Perzynski, A. T. (2017). Addressing disparities in diabetes management through novel approaches to encourage technology adoption and use. JMIR Diabetes, 2 (2), Article e6751. https://doi.org/ 10.2196/diabetes.6751
- Simpson, B., McGrimmon, T., &amp; Irwin, K. (2007). Are blacks really less trusting than whites? Revisiting the race and trust question. Social Forces, 86 (2), 525 -552. https:// doi.org/10.1093/sf/86.2.525

Z.W. Petzel and L. Sowerby

- Smith, S. S. (2010). Race and trust. Annual Review of Sociology, 36 , 453 -475. https://doi. org/10.1146/annurev.soc.012809.102526
- Sniehotta, F. F., Presseau, J., &amp; Araújo-Soares, V. (2014). Time to retire the theory of planned behaviour. Health Psychology Review, 8 (1), 1 -7. https://doi.org/10.1080/ 17437199.2013.869710
- Sohn, K., &amp; Kwon, O. (2020). Technology acceptance theories and factors influencing artificial Intelligence-based intelligent products. Telematics and Informatics, 47 , Article 101324. https://doi.org/10.1016/j.tele.2019.101324
- Sriram, N., &amp; Greenwald, A. G. (2009). The brief implicit association test. Experimental Psychology, 56 (4), 283 -294. https://doi.org/10.1027/1618-3169.56.4.283
- Stanley, D. A., Sokol-Hessner, P., Banaji, M. R., &amp; Phelps, E. A. (2011). Implicit race attitudes predict trustworthiness judgments and economic trust decisions. Proceedings of the National Academy of Sciences, 108 (19), 7710 -7715. https://doi. org/10.1073/pnas.101434510
- Stets, J. E., &amp; Fares, P. (2019). The effects of race/ethnicity and racial/ethnic identification on general trust. Social Science Research, 80 , 1 -14. https://doi.org/ 10.1016/j.ssresearch.2019.02.001
- Suh, B., &amp; Han, I. (2002). Effect of trust on customer acceptance of Internet banking. Electronic Commerce Research and Applications, 1 (3 -4), 247 -263. https://doi.org/ 10.1016/S1567-4223(02)00017-0
- Swim, J. K., Aikin, K. J., Hall, W. S., &amp; Hunter, B. A. (1995). Sexism and racism: Oldfashioned and modern prejudices. Journal of Personality and Social Psychology, 68 (2), 199 -214. https://doi.org/10.1037/0022-3514.68.2.199
- Swim, J. K., Eyssell, K. M., Murdoch, E. Q., &amp; Ferguson, M. J. (2010). Self-silencing to sexism. Journal of Social Issues, 66 (3), 493 -507. https://doi.org/10.1111/j.15404560.2010.01658.x
- Swim, J. K., &amp; Hyers, L. L. (1999). Excuse me -what did you just say?!: Women ' s public and private responses to sexist remarks. Journal of Experimental Social Psychology, 35 (1), 68 -88. https://doi.org/10.1006/jesp.1998.1370
- Tanis, M., &amp; Postmes, T. (2005). Short communication: A social identity approach to trust: Interpersonal perception, group membership and trusting behaviour. European Journal of Social Psychology, 35 (3), 413 -424. https://doi.org/10.1002/ejsp.256
- Tomasetto, C., &amp; Appoloni, S. (2013). A lesson not to be learned? Understanding stereotype threat does not protect women from stereotype threat. Social Psychology of Education: International Journal, 16 (2), 199 -213. https://doi.org/10.1007/s11218012-9210-6
- Towns, A. E. (2010). Women and states: Norms and hierarchies in international society . Cambridge, UK: Cambridge University Press.
- U.S. Department of Labor. (2023). Earnings Disparities by Race and ethnicity. Office of federal contract compliance programs . Department of Labor. https://www.dol.go v/agencies/ofccp/about/data/earnings/race-and-ethnicity.
- van Zomeren, M., Postmes, T., &amp; Spears, R. (2008). Toward an integrative social identity model of collective action: A quantitative research synthesis of three sociopsychological perspectives. Psychological Bulletin, 134 (4), 504 -535. https://doi.org/ 10.1037/0033-2909.134.4.504
- Venkatesh, V., &amp; Morris, M. G. (2000). Why don ' t men ever stop to ask for directions? Gender, social influence, and their role in technology acceptance and usage behavior. MIS quarterly , 115 -139. https://doi.org/10.2307/3250981
- Venkatesh, V., Morris, M. G., &amp; Ackerman, P. L. (2000). A longitudinal field investigation of gender differences in individual technology adoption decision-making processes. Organizational Behavior and Human Decision Processes, 83 (1), 33 -60. https://doi.org/ 10.1006/obhd.2000.2896
- Venkatesh, V., Morris, M. G., Davis, G. B., &amp; Davis, F. D. (2003). User acceptance of information technology: Toward a unified view. MIS Quarterly , 425 -478. https:// doi.org/10.2307/30036540
- Vlasceanu, M., &amp; Amodio, D. M. (2022). Propagation of societal gender inequality by internet search algorithms. Proceedings of the National Academy of Sciences, 119 (29), Article e2204529119. https://doi.org/10.1073/pnas.2204529119
- Von Eschenbach, W. J. (2021). Transparency and the black box problem: Why we do not trust AI. Philosophy &amp; Technology, 34 (4), 1607 -1622. https://doi.org/10.1007/ s13347-021-00477-0
- Wang, K., Stroebe, K., &amp; Dovidio, J. F. (2012). Stigma consciousness and prejudice ambiguity: Can it be adaptive to perceive the world as biased? Personality and Individual Differences, 53 (3), 241 -245. https://doi.org/10.1016/j.paid.2012.03.021
- Wang, X., Wong, Y. D., Chen, T., &amp; Yuen, K. F. (2021). Adoption of shopper-facing technologies under social distancing: A conceptualisation and an interplay between task-technology fit and technology trust. Computers in Human Behavior, 124 , Article 106900. https://doi.org/10.1016/j.chb.2021.106900
- Wanner, J., Herm, L. V., Heinrich, K., &amp; Janiesch, C. (2022). The effect of transparency and trust on intelligent system acceptance: Evidence from a user-based study. Electronic Markets, 32 (4), 2079 -2102. https://doi.org/10.1007/s12525-022-00593-5
- Wichman, A. L. (2012). Uncertainty threat can cause stereotyping: The moderating role of personal need for structure. Sage Open, 2 (2), Article 2158244012444442. https:// doi.org/10.1177/2158244012444442
- Williams, D. R., Priest, N., &amp; Anderson, N. B. (2016). Understanding associations among race, socioeconomic status, and health: Patterns and prospects. Health Psychology, 35 (4), 407 -411. https://doi.org/10.1037/hea0000242
- Woodzicka, J. A., Mallett, R. K., Hendricks, S., &amp; Pruitt, A. V. (2015). It ' s just a (sexist) joke: Comparing reactions to sexist versus racist communications. Humor, 28 (2), 289 -309. https://doi.org/10.1515/humor-2015-0025
- Woodzicka, J. A., Mallett, R. K., &amp; Melchiori, K. J. (2020). Gender differences in using humor to respond to sexist jokes. Humor, 33 (2), 219 -238. https://doi.org/10.1515/ humor-2019-0018
- Wu, J., &amp; Liu, D. (2007). The effects of trust and enjoyment on intention to play online games. Journal of Electronic Commerce Research, 8 (2).
- Wu, K., Zhao, Y., Zhu, Q., Tan, X., &amp; Zheng, H. (2011). A meta-analysis of the impact of trust on technology acceptance model: Investigation of moderating influence of subject and context type. International Journal of Information Management, 31 (6), 572 -581. https://doi.org/10.1016/j.ijinfomgt.2011.03.004
- Zhang, W., &amp; Liu, L. (2022). How consumers ' adopting intentions towards eco-friendly smart home services are shaped? An extended technology acceptance model. The Annals of Regional Science , 1 -24. https://doi.org/10.1007/s00168-021-01082-x