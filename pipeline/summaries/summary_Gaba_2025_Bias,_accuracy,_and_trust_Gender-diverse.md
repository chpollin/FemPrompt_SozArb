---
title: "Gaba 2025 Bias"
original_document: Gaba_2025_Bias.md
document_type: Empirical Study
research_domain: AI Ethics, AI Bias & Fairness, Generative AI
methodology: Qualitative
keywords: gender diversity, LLM bias, trust in AI, ChatGPT, algorithmic fairness
mini_abstract: "Qualitative study of 25 interviews examining how gender-diverse populations perceive bias, accuracy, and trustworthiness in ChatGPT, revealing that non-binary users face stereotypical portrayals and that gender identity significantly influences AI trust formation."
target_audience: Researchers, Practitioners, Industry
key_contributions: "Gender-diverse perspectives on LLM bias and trustworthiness perception"
geographic_focus: North America
publication_year: 2025
related_fields: Human-Computer Interaction, Computer-Supported Cooperative Work, Responsible AI
summary_date: 2025-11-07
language: English
ai_model: claude-haiku-4-5
---

# Summary: Gaba 2025 Bias

## Overview

This empirical study examines how gender-diverse populations perceive bias, accuracy, and trustworthiness in Large Language Models, specifically ChatGPT. Conducted by researchers across multiple institutions and submitted to ACM for publication in the CSCW/HCI field, the work addresses a critical gap in AI development by centering the experiences of non-binary, transgender, male, and female users. The research operates from the premise that LLMs, like other machine learning systems, reflect and perpetuate societal inequalities, particularly regarding gender representation. By investigating differential responses to gendered versus neutral prompts and analyzing user evaluations across gender identities, the study provides empirical evidence that gender identity materially shapes how individuals interact with and trust AI systems. This qualitative investigation contributes to broader conversations about responsible AI development and the necessity of inclusive design practices in technology development.

## Main Findings

The research reveals several significant patterns across three gender identity categories. Gendered prompts—those explicitly referencing or implying gender identity—consistently elicited identity-specific responses from ChatGPT, with non-binary participants experiencing particularly problematic outputs characterized by condescension and stereotyping. This vulnerability represents a critical equity concern. Perceived accuracy remained relatively consistent across gender groups, though errors concentrated in technical topics and creative tasks—suggesting domain-specific rather than gender-specific accuracy issues. Trustworthiness demonstrated substantial gender variation: male participants reported higher overall trust levels, while non-binary participants showed elevated performance-based trust, and female participants occupied intermediate positions. Participants offered actionable recommendations including diversifying training data, ensuring equitable response depth across gender categories, and implementing clarifying questions to reduce ambiguity. These findings suggest that gender bias in LLMs operates through multiple mechanisms affecting user experience and trust formation differently across populations.

## Methodology/Approach

The study employed qualitative methodology through 25 in-depth interviews with participants stratified across three gender identity categories: non-binary/transgender, male, and female. This approach allowed researchers to capture nuanced user experiences and evaluative processes. Participants systematically evaluated LLM responses to both gendered prompts (explicitly referencing gender identity) and neutral prompts (without gender references), enabling direct comparison of how prompt framing influenced outputs. The analysis was grounded in Human-Computer Interaction (HCI) and Computer-Supported Cooperative Work (CSCW) theoretical frameworks, positioning the research within established traditions of user-centered technology evaluation. The gender-critical theoretical lens situated LLM bias within broader patterns of technological discrimination, moving beyond purely technical analyses to incorporate social and contextual dimensions.

## Relevant Concepts

**Algorithmic bias:** Systematic errors in AI systems reflecting and amplifying existing societal inequalities, particularly regarding marginalized groups.

**Gender identity:** Individual's internal sense of gender, encompassing cisgender, transgender, and non-binary identities—distinct from biological sex.

**Gendered prompts:** User inputs explicitly referencing or implying gender identity, used to test whether LLMs produce identity-specific responses.

**Trust in AI:** Multi-dimensional construct encompassing reliability, competence, and performance-based confidence in automated systems.

**Inclusive design:** Development approach prioritizing diverse user perspectives and needs from inception rather than as afterthoughts.

**Responsible AI:** Framework emphasizing fairness, transparency, accountability, and stakeholder involvement in AI system development.

## Significance

This research advances algorithmic fairness scholarship by extending beyond demographic representation to examine user perception and trust formation across gender identities. By centering non-binary and transgender experiences—historically marginalized in HCI research—the study challenges normative assumptions about gender in technology design and highlights specific vulnerabilities of non-binary users to stereotypical AI outputs. The work bridges HCI/CSCW and AI ethics literatures, providing empirical evidence supporting calls for inclusive development practices. Practically, findings inform LLM improvement strategies and broader AI governance discussions. Theoretically, the study reinforces recognition that algorithmic bias requires integrated technical and social solutions, emphasizing that meaningful progress requires incorporating diverse stakeholder perspectives throughout development cycles rather than treating inclusion as a compliance requirement. The research contributes to growing recognition within the CSCW/HCI field that gender-diverse perspectives are essential for developing trustworthy AI systems.
