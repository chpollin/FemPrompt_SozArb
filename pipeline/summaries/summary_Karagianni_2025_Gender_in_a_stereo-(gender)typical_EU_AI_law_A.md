---
title: "Karagianni 2025 Gender"
original_document: Karagianni_2025_Gender.md
document_type: Research Paper
research_domain: AI Ethics
methodology: Theoretical
keywords: AI Act, feminist legal theory, gender bias, intersectionality, algorithmic discrimination
mini_abstract: "This paper critically examines the EU AI Act through a feminist lens, arguing that while it attempts to mitigate gendered risks, it fails to address structural biases that disproportionately harm marginalized communities. The author proposes feminist-informed revisions emphasizing intersectionality and accountability in AI governance."
target_audience: Researchers, Policymakers, Mixed
key_contributions: "Feminist critique of EU AI Act regulatory framework"
geographic_focus: Europe
publication_year: 2025
related_fields: Feminist jurisprudence, AI governance, Decolonial theory
summary_date: 2025-11-07
language: English
ai_model: claude-haiku-4-5
---

# Summary: Karagianni 2025 Gender

## Overview

Anastasia Karagianni's research article, published in 2025, provides a critical feminist examination of the European Union's AI Act (Regulation 2024/1689), questioning whether this landmark regulatory framework adequately protects marginalized communities from gender-based discrimination embedded in artificial intelligence systems. Conducted within the Law Science Technology and Society (LSTS) Research Group at Vrije Universiteit Brussels, the paper challenges the assumption that formal regulatory provisions automatically translate into substantive protection against algorithmic bias. By applying feminist legal theory to specific AI Act provisions—particularly those addressing "special categories of personal data" processing—Karagianni argues that the regulation, while well-intentioned, perpetuates structural inequalities by failing to address the systemic power imbalances that shape AI development and deployment. The work positions itself within emerging critical scholarship that demands transformative rather than incremental approaches to technology governance.

## Main Findings

The analysis reveals significant gaps between the AI Act's stated objectives and its practical capacity to protect vulnerable populations. While the regulation permits processing "special categories of personal data" (including gender identity information) to prevent algorithmic discrimination, this mechanism proves insufficient for addressing deeply embedded structural biases. The paper identifies four critical deficiencies: first, existing provisions fail to challenge underlying structural inequalities that predate and shape AI systems; second, binary gender frameworks persist throughout AI development and deployment, marginalizing transgender and non-binary individuals; third, accountability mechanisms lack enforceability for gender-based harms; and fourth, the regulation inadequately addresses gender-based violence enabled by generative AI, particularly non-consensual deepfakes. The research demonstrates that current regulatory approaches treat discrimination as isolated technical problems rather than manifestations of systemic power hierarchies. Concrete examples—including Amazon's biased recruitment algorithm disadvantaging women, healthcare AI systems operating within binary gender frameworks that misgendered patients, and non-consensual deepfake technology constituting gender-based violence—illustrate how AI amplifies existing societal inequalities. Karagianni concludes that intersectional perspectives remain absent from regulatory design, limiting the AI Act's capacity to address compounded discrimination experienced by individuals with multiple marginalized identities (women, LGBTQIA+ people, and other marginalized communities).

## Methodology/Approach

The paper employs sophisticated feminist legal methods grounded in four complementary theoretical frameworks. Miranda Fricker's hermeneutical injustice theory illuminates how epistemic marginalization prevents certain groups from contributing to knowledge production about AI harms. Catharine MacKinnon's feminist legal theory reveals how male dominance structures become embedded in technological systems and legal frameworks. Aníbal Quijano's coloniality of power concept exposes how historical hierarchies persist within contemporary technology governance. Walter Mignolo's decolonial epistemology challenges whose knowledge counts in law and technology development. This multidisciplinary theoretical architecture enables systematic critical examination of specific AI Act articles, revealing how regulatory language either reinforces or fails to challenge algorithmic discrimination. The methodology combines textual analysis of regulatory provisions with theoretical critique of underlying epistemological assumptions.

## Relevant Concepts

**Hermeneutical injustice:** Epistemic marginalization preventing certain groups from contributing to collective understanding of AI harms and discrimination.

**Structural bias:** Systemic inequalities embedded within AI systems that disproportionately harm marginalized communities, rooted in biased training data and design choices.

**Intersectionality:** Framework recognizing how multiple marginalized identities (gender, sexuality, race, disability) compound discrimination experiences in AI systems.

**Coloniality of power:** Historical power hierarchies and epistemological dominance persisting within contemporary institutions, technologies, and regulatory frameworks.

**Algorithmic discrimination:** Discriminatory outcomes produced through automated decision-making systems, often reproducing historical biases at scale.

**Special categories of personal data:** EU legal concept referring to sensitive data (including gender identity) whose processing is restricted but permitted under specific conditions for non-discrimination purposes.

**Gender-based violence:** Harm targeting individuals based on gender, including non-consensual deepfakes and sexualized content enabled by generative AI.

## Significance

This research significantly contributes to critical AI governance scholarship by demonstrating that formal regulatory frameworks require feminist-informed revision to achieve substantive equity. The paper challenges technocratic optimism surrounding the AI Act, arguing that regulatory provisions alone cannot address discrimination rooted in epistemological and structural inequalities. By proposing feminist-informed revisions emphasizing gender inclusivity, intersectionality, and accountability mechanisms with enforcement capacity, Karagianni advances scholarship demanding transformative approaches to technology governance. The work proves particularly significant for policymakers, legal scholars, technology ethicists, and marginalized communities seeking to develop regulations that genuinely protect vulnerable populations rather than merely appearing to do so. It contributes to broader EU policy discourse on AI governance by identifying specific gaps requiring legislative amendment before the AI Act's full implementation.
