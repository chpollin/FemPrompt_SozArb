# Workflow für eine Deep-Research-gestützte Literaturanalyse am Beispiel von feministischem AI-Literacy
# Einleitung und Zielsetzung

## Problem-Reframing: Jenseits simpler Bias-Narrative

Large Language Models (LLM) verändern die berufliche Praxis. Laut Chatterji et al. (2025) nutzen wöchentlich 700 Millionen Menschen ChatGPT, was eine beispiellose Diffusionsgeschwindigkeit darstellt. Die Analyse von 18 Milliarden wöchentlichen Nachrichten zeigt, dass Schreibaufgaben mit 42 Prozent und die Entscheidungsunterstützung die berufliche Nutzung dominieren. Beide Funktionen sind für die Sozialarbeit bei Fallberichten, Dokumentation, Antragsstellung und Interventionsplanung von zentraler Bedeutung. Gleichzeitig fehlen granulare Daten zur Kategorie „Social Services”. Die zunehmende Verschiebung in den privaten Bereich mit einer Non-Work-Nutzung von 73 Prozent macht LLM-Literacy zur gesellschaftlichen Herausforderung. Aktuelle Erhebungen zur spezifischen LLM-Nutzung in der Sozialarbeit existieren nicht. Unklar bleibt, welche Modelle Praktiker:innen verwenden, für welche Aufgaben sie eingesetzt werden und wie Prompts formuliert werden.

Gallegos et al. (2024) zeigen, dass sich Bias in KI-Systemen als ungleiche Behandlung zwischen sozialen Gruppen manifestiert. Diese resultiert aus historischen und strukturellen Machtasymmetrien. Die Autoren identifizieren sieben Kategorien repräsentationaler Schäden. Suresh und Guttag (2021) ergänzen dies durch ein pipelinebasiertes Framework, das die Entstehung von Bias in jeder Phase des ML-Lebenszyklus dokumentiert.

Aktuelle Forschungsergebnisse deuten auf emergente, kontextabhängige Verhaltensweisen bei Frontier-LLMs hin. Studien zeigen, dass diese erhebliche Prompt-Sensitivität und kontextuelle Variabilität aufweisen, statt konsistente Bias-Muster zu zeigen. Die pauschale Annahme konstanter Bias-Muster erfasst diese Komplexität nicht. Formuliert eine Sozialarbeiterin beispielsweise „afghanische Frau mit Gewalterfahrung” oder „Klientin mit Gewalterfahrung”, stellt sich die Frage, wie die verschiedenen Modelle auf diese unterschiedlichen Formulierungen reagieren. Empirische Untersuchungen solcher kontextspezifischer Reaktionen fehlen bislang, wären für den KI-Einsatz in der Sozialen Arbeit aber entscheidend. Die Herausforderung liegt nicht mehr in simplen Vorurteilen, sondern in Phänomenen wie Sycophancy und Spannungen zwischen AI-Alignment und den professionellen Werten der Sozialen Arbeit.

## Forschungslücke: Veraltete Frameworks für neue Systeme

Bestehende Frameworks wie algorithmische Fairness-Checklisten (Selbst & Barocas, 2019) adressieren statische Bias-Muster, erfassen aber nicht die emergenten, kontextabhängigen Verhaltensweisen von Frontier-LLMs wie Sycophancy oder Alignment-Spannungen. Während Forschende noch Checklisten für algorithmische Fairness entwickeln, haben sich LLMs zu Systemen entwickelt, deren Capabilities ohne explizites Training emergieren und deren Alignment-Mechanismen komplexe Persönlichkeitsstrukturen aufweisen. Die empirische Realität der LLM-Nutzung in der Sozialarbeit bleibt unerforscht. Dokumentation zu Anwendungsszenarien, Prompt-Strategien oder impliziten Annahmen in der Praxis-Interaktion existiert kaum.

Insbesondere fehlen vier kritische Komponenten. Erstens, empirische Daten zur tatsächlichen LLM-Nutzung in der Sozialarbeitspraxis. Zweitens, methodische Ansätze zur systematischen Untersuchung dieser neuen Systemklasse der "Exotic Mind-Like Entities". Drittens, empirisch fundierte Frameworks für intersektionales Prompting, das die Sophistication moderner LLMs nutzt statt sie zu fürchten. Viertens, skalierbare Workflows zur Literaturerschließung in diesem hochdynamischen Feld, wo traditionelle Reviews der Publikationsgeschwindigkeit nicht mehr folgen können. Sozialarbeitende können ohne entsprechende AI-Literacy weder die Potenziale noch die spezifischen Risiken von Frontier-LLMs einschätzen können.

## Forschungsfrage und methodische Innovation

Forschungsfrage: Wie operationalisiert ein Expert-in-the-Loop Workflow mittels paralleler LLM-Deep-Research die systematische Erschließung von Literatur zu intersektionalem Prompting und feministischer AI-Literacy in der Sozialarbeit?

Diese Frage adressiert eine doppelte Zielsetzung. Methodisch entwickelt der Beitrag einen neuartigen Literature Review Workflow, der die Deep Research-Capabilities verschiedener LLM-Systeme (ChatGPT, Claude, Gemini, Perplexity) parallel nutzt und durch Expert-in-the-Loop-Verfahren validiert. Inhaltlich synthetisiert er Erkenntnisse zu diskriminierungssensiblem Prompting und feministischer AI-Literacy für die Sozialarbeitspraxis. Die methodische Besonderheit liegt in der Verwendung von LLMs zur Identifikation von LLM-relevanter Literatur. Diese Vorgehensweise ermöglicht die Dokumentation systemspezifischer Selektionsmuster: Unterschiedliche LLM-Systeme identifizieren teilweise unterschiedliche Publikationen als relevant. Diese Divergenzen werden als methodische Daten erfasst und zeigen, welche Literatur durch welches System auffindbar ist. Dies ist besonders relevant, da Praktiker:innen in der Sozialarbeit zunehmend LLMs für Recherchen nutzen und dabei genau diese systemspezifisch gefilterten Ergebnisse erhalten.

## Aufbau und Implementation

Der Beitrag strukturiert sich in vier Hauptteile. Kapitel 2 etabliert den theoretischen Rahmen, insbesondere die Konzeption von LLMs als "Strange New Minds" und deren Implikationen für feministische AI-Literacy. Es problematisiert zudem den Mangel an empirischen Daten zur tatsächlichen Nutzungspraxis. Kapitel 3 detailliert den dreistufigen PRISMA-konformen Workflow. Die erste Phase umfasst die Identifikation via parallele Deep Research mit kontextparametrisierten Prompts und RIS-Export. Die zweite Phase beschreibt das Expert-in-the-Loop-Assessment mit paralleler Human-LLM-Bewertung in Zotero. Die dritte Phase dokumentiert die Wissenssynthese mittels Obsidian und Model Context Protocol (MCP).

Kapitel 4 präsentiert die empirischen Ergebnisse. Identifizierte Literaturcluster zu LLM-Nutzung in der Sozialarbeit werden kartiert, dokumentierte Prompt-Pattern systematisiert und ein evidenzbasierter Leitfaden für intersektionales Prompting entwickelt. Dieser berücksichtigt sowohl die Systemkomplexität von Frontier-LLMs als auch die praktischen Bedürfnisse der Sozialarbeit. Kapitel 5 diskutiert methodische Limitationen, insbesondere die Zirkularität der LLM-basierten LLM-Analyse. Es entwickelt Implikationen für eine AI-Literacy, die der Komplexität von Frontier-LLMs gerecht wird und die Realität ihrer bereits stattfindenden Nutzung anerkennt. Das GitHub-Repository FemPrompt\_SozArb dokumentiert alle Prompts, Zwischenergebnisse und Entscheidungsprozesse transparent für Replikation und Weiterentwicklung.

# Theoretischer Rahmen und Forschungsstand

## Forschungsstand

Die verfügbare empirische Forschung zu LLMs in der Sozialarbeit hinkt der technologischen Entwicklung hinterher. Segal (2024) dokumentiert erhebliche Limitationen von ChatGPT 3.5 bei ethischen Dilemmata der Sozialarbeit: eine oberflächliche Problemanalyse, die fehlende Integration professionsspezifischer Ethikkodizes und eine mangelnde emotionale Kontextualisierung. Während die Studienteilnehmer von Segal die „Oberflächlichkeit” eines Modells von 2022 kritisierten, operieren heutige Frontier-Systeme mit emergenten Fähigkeiten, die neue Phänomene wie Sycophancy oder Cross-Trait-Effekte nach sich ziehen. Die Herausforderung liegt somit nicht mehr in offensichtlichen Defiziten, sondern in der subtilen Komplexität dieser „Exotic Mind-Like Entities”.

## *The Strange New Minds* und warum LLMs viel komplexer sind

Die Bias-Problematik hat sich bei aktuellen LLMs von offensichtlichen Stereotypen zu subtileren Phänomenen verschoben. Shanahan (2024) prägt den Begriff "Exotic Mind-Like Entities" für diese Systeme, die anders als Menschen operieren, auch wenn ihr Verhalten oft menschenähnlich erscheint. LLMs "lack the means to exercise concepts" wie Verstehen oder Glauben "in anything like the way we do" (Shanahan, 2024, S. 71\) \- sie produzieren statistisch wahrscheinliche Fortsetzungen durch "next-token prediction" mit emergenten, kognitionsähnlichen Mustern. Wenn Praktizierende diesen Systemen Einsicht in Klientensituationen zuschreiben, übersehen sie, dass LLMs ohne das Weltverständnis oder die kritische Reflexionsfähigkeit operieren, die professionelle Sozialarbeit erfordert. Gleichzeitig verfügen diese Systeme über umfangreiches faktisches und prozedurales Wissen, was ihre überzeugende Wirkung ohne tatsächliches Verstehen erklärt.

Erstens emergieren Capabilities ohne explizites Training. Moderne LLMs zeigen Fähigkeiten, die nicht gezielt antrainiert wurden – von In-context Learning bis zur Generalisierung über Domänen hinweg. Zweitens operieren sie mit komplexen Alignment-Mechanismen. Constitutional AI verfolgt die Prinzipien "helpful, harmless, honest" (Anthropic, 2025), doch diese können in Spannung zu professionsspezifischen Werten der Sozialen Arbeit stehen. Drittens entwickeln Frontier-LLMs komplexe Persönlichkeitsstrukturen. Chen et al. (2025) dokumentieren "Persona Vectors" – konsistente Charaktereigenschaften, die sich durch Finetuning verschieben können und unvorhersehbare Cross-Trait-Effekte erzeugen.

In der Praxis manifestiert sich diese Komplexität durch Phänomene wie Sycophancy – die Tendenz, Nutzerannahmen übermäßig zuzustimmen, selbst wenn diese fehlerhaft sind. Malmqvist (2024) quantifiziert dies als „Tendenz der Modelle, Nutzer\*innen übermäßig zuzustimmen oder zu schmeicheln, oft auf Kosten der faktischen Genauigkeit" und dokumentiert Error Introduction Rates von bis zu 40 % bei führenden Anfragen. In der Sozialarbeitspraxis könnte ein LLM so problematische Vorannahmen über Klient:innen verstärken, anstatt sie kritisch zu hinterfragen. Claude etwa, beschrieben als System mit „good character traits" (Askell, 2025), mag hilfreich erscheinen, doch wenn kritisches Hinterfragen als „unhelpful" interpretiert wird, untergräbt dies die professionelle Reflexion.

## Was wir (nicht) über die Nutzung von LLMs in der Sozialarbeit wissen

Die empirische Datenlage zur tatsächlichen LLM-Nutzung in der Sozialarbeit ist dünn. Wir wissen nicht: Welche Nutzungsmuster sich in der Praxis etabliert haben, welche Standards für "gute Nutzung" gelten könnten, und welche Risiken jenseits offensichtlicher Bias-Problematiken existieren. 

Die fehlende empirische Grundlage verhindert die Entwicklung evidenzbasierter Guidelines. Während andere Professionen beginnen, Best Practices zu dokumentieren, operiert die Sozialarbeit im Blindflug. Dies macht einen systematischen Literature Review umso dringlicher: Wir müssen zunächst verstehen, was über intersektionales Prompting und feministische AI-Literacy bereits bekannt ist, bevor wir praxistaugliche Frameworks entwickeln können.

# Methodisches Vorgehen: Multi-Model Literature Review im Expert-in-the-Loop Paradigma

Der vorliegende Workflow operiert im Paradigma der Co-Intelligence (Mollick, 2024), wobei LLMs als kooperative Sparringspartner im Expert-in-the-Loop-Ansatz fungieren: Die Fachperson behält die Entscheidungshoheit und bewertet Ergebnisse ausschließlich nach fachlichem Nutzen (Natarajan et al., 2024).

![[socialai-literature-review-workflow.png]]

*Abbildung 1: Expert-in-the-Loop Literature Review Workflow.* Die Grafik zeigt einen dreistufigen, LLM-unterstützten Literaturrecherche-Workflow. Im Abschnitt **Identifikation & Import** werden Quellen aus Bibliotheksdatenbanken und Open-Access-Repos über einen Deep-Research-Prompt ermittelt und als strukturierte Metadaten (RIS/LLM) für den Import vorbereitet. Anschließend folgt **Kuration & Bewertung** in Zotero: Ein Human-in-the-Loop markiert das manuelle Screening auf Qualität und Relevanz; parallel reichern Pipelines (Python/Docling/LLM) die Datensätze an und klassifizieren sie. In **Analyse & Synthese** wird das kuratierte Material in Obsidian zur Wissensbasis verdichtet—mit Tags/Keywords, Verknüpfungen und anwendungsorientierter Aufbereitung. Ein bidirektionaler MCP-Kanal verbindet die Wissensbasis mit einem LLM für Abfragen und Synthesen; PRISMA dient als methodischer Referenzrahmen für den gesamten Prozess.

Diese methodische Positionierung erfordert drei Kompetenzen: (1) Fachliche Einordnung der LLM-Antworten basierend auf domänenspezifischer Expertise, (2) technisches Modellverständnis bezüglich Context Window, Sycophancy-Tendenzen und Persona-Effekten, sowie (3) methodische AI-Literacy für effektives Prompt Engineering. Nur durch diese Verschränkung von Fachexpertise und technischem Verständnis kann die "Co-Intelligence" produktiv genutzt werden, ohne in die Falle unkritischer Übernahme von LLM-Outputs zu geraten.

Der in Abbildung 1 dargestellte Workflow gliedert sich in drei aufeinander aufbauende Phasen, die im Folgenden detailliert beschrieben werden.

## Identifikation

### Parallele Deep Research mit kontextparametrisierten Prompts

Deep Research bezeichnet die Fähigkeit aktueller Frontier-LLMs, autonom mehrstufige Recherchen zu komplexen Anfragen durchzuführen. Im Gegensatz zu einfachen Einzelantworten zerlegen diese Systeme komplexe Fragen, führen multiple Suchanfragen durch, analysieren zahlreiche Quellen und synthetisieren umfassende Antworten mit überprüfbaren Zitationen.

Der Workflow nutzt vier unterschiedliche Deep Research-Implementierungen parallel (ChatGPT, Claude, Gemini, Perplexity), deren spezifische technische Ansätze sich unterscheiden.[^1] Die Kontextparametrisierung erfolgt durch theoriegeleitete Prompt-Templates, die domänenspezifische Terminologie (intersektionale Analyse, feministische AI-Literacy), methodische Anforderungen (PRISMA-konforme Dokumentation) und feldspezifische Publikationskanäle (Social Work Journals, DH-Konferenzen) integrieren. Die aggregierten Suchergebnisse werden via RIS-Export in eine gemeinsame Zotero-Bibliothek überführt, wobei die Quelltransparenz durch Dokumentation des jeweiligen Deep Research-Systems gewährleistet bleibt. Detaillierte Prompt-Templates und RIS-Konvertierungsanleitungen sind im GitHub-Repository FemPrompt\_SozArb verfügbar.

Die Implementierung folgt mit entsprechenden Anpassungen den PRISMA-2020-Richtlinien. Die Identifikationsphase dokumentiert jedes Deep Research-System als distinkte Informationsquelle mit präzisen Trefferzahlen und verwendeten Prompts (Items 7-8). Das Screening erfolgt über standardisierte Eligibility-Kriterien mit sieben Ausschlusskategorien (Items 9-10). Die Datenextraktion nutzt strukturierte Templates für maschinelle Auswertbarkeit (Items 11-13). Diese PRISMA-Compliance wird durch KI-spezifische Transparenzdokumentation erweitert: vollständige Modellversionierung, archivierte Prompt-Templates und timestamped Expert-Validierungen.

## Kuration und Bewertung

### Parallel Human-AI Assessment

Das Assessment-Design adaptiert den Benchmarking-Ansatz von Woelfle et al. (2024), bei dem identische Items von menschlichen Expert:innen und LLMs mit demselben Schema bewertet werden. Im Unterschied zu Woelfles *Evidence Appraisal* (strukturierte Checklisten-Items) erfordert die hier implementierte Screening-Aufgabe thematische Kategorisierung auf Basis disziplinären Kontextwissens. Dies positioniert die Aufgabenkomplexität näher an PRECIS-2-Niveau, wo selbst erfahrene menschliche Rater nur moderate Übereinstimmung erreichen (κ ≈ 0.29; Woelfle et al., 2024). Hanegraaf et al. (2024) zeigen, dass menschliche Inter-Reviewer-Reliabilität in systematischen Reviews bei κ = 0.77–0.88 liegt – allerdings in Kontexten mit klareren Einschlusskriterien als "feministische AI-Literacy". Diese Werte dienen als oberer Erwartungshorizont.

#### Empirischer Referenzrahmen

Drei Studien bilden den empirischen Referenzrahmen für das Assessment-Design:

**Woelfle et al. (2024)** vergleichen fünf LLMs mit menschlichem Konsens bei drei PRISMA-basierten Bewertungsinstrumenten steigender Komplexität (n=168 Publikationen, *J Clin Epidemiol*). Der zentrale Befund ist die Komplexitätsabhängigkeit der LLM-Leistung: Bei textbasierten Items (PRISMA) schneiden LLMs moderat ab, bei kontextuellen Bewertungen (PRECIS-2) deutlich schlechter, jeweils unter der menschlichen Baseline. Human-AI-Kollaboration mit einem Deferring-Ansatz kann die menschliche Leistung übertreffen, allerdings steigt die Delegationsrate an Menschen mit der Aufgabenkomplexität erheblich. Auch die menschliche Inter-Rater-Reliabilität sinkt bei komplexen Aufgaben stark (κ = 0.84 bei einfachen vs. κ = 0.29 bei komplexen Items). Methodisch vorbildlich durch offenen Code, Daten, Prompts und Bootstrap-Konfidenzintervalle.

**Hanegraaf et al. (2024)** quantifizieren erstmals systematisch die menschliche IRR-Baseline in systematischen Reviews (n=45 SLRs, Survey n=37, *BMJ Open*). Erfahrene Reviewer erreichen κ = 0.77–0.88 je nach Screening-Phase – moderate bis starke Übereinstimmung, aber keine perfekte. Die Survey zeigt einen Doppelstandard: Befragte erwarten von ML-gestützten Reviews höhere Reliabilität als von menschlichen, analog zum *better-than-average*-Effekt bei autonomen Fahrzeugen. Einschränkung: nur SLRs von RCTs, also Kontexte mit klareren Kriterien als interpretative Kategorien.

**Sandner et al. (2025)** untersuchen IRR bei Novizen (54 Studierende, 10 Teams) im Vergleich zu LLM-Screening (Konferenzpräsentation, OSSYM). Der Kernbefund: Die Abweichung zwischen LLM und menschlichem Konsens (κ ≈ 0.52) liegt in derselben Größenordnung wie die Übereinstimmung der Novizen untereinander (κ ≈ 0.39). Das LLM weicht also nicht stärker von Menschen ab als Menschen voneinander – die einzige Studie, die die Expertise-Variable direkt adressiert.

**Konvergierende Erkenntnis:** Alle drei Studien zeigen, dass die menschliche Baseline selbst nicht perfekt ist und dass Aufgabenkomplexität und Reviewer-Erfahrung die Übereinstimmung stärker bestimmen als die Frage Mensch vs. Maschine. LLMs allein liegen unter erfahrener menschlicher Leistung; Human-AI-Kollaboration kann sie übertreffen, aber nur bei hinreichend strukturierten Aufgaben.

#### Position des eigenen Projekts

Das 10-Kategorien-Schema mit interpretativen Kategorien ("Feministisch", "Diversität") liegt auf der Komplexitätsskala vermutlich zwischen Woelfles mittleren und schwierigen Instrumenten. Die Expert:innen positionieren sich zwischen Sandners Novizen und Woelfles erfahrenen Ratern. Die erwartbare Spanne liegt bei κ ≈ 0.5–0.8 (Human-Human) und κ ≈ 0.3–0.7 (Human-LLM), jeweils kategorienabhängig.

Die konvergierende Erkenntnis stützt das Expert-in-the-Loop-Narrativ des Projekts, allerdings mit einer wichtigen Nuancierung: Die Unterstützungsleistung des LLM ist nicht symmetrisch. Bei keyword-nahen Kategorien (Prompting, Generative_KI) kann das LLM als zuverlässiger zweiter Reviewer fungieren. Bei interpretativen Kategorien (Feministisch, Diversität) liegt sein Beitrag vermutlich weniger in der eigenständigen Bewertung als in der Identifikation von Fällen, die menschliche Aufmerksamkeit verdienen – also eher als Filtersystem denn als Urteilsinstanz.

Die Referenzstudien operieren in Domänen mit relativ klaren Kriterien (klinische Evidenz, RCT-Einschluss, IR-Relevanz). Das eigene Projekt testet, ob Expert-in-the-Loop auch in einer Domäne funktioniert, in der die Kriterien selbst disziplinäres Kontextwissen voraussetzen. Falls sich zeigt, dass das LLM bei interpretativen Kategorien systematisch anders urteilt als die Expert:innen, wäre das ein empirischer Beleg dafür, wo genau die Grenze zwischen maschineller Zuarbeit und fachlicher Urteilsbildung verläuft. Expert-in-the-Loop wird damit nicht als Qualitätskontrolle über ein autonomes System verstanden, sondern als methodisch begründete Arbeitsteilung, bei der die fachliche Expertise bestimmt, welche Entscheidungen delegierbar sind und welche nicht.

Drei Aspekte gehen über die Referenzliteratur hinaus: die Domänenspezifik feministischer Theorietraditionen, die mögliche Sycophancy des LLM bei wertgeladenen Kategorien und die Zirkularität der LLM-basierten LLM-Analyse.

Die in Zotero gesammelten Publikationen werden exportiert und durchlaufen eine parallele Bewertung durch Fachexpertinnen und Claude Haiku 4.5. Diese Doppelbewertung kombiniert PRISMA-Kriterien mit domänenspezifischen Relevanzdimensionen. In der ersten Stufe erfolgt die binäre Inklusions-/Exklusionsentscheidung basierend auf formalen Kriterien: Peer-Review-Status, Publikationssprache (DE/EN), und Volltext-Verfügbarkeit. Papers werden als "Include" klassifiziert, wenn alle Einschlusskriterien erfüllt sind, als "Exclude" bei Vorliegen mindestens eines Ausschlusskriteriums (thematische Irrelevanz, falscher Publikationstyp, Sprachbarriere, Duplikat, fehlender Volltext), oder als "Unclear" für unabhängige Zweitbewertung markiert. Die zweite Stufe kartiert inkludierte Paper mittels einer vierstufigen Relevanzskala (0=nicht behandelt, 1=Randerwähnung, 2=substanzieller Beitrag, 3=zentraler Fokus) über fünf theoriegeleitete Dimensionen: (1) AI-Kompetenzen und Literacy-Frameworks, (2) vulnerable Gruppen und Zugangsbarrieren im Kontext digitaler Ungleichheit, (3) algorithmische Bias und Fairness-Metriken, (4) praktische Implementation mit konkreten Tools oder Guidelines, sowie (5) professioneller Care-Kontext in Sozialarbeit und Beratung. Diese granulare Bewertungsmatrix ermöglicht nicht nur transparente und nachvollziehbare Selektionsentscheidungen, sondern generiert gleichzeitig eine multidimensionale Kartierung des Forschungsfeldes – wodurch sowohl thematische Cluster als auch Forschungslücken an der Schnittstelle von Frontier-LLMs, intersektionaler Gerechtigkeit und Sozialarbeitspraxis systematisch sichtbar werden. Die Bewertungsergebnisse werden nach Validierung als strukturierte Tags in die Zotero-Bibliothek reimportiert. Divergenzen zwischen menschlicher und maschineller Einschätzung werden als epistemische Marker dokumentiert und fließen in die methodische Reflexion ein.

## Analyse & Synthese

Die Analyse erfolgt in einem Obsidian-Vault mit vier Ebenen: Papers (Originaltexte), Concepts (extrahierte Kernkonzepte), MOCs (thematische Übersichten) und Synthesis (aggregierte Erkenntnisse). Mittels regelbasierter Extraktion werden Bias-Typen und Mitigation-Strategien identifiziert und über bidirektionale Wikilinks vernetzt. Die Konzeptextraktion inkludiert nur Terme mit mindestens zwei Vorkommen im Korpus und normalisiert Synonyme. Diese Vernetzung ermöglicht Identifikation von Konzept-Clustern basierend auf gemeinsamer Paper-Zuordnung und Häufigkeitsanalyse und gemeinsamen Konzepten. Die generierten Strukturen und der Analysecode sind im Repository FemPrompt\_SozArb dokumentiert. Limitationen der regelbasierten Extraktion und das Fehlen semantischer Analysemethoden werden in der methodischen Diskussion adressiert.

# Ergebnisse des Literature Reviews

\[\[Platzhalter: In diesem Kapitel werden die Ergebnisse des systematischen Literature Reviews präsentiert. Die Darstellung erfolgt entlang der entwickelten fünfdimensionalen Bewertungsmatrix (AI-Kompetenzen, vulnerable Gruppen, Bias/Fairness, praktische Implementation, Care-Kontext). Quantitative Aspekte umfassen die Anzahl identifizierter Publikationen pro Deep Research System, die Verteilung nach Publikationstypen und zeitliche Trends. Die qualitative Analyse mittels Obsidian wird thematische Cluster, Konzeptvernetzungen und identifizierte Forschungslücken visualisieren. Besondere Aufmerksamkeit gilt der Frage, inwieweit die existierende Literatur bereits die Komplexität von Frontier-LLMs als "Strange New Minds" adressiert oder noch mit Analysekategorien der Pre-Transformer-Ära operiert. Die Synthese wird zeigen, welche Aspekte von intersektionalem Prompting und feministischer AI-Literacy in der Literatur behandelt werden und wo kritische Leerstellen existieren.\]\]

# Diskussion

\[\[Platzhalter: Die LLM-basierte Literaturidentifikation unterliegt systematischen Einschränkungen, die über technische Limitationen hinausgehen. Erstens können LLMs ausschließlich auf Open-Access-Publikationen zugreifen, wodurch ein erheblicher Teil kritischer akademischer Diskurse hinter Paywalls unsichtbar bleibt. Zweitens deuten Studien zur Selbst-Introspection von LLMs darauf hin, dass diese Systeme möglicherweise Schwierigkeiten haben, explizit selbstkritische Inhalte zu identifizieren oder zu priorisieren – ein Phänomen, das als 'epistemische Selbstimmunisierung' bezeichnet werden könnte. Diese doppelte Blindheit – strukturell durch Paywalls, systemisch durch potenzielle Selbstzensur – macht die Expert-in-the-Loop-Validierung unerlässlich und unterstreicht, warum LLM-basierte Reviews traditionelle systematische Reviews ergänzen, aber nicht ersetzen können.

Der Multi-Model-Workflow zeigt spezifische Stärken und Limitationen. Die parallele Nutzung verschiedener Deep Research-Systeme dokumentiert systemspezifische Selektionsmuster, die als methodische Daten behandelt werden. Gleichzeitig bleiben Fragen der Vollständigkeit und Reproduzierbarkeit bestehen, da proprietäre Systeme verwendet werden, deren interne Mechanismen intransparent sind.

Die identifizierten Forschungslücken verdeutlichen ein strukturelles Problem: Während die technologische Diffusion von LLMs in der Sozialarbeit fortschreitet, fehlen empirische Grundlagen für evidenzbasierte Nutzung. Die Diskrepanz zwischen der Geschwindigkeit technologischer Adoption und wissenschaftlicher Reflexion gefährdet eine verantwortungsvolle Integration in die professionelle Praxis.

Der Review bildet die konzeptuelle Grundlage für den geplanten FAIR-SW-Bench (Fairness in AI for Social Work Benchmark). Während der Review zeigt, dass empirische Daten zu Prompt-Effektivität fehlen, wird der Benchmark untersuchen, wie unterschiedliche Prompt-Strategien die Qualität und Fairness von LLM-Outputs in sozialarbeiterischen Kontexten beeinflussen – von Dokumentationsaufgaben über Entscheidungsunterstützung bis zu bias-kritischen Formulierungsvarianten.

Zukünftige Forschung sollte drei Prioritäten verfolgen: Erstens, empirische Studien zur tatsächlichen LLM-Nutzung in der Sozialarbeit. Zweitens, Entwicklung professionsspezifischer Frameworks für intersektionales Prompting. Drittens, Etablierung validierter Standards für LLM-gestützte systematische Reviews. Nur durch diese dreifache Strategie kann die Sozialarbeit die Potenziale von Frontier-LLMs nutzen und gleichzeitig ihre professionellen Werte wahren.\]\]

# Referenzen

* Anthropic. (2025). Anthropic Education Report: How University Students Use Claude. [https://www.anthropic.com/news/anthropic-education-report-how-university-students-use-claude](https://www.anthropic.com/news/anthropic-education-report-how-university-students-use-claude)  
* Chatterji, A., Cunningham, T., Deming, D. J., et al. (2025). How People Use ChatGPT (Working Paper No. W34255). National Bureau of Economic Research. [https://doi.org/10.3386/w34255](https://doi.org/10.3386/w34255)  
* McGowan, J., Sampson, M., Salzwedel, D. M., et al. (2021). PRISMA-S: An extension to the PRISMA Statement for Reporting Literature Searches in Systematic Reviews. Systematic Reviews, 10, 39\.  
* Moher, D., Liberati, A., Tetzlaff, J., Altman, D. G., & PRISMA Group. (2009). Preferred Reporting Items for Systematic Reviews and Meta-Analyses: The PRISMA Statement. PLoS Medicine, 6(7), e1000097.  
* Page, M. J., McKenzie, J. E., Bossuyt, P. M., et al. (2021). The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. BMJ, 372, n71. [https://doi.org/10.1136/bmj.n71](https://doi.org/10.1136/bmj.n71)  
* Segal, M. (2025). Social workers’ evaluation of ChatGPT for solving ethical dilemmas within the limits of confidentiality. Journal of Social Work Practice, 1–14. [https://doi.org/10.1080/02650533.2025.2480092](https://doi.org/10.1080/02650533.2025.2480092)   
* GitHub Repository: [https://github.com/chpollin/FemPrompt\_SozArb](https://github.com/chpollin/FemPrompt_SozArb)  
* Mollick, E. (2024). Co-Intelligence: Living and Working with AI. Portfolio/Penguin.  
* Natarajan, S., Mathur, S., Sidheekh, S., Stammer, W., & Kersting, K. (2024). Human-in-the-Loop or AI-in-the-Loop? Automate or Collaborate? arXiv:2412.14232. [https://doi.org/10.48550/arXiv.2412.14232](https://doi.org/10.48550/arXiv.2412.14232)   
* Gallegos, I., et al. (2024). Bias and Fairness in Large Language Models: A Survey. *Computational Linguistics*, 50(3).  
* Suresh, H., & Guttag, J. (2021). A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. *Proceedings of EAAMO '21: Equity and Access in Algorithms, Mechanisms, and Optimization*. ACM.
* Selbst, A. D., & Barocas, S. (2019). Fairness and Abstraction in Sociotechnical Systems. *Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* '19)*. ACM. [https://doi.org/10.1145/3287560.3287598](https://doi.org/10.1145/3287560.3287598)
* Malmqvist, Lars. 'Sycophancy in Large Language Models: Causes and Mitigations'. Preprint, 22 November 2024\. [https://arxiv.org/abs/2411.15287v1](https://arxiv.org/abs/2411.15287v1).
* Woelfle, T., et al. (2024). Benchmarking Human–AI collaboration for common evidence appraisal tools. *Journal of Clinical Epidemiology*, 175, 111533.
* Hanegraaf, G., et al. (2024). Inter-reviewer reliability of human literature reviewing and implications for the introduction of machine-assisted systematic reviews. *BMJ Open*, 14, e076912.
* Sandner, F., et al. (2025). Assessing the Reliability of Human and LLM-Based Screening in Systematic Reviews: A Study of First-Time Reviewers. Konferenzpräsentation (OSSYM).


[^1]:  ChatGPT Research Mode verwendet einen Single-Agent-Ansatz mit End-to-End Reinforcement Learning für autonomes Browsing und iterative Strategieanpassung. Claude Research operiert als Multi-Agent-System, bei dem ein Lead-Agent die Recherchestrategie formuliert und spezialisierte Sub-Agenten parallel verschiedene Aspekte untersuchen. Gemini Deep Think balanciert durch konfigurierbaren Thinking Budget zwischen Analysetiefe und Bearbeitungszeit. Perplexity Deep Research kombiniert Web-Scale-Crawling mit proprietären Ranking-Algorithmen zur Identifikation aktueller und autoritativer Quellen.