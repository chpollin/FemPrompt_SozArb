paper_id,title,author_year,has_human,has_agent,human_AI_Literacies,agent_AI_Literacies,agree_AI_Literacies,human_Generative_KI,agent_Generative_KI,agree_Generative_KI,human_Prompting,agent_Prompting,agree_Prompting,human_KI_Sonstige,agent_KI_Sonstige,agree_KI_Sonstige,human_Soziale_Arbeit,agent_Soziale_Arbeit,agree_Soziale_Arbeit,human_Bias_Ungleichheit,agent_Bias_Ungleichheit,agree_Bias_Ungleichheit,human_Gender,agent_Gender,agree_Gender,human_Diversitaet,agent_Diversitaet,agree_Diversitaet,human_Feministisch,agent_Feministisch,agree_Feministisch,human_Fairness,agent_Fairness,agree_Fairness,human_decision,agent_decision,agree_decision,human_exclusion_reason,agent_exclusion_reason,human_studientyp,agent_studientyp,agent_confidence,agent_reasoning,category_agreements,category_total,category_agreement_rate
1,"Navigating the Nexus of Trust: Prompt Engineering, Professional Judgment, and the Integration of Large Language Models in Social Work",[Author not specified] (2025),Ja,Ja,,Ja,,,Ja,,,Nein,,,Nein,,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,,Nein,,,Nein,,,Nein,,,Exclude,,,No_full_text,,Unclear,0.3,"Nur Titel und Abstract vorhanden. Der Titel deutet auf Nutzungsforschung zu ChatGPT hin (Generative_KI, potentiell AI_Literacies). Jedoch fehlt der volle Text zur substanziellen Bewertung. Das Abstract ist nur eine NBER-Standardbeschreibung ohne inhaltliche Details. Keine erkennbaren sozialen Aspekte (Soziale_Arbeit, Bias, Gender, etc.). Daher: Exclude (zu wenig Information + wahrscheinlich keine sozialen Dimensionen).",1,3,33.33%
2,Incubating Feminist AI: Executive Summary 2021-2024,A+ Alliance (2024),Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Exclude,Exclude,Ja,Other,Not_relevant_topic,Incubating Feminist AI: Our Executive Summary,Konzept,0.7,"Das Paper behandelt AI Literacy und Kompetenzentwicklung im KI-Bereich (AI_Literacies = Ja), erfüllt aber die zweite Bedingung nicht: Es fehlt ein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität, Feminismus oder Fairness. Das Paper ist relevant für AI Literacy generell, nicht aber für den Kontext von KI und Sozialer Arbeit. Ohne Volltext kann die fehlende Sozial-Komponente nicht widerlegt werden.",2,10,20.00%
3,Feminist perspectives on AI: Ethical considerations in algorithmic decision-making,Ahmed (2024),Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Include,Exclude,Nein,,"Nur TECHNIK-Kategorie erfüllt (AI_Literacies), aber keine SOZIAL-Kategorie. Paper behandelt KI-Kompetenzentwicklung in Organisationen allgemein, ohne Bezug zu Sozialer Arbeit, Bias, Ungleichheit, Gender oder Fairness.",Literaturreview,Konzept,0.85,"Titel deutet auf AI Literacy Framework hin - klare AI_Literacies-Kategorie. Kein Abstract vorhanden; basierend auf Titel keine Indikation für soziale Dimensionen (Bias, Ungleichheit, Gender, Fairness, Soziale Arbeit). Paper erfüllt nur Technik-Kriterium, nicht Sozial-Kriterium. EXCLUDE nach strikter Logik.",2,10,20.00%
4,Feminist Perspectives on AI: Ethical Considerations in Algorithmic Decision-Making,Ahmed (2024),Ja,Ja,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,Exclude,Include,Nein,Duplicate,,,Empirisch,0.95,"Paper adressiert zentral AI-Kompetenzen von Fachkräften (AI_Literacies: Ja - Kompetenzrahmen für Jugendhilfe), behandelt Generative AI (Ja - explizit Gen Z und generative Tools), hat direkten Bezug zu Sozialarbeit/Jugendhilfe (Ja - Youth Workers, Jugendhilfesektor). Empirische Studie mit Fokusgruppen und Interviews. Beide Bedingungen erfüllt: Technik (AI_Literacies + Generative_KI) UND Sozial (Soziale_Arbeit).",0,0,
5,Artificial Intelligence (AI) Literacy for Social Work: Implications for Core Competencies,Ahn (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,Only_Technik_erfüllt,,Literaturreview,0.95,"Das Paper ist eine systematische Literaturreviw zu AI Literacy im K-12 Bildungskontext und behandelt substantiell AI-Kompetenzen, Curricula und Lernansätze (AI_Literacies = Ja). Es erfüllt jedoch KEINE sozialen Kriterien (Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness alle Nein). Der Fokus liegt rein auf Bildungsintegration und Kompetenzentwicklung, nicht auf sozialen Auswirkungen, Ungleichheiten oder Soziale Arbeit. Beide Bedingungen (Technik UND Sozial) sind nicht erfüllt → Exclude.",0,0,
6,Artificial Intelligence (AI) Literacy for Social Work: Implications for Core Competencies,Ahn (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,Exclude,Exclude,Ja,Duplicate,"Only TECHNIK-Dimension erfüllt (AI_Literacies), aber keine SOZIAL-Dimension. SOZIAL-Kriterium nicht erfüllt: Keine direkten Bezüge zu Sozialer Arbeit, keine substantielle Behandlung von Bias/Ungleichheit, Gender, Diversität, Feminismus oder Fairness. Die ethischen Implikationen werden allgemein erwähnt, aber nicht substantiell in einer der SOZIAL-Kategorien behandelt.",,Empirisch,0.92,"Paper untersucht AI Literacy bei Doktoranden auf vier Dimensionen (kognitiv, operativ, kritisch, ethisch). Dies erfüllt AI_Literacies-Kriterium substantiell. Jedoch adressiert das Paper keinen direkten Bezug zu Sozialer Arbeit, behandelt Bias/Ungleichheit nicht fokussiert und hat keinen Gender-, Diversitäts-, Feminist. oder Fairness-Schwerpunkt. Allgemeine Ethik-Erwähnung reicht nicht aus.",0,0,
7,Artificial Intelligence (AI) literacy for social work,Ahn (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,Exclude,Exclude,Ja,Duplicate,Not_relevant_topic,,Literaturreview,0.95,"Das Paper entwickelt ein umfassendes AI-Literacy-Kompetenzframework und erfüllt die TECHNIK-Bedingung substanziell (AI_Literacies=Ja). Allerdings fehlt jeglicher SOZIAL-Bezug: Es gibt keinen Fokus auf Soziale Arbeit, Bias/Ungleichheit, Gender, Diversität, feministische Perspektiven oder Fairness. Das Paper behandelt rein Bildungscurricula und Kompetenzentwicklung ohne sozialen oder kritischen Perspektivbezug. Die SOZIAL-Bedingung ist nicht erfüllt.",0,0,
8,Artificial Intelligence (AI) literacy for social work: Implications for core competencies,Ahn (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,Exclude,Exclude,Ja,Duplicate,No_full_text / SOZIAL-Kriterium nicht erfüllt,,Literaturreview,0.85,"Das Paper behandelt substantiell AI Literacy und Kompetenzen (AI_Literacies = Ja). Allerdings fehlt jeglicher Bezug zu sozialen Aspekten wie Soziale Arbeit, Bias, Gender, Diversität, Fairness oder feministische Perspektiven. TECHNIK-Kriterium erfüllt, aber SOZIAL-Kriterium nicht. Daher: Exclude.",0,0,
9,AI FORA – Artificial Intelligence for Assessment: Fairness bei der Verteilung öffentlicher sozialer Leistungen,Ahrweiler (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,Not_relevant_topic,,Konzept,0.85,"DigComp 2.1 ist ein Rahmenwerk für digitale Kompetenzen im Allgemeinen, nicht spezifisch für KI/ML-Systeme. Es behandelt digitale Literalität umfassend, aber nicht AI Literacy im Sinne der Definition (KI-spezifische Kompetenzen, technisches KI-Verständnis). Keine Bezüge zu sozialen Arbeit, Bias, Fairness oder kritische KI-Reflexion erkennbar. Technik-Kategorie nicht erfüllt.",0,0,
10,"Social work in the age of artificial intelligence: A rights-based framework for evidence-based practice through social psychology, group dynamics, and institutional analysis",Alam (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,"Wrong_publication_type und Not_relevant_topic - Paper adressiert Digital Health Literacy im medizinischen Kontext, nicht KI-Literacies. Thema ist Gesundheitswesen, nicht KI/Technik. Keine substantielle Verschränkung von KI-Technologie und Sozialer Arbeit.",,Literaturreview,0.95,"Das Paper ist ein Scoping Review zu Digital Health Literacy im Gesundheitssektor. Obwohl es marginale Überschneidungen mit AI_Literacies und Bias/Ungleichheit hat, fehlt jede substanzielle Verknüpfung zu KI-Technologien oder sozialer Arbeit. Es behandelt digitale Gesundheitskompetenz allgemein, nicht KI-spezifisch, und hat keinen Bezug zu Sozialer Arbeit als Praxis/Theorie.",0,0,
11,Policy advice and best practices on bias and fairness in AI,Alvarez (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,Not_relevant_topic,,Literaturreview,0.95,"Das Paper behandelt digitale Literalität allgemein, NICHT AI Literacy im Sinne von Kompetenzen im Umgang mit KI-Systemen. Der Fokus liegt auf digital competencies, digital skills und digital thinking ohne spezifischen Bezug zu KI, generativen Modellen oder algorithmischen Systemen. Weder Technologie- noch Sozialkomponenten des relevanten Spektrums werden erfüllt.",0,0,
12,Coded injustice: Surveillance and discrimination in Denmark's automated welfare state,Amnesty International (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Exclude,,,Not_relevant_topic,,Literaturreview,0.95,"Das Paper behandelt digitale Literalität von Studierenden im Allgemeinen, nicht KI-spezifische Kompetenzen (AI Literacy). Gender wird erwähnt als Einflussfaktor, aber substantiell nicht fokussiert. Keine KI/ML-Technologie-Komponente. Kein Bezug zu Sozialer Arbeit. TECHNIK-Bedingung nicht erfüllt → Exclude.",0,0,
13,Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation,An (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,"Nur TECHNIK-Kategorie erfüllt, SOZIAL-Kategorie nicht erfüllt. Paper behandelt KI-Literacies/Kompetenzen, hat aber keinen substantiellen Bezug zu sozialen Dimensionen (Soziale_Arbeit, Bias/Ungleichheit, Gender, Diversität, Fairness oder feministische Perspektive).",,Literaturreview,0.95,"Das Paper fokussiert auf AI Literacy als Kompetenzbreich – ein eindeutiges TECHNIK-Thema (AI_Literacies: Ja). Der Titel und Fokus deuten auf eine Übersichtsarbeit zu Lernmethoden und Effekten hin, ohne soziale/kritische Dimensionen. Ohne Abstract und Hinweise auf Soziale Arbeit oder kritische Bias-/Fairness-Analysen erfüllt das Paper nicht die SOZIAL-Bedingung.",0,0,
14,Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation,An (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,Exclude,Exclude,Ja,Duplicate,Not_relevant_topic,,Literaturreview,0.75,"Das Paper behandelt AI Literacy conceptually durch eine exploratorische Literaturübersicht. Dies erfüllt die TECHNIK-Bedingung (AI_Literacies = Ja). Jedoch keine SOZIAL-Kategorie wird erfüllt. Ohne Abstract und nur Titel ist unklar, ob das Paper eine Sozialarbeitsperspektive, Bias-Diskussion, Diversität oder Fairness integriert. Die STRIKTE Entscheidungslogik verlangt BEIDE Bedingungen (Technik UND Sozial). Mit nur Technik → Exclude.",0,0,
15,Measuring gender and racial biases in large language models,An (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,"Nur TECHNIK_OK (AI_Literacies erfüllt), aber SOZIAL_OK nicht erfüllt",,Empirisch,0.85,"Das Paper behandelt AI Literacy als Bildungsintervention für Studierende mit unterschiedlichen Hintergründen. Während die AI_Literacies-Kategorie klar erfüllt ist (Evaluation eines Kompetenz-Kursus), fehlt jeder Bezug zu sozialen Aspekten wie Bias, Ungleichheit, Gender, Diversität, Fairness oder Sozialer Arbeit. Es ist ein reines Bildungs-Evaluationspaper ohne soziale/kritische Dimension.",0,0,
16,Digital literacy as a new determinant of health: A scoping review,Arias López (2023),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,Not_relevant_topic,,Empirisch,0.85,"Das Paper behandelt AI Literacy durch die Entwicklung und Validierung einer Messskala. Obwohl AI_Literacies erfüllt ist (Kompetenzen im Umgang mit KI), fehlt jeglicher Bezug zu sozialen Dimensionen (Soziale_Arbeit, Bias, Gender, Diversität, Fairness). Die Entscheidungslogik erfordert BEIDE Technik UND Sozial. Das Paper ist rein bildungstechnisch ohne soziale Komponente.",0,0,
17,How to Create Inclusive AI Images: A Guide to Bias-Free Prompting,Articulate (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,"Fehlende SOZIAL-Komponente: Das Paper behandelt AI Literacy (TECHNIK erfüllt), adressiert aber keine der erforderlichen sozialen Dimensionen (Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness). Es ist ein allgemeines Education/Assessment-Paper ohne Bezug zu Sozialer Arbeit, Ungleichheit oder Fairness.",,Empirisch,0.85,"Der Titel deutet auf ein Instrument zur Messung von AI Literacy hin – ein TECHNIK-Thema. Jedoch fehlt jede Evidenz für soziale Dimensionen wie Anwendung in der Sozialen Arbeit oder Fokus auf Bias/Fairness. Das Paper erfüllt nur eine Bedingung der strikten Entscheidungslogik (TECHNIK_OK, aber nicht SOZIAL_OK). Daher: Exclude.",0,0,
18,Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,Asseri (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,"TECHNIK_OK (AI_Literacies + KI_Sonstige = Ja), aber SOZIAL nicht erfüllt. Paper behandelt AI Literacy im Journalismus, nicht in Sozialer Arbeit. Kein substantieller Bezug zu Bias, Fairness, Gender, Diversität oder feministischen Ansätzen erkennbar. Soziale Arbeit ist nicht Zielgruppe oder Anwendungsbereich.",,Unclear,0.75,"Das Paper adressiert AI Literacy (TECHNIK erfüllt), aber der Fokus liegt auf Journalismus, nicht auf Sozialer Arbeit oder damit verbundenen sozialen Dimensionen wie Bias, Fairness oder Diversität. Keine der SOZIAL-Kategorien ist erfüllt. Ausschluss erforderlich.",0,0,
19,Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,Asseri (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,Exclude,Exclude,Ja,Duplicate,"Nur Technik-Kategorie erfüllt (AI_Literacies), aber keine Sozial-Kategorie",,Empirisch,0.85,"Das Paper entwickelt ein KI-Literacy-Framework und evaluiert einen Kurs für Sekundarschüler – klarer Fokus auf AI Literacies als Kompetenzentwicklung. Jedoch kein Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität, Feminismus oder Fairness erkennbar. Erfüllt nur TECHNIK-Bedingung, nicht SOZIAL-Bedingung.",0,0,
20,Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,Asseri (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,Exclude,Exclude,Ja,Duplicate,No_full_text - Nur Titel und Metadaten verfügbar; SOZIAL-Kriterium nicht erfüllt,,Konzept,0.85,"Titel deutet auf AI Literacy in Bildung hin (TECHNIK-Kategorie erfüllt). Ohne Abstract/Volltext ist keine reliable Bewertung möglich. Kritisch: Kein erkennbarer Bezug zu Sozialer Arbeit, Bias, Gender, Diversität, Fairness oder feministischen Perspektiven. Das Paper adressiert offenbar allgemeine Bildung, nicht die erforderliche soziale Dimension. SOZIAL-Kriterium nicht erfüllt → Exclude.",0,0,
21,Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,Asseri (2025),Ja,Ja,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,Exclude,Exclude,Ja,Duplicate,"Nur TECHNIK erfüllt (AI_Literacies + Generative_KI), aber kein SOZIAL-Kriterium (Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness). Beide Bedingungen sind notwendig.",,Konzept,0.85,"Das Paper behandelt ein Assessment-Tool für generative KI-Kompetenzen und liegt damit im Bereich AI Literacies. Es fehlt jedoch jeder erkennbare Bezug zu sozialen Dimensionen (Soziale Arbeit, Bias, Gender, Diversität, Fairness). Das Paper adressiert rein bildungstechnische Aspekte von KI-Kompetenzermittlung ohne sozialwissenschaftliche oder kritische Perspektive.",0,0,
22,AI Countergovernance: Lessons Learned from Canada and Paris,Attard-Frost (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,"Not_relevant_topic - KI-Literacies in Lehrerausbildung, nicht Soziale Arbeit",,Literaturreview,0.92,"Das Paper behandelt AI Literacy explizit, aber ausschließlich im Kontext von Lehrerausbildung (Teacher Education). Es fehlt jeder Bezug zu Sozialer Arbeit, Bias, Fairness, Gender oder Diversität. Die SOZIAL-Kategorie ist nicht erfüllt: Weder Soziale_Arbeit noch eine der Ungleichheits-/Fairness-Kategorien sind relevant. Daher: Exclude nach Konsistenzregel.",0,0,
23,Explicitly unbiased large language models still form biased associations,Bai (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,"Technik-Komponente erfüllt (AI_Literacies), aber keine Sozial-Komponente. Fehlt: direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder Fairness. Das Paper behandelt AI Literacy als generische Kompetenz ohne spezifischen Sozialarbeitsbezug.",,Konzept,0.85,"Das Paper adressiert AI Literacy als Messung von KI-Kompetenzen (TECHNIK_OK). Es fehlt jedoch jede SOZIAL-Komponente: kein Bezug zu Sozialer Arbeit, keinen spezifischen Fokus auf Bias, Gender, Diversität oder Fairness. Ohne Sozialarbeitsbezug oder sozialwissenschaftliche Kritik wird es ausgeschlossen.",0,0,
24,Artificial intelligence in social work: An EPIC model for practice,Baker (2025),Ja,Ja,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,No_full_text,,Unclear,0.3,"Basierend auf dem Titel behandelt das Paper generative KI und menschliches Lernen, was auf AI_Literacies und Generative_KI hindeutet. Allerdings fehlt das Abstract und der Volltext für eine verlässliche Bewertung. Ohne substantielle Informationen kann nicht überprüft werden, ob ein echter Sozialarbeits- oder Bias-/Fairness-Bezug vorhanden ist. Die restriktive Klassifizierungsweisung verbietet die Annahme von Sozialbezug nur aufgrund von 'learning'. Exclude.",0,0,
25,Beyond transparency and explainability: On the need for adequate and contextualized user guidelines for LLM use,Barman (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,"Nur Technik-Kategorie erfüllt (AI_Literacies), keine Sozial-Kategorie vorhanden. Keine Bedingung 2 erfüllt.",,Literaturreview,0.85,"Das Paper ist explizit ein exploratory review zu AI Literacy und behandelt somit substantiell KI-Kompetenzen und Wissenstransfer. Es erfüllt die TECHNIK-Bedingung. Jedoch gibt es keinen erkennbaren Bezug zu sozialen Dimensionen wie Soziale Arbeit, Bias/Ungleichheit, Gender, Diversität, Feminismus oder Fairness. Ohne Abstract schwer zu beurteilen, aber Titel deutet auf rein konzeptionelle/pädagogische Behandlung hin. Daher EXCLUDE nach strenger Logik.",0,0,
26,Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review,Basseri (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Include,,,,,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK durch KI_Sonstige (algorithmische Entscheidungssysteme im Sozialbereich). SOZIAL_OK durch Soziale_Arbeit (direkte Anwendung auf Wohlfahrtsdienste), Bias_Ungleichheit (Diskriminierung marginalisierter Gruppen), Diversitaet (vulnerable Populationen, marginalisierte Gruppen) und Fairness (Fairness/Equity als zentrale Dimension des Frameworks). Theoretischer Artikel mit substantiellem Fokus auf algorithmische Gerechtigkeit im Sozialbereich.",0,0,
27,Keynote Summary: The New Jim Code: Reimagining the Default Settings of Technology & Society,Benjamin (2023),Ja,Ja,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Konzept,0.92,"Paper adressiert substantiell AI_Literacies (kritische Reflexion über KI-Nutzung), Generative_KI (LLMs im Fokus) und Prompting (explizit prompt engineering erwähnt). Direkter Bezug zur Soziale_Arbeit durch Zielgruppe (social work researchers) und Empfehlungen für Praxis. TECHNIK_OK (3/4 Kategorien erfüllt) UND SOZIAL_OK (Soziale_Arbeit erfüllt). Einschluss gerechtfertigt.",0,0,
28,The AI literacy development canvas: Assessing and building AI literacy in organizations,Benlian (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Theoretisch,0.95,"Paper adressiert substantiell AI_Literacies durch kritische Diskussion von Kompetenzen und Wissen im Umgang mit KI in der Ausbildung (critical digital literacies). Direkter Bezug zu Soziale_Arbeit durch Fokus auf Social Work Education und berufliche Vorbereitung. Bias_Ungleichheit ist zentral: kritische Auseinandersetzung mit ungleichen Machtverteilungen, die technologischer Wandel verstärkt. TECHNIK (AI_Literacies=Ja) + SOZIAL (Soziale_Arbeit=Ja + Bias_Ungleichheit=Ja) erfüllt → Include.",0,0,
29,"Less knowledge, more trust? Exploring potentially uncritical attitudes towards AI in higher education",Biagini (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Empirisch,0.85,"Paper behandelt ML-basierte Entscheidungssysteme (Gradient Boosting) in der Jugendhilfe (KI_Sonstige=Ja). Direkter Bezug zu sozialarbeiterischer Praxis durch Vorhersage von Platzierungserfolgen im Child Welfare System (Soziale_Arbeit=Ja). TECHNIK und SOZIAL erfüllt → Include. Keine explizite Behandlung von Bias, Fairness oder Gender-Aspekten trotz hoher sozialer Relevanz algorithmischer Entscheidungen in diesem Kontext.",0,0,
30,"Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden für KI in der Verwaltung, Version 1.0",Biegelbauer (2023),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Konzept,0.92,"Das Paper etabliert Standards für Technologienutzung in der Sozialarbeit, direkt relevant für KI-Anwendungen. AI_Literacies: Ja (Kompetenzanforderungen für Sozialarbeiter in Technologienutzung). KI_Sonstige: Ja (algorithmische Systeme im Sozialbereich adressiert durch Ethics-Framework). Soziale_Arbeit: Ja (Landmark-Standards von vier Major-Verbänden für die Profession). TECHNIK + SOZIAL erfüllt → Include.",0,0,
31,Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development,Birru (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Empirisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK (KI_Sonstige: algorithmisches Entscheidungssystem in Jugendhilfe, Fairness: explizit Racial Bias und Fairness-Aspekte) und SOZIAL (Soziale_Arbeit: direkter Bezug zu Child Welfare Practice, Bias_Ungleichheit: Analyse rassistischer Diskriminierung, Fairness: Algorithmic Fairness in sozialen Systemen). Substantielle Mixed-Methods-Studie mit hohem Relevanzpotential für Sozialarbeit und algorithmische Gerechtigkeit.",0,0,
32,A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization,Bisconti (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,Not_relevant_topic,,Empirisch,0.95,"Das Paper behandelt Datafizierung und Digitalisierung im Gesundheitswesen, nicht aber KI/ML-Systeme spezifisch. Es fehlt jeglicher Bezug zu KI-Technologien (klassisches ML, generative KI, Prompting, AI Literacies). Ohne TECHNIK-Komponente kann die inklusive Entscheidungslogik nicht erfüllt werden. Kritik von Algorithmen/Datenregimen reicht nicht ohne KI-Technologiebezug.",0,0,
33,A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization,Bisconti (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,Exclude,Exclude,Ja,Duplicate,"TECHNIK_OK (KI_Sonstige=Ja), aber SOZIAL_OK nicht erfuellt. Bias_Ungleichheit und Fairness sind vorhanden, aber Soziale_Arbeit=Nein. Das Paper behandelt Healthcare-KI (allgemein medizinisch), nicht Soziale Arbeit oder deren Zielgruppen. 'Direkter Bezug zu sozialarbeiterischer Praxis' ist nicht vorhanden, nur allgemeine ethische Konzepte im Healthcare-Kontext.",,Literaturreview,0.85,"Paper ist Ethik-Review zu KI im Healthcare mit substantiellem Fokus auf algorithmischen Bias und Fairness. Erfüllt TECHNIK-Bedingung (KI_Sonstige). Aber: Kein direkter Bezug zu Sozialer Arbeit oder deren Zielgruppen erkennbar; Bias/Fairness ohne sozialarbeiterischen Kontext. Orientierung an Healthcare, nicht Soziale Arbeit.",0,0,
34,A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization,Bisconti (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,Exclude,Include,Nein,Duplicate,,,Empirisch,0.95,"Paper erfüllt beide Kriterien: (1) TECHNIK: KI_Sonstige=Ja (NLP für Risikobewertung, algorithmische Systeme im Sozialbereich); (2) SOZIAL: Soziale_Arbeit=Ja (Child Protective Services), Bias_Ungleichheit=Ja (racial bias, automated discrimination), Diversitaet=Ja (Black/Latinx families), Fairness=Ja (bias in risk scoring). Empirische Studie mit substantiellem Fokus auf diskriminatorische Auswirkungen von ML-Systemen in der Jugendhilfe.",0,0,
35,Artificial Intelligence in Social Work: An EPIC Model for Practice,Boetto (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,Not_relevant_topic,,Theoretisch,0.85,"Paper behandelt algorithimische Systeme (KI_Sonstige: Ja) und thematisiert Machtstrukturen sowie Ungleichheit am Arbeitsplatz (Bias_Ungleichheit: Ja). ABER: Kein direkter Bezug zu Sozialer Arbeit, deren Praxis, Theorie oder Zielgruppen. Der Fokus liegt auf allgemeinen Arbeitskontexten/Organisationen, nicht auf sozialarbeiterischen Settings oder Zielgruppen (Kinder, Jugendliche, vulnerable Gruppen in Unterstützungssystemen). Arbeitssoziologie ≠ Soziale Arbeit. Soziale_Arbeit bleibt Nein → SOZIAL-Bedingung nicht erfüllt → Exclude.",0,0,
36,Generative AI & social work practice guidance,British Association of Social Workers (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Theoretisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmische Entscheidungssysteme im Wohlfahrtssektor), SOZIAL_OK (Soziale_Arbeit: Wohlfahrtsadministration; Bias_Ungleichheit: Bürgerdisempowerment durch Überwachung; Fairness: Kritik an automatisierten Entscheidungen). Theoretische kritische Analyse von Surveillance Capitalism in der dänischen Wohlfahrtsverwaltung mit Fokus auf Bürgerrechte und Fairness-Aspekte.",0,0,
37,"Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligent Machines",Browne (2023),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Include,,,,,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: predictive algorithms in child protection), SOZIAL_OK (Soziale_Arbeit: child protection/welfare practice, Bias_Ungleichheit: racial und socioeconomic disparities, Diversitaet: marginalisierte Gruppen, Fairness: statistical fairness analysis). Kritische Analyse algorithmischer Systeme im Sozialbereich mit direktem Praxisbezug.",0,0,
38,Engineers on responsibility: feminist approaches to who's responsible for ethical AI,Browne (2024),Ja,Ja,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Konzept,0.95,"Paper erfüllt beide Inklusionskriterien: TECHNIK_OK (Generative_KI: ChatGPT/LLMs, AI_Literacies: kritische Evaluationskompetenz für Forscher), SOZIAL_OK (Soziale_Arbeit: direkter Bezug zu sozialer Arbeit, Bias_Ungleichheit: algorithmischer Bias). Invited paper mit Fokus auf ethische Reflexion und praktische Empfehlungen für KI-Nutzung in Sozialarbeitsforschung.",0,0,
39,Tech workers' perspectives on ethical issues in AI development: Foregrounding feminist approaches,Browne (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Konzept,0.95,Paper adressiert algorithmische Literacität (AI_Literacies) und algorithmic decision-making systems (KI_Sonstige) im Kontext der Sozialarbeiter:innen-Ausbildung (Soziale_Arbeit). Kritische Auseinandersetzung mit Machtverteilungen und strukturellen Ungleichheiten (Bias_Ungleichheit). Erfüllt beide Bedingungen: TECHNIK (AI_Literacies + KI_Sonstige) UND SOZIAL (Soziale_Arbeit + Bias_Ungleichheit).,0,0,
40,AI literacy in K-12: a systematic literature review,Casal-Otero (2023),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Include,,,,,Theoretisch,0.95,"Paper adressiert KI-Kompetenzen für Sozialarbeiter:innen (AI_Literacies), algorithmische Systeme in der Sozialarbeit wie Risikobewertung (KI_Sonstige), direkte sozialarbeiterische Praxis und Ethik (Soziale_Arbeit), algorithmischen Bias und Fairness bei Algorithmen (Bias_Ungleichheit, Fairness) sowie diverse Stakeholder-Perspektiven durch Focus Groups (Diversitaet). Erfüllt beide Bedingungen: TECHNIK (AI_Literacies + KI_Sonstige) und SOZIAL (Soziale_Arbeit + Bias_Ungleichheit + Fairness + Diversitaet).",0,0,
41,Flexible intersectional stereotype extraction (FISE): Analyzing intersectional biases in large language models,Charlesworth (2024),Ja,Ja,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,No_relevant_topic,,Empirisch,0.92,"Paper behandelt Generative KI (Chatbot mit Sprachverarbeitung), erfüllt aber nicht die Sozial-Kriterien. Mental-Health-Anwendung für Universitätsstudenten ist kein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder Fairness. Psychologische Intervention ohne sozialarbeiterischen Kontext. Reine klinische Evaluierung einer KI-Anwendung.",0,0,
42,A Competency Framework for AI Literacy: Variations by Different Learner Groups and an Implied Learning Pathway,Chee (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Literaturreview,0.92,"Paper behandelt ethische Herausforderungen von Technologie in der Sozialen Arbeit substantiell. KI_Sonstige erfüllt (automatisierte Systeme, digitale Technologien), Soziale_Arbeit erfüllt (direkte Praxisanalyse), Bias_Ungleichheit erfüllt (digital divide, soziale Gerechtigkeit, Selbstbestimmung). Systematischer Review mit klarem Sozialarbeitsbezug und Fokus auf algorithmische Systeme und deren Auswirkungen.",0,0,
43,Exploring complex mental health symptoms via classifying social media data with explainable LLMs,Chen (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Theoretisch,0.92,"Paper erfüllt beide Kriterien: TECHNIK_OK durch KI_Sonstige (algorithmische Systeme in öffentlicher Verwaltung: Fraud Detection, Chatbots). SOZIAL_OK durch Soziale_Arbeit (Wohlfahrtssysteme, marginalisierte Populationen) und Bias_Ungleichheit (Analyse von Machtsymmetrien, Surveillance von benachteiligten Gruppen). Kritische Betrachtung algorithmischer Kontrollsysteme im Sozialstaat.",0,0,
44,Social work and artificial intelligence: Collaboration and challenges,Chen (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Include,,,,,Empirisch,0.95,"Report untersucht substantiell algorithmische Entscheidungssysteme (KI_Sonstige: Ja) im Wohlfahrtsbereich, direkt relevant für Soziale Arbeit (Ja). Zentral sind Bias/Diskriminierung gegen vulnerable Gruppen (Bias_Ungleichheit: Ja), Diversität marginalisierter Bevölkerungen (Diversitaet: Ja) und Fairness-Probleme automatisierter Systeme (Fairness: Ja). TECHNIK-Kriterium erfüllt (KI_Sonstige). SOZIAL-Kriterium erfüllt (vier Ja). Empirischer Forschungsbericht.",0,0,
45,Exploring machine learning to support decision-making for placement stabilization and preservation in child welfare,Cher (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK-Ja (KI_Sonstige: algorithmische Entscheidungssysteme in der Jugendhilfe), SOZIAL-Ja (Soziale_Arbeit: direkter Fokus auf Auswirkungen von Algorithmen auf sozialarbeiterische Praxis und Professionalität; Bias_Ungleichheit und Fairness: kritische Analyse von Machtstrukturen und Neutralitätsansprüchen). Kritische Analyse struktureller Effekte von Algorithmen auf professionelle Autonomie und Care-Beziehungen.",0,0,
46,Prompting fairness: Learning prompts for debiasing large language models,Chisca (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Include,,,,,Theoretisch,0.92,"Paper behandelt algorithmische Systeme im Sozialbereich (digital welfare, automated decision-making) substantiell → KI_Sonstige: Ja. Direkter Bezug zu Sozialarbeit und Welfare-Praxis → Soziale_Arbeit: Ja. Kritische Analyse von algorithmischer Diskriminierung marginalisierter Populationen → Bias_Ungleichheit: Ja. Fokus auf strukturelle Ungleichheiten und marginalisierte Gruppen → Diversitaet: Ja. Fairness-Dimension in automatisierten Entscheidungssystemen → Fairness: Ja. TECHNIK (KI_Sonstige) + SOZIAL (5 Kategorien) erfüllt → Include.",0,0,
47,Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,Chisca (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Empirisch,0.92,"Paper entwickelt ML-Modelle (Random Forest, logistische Regression) für Vorhersage von Platzierungsabbruch in Jugendhilfe. Erfüllt TECHNIK-Kriterium durch KI_Sonstige (klassisches ML für algorithmische Entscheidungssysteme im Sozialbereich). Erfüllt SOZIAL-Kriterium durch: (1) direkter Bezug zu Sozialarbeitspraxis/Jugendhilfe (Soziale_Arbeit=Ja), (2) expliziter Fairness-Fokus (Fairness=Ja). Statewide child welfare study mit Fokus auf Unterstützung von Caseworker-Entscheidungen.",0,0,
48,What are artificial intelligence literacy and competency? A comprehensive framework to support them,Chiu (2024),Ja,Ja,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,Not_relevant_topic,,Empirisch,0.95,"Das Paper untersucht LLM-basierte Psychotherapie in klinischem Kontext. Während es Generative KI behandelt (RCT mit Chatbot), fehlt der Bezug zur Sozialen Arbeit vollständig. Die Anwendung ist klinische Psychologie/Psychiatrie, nicht Soziale Arbeit. Keine Thematisierung von Bias, Fairness, Gender, Diversität oder feministischen Perspektiven. TECHNIK erfüllt (Generative_KI=Ja), aber SOZIAL nicht erfüllt (alle Kategorien=Nein). Daher Exclude.",0,0,
49,"AI literacy and competency: definitions, frameworks, development and future research directions",Chiu (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Empirisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK: KI_Sonstige (Machine Learning, Gradient Boosting für Prediction) + AI_Literacies (Training von Fachkräften, eXplainable AI für Transparenz). SOZIAL: Soziale_Arbeit (Jugendhilfe/Child Guidance Centers, direkte sozialarbeiterische Praxis und Zielgruppen). Implementierungsstudie mit substantiellem Fokus auf Kompetenzentwicklung und praktische Anwendung algorithmischer Systeme im Sozialbereich.",0,0,
50,Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals,Choudhury (2024),Ja,Ja,,Nein,,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Empirisch,0.92,"Paper untersucht Nutzung von generativen KI-Chatbots (Pi, ChatGPT) für mentale Gesundheitsunterstützung. Generative_KI=Ja (LLM-basierte Chatbots im Fokus). Soziale_Arbeit=Ja (mentale Gesundheit, therapeutische Kontexte, Unterstützung von vulnerable Personen - Kernbereich Sozialer Arbeit). Beide TECHNIK- und SOZIAL-Kriterien erfüllt → Include.",0,0,
51,Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals,Choudhury (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,Exclude,Include,Nein,Duplicate,,,Konzept,0.85,"Paper behandelt KI-Kompetenzen und -Integration (AI_Literacies) sowie allgemeine KI-Governance (KI_Sonstige) im Kontext sozialer Innovationen und Dienstleistungen (Soziale_Arbeit). PRISM-Framework adressiert direkt Organisationen in sozialen Sektoren mit Readiness-Assessments. TECHNIK_OK (AI_Literacies + KI_Sonstige) und SOZIAL_OK (Soziale_Arbeit) erfüllt. Framework-Dokument ohne explizite Bias-, Gender-, oder Fairness-Fokussierung.",0,0,
52,"Intersectional Artificial Intelligence Is Essential: Polyvocal, Multimodal, Experimental Methods to Save AI",Ciston (2024),Ja,Ja,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Konzept,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + AI_Literacies durch Praxis-Guidance) und SOZIAL (Soziale_Arbeit + Bias_Ungleichheit durch Warnung vor racist/sexist assumptions). Es ist eine direkte Praxis-Orientierung für Sozialarbeiter:innen mit ethischen Reflexionspunkten und adressiert substantiell generative KI-Risks (Bias, Halluzinationen, Datenschutz).",0,0,
53,PreciseDebias: An automatic prompt engineering approach for generative AI to mitigate image demographic biases,Clemmer (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Include,,,,,Empirisch,0.95,"Paper erfüllt beide Einschlusskriterien: (1) TECHNIK: KI_Sonstige=Ja (algorithmisches Entscheidungssystem in der Jugendhilfe). (2) SOZIAL: Soziale_Arbeit=Ja (direkter Bezug zu Child Protection/Jugendhilfe), Bias_Ungleichheit=Ja (Analyse von Diskriminierung gegenüber Immigrantenfamilien), Diversitaet=Ja (Repräsentationsfragen marginalisierter Communities), Fairness=Ja (Fehlerpräzision und Fairness-Metriken). Substantielle empirische Analyse mit dokumentierten Systemharms.",0,0,
54,The influence of mental state attributions on trust in large language models,Colombatto (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Empirisch,0.95,"Paper adressiert Allegheny Family Screening Tool (algorithmisches Entscheidungssystem in der Jugendhilfe). TECHNIK erfüllt: KI_Sonstige (algorithmische Systeme), AI_Literacies (worker training, tool understanding). SOZIAL erfüllt: Soziale_Arbeit (child welfare practice), Bias_Ungleichheit (discrimination risks), Fairness (misalignments, limitations). Empirische Studie mit direktem Sozialarbeitsbezug und kritischer Analyse von Systemrisiken.",0,0,
55,The influence of mental state attributions on trust in large language models,Colombatto (2025),Ja,Ja,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,Exclude,Include,Nein,Duplicate,,,Experimentell,0.85,"Paper erfüllt TECHNIK- und SOZIAL-Kriterien: (1) Generative_KI (LLM-basierter AI Coach), Prompting (strukturiertes Prompting für reflektive Praxis), AI_Literacies (Entwicklung von Self-Reflection-Kompetenzen). (2) Soziale_Arbeit (öffentliche Verwaltung, human services context, Kompetenzentwicklung). Empirische User Study mit klarem Fokus auf KI-gestützte Professionalisierung im sozialen Sektor.",0,0,
56,Clinical Social Workers’ Perceptions of Large Language Models in Practice: Resistance to Automation and Prospects for Integration,Creswell Báez (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Include,,,,,Literaturreview,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK durch KI_Sonstige (predictive/prescriptive algorithms); SOZIAL_OK durch Soziale_Arbeit (child welfare, social workers), Bias_Ungleichheit (discrimination against low-income families and communities of color), Diversitaet (marginalisierte Communities), Fairness (algorithmic fairness). Systematische Literaturreview mit substantiellem Fokus auf algorithmische Systeme im Sozialbereich.",0,0,
57,Queer in AI: A case study in community-led participatory AI,Cvoelcker (2023),Ja,Ja,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Theoretisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK-OK (Generative_KI: ChatGPT; Prompting: explizit 'thoughtful prompting practices'; AI_Literacies: kritische Reflexion und kompetenter Umgang), SOZIAL-OK (Soziale_Arbeit: direkter Fokus auf Profession; Fairness: 'just technology use', 'fair use'). Editorial mit kritischer Perspektive auf KI-Integration in Sozialarbeit.",0,0,
58,Data Feminism for AI,D'Ignazio (2024),Ja,Ja,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Empirisch,0.87,"Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (Kompetenzentwicklung für Prompt Engineers), Generative_KI (LLM-Fokus), Prompting (Kernthema: Prompt Recommendations). SOZIAL: Soziale_Arbeit (explizite Relevanz für social services), Bias_Ungleichheit (harmful content removal), Fairness (Responsible AI framework). Empirische Studie mit Interviews und User Studies. Substantieller Fokus auf Prompting-Strategien und Responsible AI-Praktiken.",0,0,
59,Measuring and identifying factors of individuals' trust in large language models,De Duro (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies + KI_Sonstige für algorithmische Systeme); SOZIAL_OK (Soziale_Arbeit + Bias_Ungleichheit + Fairness). Zentral ist die Integration von KI-Kompetenzen in CSWE-Kernkompetenzen für Sozialarbeiter:innen mit explizitem Fokus auf algorithmischen Bias, Ungleichheit und ethische Governance in der Praxis.",0,0,
60,Measuring and identifying factors of individuals' trust in large language models,De Duro (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,Exclude,Include,Nein,Duplicate,,,Literaturreview,0.95,Systematic Review mit substanziellem Fokus auf AI Literacy und Computational Thinking in der Sozialen Arbeit (AI_Literacies: Ja). Direkter Bezug zu sozialarbeiterischer Praxis und Profession via Staub-Bernasconi's Triple Mandate (Soziale_Arbeit: Ja). Thematisiert auch algorithmische Systeme/KI-Systeme generell (KI_Sonstige: Ja). TECHNIK und SOZIAL beide erfüllt → Include.,0,0,
61,Can LLMs reason about trust? A pilot study,Debnath (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Include,,,,,Literaturreview,0.95,"Paper erfüllt beide Bedingungen: TECHNIK-OK durch AI_Literacies (community-based AI digital literacy), KI_Sonstige (AI in social work), SOZIAL-OK durch Soziale_Arbeit (direkter Fokus auf sozialarbeiterische Praxis), Bias_Ungleichheit (ethische Gerechtigkeit), Diversitaet (vulnerable populations, technology access) und Fairness (ethics and justice component). EPIC-Modell adressiert systematisch KI-Integration in Sozialarbeit mit Schwerpunkt auf ethische und gerechte Outcomes.",0,0,
62,Can LLMs reason about trust? A pilot study,Debnath (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,Exclude,Exclude,Ja,Duplicate,Not_relevant_topic,,Theoretisch,0.85,"Das Paper behandelt Digitalisierung in Familien und Sharenting, nicht aber KI-Systeme. Es adressiert zwar sozialarbeiterische Praxisbereiche (Familie, Kindeswohl), erfüllt aber die TECHNIK-Bedingung nicht: Weder AI_Literacies (kein fokussierter KI-Kompetenzrahmen), noch Generative_KI, Prompting oder KI_Sonstige sind substantiell behandelt. Digitale Technologie allgemein ≠ KI. Daher: Exclude.",0,0,
63,Can LLMs reason about trust? A pilot study,Debnath (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,Exclude,Exclude,Ja,Duplicate,Not_relevant_topic,,Empirisch,0.85,"Paper behandelt digitale Medien, Bildungszugang und junge Geflüchtete. Erfüllt SOZIAL-Kriterien (Soziale_Arbeit, Bias_Ungleichheit, Diversität). ABER: Kein Fokus auf KI/ML-Systeme. Digitalisierung ≠ KI. Keine Kategorie aus TECHNIK-Bereich (AI_Literacies, Generative_KI, Prompting, KI_Sonstige) ist substantiell erfüllt. Anthropologisch-ethnografisches Paper zu Digitalisierung ohne KI-Bezug.",0,0,
64,Automated government benefits and welfare surveillance,Dencik (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Theoretisch,0.92,"Paper erfüllt beide Inklusionskriterien: (1) TECHNIK-Kriterium: KI_Sonstige=Ja (Algorithmen, datengetriebene Systeme, digitale Plattformen sind zentral). (2) SOZIAL-Kriterium: Soziale_Arbeit=Ja (direkter Fokus auf sozialarbeiterische Praxis und deren Transformation) UND Bias_Ungleichheit=Ja (Analyse von Machtstrukturen, Überwachung und sozialen Ungleichheiten). Kutschers Fokus auf Digitalisierung als Machtfrage und soziale Gerechtigkeit ist substantiell für beide Dimensionen.",0,0,
65,Feminist reflections for the development of Artificial Intelligence,Derechos Digitales (2023),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Empirisch,0.95,Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Entscheidungsunterstützungssysteme/algorithmische Systeme) und SOZIAL_OK (Soziale_Arbeit: expliziter Fokus auf Sozialarbeiter und Behindertenservices; Bias_Ungleichheit: Reduktion subjektiver Vorurteile; Fairness: Transparenz in Ressourcenallokation). Empirische Untersuchung mit direktem Bezug zu sozialarbeiterischer Praxis und algorithmischen Systemen im Sozialbereich.,0,0,
66,"Imagination, Algorithms and News: Developing AI Literacy for Journalism",Deuze (2022),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Exclude,,,Wrong_publication_type / Not_relevant_topic,,Empirisch,0.85,"Paper behandelt AI-CDSS im Healthcare-Kontext (KI_Sonstige: Ja), adressiert algorithmische Fairness und Bias-Aspekte (Bias_Ungleichheit, Fairness: Ja). JEDOCH: Kein direkter Bezug zu Sozialer Arbeit. Healthcare-Setting mit Patient:innen-Perspektiven ist nicht Sozialarbeit (keine Zielgruppen-Überschneidung, keine sozialarbeiterische Praxis/Theorie). Paper erfüllt Technik-Bedingung, aber nicht die erforderliche Sozial-Bedingung für die Forschungsfrage des Literature Review zur KI in Sozialer Arbeit.",0,0,
67,AI literacy in teacher education: Empowering educators through critical co-discovery,Dilek (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Literaturreview,0.85,"Paper behandelt algorithmische Entscheidungssysteme (KI_Sonstige: Ja) in sensiblen Bereichen (Healthcare, Justiz, Grenzschutz). Substantielle Behandlung von Diskriminierungsrisiken in Trainingsdaten (Bias_Ungleichheit: Ja) und Fairness-Aspekten durch Analyse von Opazität und Accountability (Fairness: Ja). Erfüllt beide Bedingungen (TECHNIK + SOZIAL). Kein Soziale_Arbeit-Bezug, daher nicht umfassend in diesem Kontext, aber relevant für AI-Governance und gesellschaftliche Auswirkungen.",0,0,
68,Measuring and mitigating unintended bias in text data,Dixon (2018),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Theoretisch,0.85,Paper behandelt Algorithmische Entscheidungssysteme (KI_Sonstige: Ja) im Kontext Sozialer Arbeit (Soziale_Arbeit: Ja). Expliziter Fokus auf Klassifikationssysteme und deren Limitations bezüglich Bias und Kontextverlust (Bias_Ungleichheit: Ja). TECHNIK und SOZIAL Bedingungen erfüllt → Include.,0,0,
69,Reflexive prompt engineering: A framework for responsible prompt engineering and AI interaction design,Djeffal (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,Not_relevant_topic,,Empirisch,0.85,"Das Paper behandelt Datenmanagement und -pflege in Organisationen (Sozialarbeit, Bildung), nicht aber KI/ML-Systeme. Es fehlt jede technische KI-Komponente (keine AI_Literacies, Generative_KI, Prompting oder KI_Sonstige). Das Thema ist zu weit entfernt von KI-Anwendungen. Nur eine SOZIAL-Kategorie erfüllt, TECHNIK-Bedingung nicht erfüllt → Exclude.",0,0,
70,Revisiting Technical Bias Mitigation Strategies,Djiberou Mahamadou (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Theoretisch,0.85,"Paper adressiert algorithmische Governance und digitale Systeme in Wohlfahrtsstaat (KI_Sonstige: Ja). Direkter Bezug zu Sozialarbeit durch Analyse von Welfare Services und deren Digitalisierung (Soziale_Arbeit: Ja). Thematisiert Spannungen zwischen Care und Control, was Fragen zu Überwachung und algorithmischer Kontrolle aufwirft (Bias_Ungleichheit: Ja, da Machtasymmetrien und potenzielle Diskriminierung in digitalen Wohlfahrtssystemen adressiert werden). Beide Bedingungen erfüllt: TECHNIK_OK + SOZIAL_OK → Include.",0,0,
71,Voll (dia)logisch? Ein Werkstattbericht über den Einsatz von generativer KI in der Hochschulbildung für Soziale Arbeit – Curriculare Überlegungen und veränderte Akteurskonstellationen,Engelhardt (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Include,,,,,Konzept,0.82,"Paper behandelt Datengetriebene Systeme (KI_Sonstige) und deren Auswirkungen auf vulnerable Bevölkerungsgruppen (ältere Erwachsene). Adressiert Bias durch ageist assumptions (Bias_Ungleichheit), partizipative Inklusion marginalisierter Gruppen (Diversitaet). Verwendet implizit-feministische Perspektive durch Kritik von Machtstrukturen und Dominanz mächtiger Akteure (Feministisch). Fokus auf demokratisierte Futures-Making und alternative Narrativen reflektiert Fairness-Perspektive. TECHNIK_OK (KI_Sonstige) + SOZIAL_OK (Bias, Diversitaet, Feministisch, Fairness) → Include.",0,0,
72,DigComp 2.1: the digital competence framework for citizens with eight proficiency levels and examples of use.,European Commission. Joint Research Centre. (2017),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (KI-Kompetenzen in Bildung/Organisationen), KI_Sonstige (technische Grundlagen, Automatisierungsbias). SOZIAL: Soziale_Arbeit (Direktbezug zu Praxis, Theorie und Anwendungsfeldern), Bias_Ungleichheit (Diskriminierung in Trainingsdaten), Fairness (ethische Rahmenbedingungen). Systematische Behandlung von KI in der Sozialen Arbeit mit explizitem Fokus auf verantwortungsvolle Anwendung.",0,0,
73,Explainable Artificial Intelligence,European Data Protection Supervisor (2023),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Theoretisch,0.92,"Paper behandelt substantiell AI_Literacies (kritische Reflexion über KI-Kompetenzen von Fachkräften, Automation Bias), KI_Sonstige (Algorithmen, Big Data Analytics), Soziale_Arbeit (direkter Bezug zur sozialen Arbeit und professionellen Entscheidungsfindung) und Bias_Ungleichheit (Automation Bias). Beide Voraussetzungen erfüllt: TECHNIK_OK (KI_Sonstige + AI_Literacies) und SOZIAL_OK (Soziale_Arbeit + Bias_Ungleichheit).",0,0,
74,KI-basiertes Assistenzsystem im Kinderschutzverfahren,Feist-Ortmanns (2025),Ja,Ja,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Literaturreview,0.92,"Das Paper behandelt KI-Anwendungen in der Sozialen Arbeit (Chatbots, Predictive Risk Modeling) und analysiert algorithmischen Bias sowie ethische Fragen. Es erfüllt beide Anforderungen: TECHNIK (Generative_KI für Chatbots, KI_Sonstige für PRM und neuronale Netze) und SOZIAL (Soziale_Arbeit als Fokus, Bias_Ungleichheit als substantielles Thema). Keine feministischen Theorien explizit erwähnt, daher Feministisch=Nein.",0,0,
75,Female perspectives on algorithmic bias: Implications for AI researchers and practitioners,Fraile-Rojas (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,Not_relevant_topic,,Theoretisch,0.95,"Das Paper behandelt Digitalisierung in der Sozialen Arbeit (erfüllt Soziale_Arbeit = Ja), thematisiert aber keine KI-Systeme, ML-Algorithmen oder andere technische KI-Aspekte (alle Technik-Kategorien = Nein). Der Fokus liegt auf Rationalisierungslogik und Interaktionssoziologie. Ohne TECHNIK-Komponente ist das Paper nicht relevant für ein KI-Literature Review, auch wenn es sozialarbeiterische Theorie berührt.",0,0,
76,Prompten nach Plan: Das PCRR-Framework als pädagogisches Werkzeug für den Einsatz von Künstlicher Intelligenz.,Freinhofer (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Theoretisch,0.92,"Paper erfüllt beide Kriterien: (1) TECHNIK_OK: Behandelt AI_Literacies (kritische Reflexion von NLP-Technologien in sozialer Arbeit) und KI_Sonstige (NLP als spezifisches ML-System). (2) SOZIAL_OK: Direkter Fokus auf Soziale_Arbeit (arbeitsfeldübergreifende Analyse, Auswirkungen auf sozialarbeiterische Praxis). Kritische Auseinandersetzung mit Chancen/Risiken der KI-Implementierung im Kontext authentischer Sozialer Arbeit.",0,0,
77,The EU artificial intelligence act through a gender lens,Friedrich-Ebert-Stiftung (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Theoretisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK durch AI_Literacies (Vorbereitung von Social Workers auf KI-Nutzung), KI_Sonstige (Predictive Modeling, Algorithmen in Social Services) und Fairness. SOZIAL_OK durch Soziale_Arbeit (direkter Bezug zu Social Work Practice), Bias_Ungleichheit (algorithmic bias als Risiko), und Fairness (Emphasis auf ethische, faire KI-Integration). Paper ist substantiell und adressiert zentrale Schnittstelle KI-Soziale Arbeit.",0,0,
78,Bildungsteilhabe - Flucht - Digitalisierung: Eine multilokale Ethnografie im (digitalen) Alltag junger Geflüchteter,Fujii (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Literaturreview,0.92,"Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (Kompetenzen/Verständnis von KI-Risiken/-Nutzen in der Praxis), KI_Sonstige (allgemeine KI im Kontext). SOZIAL: Soziale_Arbeit (direkter Bezug zu Profession, Praxis, Werten), Bias_Ungleichheit (sozial gerechte Outcomes, ethische Praxis), Fairness (EPIC-Modell mit Ethik und Gerechtigkeit). Literaturreview mit explizitem Fokus auf Schnittstelle KI-Sozialarbeit und deren Auswirkungen auf professionelle Werte.",0,0,
79,"Bias, accuracy, and trust: Gender-diverse perspectives on large language models",Gaba (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Literaturreview,0.92,"Das Paper ist ein Literaturreview, das KI-Technologien (Machine Learning, NLP) substantiell im Kontext Sozialer Arbeit und Human Services behandelt. Es adressiert Bias, Fairness und Ethical Concerns—zentrale soziale Aspekte. TECHNIK erfüllt (KI_Sonstige + AI_Literacies: Diskurs über KI-Integration). SOZIAL erfüllt (Soziale_Arbeit + Bias_Ungleichheit + Fairness). Keine Gender/Feminismus-Komponente evident.",0,0,
80,"Bias, accuracy, and trust: Gender-diverse perspectives on large language models",Gaba (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,Exclude,Include,Nein,Duplicate,,,Theoretisch,0.85,"Das Paper adressiert algorithmic decision-making in Sozialarbeit (KI_Sonstige + Soziale_Arbeit). Der Titel hebt die Competency/Critique-Dilemma hervor, was auf AI Literacies verweist. Bias, Fairness und ethische Bedenken in Algorithmen sind zentral (Bias_Ungleichheit + Fairness). TECHNIK erfüllt (3x Ja), SOZIAL erfüllt (4x Ja). Inklusion gerechtfertigt.",0,0,
81,Bias and fairness in large language models: A survey,Gallegos (2024),Ja,Ja,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Konzept,0.85,Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI: ChatGPT; AI_Literacies: proaktive Auseinandersetzung mit KI-Kompetenzen für Profession) + SOZIAL (Soziale_Arbeit: direkter Bezug zur Profession; Bias_Ungleichheit: Warnung vor inequitable outcomes). Kommentar-Format mit konzeptionellem Fokus auf Wertausrichtung und Gerechtigkeit.,0,0,
82,Bias and fairness in large language models: A survey,Gallegos (2024),Ja,Ja,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,Exclude,Include,Nein,Duplicate,,,Konzept,0.95,"Paper behandelt substantiell KI-Kompetenzen (AI_Literacies) und generative KI im MSW-Curriculum. Direkter Bezug zu Sozialer Arbeit (Ausbildung, Akkreditierungsstandards). Adressiert Bias, Transparenz und Fairness als zentrale Themen für verantwortungsvolle AI-Nutzung im Kontext von Klientenschutz und Equity. Erfüllt beide Bedingungen: TECHNIK (AI_Literacies + Generative_KI) und SOZIAL (Soziale_Arbeit + Bias_Ungleichheit + Fairness).",0,0,
83,Counterfactual fairness in text classification through robustness,Garg (2019),Ja,Ja,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Empirisch,0.95,"Paper erfüllt TECHNIK- und SOZIAL-Kriterien: (1) TECHNIK: Generative_KI (LLMs in klinischer Praxis) und AI_Literacies (Training, Kompetenzentwicklung im Umgang mit LLMs) sind substantiell. (2) SOZIAL: Direkter Bezug zu Sozialarbeiter:innen (clinical social workers) und ihrer Praxis. Qualitative Studie mit praktischen Implikationen für sozialarbeiterische Integration von LLMs. Betont Augmentation statt Automation, was auf Kompetenzentwicklung abzielt.",0,0,
84,Considering a unified model of artificial intelligence enhanced social work: A systematic review,Garkisch (2024),Ja,Ja,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Konzept,0.92,Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI: LLM-Fokus; AI_Literacies: kritischer Umgang mit KI-Systemen) und SOZIAL_OK (Soziale_Arbeit: Ethik für sozialarbeiterische Forschung; Bias_Ungleichheit & Fairness: explizit Bias-Bekämpfung und Inklusion/Social Justice). Substantieller Bezug zu ethischen Herausforderungen und KI-Kompetenzen in der Sozialarbeit.,0,0,
85,Faires KI-Prompting – Ein Leitfaden für Unternehmen,Gengler (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK durch AI_Literacies (ethische Kompetenzen, professionelle Ausbildung), KI_Sonstige (algorithmische Systeme in Sozialbereich) und Fairness (Bias, Transparenz). SOZIAL_OK durch Soziale_Arbeit (direkte Anwendung in SW-Praxis), Bias_Ungleichheit (algorithmic bias) und Fairness. Umfassende ethische Analyse mit Bezug zu NASW-Kodex und SW-Zielgruppen.",0,0,
86,An empirical study of structural social and ethical challenges in AI,Ghosal (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Include,,,,,Literaturreview,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige) + SOZIAL_OK (Soziale_Arbeit, Bias_Ungleichheit, Diversitaet, Fairness, Feministisch). EPIC-Modell adressiert KI-Kompetenzen und strukturierte AI-Integration in Sozialarbeit. Expliziter Fokus auf Bias-Risiken, soziale Gerechtigkeit, Intersektionalität und ethische Fairness. Intersektionalität deutet auf feministische Perspektive hin.",0,0,
87,Unequal voices: How LLMs construct constrained queer narratives,Ghosal (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Konzept,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (Kernthema: AI Literacy als Kompetenz in Curriculum), KI_Sonstige (algorithmische Systeme und deren Auswirkungen). SOZIAL: Soziale_Arbeit (direkter Bezug zu SW-Kompetenzentwicklung und Schutz vulnerabler Populationen), Bias_Ungleichheit (kritische Analyse von Inequities durch AI), Fairness (ethische Safeguards und bias detection). Konzeptionelles Paper mit substanziellem Fokus auf Integration von AI-Kritik in SW-Ausbildung und Praxis.",0,0,
88,"The AI Act, gender equality and non-discrimination: what role for the AI Office?",Giannoni Adielsson (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Literaturreview,0.95,Paper adressiert algorithmic decision-making tools (KI_Sonstige) in child welfare (Soziale_Arbeit). Zentrale Themen sind Bias und Fairness. Nennt Practitioner Training (AI_Literacies). Systematischer Review mit PRISMA-Methode. Erfüllt beide Bedingungen: TECHNIK (KI_Sonstige + AI_Literacies) UND SOZIAL (Soziale_Arbeit + Bias_Ungleichheit + Fairness). Direkt relevant für KI in Sozialer Arbeit.,0,0,
89,Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room,Goellner (2025),Ja,Ja,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Konzept,0.95,"Paper erfüllt beide Bedingungen: (1) TECHNIK: Generative_KI (LLMs in Social Work) + AI_Literacies (Pädagogische Integration, Kompetenzentwicklung). (2) SOZIAL: Soziale_Arbeit (direkter Bezug zu Lehre und Praxis Sozialer Arbeit) + Bias_Ungleichheit (Warnung vor Bias, faktischen Fehlern). Substantielle Behandlung aller Kategorien, kein Gender/Feministisch-Fokus erforderlich.",0,0,
90,"A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges",Gohar (2023),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Empirisch,0.92,Paper erfüllt beide Bedingungen: TECHNIK_OK durch AI_Literacies (Fokus auf digitale Kompetenz und kritische KI-Literacy in Aus- und Weiterbildung) und KI_Sonstige (Organisationale KI-Rahmenbedingungen). SOZIAL_OK durch Soziale_Arbeit (direkter Bezug zu Berufseinsteiger:innen und Klientenarbeit in der Sozialen Arbeit). Empirische Untersuchung mit substantiellem Bildungs- und Kompetenzbezug.,0,0,
91,A Survey on Intersectional Fairness in Machine Learning: Opportunities and Challenges,Gohar (2023),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Theoretisch,0.92,"Paper behandelt NLP als KI-Technologie (KI_Sonstige: Ja) und adressiert direkt sozialarbeiterische Praxis, Theorie und professionsethische Fragen (Soziale_Arbeit: Ja). Kritische Auseinandersetzung mit KI-Chancen/-Risiken impliziert auch AI Literacy-Anforderungen (AI_Literacies: Ja). TECHNIK-Kriterium erfüllt (3x Ja). SOZIAL-Kriterium erfüllt (Soziale_Arbeit: Ja). Inklusionskriterien erfüllt.",0,0,
92,The End of the World as We Know It? ChatGPT and Social Work,Goldkind (2023),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Include,,,,,Konzept,0.92,"Das Paper behandelt algorithmische Diskriminierung (KI_Sonstige: Ja) und kritische KI-Kompetenzen (AI_Literacies: Ja). Es adressiert substantiell Bias, Ungleichheit (Bias_Ungleichheit: Ja), digitale Teilhabe/Inklusion (Diversitaet: Ja) und implizit Fairness durch Fokus auf Benachteiligungen. TECHNIK erfüllt (AI_Literacies + KI_Sonstige). SOZIAL erfüllt (Bias_Ungleichheit + Diversitaet + Fairness). → Include",0,0,
93,Artificial intelligence in social work: An EPIC model for practice,Goldkind (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Empirisch,0.92,Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmisches Entscheidungssystem in Kinderschutz; AI_Literacies: Schulungsbedarf erwähnt) + SOZIAL_OK (Soziale_Arbeit: Gefährdungseinschätzung im Kinderschutz; Bias_Ungleichheit: explizite Bias-Risiken; Fairness: Human-in-the-Loop und Fehlklassifikationen). Direkt relevant für Praxis Sozialer Arbeit.,0,0,
94,Artificial intelligence in social work: An EPIC model for practice,Goldkind (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,Exclude,Include,Nein,Duplicate,,,Literaturreview,0.85,"Das Paper behandelt KI in der Sozialen Arbeit und adressiert dabei sowohl Kompetenzentwicklung (AI Literacies: reflektierte Implementationsstrategien, professioneller Diskurs) als auch allgemeine KI/algorithmische Systeme (KI_Sonstige) in sozialarbeiterischen Praxisfeldern (Soziale_Arbeit: direkter Bezug zur Praxis). Beide Bedingungen (TECHNIK + SOZIAL) sind erfüllt → Include. Bias/Fairness nicht substantiell behandelt basierend auf Abstract.",0,0,
95,Artificial intelligence in social work: An EPIC model for practice,Goldkind (2024),Ja,Ja,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,Exclude,Include,Nein,Duplicate,,,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, Generative_KI, Prompting sind substantiell) + SOZIAL_OK (Soziale_Arbeit durch expliziten Hochschulbezug zur Soziale-Arbeit-Ausbildung). Prompting als metakognitive Schlüsselkompetenz ist Kernthema. Curriculare Integration in Soziale Arbeit direkt adressiert. Konzept/Werkstattbericht, kein empirisch-experimenteller Nachweis.",0,0,
96,The end of the world as we know it? ChatGPT and social work,Goldkind (2024),Ja,Ja,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,"Nur TECHNIK erfüllt (AI_Literacies, Generative_KI, Prompting), SOZIAL-Kriterien nicht erfüllt. Paper behandelt Prompt Engineering als Pädagogik-Tool im Schulkontext, adressiert aber keine der SOZIAL-Kategorien (Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness).",,Konzept,0.95,"Das Paper konzentriert sich auf AI Literacy und Prompting-Strategien (TECHNIK-Anforderung erfüllt), fehlt aber vollständig die SOZIAL-Komponente. Es diskutiert weder Bias, Ungleichheit, Gender-Perspektiven, Fairness noch direkte sozialarbeiterische Anwendungen. Rein schulpädagogisches Framework ohne kritische soziale Dimension.",0,0,
97,Künstliche Intelligenz in der Sozialen Arbeit – Zwischen Bedenken und Optionen,Gravelmann (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Theoretisch,0.85,"Paper adressiert KI_Sonstige durch Natural Language Processing (NLP), ein klassisches ML-Thema, und hat starken direkten Bezug zu Sozialer Arbeit (Staub-Bernasconi, Klient*innen, sozialarbeiterische Praxis). Beide Bedingungen erfüllt (TECHNIK + SOZIAL). Theoretisch-diskursiver Charakter ohne empirische Daten.",0,0,
98,Künstliche Intelligenz in der Sozialen Arbeit – Zwischen Bedenken und Optionen,Gravelmann (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,Exclude,Include,Nein,Duplicate,,,Empirisch,0.95,"Paper erfüllt TECHNIK-Bedingung: AI_Literacies (Ja - untersucht digitale Kompetenzen und KI-Akzeptanz von Fachkräften) und KI_Sonstige (Ja - behandelt KI-Einsatz in Organisationen). Erfüllt SOZIAL-Bedingung: Soziale_Arbeit (Ja - direkter Fokus auf KI-Akzeptanz und Praxis in der Sozialen Arbeit, Bedenken zu Datenschutz und ethischen Herausforderungen). Empirische Studie mit Berufseinsteiger:innen. Inklusion berechtigt.",0,0,
99,Feminist reflections for the development of artificial intelligence,Guerra (2023),Ja,Ja,,Nein,,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,Not_relevant_topic,,Literaturreview,0.95,"Das Handbuch behandelt Digitalisierung in der Sozialen Arbeit substantiell (Soziale_Arbeit = Ja). Allerdings fehlt der erforderliche TECHNIK-Aspekt vollständig: Es gibt keinen Bezug zu KI-Systemen, Machine Learning, generativer KI oder KI-Kompetenzen. Das Werk konzentriert sich auf breite Digitalisierungsthemen (Social Media, E-Government, Datenschutz, Mediatisierung), nicht auf KI/ML-spezifische Technologien. Die Inklusionskriterien erfordern SOWOHL einen Technik-Aspekt (AI_Literacies, Generative_KI, Prompting oder KI_Sonstige) ALS AUCH einen Sozial-Aspekt. Hier ist nur Soziale_Arbeit erfüllt.",0,0,
100,"A systematic review of sophisticated predictive and prescriptive analytics in child welfare: Accuracy, equity, and bias",Hall (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Empirisch,0.95,"Das Paper behandelt substantiell AI Literacy (Gaps in technical literacy, educational training), KI allgemein (Automation Bias, explainable AI), Soziale Arbeit (AI applications in social work, professional autonomy), Bias (automation bias), und Fairness (ethical governance, transparency). Beide Bedingungen sind erfüllt: TECHNIK_OK (AI_Literacies + KI_Sonstige), SOZIAL_OK (Soziale_Arbeit + Bias_Ungleichheit + Fairness).",0,0,
101,Generative AI and the Future of Digital Literacy: Opportunities for Gender Inclusion,Hartshorne (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Include,,,,,Literaturreview,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Predictive Analytics, algorithmische Systeme; AI_Literacies: Involvement von Social Workers in AI-Entwicklung) und SOZIAL_OK (Soziale_Arbeit: direkter Bezug zu SW-Praxis und Kinderschutz; Bias_Ungleichheit: algorithmischer Bias gegen Familien of Color; Gender: explizite Erwähnung von Gender-Disparitäten; Diversitaet: Fokus auf diverse Datasets und Community-Involvement; Fairness: Algorithmic fairness und Auditing). Substantieller Review zu AI in Social Work mit kritischer Perspektive auf strukturelle Ungleichheit.",0,0,
102,A framework for the learning and teaching of Critical AI Literacy skills (Version 0.1),Hauck (2025),Ja,Ja,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Include,,,,,Konzept,0.92,"Paper entwickelt ein Framework für Critical AI Literacy (TECHNIK: AI_Literacies, Generative_KI, Prompting). Substantieller Fokus auf Bias, Ungleichheit, Diversität und Fairness (SOZIAL) durch explizite Thematisierung von Epistemic Injustices, Power Relationships, Inequalities und Equity/Inclusion-Prinzipien. Beide TECHNIK- und SOZIAL-Bedingungen erfüllt. Nicht feministisch, da keine explizite feminist theory verwendet.",0,0,
103,How Far Can We Extract Diverse Perspectives from Large Language Models?,Hayati (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Include,,,,,Konzept,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige) durch Framework für AI-Integration in Sozialarbeit; SOZIAL_OK (Soziale_Arbeit, Bias_Ungleichheit, Diversitaet, Fairness) durch expliziten Fokus auf vulnerable Populationen, Gerechtigkeit, Menschenrechte und ethische Implikationen. Substantielle Behandlung von KI-Kompetenzen für SozialarbeiterInnen und algorithmischen Systemauswirkungen.",0,0,
104,On the steerability of large language models,He (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Konzept,0.95,"Das Paper behandelt AI Literacy substantiell als Kernkompetenz für Sozialarbeiter (AI_Literacies: Ja). Es hat direkten Bezug zu Soziale_Arbeit (Ja) durch explizite Fokussierung auf sozialarbeiterische Praxis und Professionalisierung. Bias und ethische Implikationen von KI werden adressiert (Bias_Ungleichheit: Ja, KI_Sonstige: Ja). TECHNIK- und SOZIAL-Kriterien sind erfüllt → Include.",0,0,
105,Clinical trial of an LLM-based conversational AI psychotherapy,Heinz (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Theoretisch,0.95,Das Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies: Fokus auf AI Literacy in Social Work Education; KI_Sonstige: algorithmische Systeme im Sozialbereich). SOZIAL (Soziale_Arbeit: direkter Bezug zu sozialarbeiterischer Praxis und Ausbildung; Bias_Ungleichheit: algorithmic bias; Fairness: unfairness in AI). Umfassende ethische Analyse von KI-Implementierung im sozialarbeiterischen Kontext mit konkretem Implementierungsprotokoll.,0,0,
106,Artificial intelligence and mass personalization of communication content—An ethical and literacy perspective,Hermann (2022),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Theoretisch,0.92,"Das Paper erfüllt beide Bedingungen: (1) TECHNIK: KI_Sonstige=Ja (algorithmische Systeme in Sozialer Arbeit, insbesondere in Kindesschutz). (2) SOZIAL: Soziale_Arbeit=Ja (direkter Fokus auf Auswirkungen auf Profession) und Bias_Ungleichheit=Ja (kritische Analyse von Diskriminierung und Machtstrukturen in KI-Daten). Die Analyse von automatisierten Verfahren im Kindesschutz und die Thematisierung von Bias in KI-Systemen sind zentrale Inhalte.",0,0,
107,"Artificial Intelligence and Structural Injustice: Foundations for Equity, Values, and Responsibility",Himmelreich (2022),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Theoretisch,0.92,Paper erfüllt beide Einschlusskriterien: TECHNIK_OK (KI_Sonstige: Algorithmen in digitaler Sozialer Arbeit; AI_Literacies: kritische Reflexion digitaler Technologien) und SOZIAL_OK (Soziale_Arbeit: direkter Bezug zu sozialarbeiterischer Praxis; Bias_Ungleichheit: Diskriminierung durch biased data; Fairness: Transparenz und faire Nutzung von Technologien). Substantielle theoretische Analyse von Machtasymmetrien und Verantwortungsfragen im digitalen Kontext.,0,0,
108,Algorithmic decision-making in social work practice and pedagogy: confronting the competency/critique dilemma,James (2023),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,"TECHNIK_OK (AI_Literacies=Ja, KI_Sonstige=Ja), aber SOZIAL_OK nicht erfüllt. Bias_Ungleichheit=Ja bezieht sich auf 'automation bias' als technisches Risiko, nicht auf soziale Ungleichheit oder Diskriminierung von Gruppen. Kein direkter Bezug zu Sozialer Arbeit, Gender, Diversität oder feministischer Perspektive. Allgemeiner Verwaltungs-/Ethik-Kontext ohne soziale Arbeit.",,Konzept,0.85,"Der Leitfaden adressiert AI Literacy und KI-Risiken in der Verwaltung substantiell. Das Konzept von 'automation bias' ist technisch orientiert (KI-Versagen), nicht sozial (Diskriminierung von Gruppen). Fehlender Bezug zu Sozialer Arbeit, Gender, Diversität oder kritischen sozialen Perspektiven führt zur Exclusion.",0,0,
109,Responsible prompting recommendation: Fostering responsible AI practices in prompting-time,James (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Include,,,,,Empirisch,0.92,"Paper adressiert algorithmische Entscheidungssysteme (KI_Sonstige: Ja) in der Sozialleistungsverwaltung (Soziale_Arbeit: Ja). Zentrale Themen sind Fairness bei der Ressourcenverteilung (Fairness: Ja), kulturelle/kontextuelle Ungleichheiten (Bias_Ungleichheit: Ja) und inklusive Partizipation von marginalisierten Gruppen (Diversitaet: Ja). Erfüllt TECHNIK- und SOZIAL-Kriterium.",0,0,
110,Who cares about data? Data care arrangements in everyday organisational practice,Jarke (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,Not_relevant_topic,,Konzept,0.85,"Das Paper behandelt Critical AI Literacy und Automation Bias (TECHNIK: AI_Literacies + KI_Sonstige erfüllt). Der Kontext ist jedoch betrieblich/wirtschaftlich orientiert, nicht sozialarbeiterisch. Bias_Ungleichheit ist nur marginal via 'automation bias' berührt, nicht substantiell. SOZIAL-Kriterien sind nicht erfüllt. Relevanz für Soziale-Arbeit-Review fraglich.",0,0,
111,Datafied ageing futures: Regimes of anticipation and participatory futuring,Jarke (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,Not_relevant_topic,,Konzept,0.7,"Das Paper behandelt AI Literacy und Kompetenzenrahmen (erfüllt TECHNIK-Kriterium). Allerdings erfüllt es KEINE der SOZIAL-Kategorien: Es gibt keinen Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder Fairness. Es ist ein rein bildungstechnologisches Konzept-Paper ohne sozialarbeiterischen oder kritischen Ungleichheitsfokus. Beide Bedingungen (TECHNIK UND SOZIAL) müssen erfüllt sein.",0,0,
112,Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia,Jiang (2022),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Konzept,0.85,"Das Paper behandelt explizit AI Literacy im Kontext von Social Work und deren Implikationen für Kernkompetenzen. Dies erfüllt beide Bedingungen: (1) TECHNIK: AI_Literacies = Ja (KI-Kompetenzen für Sozialarbeit), (2) SOZIAL: Soziale_Arbeit = Ja (direkter Bezug zu sozialarbeiterischer Praxis und Ausbildung). Include.",0,0,
113,GLAT: The generative AI literacy assessment test,Jin (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,No_relevant_topic,,Literaturreview,0.95,"Das Paper behandelt AI Literacy substantiell (Kategorie AI_Literacies = Ja) und ist ein hochwertiger Umbrella Review. Allerdings erfüllt es nicht die SOZIAL-Bedingung: Es gibt keinen direkten Bezug zu Sozialer Arbeit, keine Analyse von Bias/Ungleichheit, kein Gender/Diversitäts-Fokus und keine Fairness-Diskussion. Der Fokus liegt auf allgemeiner KI-Bildung in Schulen, nicht auf sozialen Auswirkungen oder marginalisierten Gruppen. TECHNIK erfüllt (AI_Literacies), SOZIAL nicht erfüllt → Exclude.",0,0,
114,Intersectional analysis of visual generative AI: The case of Stable Diffusion,Jääskeläinen (2025),Ja,Ja,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Include,,,,,Literaturreview,0.85,"Das Paper behandelt substantiell Generative KI (Foundation Models, ChatGPT, Gemini) mit kritischem Fokus. Es adressiert Systemische Risiken und Gefahren für vulnerable Gruppen (Bias_Ungleichheit) sowie kritisches Verständnis der KI-Technologie (AI_Literacies). Die Kombination TECHNIK (Generative_KI + AI_Literacies) UND SOZIAL (Bias_Ungleichheit) erfüllt beide Einschlussbedingungen. Mission-driven organizations können als sozialrelevanter Kontext interpretiert werden, aber Fokus liegt primär auf Risiken für vulnerable Gruppen.",0,0,
115,Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,Kamruzzaman (2024),Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Include,Include,Ja,,,Experimentell,Unclear,0.85,"Der Titel signalisiert explizit: (1) Generative KI (ChatGPT), (2) Soziale Arbeit, (3) Anwendungsrisiken und Gegenmaßnahmen. Dies erfüllt beide Inklusionsbedingungen: TECHNIK_OK (Generative_KI=Ja), SOZIAL_OK (Soziale_Arbeit=Ja). AI_Literacies wird mit Ja bewertet, da Risiken/Gegenmaßnahmen KI-Kompetenzentwicklung implizieren. Kein Abstract liegt vor, aber der Titel ist ausreichend informativ.",4,10,40.00%
116,Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,Kamruzzaman (2024),Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Exclude,Include,Nein,Duplicate,,Experimentell,Unclear,0.85,"Der Titel deutet auf eine Analyse von ChatGPT (generative KI) in der Sozialen Arbeit hin. Dies erfüllt beide notwendigen Bedingungen: TECHNIK (Generative_KI = Ja, da ChatGPT explizit genannt) und SOZIAL (Soziale_Arbeit = Ja, da direkt im Titel). AI_Literacies ist ebenfalls Ja, da es um Kompetenzen/Risiken im Umgang mit ChatGPT geht. Ohne Abstract kann Studientyp nicht genau bestimmt werden.",4,10,40.00%
117,Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,Kamruzzaman (2024),Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Exclude,Exclude,Ja,Duplicate,No_full_text,Experimentell,Empirisch,0.65,"Titel deutet auf AI Literacy hin (Technik-Komponente erfüllt). Jedoch fehlt Abstract und Volltextinformation. Ohne Textmaterial kann nicht überprüft werden, ob die SOZIAL-Komponente erfüllt ist (Bezug zu Sozialer Arbeit, Bias, Gender, Diversität, Fairness oder Feministik). Konservative Bewertung: Exclude wegen fehlender Information.",4,10,40.00%
118,Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,Kamruzzaman (2024),Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Exclude,Exclude,Ja,Duplicate,No_full_text,Experimentell,Unclear,0.3,"Der Titel deutet auf AI Literacies (Kompetenzen für Studierende) hin, was die TECHNIK-Bedingung erfüllen könnte. Jedoch fehlt das Abstract und der vollständige Text, was eine fundierte Bewertung unmöglich macht. Zudem ist unklar, ob ein Sozialbezug vorliegt. Ohne Volltext kann nicht überprüft werden, ob die SOZIAL-Bedingung erfüllt ist.",4,10,40.00%
119,Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,Kamruzzaman (2024),Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Exclude,Exclude,Ja,Duplicate,Not_relevant_topic,Experimentell,Konzept,0.95,"Das Paper behandelt AI Literacy im Bildungskontext (PK-12 Education) und erfüllt die TECHNIK-Bedingung durch AI_Literacies = Ja. Es fehlt jedoch jegliche SOZIAL-Komponente: Kein Bezug zu Sozialer Arbeit, keine Behandlung von Bias/Ungleichheit, Gender, Diversität, feministischen Perspektiven oder Fairness. Es ist ein rein pädagogisches Framework für allgemeine Bildung ohne Sozialarbeitsfokus oder kritische Betrachtung struktureller Ungleichheiten.",4,10,40.00%
120,Debiasing prompts for gender bias in large language models,Kaneko (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,Exclude,Include,Nein,Other,,,Literaturreview,0.92,"Das Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (digitale Kompetenzen zur Empowerment), KI_Sonstige (algorithmische Systeme in Recruitment, Healthcare, Finance). SOZIAL: Gender (expliziter Gender-Bias-Fokus), Bias_Ungleichheit (Diskriminierung durch biased datasets und unter-representation), Diversitaet (Inklusion in AI-Entwicklung), Fairness (Bias-Mitigation). Kein explizit feministischer Rahmen erkennbar, aber substantielle Behandlung von Gender und Ungleichheit.",0,0,
121,Evaluating gender bias in large language models via chain-of-thought prompting,Kaneko (2024),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Include,Exclude,Nein,,Not_relevant_topic,Experimentell,Konzept,0.95,"Das Paper behandelt rein technische Aspekte von LLM-Inferenz (Nondeterminismus, Sampling, Reproducibility). Während es Generative KI erwähnt, fehlt jeder soziale Bezug: keine AI Literacies, keine Bias/Fairness-Analyse, keine Bezüge zu Soziale Arbeit, Diversität oder Ungleichheit. Es ist ein rein technisches Konzeptpaper ohne gesellschaftliche, ethische oder pädagogische Dimension.",6,10,60.00%
122,Gender in a stereo-(gender)typical EU AI law: A feminist reading of the AI Act,Karagianni (2025),Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Theoretisch,Konzept,0.85,"Paper adressiert AI Literacy durch Forderung nach Stakeholder-Involvement und Transparenz in KI-Systemen für Bildung. KI_Sonstige zutreffend: Neural-Symbolic AI ist ML-Ansatz. Bias_Ungleichheit und Fairness erfüllt durch Kritik an irresponsible KI-Use, unreliablen XAI-Methoden und Forderung nach Verantwortlichkeit. TECHNIK (AI_Literacies + KI_Sonstige) UND SOZIAL (Bias_Ungleichheit + Fairness) erfüllt → Include. Keine direkten Sozialen Arbeits- oder Gender-Bezüge identifiziert.",5,10,50.00%
123,Assessing trustworthy AI: Technical and legal perspectives of fairness in AI,Kattnig (2024),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Include,Include,Ja,,,Theoretisch,Empirisch,0.92,"Das Paper untersucht Diskriminierungsinhalte in conversational AI-Systemen (Generative_KI: Ja). Die Audit-Analyse fokussiert auf Bias und Diskriminierung (Bias_Ungleichheit: Ja), behandelt Unterschiede across Sprachen und Themen (Diversitaet: Ja) und diskutiert Fairness-Aspekte durch RLHF und Guardrails (Fairness: Ja). TECHNIK-Bedingung erfüllt (Generative_KI), SOZIAL-Bedingung erfüllt (Bias_Ungleichheit, Diversitaet, Fairness). Include.",8,10,80.00%
124,"Improving human-AI partnerships in child welfare: Understanding worker practices, challenges, and desires for algorithmic decision support",Kawakami (2022),Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Exclude,Exclude,Ja,Not relevant topic,Not_relevant_topic,Empirisch,Empirisch,0.85,"Paper erfüllt TECHNIK-Kriterium (Generative_KI, KI_Sonstige) und SOZIAL-Kriterium (Bias_Ungleichheit, Diversitaet, Fairness). Allerdings: Paper behandelt allgemeine LLM-Bias-Evaluation ohne direkten Bezug zu Sozialer Arbeit, sozialarbeiterischen Zielgruppen oder sozialarbeiterischer Praxis. Bias-Analyse ist technisch-orientiert (Fairness-Metriken für LLMs), nicht anwendungsorientiert für Sozialarbeit. Keine Verbindung zu Soziale_Arbeit identifizierbar.",7,10,70.00%
125,Why and What to Teach: AI Curriculum for Elementary School,Kim (2021),Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Unclear,Exclude,Nein,,Only_Technik_keine_Sozial,Konzept,Theoretisch,0.95,"Paper behandelt philosophische Grundlagen von AI-Explainability und Systematizität (KI_Sonstige: Ja). Dies ist jedoch rein technisch-philosophischer Natur ohne Bezug zu sozialen Auswirkungen, Bias, Ungleichheit, Gender, Diversität, Feminismus oder Fairness. Kein Bezug zu Sozialer Arbeit. TECHNIK erfüllt (KI_Sonstige), SOZIAL nicht erfüllt → Exclude.",7,10,70.00%
126,Data Feminism for AI,Klein (2024),Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Theoretisch,Empirisch,0.92,Paper behandelt KI-Governance und -Harms substantiell (KI_Sonstige: Ja). Adressiert algorithmische Diskriminierung und Schadensanalyse (Bias_Ungleichheit: Ja) sowie Accountability und Fairness in KI-Systemen (Fairness: Ja). Erfüllt beide Bedingungen: TECHNIK (KI_Sonstige) UND SOZIAL (Bias_Ungleichheit + Fairness). Basiert auf empirischer Analyse von 1.060 realen Incidents.,5,10,50.00%
127,Data Feminism for AI,Klein (2024),Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Exclude,Include,Nein,Duplicate,,Theoretisch,Theoretisch,0.85,"Das Paper behandelt substantiell Generative KI (Kategorie erfüllt) und adressiert Bias-Amplifikation, Machtungleichgewichte und Fairness-Fragen (Bias_Ungleichheit und Fairness erfüllt). Es erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja) und SOZIAL_OK (Bias_Ungleichheit=Ja, Fairness=Ja). Keine Sozialen-Arbeit-Perspektive erkennbar, daher diese Kategorie Nein.",5,10,50.00%
128,Data feminism for AI,Klein (2024),Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Exclude,Include,Nein,Duplicate,,Theoretisch,Empirisch,0.82,"Paper untersucht LLMs (Generative_KI: Ja) systematisch mit 43.200 Prompts (Prompting: Ja, substantiell). Zentrale Ergebnisse adressieren Bias und Malleabilität sozialer Normen (Bias_Ungleichheit: Ja) sowie Fairness-Aspekte bei moralischen Urteilen (Fairness: Ja). TECHNIK und SOZIAL erfüllt → Include. Kein direkter Soziale-Arbeit-Bezug, aber relevante Erkenntnisse zu KI-Systemen und Ungleichheiten.",4,10,40.00%
129,A sociolinguistic approach to stereotype assessment in large language models,Klinge (2024),Ja,Ja,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Include,,,,,Experimentell,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting substantiell behandelt), SOZIAL_OK (Bias_Ungleichheit + Fairness adressiert). Empirische Untersuchung von Prompt-Engineering-Strategien zur Bias-Reduktion in LLMs. Keine sozialen Arbeitskontexte, aber algorithmische Fairness ist relevant für Inklusion.",0,0,
130,Trustworthy AI and the Logics of Intersectional Resistance,Knowles (2023),Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Exclude,Include,Nein,Not relevant topic,,Theoretisch,Konzept,0.85,"Paper behandelt substantiell AI_Literacies (user guidelines und training für LLM-Nutzung), Generative_KI (LLM-fokussiert), Prompting (diversity-sensitive prompting techniques explizit genannt) und adressiert Bias_Ungleichheit sowie Diversität (diversity-sensitive approaches). Erfüllt TECHNIK (3 Kategorien) und SOZIAL (2 Kategorien). Kein direkter Sozialarbeitsbezug identifizierbar.",3,10,30.00%
131,Large language models are zero-shot reasoners,Kojima (2022),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Exclude,Include,Nein,Not relevant topic,,Empirisch,Empirisch,0.92,"Paper untersucht systematisch Bias und Diskriminierung in LLM-generierten Narrativen von queeren Personen. Erfüllt TECHNIK-Kriterium (Generative_KI: Fokus auf LLM-Outputs) und SOZIAL-Kriterium (Bias_Ungleichheit: Stereotypisierung marginalisierter Gruppen; Gender: queere Identitäten; Diversitaet: Repräsentation von LGBTQ+-Personen in KI-Systemen). Keine explizite feministische Theorie erkannt, daher Feministisch=Nein.",6,10,60.00%
132,Evaluation of an artificial intelligence literacy course for university students with diverse study backgrounds,Kong (2021),Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Unclear,Include,Nein,,,Konzept,Empirisch,0.92,"Paper untersucht substantiell LLM-Responses auf LGBTQ+-Prompts (Generative_KI: Ja; Prompting: Ja - systematische Prompt-Variation). Zeigt algorithmischen Bias gegen queer/marginalisierte Communities (Bias_Ungleichheit: Ja), thematisiert Gender/sexuelle Orientierung (Gender: Ja; Diversitaet: Ja - LGBTQ+ als marginalisierte Gruppe; Fairness: Ja - Asymmetrien in Safety-Guardrails). TECHNIK + SOZIAL erfüllt.",4,10,40.00%
133,"Are ""Intersectionally Fair"" AI Algorithms Really Fair to Women of Color? A Philosophical Analysis",Kong (2022),Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Theoretisch,Experimentell,0.92,"Paper untersucht implizite Biase in LLMs durch neuartige Evaluationsmethoden (LLM-WAT, LLM-RDT). Erfüllt TECHNIK-Kriterium (Generative_KI=Ja) und SOZIAL-Kriterien (Bias_Ungleichheit=Ja, Fairness=Ja). Fokus auf stereotype Assoziationen und subtile Diskriminierung in State-of-the-Art Modellen. Kein expliziter Gender-, Diversitäts- oder feministischer Fokus erkennbar, daher diese Kategorien Nein. Kein direkter Sozialarbeitsbezug.",5,10,50.00%
134,Developing an artificial intelligence literacy framework: Evaluation of a literacy course for senior secondary students using a project-based learning approach,Kong (2024),Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Unclear,Exclude,Nein,,"Technik-Bedingung erfüllt (Generative_KI=Ja), aber keine Soziale_Arbeit-Kategorie. Bias_Ungleichheit, Diversitaet und Fairness sind vorhanden, aber der Sozial-Kreis ist nur über allgemeine KI-Ethik-Themen erfüllt, nicht durch Soziale_Arbeit spezifisch. Da das Paper auf allgemeine LLM-Bias-Literatur fokussiert und keinen direkten Sozialarbeit-Bezug hat, wird Soziale_Arbeit korrekt als 'Nein' bewertet.",Konzept,Literaturreview,0.95,"Das Paper ist ein Überblicks-Survey zu Bias und Fairness in LLMs. Es erfüllt die TECHNIK-Bedingung (Generative_KI=Ja) und behandelt substantiell Bias_Ungleichheit und Fairness. Allerdings fehlt die SOZIAL-Bedingung im Sinne der definierten Kategorien: Es gibt keinen direkten Bezug zu Sozialer Arbeit, und die Bias/Fairness-Behandlung ist allgemein auf NLP fokussiert, nicht spezifisch auf sozialarbeiterische Kontexte oder marginalisierte Gruppen im Sozialbereich. Daher: Exclude.",4,10,40.00%
135,Artificial Intelligence (AI) literacy – an argument for AI literacy in education,Kong (2025),Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Exclude,Include,Nein,Not relevant topic,,Konzept,Empirisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI: LLMs sind zentral), SOZIAL_OK (Bias_Ungleichheit: Analyse von Prejudice in LLM-Outputs; Diversitaet: Fokus auf marginalized groups; Fairness: Implikationen für vertrauenswürdige KI-Nutzung). Drei preregistrierte experimentelle Studien mit klarem Fokus auf Diskriminierung und deren Auswirkungen auf Nutzervertrauen.",5,10,50.00%
136,Feministische KI – Künstliche Intelligenz für alle? [Feminist AI – Artificial intelligence for everyone?],Kubes (2024),Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Exclude,Include,Nein,Wrong publication type,,Theoretisch,Empirisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + KI_Sonstige: LLM-basierte Entscheidungssysteme), SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch: intersektionale Analyse von Geschlechts- und Rassenbias). Explizit intersektionales Design nach Crenshaw-Ansatz (Gender × Rasse). Substanzielle empirische Analyse mit sozialen Implikationen für algorithmic fairness.",9,10,90.00%
137,How AI hype impacts the LGBTQ+ community,Kumar (2024),Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Exclude,Include,Nein,Not relevant topic,,Theoretisch,Literaturreview,0.92,"Paper erfüllt TECHNIK-Kriterium (Generative_KI: LLMs) und SOZIAL-Kriterium (Bias_Ungleichheit, Gender, Diversitaet, Fairness). Systematische Analyse von sozialen Biases in LLMs mit Fokus auf Geschlecht, Ethnie, sexuelle Orientierung, Alter und Religion. Behandelt Messungs- und Mitigationsstrategien. Nicht feministisch (keine explizite feministische Theorie), nicht zu Sozialer Arbeit.",7,10,70.00%
138,Handbuch Soziale Arbeit und Digitalisierung,Kutscher (2020),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,Exclude,Exclude,Ja,No full text,Not_relevant_topic,,Konzept,0.85,"Paper erfüllt TECHNIK-Kriterium (KI_Sonstige=Ja) und SOZIAL-Kriterium (Bias_Ungleichheit=Ja, Fairness=Ja). ABER: Kein direkter Bezug zu Sozialer Arbeit, keine spezifische soziale Ungleichheit oder marginalisierte Gruppen adressiert. Allgemeine AI-Governance und Fairness-Diskussion ohne Sozialarbeitsbezug. Daher außerhalb des Fokus eines SA-Literature-Reviews.",0,0,
139,"Positionings, challenges, and ambivalences in children's and parents' perspectives in digitalized familial contexts",Kutscher (2023),Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Exclude,Include,Nein,Not relevant topic,,Empirisch,Experimentell,0.85,"Das Paper behandelt Prompting (Dipper-Framework mit diversen Prompts) und Generative KI (LLMs) substantiell. Es adressiert Diversity (marginalisierte Gruppen, Perspektivenvielfalt), Bias_Ungleichheit (Verbesserung von Darstellung minorities) und Fairness (faire Output-Generierung). TECHNIK und SOZIAL sind erfüllt. Kein Soziale_Arbeit-Bezug, kein Gender- oder Feministisch-Fokus.",5,10,50.00%
140,Digitalität und Digitalisierung als Gegenstand der Sozialen Arbeit,Kutscher (2024),Ja,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Exclude,Include,Nein,Not relevant topic,,Unclear,Konzept,0.85,"Paper behandelt XAI als Kompetenz- und Transparenzthema (AI_Literacies), mit Fokus auf algorithmische Systeme (KI_Sonstige). Substantielle Thematisierung von Bias/Ungleichheit durch Marginalisierung, Diversität durch explizite Nennung marginalisierter Gruppen und Fairness durch ethische/gerechte Entscheidungsfindung. TECHNIK und SOZIAL erfüllt → Include.",4,10,40.00%
141,Improving diversity of demographic representation in people entities in Large Language Models,Lahoti (2023),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Include,Include,Ja,,,Experimentell,Experimentell,0.92,"Paper adressiert Generative KI (LLMs), Prompting (CCSV-Methode), sowie Bias/Ungleichheit durch demografische Unterrepräsentation, Diversität und Fairness. TECHNIK_OK (Generative_KI + Prompting), SOZIAL_OK (Bias_Ungleichheit + Diversitaet + Fairness). Erfüllt Einschlusskriterien. Kein expliziter Gender-Fokus, keine feministische Theorie, keine Soziale Arbeit.",7,10,70.00%
142,Avoiding Catastrophe Through Intersectionality in Global AI Governance,Laine (2025),Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Empirisch,Empirisch,0.85,"Paper adressiert LLMs (Generative_KI) und Prompting-Strategien (Diversity Prompting) substanziell. Zentraler Fokus liegt auf Mitigation von ""dominant group bias"" und Erhöhung von Perspektivenvielfalt - klare Bias_Ungleichheit und Diversitaet sowie Fairness-Themen. TECHNIK und SOZIAL erfüllt → Include.",5,10,50.00%
143,Artificial Intelligence Competence Needs for Youth Workers,Lanzetta (2024),Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Include,Include,Ja,,,Empirisch,Empirisch,0.85,"Paper erfüllt TECHNIK-Kriterium: Generative_KI (LLM-Fokus) + Prompting (Multilingual Prompting als Kern). Erfüllt SOZIAL-Kriterium: Bias_Ungleichheit (Halluzinationen, Representation), Diversitaet (demografische und kulturelle Vielfalt), Fairness (representative generation). Kein Soziale_Arbeit- oder Gender-Fokus. Empirische Evaluation mit substantiellem Prompting-Engineering-Beitrag.",3,10,30.00%
144,"AI gender bias, disparities, and fairness: Does training data matter?",Latif (2023),Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Exclude,Exclude,Ja,Not relevant topic,Not_relevant_topic,Empirisch,Theoretisch,0.95,"Paper entwickelt formale Konzeptualisierung von KI-Vertrauenswürdigkeit (KI_Sonstige erfüllt). Es fehlt jedoch jeglicher Bezug zu sozialen Dimensionen: Keine Behandlung von Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversität, Fairness oder feministischen Perspektiven. Rein technisch-theoretisches Paper ohne sozialen Anwendungskontext oder kritische Gesellschaftsperspektive. TECHNIK_OK, aber SOZIAL_NICHT_OK → Exclude.",6,10,60.00%
145,"AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?",Latif (2024),Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Include,Exclude,Nein,,SOZIAL-Kriterium nicht erfüllt,Experimentell,Experimentell,0.95,"Paper adressiert AI Literacy (kritisches Verständnis von KI-Limitations und vertrauensbasierter Reliance), KI_Sonstige (Decision-Support-Systeme), und Fairness (angemessenes Vertrauen, bias-freie Entscheidungen). ABER: Kein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder feministischer Theorie. Rein HCI-/Human-AI-Interaction-Forschung ohne sozialarbeiterischen oder kritischen Ungleichheits-Fokus. Sozialarbeitsbezug nicht substantiell gegeben.",4,10,40.00%
146,Dipper: Diversity in Prompts for Producing Large Language Model Outputs,Lau (2023),Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Include,Include,Ja,,,Konzept,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK durch AI_Literacies (EPIC-Modell für KI-Integration), KI_Sonstige (algorithmische Systeme im Sozialbereich) UND SOZIAL durch Soziale_Arbeit (direkter Fokus auf sozialarbeiterische Praxis), Bias_Ungleichheit (Bias-Mitigation) und Fairness (Transparenz, Bias-Adressierung). Intersektionales Denken impliziert, aber nicht explizit feministisch.",2,10,20.00%
147,Development of the “Scale for the assessment of non-experts’ AI literacy” – An exploratory factor analysis,Laupichler (2023),Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Exclude,Include,Nein,Not relevant topic,,Empirisch,Literaturreview,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI, Prompting substantiell behandelt), SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Fairness adressiert). Systematische Review zu Prompt-Engineering-Techniken für Bias-Mitigation gegen arabische/muslimische Minderheiten in LLMs. Direkter Fokus auf kulturelle Diskriminierung und Fairness-Optimierung.",3,10,30.00%
148,Artificial Intelligence in a Structurally Unjust Society,Lin (2022),Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Unclear,Include,Nein,,,Theoretisch,Konzept,0.95,"Paper adressiert substantiell generative KI und LLMs (Generative_KI: Ja), entwickelt ein Framework für kritische Analyse (AI_Literacies: Ja) mit explizitem Fokus auf Prompt-Dokumentation und -Transparenz (Prompting: Ja). Direkter Bezug zu sozialarbeiterischer Forschungspraxis und Journal-Editoren in der Sozialen Arbeit (Soziale_Arbeit: Ja). Alle Bedingungen erfüllt: TECHNIK (3x Ja) UND SOZIAL (1x Ja).",0,10,0.00%
149,Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit,Linnemann (2023),Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Include,Include,Ja,,,Theoretisch,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies, KI_Sonstige, Fairness durch Bias-Thematisierung) und SOZIAL (Soziale_Arbeit, Bias_Ungleichheit, Fairness). Systematische ethische Analyse von KI in Sozialarbeit mit substantiellem Fokus auf algorithmischen Bias, Transparenz und Fairness-Implementierung. Direkt relevant für Sozialarbeitspraxis.",6,10,60.00%
150,Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit: Eine exemplarische arbeitsfeldübergreifende Betrachtung des Natural Language Processing (NLP),Linnemann (2023),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Exclude,Exclude,Ja,Duplicate,Not_relevant_topic,Theoretisch,Experimentell,0.85,"Paper behandelt generative KI (LLMs) substantiell, erfüllt aber KEINE der sozialen Kategorien. Untersucht LLM-Fähigkeiten im Trust-Reasoning ohne Bezug zu Soziale Arbeit, Bias/Ungleichheit, Gender, Diversität, Feminismus oder Fairness. Keine direkte Relevanz für Sozialarbeitskontext oder marginalisierte Gruppen. Rein technische KI-Evaluierungsstudie.",8,10,80.00%
151,Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit: Eine exemplarische arbeitsfeldübergreifende Betrachtung des Natural Language Processing (NLP),Linnemann (2023),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Exclude,Include,Nein,Duplicate,,Theoretisch,Empirisch,0.92,"Paper erfüllt beide Inklusionskriterien: TECHNIK_OK (Generative_KI, Prompting), SOZIAL_OK (Bias_Ungleichheit, Gender, Fairness). Qualitative Studie zu LLM-Bias mit explizitem Gender-Fokus. Prompting wird als Bias-Mitigationsstrategie thematisiert. Kein Sozialarbeitsbezug, keine explizit feministische Theorie.",4,10,40.00%
152,Künstliche Intelligenz in der Sozialen Arbeit: Grundlagen für Theorie und Praxis,Linnemann (2025),Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Include,Exclude,Nein,,"SOZIAL-Kriterium nicht erfüllt: Keine substantielle Behandlung von Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch oder Fairness",Theoretisch,Empirisch,0.95,"Paper erfüllt TECHNIK-Kriterium mit 3 Ja-Kategorien (AI_Literacies: User-Trust-Kompetenz; Generative_KI: LLMs; Prompting: Prompt-Engineering-Vorschläge). Jedoch keine SOZIAL-Kategorien erfüllt: Kein direkter Sozialarbeitsbezug, keine Ungleichheits-, Gender-, Diversitäts-, feministischen oder Fairness-Analysen. Rein kognitive Vertrauensstudie ohne sozialgerechte Dimensionen.",4,10,40.00%
153,More or less wrong: A benchmark for directional bias in LLM comparative reasoning,Liu (2025),Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Include,Exclude,Nein,,Not_relevant_topic,Experimentell,Literaturreview,0.85,"Paper erfüllt TECHNIK-Kriterium (AI_Literacies, KI_Sonstige) und SOZIAL-Kriterium (Bias_Ungleichheit, Fairness). JEDOCH: Thema ist klinische Entscheidungsunterstützung im Gesundheitswesen, NICHT Soziale Arbeit. Keine Verknüpfung zu sozialarbeiterischer Praxis, Theorie oder Zielgruppen. Bias/Fairness-Diskussion bezieht sich auf medizinische Kontexte, nicht auf Soziale Arbeit. Daher: außerhalb des Scope des Literaturreviews zu KI in Sozialer Arbeit.",5,10,50.00%
154,What is AI Literacy? Competencies and Design Considerations,Long (2020),Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Exclude,Exclude,Ja,Not relevant topic,Not_relevant_topic,Konzept,Empirisch,0.85,"Paper behandelt Trust in LLMs und AI Literacies (Vertrauen/Kompetenzwahrnehmung), fokussiert auf generative KI. Prompting wird nur beiläufig erwähnt ('prompt-engineering'), nicht substantiell behandelt. Kritisch: Keine der SOZIAL-Kategorien erfüllt - kein Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität, feministische Perspektive oder Fairness. Rein psychometrische Studie zu Trust-Faktoren ohne sozialen/kritischen Fokus. TECHNIK_OK, aber SOZIAL_NICHT_OK → Exclude.",8,10,80.00%
155,"Algorithms, artificial intelligence and discrimination",Lund (2025),Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Exclude,Exclude,Ja,Wrong publication type,No_relevant_social_dimension,Literaturreview,Empirisch,0.85,"Paper adressiert generative KI (LLMs), Prompting (uncertainty calibration durch Prompt Engineering) und AI Literacy (Vertrauen in LLM-Ausgaben verstehen). Behandelt auch Fairness-Aspekte durch Kalibrierung. ABER: Kein substantieller sozialer Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder Feminist. Perspektive. Es ist eine allgemeine KI-HCI-Studie ohne Sozialarbeitsbezug. Daher: Nur Technik erfüllt, nicht Sozial → Exclude.",6,10,60.00%
156,Intersectional Stereotypes in Large Language Models: Dataset and Analysis,Ma (2023),Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Include,Exclude,Nein,,"SOZIAL_OK nicht erfüllt: Keine substantielle Verbindung zu Sozialer Arbeit, kein expliziter Fokus auf Diskriminierung/Ungleichheit, Gender oder Diversität",Experimentell,Experimentell,0.85,"Paper behandelt AI-Vertrauen, Transparenz und Accountability (AI_Literacies, KI_Sonstige, Fairness) sowie Prompting im Design-Kontext. Erfüllt TECHNIK_OK. Fehlt aber SOZIAL_OK: Kein direkter Bezug zu Sozialer Arbeit/Zielgruppen, keine Analyse von Bias/Ungleichheit oder struktureller Benachteiligung. Fairness bezieht sich auf Transparenz-Metriken, nicht auf systematische Diskriminierung.",3,10,30.00%
157,"Toward Agency‐Centered <span style=""font-variant:small-caps;"">AI</span> Literacy: A Scoping Review",Maeda (2025),Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Include,Include,Ja,,,Literaturreview,Konzept,0.92,Paper adressiert substantiell: (1) Prompting: Prompt-Engineering-Strategien zur Bias-Mitigation und Transparenzförderung sind zentral; (2) Generative_KI: LLMs in sozialer Praxis; (3) Soziale_Arbeit: direkter Bezug zu Case Recommendations und Professional Practice; (4) Bias_Ungleichheit und Fairness: explizit Bias-Mitigation und Trust-Framework; (5) AI_Literacies: Professionelle Kompetenzentwicklung (aktive Direktoren vs. passive Konsumenten). TECHNIK und SOZIAL beide erfüllt → Include.,5,10,50.00%
158,Avoiding catastrophe through intersectionality in global AI governance,McCrory (2024),Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Include,Include,Ja,,,Theoretisch,Empirisch,0.92,"Das Paper erfüllt beide Bedingungen: TECHNIK-Bereich durch Generative_KI (LLMs) und Prompting (12 prompt-engineering Strategien sind substantieller Fokus). SOZIAL-Bereich durch Bias_Ungleichheit (explizit: 'social biases', 'stereotypical responses') und Fairness (Bias-Mitigation, Fairness-Strategien). Kein Feministisch, da keine explizite feministische Theorie verwendet.",2,10,20.00%
159,Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia,Mei (2023),Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Include,Exclude,Nein,,Not_relevant_topic,Experimentell,Experimentell,0.85,"Paper behandelt Chatbot-Transparenz und Nutzerwahrnehmung mit Fokus auf AI Literacy (Effekt bei niedriger Vorkenntnis) und Prompting (Transparenz durch Prompt-Engineering). Technische Kriterien erfüllt. Es fehlt jedoch jeglicher Bezug zu sozialen Kategorien: Keine Soziale Arbeit, keine Analyse von Bias/Ungleichheit, kein Gender-, Diversitäts- oder Fairness-Fokus. Reine HCI-Studie ohne Bezug zu marginalizierten Gruppen, struktureller Benachteiligung oder Sozialarbeitspraxis.",6,10,60.00%
160,Decision support and algorithmic support: The construction of algorithms and professional discretion in social work,Meilvang (2024),Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Exclude,Exclude,Ja,Not relevant topic,"TECHNIK_OK (AI_Literacies, Generative_KI, Prompting, Bias_Ungleichheit, Fairness erfüllt), aber SOZIAL_OK nicht erfüllt: Kein direkter Bezug zu Sozialer Arbeit. Das Paper behandelt LLMs im Healthcare/Klinischen Kontext, nicht in der Sozialen Arbeit. Die genannten SOZIAL-Kategorien (Bias_Ungleichheit, Fairness) sind zwar vorhanden, aber das Papier diskutiert primär technische/berufliche Aspekte in der Medizin, nicht sozialarbeiterische Praxis, Theorie oder deren Zielgruppen.",Unclear,Konzept,0.85,"Das Paper behandelt substantiell LLMs (Generative_KI), Prompt-Engineering für Transparenz (Prompting), AI-Literacies (kritisches Vertrauen, Deskilling), sowie Bias und Fairness. Allerdings adressiert es Healthcare-Professionals (Kliniker), nicht Soziale Arbeit oder deren Zielgruppen. Für einen Literature Review zu KI in der Sozialen Arbeit ist die fehlende Konnexion zur Sozialen Arbeit disqualifizierend.",4,10,40.00%
161,"Failing our youngest: On the biases, pitfalls, and risks in a decision support algorithm used for child protection",Moreau (2024),Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Exclude,Exclude,Ja,Not relevant topic,Not_relevant_topic,Experimentell,Theoretisch,0.85,"Das Paper behandelt algorithmische Trustworthiness und Vertrauenskonzepte in KI-Systemen (KI_Sonstige = Ja). Es erfüllt aber keine sozialen Kategorien: Kein expliziter Bezug zu Sozialer Arbeit, keine Analyse von Bias/Ungleichheit, kein Gender- oder Diversitätsfokus, keine feministische Perspektive, keine Fairness-Metriken. Es ist rein theoretisch-konzeptuell ohne Anwendungsbezug zu gesellschaftlich vulnerable Gruppen. Daher TECHNIK erfüllt, SOZIAL nicht erfüllt → Exclude.",6,10,60.00%
162,Feministische Netzpolitik und Künstliche Intelligenz in der politischen Bildung [Feminist network politics and artificial intelligence in political education],Mosene (2023),Ja,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Exclude,Include,Nein,Wrong publication type,,Theoretisch,Konzept,0.92,"Paper erfüllt beide Bedingungen: (1) TECHNIK: AI_Literacies (Rahmenmodell für KI-Integration in Praxis), KI_Sonstige (algorithmische Systeme im Sozialbereich), Fairness (Bias-Mitigation, Transparenz). (2) SOZIAL: Soziale_Arbeit (direkter Fokus auf Integrationsstrategie in sozialarbeiterischer Praxis), Bias_Ungleichheit (explizite Behandlung von Bias-Mitigation), Fairness. EPIC-Modell ist substantielle konzeptuelle Beitrag mit Ethik, Policy und Community-Perspektive für KI in Sozialer Arbeit.",5,10,50.00%
163,"Biases in large language models: Origins, inventory and discussion",Navigli (2023),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Include,Exclude,Nein,,Not_relevant_topic,Experimentell,Experimentell,0.85,"Das Paper behandelt LLMs (Generative_KI = Ja) und untersucht deren Fähigkeiten zur Analyse von Vertrauen. Es erfüllt aber KEINE der erforderlichen SOZIAL-Kategorien: Kein Bezug zu Sozialer Arbeit, keine Behandlung von Bias/Ungleichheit, Gender, Diversität, feministischen Perspektiven oder Fairness. Die Studie zu Trust-Reasoning ist rein technisch-kognitiv ausgerichtet, ohne sozialkritischen oder anwendungsbezogenen Fokus auf vulnerable Gruppen oder strukturelle Ungleichheiten.",5,10,50.00%
164,Conceptualizing AI literacy: An exploratory review,Ng (2021),Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Include,Include,Ja,,,Konzept,Literaturreview,0.95,"Paper erfüllt TECHNIK-Bedingung (Generative_KI=Ja, Prompting=Ja) und SOZIAL-Bedingung (Bias_Ungleichheit=Ja, Diversitaet=Ja, Fairness=Ja). Systematische Review zu Prompt-Engineering-Techniken zur Reduktion kultureller Vorurteile gegen Araber und Muslime in LLMs mit substantiellem Fokus auf Fairness und strukturelle Diskriminierungsprobleme.",6,10,60.00%
165,Conceptualizing AI literacy: An exploratory review,Ng (2021),Ja,Ja,Ja,Ja,Ja,?,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Exclude,Include,Nein,Duplicate,,Konzept,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, Prompting=Ja, AI_Literacies=Ja durch Fokus auf Bildung und kritisches Verständnis von LLMs) und SOZIAL_OK (Soziale_Arbeit=Ja, direkter Bezug zu Forschung und Praxis in Social Work). Framework für verantwortungsvollen KI-Einsatz mit Dokumentations- und Transparenzanforderungen ist substantiell.",2,10,20.00%
166,Conceptualizing AI literacy: An exploratory review,Ng (2021),Ja,Ja,Ja,Ja,Ja,?,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Exclude,Exclude,Ja,Duplicate,"Only TECHNIK erfüllt, nicht SOZIAL. Paper behandelt AI-Kompetenz (Trust/Reliance Management), KI-Systeme (Decision Support) und Fairness im KI-Kontext. Es fehlt aber jeglicher Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder feministischen Perspektiven. Die Fairness-Dimension ist rein technisch (faire Gewichtung von User Trust) ohne sozialen Impact-Fokus.",Konzept,Experimentell,0.95,"Paper adressiert Mensch-KI-Interaktion und Trust-Adaptivität in Decision-Support-Systemen (AI_Literacies, KI_Sonstige, Fairness). Allerdings fehlt der erforderliche SOZIAL-Anker komplett: kein Bezug zu Soziale Arbeit, keine Analyse von strukturellen Ungleichheiten, keine marginalisierten Gruppen im Fokus. Fairness wird rein technisch/kognitiv behandelt.",6,10,60.00%
167,Using digital story writing as a pedagogy to develop AI literacy among primary students,Ng (2022),Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Include,Include,Ja,,,Konzept,Theoretisch,0.95,Das Paper behandelt systematisch ethische Herausforderungen von KI in der Sozialen Arbeit und entwickelt einen umfassenden Implementierungsrahmen. TECHNIK_OK: AI_Literacies (Kompetenzrahmen für ethische KI-Implementierung) + KI_Sonstige (algorithmische Systeme im Sozialkontext). SOZIAL_OK: Soziale_Arbeit (direkter Bezug zu SW-Praxis und -Ethik) + Bias_Ungleichheit (algorithmischer Bias wird thematisiert) + Fairness (Framework-Entwicklung für faire KI). Beide Bedingungen erfüllt → Include.,6,10,60.00%
168,"Opportunities, challenges and school strategies for integrating generative AI in education",Ng (2025),Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Include,Exclude,Nein,,Not_relevant_topic,Empirisch,Empirisch,0.85,"Paper behandelt LLM-Vertrauen und individuelle Unterschiede (AI_Literacies, Generative_KI erfüllt). ABER: Kein Bezug zu sozialen Ungleichheiten, Bias, Gender, Diversität, Fairness oder Sozialer Arbeit. Prompting wird nur beiläufig erwähnt ('prompt-engineering'), nicht substantiell behandelt. Psychologische Vertrauensforschung ohne sozialen/kritischen Bezug. SOZIAL-Kriterien nicht erfüllt → Exclude.",8,10,80.00%
169,The role of artificial intelligence (AI) and machine learning in social work practice,Nuwasiima (2024),Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Include,Exclude,Nein,,Wrong_publication_type / Not_relevant_topic,Literaturreview,Literaturreview,0.85,"Paper behandelt AI-Kompetenzen (Vertrauen, Transparenzverstaendnis) und KI-Sonstige (klinische Entscheidungssysteme) sowie Bias/Fairness. ABER: Fokus liegt auf Healthcare/klinischem Kontext, nicht auf Soziale Arbeit. Keine direkter Bezug zu sozialarbeiterischer Praxis, Theorie oder Zielgruppen. SOZIAL-Kriterium nicht erfuellt: Bias_Ungleichheit und Fairness im Healthcare-Kontext sind nicht spezifisch fuer Soziale Arbeit. Paper adressiert Technik + allgemeine Ethik, aber nicht die sozialen Dimensionen des Literaturreviews (Soziale Arbeit, Gender, Diversitaet, Feministisch).",8,10,80.00%
170,ReflectAI: Design and evaluation of an AI coach to support public servants' self-reflection,Näscher (2025),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Nein,Ja,?,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Unclear,Include,Nein,,,Empirisch,Empirisch,0.95,"Paper erfüllt TECHNIK-Kriterien durch Generative_KI (ChatGPT-Fokus) und Prompting (Empfehlungen zu clarifying prompts). Erfüllt SOZIAL-Kriterien durch expliziten Gender-Fokus (unterschiedliche Geschlechteridentitäten), Bias_Ungleichheit (stereotype responses, verringerte Vertrauenswerte) und Fairness (Bias-Mitigation-Strategien). Qualitative empirische Studie mit substantiellem Bias- und Gender-Fokus.",5,10,50.00%
171,Advancing Accountability in AI,OECD (2023),Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Include,Exclude,Nein,,Not_relevant_topic,Theoretisch,Empirisch,0.85,"Das Paper behandelt substantiell AI_Literacies (Vertrauen, Kompetenzen im Umgang mit LLMs) und Generative_KI sowie Prompting-Strategien. Jedoch fehlt vollständig der zweite erforderliche Bereich (Sozial): keine direkte Soziale_Arbeit, keine Bias_Ungleichheit, kein Gender/Diversität/Feminismus/Fairness-Fokus. Es ist eine reine Nutzerpsychologie-Studie zu LLM-Vertrauen ohne sozialen Impact-Fokus. Die Prompt-Engineering-Erwähnung ist eher ein Implikation, nicht substantiell. TECHNIK_OK=Ja, SOZIAL_OK=Nein → Exclude.",4,10,40.00%
172,Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness,Ovalle (2023),Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Unclear,Exclude,Nein,,"TECHNIK_OK (AI_Literacies, Generative_KI, Prompting, Fairness: alle Ja), aber SOZIAL_OK nicht erfuellt. Keine der Kategorien Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet oder Feministisch erreicht Ja-Status. Fairness allein genuegt nicht fuer SOZIAL_OK.",Literaturreview,Empirisch,0.95,"Das Paper behandelt AI Literacy (Benutzervertrauen, Kalibrierung), Generative KI (LLMs) und Prompting-Strategien (Unsicherheitskonveyance). Fairness-relevant bezueglich Transparenz. Jedoch kein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversitaet oder feministischer Perspektive. Allgemeine KI-HCI-Studie ohne soziale Gerechtigkeitsdimension.",3,10,30.00%
173,Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness,Ovalle (2023),Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Exclude,Exclude,Ja,Duplicate,"TECHNIK_OK (AI_Literacies, Generative_KI, Prompting, Fairness, Bias_Ungleichheit), aber SOZIAL_OK nur durch Fairness und Bias_Ungleichheit erfüllt. Kein direkter Bezug zu Sozialer Arbeit, Gender, Diversität oder feministischer Perspektive. Healthcare-Kontext ist nicht gleichzusetzen mit Sozialer Arbeit. Paper adressiert allgemeine KI-Ethik und Bias im Healthcare-Kontext, nicht die definierten SOZIAL-Kategorien mit substantiellem Fokus auf marginalisierte Gruppen oder sozialarbeiterische Praxis.",Literaturreview,Konzept,0.75,"Das Paper behandelt LLMs, Prompting und Bias-Mitigation substantiell (TECHNIK erfüllt). Es adressiert Fairness und Bias-Ungleichheit. Jedoch ist Healthcare-Kontext nicht identisch mit Sozialer Arbeit, und es gibt keinen expliziten Fokus auf marginalisierte Gruppen oder strukturelle Ungleichheit im Sozialbereich. Daher nur Exclude, da die definierten SOZIAL-Kategorien nicht ausreichend substantiell erfüllt sind.",4,10,40.00%
174,Factoring the matrix of domination: A critical review and reimagination of intersectionality in AI fairness,Ovalle (2023),Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Exclude,Include,Nein,Duplicate,,Literaturreview,Empirisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, Prompting=Ja) und SOZIAL_OK (Bias_Ungleichheit=Ja, Fairness=Ja). Substantieller Fokus auf Prompt-Engineering-Strategien zur Bias-Reduktion in LLMs. Direkte Behandlung sozialer Bias und Fairness-Ansätze. Nicht auf Soziale Arbeit fokussiert, aber klar KI-Bias-Thematik mit fairness-Implikation.",5,10,50.00%
175,Towards Substantive Equality in Artificial Intelligence: Transformative AI Policy for Gender Equality and Diversity,Ovalle (2024),Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Include,Exclude,Nein,,"TECHNIK erfüllt (AI_Literacies, Prompting, KI_Sonstige, Fairness), aber SOZIAL nicht erfüllt: Kein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder Feminismus. Paper behandelt algorithmische Transparenz und Vertrauen in allgemeinem organisatorischen Kontext, nicht in sozialarbeiterischer Praxis oder mit explizitem Fokus auf marginalisierte Gruppen/strukturelle Ungleichheit.",Konzept,Experimentell,0.85,"Paper adressiert AI-Vertrauen, Transparenz und Fairness-Aspekte (TECHNIK-Seite erfüllt). Jedoch fehlt der Sozialarbeitsbezug und kein expliziter Fokus auf Ungleichheit oder Marginalisierung. Allgemeine organisationale Trust-Studie ohne Sozialbereich-Spezifika.",4,10,40.00%
176,AI literacy and trust: A multi-method study of Human-GAI team collaboration,Pan (2025),Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Include,Exclude,Nein,,SOZIAL-Kriterium nicht erfüllt,Empirisch,Experimentell,0.92,"Paper erfüllt TECHNIK-Kriterium (AI_Literacies: Ja - Nutzer-Kompetenzen/Wissen über KI; Generative_KI: Ja - Chatbot-fokussiert; Prompting: Ja - Transparenz-Strategien für Prompt-Engineering erwähnt). Aber SOZIAL-Kriterium nicht erfüllt: Kein direkter Bezug zu Sozialer Arbeit, keine substantielle Behandlung von Bias, Gender, Diversität, Feminismus oder Fairness. Allgemeine Trust/UX-Studie ohne Fokus auf soziale Ungleichheit oder marginalisierte Gruppen.",8,10,80.00%
177,LIBRA: Measuring bias of large language model from a local context,Pan (2025),Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Include,Exclude,Nein,,No_social_work_relevance,Experimentell,Empirisch,0.85,"Paper behandelt Trust-Management in AI-Systemen und Fairness durch adaptive Interventionen (TECHNIK: AI_Literacies, KI_Sonstige, Fairness erfüllt). Es befasst sich jedoch nicht mit sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder feministischen Perspektiven (SOZIAL nicht erfüllt). Fokus liegt auf allgemeiner Human-AI Collaboration ohne direkten Bezug zu sozialarbeiterischen Kontexten.",3,10,30.00%
178,"AI algorithm transparency, pipelines for trust not prisms: mitigating general negative attitudes and enhancing trust toward AI",Park (2025),Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Include,Include,Ja,,,Experimentell,Konzept,0.92,"Paper erfüllt beide Inklusionskriterien: TECHNIK (AI_Literacies, KI_Sonstige) durch EPIC-Framework für KI-Integration und Bias-Mitigation; SOZIAL (Soziale_Arbeit, Bias_Ungleichheit, Fairness) durch direkten Bezug zu Sozialer Arbeit, Ethik, Transparenz und Fairness-Thematisierung. Substantielle Behandlung von KI-Kompetenzen in Sozialarbeit und strukturierter Fairness-Ansatz.",6,10,60.00%
179,"AI algorithm transparency, pipelines for trust not prisms: mitigating general negative attitudes and enhancing trust toward AI",Park (2025),Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Exclude,Exclude,Ja,Duplicate,Not_relevant_topic,Experimentell,Theoretisch,0.85,"Das Paper behandelt AI trustworthiness aus theoretischer, KI-ethischer Perspektive (KI_Sonstige: Ja). Es bietet jedoch keinen Bezug zu sozialen Anwendungsdomänen, Bias, Gender, Diversität, Fairness oder Soziale Arbeit. Es ist eine rein konzeptionelle Formalisierung von Vertrauen in KI-Systemen ohne direkten Bezug zu sozialen Implikationen oder marginalizierten Gruppen. SOZIAL-Komponente nicht erfüllt → Exclude.",6,10,60.00%
180,BBQ: A hand-built bias benchmark for question answering,Parrish (2022),Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Include,Exclude,Nein,,Not_relevant_topic,Empirisch,Experimentell,0.85,"Paper untersucht LLM-Fähigkeiten zur Vertrauensanalyse zwischen Individuen. Während Generative_KI (LLMs) relevant ist, fehlt jeglicher substantieller Bezug zu den sozialen Kategorien: kein direkter Sozialarbeitsbezug, keine Bias/Ungleichheit-Analyse, kein Gender-, Diversitäts- oder Fairness-Fokus. Die Vertrauensforschung ist allgemein sozialwissenschaftlich, nicht sozialarbeiterisch spezifisch. TECHNIK_OK, aber SOZIAL_NICHT_OK → Exclude.",4,10,40.00%
181,Self-debiasing large language models: Zero-shot recognition and reduction of stereotypes,Parrish (2025),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Include,Include,Ja,,,Experimentell,Empirisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting), SOZIAL_OK (Bias_Ungleichheit + Fairness). Substantieller Fokus auf Prompt-Engineering-Strategien zur Bias-Reduktion in LLMs mit empirischer Evaluation. Adressiert soziale Bias und Fairness-Aspekte von KI-Systemen, nicht aber Soziale Arbeit oder feministische Perspektiven.",7,10,70.00%
182,ChatGPT for Social Work Science: Ethical Challenges and Opportunities,Patton (2023),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Include,Include,Ja,,,Theoretisch,Literaturreview,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, Prompting=Ja: systematische Analyse von Prompt-Engineering-Techniken für LLMs). SOZIAL_OK (Bias_Ungleichheit=Ja: kultureller Bias gegen Araber und Muslime; Diversitaet=Ja: kulturelle Repräsentation; Fairness=Ja: Bias-Reduktion und Fairness-Metriken). Substantieller Fokus auf Prompt-Strategien zur Bias-Mitigation in LLMs.",7,10,70.00%
183,A Literature Review of Digital Literacy over Two Decades,Peng (2022),Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Exclude,Include,Nein,Not relevant topic,,Konzept,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, Prompting=Ja, AI_Literacies=Ja durch Fokus auf Wissen/Kompetenzen für Forschende) und SOZIAL_OK (Soziale_Arbeit=Ja durch expliziten Bezug zu Social-Work-Forschung und -Praxis). Entwickelt Framework für KI-Einsatz in Sozialarbeit mit Empfehlungen für Transparenz, Dokumentation und kontinuierliche Bildung.",6,10,60.00%
184,Prejudiced interactions with large language models (LLMs) reduce trustworthiness and behavioral intentions among members of stigmatized groups,Petzel (2025),Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Include,Include,Ja,,,Experimentell,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies + KI_Sonstige: Bezug zu ethischen Rahmenbedingungen und Implementierung von KI-Systemen in sozialer Arbeit) und SOZIAL_OK (Soziale_Arbeit + Bias_Ungleichheit + Fairness: Direkter Bezug zu Sozialarbeitspraxis, explizite Behandlung von algorithmischen Bias und Fairness-Anforderungen). Das Paper entwickelt systematisch ethische Richtlinien und einen Implementierungsrahmen für KI in der Sozialen Arbeit.",2,10,20.00%
185,AI Literacy - Towards Measuring Human Competency in Artificial Intelligence,Pinski (2023),Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Exclude,Exclude,Ja,Not relevant topic,"Technik erfüllt (Generative_KI, Prompting, Bias_Ungleichheit, Fairness), aber Sozial-Dimension nicht erfüllt. Paper behandelt Bias in LLMs ohne direkten Bezug zu Sozialer Arbeit, struktureller Ungleichheit oder marginalisierten Gruppen im sozialen Kontext.",Empirisch,Empirisch,0.85,"Paper adressiert impliziten Bias in Transformer-Sprachmodellen mittels Embedding Association Test und Prompt Engineering. Dies deckt Generative_KI, Prompting und Bias ab. Jedoch fehlt der Sozial-Kontext: Keine Bezüge zu Sozialer Arbeit, struktureller Benachteiligung oder konkreten sozialen Auswirkungen. Bias-Analyse ohne Kontextualisierung sozialer Implikationen erfüllt nicht die SOZIAL-Kriterien.",4,10,40.00%
186,AI Literacy for the top management: An upper echelons perspective on corporate AI orientation and implementation ability,Pinski (2024),Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Exclude,Exclude,Ja,Not relevant topic,Not_relevant_topic,Empirisch,Unclear,0.95,Dies ist ein Statistikbericht des U.S. Bureau of Labor Statistics über Beschäftigungsstatistiken nach Beruf. Es behandelt weder KI-Themen (TECHNIK-Kriterien nicht erfüllt) noch soziale Gerechtigkeit/Bias-Probleme im KI-Kontext (SOZIAL-Kriterien nicht erfüllt). Kein Bezug zu KI-Systemen oder deren Auswirkungen erkennbar.,8,10,80.00%
187,"AI literacy for users – A comprehensive review and future research directions of learning methods, components, and effects",Pinski (2024),Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Include,Include,Ja,,,Literaturreview,Unclear,0.65,"Das Paper behandelt 'Homoglyph unlearning' als Bias-Mitigationsmethode (KI_Sonstige: Ja, technischer Fokus). Der Titel signalisiert explizit Bias Mitigation (Bias_Ungleichheit + Fairness: Ja). Allerdings fehlt das Abstract für vollständige Bewertung. TECHNIK_OK (KI_Sonstige) + SOZIAL_OK (Bias/Fairness) → Include. Confidence moderat wegen fehlender Kontextinformationen.",5,10,50.00%
188,Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,Prakash (2023),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Include,Exclude,Nein,,No_full_text,Literaturreview,Unclear,0.3,"Titel deutet auf Generative KI und Bias hin, aber ohne Abstract oder Volltext ist die Einordnung spekulativ. Sicher sind: Generative_KI=Ja (ChatGPT/Claude), Bias_Ungleichheit=Ja (Titel erwähnt 'Bias'). ABER: Kein Bezug zu Sozialer Arbeit erkennbar. Die Themen 'ethical dilemmas' sind philosophisch, nicht sozialarbeitsbezogen. Daher TECHNIK_OK (Generative_KI), aber SOZIAL nicht erfüllt (Bias allein ohne Sozialarbeitsbezug reicht nicht, und ohne Text kann nicht überprüft werden, ob tatsächlicher Fokus auf Ungleichheit besteht). Fehlende vollständige Information → Exclusion.",7,10,70.00%
189,DR.GAP: Mitigating bias in large language models using gender-aware prompting with demonstration and reasoning,Qiu (2025),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Include,Exclude,Nein,,Not_relevant_topic,Experimentell,Literaturreview,0.85,"Paper behandelt Fairness in Large Language Models (Generative_KI: Ja, Fairness: Ja, Bias_Ungleichheit: Ja). Jedoch fehlt der erforderliche SOZIAL-Kontext: Es gibt keinen direkten Bezug zu Sozialer Arbeit, Gender-Perspektive, Diversität oder feministischen Ansätzen. Allgemeine Fairness-Diskussionen in KI ohne Sozialarbeitsbezug sind nicht relevant für das Literature Review zu KI und Sozialer Arbeit. TECHNIK-Bedingung erfüllt, aber SOZIAL-Bedingung nicht erfüllt → Exclude.",6,10,60.00%
190,Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy,Quaid-i-Azam University (2025),Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Exclude,Exclude,Ja,Duplicate,"No_full_text - Kein Abstract verfügbar. Trotz erkennbarem KI-Fokus (Generative_KI, KI_Sonstige, Bias_Ungleichheit) kann ohne Textinhalt nicht verifiziert werden, ob eine substantielle Sozialbezug vorliegt. Die verfügbaren Informationen deuten auf eine rein technische Fehleranalyse hin.",Literaturreview,Unclear,0.4,"Der Titel deutet auf eine technische Vergleichsstudie zu Sprachmodellen und Vision-Language-Modellen hin. Generative_KI und KI_Sonstige sind relevant; 'error' könnte auf Bias hinweisen. Allerdings: Ohne Abstract/Volltext kann nicht eingeschätzt werden, ob ein Sozialkontext (Soziale_Arbeit, Bias_Ungleichheit mit sozialem Fokus) substantiell adressiert wird. Reine technische Fehleranalyse würde die SOZIAL-Bedingung nicht erfüllen.",5,10,50.00%
191,Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy,Quaid-i-Azam University (2025),Ja,Ja,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,Exclude,Include,Nein,Duplicate,,,Unclear,0.72,"Das Paper behandelt Bias-Mitigation in LLMs mit Fokus auf altersbedingte Diskriminierung. Generative_KI (LLMs), Bias_Ungleichheit (Age Bias), Diversitaet (Age-related groups), und Fairness (Debiasing-Strategien) sind substantiell adressiert. TECHNIK_OK (Generative_KI=Ja) und SOZIAL_OK (Bias_Ungleichheit=Ja, Diversitaet=Ja, Fairness=Ja) erfüllt. Ohne Abstract ist Zuordnung unsicher, aber Titel deutet auf relevantes Thema hin.",0,0,
192,The Algorithmic Auditing Landscape: A Social Justice Approach,Raji (2024),Ja,Ja,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,Exclude,Include,Nein,Other,,,Unclear,0.75,"Das Paper adressiert explizit Bias-Mitigation in KI-generierten Narrativen mit Fokus auf Gender und Ethnizität. Generative_KI (narratives generation), Bias_Ungleichheit (gender and ethnicity bias), Gender (expliziter Gender-Fokus), Diversitaet (ethnicity representation) und Fairness (bias mitigation) sind substantiell behandelt. TECHNIK (Generative_KI) und SOZIAL (Bias_Ungleichheit, Gender, Fairness) sind erfüllt → Include. Confidence begrenzt durch fehlenden Abstract und unklaren Studientyp.",0,0,
193,Artificial intelligence in social work: Emerging ethical issues,Reamer (2023),Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Include,Include,Ja,,,Theoretisch,Literaturreview,0.85,"Das Paper behandelt substantiell Prompting-Techniken (TECHNIK) zur Bias-Mitigation gegen Arabs/Muslims in LLMs (SOZIAL). Erfüllt beide Bedingungen: Generative_KI + Prompting (TECHNIK) und Bias_Ungleichheit + Diversitaet + Fairness (SOZIAL). Kein expliziter Soziale_Arbeit-Bezug, aber kulturelle Diskriminierung und algorithmische Fairness sind relevant für Inklusion und soziale Gerechtigkeit.",6,10,60.00%
194,Artificial Intelligence in Social Work: Emerging Ethical Issues,Reamer (2023),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Exclude,Exclude,Ja,Duplicate,"Technik erfüllt (Generative_KI, Prompting), aber Sozial nicht erfüllt: Paper adressiert Bias und Fairness auf technischer Ebene, nicht im Kontext Sozialer Arbeit, sozialer Ungleichheit strukturell oder feministischer Perspektiven.",Theoretisch,Empirisch,0.85,"Das Paper behandelt deutlich Generative KI (LLMs) und Prompting-Strategien zur Bias-Reduktion (Self-Debiasing). Es adressiert auch Fairness und Bias. Jedoch: Keine direkten sozialen Aspekte im Sinne der Review-Kriterien (Soziale_Arbeit, strukturelle Ungleichheit, Gender-Fokus, Diversität, oder feministische Perspektive). Es bleibt eine technisch-methodische Studie ohne sozialarbeiterischen oder strukturellen sozialen Kontext.",6,10,60.00%
195,Artificial intelligence in social work: Emerging ethical issues,Reamer (2023),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Exclude,Include,Nein,Duplicate,,Theoretisch,Empirisch,0.85,Das Paper untersucht Bias in GPT (Generative_KI: Ja) durch eine intersektionale Fallstudie (Feministisch: Ja - Intersektionalitätsansatz nach Crenshaw). Es thematisiert Diskriminierung gegen stigmatisierte Gruppen (Bias_Ungleichheit: Ja) und Diversität von Gruppen (Diversitaet: Ja). Fairness ist impliziert durch Bias-Analyse. TECHNIK und SOZIAL sind erfüllt → Include.,5,10,50.00%
196,Artificial intelligence in social work: Emerging ethical issues,Reamer (2023),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Exclude,Exclude,Ja,Duplicate,Not_relevant_topic,Theoretisch,Experimentell,0.85,"Paper behandelt Bias und Fairness in LLMs durch ein Benchmark für direktionalen Bias. Dies erfüllt TECHNIK-Bedingung (Generative_KI, Bias_Ungleichheit, Fairness). JEDOCH: Keine Verbindung zu Soziale_Arbeit erkennbar. Das Paper fokussiert auf technische Bias-Metriken in LLMs, nicht auf sozialarbeiterische Praxis oder deren Zielgruppen. Reine KI-Fairness-Technologie ohne sozialen Anwendungskontext → Exclude.",5,10,50.00%
197,Artificial intelligence in social work: Emerging ethical issues,Reamer (2023),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Exclude,Exclude,Ja,Duplicate,"No_full_text; Titel deutet auf technisches Framework für LLM-basierte Systeme hin, ohne erkennbaren Bezug zu sozialen/arbeitsspezifischen Kontexten. Keine Abstract verfügbar für detaillierte Bewertung, aber basierend auf Titel: TECHNIK_OK (Generative_KI + Prompting), aber SOZIAL_NOT_OK (keine Kategorien erfüllt).",Theoretisch,Unclear,0.4,"Paper adressiert offenbar generative KI und Prompting-Strategien (Dual-Module Framework für Action Planning mit LLMs). Jedoch: Kein Abstract, fehlender Volltext. Titel suggeriert rein technisches ML-Framework ohne erkennbaren Bezug zu Sozialer Arbeit, Fairness, Bias oder verwandten sozialen Dimensionen. Keine der SOZIAL-Kategorien erfüllt.",4,10,40.00%
198,How can feminism inform AI governance in practice?,Ricaurte (2024),Ja,Ja,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,Exclude,Include,Nein,Wrong publication type,,,Experimentell,0.85,"BBQ ist ein etablierter Benchmark für Bias-Evaluationen in QA-Systemen (KI-Sonstige: Ja; Generative_KI: Ja, da oft für LLM-Evaluation genutzt). Das Paper adressiert substantiell Bias, Fairness und Diversität mit explizitem Gender-Fokus. Keine Soziale_Arbeit. Technik+Sozial erfüllt → Include.",0,0,
199,Towards substantive equality in artificial intelligence: Transformative AI policy for gender equality and diversity,Ricaurte Quijano (2024),Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Unclear,Exclude,Nein,,Not_relevant_topic,Konzept,Experimentell,0.95,"Paper behandelt Chain-of-Thought Prompting für LLMs (Generative_KI=Ja, Prompting=Ja). Dies ist eine rein technische Methoden-Studie ohne Bezug zu sozialen Themen. Keine Kategorien aus dem SOZIAL-Set (Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness) sind erfüllt. Damit fehlt die SOZIAL_OK-Bedingung für Inklusion.",0,10,0.00%
200,Introducing Generative Artificial Intelligence into the MSW Curriculum: A Proposal for the 2029 Educational Policy and Accreditation Standards,Rodriguez (2024),Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Include,Exclude,Nein,,Not_relevant_topic,Konzept,Unclear,0.6,"Paper behandelt Steerability von LLMs (Generative_KI: Ja, Prompting: Ja). Dies sind Technik-Kategorien. Jedoch fehlt jeglicher Bezug zu sozialen Aspekten (Bias, Fairness, Soziale Arbeit, etc.). Ohne Abstract kann keine substantielle Einschätzung des Inhalts erfolgen, aber auf Basis des Titels liegt ein rein technisches Paper zu LLM-Kontrollierbarkeit vor. TECHNIK_OK, aber SOZIAL nicht erfüllt → Exclude.",3,10,30.00%
201,Ethical issues related to the use of technology in social work practice: A systematic review,Rodríguez-Martínez (2024),Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Exclude,Include,Nein,Not relevant topic,,Literaturreview,Empirisch,0.85,"Paper fokussiert auf Prompting-Techniken (substantiell im Titel) und Generative KI (LLMs). Adressiert social bias reduction - direkt relevant für Bias_Ungleichheit und Fairness. TECHNIK erfüllt (Generative_KI + Prompting), SOZIAL erfüllt (Bias_Ungleichheit + Fairness). Kein Soziale_Arbeit-Bezug, aber algorithmische Bias-Reduktion ist relevant für LLM-Fairness insgesamt.",5,10,50.00%
202,"AI Literacy: A Framework to Understand, Evaluate, and Use Emerging Technology",Ruiz (2024),Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Include,Exclude,Nein,,No_full_text; nur Titel verfügbar - Klassifikation unter Unsicherheit,Konzept,Unclear,0.35,"Der Titel deutet auf LLM-basierte Klassifikation von Mentalhygiene-Daten hin (Generative_KI + KI_Sonstige erfüllt, TECHNIK_OK). Jedoch: kein Abstract verfügbar, daher nicht erkennbar, ob ein substantieller Bezug zu sozialen/ethischen Dimensionen (SOZIAL_OK) besteht. Mental Health ist nicht automatisch Soziale Arbeit. Ohne Volltext: konservative Einstufung, da SOZIAL-Dimensionen nicht verifiable sind.",5,10,50.00%
203,Model explanations for gender and ethnicity bias mitigation in AI-generated narratives,Salecha (2025),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Include,Include,Ja,,,Experimentell,Empirisch,0.75,Paper analysiert Stereotypen in LLMs aus soziolinguistischer Perspektive. Erfüllt TECHNIK-Bedingung durch Generative_KI (Fokus auf LLMs). Erfüllt SOZIAL-Bedingung durch Bias_Ungleichheit (Stereotype-Assessment) und Gender (implizit durch Stereotype-Analyse). Sociolinguistischer Zugang deutet auf Analyse von Sprachmustern und sozialen Kategorien hin. Ohne Abstract: Confidence moderate.,7,10,70.00%
204,What’s in a name? Auditing large language models for race and gender bias,Salinas (2025),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Include,Exclude,Nein,,"SOZIAL-Bedingung nicht erfüllt: Paper adressiert Bias und Gender in generativen KI-Systemen, hat aber keinen Bezug zu Sozialer Arbeit, marginalisierten Communities oder strukturellen Ungleichheiten im sozialen Kontext. Es ist eine rein technische Studie zu Fairness in LLMs ohne sozialpolitische/sozialarbeiterische Dimension.",Experimentell,Experimentell,0.92,"Paper behandelt Prompting-Strategien zur Reduktion von Gender-Bias in LLMs—klare technische Komponente mit Fairness-Fokus. Allerdings fehlt jeder Bezug zu Sozialer Arbeit, Zielgruppen sozialer Dienste oder strukturellen sozialen Implikationen. Es ist eine Standard-Debiasing-Studie ohne sozialarbeitlichen Kontext.",9,10,90.00%
205,The power of prompts: Evaluating and mitigating gender bias in MT with LLMs,Sant (2024),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Include,Include,Ja,,,Experimentell,Empirisch,0.85,"Das Paper untersucht systematisch Bias in GPT gegenüber stigmatisierten Gruppen (Generative_KI: Ja). Der Fokus auf Nationalität und psychische Erkrankungen adressiert Bias und Ungleichheit (Bias_Ungleichheit: Ja). Der explizit erwähnte intersektionale Ansatz deutet auf feministische Methodik hin (Feministisch: Ja, intersektional nach Crenshaw). Die Analyse marginalisierter Gruppen und deren Repräsentation (Diversitaet: Ja) sowie Fairness-Bezug (Fairness: Ja) sind substantiell. TECHNIK (Generative_KI) ✓ + SOZIAL (Bias_Ungleichheit, Diversitaet, Feministisch, Fairness) ✓ = Include.",5,10,50.00%
206,Explainability through systematicity: The hard systematicity challenge,Santos (2024),Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Include,Include,Ja,,,Unclear,Theoretisch,0.75,"Paper adressiert Counterfactual Fairness in Text Classification - ein KI/ML-Thema (KI_Sonstige: Ja). Fokus auf Fairness und Robustheit deutet auf Bias-Mitigation hin (Fairness: Ja, Bias_Ungleichheit: Ja). TECHNIK und SOZIAL sind erfüllt. Ohne Abstract ist Confidence begrenzt, aber Titel und Autorschaft (Garg et al., bekannt für Fairness in ML) stützen Klassifizierung.",7,10,70.00%
207,How large language models judge cooperation,Santos (2025),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Include,Include,Ja,,,Empirisch,Experimentell,0.85,"Das Paper behandelt Sprachmodelle (Generative_KI: Ja) mit explizitem Fokus auf Bias-Mitigation und Fairness (Bias_Ungleichheit: Ja, Fairness: Ja). Der Titel signalisiert einen technischen Ansatz zur Reduktion von Bias durch Fine-Tuning. TECHNIK_OK (Generative_KI) und SOZIAL_OK (Fairness + Bias_Ungleichheit erfüllt). Include-Kriterien sind erfüllt, trotz fehlenden Sozialen-Arbeit-Bezugs.",7,10,70.00%
208,NLPositionality: Characterizing design biases of datasets and models,Santy (2023),Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Include,Include,Ja,,,Konzept,Empirisch,0.75,"Paper behandelt Bias-Mitigation und Fairness in ML-Systemen (KI_Sonstige: Ja, Bias_Ungleichheit: Ja, Fairness: Ja). Dies ist ein etabliertes Paper zur algorithmischen Fairness. TECHNIK-Bedingung erfüllt (KI_Sonstige), SOZIAL-Bedingung erfüllt (Bias_Ungleichheit + Fairness). Kein Sozialarbeitsbezug erkannt, aber Fairness-Fokus ist substantiell genug für Inclusion im breiter definierten KI-Ethik-Kontext.",8,10,80.00%
209,Der Einfluss der Algorithmen: Neue Qualitäten durch Big Data Analytics und Künstliche Intelligenz,Schneider (2018),Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Exclude,Include,Nein,Not relevant topic,,Theoretisch,Unclear,0.75,"Das Paper behandelt Large Language Models (Generative_KI=Ja) und fokussiert auf Bias-Mitigation bezüglich Altersmerkmalen (Bias_Ungleichheit=Ja, Diversitaet=Ja, Fairness=Ja). Titel und Fokus auf 'responsible AI development' und 'mitigating bias' sind substantiell. TECHNIK_OK (Generative_KI) + SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Fairness) = Include. Kein direkter Bezug zu Sozialer Arbeit identifizierbar, aber Bias/Fairness-Fokus ausreichend für Inklusion.",4,10,40.00%
210,Exploring opportunities and risks in decision support technologies for social workers: An empirical study in the field of disabled people's services,Schneider (2022),Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Exclude,Include,Nein,Not relevant topic,,Unclear,Experimentell,0.95,"Paper behandelt substantiell Generative KI (LLMs), Prompting (Chain-of-Thought als zentrale Intervention), Gender-Bias in LLM-Outputs und Fairness-Aspekte (Bias-Reduktion). Erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting) und SOZIAL_OK (Bias_Ungleichheit + Gender + Fairness). Experimentelle Evaluationsstudie mit klarem Fokus auf Prompt-Strategien zur Bias-Mitigation.",3,10,30.00%
211,Das verflixte Problem mit Klassifikationen: Zum Einfluss der Digitalisierung auf die Soziale Diagnostik in der Sozialen Arbeit,Schneider (2024),Ja,Ja,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,Exclude,Include,Nein,No full text,,,Literaturreview,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting substantiell behandelt), SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Fairness adressiert). Systematische Review zu Prompt-Engineering-Techniken für Bias-Mitigation gegen Arabs/Muslims in LLMs mit quantifizierten Ergebnissen. Fokus auf kulturelle Repräsentation und algorithmische Fairness. Kein direkter Sozialarbeitsbezug, aber relevantes Anwendungsfeld für diskriminierungssensible KI-Nutzung.",0,0,
212,"AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?",Schneider (2024),Ja,Ja,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,Exclude,Include,Nein,Wrong publication type,,,Empirisch,0.92,"Paper erfüllt TECHNIK-Kriterium: Fokus auf Generative KI (ChatGPT-3.5/GPT-4), substantielle Analyse von Prompting-Strategien zur Bias-Reduktion. Erfüllt SOZIAL-Kriterium: Explizite Behandlung von Bias (kulturelle Stereotype), Diversität (diversity-sensitive prompts) und Fairness (Bias-Mitigation-Strategien). Empirische Studie mit drei experimentellen Phasen.",0,0,
213,"Indecision on the use of artificial intelligence in healthcare: A qualitative study of patient perspectives on trust, responsibility and self-determination using AI-CDSS",Schneider (2025),Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Exclude,Include,Nein,Not relevant topic,,Empirisch,Experimentell,0.92,"Paper behandelt systematisch Prompting-Strategien (Ja: substantieller Fokus auf 12 Prompt-Varianten) zur Bias-Reduktion in LLMs (Ja: Generative_KI). Bias-Reduktion und Fairness (Ja: beide Kategorien) sind zentrale Themen. TECHNIK-Bedingung erfüllt (Prompting + Generative_KI), SOZIAL-Bedingung erfüllt (Bias_Ungleichheit + Fairness). Kein Bezug zu Sozialer Arbeit oder Gender-spezifisch, kein feministischer Rahmen.",4,10,40.00%
214,Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen Arbeit – Eine empirische Untersuchung aus der Perspektive von Berufseinsteiger:innen,Schönauer (2025),Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Include,Include,Ja,,,Empirisch,Empirisch,0.95,"Das Paper adressiert Generative KI (GPT-4, LLMs) mit Fokus auf Prompting (42 Prompt-Templates als Audit-Instrument). Es thematisiert explizit intersektionalen Bias (Rasse × Geschlecht nach Crenshaw-Logik), Gender-Bias und Fairness. Die intersektionale Analyse erfüllt den feministischen Kriterium. TECHNIK_OK (Generative_KI + Prompting), SOZIAL_OK (Bias_Ungleichheit + Gender + Feministisch + Fairness). Einschluss gerechtfertigt.",2,10,20.00%
215,Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen Arbeit,Schönauer (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,Exclude,Include,Nein,Duplicate,,,Empirisch,0.85,"Paper behandelt algorithmische Systeme und deren Schäden (KI_Sonstige=Ja). Community-geleiteter Ansatz zur fairen KI adressiert Fairness, Bias/Ungleichheit und Diversität. Queer-Fokus involviert Gender-Perspektive und intersektionale Analysen (feministisch). Erfüllt beide Bedingungen: Technik (KI_Sonstige) + Sozial (Bias, Gender, Diversität, Fairness, Feministisch).",0,0,
216,Gender bias in artificial intelligence: Empowering women through digital literacy,Shah (2025),Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Literaturreview,Empirisch,0.95,"Paper analysiert LLM-Biases mit FISE-Methode (Generative_KI: Ja). Fokus auf intersektionale Repräsentationsverzerrungen erfüllt Bias_Ungleichheit, Gender (Geschlechterstereotype), Diversitaet (Repräsentation). Intersektionale Analyse nach Crenshaw ist explizit feministisch (Feministisch: Ja). Messung von Biases adressiert Fairness. TECHNIK (Generative_KI) + SOZIAL (alle 4 Kategorien) → Include.",6,10,60.00%
217,Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy,Shah (2025),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,Exclude,Include,Nein,Duplicate,,,Konzept,0.85,"Paper erfüllt TECHNIK-Kriterien (AI_Literacies für KI-Kompetenzen/Beteiligung; KI_Sonstige für allgemeine KI-Entwicklung; Fairness für genderinklusive/faire KI-Systeme) und SOZIAL-Kriterien (Gender explizit im Titel; Bias_Ungleichheit durch Menschenrechtsfokus; Diversitaet durch globale Kooperation und Expertinnenbeteiligung marginalisierter Gruppen). Feministisch=Nein, da keine explizite feministische Theorie erkennbar ist.",0,0,
218,Intersectional analysis of visual generative AI: the case of stable diffusion,Sharma (2024),Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Unclear,Politikanalyse,0.75,"Paper analysiert EU AI Act durch Gender-Lens: KI_Sonstige (algorithmische Systeme unter Regulierung) + Gender (expliziter Geschlechterfokus) + Bias_Ungleichheit (Geschlechtergerechtigkeit) + Fairness (Regulierungsanalyse). Beide Bedingungen erfüllt: TECHNIK_OK (KI_Sonstige), SOZIAL_OK (Gender + Bias_Ungleichheit + Fairness).",5,10,50.00%
219,Can prompt modifiers control bias? A comparative analysis of text-to-image generative models,Shin (2024),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Include,Exclude,Nein,,Not_relevant_topic,Experimentell,Experimentell,0.85,"Paper erfüllt TECHNIK-Bedingung (Generative_KI, Prompting). Jedoch fehlt SOZIAL-Bedingung für Anwendungsgebiet mit sozialem Fokus: Bias_Ungleichheit, Gender und Fairness behandeln allgemeine LLM-Bias, nicht spezifisch Soziale Arbeit, marginalisierte Communities oder sozialarbeiterische Kontexte. Reine KI-Ethik-Studie ohne Sozialarbeitsbezug → Exclude für dieses Review.",8,10,80.00%
220,Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development,Shin (2025),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Include,Include,Ja,,,Experimentell,Empirisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI: GPT-3.5, KI_Sonstige: BERT, Fairness-Metriken) und SOZIAL_OK (Gender-Bias explizit im Titel, Bias_Ungleichheit als Analysefokus, Fairness-Metriken substantiell behandelt). Empirische Studie zu Gender-Bias in KI-Systemen mit direktem Fairness-Bezug.",8,10,80.00%
221,Investigating AI systems: examining data and algorithmic bias through hermeneutic reverse engineering,Shukla (2025),Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Empirisch,Theoretisch,0.92,"Paper behandelt KI-Sonstige (Computer Vision, Content-Moderation, Algorithmen) und adressiert multiple soziale Dimensionen: Bias_Ungleichheit (algorithmische Marginalisierung queerer Identitäten), Gender (Geschlechtsklassifikation, heteronormative Annahmen), Diversitaet (queere Communities), Feministisch (kritische Analyse von Machtstrukturen und Marginalisierung) und Fairness (algorithmische Diskriminierung). TECHNIK_OK und SOZIAL_OK → Include.",4,10,40.00%
222,Towards a Feminist Metaethics of AI,Siapka (2023),Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Include,Include,Ja,,,Theoretisch,Theoretisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja; algorithmische Entscheidungssysteme), SOZIAL_OK (Bias_Ungleichheit=Ja, Gender=Ja, Diversitaet=Ja, Feministisch=Ja). Explizit feministische Perspektive mit intersektionaler Analyse und Fokus auf strukturelle Diskriminierung. Kein Bezug zu Sozialer Arbeit, daher diese=Nein.",10,10,100.00%
223,"""It happened to be the perfect thing"": Experiences of generative AI chatbots for mental health",Siddals (2024),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Unclear,Include,Nein,,,Empirisch,Empirisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, LLM-Analyse), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversität, Fairness alle Ja). Dokumentiert Gender-Stereotypen und algorithmischen Bias in LLMs systematisch. Kein explizit feministischer Theorie-Einsatz, daher Feministisch=Nein. Keine Soziale_Arbeit-Dimension erkennbar.",6,10,60.00%
224,Feminist Data Set,Sinders (2017),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,Exclude,Include,Nein,Wrong publication type,,,Literaturreview,0.92,"Paper erfüllt beide Bedingungen: TECHNIK (KI_Sonstige: algorithmische Systeme in Robotik, HR-Tools) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch: Intersektionalität ist etablierter feministischer Forschungsrahmen nach Crenshaw; Fairness). Substantielle Behandlung von Vorurteilen in KI-Systemen und struktureller Benachteiligung.",0,0,
225,AI Creates the Message: Integrating AI Language Learning Models into Social Work Education and Practice,Singer (2023),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Exclude,Include,Nein,Wrong publication type,,Theoretisch,Empirisch,0.95,Paper behandelt generative KI (LLMs) mit substantiellem Fokus auf intersektionale Gender-Rassen-Biases. Explizite Bezüge zu Intersektionalitätstheorie (feministische Perspektive nach Crenshaw). Prompting wird als Interventionsstrategie adressiert. Erfüllt beide Bedingungen: TECHNIK (Generative_KI + Prompting) und SOZIAL (Bias_Ungleichheit + Gender + Diversitaet + Feministisch + Fairness).,2,10,20.00%
226,A reparative turn in AI,Singh (2025),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Empirisch,Empirisch,0.95,"Paper behandelt substantiell Generative KI (LLMs) und Prompting (Prompt Engineering zur Bias-Reduktion). SOZIAL-Kriterien erfüllt: (1) Bias_Ungleichheit (intersektionale Stereotypes analyse), (2) Gender (Geschlechter-Stereotypen in LLMs), (3) Diversitaet (multiple Attribute/Gruppen), (4) Feministisch (intersektionale Analyse nach Crenshaw-Tradition). TECHNIK+SOZIAL erfüllt → Include.",6,10,60.00%
227,Inclusive prompt engineering: A methodology for hacking biased AI image generation,Skilton (2024),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Include,Include,Ja,,,Empirisch,Literaturreview,0.95,Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + Prompting substantiell fokussiert) und SOZIAL (Bias_Ungleichheit + Diversitaet + Fairness adressieren kulturelle und intersektionale Diskriminierung gegen Araber und Muslime). Systematischer Review von Prompt-Engineering zur Bias-Mitigation ist zentral relevant für KI-Ethik im Sozialkontext.,8,10,80.00%
228,Training in Co-Creation as a Methodological Approach to Improve AI Fairness,Slesinger (2024),Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Include,Include,Ja,,,Empirisch,Empirisch,0.85,"Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + Prompting substantiell behandelt) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Fairness). Fokus auf LLM-generierte Stories mit explizitem Prompting für Diversität. Empirische Analyse von Bias und Engagement-Disparitäten nach Identitätsmerkmalen. Kein direkter Sozialarbeitsbezug, daher kein Soziale_Arbeit=Ja.",5,10,50.00%
229,Generative AI and opportunities for feminist classroom assignments,Small (2023),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Include,Include,Ja,,,Experimentell,Experimentell,0.85,"Das Paper adressiert TECHNIK-Aspekte durch Generative_KI (LLMs) und substantiell Prompting (multilingual prompting als zentrale Strategie). SOZIAL-Aspekte sind erfüllt durch Bias_Ungleichheit (kulturelle Halluzinationen, demographische Bias), Diversitaet (narrative und demographische Diversität) und Fairness (Reduktion von Halluzinationen/Bias). Einschlusskriterien erfüllt.",7,10,70.00%
230,In search of artificial intelligence (AI) literacy in teacher education: A scoping review,Sperling (2024),Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Exclude,Include,Nein,Not relevant topic,,Literaturreview,Konzept,0.95,"Paper behandelt KI-Governance (KI_Sonstige: Ja) mit explizit feministischer Theorie (intersektionale Analyse nach Crenshaw-Tradition, Feministisch: Ja). Adressiert strukturelle Ungleichheiten, Bias und Gerechtigkeit (Bias_Ungleichheit, Gender, Diversitaet, Fairness alle Ja). Erfüllt beide Bedingungen: TECHNIK (KI_Sonstige) UND SOZIAL (Feministisch + multiple Dimensionen). Theoretischer Policy-Rahmen mit intersektionalem Fokus.",4,10,40.00%
231,Mitigating trust-induced inappropriate reliance on AI assistance,Srinivasan (2025),Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Empirisch,Theoretisch,0.95,"TECHNIK_OK: KI_Sonstige=Ja (algorithmische Systeme und Bias-Analyse). SOZIAL_OK: Bias_Ungleichheit=Ja, Gender=Ja, Diversitaet=Ja, Feministisch=Ja (explizite intersektionale Perspektive nach Crenshaw), Fairness=Ja. Alle Bedingungen erfüllt. Paper nutzt feministische/intersektionale Theorie substantiell zur Analyse von KI-Bias.",6,10,60.00%
232,Mitigating trust-induced inappropriate reliance on AI assistance,Srinivasan (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,Exclude,Include,Nein,Duplicate,,,Konzept,0.85,"Der Bericht adressiert algorithmische Diskriminierung (KI_Sonstige: Ja) und behandelt substantiell Bias/Ungleichheit sowie Fairness im Kontext von Antidiskriminierungsrecht. Er erfüllt beide Bedingungen (TECHNIK + SOZIAL) und ist damit included, obwohl er keinen direkten Soziale-Arbeit-Bezug hat.",0,0,
233,Mitigating trust-induced inappropriate reliance on AI assistance,Srinivasan (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,Exclude,Include,Nein,Duplicate,,,Theoretisch,0.92,"Paper analysiert EU AI Act durch die Linse von Geschlechtergerechtigkeit und Nichtdiskriminierung. TECHNIK erfüllt: KI_Sonstige (algorithmische Systeme reguliert durch AI Act). SOZIAL erfüllt: Bias_Ungleichheit (Geschlechterverzerrungen, Diskriminierungsrisiken), Gender (expliziter Gender-Fokus), Fairness (Bias-Audits, grundrechtliche Folgenabschätzungen). Keine Soziale_Arbeit, kein expliziter feministischer Theorierahmen erkennbar.",0,0,
234,Algorithmic Governance and the International Politics of Big Tech,Srivastava (2024),Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Unclear,Include,Nein,,,Theoretisch,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Bias in KI-Systemen), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch als intersektionale Kritik an Machtverhältnissen, Fairness). Kritische Analyse von algorithmischen Systemen mit explizitem Gender- und Rassismus-Fokus sowie intersektionaler Perspektive auf Diskriminierung.",8,10,80.00%
235,"Künstliche Intelligenz in der Sozialen Arbeit: Grundlagen, Entwicklungen, Herausforderungen",Steiner (2022),Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Exclude,Exclude,Ja,Not relevant topic,Not_relevant_topic,Theoretisch,Literaturreview,0.85,"Das Paper behandelt technische Bias-Mitigation und Fairness-Metriken (TECHNIK_OK), aber ausschließlich im Gesundheitswesen ohne Bezug zu Sozialer Arbeit. Während Bias_Ungleichheit und Fairness adressiert werden, fehlt ein direkter Bezug zu den definierten SOZIAL-Kategorien (Soziale_Arbeit, Gender, Diversität, Feministisch). Das Paper liegt außerhalb des Scope eines Literature Review zur KI in der Sozialen Arbeit.",8,10,80.00%
236,What large language models know and what people think they know,Steyvers (2025),Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Empirisch,Konzept,0.92,"Das Paper behandelt KI-Systeme substantiell unter der Linse sozialer Gerechtigkeit. KI_Sonstige: Ja (algorithmische Systeme und deren gesellschaftliche Auswirkungen). Bias_Ungleichheit: Ja (expliziter Fokus auf Verzerrungen gegenüber marginalisierten Gruppen). Gender: Ja (Geschlechtergleichstellung als Kernthema). Diversitaet: Ja (Inklusion verschiedener Gruppen). Feministisch: Ja (intersektionale, menschenrechtsbasierte Perspektive mit Fokus auf marginalisierte Gruppen und strukturelle Gerechtigkeit). Fairness: Ja (Fairness als explizites Ziel). TECHNIK_OK (KI_Sonstige) + SOZIAL_OK (Bias, Gender, Diversität, Feministisch, Fairness) → Include.",3,10,30.00%
237,What large language models know and what people think they know,Steyvers (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,Exclude,Include,Nein,Duplicate,,,Theoretisch,0.92,"Paper behandelt KI/ML-Systeme (KI_Sonstige: mathematische Fairness-Metriken für Klassifikation). Substantieller Fokus auf intersektionale Fairness und algorithmischen Bias (Bias_Ungleichheit). Intersektionale Analyse nach Crenshaw ist explizit feministisches Konzept (Feministisch). Fairness-Metriken sind Kernthema (Fairness). Diversität durch Fokus auf Untergruppen und marginalisierte Intersektionen adressiert (Diversitaet). Erfüllt TECHNIK (KI_Sonstige) + SOZIAL (Bias_Ungleichheit, Feministisch, Fairness, Diversitaet).",0,0,
238,CAIL – Critical AI Literacy: Kritische Technikkompetenz für konstruktiven Umgang mit KI-basierter Technologie in Betrieben,Strauß (2024),Ja,Ja,Ja,Ja,Ja,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,Exclude,Include,Nein,Not relevant topic,,,Konzept,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige) durch Framework-Entwicklung für KI-Kompetenzentwicklung und Machtanalyse in KI-Systemen. SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness) durch explizite feministische Methodologie nach intersektionalem Ansatz, Bekämpfung struktureller Ungleichheiten und Fairness-Fokus. Das Paper entwickelt substantiell einen feministischen Rahmen für KI-Governance.",1,1,100.00%
239,Homoglyph unlearning: A novel approach to bias mitigation,Struppek (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,Exclude,Include,Nein,Other,,,Literaturreview,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmische Diskriminierungssysteme), SOZIAL_OK (Bias_Ungleichheit + Fairness: systematische Analyse von Diskriminierungstypen, Regelungsansätze, strukturelle Ungleichheiten). Kein direkter Sozialarbeitsbezug, daher nicht als SA-Paper klassifiziert. Substantieller Fokus auf algorithmische Fairness und Bias-Analyse.",0,0,
240,Digitale Werkzeuge und Machtasymmetrien?,Studeny (2025),Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Theoretisch,Literaturreview,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja: algorithmische Fairness-Systeme) und SOZIAL_OK (Bias_Ungleichheit=Ja, Diversitaet=Ja, Feministisch=Ja [explizite Verwendung von Intersektionalität nach Crenshaw, Analyse von Machtstrukturen], Fairness=Ja). Kritische intersektionale Analyse struktureller Machtverhältnisse in KI-Systemen mit explizitem feministischem Rahmen (Crenshaw).",7,10,70.00%
241,Diskriminierung durch Algorithmen – Überlegungen zur Stärkung KI-bezogener Kompetenzen,Sūna (2024),Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Include,Include,Ja,,,Theoretisch,Theoretisch,0.95,"Paper adressiert KI-Governance als algorithmisches System (KI_Sonstige) mit explizitem Fokus auf feministische Theorie und Methodik (Feministisch). Substantielle Behandlung von Machtungleichgewichten, Bias, strukturellen Ungleichheiten (Bias_Ungleichheit), Gender-Perspektive und Diversität/Inklusion von marginalisierten Communities der globalen Mehrheit. Fairness im Governance-Kontext impliziert. Beide Bedingungen (TECHNIK + SOZIAL) erfüllt.",7,10,70.00%
242,Governance of generative AI: A comprehensive framework for navigating challenges and opportunities,Taeihagh (2025),Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Konzept,Theoretisch,0.95,"Paper kombiniert Technik (KI_Sonstige: algorithmische Fairness-Systeme) mit starkem sozialen Bezug. Erfüllt mehrere SOZIAL-Kategorien: Bias_Ungleichheit (strukturelle Unterdrückungssysteme), Gender (Fokus auf Women of Color), Diversitaet (intersektionale Analyse), Feministisch (kritisch-theoretischer Ansatz, Intersektionalität nach Crenshaw-Tradition), Fairness (direkter Bezug zu Fair ML). TECHNIK_OK und SOZIAL_OK → Include.",6,10,60.00%
243,AI implementation science for social issues: Pitfalls and tips,Takaoka (2022),Ja,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Exclude,Include,Nein,No relevant topic,,Theoretisch,Konzept,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (Policy-Rahmen für AI-Governance), KI_Sonstige (algorithmische Diskriminierung). SOZIAL: Bias_Ungleichheit (Fokus auf algorithmische Diskriminierung), Gender (explizite intersektionale Genderanalyse), Diversitaet (inklusive Repräsentation), Feministisch (intersektionale Analyse nach Crenshaw-Logik), Fairness (substantive equality vs. formale Fairness). Transformatives Policy-Framework mit starkem Fokus auf strukturelle Gerechtigkeit.",3,10,30.00%
244,GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models,Tang (2024),Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Konzept,Theoretisch,0.92,"Paper behandelt algorithmische Systeme (KI_Sonstige: Ja) mit substantiellem Fokus auf Bias-Mitigation und Diskriminierung (Bias_Ungleichheit: Ja) sowie Fairness-Konzepte (Fairness: Ja). Kritische Analyse von Fairness-Definitionen und Bias-Mitigation-Methoden erfüllt beide Bedingungen (TECHNIK + SOZIAL). Kein direkter Sozialarbeit-Bezug, daher Kategorien Soziale_Arbeit/Gender/Diversitaet/Feministisch korrekt auf Nein.",4,10,40.00%
245,Operationalizing positive-constructive pedagogy to artificial intelligence: the 3E model for scaffolding AI technology adoption,Thwaites (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Exclude,,,"TECHNIK_OK (KI_Sonstige: Ja) aber SOZIAL_OK nicht erfüllt. Bias_Ungleichheit und Fairness sind erfüllt, aber kein direkter Bezug zu Sozialer Arbeit, Gender-Fokus, Diversität oder feministischer Theorie. Das Paper adressiert allgemeine KI-Policy und Fairness im breiteren Kontext, nicht spezifisch die Schnittmenge von KI und Sozialer Arbeit.",,Literaturreview,0.85,"Das Paper behandelt algorithmische Fairness und Bias-Mitigation mit Policy-Fokus (KI_Sonstige: Ja, Fairness: Ja). Es erfüllt die TECHNIK-Bedingung. Allerdings fehlt der explizite Bezug zu Sozialer Arbeit oder marginalisierten Gruppen im Kontext sozialer Dienste. Intersektionale Diskriminierung wird erwähnt, aber nicht aus feministischer oder diversitätsorientierter Perspektive. Das Paper bleibt auf allgemeiner KI-Policy-Ebene und nicht im Sozialbereich.",0,0,
246,A systematic review on digital literacy,Tinmaz (2022),Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Exclude,Include,Nein,Not relevant topic,,Literaturreview,Theoretisch,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmische Governance und AI-Systeme sind substantiell adressiert) und SOZIAL_OK (Bias_Ungleichheit, Gender, Diversität und explizit Feministisch durch Intersektionalitätsanalyse nach Crenshaw-Perspektive, Fairness durch Kritik ungleicher AI-Harm-Verteilung). Kritische Analyse von AI Governance aus intersektionaler, feministischer Perspektive mit Fokus auf marginalisierte Gruppen.",5,10,50.00%
247,"Guardrails, not guidance: Understanding responses to LGBTQ+ language in large language models",Tint (2025),Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Experimentell,Theoretisch,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmische Systeme und Bias), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch via intersektionale Analyse nach Crenshaw-Rahmen, Fairness). Intersektionalität und kritische Machtanalyse sind explizit feministisch. Fokus auf strukturelle Systeminnovation, nicht nur technische Fixes.",6,10,60.00%
248,Shaping feminist artificial intelligence,Toupin (2024),Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Include,Include,Ja,,,Theoretisch,Theoretisch,0.95,"Das Paper erfüllt TECHNIK (KI_Sonstige: algorithmische Systeme und deren Regulierung) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness). Es nutzt explizit feministische Theorie (hermeneutical injustice, feminist legal theory, Intersektionalität) zur Kritik von KI-Governance. Substantielle Behandlung von Geschlechter-Bias, Diskriminierung marginalisierter Gruppen und Fairness in AI-Regulierung.",8,10,80.00%
249,Shaping feminist artificial intelligence,Toupin (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Nein,,,Ja,,,Ja,,Exclude,Include,Nein,Duplicate,,,Theoretisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige) + SOZIAL_OK (Bias_Ungleichheit, Gender, Feministisch, Fairness). Behandelt substantiell feministische Perspektive auf KI (intersektional), Gender-Bias in KI-Systemen, AI-Literacy in politischer Bildung. Kein Sozialarb.-Bezug, daher nicht in dieser Kategorie.",0,0,
250,Shaping feminist artificial intelligence,Toupin (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,Exclude,Include,Nein,Duplicate,,,Konzept,0.95,"Paper behandelt substantiell feministische Perspektiven auf KI-Entwicklung (Feministisch: Ja), thematisiert Gender-Perspektiven und Geschlechtergerechtigkeit (Gender: Ja), adressiert Diversität und Inklusion durch intersektionale Teams und Community-Partizipation (Diversitaet: Ja), diskutiert Bias und Ungleichheit (Bias_Ungleichheit: Ja), und entwickelt KI-Kompetenzen und Methodologien (AI_Literacies: Ja). Erfüllt beide TECHNIK- und SOZIAL-Kriterien.",0,0,
251,Shaping feminist artificial intelligence,Toupin (2024),Ja,Ja,,Ja,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,Exclude,Include,Nein,Duplicate,,,Konzept,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK-Seite mit AI_Literacies (kritische Kompetenzentwicklung), Generative_KI (Chatbot-Design), Prompting (explizit critical prompting practices), und KI_Sonstige (Datensammlung, Training, Algorithmenauswahl). SOZIAL-Seite mit starkem Feministisch-Fokus (explizit feministische und intersektionale Perspektive), Bias_Ungleichheit (Protest gegen biased AI), Gender und Diversitaet (community-based approach). Substanzielle Behandlung aller Kategorien.",0,0,
252,Trust in artificial intelligence–based clinical decision support systems among health care workers: Systematic review,Tun (2025),Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Literaturreview,Theoretisch,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja: algorithmische Systeme und KI-Diskriminierung), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet und Feministisch alle Ja). Data Feminism ist ein kanonales Werk der feministischen Technikforschung (D'Ignazio explizit), das strukturelle Diskriminierung in KI-Systemen mittels intersektionaler, feministischer Analyse adressiert. Kein Sozialarbeit-Bezug, aber nicht erforderlich.",5,10,50.00%
253,Trust in artificial intelligence–based clinical decision support systems among health care workers: Systematic review,Tun (2025),Ja,Ja,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Nein,,Exclude,Include,Nein,Duplicate,,,Literaturreview,0.95,"Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies, Prompting) + SOZIAL (Feministisch, Gender, Bias_Ungleichheit, Diversitaet). Explizit feministische Theorie (Rahmenwerk mit 6 Kategorien), Fokus auf Design und Prompting-Praktiken sowie Geschlechterperspektive auf KI-Systeme. Typen-orientierter Literaturreview mit substanziellem feministischem Ansatz.",0,0,
254,Intersectionality in artificial intelligence: Framing concerns and recommendations for action,Ulnicane (2024),Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Include,Exclude,Nein,,Not_relevant_topic,Konzept,Theoretisch,0.85,"Das Paper behandelt algorithmische Systeme (KI_Sonstige: Ja) und Machtungleichgewichte durch Big Tech (Bias_Ungleichheit: Ja). Allerdings fehlt ein direkter Bezug zu Sozialer Arbeit oder deren Praxis/Zielgruppen. Die Analyse fokussiert auf politische Wirtschaft und staatliche Souveränität, nicht auf Sozialbereich. SOZIAL-Bedingung ist nicht erfüllt.",6,10,60.00%
255,Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,Ulnicane (2024),Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Exclude,Exclude,Ja,Duplicate,"TECHNIK_OK (KI_Sonstige: Ja - Verantwortung in KI-Entwicklung), aber SOZIAL_NOT_OK. Feministisch ist Ja, aber das reicht nicht: Soziale_Arbeit=Nein (kein direkter Bezug zu sozialer Arbeit, Zielgruppen oder Praxis), Bias_Ungleichheit=Nein (nicht explizit zum Thema), Gender=Nein (keine Gender-spezifische Analyse). Fairness=Nein (Verantwortung ist nicht Fairness im technischen Sinne). Das Paper ist zwar feministisch theoretisch fundiert und behandelt KI, aber es adressiert keinen der sozialen Arbeitsbereiche des Literature Reviews (Soziale Arbeit, Bias/Ungleichheit, Gender, Diversität, Fairness). Reine AI-Ethik-Kritik ohne Sozialarbeitsbezug.",Konzept,Theoretisch,0.85,"Paper kombiniert feministische Theorie (Browne et al.) mit KI-Verantwortungsdiskurs. KI_Sonstige=Ja (KI-Entwicklung, Verantwortungsrahmen), Feministisch=Ja (explizit feministische politische Theorie). Jedoch: Soziale_Arbeit=Nein (kein Bezug zu SW-Praxis/Zielgruppen), Bias_Ungleichheit=Nein (nicht substantiell), Gender=Nein (keine Gender-Spezifik). Fehlt: SOZIAL-Komponente im engeren Sinne des Reviews. Nur eines der 6 sozialen Kriterien erfüllt → Exclude.",7,10,70.00%
256,Artificial Intelligence and Intersectionality,Ulnicane (2024),Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Exclude,Include,Nein,Duplicate,,Konzept,Literaturreview,0.95,"Das Paper behandelt substantiell algorithmische Fairness und Bias in ML-Systemen (KI_Sonstige, Fairness). Es greift EXPLIZIT auf Crenshaw's intersektionale Theorie zurück und analysiert Diskriminierungsmechanismen aus intersektionaler Perspektive (Feministisch, Bias_Ungleichheit, Gender, Diversitaet). TECHNIK-Bedingung erfüllt (KI_Sonstige=Ja), SOZIAL-Bedingung erfüllt (Bias_Ungleichheit, Gender, Diversitaet, Feministisch = alle Ja). Include-Schwelle erreicht.",9,10,90.00%
257,Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,Ulnicane (2024),Ja,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Exclude,Include,Nein,Duplicate,,Konzept,Konzept,0.92,"Paper behandelt substantiell AI Literacy durch feministische Perspektive (Kubes, Gender-kritisch). Analysiert KI-Anwendungen im Kontext Bias (androzentrisch, eurozentrisch, kapitalistisch-patriarchal), Geschlechterperspektive und Inklusion. Praktischer Bezug durch Curriculum und Workshops. Beide Bedingungen erfüllt: TECHNIK (AI_Literacies, KI_Sonstige), SOZIAL (Feministisch, Gender, Bias_Ungleichheit, Diversitaet).",9,10,90.00%
258,Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,Ulnicane (2024),Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Exclude,Include,Nein,Duplicate,,Konzept,Empirisch,0.92,"Paper erfüllt TECHNIK-Kriterium durch AI_Literacies (Fokus auf kritische KI-Kompetenzentwicklung), Generative_KI (Erwähnung von Language Models) und SOZIAL-Kriterium durch Bias_Ungleichheit (expliziter Fokus auf Bias-Mechanismen) und Fairness (Verständnis von Fairness als zentrales Element). Empirische Studie mit großer Stichprobe (n=605). Kein Soziale_Arbeit-Bezug erkennbar.",3,10,30.00%
259,Intersectionality in artificial intelligence: Framing concerns and recommendations for action,Ulnicane (2024),Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Exclude,Include,Nein,Duplicate,,Konzept,Empirisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK (KI_Sonstige: algorithmische Systeme für Recruitment/Resume-Screening) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet: explizite Analyse von Rassismus und Geschlechterdiskriminierung; Feministisch: intersektionale Analyse nach Crenshaw-Logik; Fairness: algorithmic fairness in hiring). Substanzielle empirische Studie mit klarem intersektionalem Fokus.",9,10,90.00%
260,Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,Ulnicane (2024),Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Exclude,Include,Nein,Duplicate,,Konzept,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: analysiert AI-Systeme wie Voice Assistants, Robots, Hiring Tools). SOZIAL_OK (Bias_Ungleichheit: Diskriminierung durch homogene Dev-Teams; Gender + Diversitaet: mehrere Diskriminierungsformen; Feministisch: explizite Anwendung von Crenshaw's Intersektionalitätstheorie). Substantielle intersektionale Analyse von strukturellen KI-Ungleichheiten.",10,10,100.00%
261,Artificial Intelligence and gender equality,UN Women (2024),Ja,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,Exclude,Include,Nein,Wrong publication type,,,Experimentell,0.85,Paper behandelt substantiell AI Literacy durch teacher education und co-discovery-Ansätze (TECHNIK erfüllt). Expliziter Fokus auf kritische Examination von AI's power dynamics und social justice implications erfüllt Bias_Ungleichheit-Kategorie (SOZIAL erfüllt). Beide Bedingungen der Inklusionskriterien sind erfüllt.,0,0,
262,Recommendation on the Ethics of Artificial Intelligence,UNESCO (2021),Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,Exclude,Nein,,Not_relevant_topic,Konzept,Konzept,0.85,"Das Paper erfüllt die SOZIAL-Bedingung (Bias_Ungleichheit, Diversitaet, Feministisch, Fairness alle Ja) und auch die TECHNIK-Bedingung (KI_Sonstige Ja). Jedoch ist dies eine Keynote-Zusammenfassung von fundamentaler Kritischer Techniktheorie/Philosophie ohne direkten Bezug zu Sozialer Arbeit als Praxis oder Disziplin. Das Literature Review fokussiert auf KI-Anwendungen in der Sozialen Arbeit, nicht auf allgemeine KI-Ethik und Gesellschaftskritik. Benjamin's Werk ist relevant für theoretische Grundlagen, aber nicht spezifisch für das Scopus des Reviews.",6,10,60.00%
263,Women4Ethical AI: Global cooperation for gender-inclusive AI,UNESCO (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Nein,,Exclude,Include,Nein,Wrong publication type,,,Theoretisch,0.95,Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja: kritische Analyse von KI-Systemen und deren Entwicklung) UND SOZIAL_OK (Bias_Ungleichheit=Ja: Fokus auf Gender Bias in AI; Gender=Ja: expliziter Gender-Fokus; Diversitaet=Ja: Unterrepräsentation von Frauen; Feministisch=Ja: verwendet feministische Theorie und Kritik von Machtstrukturen). Theoretisches Paper mit substantiellem feministischem Anspruch.,0,0,
264,Bias against women and girls in large language models: A UNESCO study,UNESCO (2024),Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Experimentell,Theoretisch,0.92,"Paper erfüllt TECHNIK-Kriterium über KI_Sonstige (algorithmische Auditing-Systeme) und SOZIAL-Kriterium über multiple Kategorien: Bias_Ungleichheit (Fokus auf Diskriminierung und marginalisierte Communities), Diversitaet (Multi-Stakeholder-Prozess, Community-Involvement), Feministisch (intersektionale Kritik an Machtstrukturen, Fokus auf strukturelle Benachteiligung), Fairness (algorithmic auditing für gerechtere Systeme). Kritik an technischen Metriken und Betonung von Community-Perspektiven zeigt feministische/kritische Technikforschung.",5,10,50.00%
265,,Unknown (),Ja,Ja,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,Exclude,Include,Nein,No full text,,,Konzept,0.85,Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies Ja: Pedagogisches Framework für kritische KI-Kompetenzentwicklung; Prompting Ja: Expliziter Fokus auf 'critical prompting' in Explore-Phase). SOZIAL_OK (Bias_Ungleichheit Ja: Substantielle Behandlung von Bias-Erkennung und Sichtbarmachung von Diskriminierung in KI-Outputs). Kein Bezug zu Sozialer Arbeit oder explizit feministischer Theorie; daher nur diese Kategorien Ja.,0,0,
266,AI competency framework for students,Unknown (2024),Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Include,Include,Ja,,,Konzept,Theoretisch,0.92,"Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Kritische Analyse von KI-Policy), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch: Explizit intersektionale Analyse mit Fokus auf strukturelle Machtdynamiken; Fairness: Kritik an unzureichenden Fairness-Ansätzen). Die feministische Perspektive ist substantiell durch intersektionale Theoriearbeit (Crenshaw-Rahmen impliziert) verankert.",5,10,50.00%
267,"RETHINKING SOCIAL SERVICES WITH ARTIFICIAL INTELLIGENCE: OPPORTUNITIES, RISKS, AND FUTURE PERSPECTIVES",Unknown (2024),Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Exclude,Include,Nein,Not relevant topic,,Theoretisch,Experimentell,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting), SOZIAL_OK (Bias_Ungleichheit + Gender + Feministisch + Fairness). Empirische Analyse von Gender-Bias in GPT-4/Copilot-generierten Policies mit intersektionalem Rahmen. Fokus auf Androzentrizität und strukturelle Biases entspricht feministischer Technikforschung. Keine direkte Soziale-Arbeit-Verbindung, aber Policymaking ist relevant für soziale Systeme.",3,10,30.00%
268,Artificial Intelligence in Social Work: An EPIC Model for Practice,Unknown (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,Exclude,Include,Nein,Duplicate,,,Literaturreview,0.95,"Das Paper behandelt substantiell feministische Perspektiven auf KI (KI_Sonstige: Ja), thematisiert Gender-Bias und Machtdynamiken in AI-Systemen (Bias_Ungleichheit: Ja, Gender: Ja) und nutzt explizit feministische Theorie als Analyserahmen (Feministisch: Ja). Erfüllt beide Bedingungen: TECHNIK (KI_Sonstige) und SOZIAL (Bias_Ungleichheit, Gender, Feministisch).",0,0,
269,"Artificial Intelligence in Social Sciences and Social Work: Bridging Technology and Humanity to Revolutionize Research, Policy, and Human Services",Unknown (2025),Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Include,Include,Ja,,,Literaturreview,Literaturreview,0.92,"Paper analysiert intersektionale Perspektiven in KI-Entwicklung und befasst sich substantiell mit Bias, Geschlechterperspektiven und struktureller Unterrepräsentation. Intersektionalität (Crenshaw-Framework) ist eine feministische Methode. Erfüllt TECHNIK (KI_Sonstige) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch). Kein direkter Soziale_Arbeit-Bezug, aber AI-Systemkritik mit marginalisierten Gruppen im Fokus.",5,10,50.00%
270,"Introduction to the digital welfare state: Contestations, considerations and entanglements",van Toorn (2024),Ja,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,,Ja,,,Nein,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,Exclude,Include,Nein,Not relevant topic,,,Konzept,0.82,"Paper behandelt kritische AI Literacies (TECHNIK) und thematisiert strukturelle Ungleichheiten bezüglich Rasse, Geschlecht und Arbeit (SOZIAL). Expliziter Fokus auf Bias/Ungleichheit und Gender. Kritische Perspektive auf Strukturen deutet auf feministische Ansätze hin. Beide Bedingungen erfüllt → Include.",2,3,66.67%
271,Fairness Beyond the Algorithmic Frame: Actionable Recommendations for an Intersectional Approach,Vethman (2025),Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Include,Include,Ja,,,Empirisch,Theoretisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Ja - Kritik an AI-Ethics-Frameworks als algorithmische Systeme) und SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness alle Ja). Explizit intersektional-feministischer Ansatz mit Fokus auf marginalisierte Gruppen und strukturelle Gerechtigkeit. Kein Bezug zu Sozialer Arbeit als Disziplin, aber substantielle Behandlung von Fairness und intersektionalen Gerechtigkeitsfragen.",7,10,70.00%
272,Recommendations for social work researchers and journal editors on the use of generative AI and large language models,Victor (2023),Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Include,Include,Ja,,,Konzept,Theoretisch,0.85,"Paper behandelt Bias in AI-Systemen (KI_Sonstige: Ja) mittels hermeneutischer Reverse Engineering. Fokus auf algorithmischen Bias (Bias_Ungleichheit: Ja), Auswirkungen auf verschiedene soziale Gruppen (Diversitaet: Ja) und Fairness-Aspekte durch participatory design (Fairness: Ja). Beide Bedingungen erfüllt: TECHNIK (KI_Sonstige) + SOZIAL (Bias_Ungleichheit, Diversitaet, Fairness).",4,10,40.00%
273,Recommendations for social work researchers and journal editors on the use of generative AI and large language models,Victor (2023),Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Exclude,Include,Nein,Duplicate,,Konzept,Empirisch,0.85,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja, da es sich um Studie zu AI-Development befasst) und SOZIAL_OK (Bias_Ungleichheit=Ja: 'injustices and inequalities'; Fairness=Ja: Responsibility und strukturelle Herausforderungen). Kein Bezug zu Sozialer Arbeit oder feministischer Theorie erkennbar. Empirische Studie mit N=32 Professionals.",5,10,50.00%
274,Recommendations for social work researchers and journal editors on the use of generative AI and large language models,Victor (2023),Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Exclude,Include,Nein,Duplicate,,Konzept,Literaturreview,0.92,"Das Paper behandelt substantiell feministische Theorie und Perspektiven im KI-Kontext (KI_Sonstige: Ja). Es adressiert explizit feministische Ansätze, Machtstrukturen und soziale Gerechtigkeit (Feministisch: Ja). Gender und Diversität sind zentral (Gender: Ja, Diversitaet: Ja). Bias/Ungleichheit wird durch die Fokussierung auf Machtrelationen thematisiert (Bias_Ungleichheit: Ja). TECHNIK erfüllt (KI_Sonstige), SOZIAL erfüllt (multiple Kategorien). → Include",4,10,40.00%
275,Algorithmic Governance: Gender Bias in AI-Generated Policymaking?,Voutyrakou (2025),Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Experimentell,Theoretisch,0.85,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja, algorithmische Systeme im Gesundheitskontext), SOZIAL_OK (Bias_Ungleichheit=Ja zu struktureller Ungerechtigkeit, Fairness=Ja zu AI Fairness). Kein direkter Soziale_Arbeit-Bezug, daher keine entsprechende Kategorie.",5,10,50.00%
276,Rationalisierung durch Digitalisierung?,Waag (2023),Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Exclude,Include,Nein,Not relevant topic,,Theoretisch,Empirisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies: Training zu KI-Konzepten; KI_Sonstige: AI Bias Detection/Mitigation Tools; Fairness: Algorithmen-Fairness) + SOZIAL (Bias_Ungleichheit: Fokus auf Bias Detection; Diversitaet: vulnerable/marginalized stakeholder groups, inklusive AI-Entwicklung; Feministisch: Co-creation mit marginalisierten Gruppen, Machtstrukturen in AI-Design). Substantieller Fokus auf Training zur Befähigung von Non-Technical Stakeholders und kritische Reflexion von Inklusion in AI-Systemen.",3,10,30.00%
277,Feminism Confronts AI: The Gender Relations of Digitalisation,Wajcman (2023),Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Include,Include,Ja,,,Theoretisch,Empirisch,0.95,"Paper erfüllt beide Bedingungen: (1) TECHNIK: Generative_KI=Ja (Stable Diffusion als visuelles generatives KI-System). (2) SOZIAL: Bias_Ungleichheit=Ja (Analyse von Sexismus, Rassismus, Heteronormativität, Ableismus), Gender=Ja (expliziter Fokus auf Geschlechterstereotype/masculine-presenting standard), Diversitaet=Ja (Repräsentation marginalisierter Gruppen), Feministisch=Ja (intersektionale Analyse nach kritischer Perspektive, Fokus auf Machtstrukturen und Marginalisierung). Empirische Inhaltsanalyse von 180 generierten Bildern.",6,10,60.00%
278,Measuring user competence in using artificial intelligence: validity and reliability of artificial intelligence literacy scale,Wang (2023),Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Exclude,Include,Nein,Not relevant topic,,Empirisch,Konzept,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige, Fairness behandeln KI-Systeme und Kompetenzentwicklung). SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Feministisch durch intersektionale Perspektive, expliziter Fokus auf Machtstrukturen und marginalisierte Gruppen). Intersektionalität ist feministische Theorie (Crenshaw). Framework für verantwortungsvolle AI-Entwicklung mit gesellschaftlichem Kontext.",6,10,60.00%
279,Algorithmic discrimination: examining its types and regulatory measures with emphasis on US legal practices,Wang (2024),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,Exclude,Include,Nein,Not relevant topic,,,Literaturreview,0.95,"Das Paper behandelt substantiell AI Fairness und Bias/Ungleichheit im KI-Kontext (KI_Sonstige erfüllt Technik-Bedingung). Bias_Ungleichheit, Diversitaet und Fairness sind zentrale Themen. Explizite feministische Perspektive durch Bezug zu Crenshaw's Intersectionality-Framework und kritische Analyse von Machtstrukturen (Feministisch = Ja). Beide Bedingungen (Technik + Sozial) erfüllt → Include.",0,0,
280,Multilingual Prompting for Improving LLM Generation Diversity,Wang (2024),Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Include,Include,Ja,,,Empirisch,Theoretisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK durch KI_Sonstige (algorithmische Systeme und AI Bias); SOZIAL_OK durch Bias_Ungleichheit (strukturelle Gerechtigkeit), Diversitaet (Diversity/Equity/Inclusion) und Fairness (Frameworks für faire KI-Governance). Theoretischer Ansatz basierend auf Young's Strukturtheorie, nicht auf feministischer Methode selbst (daher Feministisch=Nein).",6,10,60.00%
281,A survey on fairness in large language models,Wang (2024),Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Include,Include,Ja,,,Experimentell,Theoretisch,0.92,"Das Paper entwickelt einen explizit feministischen theoretischen Rahmen (feminist metaethics) für KI-Ethik und untersucht Machtstrukturen, Kontexte und die Rolle von Akteur:innen. KI_Sonstige=Ja (KI-Ethik als Teilbereich), Feministisch=Ja (explizite feministische Metaethik-Perspektive), Bias_Ungleichheit=Ja (Untersuchung von Machtrelationen und Kontexten), Gender=Ja (Gender-expliziter Fokus). TECHNIK-Bedingung (KI_Sonstige) + SOZIAL-Bedingungen (Feministisch, Bias_Ungleichheit, Gender) erfüllt → Include.",4,10,40.00%
282,Multilingual Prompting for Improving LLM Generation Diversity,Wang (2025),Ja,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,Exclude,Include,Nein,Duplicate,,,Konzept,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Ja - behandelt AI-Systeme im Allgemeinen), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness alle Ja). Explizit feministische Arbeit (D'Ignazio ist Autorin von Data Feminism), intersektionale Perspektive auf KI-Systeme, Fokus auf ungleiche Machtbeziehungen und Gerechtigkeit. Substantielle Behandlung aller relevanten Kategorien.",0,0,
283,Messung von AI Literacy – Empirische Evidenz und Implikationen,Weber (2023),Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Exclude,Include,Nein,Not relevant topic,,Empirisch,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies: Ja – Framework für KI-Kompetenzen/Werte-Integration; KI_Sonstige: Ja – Systemische Analyse von AI-Biases). SOZIAL_OK (Bias_Ungleichheit: Ja – adressiert Biases und Injustices; Gender: Ja – expliziter Gender-Fokus; Diversitaet: Ja – Inklusion; Feministisch: Ja – intersektionale Feminismus, feministische Theorie; Fairness: Ja – Gerechtigkeit und Equity als zentrale Werte). Praktische Anwendungen (MIRA, EU AI Act) zeigen Substanz.",5,10,50.00%
284,"Discriminating Systems: Gender, Race, and Power in AI",West (2023),Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Exclude,Include,Nein,,,Theoretisch,Experimentell,0.95,"Paper behandelt Generative KI (Bildgeneratoren) mit substantiellem Fokus auf Prompt-Engineering durch automatisierte Prompt-Rewriting. Adressiert demografische Bias-Reduktion explizit durch Gender-, Ethnizität- und Altersrepräsentation. Fairness-Metriken zur Bias-Messung zentral. TECHNIK erfüllt (Generative_KI + Prompting), SOZIAL erfüllt (Bias_Ungleichheit + Gender + Diversitaet + Fairness). Keine feministische Theorie-Nutzung, kein SA-Bezug.",6,10,60.00%
285,AI tools show biases in ranking job applicants' names according to perceived race and gender,Wilson (2024),Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Include,Include,Ja,,,Experimentell,Konzept,0.92,"Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies, Generative_KI, Prompting alle Ja) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness alle Ja). Fokus auf feministisches KI-Prompting mit KI-FAIRNESS-Framework für faire, diskriminierungsvermeidende Prompt-Gestaltung. Substantielle Behandlung von Stereotyp-Reduktion und Repräsentation in KI-Outputs.",7,10,70.00%
286,"Gender, race, and intersectional bias in AI resume screening via language model retrieval",Wilson (2024),Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Include,Include,Ja,,,Experimentell,Konzept,0.85,"Paper behandelt substantiell algorithmische Bias und Repräsentationsprobleme in KI-Datasets/Modellen (KI_Sonstige: Ja). Das Konzept der 'Positionality' basiert auf einer kritischen Perspektive, die implizit feministische Technikforschung (d'Ignazio & Klein) widerspiegelt - Fokus auf positionality, marginalisierte Gruppen, strukturelle Machtdynamiken. Adressiert Diversität, Bias und Fairness substantiell. Kein direkter Sozialarbeitsbezug, keine Generative KI. TECHNIK + SOZIAL erfüllt → Include.",5,10,50.00%
287,Broadening artificial intelligence education in K-12: where to start?,Wong (2020),Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Exclude,Include,Nein,,,Konzept,Empirisch,0.92,"Das Paper erfüllt beide Bedingungen: (1) TECHNIK: Generative_KI (Text-to-Image Modelle), Prompting (Analyse von Prompt-Modifizierern zur Bias-Reduktion) → Ja. (2) SOZIAL: Bias_Ungleichheit (explizit: Analyse gesellschaftlicher Biases), Diversitaet (diversity-reflective prompting), Fairness (ethische KI-Entwicklung) → Ja. Empirische Vergleichsstudie mit substantiellem Fokus auf Prompt-Engineering und algorithmischen Bias.",3,10,30.00%
288,AI for impact: The PRISM framework for responsible AI in social innovation,World Economic Forum (2024),Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Include,Include,Ja,,,Konzept,Experimentell,0.92,"Paper behandelt substantiell Prompting-Strategien zur Bias-Mitigation in generativen KI-Systemen (Bildgeneratoren). TECHNIK erfüllt: AI_Literacies (Kompetenzentwicklung für inklusives Prompting), Generative_KI (Bildgeneratoren), Prompting (Prompt-Engineering als Kernmethodologie). SOZIAL erfüllt: Bias_Ungleichheit (Stereotypen-Analyse), Diversitaet (inklusive Repräsentation), Gender (implizit durch Stereotypen), Fairness (Bias-Mitigation). Beide Bedingungen erfüllt → Include.",6,10,60.00%
289,Bias in decision-making for AI's ethical dilemmas: A comparative study of ChatGPT and Claude,Wu (2025),Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Include,Include,Ja,,,Experimentell,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK durch Generative_KI (Stable Diffusion-Analyse). SOZIAL_OK durch Bias_Ungleichheit (Stereotype, Diskriminierung), Gender (Sexismus-Fokus), Diversitaet (Inklusion marginalisierter Gruppen), und Feministisch (explizite feministische intersektionale Methodik nach Crenshaw-Tradition, kritische Machtanalyse). Substantielle qualitative Analyse von AI-Outputs unter sozialkritischer Perspektive.",8,10,80.00%
290,What is Feminist AI?,Wudel (2025),Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Unclear,Include,Nein,,,Konzept,Konzept,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, Generative_KI, Prompting alle Ja), SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Fairness alle Ja). Framework für verantwortungsvolles Prompt Engineering mit explizitem Fokus auf ethische Prinzipien, Menschenrechte und Diversität. Keine Soziale_Arbeit oder feministischen Theoriebezug identifiziert.",3,10,30.00%
291,What is Feminist AI?,Wudel (2025),Ja,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Exclude,Include,Nein,Duplicate,,Konzept,Empirisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: NLP/ML für Datenanalyse; AI_Literacies: feministische digitale Kompetenzen) und SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness substantiell behandelt). Empirische Analyse von 172K Tweets zu Geschlechter-Perspektiven auf algorithmischen Bias mit explizit feministischem Rahmen.",8,10,80.00%
292,What is Feminist AI?,Wudel (2025),Ja,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Exclude,Include,Nein,Duplicate,,Konzept,Literaturreview,0.88,"Paper erfüllt BEIDE Bedingungen: TECHNIK (AI_Literacies: digitale Literacy als Intervention + KI_Sonstige: Bias in Hiring/Healthcare/Finance-Systemen) UND SOZIAL (Gender: expliziter Gender-Bias-Fokus; Bias_Ungleichheit: Underrepresentation von Frauen; Diversitaet: Women-led AI projects; Fairness: algorithmic bias combating). Substantielle Behandlung von Gender und Bias in KI-Systemen. Keine explizit feministische Theorie erkennbar, daher Feministisch=Nein.",7,10,70.00%
293,Transparency enhances positive perceptions of social artificial intelligence,Xu (2023),Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Exclude,Include,Nein,Not relevant topic,,Empirisch,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies, Generative_KI, KI_Sonstige durch intersektionale AI-Strategien und Chatbot-Beispiel) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch durch explizite intersektionale und queer-feministische Theorie nach Crenshaw; Fairness durch Data Nutrition Label für Bias-Assessment). Substantiell behandelt.",4,10,40.00%
294,Transparency enhances positive perceptions of social artificial intelligence,Xu (2023),Ja,Ja,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,Exclude,Exclude,Ja,Duplicate,Not_relevant_topic,,Experimentell,0.85,"Paper erfüllt TECHNIK-Bedingung (Generative_KI, KI_Sonstige) und SOZIAL-Bedingung (Bias_Ungleichheit, Gender, Fairness). ABER: Kein Bezug zu Sozialer Arbeit oder deren Zielgruppen/Praxis. Rein technische Studie zu LLM-Bias ohne Anwendungskontext in Sozialbereich. Für Literature Review Soziale Arbeit nicht relevant.",0,0,
295,Promises and challenges of generative artificial intelligence for human learning,Yan (2024),Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Include,Include,Ja,,,Konzept,Konzept,0.92,"Paper erfüllt TECHNIK-Kriterium (Generative_KI + Prompting + AI_Literacies) und SOZIAL-Kriterium (Bias_Ungleichheit + Gender + Diversitaet + Fairness). Substantieller Fokus auf Prompt-Engineering-Strategien zur Reduktion von Bias in generativen KI-Bildern. Adressiert stereotype Darstellungen und präsentiert konkrete inklusive Prompting-Techniken. Kein explizit feministischer oder sozialarbeiterischer Bezug, daher keine Ja-Vergabe für diese Kategorien.",6,10,60.00%
296,Algorithmic-Assisted Decision-Making Tools in Child Welfare Practice: A Systematic Review,Yu (2025),Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Literaturreview,Theoretisch,0.95,"Paper erfüllt TECHNIK-Kriterium durch KI_Sonstige (algorithmische Entscheidungssysteme in Hiring, Healthcare, Law Enforcement). SOZIAL-Kriterium erfüllt durch: Bias_Ungleichheit (Diskriminierung, Bias in automatisierten Systemen), Gender (Unterrepräsentation von Frauen in AI), Diversitaet (marginalisierte Gruppen), Feministisch (explizit feministische Perspektive und Rahmen). Substanzielle Behandlung aller Kategorien im Abstract dokumentiert.",6,10,60.00%
297,The cultural stereotype and cultural bias of ChatGPT,Yuan (2025),Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Experimentell,Literaturreview,0.95,"Paper analysiert Intersektionalität in KI-Systemen explizit mit feministischer Perspektive (Intersektionalitätsagenda nach Crenshaw-Tradition). Thematisiert substantiell: algorithmische Bias/Diskriminierung (KI_Sonstige), strukturelle Ungleichheit (Bias_Ungleichheit), Gender-Perspektive (homogene Gruppen weißer Männer), Diversität in AI-Entwicklung (Diversitaet), feministische Technikforschung (Feministisch) und Fairness-Probleme. Beide Bedingungen erfüllt: Technik (KI_Sonstige=Ja) + Sozial (multiple Kategorien=Ja).",6,10,60.00%
298,MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models,Yunusov (2024),Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Include,Include,Ja,,,Experimentell,Empirisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies: Ja, Generative_KI: Ja); SOZIAL_OK (Bias_Ungleichheit: Ja, Gender: Ja, Diversitaet: Ja). Qualitative empirische Studie zu generativen KI in Bildung mit explizitem Gender-Fokus und Analyse von Bias-Risiken. Kein explizit feministischer Theoriebezug erkannt, daher Feministisch=Nein. Keine Prompting-Fokussierung oder sozialarbeiterischer Bezug.",7,10,70.00%
299,Tensions in digital welfare states: Three perspectives on care and control,Zakharova (2024),Ja,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,Exclude,Include,Nein,Not relevant topic,,,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK durch AI_Literacies (Framework für KI-Kompetenzen/ethische Prinzipien) und KI_Sonstige (ML-Design, algorithmische Systeme). SOZIAL_OK durch Feministisch (explizite Data Feminism Theorie nach D'Ignazio), Bias_Ungleichheit (Fokus auf diskriminatorische Systeme), Gender (Geschlechterperspektive), Diversitaet (Pluralismus, marginalisierte Gruppen), Fairness (gerechte KI-Entwicklung). Substantielle Behandlung aller Dimensionen.",3,5,60.00%
300,Intersectional Fairness: A Fractal Approach,Zannone (2023),Ja,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Include,Include,Ja,,,Theoretisch,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK-OK durch KI_Sonstige (Bias- und Fairness-Fokus auf AI-Systeme). SOZIAL-OK durch Bias_Ungleichheit, Gender, Diversitaet, Feministisch (explizit intersektionales Feminist AI Framework) und Fairness. Verwendet feministische Theorie substantiell als Kernrahmen zur AI-Kritik. Keine Soziale_Arbeit-Verbindung erkannt.",10,10,100.00%
301,Scaling implicit bias analysis across transformer-based language models through embedding association test and prompt engineering,Zayed (2024),Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Experimentell,Literaturreview,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: ML-Systeme, Fairness-Metriken); SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch via intersektionale Analyse). Explizit intersektionales Framing (Crenshaw-Tradition) mit Fokus auf marginalisierte Gruppen und Machtstrukturen ist substantiell feministisch.",6,10,60.00%
302,"Governing discriminatory content in conversational AI: A cross-system, cross-lingual, and cross-topic audit",Zeng (2025),Ja,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Include,Include,Ja,,,Experimentell,Literaturreview,0.95,"Paper erfüllt beide Bedingungen: (1) TECHNIK: AI_Literacies (Digital Literacy Programme), KI_Sonstige (algorithmische Systeme in Recruitment/Healthcare/Finance); (2) SOZIAL: Bias_Ungleichheit (systematische Gender-Biases), Gender (expliziter Gender-Fokus), Diversitaet (Unterrepräsentation von Frauen), Fairness (algorithmische Fairness). Kein feministischer Theorierahmen explizit erwähnt, daher konservativ Nein.",6,10,60.00%
303,Learning About AI: A Systematic Review of Reviews on AI Literacy,Zhang (2025),Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Exclude,Include,Nein,,,Literaturreview,Literaturreview,0.92,"TECHNIK_OK: KI_Sonstige=Ja (algorithmische Systeme und deren Auswirkungen). SOZIAL_OK: Bias_Ungleichheit=Ja (Gender-Bias in 44% der KI-Systeme), Gender=Ja (expliziter Gender-Fokus), Fairness=Ja (Policy-Empfehlungen). Kein direkter Soziale_Arbeit-Bezug erkennbar. Nicht feministisch theoretisch fundiert, aber empirisch-analytisch zu Gender-Bias.",5,10,50.00%
304,Thinking like a scientist: Can interactive simulations foster critical AI literacy?,Zhao (2025),Ja,Ja,Ja,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Include,Include,Ja,,,Empirisch,Empirisch,0.95,Paper kombiniert TECHNIK (AI_Literacies: KI-Ethik-Verständnis; KI_Sonstige: algorithmische Bias) mit SOZIAL (Bias_Ungleichheit: Analyse von Bias-Konzepten; Gender + Diversität + Feministisch: expliziter feministischer Ansatz; Fairness: Design Justice). Empirische Studie mit Tech-Worker-Perspektive und Design-Justice-Fokus erfüllt beide Bedingungen eindeutig.,6,10,60.00%
305,Data feminism for AI,Klein & D'Ignazio (2024),Nein,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,,Include,,,,,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK durch KI_Sonstige (allgemeiner KI/ML-Bezug); SOZIAL_OK durch Bias_Ungleichheit, Gender, Diversitaet und Feministisch (explizite Verwendung des Data-Feminism-Frameworks von D'Ignazio, intersektionale feministische Perspektive). Starker konzeptioneller Beitrag zu equitable und ethical AI.",0,0,
306,Incubating Feminist AI: Executive Summary 2021-2024,A+ Alliance (2024),Nein,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,,Include,,,,,Konzept,0.92,"Das Paper erfüllt beide Bedingungen: TECHNIK (KI_Sonstige: Bias-Detection in NLP, algorithmische Systeme im sozialen Kontext) und SOZIAL (Feministisch: explizit feminist AI; Gender: gender-based violence; Soziale_Arbeit: praktische Anwendungen bei Gewalt und Justiz; Bias_Ungleichheit: Bias-Detection; Diversitaet: intersektionale Analyse, multi-regionale Gemeinschaften). Starker feministischer Rahmen mit substantiellem Fokus auf strukturelle Gerechtigkeit.",0,0,
307,Recommendation on the Ethics of Artificial Intelligence,UNESCO (2021),Nein,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Include,,,,,Konzept,0.85,"UNESCO-Empfehlung behandelt AI Ethics als globale Governance-Frage (KI_Sonstige: Ja). Substantieller Fokus auf Gender Equality, Geschlechterstereotypen und diskriminatorische Biases (Gender: Ja, Bias_Ungleichheit: Ja), sowie equitable participation und Fairness (Diversitaet: Ja, Fairness: Ja). Erfüllt beide Bedingungen: TECHNIK (KI_Sonstige) + SOZIAL (Gender, Bias, Fairness). Nicht-Feministisch, da keine explizite feministische Theorie erkennbar.",0,0,
308,Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation,An et al. (2025),Nein,Ja,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,,Include,,,,,Experimentell,0.95,"Paper kombiniert Technik (Generative_KI: Evaluation von LLMs) mit mehreren sozialen Kategorien. Explizit intersektionale Analyse (Feministisch: Crenshaw'sche Intersektionalität), Gender- und Diversitätsfokus, algorithmischer Bias in Hiring-Kontexten (Bias_Ungleichheit, Fairness). Erfüllt beide Bedingungen klar.",0,0,
309,Factoring the matrix of domination: A critical review and reimagination of intersectionality in AI fairness,Ovalle et al. (2023),Nein,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,,Include,,,,,Literaturreview,0.95,"Paper adressiert AI Fairness (KI_Sonstige) mit explizit feministisch-theoretischem Rahmen (Collins, Bilge). Substantielle Behandlung von Intersektionalität (Feministisch, Gender, Diversität, Bias_Ungleichheit). Kritische Analyse algorithmischer Systeme unter intersektionaler Perspektive. TECHNIK (KI_Sonstige) + SOZIAL (Bias, Gender, Diversität, Feministisch, Fairness) erfüllt.",0,0,
310,"Gender, race, and intersectional bias in AI resume screening via language model retrieval",Wilson & Caliskan (2024),Nein,Ja,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,,Include,,,,,Empirisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + KI_Sonstige: LLM-basierte Systeme für Resume Screening), SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch via explizite intersektionale Analyse nach Crenshaw-Theorie, Fairness). Substanzielle empirische Analyse von Diskriminierung in KI-Systemen mit intersektionalem Fokus.",0,0,
311,"Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligent Machines",Browne et al. (2023),Nein,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,,Include,,,,,Literaturreview,0.95,"Sammlung mit explizit feministischer Perspektive (TECHNIK: KI_Sonstige durch algorithmische Systeme und deren gesellschaftliche Auswirkungen; SOZIAL: Bias_Ungleichheit, Gender, Diversitaet und Feministisch alle substantiell durch Fokus auf Gerechtigkeit, Diskriminierung und feministische Theorie). Beide Bedingungen erfüllt → Include.",0,0,
312,Prompting fairness: Learning prompts for debiasing large language models,Chisca et al. (2024),Nein,Ja,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Exclude,,,"TECHNIK_OK (Prompting + KI_Sonstige erfüllt), aber SOZIAL_OK nicht erfüllt - nur Bias/Fairness im technischen Sinne, kein Bezug zu sozialer Arbeit, Gesellschaft oder marginalisierten Gruppen",,Experimentell,0.92,"Paper behandelt Prompt-Tuning für Debiasing von LLMs und adressiert Fairness/Bias-Reduktion. Dies sind technische Kategorien. Allerdings fehlt ein substantieller Bezug zu sozialen Auswirkungen, sozialarbeiterischer Praxis oder marginalisierten Gruppen. Die Bias-Thematisierung ist rein algorithmisch-technisch, nicht sozialwissenschaftlich verankert. Damit erfüllt das Paper die SOZIAL-Bedingung nicht.",0,0,
313,Bias and fairness in large language models: A survey,Gallegos et al. (2024),Nein,Ja,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Exclude,,,Not_relevant_topic,,Literaturreview,0.85,"Das Paper erfüllt die TECHNIK-Bedingung (Generative_KI, Bias_Ungleichheit, Fairness alle Ja). Jedoch fehlt die SOZIAL-Bedingung im Sinne des Literature Reviews: Bias_Ungleichheit und Fairness sind zwar vorhanden, aber das Paper behandelt allgemeine KI-Ethik und -Fairness OHNE spezifischen Bezug zu Sozialer Arbeit, sozialen Zielgruppen oder sozialarbeiterischen Kontexten. Die Umfrage konzentriert sich auf technische Metriken und NLP-Debiasing, nicht auf Anwendungen im Sozialbereich. Für den Review erforderlich: KI im Kontext Sozialer Arbeit oder marginalisierten Gruppen in sozialen Handlungsfeldern.",0,0,
314,Generative AI and opportunities for feminist classroom assignments,Small (2023),Nein,Ja,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Include,,,,,Konzept,0.85,"Paper behandelt Integration generativer KI in Bildung (AI_Literacies: Ja, reflexive Kompetenzentwicklung). Fokus auf generative AI-Tools (Ja). Explizit feministische Perspektive und Epistemologie (Feministisch: Ja). Gender-Fokus durch feministische Theorieentwicklung (Gender: Ja). TECHNIK erfüllt (AI_Literacies + Generative_KI), SOZIAL erfüllt (Feministisch + Gender). Kein direkter Sozialer-Arbeit-Bezug, daher Nein dort.",0,0,
315,The power of prompts: Evaluating and mitigating gender bias in MT with LLMs,Sant et al. (2024),Nein,Ja,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,,Exclude,,,"Technik-Kriterien erfüllt (Generative_KI, Prompting), aber kein Bezug zu Sozialer Arbeit, Diversität oder implizit-feministischen Ansätzen. Gender und Bias werden zwar adressiert, aber in einem rein technischen ML-Kontext ohne sozialarbeitsbezogene Anwendung.",,Experimentell,0.85,"Das Paper behandelt LLMs (Generative_KI=Ja), Prompt-Engineering (Prompting=Ja), Gender-Bias (Gender=Ja) und Fairness-Metriken (Fairness=Ja). Dies erfüllt die TECHNIK-Bedingung klar. SOZIAL-Bedingung: Bias_Ungleichheit=Ja ist erfüllt. Allerdings liegt der Fokus ausschließlich auf technischer Sprachübersetzung ohne direkten Bezug zu Sozialer Arbeit oder ihren Zielgruppen/Praktiken. Für Inclusion ist substantieller Bezug zu Soziale_Arbeit erforderlich oder starke strukturelle Ungleichheits-Dimension. Hier handelt es sich um ein reines ML/NLP-Paper mit Gender-Bias-Fokus.",0,0,
316,Feminist reflections for the development of Artificial Intelligence,Derechos Digitales (2023),Nein,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Nein,,,Include,,,,,Konzept,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja, da methodologische Frameworks für AI-Entwicklung), SOZIAL_OK (Feministisch=Ja durch explizite feministische Perspektive; Gender=Ja, Latin American women focus; Diversitaet=Ja, intersektionale Ansätze; Bias_Ungleichheit=Ja, power-balancing und Autonomie). Kein direkter Sozialarbeitsbezug, aber substantielle feminist AI-Entwicklung.",0,0,
317,Shaping feminist artificial intelligence,Toupin (2024),Nein,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Nein,,,Include,,,,,Literaturreview,0.92,"Das Paper analysiert Feminist AI (FAI) mit explizit feministischer Theorie und Perspektive (erfüllt Feministisch=Ja). Es behandelt algorithmische Systeme und deren gesellschaftliche Auswirkungen (KI_Sonstige=Ja, Bias_Ungleichheit=Ja). Gender und Diversität sind zentral für die Analyse von FAI-Frameworks. TECHNIK erfüllt (KI_Sonstige), SOZIAL erfüllt (Feministisch, Gender, Bias_Ungleichheit, Diversität). Inklusion korrekt.",0,0,
318,Gender und KI-Anwendungen. Trägt KI zum Genderproblem oder zu seiner Lösung bei?,Franken & Mauritz (n.d.),Nein,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Nein,,,Include,,,,,Theoretisch,0.75,"Der Titel signalisiert explizit einen Gender-Fokus im KI-Kontext ('Gender und KI-Anwendungen', Frage zu Genderproblemen). Dies aktiviert KI_Sonstige (allgemeine KI-Anwendungen) und Gender sowie Bias_Ungleichheit (Gender-Bias ist strukturelle Ungleichheit). TECHNIK + SOZIAL erfüllt → Include. Ohne Abstract ist Confidence reduziert; feministischer Theoriebezug unklar.",0,0,
319,Challenging systematic prejudices: an Investigation into Gender Bias in Large Language Models,"UNESCO, IRCAI (2024)",Nein,Ja,,Nein,,,Ja,,,Nein,,,Nein,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Ja,,,Include,,,,,Empirisch,0.95,"Paper untersucht Gender-Bias in LLMs (ChatGPT, GPT-2, Llama 2) – erfüllt TECHNIK-Kriterium (Generative_KI). Expliziter Fokus auf Geschlechter-Stereotypen, soziale Vorurteile und Fairness in generierten Texten erfüllt SOZIAL-Kriterien (Bias_Ungleichheit, Gender, Diversitaet, Fairness). Kein feministischer Theoriebezug. Keine Verbindung zu Sozialer Arbeit, daher nicht relevant für diesen Kontext, aber sachlich korrekt kategorisiert.",0,0,
320,Intersektionalität,Arnold et al. (2023),Nein,Ja,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Nein,,,Exclude,,,No_full_text,,Unclear,0.95,Keine ausreichende Information vorhanden. Titel allein ('Intersektionalität') ermöglicht keine verlässliche Kategorisierung. Ohne Abstract und Volltext können KI-Technologie-Bezug und spezifisches thematisches Fokus nicht bewertet werden. Restriktives Klassifizieren erfordert Ausschluss bei unzureichender Evidenz.,0,0,
321,ARTIFICIAL INTELLIGENCE and GENDER EQUALITY,UNESCO (2020),Nein,Ja,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Include,,,,,Literaturreview,0.85,"Das UNESCO-Paper behandelt substantiell die Themen Gender Equality und AI (Kategorie Gender: Ja). Es adressiert digitale Kompetenzen und Geschlechterkluft (AI_Literacies: Ja), Gender Bias in KI-Systemen (KI_Sonstige: Ja, Bias_Ungleichheit: Ja) sowie Diversität. Beide Bedingungen (Technik + Sozial) sind erfüllt → Include.",0,0,
322,When Good Algorithms Go Sexist: Why and How to Advance AI Gender Equity,Smith & Rustagi (2021),Nein,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,,Include,,,,,Konzept,0.85,"Das Paper adressiert algorithmen-basierte Systeme (KI_Sonstige) mit explizitem Fokus auf Gender-Bias und Geschlechter-Gerechtigkeit (Gender, Bias_Ungleichheit). Es behandelt konkrete Strategien für faire und gender-sensible KI-Entwicklung (Fairness). Beide Bedingungen erfüllt: TECHNIK (KI_Sonstige=Ja) + SOZIAL (Gender=Ja, Bias_Ungleichheit=Ja, Fairness=Ja). Kein explizit feministischer Theoriebezug erkennbar aus dem Titel/Abstract.",0,0,
323,Faires KIPrompting – Ein Leitfaden für Unternehmen. BSP Business and Law School – Hochschule für Management und Recht,"Gengler, & Bodrožić-Brnić, (2024)",Nein,Ja,,Ja,,,Ja,,,Ja,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Nein,,,Ja,,,Include,,,,,Konzept,0.78,"Das Paper behandelt Kompetenzen im Umgang mit generativer KI (Leitfaden für verantwortungsvolle Nutzung), fokussiert explizit auf Prompting und Fairness (‚faires KI-Prompting'), sowie Diversität und Bias-Mitigation. TECHNIK-Bedingung erfüllt (AI_Literacies, Generative_KI, Prompting). SOZIAL-Bedingung erfüllt (Bias_Ungleichheit, Diversitaet, Fairness). Fehlender Bezug zu Sozialer Arbeit spricht nicht gegen Inklusion, da andere soziale Kategorien substantiell adressiert werden.",0,0,
324,feminist AI | ACADEMY,[Author not specified] (n.d.),Nein,Ja,,Ja,,,Nein,,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Nein,,,Ja,,,Ja,,,Include,,,,,Konzept,0.72,"Das Material behandelt KI-Kompetenzen/Literacies durch Workshops und Schulungen zur Erstellung von 'equitable AI'. Der Titel 'feminist AI' und der explizite Fokus auf Gerechtigkeit und Fairness deuten auf feministische und Fairness-Perspektiven hin. Es erfüllt TECHNIK (AI_Literacies) und SOZIAL (Bias_Ungleichheit, Feministisch, Fairness). Allerdings: Kein Volltext vorhanden, daher nur Abstract-basierte Bewertung mit moderater Konfidenz.",0,0,
325,AI & Intersectionality: A Toolkit For Fairness & Inclusion,DIVERSIFAIR Project (2024),Nein,Ja,,Ja,,,Nein,,,Ja,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,,Include,,,,,Konzept,0.95,"Das Toolkit adressiert substantiell KI-Kompetenzen (Bewusstsein und Handlungsstrategien für Stakeholder), Prompting-Strategien (intersektionale Szenarien zur Bias-Aufdeckung) und KI-Systeme generell. Die intersektionale Perspektive erfüllt implizit feministische Ansätze (Crenshaw'sche Intersektionalität). Starker Fokus auf Bias_Ungleichheit, Gender, Diversität und Fairness. TECHNIK (AI_Literacies + Prompting + KI_Sonstige) und SOZIAL (Bias_Ungleichheit + Gender + Diversität + Feministisch + Fairness) beide erfüllt → Include.",0,0,
326,Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,Ulnicane (2024),Nein,Ja,,Nein,,,Nein,,,Nein,,,Ja,,,Nein,,,Ja,,,Ja,,,Ja,,,Ja,,,Ja,,,Include,,,,,Literaturreview,0.95,"Paper erfüllt beide Bedingungen: (1) TECHNIK_OK: KI_Sonstige=Ja (algorithmische Systeme und ihre sozialen Auswirkungen). (2) SOZIAL_OK: Bias_Ungleichheit=Ja (systematische Diskriminierung durch AI), Gender=Ja (Geschlechterstereotypen), Diversitaet=Ja (Diversitätskrisen in AI-Entwicklung), Feministisch=Ja (explizit intersektionale Analyse nach Crenshaw-Tradition). Substantielle Behandlung aller Kategorien, nicht beiläufig.",0,0,
