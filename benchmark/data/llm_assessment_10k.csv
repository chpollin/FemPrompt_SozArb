ID,Zotero_Key,Author_Year,Title,AI_Literacies,Generative_KI,Prompting,KI_Sonstige,Soziale_Arbeit,Bias_Ungleichheit,Gender,Diversitaet,Feministisch,Fairness,Decision,Exclusion_Reason,Studientyp,LLM_Confidence,LLM_Reasoning
1,BHXDU7VM,Chatterji et al. (2025),How People Use ChatGPT,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,No_full_text,Unclear,0.3,"Nur Titel und Abstract vorhanden. Der Titel deutet auf Nutzungsforschung zu ChatGPT hin (Generative_KI, potentiell AI_Literacies). Jedoch fehlt der volle Text zur substanziellen Bewertung. Das Abstract ist nur eine NBER-Standardbeschreibung ohne inhaltliche Details. Keine erkennbaren sozialen Aspekte (Soziale_Arbeit, Bias, Gender, etc.). Daher: Exclude (zu wenig Information + wahrscheinlich keine sozialen Dimensionen)."
2,I78CL6R5,Chiu et al. (2024),What are artificial intelligence literacy and competency? A comprehensive framework to support them,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Konzept,0.7,"Das Paper behandelt AI Literacy und Kompetenzentwicklung im KI-Bereich (AI_Literacies = Ja), erfüllt aber die zweite Bedingung nicht: Es fehlt ein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität, Feminismus oder Fairness. Das Paper ist relevant für AI Literacy generell, nicht aber für den Kontext von KI und Sozialer Arbeit. Ohne Volltext kann die fehlende Sozial-Komponente nicht widerlegt werden."
3,VP6SXQHY,Benlian & Pinski (2025),The AI literacy development canvas: Assessing and building AI literacy in organizations,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,"Nur TECHNIK-Kategorie erfüllt (AI_Literacies), aber keine SOZIAL-Kategorie. Paper behandelt KI-Kompetenzentwicklung in Organisationen allgemein, ohne Bezug zu Sozialer Arbeit, Bias, Ungleichheit, Gender oder Fairness.",Konzept,0.85,"Titel deutet auf AI Literacy Framework hin - klare AI_Literacies-Kategorie. Kein Abstract vorhanden; basierend auf Titel keine Indikation für soziale Dimensionen (Bias, Ungleichheit, Gender, Fairness, Soziale Arbeit). Paper erfüllt nur Technik-Kriterium, nicht Sozial-Kriterium. EXCLUDE nach strikter Logik."
4,AXEIVEW3,Lanzetta et al. (2024),Artificial Intelligence Competence Needs for Youth Workers,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Empirisch,0.95,"Paper adressiert zentral AI-Kompetenzen von Fachkräften (AI_Literacies: Ja - Kompetenzrahmen für Jugendhilfe), behandelt Generative AI (Ja - explizit Gen Z und generative Tools), hat direkten Bezug zu Sozialarbeit/Jugendhilfe (Ja - Youth Workers, Jugendhilfesektor). Empirische Studie mit Fokusgruppen und Interviews. Beide Bedingungen erfüllt: Technik (AI_Literacies + Generative_KI) UND Sozial (Soziale_Arbeit)."
5,UFE85SCV,Casal-Otero et al. (2023),AI literacy in K-12: a systematic literature review,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Only_Technik_erfüllt,Literaturreview,0.95,"Das Paper ist eine systematische Literaturreviw zu AI Literacy im K-12 Bildungskontext und behandelt substantiell AI-Kompetenzen, Curricula und Lernansätze (AI_Literacies = Ja). Es erfüllt jedoch KEINE sozialen Kriterien (Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness alle Nein). Der Fokus liegt rein auf Bildungsintegration und Kompetenzentwicklung, nicht auf sozialen Auswirkungen, Ungleichheiten oder Soziale Arbeit. Beide Bedingungen (Technik UND Sozial) sind nicht erfüllt → Exclude."
6,J9IZDVTW,Biagini et al. (2024),"Less knowledge, more trust? Exploring potentially uncritical attitudes towards AI in higher education",Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,"Only TECHNIK-Dimension erfüllt (AI_Literacies), aber keine SOZIAL-Dimension. SOZIAL-Kriterium nicht erfüllt: Keine direkten Bezüge zu Sozialer Arbeit, keine substantielle Behandlung von Bias/Ungleichheit, Gender, Diversität, Feminismus oder Fairness. Die ethischen Implikationen werden allgemein erwähnt, aber nicht substantiell in einer der SOZIAL-Kategorien behandelt.",Empirisch,0.92,"Paper untersucht AI Literacy bei Doktoranden auf vier Dimensionen (kognitiv, operativ, kritisch, ethisch). Dies erfüllt AI_Literacies-Kriterium substantiell. Jedoch adressiert das Paper keinen direkten Bezug zu Sozialer Arbeit, behandelt Bias/Ungleichheit nicht fokussiert und hat keinen Gender-, Diversitäts-, Feminist. oder Fairness-Schwerpunkt. Allgemeine Ethik-Erwähnung reicht nicht aus."
7,P82X89Q4,Chee et al. (2025),A Competency Framework for AI Literacy: Variations by Different Learner Groups and an Implied Learning Pathway,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Literaturreview,0.95,"Das Paper entwickelt ein umfassendes AI-Literacy-Kompetenzframework und erfüllt die TECHNIK-Bedingung substanziell (AI_Literacies=Ja). Allerdings fehlt jeglicher SOZIAL-Bezug: Es gibt keinen Fokus auf Soziale Arbeit, Bias/Ungleichheit, Gender, Diversität, feministische Perspektiven oder Fairness. Das Paper behandelt rein Bildungscurricula und Kompetenzentwicklung ohne sozialen oder kritischen Perspektivbezug. Die SOZIAL-Bedingung ist nicht erfüllt."
8,KPSF8EZE,Chiu (2025),"AI literacy and competency: definitions, frameworks, development and future research directions",Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,No_full_text / SOZIAL-Kriterium nicht erfüllt,Literaturreview,0.85,"Das Paper behandelt substantiell AI Literacy und Kompetenzen (AI_Literacies = Ja). Allerdings fehlt jeglicher Bezug zu sozialen Aspekten wie Soziale Arbeit, Bias, Gender, Diversität, Fairness oder feministische Perspektiven. TECHNIK-Kriterium erfüllt, aber SOZIAL-Kriterium nicht. Daher: Exclude."
9,XCS4YCQH,European Commission. Joint Research Centre. (2017),DigComp 2.1: the digital competence framework for citizens with eight proficiency levels and examples of use.,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Konzept,0.85,"DigComp 2.1 ist ein Rahmenwerk für digitale Kompetenzen im Allgemeinen, nicht spezifisch für KI/ML-Systeme. Es behandelt digitale Literalität umfassend, aber nicht AI Literacy im Sinne der Definition (KI-spezifische Kompetenzen, technisches KI-Verständnis). Keine Bezüge zu sozialen Arbeit, Bias, Fairness oder kritische KI-Reflexion erkennbar. Technik-Kategorie nicht erfüllt."
10,SY8LNID7,Arias López et al. (2023),Digital literacy as a new determinant of health: A scoping review,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Exclude,"Wrong_publication_type und Not_relevant_topic - Paper adressiert Digital Health Literacy im medizinischen Kontext, nicht KI-Literacies. Thema ist Gesundheitswesen, nicht KI/Technik. Keine substantielle Verschränkung von KI-Technologie und Sozialer Arbeit.",Literaturreview,0.95,"Das Paper ist ein Scoping Review zu Digital Health Literacy im Gesundheitssektor. Obwohl es marginale Überschneidungen mit AI_Literacies und Bias/Ungleichheit hat, fehlt jede substanzielle Verknüpfung zu KI-Technologien oder sozialer Arbeit. Es behandelt digitale Gesundheitskompetenz allgemein, nicht KI-spezifisch, und hat keinen Bezug zu Sozialer Arbeit als Praxis/Theorie."
11,43CRXRRT,Tinmaz et al. (2022),A systematic review on digital literacy,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Literaturreview,0.95,"Das Paper behandelt digitale Literalität allgemein, NICHT AI Literacy im Sinne von Kompetenzen im Umgang mit KI-Systemen. Der Fokus liegt auf digital competencies, digital skills und digital thinking ohne spezifischen Bezug zu KI, generativen Modellen oder algorithmischen Systemen. Weder Technologie- noch Sozialkomponenten des relevanten Spektrums werden erfüllt."
12,HTXCWNQ9,Peng et al. (2022),A Literature Review of Digital Literacy over Two Decades,Nein,Nein,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Exclude,Not_relevant_topic,Literaturreview,0.95,"Das Paper behandelt digitale Literalität von Studierenden im Allgemeinen, nicht KI-spezifische Kompetenzen (AI Literacy). Gender wird erwähnt als Einflussfaktor, aber substantiell nicht fokussiert. Keine KI/ML-Technologie-Komponente. Kein Bezug zu Sozialer Arbeit. TECHNIK-Bedingung nicht erfüllt → Exclude."
13,7G78SADI,Pinski & Benlian (2024),"AI literacy for users – A comprehensive review and future research directions of learning methods, components, and effects",Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,"Nur TECHNIK-Kategorie erfüllt, SOZIAL-Kategorie nicht erfüllt. Paper behandelt KI-Literacies/Kompetenzen, hat aber keinen substantiellen Bezug zu sozialen Dimensionen (Soziale_Arbeit, Bias/Ungleichheit, Gender, Diversität, Fairness oder feministische Perspektive).",Literaturreview,0.95,"Das Paper fokussiert auf AI Literacy als Kompetenzbreich – ein eindeutiges TECHNIK-Thema (AI_Literacies: Ja). Der Titel und Fokus deuten auf eine Übersichtsarbeit zu Lernmethoden und Effekten hin, ohne soziale/kritische Dimensionen. Ohne Abstract und Hinweise auf Soziale Arbeit oder kritische Bias-/Fairness-Analysen erfüllt das Paper nicht die SOZIAL-Bedingung."
14,3AQ2K3BX,Ng et al. (2021),Conceptualizing AI literacy: An exploratory review,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Literaturreview,0.75,"Das Paper behandelt AI Literacy conceptually durch eine exploratorische Literaturübersicht. Dies erfüllt die TECHNIK-Bedingung (AI_Literacies = Ja). Jedoch keine SOZIAL-Kategorie wird erfüllt. Ohne Abstract und nur Titel ist unklar, ob das Paper eine Sozialarbeitsperspektive, Bias-Diskussion, Diversität oder Fairness integriert. Die STRIKTE Entscheidungslogik verlangt BEIDE Bedingungen (Technik UND Sozial). Mit nur Technik → Exclude."
15,7VFNS5R3,Kong et al. (2021),Evaluation of an artificial intelligence literacy course for university students with diverse study backgrounds,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,"Nur TECHNIK_OK (AI_Literacies erfüllt), aber SOZIAL_OK nicht erfüllt",Empirisch,0.85,"Das Paper behandelt AI Literacy als Bildungsintervention für Studierende mit unterschiedlichen Hintergründen. Während die AI_Literacies-Kategorie klar erfüllt ist (Evaluation eines Kompetenz-Kursus), fehlt jeder Bezug zu sozialen Aspekten wie Bias, Ungleichheit, Gender, Diversität, Fairness oder Sozialer Arbeit. Es ist ein reines Bildungs-Evaluationspaper ohne soziale/kritische Dimension."
16,23Y3627L,Wang et al. (2023),Measuring user competence in using artificial intelligence: validity and reliability of artificial intelligence literacy scale,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Empirisch,0.85,"Das Paper behandelt AI Literacy durch die Entwicklung und Validierung einer Messskala. Obwohl AI_Literacies erfüllt ist (Kompetenzen im Umgang mit KI), fehlt jeglicher Bezug zu sozialen Dimensionen (Soziale_Arbeit, Bias, Gender, Diversität, Fairness). Die Entscheidungslogik erfordert BEIDE Technik UND Sozial. Das Paper ist rein bildungstechnisch ohne soziale Komponente."
17,4BRSDIPP,Laupichler et al. (2023),Development of the “Scale for the assessment of non-experts’ AI literacy” – An exploratory factor analysis,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,"Fehlende SOZIAL-Komponente: Das Paper behandelt AI Literacy (TECHNIK erfüllt), adressiert aber keine der erforderlichen sozialen Dimensionen (Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness). Es ist ein allgemeines Education/Assessment-Paper ohne Bezug zu Sozialer Arbeit, Ungleichheit oder Fairness.",Empirisch,0.85,"Der Titel deutet auf ein Instrument zur Messung von AI Literacy hin – ein TECHNIK-Thema. Jedoch fehlt jede Evidenz für soziale Dimensionen wie Anwendung in der Sozialen Arbeit oder Fokus auf Bias/Fairness. Das Paper erfüllt nur eine Bedingung der strikten Entscheidungslogik (TECHNIK_OK, aber nicht SOZIAL_OK). Daher: Exclude."
18,YDW5TX8M,Deuze & Beckett (2022),"Imagination, Algorithms and News: Developing AI Literacy for Journalism",Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,"TECHNIK_OK (AI_Literacies + KI_Sonstige = Ja), aber SOZIAL nicht erfüllt. Paper behandelt AI Literacy im Journalismus, nicht in Sozialer Arbeit. Kein substantieller Bezug zu Bias, Fairness, Gender, Diversität oder feministischen Ansätzen erkennbar. Soziale Arbeit ist nicht Zielgruppe oder Anwendungsbereich.",Unclear,0.75,"Das Paper adressiert AI Literacy (TECHNIK erfüllt), aber der Fokus liegt auf Journalismus, nicht auf Sozialer Arbeit oder damit verbundenen sozialen Dimensionen wie Bias, Fairness oder Diversität. Keine der SOZIAL-Kategorien ist erfüllt. Ausschluss erforderlich."
19,BBLJ4RG3,Kong et al. (2024),Developing an artificial intelligence literacy framework: Evaluation of a literacy course for senior secondary students using a project-based learning approach,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,"Nur Technik-Kategorie erfüllt (AI_Literacies), aber keine Sozial-Kategorie",Empirisch,0.85,"Das Paper entwickelt ein KI-Literacy-Framework und evaluiert einen Kurs für Sekundarschüler – klarer Fokus auf AI Literacies als Kompetenzentwicklung. Jedoch kein Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität, Feminismus oder Fairness erkennbar. Erfüllt nur TECHNIK-Bedingung, nicht SOZIAL-Bedingung."
20,FJQ6XYHH,Kong et al. (2025),Artificial Intelligence (AI) literacy – an argument for AI literacy in education,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,No_full_text - Nur Titel und Metadaten verfügbar; SOZIAL-Kriterium nicht erfüllt,Konzept,0.85,"Titel deutet auf AI Literacy in Bildung hin (TECHNIK-Kategorie erfüllt). Ohne Abstract/Volltext ist keine reliable Bewertung möglich. Kritisch: Kein erkennbarer Bezug zu Sozialer Arbeit, Bias, Gender, Diversität, Fairness oder feministischen Perspektiven. Das Paper adressiert offenbar allgemeine Bildung, nicht die erforderliche soziale Dimension. SOZIAL-Kriterium nicht erfüllt → Exclude."
21,JMAWNUEV,Jin et al. (2025),GLAT: The generative AI literacy assessment test,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,"Nur TECHNIK erfüllt (AI_Literacies + Generative_KI), aber kein SOZIAL-Kriterium (Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness). Beide Bedingungen sind notwendig.",Konzept,0.85,"Das Paper behandelt ein Assessment-Tool für generative KI-Kompetenzen und liegt damit im Bereich AI Literacies. Es fehlt jedoch jeder erkennbare Bezug zu sozialen Dimensionen (Soziale Arbeit, Bias, Gender, Diversität, Fairness). Das Paper adressiert rein bildungstechnische Aspekte von KI-Kompetenzermittlung ohne sozialwissenschaftliche oder kritische Perspektive."
22,7U29SIC8,Sperling et al. (2024),In search of artificial intelligence (AI) literacy in teacher education: A scoping review,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,"Not_relevant_topic - KI-Literacies in Lehrerausbildung, nicht Soziale Arbeit",Literaturreview,0.92,"Das Paper behandelt AI Literacy explizit, aber ausschließlich im Kontext von Lehrerausbildung (Teacher Education). Es fehlt jeder Bezug zu Sozialer Arbeit, Bias, Fairness, Gender oder Diversität. Die SOZIAL-Kategorie ist nicht erfüllt: Weder Soziale_Arbeit noch eine der Ungleichheits-/Fairness-Kategorien sind relevant. Daher: Exclude nach Konsistenzregel."
23,3K9RDWLY,Pinski & Benlian (2023),AI Literacy - Towards Measuring Human Competency in Artificial Intelligence,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,"Technik-Komponente erfüllt (AI_Literacies), aber keine Sozial-Komponente. Fehlt: direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder Fairness. Das Paper behandelt AI Literacy als generische Kompetenz ohne spezifischen Sozialarbeitsbezug.",Konzept,0.85,"Das Paper adressiert AI Literacy als Messung von KI-Kompetenzen (TECHNIK_OK). Es fehlt jedoch jede SOZIAL-Komponente: kein Bezug zu Sozialer Arbeit, keinen spezifischen Fokus auf Bias, Gender, Diversität oder Fairness. Ohne Sozialarbeitsbezug oder sozialwissenschaftliche Kritik wird es ausgeschlossen."
24,2SNYUZG4,Yan et al. (2024),Promises and challenges of generative artificial intelligence for human learning,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,No_full_text,Unclear,0.3,"Basierend auf dem Titel behandelt das Paper generative KI und menschliches Lernen, was auf AI_Literacies und Generative_KI hindeutet. Allerdings fehlt das Abstract und der Volltext für eine verlässliche Bewertung. Ohne substantielle Informationen kann nicht überprüft werden, ob ein echter Sozialarbeits- oder Bias-/Fairness-Bezug vorhanden ist. Die restriktive Klassifizierungsweisung verbietet die Annahme von Sozialbezug nur aufgrund von 'learning'. Exclude."
25,SE579V7B,Ng et al. (2021),Conceptualizing AI literacy: An exploratory review,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,"Nur Technik-Kategorie erfüllt (AI_Literacies), keine Sozial-Kategorie vorhanden. Keine Bedingung 2 erfüllt.",Literaturreview,0.85,"Das Paper ist explizit ein exploratory review zu AI Literacy und behandelt somit substantiell KI-Kompetenzen und Wissenstransfer. Es erfüllt die TECHNIK-Bedingung. Jedoch gibt es keinen erkennbaren Bezug zu sozialen Dimensionen wie Soziale Arbeit, Bias/Ungleichheit, Gender, Diversität, Feminismus oder Fairness. Ohne Abstract schwer zu beurteilen, aber Titel deutet auf rein konzeptionelle/pädagogische Behandlung hin. Daher EXCLUDE nach strenger Logik."
26,KI9GRGHB,Marjanovic et al. (2022),Theorising algorithmic justice,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Include,,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK durch KI_Sonstige (algorithmische Entscheidungssysteme im Sozialbereich). SOZIAL_OK durch Soziale_Arbeit (direkte Anwendung auf Wohlfahrtsdienste), Bias_Ungleichheit (Diskriminierung marginalisierter Gruppen), Diversitaet (vulnerable Populationen, marginalisierte Gruppen) und Fairness (Fairness/Equity als zentrale Dimension des Frameworks). Theoretischer Artikel mit substantiellem Fokus auf algorithmische Gerechtigkeit im Sozialbereich."
27,AMYZFAPH,Perron et al. (2023),Recommendations for social work researchers and journal editors on the use of generative AI and large language models,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Konzept,0.92,"Paper adressiert substantiell AI_Literacies (kritische Reflexion über KI-Nutzung), Generative_KI (LLMs im Fokus) und Prompting (explizit prompt engineering erwähnt). Direkter Bezug zur Soziale_Arbeit durch Zielgruppe (social work researchers) und Empfehlungen für Praxis. TECHNIK_OK (3/4 Kategorien erfüllt) UND SOZIAL_OK (Soziale_Arbeit erfüllt). Einschluss gerechtfertigt."
28,VSZM7CT6,Hodgson et al. (2022),"Problematising artificial intelligence in social work education: Challenges, issues and possibilities",Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Include,,Theoretisch,0.95,"Paper adressiert substantiell AI_Literacies durch kritische Diskussion von Kompetenzen und Wissen im Umgang mit KI in der Ausbildung (critical digital literacies). Direkter Bezug zu Soziale_Arbeit durch Fokus auf Social Work Education und berufliche Vorbereitung. Bias_Ungleichheit ist zentral: kritische Auseinandersetzung mit ungleichen Machtverteilungen, die technologischer Wandel verstärkt. TECHNIK (AI_Literacies=Ja) + SOZIAL (Soziale_Arbeit=Ja + Bias_Ungleichheit=Ja) erfüllt → Include."
29,EXRF5629,Spaulding et al. (2023),Predicting successful placements for youth in child welfare with machine learning,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Empirisch,0.85,"Paper behandelt ML-basierte Entscheidungssysteme (Gradient Boosting) in der Jugendhilfe (KI_Sonstige=Ja). Direkter Bezug zu sozialarbeiterischer Praxis durch Vorhersage von Platzierungserfolgen im Child Welfare System (Soziale_Arbeit=Ja). TECHNIK und SOZIAL erfüllt → Include. Keine explizite Behandlung von Bias, Fairness oder Gender-Aspekten trotz hoher sozialer Relevanz algorithmischer Entscheidungen in diesem Kontext."
30,NSI6S5QE,National Association of Social Workers et al. (2017),"NASW, ASWB, CSWE, & CSWA standards for technology in social work practice",Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Konzept,0.92,"Das Paper etabliert Standards für Technologienutzung in der Sozialarbeit, direkt relevant für KI-Anwendungen. AI_Literacies: Ja (Kompetenzanforderungen für Sozialarbeiter in Technologienutzung). KI_Sonstige: Ja (algorithmische Systeme im Sozialbereich adressiert durch Ethics-Framework). Soziale_Arbeit: Ja (Landmark-Standards von vier Major-Verbänden für die Profession). TECHNIK + SOZIAL erfüllt → Include."
31,7FEFMCBZ,Cheng et al. (2022),How child welfare workers reduce racial disparities in algorithmic decisions,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Empirisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK (KI_Sonstige: algorithmisches Entscheidungssystem in Jugendhilfe, Fairness: explizit Racial Bias und Fairness-Aspekte) und SOZIAL (Soziale_Arbeit: direkter Bezug zu Child Welfare Practice, Bias_Ungleichheit: Analyse rassistischer Diskriminierung, Fairness: Algorithmic Fairness in sozialen Systemen). Substantielle Mixed-Methods-Studie mit hohem Relevanzpotential für Sozialarbeit und algorithmische Gerechtigkeit."
32,LR8Z3YHP,Wadmann & Hoeyer (2020),'Meaningless work': How the datafication of health reconfigures knowledge about work and erodes professional judgement,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Empirisch,0.95,"Das Paper behandelt Datafizierung und Digitalisierung im Gesundheitswesen, nicht aber KI/ML-Systeme spezifisch. Es fehlt jeglicher Bezug zu KI-Technologien (klassisches ML, generative KI, Prompting, AI Literacies). Ohne TECHNIK-Komponente kann die inklusive Entscheidungslogik nicht erfüllt werden. Kritik von Algorithmen/Datenregimen reicht nicht ohne KI-Technologiebezug."
33,P4YQIKJX,Li et al. (2023),Ethics & AI: A systematic review on ethical concerns and related strategies for designing with AI in healthcare,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Exclude,"TECHNIK_OK (KI_Sonstige=Ja), aber SOZIAL_OK nicht erfuellt. Bias_Ungleichheit und Fairness sind vorhanden, aber Soziale_Arbeit=Nein. Das Paper behandelt Healthcare-KI (allgemein medizinisch), nicht Soziale Arbeit oder deren Zielgruppen. 'Direkter Bezug zu sozialarbeiterischer Praxis' ist nicht vorhanden, nur allgemeine ethische Konzepte im Healthcare-Kontext.",Literaturreview,0.85,"Paper ist Ethik-Review zu KI im Healthcare mit substantiellem Fokus auf algorithmischen Bias und Fairness. Erfüllt TECHNIK-Bedingung (KI_Sonstige). Aber: Kein direkter Bezug zu Sozialer Arbeit oder deren Zielgruppen erkennbar; Bias/Fairness ohne sozialarbeiterischen Kontext. Orientierung an Healthcare, nicht Soziale Arbeit."
34,R7V99ERA,Field et al. (2023),Examining risks of racial biases in NLP tools for child protective services,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Include,,Empirisch,0.95,"Paper erfüllt beide Kriterien: (1) TECHNIK: KI_Sonstige=Ja (NLP für Risikobewertung, algorithmische Systeme im Sozialbereich); (2) SOZIAL: Soziale_Arbeit=Ja (Child Protective Services), Bias_Ungleichheit=Ja (racial bias, automated discrimination), Diversitaet=Ja (Black/Latinx families), Fairness=Ja (bias in risk scoring). Empirische Studie mit substantiellem Fokus auf diskriminatorische Auswirkungen von ML-Systemen in der Jugendhilfe."
35,4ZL5Q48E,Jarrahi et al. (2021),Algorithmic management in a work context,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Theoretisch,0.85,"Paper behandelt algorithimische Systeme (KI_Sonstige: Ja) und thematisiert Machtstrukturen sowie Ungleichheit am Arbeitsplatz (Bias_Ungleichheit: Ja). ABER: Kein direkter Bezug zu Sozialer Arbeit, deren Praxis, Theorie oder Zielgruppen. Der Fokus liegt auf allgemeinen Arbeitskontexten/Organisationen, nicht auf sozialarbeiterischen Settings oder Zielgruppen (Kinder, Jugendliche, vulnerable Gruppen in Unterstützungssystemen). Arbeitssoziologie ≠ Soziale Arbeit. Soziale_Arbeit bleibt Nein → SOZIAL-Bedingung nicht erfüllt → Exclude."
36,ZQHP5G35,Jørgensen (2023),Data and rights in the digital welfare state: the case of Denmark,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Theoretisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmische Entscheidungssysteme im Wohlfahrtssektor), SOZIAL_OK (Soziale_Arbeit: Wohlfahrtsadministration; Bias_Ungleichheit: Bürgerdisempowerment durch Überwachung; Fairness: Kritik an automatisierten Entscheidungen). Theoretische kritische Analyse von Surveillance Capitalism in der dänischen Wohlfahrtsverwaltung mit Fokus auf Bürgerrechte und Fairness-Aspekte."
37,A2P8MXMY,Keddell (2019),"Algorithmic justice in child protection: Statistical fairness, social justice and the implications for practice",Nein,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Include,,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: predictive algorithms in child protection), SOZIAL_OK (Soziale_Arbeit: child protection/welfare practice, Bias_Ungleichheit: racial und socioeconomic disparities, Diversitaet: marginalisierte Gruppen, Fairness: statistical fairness analysis). Kritische Analyse algorithmischer Systeme im Sozialbereich mit direktem Praxisbezug."
38,ZITLBM8A,Singer et al. (2023),ChatGPT for social work science: Ethical challenges and opportunities,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Include,,Konzept,0.95,"Paper erfüllt beide Inklusionskriterien: TECHNIK_OK (Generative_KI: ChatGPT/LLMs, AI_Literacies: kritische Evaluationskompetenz für Forscher), SOZIAL_OK (Soziale_Arbeit: direkter Bezug zu sozialer Arbeit, Bias_Ungleichheit: algorithmischer Bias). Invited paper mit Fokus auf ethische Reflexion und praktische Empfehlungen für KI-Nutzung in Sozialarbeitsforschung."
39,QI5AYE4V,McDonald et al. (2023),Algorithmic decision-making in social work practice and pedagogy: Confronting the competency/critique dilemma,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Include,,Konzept,0.95,Paper adressiert algorithmische Literacität (AI_Literacies) und algorithmic decision-making systems (KI_Sonstige) im Kontext der Sozialarbeiter:innen-Ausbildung (Soziale_Arbeit). Kritische Auseinandersetzung mit Machtverteilungen und strukturellen Ungleichheiten (Bias_Ungleichheit). Erfüllt beide Bedingungen: TECHNIK (AI_Literacies + KI_Sonstige) UND SOZIAL (Soziale_Arbeit + Bias_Ungleichheit).
40,WTLVG29I,Reamer (2023),Artificial intelligence in social work: Emerging ethical issues,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Include,,Theoretisch,0.95,"Paper adressiert KI-Kompetenzen für Sozialarbeiter:innen (AI_Literacies), algorithmische Systeme in der Sozialarbeit wie Risikobewertung (KI_Sonstige), direkte sozialarbeiterische Praxis und Ethik (Soziale_Arbeit), algorithmischen Bias und Fairness bei Algorithmen (Bias_Ungleichheit, Fairness) sowie diverse Stakeholder-Perspektiven durch Focus Groups (Diversitaet). Erfüllt beide Bedingungen: TECHNIK (AI_Literacies + KI_Sonstige) und SOZIAL (Soziale_Arbeit + Bias_Ungleichheit + Fairness + Diversitaet)."
41,J7V3AAQT,Sabour et al. (2023),A chatbot for mental health support: Exploring the impact of Emohaa on reducing mental distress in China,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,No_relevant_topic,Empirisch,0.92,"Paper behandelt Generative KI (Chatbot mit Sprachverarbeitung), erfüllt aber nicht die Sozial-Kriterien. Mental-Health-Anwendung für Universitätsstudenten ist kein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder Fairness. Psychologische Intervention ohne sozialarbeiterischen Kontext. Reine klinische Evaluierung einer KI-Anwendung."
42,9WIGR47Y,Rodríguez-Martínez et al. (2024),Ethical issues related to the use of technology in social work practice: A systematic review,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Include,,Literaturreview,0.92,"Paper behandelt ethische Herausforderungen von Technologie in der Sozialen Arbeit substantiell. KI_Sonstige erfüllt (automatisierte Systeme, digitale Technologien), Soziale_Arbeit erfüllt (direkte Praxisanalyse), Bias_Ungleichheit erfüllt (digital divide, soziale Gerechtigkeit, Selbstbestimmung). Systematischer Review mit klarem Sozialarbeitsbezug und Fokus auf algorithmische Systeme und deren Auswirkungen."
43,S8WGUVQT,Dencik et al. (2024),Automated government benefits and welfare surveillance,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Include,,Theoretisch,0.92,"Paper erfüllt beide Kriterien: TECHNIK_OK durch KI_Sonstige (algorithmische Systeme in öffentlicher Verwaltung: Fraud Detection, Chatbots). SOZIAL_OK durch Soziale_Arbeit (Wohlfahrtssysteme, marginalisierte Populationen) und Bias_Ungleichheit (Analyse von Machtsymmetrien, Surveillance von benachteiligten Gruppen). Kritische Betrachtung algorithmischer Kontrollsysteme im Sozialstaat."
44,LXDG4KQK,Amnesty International (2024),Coded injustice: Surveillance and discrimination in Denmark's automated welfare state,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Include,,Empirisch,0.95,"Report untersucht substantiell algorithmische Entscheidungssysteme (KI_Sonstige: Ja) im Wohlfahrtsbereich, direkt relevant für Soziale Arbeit (Ja). Zentral sind Bias/Diskriminierung gegen vulnerable Gruppen (Bias_Ungleichheit: Ja), Diversität marginalisierter Bevölkerungen (Diversitaet: Ja) und Fairness-Probleme automatisierter Systeme (Fairness: Ja). TECHNIK-Kriterium erfüllt (KI_Sonstige). SOZIAL-Kriterium erfüllt (vier Ja). Empirischer Forschungsbericht."
45,BTLTEA6Y,Meilvang & Dahler (2024),Decision support and algorithmic support: The construction of algorithms and professional discretion in social work,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK-Ja (KI_Sonstige: algorithmische Entscheidungssysteme in der Jugendhilfe), SOZIAL-Ja (Soziale_Arbeit: direkter Fokus auf Auswirkungen von Algorithmen auf sozialarbeiterische Praxis und Professionalität; Bias_Ungleichheit und Fairness: kritische Analyse von Machtstrukturen und Neutralitätsansprüchen). Kritische Analyse struktureller Effekte von Algorithmen auf professionelle Autonomie und Care-Beziehungen."
46,7D3ICY7Z,van Toorn et al. (2024),"Introduction to the digital welfare state: Contestations, considerations and entanglements",Nein,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Include,,Theoretisch,0.92,"Paper behandelt algorithmische Systeme im Sozialbereich (digital welfare, automated decision-making) substantiell → KI_Sonstige: Ja. Direkter Bezug zu Sozialarbeit und Welfare-Praxis → Soziale_Arbeit: Ja. Kritische Analyse von algorithmischer Diskriminierung marginalisierter Populationen → Bias_Ungleichheit: Ja. Fokus auf strukturelle Ungleichheiten und marginalisierte Gruppen → Diversitaet: Ja. Fairness-Dimension in automatisierten Entscheidungssystemen → Fairness: Ja. TECHNIK (KI_Sonstige) + SOZIAL (5 Kategorien) erfüllt → Include."
47,XQM6WRU2,Cher et al. (2024),Exploring machine learning to support decision-making for placement stabilization and preservation in child welfare,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Include,,Empirisch,0.92,"Paper entwickelt ML-Modelle (Random Forest, logistische Regression) für Vorhersage von Platzierungsabbruch in Jugendhilfe. Erfüllt TECHNIK-Kriterium durch KI_Sonstige (klassisches ML für algorithmische Entscheidungssysteme im Sozialbereich). Erfüllt SOZIAL-Kriterium durch: (1) direkter Bezug zu Sozialarbeitspraxis/Jugendhilfe (Soziale_Arbeit=Ja), (2) expliziter Fairness-Fokus (Fairness=Ja). Statewide child welfare study mit Fokus auf Unterstützung von Caseworker-Entscheidungen."
48,E4G328PD,Heinz et al. (2025),Clinical trial of an LLM-based conversational AI psychotherapy,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Empirisch,0.95,"Das Paper untersucht LLM-basierte Psychotherapie in klinischem Kontext. Während es Generative KI behandelt (RCT mit Chatbot), fehlt der Bezug zur Sozialen Arbeit vollständig. Die Anwendung ist klinische Psychologie/Psychiatrie, nicht Soziale Arbeit. Keine Thematisierung von Bias, Fairness, Gender, Diversität oder feministischen Perspektiven. TECHNIK erfüllt (Generative_KI=Ja), aber SOZIAL nicht erfüllt (alle Kategorien=Nein). Daher Exclude."
49,GFXYER4F,Takaoka (2022),AI implementation science for social issues: Pitfalls and tips,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Empirisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK: KI_Sonstige (Machine Learning, Gradient Boosting für Prediction) + AI_Literacies (Training von Fachkräften, eXplainable AI für Transparenz). SOZIAL: Soziale_Arbeit (Jugendhilfe/Child Guidance Centers, direkte sozialarbeiterische Praxis und Zielgruppen). Implementierungsstudie mit substantiellem Fokus auf Kompetenzentwicklung und praktische Anwendung algorithmischer Systeme im Sozialbereich."
50,3B87U5LN,Siddals et al. (2024),"""It happened to be the perfect thing"": Experiences of generative AI chatbots for mental health",Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Empirisch,0.92,"Paper untersucht Nutzung von generativen KI-Chatbots (Pi, ChatGPT) für mentale Gesundheitsunterstützung. Generative_KI=Ja (LLM-basierte Chatbots im Fokus). Soziale_Arbeit=Ja (mentale Gesundheit, therapeutische Kontexte, Unterstützung von vulnerable Personen - Kernbereich Sozialer Arbeit). Beide TECHNIK- und SOZIAL-Kriterien erfüllt → Include."
51,C325Y32P,World Economic Forum (2024),AI for impact: The PRISM framework for responsible AI in social innovation,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Konzept,0.85,"Paper behandelt KI-Kompetenzen und -Integration (AI_Literacies) sowie allgemeine KI-Governance (KI_Sonstige) im Kontext sozialer Innovationen und Dienstleistungen (Soziale_Arbeit). PRISM-Framework adressiert direkt Organisationen in sozialen Sektoren mit Readiness-Assessments. TECHNIK_OK (AI_Literacies + KI_Sonstige) und SOZIAL_OK (Soziale_Arbeit) erfüllt. Framework-Dokument ohne explizite Bias-, Gender-, oder Fairness-Fokussierung."
52,7QQV7R9S,British Association of Social Workers (2025),Generative AI & social work practice guidance,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Include,,Konzept,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + AI_Literacies durch Praxis-Guidance) und SOZIAL (Soziale_Arbeit + Bias_Ungleichheit durch Warnung vor racist/sexist assumptions). Es ist eine direkte Praxis-Orientierung für Sozialarbeiter:innen mit ethischen Reflexionspunkten und adressiert substantiell generative KI-Risks (Bias, Halluzinationen, Datenschutz)."
53,PI5H2LZ2,Moreau et al. (2024),"Failing our youngest: On the biases, pitfalls, and risks in a decision support algorithm used for child protection",Nein,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Include,,Empirisch,0.95,"Paper erfüllt beide Einschlusskriterien: (1) TECHNIK: KI_Sonstige=Ja (algorithmisches Entscheidungssystem in der Jugendhilfe). (2) SOZIAL: Soziale_Arbeit=Ja (direkter Bezug zu Child Protection/Jugendhilfe), Bias_Ungleichheit=Ja (Analyse von Diskriminierung gegenüber Immigrantenfamilien), Diversitaet=Ja (Repräsentationsfragen marginalisierter Communities), Fairness=Ja (Fehlerpräzision und Fairness-Metriken). Substantielle empirische Analyse mit dokumentierten Systemharms."
54,JC7X3MM7,Kawakami et al. (2022),"Improving human-AI partnerships in child welfare: Understanding worker practices, challenges, and desires for algorithmic decision support",Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Empirisch,0.95,"Paper adressiert Allegheny Family Screening Tool (algorithmisches Entscheidungssystem in der Jugendhilfe). TECHNIK erfüllt: KI_Sonstige (algorithmische Systeme), AI_Literacies (worker training, tool understanding). SOZIAL erfüllt: Soziale_Arbeit (child welfare practice), Bias_Ungleichheit (discrimination risks), Fairness (misalignments, limitations). Empirische Studie mit direktem Sozialarbeitsbezug und kritischer Analyse von Systemrisiken."
55,TNLGELEQ,Näscher et al. (2025),ReflectAI: Design and evaluation of an AI coach to support public servants' self-reflection,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Experimentell,0.85,"Paper erfüllt TECHNIK- und SOZIAL-Kriterien: (1) Generative_KI (LLM-basierter AI Coach), Prompting (strukturiertes Prompting für reflektive Praxis), AI_Literacies (Entwicklung von Self-Reflection-Kompetenzen). (2) Soziale_Arbeit (öffentliche Verwaltung, human services context, Kompetenzentwicklung). Empirische User Study mit klarem Fokus auf KI-gestützte Professionalisierung im sozialen Sektor."
56,8MDXCTA6,Hall et al. (2024),"A systematic review of sophisticated predictive and prescriptive analytics in child welfare: Accuracy, equity, and bias",Nein,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Include,,Literaturreview,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK durch KI_Sonstige (predictive/prescriptive algorithms); SOZIAL_OK durch Soziale_Arbeit (child welfare, social workers), Bias_Ungleichheit (discrimination against low-income families and communities of color), Diversitaet (marginalisierte Communities), Fairness (algorithmic fairness). Systematische Literaturreview mit substantiellem Fokus auf algorithmische Systeme im Sozialbereich."
57,4GYXUS9Y,Goldkind et al. (2024),The end of the world as we know it? ChatGPT and social work,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Include,,Theoretisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK-OK (Generative_KI: ChatGPT; Prompting: explizit 'thoughtful prompting practices'; AI_Literacies: kritische Reflexion und kompetenter Umgang), SOZIAL-OK (Soziale_Arbeit: direkter Fokus auf Profession; Fairness: 'just technology use', 'fair use'). Editorial mit kritischer Perspektive auf KI-Integration in Sozialarbeit."
58,CLKAD87H,James et al. (2025),Responsible prompting recommendation: Fostering responsible AI practices in prompting-time,Ja,Ja,Ja,Nein,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Empirisch,0.87,"Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (Kompetenzentwicklung für Prompt Engineers), Generative_KI (LLM-Fokus), Prompting (Kernthema: Prompt Recommendations). SOZIAL: Soziale_Arbeit (explizite Relevanz für social services), Bias_Ungleichheit (harmful content removal), Fairness (Responsible AI framework). Empirische Studie mit Interviews und User Studies. Substantieller Fokus auf Prompting-Strategien und Responsible AI-Praktiken."
59,CY5IMM6G,Ahn et al. (2025),Artificial Intelligence (AI) literacy for social work: Implications for core competencies,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies + KI_Sonstige für algorithmische Systeme); SOZIAL_OK (Soziale_Arbeit + Bias_Ungleichheit + Fairness). Zentral ist die Integration von KI-Kompetenzen in CSWE-Kernkompetenzen für Sozialarbeiter:innen mit explizitem Fokus auf algorithmischen Bias, Ungleichheit und ethische Governance in der Praxis."
60,UENFDPH9,Garkisch et al. (2024),Considering a unified model of artificial intelligence enhanced social work: A systematic review,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Literaturreview,0.95,Systematic Review mit substanziellem Fokus auf AI Literacy und Computational Thinking in der Sozialen Arbeit (AI_Literacies: Ja). Direkter Bezug zu sozialarbeiterischer Praxis und Profession via Staub-Bernasconi's Triple Mandate (Soziale_Arbeit: Ja). Thematisiert auch algorithmische Systeme/KI-Systeme generell (KI_Sonstige: Ja). TECHNIK und SOZIAL beide erfüllt → Include.
61,AK7K4P97,Baker et al. (2025),Artificial intelligence in social work: An EPIC model for practice,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Include,,Literaturreview,0.95,"Paper erfüllt beide Bedingungen: TECHNIK-OK durch AI_Literacies (community-based AI digital literacy), KI_Sonstige (AI in social work), SOZIAL-OK durch Soziale_Arbeit (direkter Fokus auf sozialarbeiterische Praxis), Bias_Ungleichheit (ethische Gerechtigkeit), Diversitaet (vulnerable populations, technology access) und Fairness (ethics and justice component). EPIC-Modell adressiert systematisch KI-Integration in Sozialarbeit mit Schwerpunkt auf ethische und gerechte Outcomes."
62,6Y5EPQRR,Kutscher (2023),"Positionings, challenges, and ambivalences in children's and parents' perspectives in digitalized familial contexts",Nein,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Theoretisch,0.85,"Das Paper behandelt Digitalisierung in Familien und Sharenting, nicht aber KI-Systeme. Es adressiert zwar sozialarbeiterische Praxisbereiche (Familie, Kindeswohl), erfüllt aber die TECHNIK-Bedingung nicht: Weder AI_Literacies (kein fokussierter KI-Kompetenzrahmen), noch Generative_KI, Prompting oder KI_Sonstige sind substantiell behandelt. Digitale Technologie allgemein ≠ KI. Daher: Exclude."
63,QKFG72IT,Fujii et al. (2024),Bildungsteilhabe - Flucht - Digitalisierung: Eine multilokale Ethnografie im (digitalen) Alltag junger Geflüchteter,Nein,Nein,Nein,Nein,Ja,Ja,Nein,Ja,Nein,Nein,Exclude,Not_relevant_topic,Empirisch,0.85,"Paper behandelt digitale Medien, Bildungszugang und junge Geflüchtete. Erfüllt SOZIAL-Kriterien (Soziale_Arbeit, Bias_Ungleichheit, Diversität). ABER: Kein Fokus auf KI/ML-Systeme. Digitalisierung ≠ KI. Keine Kategorie aus TECHNIK-Bereich (AI_Literacies, Generative_KI, Prompting, KI_Sonstige) ist substantiell erfüllt. Anthropologisch-ethnografisches Paper zu Digitalisierung ohne KI-Bezug."
64,BKSU66QB,Kutscher (2024),Digitalität und Digitalisierung als Gegenstand der Sozialen Arbeit,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Include,,Theoretisch,0.92,"Paper erfüllt beide Inklusionskriterien: (1) TECHNIK-Kriterium: KI_Sonstige=Ja (Algorithmen, datengetriebene Systeme, digitale Plattformen sind zentral). (2) SOZIAL-Kriterium: Soziale_Arbeit=Ja (direkter Fokus auf sozialarbeiterische Praxis und deren Transformation) UND Bias_Ungleichheit=Ja (Analyse von Machtstrukturen, Überwachung und sozialen Ungleichheiten). Kutschers Fokus auf Digitalisierung als Machtfrage und soziale Gerechtigkeit ist substantiell für beide Dimensionen."
65,M2FYV58I,Schneider et al. (2022),Exploring opportunities and risks in decision support technologies for social workers: An empirical study in the field of disabled people's services,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Empirisch,0.95,Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Entscheidungsunterstützungssysteme/algorithmische Systeme) und SOZIAL_OK (Soziale_Arbeit: expliziter Fokus auf Sozialarbeiter und Behindertenservices; Bias_Ungleichheit: Reduktion subjektiver Vorurteile; Fairness: Transparenz in Ressourcenallokation). Empirische Untersuchung mit direktem Bezug zu sozialarbeiterischer Praxis und algorithmischen Systemen im Sozialbereich.
66,LMW8DZ78,Schneider et al. (2025),"Indecision on the use of artificial intelligence in healthcare: A qualitative study of patient perspectives on trust, responsibility and self-determination using AI-CDSS",Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Exclude,Wrong_publication_type / Not_relevant_topic,Empirisch,0.85,"Paper behandelt AI-CDSS im Healthcare-Kontext (KI_Sonstige: Ja), adressiert algorithmische Fairness und Bias-Aspekte (Bias_Ungleichheit, Fairness: Ja). JEDOCH: Kein direkter Bezug zu Sozialer Arbeit. Healthcare-Setting mit Patient:innen-Perspektiven ist nicht Sozialarbeit (keine Zielgruppen-Überschneidung, keine sozialarbeiterische Praxis/Theorie). Paper erfüllt Technik-Bedingung, aber nicht die erforderliche Sozial-Bedingung für die Forschungsfrage des Literature Review zur KI in Sozialer Arbeit."
67,2WHGF83D,Schneider & Weber (2024),"AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?",Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Literaturreview,0.85,"Paper behandelt algorithmische Entscheidungssysteme (KI_Sonstige: Ja) in sensiblen Bereichen (Healthcare, Justiz, Grenzschutz). Substantielle Behandlung von Diskriminierungsrisiken in Trainingsdaten (Bias_Ungleichheit: Ja) und Fairness-Aspekten durch Analyse von Opazität und Accountability (Fairness: Ja). Erfüllt beide Bedingungen (TECHNIK + SOZIAL). Kein Soziale_Arbeit-Bezug, daher nicht umfassend in diesem Kontext, aber relevant für AI-Governance und gesellschaftliche Auswirkungen."
68,44AA6ETH,Schneider (2024),Das verflixte Problem mit Klassifikationen: Zum Einfluss der Digitalisierung auf die Soziale Diagnostik in der Sozialen Arbeit,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Include,,Theoretisch,0.85,Paper behandelt Algorithmische Entscheidungssysteme (KI_Sonstige: Ja) im Kontext Sozialer Arbeit (Soziale_Arbeit: Ja). Expliziter Fokus auf Klassifikationssysteme und deren Limitations bezüglich Bias und Kontextverlust (Bias_Ungleichheit: Ja). TECHNIK und SOZIAL Bedingungen erfüllt → Include.
69,QG8IAB53,Jarke & Büchner (2024),Who cares about data? Data care arrangements in everyday organisational practice,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Empirisch,0.85,"Das Paper behandelt Datenmanagement und -pflege in Organisationen (Sozialarbeit, Bildung), nicht aber KI/ML-Systeme. Es fehlt jede technische KI-Komponente (keine AI_Literacies, Generative_KI, Prompting oder KI_Sonstige). Das Thema ist zu weit entfernt von KI-Anwendungen. Nur eine SOZIAL-Kategorie erfüllt, TECHNIK-Bedingung nicht erfüllt → Exclude."
70,49PPZJ7Z,Zakharova et al. (2024),Tensions in digital welfare states: Three perspectives on care and control,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Include,,Theoretisch,0.85,"Paper adressiert algorithmische Governance und digitale Systeme in Wohlfahrtsstaat (KI_Sonstige: Ja). Direkter Bezug zu Sozialarbeit durch Analyse von Welfare Services und deren Digitalisierung (Soziale_Arbeit: Ja). Thematisiert Spannungen zwischen Care und Control, was Fragen zu Überwachung und algorithmischer Kontrolle aufwirft (Bias_Ungleichheit: Ja, da Machtasymmetrien und potenzielle Diskriminierung in digitalen Wohlfahrtssystemen adressiert werden). Beide Bedingungen erfüllt: TECHNIK_OK + SOZIAL_OK → Include."
71,NQEUZQF9,Jarke & Manchester (2025),Datafied ageing futures: Regimes of anticipation and participatory futuring,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,,Konzept,0.82,"Paper behandelt Datengetriebene Systeme (KI_Sonstige) und deren Auswirkungen auf vulnerable Bevölkerungsgruppen (ältere Erwachsene). Adressiert Bias durch ageist assumptions (Bias_Ungleichheit), partizipative Inklusion marginalisierter Gruppen (Diversitaet). Verwendet implizit-feministische Perspektive durch Kritik von Machtstrukturen und Dominanz mächtiger Akteure (Feministisch). Fokus auf demokratisierte Futures-Making und alternative Narrativen reflektiert Fairness-Perspektive. TECHNIK_OK (KI_Sonstige) + SOZIAL_OK (Bias, Diversitaet, Feministisch, Fairness) → Include."
72,L6PH7GDL,Linnemann et al. (2025),Künstliche Intelligenz in der Sozialen Arbeit: Grundlagen für Theorie und Praxis,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (KI-Kompetenzen in Bildung/Organisationen), KI_Sonstige (technische Grundlagen, Automatisierungsbias). SOZIAL: Soziale_Arbeit (Direktbezug zu Praxis, Theorie und Anwendungsfeldern), Bias_Ungleichheit (Diskriminierung in Trainingsdaten), Fairness (ethische Rahmenbedingungen). Systematische Behandlung von KI in der Sozialen Arbeit mit explizitem Fokus auf verantwortungsvolle Anwendung."
73,QLXLEUCG,Schneider & Seelmeyer (2018),Der Einfluss der Algorithmen: Neue Qualitäten durch Big Data Analytics und Künstliche Intelligenz,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Include,,Theoretisch,0.92,"Paper behandelt substantiell AI_Literacies (kritische Reflexion über KI-Kompetenzen von Fachkräften, Automation Bias), KI_Sonstige (Algorithmen, Big Data Analytics), Soziale_Arbeit (direkter Bezug zur sozialen Arbeit und professionellen Entscheidungsfindung) und Bias_Ungleichheit (Automation Bias). Beide Voraussetzungen erfüllt: TECHNIK_OK (KI_Sonstige + AI_Literacies) und SOZIAL_OK (Soziale_Arbeit + Bias_Ungleichheit)."
74,4JN7NIS4,Steiner & Tschopp (2022),"Künstliche Intelligenz in der Sozialen Arbeit: Grundlagen, Entwicklungen, Herausforderungen",Nein,Ja,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Include,,Literaturreview,0.92,"Das Paper behandelt KI-Anwendungen in der Sozialen Arbeit (Chatbots, Predictive Risk Modeling) und analysiert algorithmischen Bias sowie ethische Fragen. Es erfüllt beide Anforderungen: TECHNIK (Generative_KI für Chatbots, KI_Sonstige für PRM und neuronale Netze) und SOZIAL (Soziale_Arbeit als Fokus, Bias_Ungleichheit als substantielles Thema). Keine feministischen Theorien explizit erwähnt, daher Feministisch=Nein."
75,GZ9B3GPD,Waag (2023),Rationalisierung durch Digitalisierung?,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Theoretisch,0.95,"Das Paper behandelt Digitalisierung in der Sozialen Arbeit (erfüllt Soziale_Arbeit = Ja), thematisiert aber keine KI-Systeme, ML-Algorithmen oder andere technische KI-Aspekte (alle Technik-Kategorien = Nein). Der Fokus liegt auf Rationalisierungslogik und Interaktionssoziologie. Ohne TECHNIK-Komponente ist das Paper nicht relevant für ein KI-Literature Review, auch wenn es sozialarbeiterische Theorie berührt."
76,MA3LBJS6,Linnemann et al. (2023),Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit: Eine exemplarische arbeitsfeldübergreifende Betrachtung des Natural Language Processing (NLP),Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Theoretisch,0.92,"Paper erfüllt beide Kriterien: (1) TECHNIK_OK: Behandelt AI_Literacies (kritische Reflexion von NLP-Technologien in sozialer Arbeit) und KI_Sonstige (NLP als spezifisches ML-System). (2) SOZIAL_OK: Direkter Fokus auf Soziale_Arbeit (arbeitsfeldübergreifende Analyse, Auswirkungen auf sozialarbeiterische Praxis). Kritische Auseinandersetzung mit Chancen/Risiken der KI-Implementierung im Kontext authentischer Sozialer Arbeit."
77,2FUXNFZS,[Author not specified] (2024),"RETHINKING SOCIAL SERVICES WITH ARTIFICIAL INTELLIGENCE: OPPORTUNITIES, RISKS, AND FUTURE PERSPECTIVES",Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Theoretisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK durch AI_Literacies (Vorbereitung von Social Workers auf KI-Nutzung), KI_Sonstige (Predictive Modeling, Algorithmen in Social Services) und Fairness. SOZIAL_OK durch Soziale_Arbeit (direkter Bezug zu Social Work Practice), Bias_Ungleichheit (algorithmic bias als Risiko), und Fairness (Emphasis auf ethische, faire KI-Integration). Paper ist substantiell und adressiert zentrale Schnittstelle KI-Soziale Arbeit."
78,HSQW48VE,[Author not specified] (2025),Artificial Intelligence in Social Work: An EPIC Model for Practice,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Literaturreview,0.92,"Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (Kompetenzen/Verständnis von KI-Risiken/-Nutzen in der Praxis), KI_Sonstige (allgemeine KI im Kontext). SOZIAL: Soziale_Arbeit (direkter Bezug zu Profession, Praxis, Werten), Bias_Ungleichheit (sozial gerechte Outcomes, ethische Praxis), Fairness (EPIC-Modell mit Ethik und Gerechtigkeit). Literaturreview mit explizitem Fokus auf Schnittstelle KI-Sozialarbeit und deren Auswirkungen auf professionelle Werte."
79,SS5HTYY6,[Author not specified] (2025),"Artificial Intelligence in Social Sciences and Social Work: Bridging Technology and Humanity to Revolutionize Research, Policy, and Human Services",Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Literaturreview,0.92,"Das Paper ist ein Literaturreview, das KI-Technologien (Machine Learning, NLP) substantiell im Kontext Sozialer Arbeit und Human Services behandelt. Es adressiert Bias, Fairness und Ethical Concerns—zentrale soziale Aspekte. TECHNIK erfüllt (KI_Sonstige + AI_Literacies: Diskurs über KI-Integration). SOZIAL erfüllt (Soziale_Arbeit + Bias_Ungleichheit + Fairness). Keine Gender/Feminismus-Komponente evident."
80,H59BNSX8,James et al. (2023),Algorithmic decision-making in social work practice and pedagogy: confronting the competency/critique dilemma,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Theoretisch,0.85,"Das Paper adressiert algorithmic decision-making in Sozialarbeit (KI_Sonstige + Soziale_Arbeit). Der Titel hebt die Competency/Critique-Dilemma hervor, was auf AI Literacies verweist. Bias, Fairness und ethische Bedenken in Algorithmen sind zentral (Bias_Ungleichheit + Fairness). TECHNIK erfüllt (3x Ja), SOZIAL erfüllt (4x Ja). Inklusion gerechtfertigt."
81,RAV6DAFQ,Goldkind et al. (2023),The End of the World as We Know It? ChatGPT and Social Work,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Include,,Konzept,0.85,Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI: ChatGPT; AI_Literacies: proaktive Auseinandersetzung mit KI-Kompetenzen für Profession) + SOZIAL (Soziale_Arbeit: direkter Bezug zur Profession; Bias_Ungleichheit: Warnung vor inequitable outcomes). Kommentar-Format mit konzeptionellem Fokus auf Wertausrichtung und Gerechtigkeit.
82,YRBP6IEJ,Rodriguez et al. (2024),Introducing Generative Artificial Intelligence into the MSW Curriculum: A Proposal for the 2029 Educational Policy and Accreditation Standards,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Konzept,0.95,"Paper behandelt substantiell KI-Kompetenzen (AI_Literacies) und generative KI im MSW-Curriculum. Direkter Bezug zu Sozialer Arbeit (Ausbildung, Akkreditierungsstandards). Adressiert Bias, Transparenz und Fairness als zentrale Themen für verantwortungsvolle AI-Nutzung im Kontext von Klientenschutz und Equity. Erfüllt beide Bedingungen: TECHNIK (AI_Literacies + Generative_KI) und SOZIAL (Soziale_Arbeit + Bias_Ungleichheit + Fairness)."
83,JI24FQPV,Creswell Báez et al. (2025),Clinical Social Workers’ Perceptions of Large Language Models in Practice: Resistance to Automation and Prospects for Integration,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Empirisch,0.95,"Paper erfüllt TECHNIK- und SOZIAL-Kriterien: (1) TECHNIK: Generative_KI (LLMs in klinischer Praxis) und AI_Literacies (Training, Kompetenzentwicklung im Umgang mit LLMs) sind substantiell. (2) SOZIAL: Direkter Bezug zu Sozialarbeiter:innen (clinical social workers) und ihrer Praxis. Qualitative Studie mit praktischen Implikationen für sozialarbeiterische Integration von LLMs. Betont Augmentation statt Automation, was auf Kompetenzentwicklung abzielt."
84,LGYFN6JK,Patton et al. (2023),ChatGPT for Social Work Science: Ethical Challenges and Opportunities,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Konzept,0.92,Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI: LLM-Fokus; AI_Literacies: kritischer Umgang mit KI-Systemen) und SOZIAL_OK (Soziale_Arbeit: Ethik für sozialarbeiterische Forschung; Bias_Ungleichheit & Fairness: explizit Bias-Bekämpfung und Inklusion/Social Justice). Substantieller Bezug zu ethischen Herausforderungen und KI-Kompetenzen in der Sozialarbeit.
85,X2QQ4JH6,Reamer (2023),Artificial Intelligence in Social Work: Emerging Ethical Issues,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK durch AI_Literacies (ethische Kompetenzen, professionelle Ausbildung), KI_Sonstige (algorithmische Systeme in Sozialbereich) und Fairness (Bias, Transparenz). SOZIAL_OK durch Soziale_Arbeit (direkte Anwendung in SW-Praxis), Bias_Ungleichheit (algorithmic bias) und Fairness. Umfassende ethische Analyse mit Bezug zu NASW-Kodex und SW-Zielgruppen."
86,GXZWBNJU,Boetto (2025),Artificial Intelligence in Social Work: An EPIC Model for Practice,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Ja,Ja,Include,,Literaturreview,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige) + SOZIAL_OK (Soziale_Arbeit, Bias_Ungleichheit, Diversitaet, Fairness, Feministisch). EPIC-Modell adressiert KI-Kompetenzen und strukturierte AI-Integration in Sozialarbeit. Expliziter Fokus auf Bias-Risiken, soziale Gerechtigkeit, Intersektionalität und ethische Fairness. Intersektionalität deutet auf feministische Perspektive hin."
87,ZJS2J7E7,Ahn et al. (2025),Artificial Intelligence (AI) Literacy for Social Work: Implications for Core Competencies,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Konzept,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (Kernthema: AI Literacy als Kompetenz in Curriculum), KI_Sonstige (algorithmische Systeme und deren Auswirkungen). SOZIAL: Soziale_Arbeit (direkter Bezug zu SW-Kompetenzentwicklung und Schutz vulnerabler Populationen), Bias_Ungleichheit (kritische Analyse von Inequities durch AI), Fairness (ethische Safeguards und bias detection). Konzeptionelles Paper mit substanziellem Fokus auf Integration von AI-Kritik in SW-Ausbildung und Praxis."
88,Y6M97SWQ,Yu & Rose (2025),Algorithmic-Assisted Decision-Making Tools in Child Welfare Practice: A Systematic Review,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Literaturreview,0.95,Paper adressiert algorithmic decision-making tools (KI_Sonstige) in child welfare (Soziale_Arbeit). Zentrale Themen sind Bias und Fairness. Nennt Practitioner Training (AI_Literacies). Systematischer Review mit PRISMA-Methode. Erfüllt beide Bedingungen: TECHNIK (KI_Sonstige + AI_Literacies) UND SOZIAL (Soziale_Arbeit + Bias_Ungleichheit + Fairness). Direkt relevant für KI in Sozialer Arbeit.
89,VICS443I,Singer et al. (2023),AI Creates the Message: Integrating AI Language Learning Models into Social Work Education and Practice,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Include,,Konzept,0.95,"Paper erfüllt beide Bedingungen: (1) TECHNIK: Generative_KI (LLMs in Social Work) + AI_Literacies (Pädagogische Integration, Kompetenzentwicklung). (2) SOZIAL: Soziale_Arbeit (direkter Bezug zu Lehre und Praxis Sozialer Arbeit) + Bias_Ungleichheit (Warnung vor Bias, faktischen Fehlern). Substantielle Behandlung aller Kategorien, kein Gender/Feministisch-Fokus erforderlich."
90,R5QQTD95,Schönauer (2025),Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen Arbeit – Eine empirische Untersuchung aus der Perspektive von Berufseinsteiger:innen,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Empirisch,0.92,Paper erfüllt beide Bedingungen: TECHNIK_OK durch AI_Literacies (Fokus auf digitale Kompetenz und kritische KI-Literacy in Aus- und Weiterbildung) und KI_Sonstige (Organisationale KI-Rahmenbedingungen). SOZIAL_OK durch Soziale_Arbeit (direkter Bezug zu Berufseinsteiger:innen und Klientenarbeit in der Sozialen Arbeit). Empirische Untersuchung mit substantiellem Bildungs- und Kompetenzbezug.
91,JNLPSHD5,Linnemann et al. (2023),Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit: Eine exemplarische arbeitsfeldübergreifende Betrachtung des Natural Language Processing (NLP),Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Theoretisch,0.92,"Paper behandelt NLP als KI-Technologie (KI_Sonstige: Ja) und adressiert direkt sozialarbeiterische Praxis, Theorie und professionsethische Fragen (Soziale_Arbeit: Ja). Kritische Auseinandersetzung mit KI-Chancen/-Risiken impliziert auch AI Literacy-Anforderungen (AI_Literacies: Ja). TECHNIK-Kriterium erfüllt (3x Ja). SOZIAL-Kriterium erfüllt (Soziale_Arbeit: Ja). Inklusionskriterien erfüllt."
92,H7E3N6VR,Sūna et al. (2024),Diskriminierung durch Algorithmen – Überlegungen zur Stärkung KI-bezogener Kompetenzen,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Konzept,0.92,"Das Paper behandelt algorithmische Diskriminierung (KI_Sonstige: Ja) und kritische KI-Kompetenzen (AI_Literacies: Ja). Es adressiert substantiell Bias, Ungleichheit (Bias_Ungleichheit: Ja), digitale Teilhabe/Inklusion (Diversitaet: Ja) und implizit Fairness durch Fokus auf Benachteiligungen. TECHNIK erfüllt (AI_Literacies + KI_Sonstige). SOZIAL erfüllt (Bias_Ungleichheit + Diversitaet + Fairness). → Include"
93,MITLDF9S,Feist-Ortmanns et al. (2025),KI-basiertes Assistenzsystem im Kinderschutzverfahren,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Empirisch,0.92,Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmisches Entscheidungssystem in Kinderschutz; AI_Literacies: Schulungsbedarf erwähnt) + SOZIAL_OK (Soziale_Arbeit: Gefährdungseinschätzung im Kinderschutz; Bias_Ungleichheit: explizite Bias-Risiken; Fairness: Human-in-the-Loop und Fehlklassifikationen). Direkt relevant für Praxis Sozialer Arbeit.
94,D4FY2S3R,Gravelmann (2024),Künstliche Intelligenz in der Sozialen Arbeit – Zwischen Bedenken und Optionen,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Literaturreview,0.85,"Das Paper behandelt KI in der Sozialen Arbeit und adressiert dabei sowohl Kompetenzentwicklung (AI Literacies: reflektierte Implementationsstrategien, professioneller Diskurs) als auch allgemeine KI/algorithmische Systeme (KI_Sonstige) in sozialarbeiterischen Praxisfeldern (Soziale_Arbeit: direkter Bezug zur Praxis). Beide Bedingungen (TECHNIK + SOZIAL) sind erfüllt → Include. Bias/Fairness nicht substantiell behandelt basierend auf Abstract."
95,DGLWM93D,Engelhardt & Ley (2025),Voll (dia)logisch? Ein Werkstattbericht über den Einsatz von generativer KI in der Hochschulbildung für Soziale Arbeit – Curriculare Überlegungen und veränderte Akteurskonstellationen,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, Generative_KI, Prompting sind substantiell) + SOZIAL_OK (Soziale_Arbeit durch expliziten Hochschulbezug zur Soziale-Arbeit-Ausbildung). Prompting als metakognitive Schlüsselkompetenz ist Kernthema. Curriculare Integration in Soziale Arbeit direkt adressiert. Konzept/Werkstattbericht, kein empirisch-experimenteller Nachweis."
96,EB7PZUZZ,Freinhofer et al. (2025),Prompten nach Plan: Das PCRR-Framework als pädagogisches Werkzeug für den Einsatz von Künstlicher Intelligenz.,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,"Nur TECHNIK erfüllt (AI_Literacies, Generative_KI, Prompting), SOZIAL-Kriterien nicht erfüllt. Paper behandelt Prompt Engineering als Pädagogik-Tool im Schulkontext, adressiert aber keine der SOZIAL-Kategorien (Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness).",Konzept,0.95,"Das Paper konzentriert sich auf AI Literacy und Prompting-Strategien (TECHNIK-Anforderung erfüllt), fehlt aber vollständig die SOZIAL-Komponente. Es diskutiert weder Bias, Ungleichheit, Gender-Perspektiven, Fairness noch direkte sozialarbeiterische Anwendungen. Rein schulpädagogisches Framework ohne kritische soziale Dimension."
97,7FBB2KUC,Linnemann et al. (2023),Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Theoretisch,0.85,"Paper adressiert KI_Sonstige durch Natural Language Processing (NLP), ein klassisches ML-Thema, und hat starken direkten Bezug zu Sozialer Arbeit (Staub-Bernasconi, Klient*innen, sozialarbeiterische Praxis). Beide Bedingungen erfüllt (TECHNIK + SOZIAL). Theoretisch-diskursiver Charakter ohne empirische Daten."
98,6N2E242K,Schönauer (2025),Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen Arbeit,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Empirisch,0.95,"Paper erfüllt TECHNIK-Bedingung: AI_Literacies (Ja - untersucht digitale Kompetenzen und KI-Akzeptanz von Fachkräften) und KI_Sonstige (Ja - behandelt KI-Einsatz in Organisationen). Erfüllt SOZIAL-Bedingung: Soziale_Arbeit (Ja - direkter Fokus auf KI-Akzeptanz und Praxis in der Sozialen Arbeit, Bedenken zu Datenschutz und ethischen Herausforderungen). Empirische Studie mit Berufseinsteiger:innen. Inklusion berechtigt."
99,HT9ZSI9U,Kutscher et al. (2020),Handbuch Soziale Arbeit und Digitalisierung,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Literaturreview,0.95,"Das Handbuch behandelt Digitalisierung in der Sozialen Arbeit substantiell (Soziale_Arbeit = Ja). Allerdings fehlt der erforderliche TECHNIK-Aspekt vollständig: Es gibt keinen Bezug zu KI-Systemen, Machine Learning, generativer KI oder KI-Kompetenzen. Das Werk konzentriert sich auf breite Digitalisierungsthemen (Social Media, E-Government, Datenschutz, Mediatisierung), nicht auf KI/ML-spezifische Technologien. Die Inklusionskriterien erfordern SOWOHL einen Technik-Aspekt (AI_Literacies, Generative_KI, Prompting oder KI_Sonstige) ALS AUCH einen Sozial-Aspekt. Hier ist nur Soziale_Arbeit erfüllt."
100,79TL6HSB,Chen & Lin (2025),Social work and artificial intelligence: Collaboration and challenges,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Empirisch,0.95,"Das Paper behandelt substantiell AI Literacy (Gaps in technical literacy, educational training), KI allgemein (Automation Bias, explainable AI), Soziale Arbeit (AI applications in social work, professional autonomy), Bias (automation bias), und Fairness (ethical governance, transparency). Beide Bedingungen sind erfüllt: TECHNIK_OK (AI_Literacies + KI_Sonstige), SOZIAL_OK (Soziale_Arbeit + Bias_Ungleichheit + Fairness)."
101,PAZJHB8J,Nuwasiima et al. (2024),The role of artificial intelligence (AI) and machine learning in social work practice,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Nein,Ja,Include,,Literaturreview,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Predictive Analytics, algorithmische Systeme; AI_Literacies: Involvement von Social Workers in AI-Entwicklung) und SOZIAL_OK (Soziale_Arbeit: direkter Bezug zu SW-Praxis und Kinderschutz; Bias_Ungleichheit: algorithmischer Bias gegen Familien of Color; Gender: explizite Erwähnung von Gender-Disparitäten; Diversitaet: Fokus auf diverse Datasets und Community-Involvement; Fairness: Algorithmic fairness und Auditing). Substantieller Review zu AI in Social Work mit kritischer Perspektive auf strukturelle Ungleichheit."
102,YTDMY5W4,Hauck et al. (2025),A framework for the learning and teaching of Critical AI Literacy skills (Version 0.1),Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Konzept,0.92,"Paper entwickelt ein Framework für Critical AI Literacy (TECHNIK: AI_Literacies, Generative_KI, Prompting). Substantieller Fokus auf Bias, Ungleichheit, Diversität und Fairness (SOZIAL) durch explizite Thematisierung von Epistemic Injustices, Power Relationships, Inequalities und Equity/Inclusion-Prinzipien. Beide TECHNIK- und SOZIAL-Bedingungen erfüllt. Nicht feministisch, da keine explizite feminist theory verwendet."
103,PYN6HB3E,Alam (2025),"Social work in the age of artificial intelligence: A rights-based framework for evidence-based practice through social psychology, group dynamics, and institutional analysis",Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Include,,Konzept,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige) durch Framework für AI-Integration in Sozialarbeit; SOZIAL_OK (Soziale_Arbeit, Bias_Ungleichheit, Diversitaet, Fairness) durch expliziten Fokus auf vulnerable Populationen, Gerechtigkeit, Menschenrechte und ethische Implikationen. Substantielle Behandlung von KI-Kompetenzen für SozialarbeiterInnen und algorithmischen Systemauswirkungen."
104,THVKEETD,Ahn (2025),Artificial Intelligence (AI) literacy for social work,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Include,,Konzept,0.95,"Das Paper behandelt AI Literacy substantiell als Kernkompetenz für Sozialarbeiter (AI_Literacies: Ja). Es hat direkten Bezug zu Soziale_Arbeit (Ja) durch explizite Fokussierung auf sozialarbeiterische Praxis und Professionalisierung. Bias und ethische Implikationen von KI werden adressiert (Bias_Ungleichheit: Ja, KI_Sonstige: Ja). TECHNIK- und SOZIAL-Kriterien sind erfüllt → Include."
105,CF6T2RD7,Reamer (2023),Artificial intelligence in social work: Emerging ethical issues,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Theoretisch,0.95,Das Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies: Fokus auf AI Literacy in Social Work Education; KI_Sonstige: algorithmische Systeme im Sozialbereich). SOZIAL (Soziale_Arbeit: direkter Bezug zu sozialarbeiterischer Praxis und Ausbildung; Bias_Ungleichheit: algorithmic bias; Fairness: unfairness in AI). Umfassende ethische Analyse von KI-Implementierung im sozialarbeiterischen Kontext mit konkretem Implementierungsprotokoll.
106,HZKE8T8I,Gravelmann (2024),Künstliche Intelligenz in der Sozialen Arbeit – Zwischen Bedenken und Optionen,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Include,,Theoretisch,0.92,"Das Paper erfüllt beide Bedingungen: (1) TECHNIK: KI_Sonstige=Ja (algorithmische Systeme in Sozialer Arbeit, insbesondere in Kindesschutz). (2) SOZIAL: Soziale_Arbeit=Ja (direkter Fokus auf Auswirkungen auf Profession) und Bias_Ungleichheit=Ja (kritische Analyse von Diskriminierung und Machtstrukturen in KI-Daten). Die Analyse von automatisierten Verfahren im Kindesschutz und die Thematisierung von Bias in KI-Systemen sind zentrale Inhalte."
107,LXKXPD5H,Studeny (2025),Digitale Werkzeuge und Machtasymmetrien?,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Theoretisch,0.92,Paper erfüllt beide Einschlusskriterien: TECHNIK_OK (KI_Sonstige: Algorithmen in digitaler Sozialer Arbeit; AI_Literacies: kritische Reflexion digitaler Technologien) und SOZIAL_OK (Soziale_Arbeit: direkter Bezug zu sozialarbeiterischer Praxis; Bias_Ungleichheit: Diskriminierung durch biased data; Fairness: Transparenz und faire Nutzung von Technologien). Substantielle theoretische Analyse von Machtasymmetrien und Verantwortungsfragen im digitalen Kontext.
108,9832ZJB7,Biegelbauer et al. (2023),"Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden für KI in der Verwaltung, Version 1.0",Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Exclude,"TECHNIK_OK (AI_Literacies=Ja, KI_Sonstige=Ja), aber SOZIAL_OK nicht erfüllt. Bias_Ungleichheit=Ja bezieht sich auf 'automation bias' als technisches Risiko, nicht auf soziale Ungleichheit oder Diskriminierung von Gruppen. Kein direkter Bezug zu Sozialer Arbeit, Gender, Diversität oder feministischer Perspektive. Allgemeiner Verwaltungs-/Ethik-Kontext ohne soziale Arbeit.",Konzept,0.85,"Der Leitfaden adressiert AI Literacy und KI-Risiken in der Verwaltung substantiell. Das Konzept von 'automation bias' ist technisch orientiert (KI-Versagen), nicht sozial (Diskriminierung von Gruppen). Fehlender Bezug zu Sozialer Arbeit, Gender, Diversität oder kritischen sozialen Perspektiven führt zur Exclusion."
109,DJEVSR8D,Ahrweiler (2025),AI FORA – Artificial Intelligence for Assessment: Fairness bei der Verteilung öffentlicher sozialer Leistungen,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Nein,Ja,Include,,Empirisch,0.92,"Paper adressiert algorithmische Entscheidungssysteme (KI_Sonstige: Ja) in der Sozialleistungsverwaltung (Soziale_Arbeit: Ja). Zentrale Themen sind Fairness bei der Ressourcenverteilung (Fairness: Ja), kulturelle/kontextuelle Ungleichheiten (Bias_Ungleichheit: Ja) und inklusive Partizipation von marginalisierten Gruppen (Diversitaet: Ja). Erfüllt TECHNIK- und SOZIAL-Kriterium."
110,QVNJIQJG,Strauß (2024),CAIL – Critical AI Literacy: Kritische Technikkompetenz für konstruktiven Umgang mit KI-basierter Technologie in Betrieben,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Konzept,0.85,"Das Paper behandelt Critical AI Literacy und Automation Bias (TECHNIK: AI_Literacies + KI_Sonstige erfüllt). Der Kontext ist jedoch betrieblich/wirtschaftlich orientiert, nicht sozialarbeiterisch. Bias_Ungleichheit ist nur marginal via 'automation bias' berührt, nicht substantiell. SOZIAL-Kriterien sind nicht erfüllt. Relevanz für Soziale-Arbeit-Review fraglich."
111,UHEM78MX,Long & Magerko (2020),What is AI Literacy? Competencies and Design Considerations,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Konzept,0.7,"Das Paper behandelt AI Literacy und Kompetenzenrahmen (erfüllt TECHNIK-Kriterium). Allerdings erfüllt es KEINE der SOZIAL-Kategorien: Es gibt keinen Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder Fairness. Es ist ein rein bildungstechnologisches Konzept-Paper ohne sozialarbeiterischen oder kritischen Ungleichheitsfokus. Beide Bedingungen (TECHNIK UND SOZIAL) müssen erfüllt sein."
112,JHRVDXSD,Ahn et al. (2025),Artificial Intelligence (AI) Literacy for Social Work: Implications for Core Competencies,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Konzept,0.85,"Das Paper behandelt explizit AI Literacy im Kontext von Social Work und deren Implikationen für Kernkompetenzen. Dies erfüllt beide Bedingungen: (1) TECHNIK: AI_Literacies = Ja (KI-Kompetenzen für Sozialarbeit), (2) SOZIAL: Soziale_Arbeit = Ja (direkter Bezug zu sozialarbeiterischer Praxis und Ausbildung). Include."
113,2MVSL3UV,Zhang et al. (2025),Learning About AI: A Systematic Review of Reviews on AI Literacy,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,No_relevant_topic,Literaturreview,0.95,"Das Paper behandelt AI Literacy substantiell (Kategorie AI_Literacies = Ja) und ist ein hochwertiger Umbrella Review. Allerdings erfüllt es nicht die SOZIAL-Bedingung: Es gibt keinen direkten Bezug zu Sozialer Arbeit, keine Analyse von Bias/Ungleichheit, kein Gender/Diversitäts-Fokus und keine Fairness-Diskussion. Der Fokus liegt auf allgemeiner KI-Bildung in Schulen, nicht auf sozialen Auswirkungen oder marginalisierten Gruppen. TECHNIK erfüllt (AI_Literacies), SOZIAL nicht erfüllt → Exclude."
114,UKKQKL7I,Washington (2025),Fragile Foundations: Hidden Risks of Generative AI,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Include,,Literaturreview,0.85,"Das Paper behandelt substantiell Generative KI (Foundation Models, ChatGPT, Gemini) mit kritischem Fokus. Es adressiert Systemische Risiken und Gefahren für vulnerable Gruppen (Bias_Ungleichheit) sowie kritisches Verständnis der KI-Technologie (AI_Literacies). Die Kombination TECHNIK (Generative_KI + AI_Literacies) UND SOZIAL (Bias_Ungleichheit) erfüllt beide Einschlussbedingungen. Mission-driven organizations können als sozialrelevanter Kontext interpretiert werden, aber Fokus liegt primär auf Risiken für vulnerable Gruppen."
115,8MRNK6FX,[Author not specified] (2024),Research on the application risks and countermeasures of ChatGPT generative artificial intelligence in social work,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Unclear,0.85,"Der Titel signalisiert explizit: (1) Generative KI (ChatGPT), (2) Soziale Arbeit, (3) Anwendungsrisiken und Gegenmaßnahmen. Dies erfüllt beide Inklusionsbedingungen: TECHNIK_OK (Generative_KI=Ja), SOZIAL_OK (Soziale_Arbeit=Ja). AI_Literacies wird mit Ja bewertet, da Risiken/Gegenmaßnahmen KI-Kompetenzentwicklung implizieren. Kein Abstract liegt vor, aber der Titel ist ausreichend informativ."
116,SSF5Q33W,[Author not specified] (2024),Research on the application risks and countermeasures of ChatGPT generative artificial intelligence in social work,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Unclear,0.85,"Der Titel deutet auf eine Analyse von ChatGPT (generative KI) in der Sozialen Arbeit hin. Dies erfüllt beide notwendigen Bedingungen: TECHNIK (Generative_KI = Ja, da ChatGPT explizit genannt) und SOZIAL (Soziale_Arbeit = Ja, da direkt im Titel). AI_Literacies ist ebenfalls Ja, da es um Kompetenzen/Risiken im Umgang mit ChatGPT geht. Ohne Abstract kann Studientyp nicht genau bestimmt werden."
117,LPMP8QAY,Weber et al. (2023),Messung von AI Literacy – Empirische Evidenz und Implikationen,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,No_full_text,Empirisch,0.65,"Titel deutet auf AI Literacy hin (Technik-Komponente erfüllt). Jedoch fehlt Abstract und Volltextinformation. Ohne Textmaterial kann nicht überprüft werden, ob die SOZIAL-Komponente erfüllt ist (Bezug zu Sozialer Arbeit, Bias, Gender, Diversität, Fairness oder Feministik). Konservative Bewertung: Exclude wegen fehlender Information."
118,KQ5A8D6E,[Author not specified] (2024),AI competency framework for students,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,No_full_text,Unclear,0.3,"Der Titel deutet auf AI Literacies (Kompetenzen für Studierende) hin, was die TECHNIK-Bedingung erfüllen könnte. Jedoch fehlt das Abstract und der vollständige Text, was eine fundierte Bewertung unmöglich macht. Zudem ist unklar, ob ein Sozialbezug vorliegt. Ohne Volltext kann nicht überprüft werden, ob die SOZIAL-Bedingung erfüllt ist."
119,K3YCLBXK,Ruiz et al. (2024),"AI Literacy: A Framework to Understand, Evaluate, and Use Emerging Technology",Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Konzept,0.95,"Das Paper behandelt AI Literacy im Bildungskontext (PK-12 Education) und erfüllt die TECHNIK-Bedingung durch AI_Literacies = Ja. Es fehlt jedoch jegliche SOZIAL-Komponente: Kein Bezug zu Sozialer Arbeit, keine Behandlung von Bias/Ungleichheit, Gender, Diversität, feministischen Perspektiven oder Fairness. Es ist ein rein pädagogisches Framework für allgemeine Bildung ohne Sozialarbeitsfokus oder kritische Betrachtung struktureller Ungleichheiten."
120,BFG8VUK3,"Quaid-i-Azam University, Islamabad, Pakistan & Shah (2025)",Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Nein,Ja,Include,,Literaturreview,0.92,"Das Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (digitale Kompetenzen zur Empowerment), KI_Sonstige (algorithmische Systeme in Recruitment, Healthcare, Finance). SOZIAL: Gender (expliziter Gender-Bias-Fokus), Bias_Ungleichheit (Diskriminierung durch biased datasets und unter-representation), Diversitaet (Inklusion in AI-Entwicklung), Fairness (Bias-Mitigation). Kein explizit feministischer Rahmen erkennbar, aber substantielle Behandlung von Gender und Ungleichheit."
121,FTJM5R8N,[Author not specified] (n.d.),Defeating Nondeterminism in LLM Inference,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Konzept,0.95,"Das Paper behandelt rein technische Aspekte von LLM-Inferenz (Nondeterminismus, Sampling, Reproducibility). Während es Generative KI erwähnt, fehlt jeder soziale Bezug: keine AI Literacies, keine Bias/Fairness-Analyse, keine Bezüge zu Soziale Arbeit, Diversität oder Ungleichheit. Es ist ein rein technisches Konzeptpaper ohne gesellschaftliche, ethische oder pädagogische Dimension."
122,MPCZVZEW,Goellner et al. (2025),Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Konzept,0.85,"Paper adressiert AI Literacy durch Forderung nach Stakeholder-Involvement und Transparenz in KI-Systemen für Bildung. KI_Sonstige zutreffend: Neural-Symbolic AI ist ML-Ansatz. Bias_Ungleichheit und Fairness erfüllt durch Kritik an irresponsible KI-Use, unreliablen XAI-Methoden und Forderung nach Verantwortlichkeit. TECHNIK (AI_Literacies + KI_Sonstige) UND SOZIAL (Bias_Ungleichheit + Fairness) erfüllt → Include. Keine direkten Sozialen Arbeits- oder Gender-Bezüge identifiziert."
123,XY2WVKBY,Zeng & van Es (2025),"Governing discriminatory content in conversational AI: A cross-system, cross-lingual, and cross-topic audit",Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Empirisch,0.92,"Das Paper untersucht Diskriminierungsinhalte in conversational AI-Systemen (Generative_KI: Ja). Die Audit-Analyse fokussiert auf Bias und Diskriminierung (Bias_Ungleichheit: Ja), behandelt Unterschiede across Sprachen und Themen (Diversitaet: Ja) und diskutiert Fairness-Aspekte durch RLHF und Guardrails (Fairness: Ja). TECHNIK-Bedingung erfüllt (Generative_KI), SOZIAL-Bedingung erfüllt (Bias_Ungleichheit, Diversitaet, Fairness). Include."
124,JPUCNHNU,Pan et al. (2025),LIBRA: Measuring bias of large language model from a local context,Nein,Ja,Nein,Ja,Nein,Ja,Nein,Ja,Nein,Ja,Exclude,Not_relevant_topic,Empirisch,0.85,"Paper erfüllt TECHNIK-Kriterium (Generative_KI, KI_Sonstige) und SOZIAL-Kriterium (Bias_Ungleichheit, Diversitaet, Fairness). Allerdings: Paper behandelt allgemeine LLM-Bias-Evaluation ohne direkten Bezug zu Sozialer Arbeit, sozialarbeiterischen Zielgruppen oder sozialarbeiterischer Praxis. Bias-Analyse ist technisch-orientiert (Fairness-Metriken für LLMs), nicht anwendungsorientiert für Sozialarbeit. Keine Verbindung zu Soziale_Arbeit identifizierbar."
125,YV53DKI2,Santos (2024),Explainability through systematicity: The hard systematicity challenge,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Only_Technik_keine_Sozial,Theoretisch,0.95,"Paper behandelt philosophische Grundlagen von AI-Explainability und Systematizität (KI_Sonstige: Ja). Dies ist jedoch rein technisch-philosophischer Natur ohne Bezug zu sozialen Auswirkungen, Bias, Ungleichheit, Gender, Diversität, Feminismus oder Fairness. Kein Bezug zu Sozialer Arbeit. TECHNIK erfüllt (KI_Sonstige), SOZIAL nicht erfüllt → Exclude."
126,BDBYDLVK,Singh et al. (2025),A reparative turn in AI,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Empirisch,0.92,Paper behandelt KI-Governance und -Harms substantiell (KI_Sonstige: Ja). Adressiert algorithmische Diskriminierung und Schadensanalyse (Bias_Ungleichheit: Ja) sowie Accountability und Fairness in KI-Systemen (Fairness: Ja). Erfüllt beide Bedingungen: TECHNIK (KI_Sonstige) UND SOZIAL (Bias_Ungleichheit + Fairness). Basiert auf empirischer Analyse von 1.060 realen Incidents.
127,T9KEZN3G,Taeihagh (2025),Governance of generative AI: A comprehensive framework for navigating challenges and opportunities,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Theoretisch,0.85,"Das Paper behandelt substantiell Generative KI (Kategorie erfüllt) und adressiert Bias-Amplifikation, Machtungleichgewichte und Fairness-Fragen (Bias_Ungleichheit und Fairness erfüllt). Es erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja) und SOZIAL_OK (Bias_Ungleichheit=Ja, Fairness=Ja). Keine Sozialen-Arbeit-Perspektive erkennbar, daher diese Kategorie Nein."
128,99QJDBSV,Santos et al. (2025),How large language models judge cooperation,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Empirisch,0.82,"Paper untersucht LLMs (Generative_KI: Ja) systematisch mit 43.200 Prompts (Prompting: Ja, substantiell). Zentrale Ergebnisse adressieren Bias und Malleabilität sozialer Normen (Bias_Ungleichheit: Ja) sowie Fairness-Aspekte bei moralischen Urteilen (Fairness: Ja). TECHNIK und SOZIAL erfüllt → Include. Kein direkter Soziale-Arbeit-Bezug, aber relevante Erkenntnisse zu KI-Systemen und Ungleichheiten."
129,R3VJVFCE,Kamruzzaman & Kim (2024),Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Experimentell,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting substantiell behandelt), SOZIAL_OK (Bias_Ungleichheit + Fairness adressiert). Empirische Untersuchung von Prompt-Engineering-Strategien zur Bias-Reduktion in LLMs. Keine sozialen Arbeitskontexte, aber algorithmische Fairness ist relevant für Inklusion."
130,GCQ8J9XF,Barman et al. (2024),Beyond transparency and explainability: On the need for adequate and contextualized user guidelines for LLM use,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Include,,Konzept,0.85,"Paper behandelt substantiell AI_Literacies (user guidelines und training für LLM-Nutzung), Generative_KI (LLM-fokussiert), Prompting (diversity-sensitive prompting techniques explizit genannt) und adressiert Bias_Ungleichheit sowie Diversität (diversity-sensitive approaches). Erfüllt TECHNIK (3 Kategorien) und SOZIAL (2 Kategorien). Kein direkter Sozialarbeitsbezug identifizierbar."
131,EMZ33KFH,Ghosal et al. (2025),Unequal voices: How LLMs construct constrained queer narratives,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Include,,Empirisch,0.92,"Paper untersucht systematisch Bias und Diskriminierung in LLM-generierten Narrativen von queeren Personen. Erfüllt TECHNIK-Kriterium (Generative_KI: Fokus auf LLM-Outputs) und SOZIAL-Kriterium (Bias_Ungleichheit: Stereotypisierung marginalisierter Gruppen; Gender: queere Identitäten; Diversitaet: Repräsentation von LGBTQ+-Personen in KI-Systemen). Keine explizite feministische Theorie erkannt, daher Feministisch=Nein."
132,WLV8L8PM,Tint (2025),"Guardrails, not guidance: Understanding responses to LGBTQ+ language in large language models",Nein,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Include,,Empirisch,0.92,"Paper untersucht substantiell LLM-Responses auf LGBTQ+-Prompts (Generative_KI: Ja; Prompting: Ja - systematische Prompt-Variation). Zeigt algorithmischen Bias gegen queer/marginalisierte Communities (Bias_Ungleichheit: Ja), thematisiert Gender/sexuelle Orientierung (Gender: Ja; Diversitaet: Ja - LGBTQ+ als marginalisierte Gruppe; Fairness: Ja - Asymmetrien in Safety-Guardrails). TECHNIK + SOZIAL erfüllt."
133,38E5FZDV,Bai et al. (2025),Explicitly unbiased large language models still form biased associations,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Experimentell,0.92,"Paper untersucht implizite Biase in LLMs durch neuartige Evaluationsmethoden (LLM-WAT, LLM-RDT). Erfüllt TECHNIK-Kriterium (Generative_KI=Ja) und SOZIAL-Kriterien (Bias_Ungleichheit=Ja, Fairness=Ja). Fokus auf stereotype Assoziationen und subtile Diskriminierung in State-of-the-Art Modellen. Kein expliziter Gender-, Diversitäts- oder feministischer Fokus erkennbar, daher diese Kategorien Nein. Kein direkter Sozialarbeitsbezug."
134,WNY526GN,Gallegos et al. (2024),Bias and fairness in large language models: A survey,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Exclude,"Technik-Bedingung erfüllt (Generative_KI=Ja), aber keine Soziale_Arbeit-Kategorie. Bias_Ungleichheit, Diversitaet und Fairness sind vorhanden, aber der Sozial-Kreis ist nur über allgemeine KI-Ethik-Themen erfüllt, nicht durch Soziale_Arbeit spezifisch. Da das Paper auf allgemeine LLM-Bias-Literatur fokussiert und keinen direkten Sozialarbeit-Bezug hat, wird Soziale_Arbeit korrekt als 'Nein' bewertet.",Literaturreview,0.95,"Das Paper ist ein Überblicks-Survey zu Bias und Fairness in LLMs. Es erfüllt die TECHNIK-Bedingung (Generative_KI=Ja) und behandelt substantiell Bias_Ungleichheit und Fairness. Allerdings fehlt die SOZIAL-Bedingung im Sinne der definierten Kategorien: Es gibt keinen direkten Bezug zu Sozialer Arbeit, und die Bias/Fairness-Behandlung ist allgemein auf NLP fokussiert, nicht spezifisch auf sozialarbeiterische Kontexte oder marginalisierte Gruppen im Sozialbereich. Daher: Exclude."
135,22XEFRWP,Petzel & Sowerby (2025),Prejudiced interactions with large language models (LLMs) reduce trustworthiness and behavioral intentions among members of stigmatized groups,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Empirisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI: LLMs sind zentral), SOZIAL_OK (Bias_Ungleichheit: Analyse von Prejudice in LLM-Outputs; Diversitaet: Fokus auf marginalized groups; Fairness: Implikationen für vertrauenswürdige KI-Nutzung). Drei preregistrierte experimentelle Studien mit klarem Fokus auf Diskriminierung und deren Auswirkungen auf Nutzervertrauen."
136,QDUGSBPC,An et al. (2025),Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation,Nein,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Empirisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + KI_Sonstige: LLM-basierte Entscheidungssysteme), SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch: intersektionale Analyse von Geschlechts- und Rassenbias). Explizit intersektionales Design nach Crenshaw-Ansatz (Gender × Rasse). Substanzielle empirische Analyse mit sozialen Implikationen für algorithmic fairness."
137,7L78MV2V,Navigli et al. (2023),"Biases in large language models: Origins, inventory and discussion",Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Include,,Literaturreview,0.92,"Paper erfüllt TECHNIK-Kriterium (Generative_KI: LLMs) und SOZIAL-Kriterium (Bias_Ungleichheit, Gender, Diversitaet, Fairness). Systematische Analyse von sozialen Biases in LLMs mit Fokus auf Geschlecht, Ethnie, sexuelle Orientierung, Alter und Religion. Behandelt Messungs- und Mitigationsstrategien. Nicht feministisch (keine explizite feministische Theorie), nicht zu Sozialer Arbeit."
138,IW32JGWV,OECD (2023),Advancing Accountability in AI,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Exclude,Not_relevant_topic,Konzept,0.85,"Paper erfüllt TECHNIK-Kriterium (KI_Sonstige=Ja) und SOZIAL-Kriterium (Bias_Ungleichheit=Ja, Fairness=Ja). ABER: Kein direkter Bezug zu Sozialer Arbeit, keine spezifische soziale Ungleichheit oder marginalisierte Gruppen adressiert. Allgemeine AI-Governance und Fairness-Diskussion ohne Sozialarbeitsbezug. Daher außerhalb des Fokus eines SA-Literature-Reviews."
139,LBLF9BCW,Lau (2023),Dipper: Diversity in Prompts for Producing Large Language Model Outputs,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Experimentell,0.85,"Das Paper behandelt Prompting (Dipper-Framework mit diversen Prompts) und Generative KI (LLMs) substantiell. Es adressiert Diversity (marginalisierte Gruppen, Perspektivenvielfalt), Bias_Ungleichheit (Verbesserung von Darstellung minorities) und Fairness (faire Output-Generierung). TECHNIK und SOZIAL sind erfüllt. Kein Soziale_Arbeit-Bezug, kein Gender- oder Feministisch-Fokus."
140,QFPTW4VL,European Data Protection Supervisor (2023),Explainable Artificial Intelligence,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Konzept,0.85,"Paper behandelt XAI als Kompetenz- und Transparenzthema (AI_Literacies), mit Fokus auf algorithmische Systeme (KI_Sonstige). Substantielle Thematisierung von Bias/Ungleichheit durch Marginalisierung, Diversität durch explizite Nennung marginalisierter Gruppen und Fairness durch ethische/gerechte Entscheidungsfindung. TECHNIK und SOZIAL erfüllt → Include."
141,DUV4TUG3,Lahoti et al. (2023),Improving diversity of demographic representation in people entities in Large Language Models,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Experimentell,0.92,"Paper adressiert Generative KI (LLMs), Prompting (CCSV-Methode), sowie Bias/Ungleichheit durch demografische Unterrepräsentation, Diversität und Fairness. TECHNIK_OK (Generative_KI + Prompting), SOZIAL_OK (Bias_Ungleichheit + Diversitaet + Fairness). Erfüllt Einschlusskriterien. Kein expliziter Gender-Fokus, keine feministische Theorie, keine Soziale Arbeit."
142,6L6WSDC8,Hayati et al. (2024),How Far Can We Extract Diverse Perspectives from Large Language Models?,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Empirisch,0.85,"Paper adressiert LLMs (Generative_KI) und Prompting-Strategien (Diversity Prompting) substanziell. Zentraler Fokus liegt auf Mitigation von ""dominant group bias"" und Erhöhung von Perspektivenvielfalt - klare Bias_Ungleichheit und Diversitaet sowie Fairness-Themen. TECHNIK und SOZIAL erfüllt → Include."
143,3AHQEHDF,Wang et al. (2024),Multilingual Prompting for Improving LLM Generation Diversity,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Empirisch,0.85,"Paper erfüllt TECHNIK-Kriterium: Generative_KI (LLM-Fokus) + Prompting (Multilingual Prompting als Kern). Erfüllt SOZIAL-Kriterium: Bias_Ungleichheit (Halluzinationen, Representation), Diversitaet (demografische und kulturelle Vielfalt), Fairness (representative generation). Kein Soziale_Arbeit- oder Gender-Fokus. Empirische Evaluation mit substantiellem Prompting-Engineering-Beitrag."
144,PH7JBBC8,Bisconti et al. (2024),A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Theoretisch,0.95,"Paper entwickelt formale Konzeptualisierung von KI-Vertrauenswürdigkeit (KI_Sonstige erfüllt). Es fehlt jedoch jeglicher Bezug zu sozialen Dimensionen: Keine Behandlung von Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversität, Fairness oder feministischen Perspektiven. Rein technisch-theoretisches Paper ohne sozialen Anwendungskontext oder kritische Gesellschaftsperspektive. TECHNIK_OK, aber SOZIAL_NICHT_OK → Exclude."
145,7AMRV4DT,Srinivasan & Thomason (2025),Mitigating trust-induced inappropriate reliance on AI assistance,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Ja,Exclude,SOZIAL-Kriterium nicht erfüllt,Experimentell,0.95,"Paper adressiert AI Literacy (kritisches Verständnis von KI-Limitations und vertrauensbasierter Reliance), KI_Sonstige (Decision-Support-Systeme), und Fairness (angemessenes Vertrauen, bias-freie Entscheidungen). ABER: Kein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder feministischer Theorie. Rein HCI-/Human-AI-Interaction-Forschung ohne sozialarbeiterischen oder kritischen Ungleichheits-Fokus. Sozialarbeitsbezug nicht substantiell gegeben."
146,ZGM7K3H6,Goldkind et al. (2024),Artificial intelligence in social work: An EPIC model for practice,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK durch AI_Literacies (EPIC-Modell für KI-Integration), KI_Sonstige (algorithmische Systeme im Sozialbereich) UND SOZIAL durch Soziale_Arbeit (direkter Fokus auf sozialarbeiterische Praxis), Bias_Ungleichheit (Bias-Mitigation) und Fairness (Transparenz, Bias-Adressierung). Intersektionales Denken impliziert, aber nicht explizit feministisch."
147,Y4BMCI2J,Asseri et al. (2024),Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Literaturreview,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI, Prompting substantiell behandelt), SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Fairness adressiert). Systematische Review zu Prompt-Engineering-Techniken für Bias-Mitigation gegen arabische/muslimische Minderheiten in LLMs. Direkter Fokus auf kulturelle Diskriminierung und Fairness-Optimierung."
148,4PK8UN82,Victor et al. (2023),Recommendations for social work researchers and journal editors on the use of generative AI and large language models,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Konzept,0.95,"Paper adressiert substantiell generative KI und LLMs (Generative_KI: Ja), entwickelt ein Framework für kritische Analyse (AI_Literacies: Ja) mit explizitem Fokus auf Prompt-Dokumentation und -Transparenz (Prompting: Ja). Direkter Bezug zu sozialarbeiterischer Forschungspraxis und Journal-Editoren in der Sozialen Arbeit (Soziale_Arbeit: Ja). Alle Bedingungen erfüllt: TECHNIK (3x Ja) UND SOZIAL (1x Ja)."
149,EXVG7MQR,Reamer (2023),Artificial intelligence in social work: Emerging ethical issues,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies, KI_Sonstige, Fairness durch Bias-Thematisierung) und SOZIAL (Soziale_Arbeit, Bias_Ungleichheit, Fairness). Systematische ethische Analyse von KI in Sozialarbeit mit substantiellem Fokus auf algorithmischen Bias, Transparenz und Fairness-Implementierung. Direkt relevant für Sozialarbeitspraxis."
150,BCBWSU3Z,Debnath et al. (2024),Can LLMs reason about trust? A pilot study,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Experimentell,0.85,"Paper behandelt generative KI (LLMs) substantiell, erfüllt aber KEINE der sozialen Kategorien. Untersucht LLM-Fähigkeiten im Trust-Reasoning ohne Bezug zu Soziale Arbeit, Bias/Ungleichheit, Gender, Diversität, Feminismus oder Fairness. Keine direkte Relevanz für Sozialarbeitskontext oder marginalisierte Gruppen. Rein technische KI-Evaluierungsstudie."
151,G53MCF3W,Gaba et al. (2025),"Bias, accuracy, and trust: Gender-diverse perspectives on large language models",Nein,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Include,,Empirisch,0.92,"Paper erfüllt beide Inklusionskriterien: TECHNIK_OK (Generative_KI, Prompting), SOZIAL_OK (Bias_Ungleichheit, Gender, Fairness). Qualitative Studie zu LLM-Bias mit explizitem Gender-Fokus. Prompting wird als Bias-Mitigationsstrategie thematisiert. Kein Sozialarbeitsbezug, keine explizit feministische Theorie."
152,8JFZMD5G,Colombatto et al. (2025),The influence of mental state attributions on trust in large language models,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,"SOZIAL-Kriterium nicht erfüllt: Keine substantielle Behandlung von Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch oder Fairness",Empirisch,0.95,"Paper erfüllt TECHNIK-Kriterium mit 3 Ja-Kategorien (AI_Literacies: User-Trust-Kompetenz; Generative_KI: LLMs; Prompting: Prompt-Engineering-Vorschläge). Jedoch keine SOZIAL-Kategorien erfüllt: Kein direkter Sozialarbeitsbezug, keine Ungleichheits-, Gender-, Diversitäts-, feministischen oder Fairness-Analysen. Rein kognitive Vertrauensstudie ohne sozialgerechte Dimensionen."
153,ERTJZW5M,Tun et al. (2025),Trust in artificial intelligence–based clinical decision support systems among health care workers: Systematic review,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Exclude,Not_relevant_topic,Literaturreview,0.85,"Paper erfüllt TECHNIK-Kriterium (AI_Literacies, KI_Sonstige) und SOZIAL-Kriterium (Bias_Ungleichheit, Fairness). JEDOCH: Thema ist klinische Entscheidungsunterstützung im Gesundheitswesen, NICHT Soziale Arbeit. Keine Verknüpfung zu sozialarbeiterischer Praxis, Theorie oder Zielgruppen. Bias/Fairness-Diskussion bezieht sich auf medizinische Kontexte, nicht auf Soziale Arbeit. Daher: außerhalb des Scope des Literaturreviews zu KI in Sozialer Arbeit."
154,5ERYSQCK,De Duro et al. (2025),Measuring and identifying factors of individuals' trust in large language models,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Empirisch,0.85,"Paper behandelt Trust in LLMs und AI Literacies (Vertrauen/Kompetenzwahrnehmung), fokussiert auf generative KI. Prompting wird nur beiläufig erwähnt ('prompt-engineering'), nicht substantiell behandelt. Kritisch: Keine der SOZIAL-Kategorien erfüllt - kein Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität, feministische Perspektive oder Fairness. Rein psychometrische Studie zu Trust-Faktoren ohne sozialen/kritischen Fokus. TECHNIK_OK, aber SOZIAL_NICHT_OK → Exclude."
155,RXNXJA8W,Steyvers et al. (2025),What large language models know and what people think they know,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Ja,Exclude,No_relevant_social_dimension,Empirisch,0.85,"Paper adressiert generative KI (LLMs), Prompting (uncertainty calibration durch Prompt Engineering) und AI Literacy (Vertrauen in LLM-Ausgaben verstehen). Behandelt auch Fairness-Aspekte durch Kalibrierung. ABER: Kein substantieller sozialer Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder Feminist. Perspektive. Es ist eine allgemeine KI-HCI-Studie ohne Sozialarbeitsbezug. Daher: Nur Technik erfüllt, nicht Sozial → Exclude."
156,JKF6VAQB,Park & Yoon (2025),"AI algorithm transparency, pipelines for trust not prisms: mitigating general negative attitudes and enhancing trust toward AI",Ja,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Ja,Exclude,"SOZIAL_OK nicht erfüllt: Keine substantielle Verbindung zu Sozialer Arbeit, kein expliziter Fokus auf Diskriminierung/Ungleichheit, Gender oder Diversität",Experimentell,0.85,"Paper behandelt AI-Vertrauen, Transparenz und Accountability (AI_Literacies, KI_Sonstige, Fairness) sowie Prompting im Design-Kontext. Erfüllt TECHNIK_OK. Fehlt aber SOZIAL_OK: Kein direkter Bezug zu Sozialer Arbeit/Zielgruppen, keine Analyse von Bias/Ungleichheit oder struktureller Benachteiligung. Fairness bezieht sich auf Transparenz-Metriken, nicht auf systematische Diskriminierung."
157,EJEFPZGA,[Author not specified] (2025),"Navigating the Nexus of Trust: Prompt Engineering, Professional Judgment, and the Integration of Large Language Models in Social Work",Ja,Ja,Ja,Nein,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Konzept,0.92,Paper adressiert substantiell: (1) Prompting: Prompt-Engineering-Strategien zur Bias-Mitigation und Transparenzförderung sind zentral; (2) Generative_KI: LLMs in sozialer Praxis; (3) Soziale_Arbeit: direkter Bezug zu Case Recommendations und Professional Practice; (4) Bias_Ungleichheit und Fairness: explizit Bias-Mitigation und Trust-Framework; (5) AI_Literacies: Professionelle Kompetenzentwicklung (aktive Direktoren vs. passive Konsumenten). TECHNIK und SOZIAL beide erfüllt → Include.
158,7W3RGSSG,Kamruzzaman & Kim (2024),Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Empirisch,0.92,"Das Paper erfüllt beide Bedingungen: TECHNIK-Bereich durch Generative_KI (LLMs) und Prompting (12 prompt-engineering Strategien sind substantieller Fokus). SOZIAL-Bereich durch Bias_Ungleichheit (explizit: 'social biases', 'stereotypical responses') und Fairness (Bias-Mitigation, Fairness-Strategien). Kein Feministisch, da keine explizite feministische Theorie verwendet."
159,NFW58AU8,Xu et al. (2023),Transparency enhances positive perceptions of social artificial intelligence,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Experimentell,0.85,"Paper behandelt Chatbot-Transparenz und Nutzerwahrnehmung mit Fokus auf AI Literacy (Effekt bei niedriger Vorkenntnis) und Prompting (Transparenz durch Prompt-Engineering). Technische Kriterien erfüllt. Es fehlt jedoch jeglicher Bezug zu sozialen Kategorien: Keine Soziale Arbeit, keine Analyse von Bias/Ungleichheit, kein Gender-, Diversitäts- oder Fairness-Fokus. Reine HCI-Studie ohne Bezug zu marginalizierten Gruppen, struktureller Benachteiligung oder Sozialarbeitspraxis."
160,ED6C8LD2,Choudhury & Chaudhry (2024),Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Exclude,"TECHNIK_OK (AI_Literacies, Generative_KI, Prompting, Bias_Ungleichheit, Fairness erfüllt), aber SOZIAL_OK nicht erfüllt: Kein direkter Bezug zu Sozialer Arbeit. Das Paper behandelt LLMs im Healthcare/Klinischen Kontext, nicht in der Sozialen Arbeit. Die genannten SOZIAL-Kategorien (Bias_Ungleichheit, Fairness) sind zwar vorhanden, aber das Papier diskutiert primär technische/berufliche Aspekte in der Medizin, nicht sozialarbeiterische Praxis, Theorie oder deren Zielgruppen.",Konzept,0.85,"Das Paper behandelt substantiell LLMs (Generative_KI), Prompt-Engineering für Transparenz (Prompting), AI-Literacies (kritisches Vertrauen, Deskilling), sowie Bias und Fairness. Allerdings adressiert es Healthcare-Professionals (Kliniker), nicht Soziale Arbeit oder deren Zielgruppen. Für einen Literature Review zu KI in der Sozialen Arbeit ist die fehlende Konnexion zur Sozialen Arbeit disqualifizierend."
161,XJCEMM3D,Bisconti et al. (2024),A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Theoretisch,0.85,"Das Paper behandelt algorithmische Trustworthiness und Vertrauenskonzepte in KI-Systemen (KI_Sonstige = Ja). Es erfüllt aber keine sozialen Kategorien: Kein expliziter Bezug zu Sozialer Arbeit, keine Analyse von Bias/Ungleichheit, kein Gender- oder Diversitätsfokus, keine feministische Perspektive, keine Fairness-Metriken. Es ist rein theoretisch-konzeptuell ohne Anwendungsbezug zu gesellschaftlich vulnerable Gruppen. Daher TECHNIK erfüllt, SOZIAL nicht erfüllt → Exclude."
162,RR5MJRBZ,Goldkind et al. (2024),Artificial intelligence in social work: An EPIC model for practice,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Konzept,0.92,"Paper erfüllt beide Bedingungen: (1) TECHNIK: AI_Literacies (Rahmenmodell für KI-Integration in Praxis), KI_Sonstige (algorithmische Systeme im Sozialbereich), Fairness (Bias-Mitigation, Transparenz). (2) SOZIAL: Soziale_Arbeit (direkter Fokus auf Integrationsstrategie in sozialarbeiterischer Praxis), Bias_Ungleichheit (explizite Behandlung von Bias-Mitigation), Fairness. EPIC-Modell ist substantielle konzeptuelle Beitrag mit Ethik, Policy und Community-Perspektive für KI in Sozialer Arbeit."
163,3GB9B4IJ,Debnath et al. (2024),Can LLMs reason about trust? A pilot study,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Experimentell,0.85,"Das Paper behandelt LLMs (Generative_KI = Ja) und untersucht deren Fähigkeiten zur Analyse von Vertrauen. Es erfüllt aber KEINE der erforderlichen SOZIAL-Kategorien: Kein Bezug zu Sozialer Arbeit, keine Behandlung von Bias/Ungleichheit, Gender, Diversität, feministischen Perspektiven oder Fairness. Die Studie zu Trust-Reasoning ist rein technisch-kognitiv ausgerichtet, ohne sozialkritischen oder anwendungsbezogenen Fokus auf vulnerable Gruppen oder strukturelle Ungleichheiten."
164,25XSMXKT,Asseri et al. (2024),Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Literaturreview,0.95,"Paper erfüllt TECHNIK-Bedingung (Generative_KI=Ja, Prompting=Ja) und SOZIAL-Bedingung (Bias_Ungleichheit=Ja, Diversitaet=Ja, Fairness=Ja). Systematische Review zu Prompt-Engineering-Techniken zur Reduktion kultureller Vorurteile gegen Araber und Muslime in LLMs mit substantiellem Fokus auf Fairness und strukturelle Diskriminierungsprobleme."
165,9YYPYEGY,Victor et al. (2023),Recommendations for social work researchers and journal editors on the use of generative AI and large language models,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, Prompting=Ja, AI_Literacies=Ja durch Fokus auf Bildung und kritisches Verständnis von LLMs) und SOZIAL_OK (Soziale_Arbeit=Ja, direkter Bezug zu Forschung und Praxis in Social Work). Framework für verantwortungsvollen KI-Einsatz mit Dokumentations- und Transparenzanforderungen ist substantiell."
166,Q8YPNNKL,Srinivasan & Thomason (2025),Mitigating trust-induced inappropriate reliance on AI assistance,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Ja,Exclude,"Only TECHNIK erfüllt, nicht SOZIAL. Paper behandelt AI-Kompetenz (Trust/Reliance Management), KI-Systeme (Decision Support) und Fairness im KI-Kontext. Es fehlt aber jeglicher Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder feministischen Perspektiven. Die Fairness-Dimension ist rein technisch (faire Gewichtung von User Trust) ohne sozialen Impact-Fokus.",Experimentell,0.95,"Paper adressiert Mensch-KI-Interaktion und Trust-Adaptivität in Decision-Support-Systemen (AI_Literacies, KI_Sonstige, Fairness). Allerdings fehlt der erforderliche SOZIAL-Anker komplett: kein Bezug zu Soziale Arbeit, keine Analyse von strukturellen Ungleichheiten, keine marginalisierten Gruppen im Fokus. Fairness wird rein technisch/kognitiv behandelt."
167,IUN7Z56I,Reamer (2023),Artificial intelligence in social work: Emerging ethical issues,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Theoretisch,0.95,Das Paper behandelt systematisch ethische Herausforderungen von KI in der Sozialen Arbeit und entwickelt einen umfassenden Implementierungsrahmen. TECHNIK_OK: AI_Literacies (Kompetenzrahmen für ethische KI-Implementierung) + KI_Sonstige (algorithmische Systeme im Sozialkontext). SOZIAL_OK: Soziale_Arbeit (direkter Bezug zu SW-Praxis und -Ethik) + Bias_Ungleichheit (algorithmischer Bias wird thematisiert) + Fairness (Framework-Entwicklung für faire KI). Beide Bedingungen erfüllt → Include.
168,F7WNRWIC,De Duro et al. (2025),Measuring and identifying factors of individuals' trust in large language models,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Empirisch,0.85,"Paper behandelt LLM-Vertrauen und individuelle Unterschiede (AI_Literacies, Generative_KI erfüllt). ABER: Kein Bezug zu sozialen Ungleichheiten, Bias, Gender, Diversität, Fairness oder Sozialer Arbeit. Prompting wird nur beiläufig erwähnt ('prompt-engineering'), nicht substantiell behandelt. Psychologische Vertrauensforschung ohne sozialen/kritischen Bezug. SOZIAL-Kriterien nicht erfüllt → Exclude."
169,GFHALQS2,Tun et al. (2025),Trust in artificial intelligence–based clinical decision support systems among health care workers: Systematic review,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Exclude,Wrong_publication_type / Not_relevant_topic,Literaturreview,0.85,"Paper behandelt AI-Kompetenzen (Vertrauen, Transparenzverstaendnis) und KI-Sonstige (klinische Entscheidungssysteme) sowie Bias/Fairness. ABER: Fokus liegt auf Healthcare/klinischem Kontext, nicht auf Soziale Arbeit. Keine direkter Bezug zu sozialarbeiterischer Praxis, Theorie oder Zielgruppen. SOZIAL-Kriterium nicht erfuellt: Bias_Ungleichheit und Fairness im Healthcare-Kontext sind nicht spezifisch fuer Soziale Arbeit. Paper adressiert Technik + allgemeine Ethik, aber nicht die sozialen Dimensionen des Literaturreviews (Soziale Arbeit, Gender, Diversitaet, Feministisch)."
170,MUBZ8XJL,Gaba et al. (2025),"Bias, accuracy, and trust: Gender-diverse perspectives on large language models",Nein,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Include,,Empirisch,0.95,"Paper erfüllt TECHNIK-Kriterien durch Generative_KI (ChatGPT-Fokus) und Prompting (Empfehlungen zu clarifying prompts). Erfüllt SOZIAL-Kriterien durch expliziten Gender-Fokus (unterschiedliche Geschlechteridentitäten), Bias_Ungleichheit (stereotype responses, verringerte Vertrauenswerte) und Fairness (Bias-Mitigation-Strategien). Qualitative empirische Studie mit substantiellem Bias- und Gender-Fokus."
171,7IY7AX7D,Colombatto et al. (2025),The influence of mental state attributions on trust in large language models,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Empirisch,0.85,"Das Paper behandelt substantiell AI_Literacies (Vertrauen, Kompetenzen im Umgang mit LLMs) und Generative_KI sowie Prompting-Strategien. Jedoch fehlt vollständig der zweite erforderliche Bereich (Sozial): keine direkte Soziale_Arbeit, keine Bias_Ungleichheit, kein Gender/Diversität/Feminismus/Fairness-Fokus. Es ist eine reine Nutzerpsychologie-Studie zu LLM-Vertrauen ohne sozialen Impact-Fokus. Die Prompt-Engineering-Erwähnung ist eher ein Implikation, nicht substantiell. TECHNIK_OK=Ja, SOZIAL_OK=Nein → Exclude."
172,KRUQB7L2,Steyvers et al. (2025),What large language models know and what people think they know,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Ja,Exclude,"TECHNIK_OK (AI_Literacies, Generative_KI, Prompting, Fairness: alle Ja), aber SOZIAL_OK nicht erfuellt. Keine der Kategorien Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet oder Feministisch erreicht Ja-Status. Fairness allein genuegt nicht fuer SOZIAL_OK.",Empirisch,0.95,"Das Paper behandelt AI Literacy (Benutzervertrauen, Kalibrierung), Generative KI (LLMs) und Prompting-Strategien (Unsicherheitskonveyance). Fairness-relevant bezueglich Transparenz. Jedoch kein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversitaet oder feministischer Perspektive. Allgemeine KI-HCI-Studie ohne soziale Gerechtigkeitsdimension."
173,WAYCKUZ8,Choudhury & Chaudhry (2024),Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Exclude,"TECHNIK_OK (AI_Literacies, Generative_KI, Prompting, Fairness, Bias_Ungleichheit), aber SOZIAL_OK nur durch Fairness und Bias_Ungleichheit erfüllt. Kein direkter Bezug zu Sozialer Arbeit, Gender, Diversität oder feministischer Perspektive. Healthcare-Kontext ist nicht gleichzusetzen mit Sozialer Arbeit. Paper adressiert allgemeine KI-Ethik und Bias im Healthcare-Kontext, nicht die definierten SOZIAL-Kategorien mit substantiellem Fokus auf marginalisierte Gruppen oder sozialarbeiterische Praxis.",Konzept,0.75,"Das Paper behandelt LLMs, Prompting und Bias-Mitigation substantiell (TECHNIK erfüllt). Es adressiert Fairness und Bias-Ungleichheit. Jedoch ist Healthcare-Kontext nicht identisch mit Sozialer Arbeit, und es gibt keinen expliziten Fokus auf marginalisierte Gruppen oder strukturelle Ungleichheit im Sozialbereich. Daher nur Exclude, da die definierten SOZIAL-Kategorien nicht ausreichend substantiell erfüllt sind."
174,UIIDCXLB,Kamruzzaman & Kim (2024),Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Empirisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, Prompting=Ja) und SOZIAL_OK (Bias_Ungleichheit=Ja, Fairness=Ja). Substantieller Fokus auf Prompt-Engineering-Strategien zur Bias-Reduktion in LLMs. Direkte Behandlung sozialer Bias und Fairness-Ansätze. Nicht auf Soziale Arbeit fokussiert, aber klar KI-Bias-Thematik mit fairness-Implikation."
175,ZHQMHHPQ,Park & Yoon (2025),"AI algorithm transparency, pipelines for trust not prisms: mitigating general negative attitudes and enhancing trust toward AI",Ja,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Ja,Exclude,"TECHNIK erfüllt (AI_Literacies, Prompting, KI_Sonstige, Fairness), aber SOZIAL nicht erfüllt: Kein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder Feminismus. Paper behandelt algorithmische Transparenz und Vertrauen in allgemeinem organisatorischen Kontext, nicht in sozialarbeiterischer Praxis oder mit explizitem Fokus auf marginalisierte Gruppen/strukturelle Ungleichheit.",Experimentell,0.85,"Paper adressiert AI-Vertrauen, Transparenz und Fairness-Aspekte (TECHNIK-Seite erfüllt). Jedoch fehlt der Sozialarbeitsbezug und kein expliziter Fokus auf Ungleichheit oder Marginalisierung. Allgemeine organisationale Trust-Studie ohne Sozialbereich-Spezifika."
176,NUYCHW2T,Xu et al. (2023),Transparency enhances positive perceptions of social artificial intelligence,Ja,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,SOZIAL-Kriterium nicht erfüllt,Experimentell,0.92,"Paper erfüllt TECHNIK-Kriterium (AI_Literacies: Ja - Nutzer-Kompetenzen/Wissen über KI; Generative_KI: Ja - Chatbot-fokussiert; Prompting: Ja - Transparenz-Strategien für Prompt-Engineering erwähnt). Aber SOZIAL-Kriterium nicht erfüllt: Kein direkter Bezug zu Sozialer Arbeit, keine substantielle Behandlung von Bias, Gender, Diversität, Feminismus oder Fairness. Allgemeine Trust/UX-Studie ohne Fokus auf soziale Ungleichheit oder marginalisierte Gruppen."
177,7AS5MAU9,Srinivasan & Thomason (2025),Mitigating trust-induced inappropriate reliance on AI assistance,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Ja,Exclude,No_social_work_relevance,Empirisch,0.85,"Paper behandelt Trust-Management in AI-Systemen und Fairness durch adaptive Interventionen (TECHNIK: AI_Literacies, KI_Sonstige, Fairness erfüllt). Es befasst sich jedoch nicht mit sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder feministischen Perspektiven (SOZIAL nicht erfüllt). Fokus liegt auf allgemeiner Human-AI Collaboration ohne direkten Bezug zu sozialarbeiterischen Kontexten."
178,L48P8FBG,Goldkind et al. (2024),Artificial intelligence in social work: An EPIC model for practice,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Konzept,0.92,"Paper erfüllt beide Inklusionskriterien: TECHNIK (AI_Literacies, KI_Sonstige) durch EPIC-Framework für KI-Integration und Bias-Mitigation; SOZIAL (Soziale_Arbeit, Bias_Ungleichheit, Fairness) durch direkten Bezug zu Sozialer Arbeit, Ethik, Transparenz und Fairness-Thematisierung. Substantielle Behandlung von KI-Kompetenzen in Sozialarbeit und strukturierter Fairness-Ansatz."
179,U9ACKGB4,Bisconti et al. (2024),A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Theoretisch,0.85,"Das Paper behandelt AI trustworthiness aus theoretischer, KI-ethischer Perspektive (KI_Sonstige: Ja). Es bietet jedoch keinen Bezug zu sozialen Anwendungsdomänen, Bias, Gender, Diversität, Fairness oder Soziale Arbeit. Es ist eine rein konzeptionelle Formalisierung von Vertrauen in KI-Systemen ohne direkten Bezug zu sozialen Implikationen oder marginalizierten Gruppen. SOZIAL-Komponente nicht erfüllt → Exclude."
180,2YS85B49,Debnath et al. (2024),Can LLMs reason about trust? A pilot study,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Experimentell,0.85,"Paper untersucht LLM-Fähigkeiten zur Vertrauensanalyse zwischen Individuen. Während Generative_KI (LLMs) relevant ist, fehlt jeglicher substantieller Bezug zu den sozialen Kategorien: kein direkter Sozialarbeitsbezug, keine Bias/Ungleichheit-Analyse, kein Gender-, Diversitäts- oder Fairness-Fokus. Die Vertrauensforschung ist allgemein sozialwissenschaftlich, nicht sozialarbeiterisch spezifisch. TECHNIK_OK, aber SOZIAL_NICHT_OK → Exclude."
181,Z4YXX9PZ,Kamruzzaman & Kim (2024),Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Empirisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting), SOZIAL_OK (Bias_Ungleichheit + Fairness). Substantieller Fokus auf Prompt-Engineering-Strategien zur Bias-Reduktion in LLMs mit empirischer Evaluation. Adressiert soziale Bias und Fairness-Aspekte von KI-Systemen, nicht aber Soziale Arbeit oder feministische Perspektiven."
182,GUMWKBN6,Asseri et al. (2024),Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Literaturreview,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, Prompting=Ja: systematische Analyse von Prompt-Engineering-Techniken für LLMs). SOZIAL_OK (Bias_Ungleichheit=Ja: kultureller Bias gegen Araber und Muslime; Diversitaet=Ja: kulturelle Repräsentation; Fairness=Ja: Bias-Reduktion und Fairness-Metriken). Substantieller Fokus auf Prompt-Strategien zur Bias-Mitigation in LLMs."
183,U3AJIXAJ,Victor et al. (2023),Recommendations for social work researchers and journal editors on the use of generative AI and large language models,Ja,Ja,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Include,,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, Prompting=Ja, AI_Literacies=Ja durch Fokus auf Wissen/Kompetenzen für Forschende) und SOZIAL_OK (Soziale_Arbeit=Ja durch expliziten Bezug zu Social-Work-Forschung und -Praxis). Entwickelt Framework für KI-Einsatz in Sozialarbeit mit Empfehlungen für Transparenz, Dokumentation und kontinuierliche Bildung."
184,HN7KKNYV,Reamer (2023),Artificial intelligence in social work: Emerging ethical issues,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Nein,Ja,Include,,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies + KI_Sonstige: Bezug zu ethischen Rahmenbedingungen und Implementierung von KI-Systemen in sozialer Arbeit) und SOZIAL_OK (Soziale_Arbeit + Bias_Ungleichheit + Fairness: Direkter Bezug zu Sozialarbeitspraxis, explizite Behandlung von algorithmischen Bias und Fairness-Anforderungen). Das Paper entwickelt systematisch ethische Richtlinien und einen Implementierungsrahmen für KI in der Sozialen Arbeit."
185,UB9NK8KI,Zayed (2024),Scaling implicit bias analysis across transformer-based language models through embedding association test and prompt engineering,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Exclude,"Technik erfüllt (Generative_KI, Prompting, Bias_Ungleichheit, Fairness), aber Sozial-Dimension nicht erfüllt. Paper behandelt Bias in LLMs ohne direkten Bezug zu Sozialer Arbeit, struktureller Ungleichheit oder marginalisierten Gruppen im sozialen Kontext.",Empirisch,0.85,"Paper adressiert impliziten Bias in Transformer-Sprachmodellen mittels Embedding Association Test und Prompt Engineering. Dies deckt Generative_KI, Prompting und Bias ab. Jedoch fehlt der Sozial-Kontext: Keine Bezüge zu Sozialer Arbeit, struktureller Benachteiligung oder konkreten sozialen Auswirkungen. Bias-Analyse ohne Kontextualisierung sozialer Implikationen erfüllt nicht die SOZIAL-Kriterien."
186,XG7RFFC7,U.S. Bureau of Labor Statistics (2023),Occupational employment statistics,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Unclear,0.95,Dies ist ein Statistikbericht des U.S. Bureau of Labor Statistics über Beschäftigungsstatistiken nach Beruf. Es behandelt weder KI-Themen (TECHNIK-Kriterien nicht erfüllt) noch soziale Gerechtigkeit/Bias-Probleme im KI-Kontext (SOZIAL-Kriterien nicht erfüllt). Kein Bezug zu KI-Systemen oder deren Auswirkungen erkennbar.
187,JRG3B3LE,Struppek (2024),Homoglyph unlearning: A novel approach to bias mitigation,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Unclear,0.65,"Das Paper behandelt 'Homoglyph unlearning' als Bias-Mitigationsmethode (KI_Sonstige: Ja, technischer Fokus). Der Titel signalisiert explizit Bias Mitigation (Bias_Ungleichheit + Fairness: Ja). Allerdings fehlt das Abstract für vollständige Bewertung. TECHNIK_OK (KI_Sonstige) + SOZIAL_OK (Bias/Fairness) → Include. Confidence moderat wegen fehlender Kontextinformationen."
188,JGZDWMN3,Wu (2025),Bias in decision-making for AI's ethical dilemmas: A comparative study of ChatGPT and Claude,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Exclude,No_full_text,Unclear,0.3,"Titel deutet auf Generative KI und Bias hin, aber ohne Abstract oder Volltext ist die Einordnung spekulativ. Sicher sind: Generative_KI=Ja (ChatGPT/Claude), Bias_Ungleichheit=Ja (Titel erwähnt 'Bias'). ABER: Kein Bezug zu Sozialer Arbeit erkennbar. Die Themen 'ethical dilemmas' sind philosophisch, nicht sozialarbeitsbezogen. Daher TECHNIK_OK (Generative_KI), aber SOZIAL nicht erfüllt (Bias allein ohne Sozialarbeitsbezug reicht nicht, und ohne Text kann nicht überprüft werden, ob tatsächlicher Fokus auf Ungleichheit besteht). Fehlende vollständige Information → Exclusion."
189,NVZA58ML,Wang (2024),A survey on fairness in large language models,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Exclude,Not_relevant_topic,Literaturreview,0.85,"Paper behandelt Fairness in Large Language Models (Generative_KI: Ja, Fairness: Ja, Bias_Ungleichheit: Ja). Jedoch fehlt der erforderliche SOZIAL-Kontext: Es gibt keinen direkten Bezug zu Sozialer Arbeit, Gender-Perspektive, Diversität oder feministischen Ansätzen. Allgemeine Fairness-Diskussionen in KI ohne Sozialarbeitsbezug sind nicht relevant für das Literature Review zu KI und Sozialer Arbeit. TECHNIK-Bedingung erfüllt, aber SOZIAL-Bedingung nicht erfüllt → Exclude."
190,Z9DNTBFF,Srinivasan & Bisk (2024),Worst of both worlds: A comparative analysis of error in language and vision-language models,Nein,Ja,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Exclude,"No_full_text - Kein Abstract verfügbar. Trotz erkennbarem KI-Fokus (Generative_KI, KI_Sonstige, Bias_Ungleichheit) kann ohne Textinhalt nicht verifiziert werden, ob eine substantielle Sozialbezug vorliegt. Die verfügbaren Informationen deuten auf eine rein technische Fehleranalyse hin.",Unclear,0.4,"Der Titel deutet auf eine technische Vergleichsstudie zu Sprachmodellen und Vision-Language-Modellen hin. Generative_KI und KI_Sonstige sind relevant; 'error' könnte auf Bias hinweisen. Allerdings: Ohne Abstract/Volltext kann nicht eingeschätzt werden, ob ein Sozialkontext (Soziale_Arbeit, Bias_Ungleichheit mit sozialem Fokus) substantiell adressiert wird. Reine technische Fehleranalyse würde die SOZIAL-Bedingung nicht erfüllen."
191,W8DFWR9L,Shin et al. (2025),Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Unclear,0.72,"Das Paper behandelt Bias-Mitigation in LLMs mit Fokus auf altersbedingte Diskriminierung. Generative_KI (LLMs), Bias_Ungleichheit (Age Bias), Diversitaet (Age-related groups), und Fairness (Debiasing-Strategien) sind substantiell adressiert. TECHNIK_OK (Generative_KI=Ja) und SOZIAL_OK (Bias_Ungleichheit=Ja, Diversitaet=Ja, Fairness=Ja) erfüllt. Ohne Abstract ist Zuordnung unsicher, aber Titel deutet auf relevantes Thema hin."
192,M7AGB7LI,Salecha & Srijith (2025),Model explanations for gender and ethnicity bias mitigation in AI-generated narratives,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Include,,Unclear,0.75,"Das Paper adressiert explizit Bias-Mitigation in KI-generierten Narrativen mit Fokus auf Gender und Ethnizität. Generative_KI (narratives generation), Bias_Ungleichheit (gender and ethnicity bias), Gender (expliziter Gender-Fokus), Diversitaet (ethnicity representation) und Fairness (bias mitigation) sind substantiell behandelt. TECHNIK (Generative_KI) und SOZIAL (Bias_Ungleichheit, Gender, Fairness) sind erfüllt → Include. Confidence begrenzt durch fehlenden Abstract und unklaren Studientyp."
193,6MJYP7ZX,Prakash & Lee (2023),Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Literaturreview,0.85,"Das Paper behandelt substantiell Prompting-Techniken (TECHNIK) zur Bias-Mitigation gegen Arabs/Muslims in LLMs (SOZIAL). Erfüllt beide Bedingungen: Generative_KI + Prompting (TECHNIK) und Bias_Ungleichheit + Diversitaet + Fairness (SOZIAL). Kein expliziter Soziale_Arbeit-Bezug, aber kulturelle Diskriminierung und algorithmische Fairness sind relevant für Inklusion und soziale Gerechtigkeit."
194,YUVR5YNQ,Parrish & de-Arteaga (2025),Self-debiasing large language models: Zero-shot recognition and reduction of stereotypes,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Exclude,"Technik erfüllt (Generative_KI, Prompting), aber Sozial nicht erfüllt: Paper adressiert Bias und Fairness auf technischer Ebene, nicht im Kontext Sozialer Arbeit, sozialer Ungleichheit strukturell oder feministischer Perspektiven.",Empirisch,0.85,"Das Paper behandelt deutlich Generative KI (LLMs) und Prompting-Strategien zur Bias-Reduktion (Self-Debiasing). Es adressiert auch Fairness und Bias. Jedoch: Keine direkten sozialen Aspekte im Sinne der Review-Kriterien (Soziale_Arbeit, strukturelle Ungleichheit, Gender-Fokus, Diversität, oder feministische Perspektive). Es bleibt eine technisch-methodische Studie ohne sozialarbeiterischen oder strukturellen sozialen Kontext."
195,YPYQ2TCL,Mei et al. (2023),Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Include,,Empirisch,0.85,Das Paper untersucht Bias in GPT (Generative_KI: Ja) durch eine intersektionale Fallstudie (Feministisch: Ja - Intersektionalitätsansatz nach Crenshaw). Es thematisiert Diskriminierung gegen stigmatisierte Gruppen (Bias_Ungleichheit: Ja) und Diversität von Gruppen (Diversitaet: Ja). Fairness ist impliziert durch Bias-Analyse. TECHNIK und SOZIAL sind erfüllt → Include.
196,64DQYVVB,Liu et al. (2025),More or less wrong: A benchmark for directional bias in LLM comparative reasoning,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Exclude,Not_relevant_topic,Experimentell,0.85,"Paper behandelt Bias und Fairness in LLMs durch ein Benchmark für direktionalen Bias. Dies erfüllt TECHNIK-Bedingung (Generative_KI, Bias_Ungleichheit, Fairness). JEDOCH: Keine Verbindung zu Soziale_Arbeit erkennbar. Das Paper fokussiert auf technische Bias-Metriken in LLMs, nicht auf sozialarbeiterische Praxis oder deren Zielgruppen. Reine KI-Fairness-Technologie ohne sozialen Anwendungskontext → Exclude."
197,RAY6G2R7,Lin et al. (2024),SWIFTSAGE: A new dual-module framework for better action planning in complex interactive reasoning tasks,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,"No_full_text; Titel deutet auf technisches Framework für LLM-basierte Systeme hin, ohne erkennbaren Bezug zu sozialen/arbeitsspezifischen Kontexten. Keine Abstract verfügbar für detaillierte Bewertung, aber basierend auf Titel: TECHNIK_OK (Generative_KI + Prompting), aber SOZIAL_NOT_OK (keine Kategorien erfüllt).",Unclear,0.4,"Paper adressiert offenbar generative KI und Prompting-Strategien (Dual-Module Framework für Action Planning mit LLMs). Jedoch: Kein Abstract, fehlender Volltext. Titel suggeriert rein technisches ML-Framework ohne erkennbaren Bezug zu Sozialer Arbeit, Fairness, Bias oder verwandten sozialen Dimensionen. Keine der SOZIAL-Kategorien erfüllt."
198,QY6P4RGQ,Parrish et al. (2022),BBQ: A hand-built bias benchmark for question answering,Nein,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Nein,Ja,Include,,Experimentell,0.85,"BBQ ist ein etablierter Benchmark für Bias-Evaluationen in QA-Systemen (KI-Sonstige: Ja; Generative_KI: Ja, da oft für LLM-Evaluation genutzt). Das Paper adressiert substantiell Bias, Fairness und Diversität mit explizitem Gender-Fokus. Keine Soziale_Arbeit. Technik+Sozial erfüllt → Include."
199,H3STST88,Kojima et al. (2022),Large language models are zero-shot reasoners,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Experimentell,0.95,"Paper behandelt Chain-of-Thought Prompting für LLMs (Generative_KI=Ja, Prompting=Ja). Dies ist eine rein technische Methoden-Studie ohne Bezug zu sozialen Themen. Keine Kategorien aus dem SOZIAL-Set (Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness) sind erfüllt. Damit fehlt die SOZIAL_OK-Bedingung für Inklusion."
200,RM833D5N,He et al. (2024),On the steerability of large language models,Nein,Ja,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Unclear,0.6,"Paper behandelt Steerability von LLMs (Generative_KI: Ja, Prompting: Ja). Dies sind Technik-Kategorien. Jedoch fehlt jeglicher Bezug zu sozialen Aspekten (Bias, Fairness, Soziale Arbeit, etc.). Ohne Abstract kann keine substantielle Einschätzung des Inhalts erfolgen, aber auf Basis des Titels liegt ein rein technisches Paper zu LLM-Kontrollierbarkeit vor. TECHNIK_OK, aber SOZIAL nicht erfüllt → Exclude."
201,ACDF4FL9,Chisca et al. (2024),Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Empirisch,0.85,"Paper fokussiert auf Prompting-Techniken (substantiell im Titel) und Generative KI (LLMs). Adressiert social bias reduction - direkt relevant für Bias_Ungleichheit und Fairness. TECHNIK erfüllt (Generative_KI + Prompting), SOZIAL erfüllt (Bias_Ungleichheit + Fairness). Kein Soziale_Arbeit-Bezug, aber algorithmische Bias-Reduktion ist relevant für LLM-Fairness insgesamt."
202,VEJBIZRR,Chen et al. (2024),Exploring complex mental health symptoms via classifying social media data with explainable LLMs,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,No_full_text; nur Titel verfügbar - Klassifikation unter Unsicherheit,Unclear,0.35,"Der Titel deutet auf LLM-basierte Klassifikation von Mentalhygiene-Daten hin (Generative_KI + KI_Sonstige erfüllt, TECHNIK_OK). Jedoch: kein Abstract verfügbar, daher nicht erkennbar, ob ein substantieller Bezug zu sozialen/ethischen Dimensionen (SOZIAL_OK) besteht. Mental Health ist nicht automatisch Soziale Arbeit. Ohne Volltext: konservative Einstufung, da SOZIAL-Dimensionen nicht verifiable sind."
203,QIQR449A,Klinge & Kjeldsen (2024),A sociolinguistic approach to stereotype assessment in large language models,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Nein,Nein,Nein,Include,,Empirisch,0.75,Paper analysiert Stereotypen in LLMs aus soziolinguistischer Perspektive. Erfüllt TECHNIK-Bedingung durch Generative_KI (Fokus auf LLMs). Erfüllt SOZIAL-Bedingung durch Bias_Ungleichheit (Stereotype-Assessment) und Gender (implizit durch Stereotype-Analyse). Sociolinguistischer Zugang deutet auf Analyse von Sprachmustern und sozialen Kategorien hin. Ohne Abstract: Confidence moderate.
204,55ZR64VU,Kaneko & Bollegala (2024),Debiasing prompts for gender bias in large language models,Nein,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Exclude,"SOZIAL-Bedingung nicht erfüllt: Paper adressiert Bias und Gender in generativen KI-Systemen, hat aber keinen Bezug zu Sozialer Arbeit, marginalisierten Communities oder strukturellen Ungleichheiten im sozialen Kontext. Es ist eine rein technische Studie zu Fairness in LLMs ohne sozialpolitische/sozialarbeiterische Dimension.",Experimentell,0.92,"Paper behandelt Prompting-Strategien zur Reduktion von Gender-Bias in LLMs—klare technische Komponente mit Fairness-Fokus. Allerdings fehlt jeder Bezug zu Sozialer Arbeit, Zielgruppen sozialer Dienste oder strukturellen sozialen Implikationen. Es ist eine Standard-Debiasing-Studie ohne sozialarbeitlichen Kontext."
205,7IVS7X63,Jiang et al. (2022),Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Include,,Empirisch,0.85,"Das Paper untersucht systematisch Bias in GPT gegenüber stigmatisierten Gruppen (Generative_KI: Ja). Der Fokus auf Nationalität und psychische Erkrankungen adressiert Bias und Ungleichheit (Bias_Ungleichheit: Ja). Der explizit erwähnte intersektionale Ansatz deutet auf feministische Methodik hin (Feministisch: Ja, intersektional nach Crenshaw). Die Analyse marginalisierter Gruppen und deren Repräsentation (Diversitaet: Ja) sowie Fairness-Bezug (Fairness: Ja) sind substantiell. TECHNIK (Generative_KI) ✓ + SOZIAL (Bias_Ungleichheit, Diversitaet, Feministisch, Fairness) ✓ = Include."
206,ST9UCTJE,Garg et al. (2019),Counterfactual fairness in text classification through robustness,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Theoretisch,0.75,"Paper adressiert Counterfactual Fairness in Text Classification - ein KI/ML-Thema (KI_Sonstige: Ja). Fokus auf Fairness und Robustheit deutet auf Bias-Mitigation hin (Fairness: Ja, Bias_Ungleichheit: Ja). TECHNIK und SOZIAL sind erfüllt. Ohne Abstract ist Confidence begrenzt, aber Titel und Autorschaft (Garg et al., bekannt für Fairness in ML) stützen Klassifizierung."
207,SQYLQFRU,Furniturewala et al. (2024),Reasoning towards fairness: Mitigating bias in language models through reasoning-guided fine-tuning,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Experimentell,0.85,"Das Paper behandelt Sprachmodelle (Generative_KI: Ja) mit explizitem Fokus auf Bias-Mitigation und Fairness (Bias_Ungleichheit: Ja, Fairness: Ja). Der Titel signalisiert einen technischen Ansatz zur Reduktion von Bias durch Fine-Tuning. TECHNIK_OK (Generative_KI) und SOZIAL_OK (Fairness + Bias_Ungleichheit erfüllt). Include-Kriterien sind erfüllt, trotz fehlenden Sozialen-Arbeit-Bezugs."
208,KG8JRLRQ,Dixon et al. (2018),Measuring and mitigating unintended bias in text data,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Empirisch,0.75,"Paper behandelt Bias-Mitigation und Fairness in ML-Systemen (KI_Sonstige: Ja, Bias_Ungleichheit: Ja, Fairness: Ja). Dies ist ein etabliertes Paper zur algorithmischen Fairness. TECHNIK-Bedingung erfüllt (KI_Sonstige), SOZIAL-Bedingung erfüllt (Bias_Ungleichheit + Fairness). Kein Sozialarbeitsbezug erkannt, aber Fairness-Fokus ist substantiell genug für Inclusion im breiter definierten KI-Ethik-Kontext."
209,GCN42PAM,Birru et al. (2024),Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Unclear,0.75,"Das Paper behandelt Large Language Models (Generative_KI=Ja) und fokussiert auf Bias-Mitigation bezüglich Altersmerkmalen (Bias_Ungleichheit=Ja, Diversitaet=Ja, Fairness=Ja). Titel und Fokus auf 'responsible AI development' und 'mitigating bias' sind substantiell. TECHNIK_OK (Generative_KI) + SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Fairness) = Include. Kein direkter Bezug zu Sozialer Arbeit identifizierbar, aber Bias/Fairness-Fokus ausreichend für Inklusion."
210,CYZQ6XPK,Kaneko et al. (2024),Evaluating gender bias in large language models via chain-of-thought prompting,Nein,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Include,,Experimentell,0.95,"Paper behandelt substantiell Generative KI (LLMs), Prompting (Chain-of-Thought als zentrale Intervention), Gender-Bias in LLM-Outputs und Fairness-Aspekte (Bias-Reduktion). Erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting) und SOZIAL_OK (Bias_Ungleichheit + Gender + Fairness). Experimentelle Evaluationsstudie mit klarem Fokus auf Prompt-Strategien zur Bias-Mitigation."
211,EHQBHVYV,Asseri et al. (2025),Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Literaturreview,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting substantiell behandelt), SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Fairness adressiert). Systematische Review zu Prompt-Engineering-Techniken für Bias-Mitigation gegen Arabs/Muslims in LLMs mit quantifizierten Ergebnissen. Fokus auf kulturelle Repräsentation und algorithmische Fairness. Kein direkter Sozialarbeitsbezug, aber relevantes Anwendungsfeld für diskriminierungssensible KI-Nutzung."
212,22KJL3PC,Yuan et al. (2025),The cultural stereotype and cultural bias of ChatGPT,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Empirisch,0.92,"Paper erfüllt TECHNIK-Kriterium: Fokus auf Generative KI (ChatGPT-3.5/GPT-4), substantielle Analyse von Prompting-Strategien zur Bias-Reduktion. Erfüllt SOZIAL-Kriterium: Explizite Behandlung von Bias (kulturelle Stereotype), Diversität (diversity-sensitive prompts) und Fairness (Bias-Mitigation-Strategien). Empirische Studie mit drei experimentellen Phasen."
213,RARE5UFC,Kamruzzaman & Kim (2024),Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Experimentell,0.92,"Paper behandelt systematisch Prompting-Strategien (Ja: substantieller Fokus auf 12 Prompt-Varianten) zur Bias-Reduktion in LLMs (Ja: Generative_KI). Bias-Reduktion und Fairness (Ja: beide Kategorien) sind zentrale Themen. TECHNIK-Bedingung erfüllt (Prompting + Generative_KI), SOZIAL-Bedingung erfüllt (Bias_Ungleichheit + Fairness). Kein Bezug zu Sozialer Arbeit oder Gender-spezifisch, kein feministischer Rahmen."
214,YMABYPKF,Salinas et al. (2025),What’s in a name? Auditing large language models for race and gender bias,Nein,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Ja,Ja,Include,,Empirisch,0.95,"Das Paper adressiert Generative KI (GPT-4, LLMs) mit Fokus auf Prompting (42 Prompt-Templates als Audit-Instrument). Es thematisiert explizit intersektionalen Bias (Rasse × Geschlecht nach Crenshaw-Logik), Gender-Bias und Fairness. Die intersektionale Analyse erfüllt den feministischen Kriterium. TECHNIK_OK (Generative_KI + Prompting), SOZIAL_OK (Bias_Ungleichheit + Gender + Feministisch + Fairness). Einschluss gerechtfertigt."
215,ZLMLP53P,Cvoelcker (2023),Queer in AI: A case study in community-led participatory AI,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Empirisch,0.85,"Paper behandelt algorithmische Systeme und deren Schäden (KI_Sonstige=Ja). Community-geleiteter Ansatz zur fairen KI adressiert Fairness, Bias/Ungleichheit und Diversität. Queer-Fokus involviert Gender-Perspektive und intersektionale Analysen (feministisch). Erfüllt beide Bedingungen: Technik (KI_Sonstige) + Sozial (Bias, Gender, Diversität, Fairness, Feministisch)."
216,4LY3SA4E,Charlesworth (2024),Flexible intersectional stereotype extraction (FISE): Analyzing intersectional biases in large language models,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Empirisch,0.95,"Paper analysiert LLM-Biases mit FISE-Methode (Generative_KI: Ja). Fokus auf intersektionale Repräsentationsverzerrungen erfüllt Bias_Ungleichheit, Gender (Geschlechterstereotype), Diversitaet (Repräsentation). Intersektionale Analyse nach Crenshaw ist explizit feministisch (Feministisch: Ja). Messung von Biases adressiert Fairness. TECHNIK (Generative_KI) + SOZIAL (alle 4 Kategorien) → Include."
217,TSYJ3Y57,UNESCO (2024),Women4Ethical AI: Global cooperation for gender-inclusive AI,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Nein,Ja,Include,,Konzept,0.85,"Paper erfüllt TECHNIK-Kriterien (AI_Literacies für KI-Kompetenzen/Beteiligung; KI_Sonstige für allgemeine KI-Entwicklung; Fairness für genderinklusive/faire KI-Systeme) und SOZIAL-Kriterien (Gender explizit im Titel; Bias_Ungleichheit durch Menschenrechtsfokus; Diversitaet durch globale Kooperation und Expertinnenbeteiligung marginalisierter Gruppen). Feministisch=Nein, da keine explizite feministische Theorie erkennbar ist."
218,A776TPGG,Friedrich-Ebert-Stiftung (2025),The EU artificial intelligence act through a gender lens,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Nein,Nein,Ja,Include,,Politikanalyse,0.75,"Paper analysiert EU AI Act durch Gender-Lens: KI_Sonstige (algorithmische Systeme unter Regulierung) + Gender (expliziter Geschlechterfokus) + Bias_Ungleichheit (Geschlechtergerechtigkeit) + Fairness (Regulierungsanalyse). Beide Bedingungen erfüllt: TECHNIK_OK (KI_Sonstige), SOZIAL_OK (Gender + Bias_Ungleichheit + Fairness)."
219,MTMU9UPJ,Qiu et al. (2025),DR.GAP: Mitigating bias in large language models using gender-aware prompting with demonstration and reasoning,Nein,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Exclude,Not_relevant_topic,Experimentell,0.85,"Paper erfüllt TECHNIK-Bedingung (Generative_KI, Prompting). Jedoch fehlt SOZIAL-Bedingung für Anwendungsgebiet mit sozialem Fokus: Bias_Ungleichheit, Gender und Fairness behandeln allgemeine LLM-Bias, nicht spezifisch Soziale Arbeit, marginalisierte Communities oder sozialarbeiterische Kontexte. Reine KI-Ethik-Studie ohne Sozialarbeitsbezug → Exclude für dieses Review."
220,CSJS9JGH,Latif et al. (2023),"AI gender bias, disparities, and fairness: Does training data matter?",Nein,Ja,Nein,Ja,Nein,Ja,Ja,Nein,Nein,Ja,Include,,Empirisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI: GPT-3.5, KI_Sonstige: BERT, Fairness-Metriken) und SOZIAL_OK (Gender-Bias explizit im Titel, Bias_Ungleichheit als Analysefokus, Fairness-Metriken substantiell behandelt). Empirische Studie zu Gender-Bias in KI-Systemen mit direktem Fairness-Bezug."
221,BR2LG8LD,Kumar & Gartner (2024),How AI hype impacts the LGBTQ+ community,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Theoretisch,0.92,"Paper behandelt KI-Sonstige (Computer Vision, Content-Moderation, Algorithmen) und adressiert multiple soziale Dimensionen: Bias_Ungleichheit (algorithmische Marginalisierung queerer Identitäten), Gender (Geschlechtsklassifikation, heteronormative Annahmen), Diversitaet (queere Communities), Feministisch (kritische Analyse von Machtstrukturen und Marginalisierung) und Fairness (algorithmische Diskriminierung). TECHNIK_OK und SOZIAL_OK → Include."
222,5G2QTZYD,Ahmed (2024),Feminist perspectives on AI: Ethical considerations in algorithmic decision-making,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Theoretisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja; algorithmische Entscheidungssysteme), SOZIAL_OK (Bias_Ungleichheit=Ja, Gender=Ja, Diversitaet=Ja, Feministisch=Ja). Explizit feministische Perspektive mit intersektionaler Analyse und Fokus auf strukturelle Diskriminierung. Kein Bezug zu Sozialer Arbeit, daher diese=Nein."
223,6QPLNNQK,UNESCO (2024),Bias against women and girls in large language models: A UNESCO study,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Include,,Empirisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, LLM-Analyse), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversität, Fairness alle Ja). Dokumentiert Gender-Stereotypen und algorithmischen Bias in LLMs systematisch. Kein explizit feministischer Theorie-Einsatz, daher Feministisch=Nein. Keine Soziale_Arbeit-Dimension erkennbar."
224,WS4KQPWN,Ulnicane (2024),Intersectionality in artificial intelligence: Framing concerns and recommendations for action,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Literaturreview,0.92,"Paper erfüllt beide Bedingungen: TECHNIK (KI_Sonstige: algorithmische Systeme in Robotik, HR-Tools) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch: Intersektionalität ist etablierter feministischer Forschungsrahmen nach Crenshaw; Fairness). Substantielle Behandlung von Vorurteilen in KI-Systemen und struktureller Benachteiligung."
225,QD2CXE9I,An & et al. (2025),Measuring gender and racial biases in large language models,Nein,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Empirisch,0.95,Paper behandelt generative KI (LLMs) mit substantiellem Fokus auf intersektionale Gender-Rassen-Biases. Explizite Bezüge zu Intersektionalitätstheorie (feministische Perspektive nach Crenshaw). Prompting wird als Interventionsstrategie adressiert. Erfüllt beide Bedingungen: TECHNIK (Generative_KI + Prompting) und SOZIAL (Bias_Ungleichheit + Gender + Diversitaet + Feministisch + Fairness).
226,HMDFMBV3,Ma et al. (2023),Intersectional Stereotypes in Large Language Models: Dataset and Analysis,Nein,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Empirisch,0.95,"Paper behandelt substantiell Generative KI (LLMs) und Prompting (Prompt Engineering zur Bias-Reduktion). SOZIAL-Kriterien erfüllt: (1) Bias_Ungleichheit (intersektionale Stereotypes analyse), (2) Gender (Geschlechter-Stereotypen in LLMs), (3) Diversitaet (multiple Attribute/Gruppen), (4) Feministisch (intersektionale Analyse nach Crenshaw-Tradition). TECHNIK+SOZIAL erfüllt → Include."
227,5UAHQESQ,Basseri et al. (2025),Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Literaturreview,0.95,Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + Prompting substantiell fokussiert) und SOZIAL (Bias_Ungleichheit + Diversitaet + Fairness adressieren kulturelle und intersektionale Diskriminierung gegen Araber und Muslime). Systematischer Review von Prompt-Engineering zur Bias-Mitigation ist zentral relevant für KI-Ethik im Sozialkontext.
228,UDZLIJWX,Yunusov et al. (2024),MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models,Nein,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Include,,Empirisch,0.85,"Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + Prompting substantiell behandelt) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Fairness). Fokus auf LLM-generierte Stories mit explizitem Prompting für Diversität. Empirische Analyse von Bias und Engagement-Disparitäten nach Identitätsmerkmalen. Kein direkter Sozialarbeitsbezug, daher kein Soziale_Arbeit=Ja."
229,6BZ5353S,Wang et al. (2025),Multilingual Prompting for Improving LLM Generation Diversity,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Experimentell,0.85,"Das Paper adressiert TECHNIK-Aspekte durch Generative_KI (LLMs) und substantiell Prompting (multilingual prompting als zentrale Strategie). SOZIAL-Aspekte sind erfüllt durch Bias_Ungleichheit (kulturelle Halluzinationen, demographische Bias), Diversitaet (narrative und demographische Diversität) und Fairness (Reduktion von Halluzinationen/Bias). Einschlusskriterien erfüllt."
230,Z9BIKVS3,Laine & McCrory (2025),Avoiding Catastrophe Through Intersectionality in Global AI Governance,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Konzept,0.95,"Paper behandelt KI-Governance (KI_Sonstige: Ja) mit explizit feministischer Theorie (intersektionale Analyse nach Crenshaw-Tradition, Feministisch: Ja). Adressiert strukturelle Ungleichheiten, Bias und Gerechtigkeit (Bias_Ungleichheit, Gender, Diversitaet, Fairness alle Ja). Erfüllt beide Bedingungen: TECHNIK (KI_Sonstige) UND SOZIAL (Feministisch + multiple Dimensionen). Theoretischer Policy-Rahmen mit intersektionalem Fokus."
231,SSH3LVN6,Ulnicane (2024),Artificial Intelligence and Intersectionality,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Theoretisch,0.95,"TECHNIK_OK: KI_Sonstige=Ja (algorithmische Systeme und Bias-Analyse). SOZIAL_OK: Bias_Ungleichheit=Ja, Gender=Ja, Diversitaet=Ja, Feministisch=Ja (explizite intersektionale Perspektive nach Crenshaw), Fairness=Ja. Alle Bedingungen erfüllt. Paper nutzt feministische/intersektionale Theorie substantiell zur Analyse von KI-Bias."
232,QZJL6KBZ,Lund (2025),"Algorithms, artificial intelligence and discrimination",Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Konzept,0.85,"Der Bericht adressiert algorithmische Diskriminierung (KI_Sonstige: Ja) und behandelt substantiell Bias/Ungleichheit sowie Fairness im Kontext von Antidiskriminierungsrecht. Er erfüllt beide Bedingungen (TECHNIK + SOZIAL) und ist damit included, obwohl er keinen direkten Soziale-Arbeit-Bezug hat."
233,UBYTNGNV,Giannoni Adielsson & Öberg (2024),"The AI Act, gender equality and non-discrimination: what role for the AI Office?",Nein,Nein,Nein,Ja,Nein,Ja,Ja,Nein,Nein,Ja,Include,,Theoretisch,0.92,"Paper analysiert EU AI Act durch die Linse von Geschlechtergerechtigkeit und Nichtdiskriminierung. TECHNIK erfüllt: KI_Sonstige (algorithmische Systeme reguliert durch AI Act). SOZIAL erfüllt: Bias_Ungleichheit (Geschlechterverzerrungen, Diskriminierungsrisiken), Gender (expliziter Gender-Fokus), Fairness (Bias-Audits, grundrechtliche Folgenabschätzungen). Keine Soziale_Arbeit, kein expliziter feministischer Theorierahmen erkennbar."
234,RZ4QFXQI,West et al. (2023),"Discriminating Systems: Gender, Race, and Power in AI",Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Bias in KI-Systemen), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch als intersektionale Kritik an Machtverhältnissen, Fairness). Kritische Analyse von algorithmischen Systemen mit explizitem Gender- und Rassismus-Fokus sowie intersektionaler Perspektive auf Diskriminierung."
235,K2KL8WH8,Djiberou Mahamadou (2024),Revisiting Technical Bias Mitigation Strategies,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Exclude,Not_relevant_topic,Literaturreview,0.85,"Das Paper behandelt technische Bias-Mitigation und Fairness-Metriken (TECHNIK_OK), aber ausschließlich im Gesundheitswesen ohne Bezug zu Sozialer Arbeit. Während Bias_Ungleichheit und Fairness adressiert werden, fehlt ein direkter Bezug zu den definierten SOZIAL-Kategorien (Soziale_Arbeit, Gender, Diversität, Feministisch). Das Paper liegt außerhalb des Scope eines Literature Review zur KI in der Sozialen Arbeit."
236,DWS4KXBW,Ovalle (2024),Towards Substantive Equality in Artificial Intelligence: Transformative AI Policy for Gender Equality and Diversity,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Konzept,0.92,"Das Paper behandelt KI-Systeme substantiell unter der Linse sozialer Gerechtigkeit. KI_Sonstige: Ja (algorithmische Systeme und deren gesellschaftliche Auswirkungen). Bias_Ungleichheit: Ja (expliziter Fokus auf Verzerrungen gegenüber marginalisierten Gruppen). Gender: Ja (Geschlechtergleichstellung als Kernthema). Diversitaet: Ja (Inklusion verschiedener Gruppen). Feministisch: Ja (intersektionale, menschenrechtsbasierte Perspektive mit Fokus auf marginalisierte Gruppen und strukturelle Gerechtigkeit). Fairness: Ja (Fairness als explizites Ziel). TECHNIK_OK (KI_Sonstige) + SOZIAL_OK (Bias, Gender, Diversität, Feministisch, Fairness) → Include."
237,K6JQ7SVA,Zannone (2023),Intersectional Fairness: A Fractal Approach,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,,Theoretisch,0.92,"Paper behandelt KI/ML-Systeme (KI_Sonstige: mathematische Fairness-Metriken für Klassifikation). Substantieller Fokus auf intersektionale Fairness und algorithmischen Bias (Bias_Ungleichheit). Intersektionale Analyse nach Crenshaw ist explizit feministisches Konzept (Feministisch). Fairness-Metriken sind Kernthema (Fairness). Diversität durch Fokus auf Untergruppen und marginalisierte Intersektionen adressiert (Diversitaet). Erfüllt TECHNIK (KI_Sonstige) + SOZIAL (Bias_Ungleichheit, Feministisch, Fairness, Diversitaet)."
238,NBYNRKBL,Wudel & Ehrenberg (2025),What is Feminist AI?,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Konzept,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige) durch Framework-Entwicklung für KI-Kompetenzentwicklung und Machtanalyse in KI-Systemen. SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness) durch explizite feministische Methodologie nach intersektionalem Ansatz, Bekämpfung struktureller Ungleichheiten und Fairness-Fokus. Das Paper entwickelt substantiell einen feministischen Rahmen für KI-Governance."
239,XK6G84V7,Wang et al. (2024),Algorithmic discrimination: examining its types and regulatory measures with emphasis on US legal practices,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Literaturreview,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmische Diskriminierungssysteme), SOZIAL_OK (Bias_Ungleichheit + Fairness: systematische Analyse von Diskriminierungstypen, Regelungsansätze, strukturelle Ungleichheiten). Kein direkter Sozialarbeitsbezug, daher nicht als SA-Paper klassifiziert. Substantieller Fokus auf algorithmische Fairness und Bias-Analyse."
240,T83KNEQZ,Ovalle et al. (2023),Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,,Literaturreview,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja: algorithmische Fairness-Systeme) und SOZIAL_OK (Bias_Ungleichheit=Ja, Diversitaet=Ja, Feministisch=Ja [explizite Verwendung von Intersektionalität nach Crenshaw, Analyse von Machtstrukturen], Fairness=Ja). Kritische intersektionale Analyse struktureller Machtverhältnisse in KI-Systemen mit explizitem feministischem Rahmen (Crenshaw)."
241,TRAN2GJU,Ricaurte (2024),How can feminism inform AI governance in practice?,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Theoretisch,0.95,"Paper adressiert KI-Governance als algorithmisches System (KI_Sonstige) mit explizitem Fokus auf feministische Theorie und Methodik (Feministisch). Substantielle Behandlung von Machtungleichgewichten, Bias, strukturellen Ungleichheiten (Bias_Ungleichheit), Gender-Perspektive und Diversität/Inklusion von marginalisierten Communities der globalen Mehrheit. Fairness im Governance-Kontext impliziert. Beide Bedingungen (TECHNIK + SOZIAL) erfüllt."
242,Q4LK53XW,Kong (2022),"Are ""Intersectionally Fair"" AI Algorithms Really Fair to Women of Color? A Philosophical Analysis",Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Theoretisch,0.95,"Paper kombiniert Technik (KI_Sonstige: algorithmische Fairness-Systeme) mit starkem sozialen Bezug. Erfüllt mehrere SOZIAL-Kategorien: Bias_Ungleichheit (strukturelle Unterdrückungssysteme), Gender (Fokus auf Women of Color), Diversitaet (intersektionale Analyse), Feministisch (kritisch-theoretischer Ansatz, Intersektionalität nach Crenshaw-Tradition), Fairness (direkter Bezug zu Fair ML). TECHNIK_OK und SOZIAL_OK → Include."
243,XIYX5HJS,Ricaurte Quijano & Prud’homme (2024),Towards substantive equality in artificial intelligence: Transformative AI policy for gender equality and diversity,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Konzept,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (Policy-Rahmen für AI-Governance), KI_Sonstige (algorithmische Diskriminierung). SOZIAL: Bias_Ungleichheit (Fokus auf algorithmische Diskriminierung), Gender (explizite intersektionale Genderanalyse), Diversitaet (inklusive Repräsentation), Feministisch (intersektionale Analyse nach Crenshaw-Logik), Fairness (substantive equality vs. formale Fairness). Transformatives Policy-Framework mit starkem Fokus auf strukturelle Gerechtigkeit."
244,3QUWCYVW,Kattnig et al. (2024),Assessing trustworthy AI: Technical and legal perspectives of fairness in AI,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Theoretisch,0.92,"Paper behandelt algorithmische Systeme (KI_Sonstige: Ja) mit substantiellem Fokus auf Bias-Mitigation und Diskriminierung (Bias_Ungleichheit: Ja) sowie Fairness-Konzepte (Fairness: Ja). Kritische Analyse von Fairness-Definitionen und Bias-Mitigation-Methoden erfüllt beide Bedingungen (TECHNIK + SOZIAL). Kein direkter Sozialarbeit-Bezug, daher Kategorien Soziale_Arbeit/Gender/Diversitaet/Feministisch korrekt auf Nein."
245,5F7D9PEB,Alvarez et al. (2024),Policy advice and best practices on bias and fairness in AI,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Exclude,"TECHNIK_OK (KI_Sonstige: Ja) aber SOZIAL_OK nicht erfüllt. Bias_Ungleichheit und Fairness sind erfüllt, aber kein direkter Bezug zu Sozialer Arbeit, Gender-Fokus, Diversität oder feministischer Theorie. Das Paper adressiert allgemeine KI-Policy und Fairness im breiteren Kontext, nicht spezifisch die Schnittmenge von KI und Sozialer Arbeit.",Literaturreview,0.85,"Das Paper behandelt algorithmische Fairness und Bias-Mitigation mit Policy-Fokus (KI_Sonstige: Ja, Fairness: Ja). Es erfüllt die TECHNIK-Bedingung. Allerdings fehlt der explizite Bezug zu Sozialer Arbeit oder marginalisierten Gruppen im Kontext sozialer Dienste. Intersektionale Diskriminierung wird erwähnt, aber nicht aus feministischer oder diversitätsorientierter Perspektive. Das Paper bleibt auf allgemeiner KI-Policy-Ebene und nicht im Sozialbereich."
246,2SLISKSW,McCrory (2024),Avoiding catastrophe through intersectionality in global AI governance,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Theoretisch,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmische Governance und AI-Systeme sind substantiell adressiert) und SOZIAL_OK (Bias_Ungleichheit, Gender, Diversität und explizit Feministisch durch Intersektionalitätsanalyse nach Crenshaw-Perspektive, Fairness durch Kritik ungleicher AI-Harm-Verteilung). Kritische Analyse von AI Governance aus intersektionaler, feministischer Perspektive mit Fokus auf marginalisierte Gruppen."
247,8U23BX2J,Ulnicane (2024),Intersectionality in artificial intelligence: Framing concerns and recommendations for action,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Theoretisch,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmische Systeme und Bias), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch via intersektionale Analyse nach Crenshaw-Rahmen, Fairness). Intersektionalität und kritische Machtanalyse sind explizit feministisch. Fokus auf strukturelle Systeminnovation, nicht nur technische Fixes."
248,7L93JBLR,Karagianni (2025),Gender in a stereo-(gender)typical EU AI law: A feminist reading of the AI Act,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Theoretisch,0.95,"Das Paper erfüllt TECHNIK (KI_Sonstige: algorithmische Systeme und deren Regulierung) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness). Es nutzt explizit feministische Theorie (hermeneutical injustice, feminist legal theory, Intersektionalität) zur Kritik von KI-Governance. Substantielle Behandlung von Geschlechter-Bias, Diskriminierung marginalisierter Gruppen und Fairness in AI-Regulierung."
249,NUVZI357,Mosene (2023),Feministische Netzpolitik und Künstliche Intelligenz in der politischen Bildung [Feminist network politics and artificial intelligence in political education],Ja,Nein,Nein,Ja,Nein,Ja,Ja,Nein,Ja,Ja,Include,,Theoretisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige) + SOZIAL_OK (Bias_Ungleichheit, Gender, Feministisch, Fairness). Behandelt substantiell feministische Perspektive auf KI (intersektional), Gender-Bias in KI-Systemen, AI-Literacy in politischer Bildung. Kein Sozialarb.-Bezug, daher nicht in dieser Kategorie."
250,2565PPJW,Guerra et al. (2023),Feminist reflections for the development of artificial intelligence,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Konzept,0.95,"Paper behandelt substantiell feministische Perspektiven auf KI-Entwicklung (Feministisch: Ja), thematisiert Gender-Perspektiven und Geschlechtergerechtigkeit (Gender: Ja), adressiert Diversität und Inklusion durch intersektionale Teams und Community-Partizipation (Diversitaet: Ja), diskutiert Bias und Ungleichheit (Bias_Ungleichheit: Ja), und entwickelt KI-Kompetenzen und Methodologien (AI_Literacies: Ja). Erfüllt beide TECHNIK- und SOZIAL-Kriterien."
251,DA6T4Z5B,Sinders (2017),Feminist Data Set,Ja,Ja,Ja,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Konzept,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK-Seite mit AI_Literacies (kritische Kompetenzentwicklung), Generative_KI (Chatbot-Design), Prompting (explizit critical prompting practices), und KI_Sonstige (Datensammlung, Training, Algorithmenauswahl). SOZIAL-Seite mit starkem Feministisch-Fokus (explizit feministische und intersektionale Perspektive), Bias_Ungleichheit (Protest gegen biased AI), Gender und Diversitaet (community-based approach). Substanzielle Behandlung aller Kategorien."
252,UFJ7ERFF,D'Ignazio & Klein (2024),Data Feminism for AI,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Theoretisch,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja: algorithmische Systeme und KI-Diskriminierung), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet und Feministisch alle Ja). Data Feminism ist ein kanonales Werk der feministischen Technikforschung (D'Ignazio explizit), das strukturelle Diskriminierung in KI-Systemen mittels intersektionaler, feministischer Analyse adressiert. Kein Sozialarbeit-Bezug, aber nicht erforderlich."
253,BJZYGNEE,Toupin (2024),Shaping feminist artificial intelligence,Ja,Nein,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Include,,Literaturreview,0.95,"Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies, Prompting) + SOZIAL (Feministisch, Gender, Bias_Ungleichheit, Diversitaet). Explizit feministische Theorie (Rahmenwerk mit 6 Kategorien), Fokus auf Design und Prompting-Praktiken sowie Geschlechterperspektive auf KI-Systeme. Typen-orientierter Literaturreview mit substanziellem feministischem Ansatz."
254,N7J2ZRFP,Srivastava (2024),Algorithmic Governance and the International Politics of Big Tech,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Nein,Exclude,Not_relevant_topic,Theoretisch,0.85,"Das Paper behandelt algorithmische Systeme (KI_Sonstige: Ja) und Machtungleichgewichte durch Big Tech (Bias_Ungleichheit: Ja). Allerdings fehlt ein direkter Bezug zu Sozialer Arbeit oder deren Praxis/Zielgruppen. Die Analyse fokussiert auf politische Wirtschaft und staatliche Souveränität, nicht auf Sozialbereich. SOZIAL-Bedingung ist nicht erfüllt."
255,HJ7BHX8J,Browne et al. (2024),Engineers on responsibility: feminist approaches to who's responsible for ethical AI,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Exclude,"TECHNIK_OK (KI_Sonstige: Ja - Verantwortung in KI-Entwicklung), aber SOZIAL_NOT_OK. Feministisch ist Ja, aber das reicht nicht: Soziale_Arbeit=Nein (kein direkter Bezug zu sozialer Arbeit, Zielgruppen oder Praxis), Bias_Ungleichheit=Nein (nicht explizit zum Thema), Gender=Nein (keine Gender-spezifische Analyse). Fairness=Nein (Verantwortung ist nicht Fairness im technischen Sinne). Das Paper ist zwar feministisch theoretisch fundiert und behandelt KI, aber es adressiert keinen der sozialen Arbeitsbereiche des Literature Reviews (Soziale Arbeit, Bias/Ungleichheit, Gender, Diversität, Fairness). Reine AI-Ethik-Kritik ohne Sozialarbeitsbezug.",Theoretisch,0.85,"Paper kombiniert feministische Theorie (Browne et al.) mit KI-Verantwortungsdiskurs. KI_Sonstige=Ja (KI-Entwicklung, Verantwortungsrahmen), Feministisch=Ja (explizit feministische politische Theorie). Jedoch: Soziale_Arbeit=Nein (kein Bezug zu SW-Praxis/Zielgruppen), Bias_Ungleichheit=Nein (nicht substantiell), Gender=Nein (keine Gender-Spezifik). Fehlt: SOZIAL-Komponente im engeren Sinne des Reviews. Nur eines der 6 sozialen Kriterien erfüllt → Exclude."
256,VFD9ENG6,Gohar & Cheng (2023),"A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges",Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Literaturreview,0.95,"Das Paper behandelt substantiell algorithmische Fairness und Bias in ML-Systemen (KI_Sonstige, Fairness). Es greift EXPLIZIT auf Crenshaw's intersektionale Theorie zurück und analysiert Diskriminierungsmechanismen aus intersektionaler Perspektive (Feministisch, Bias_Ungleichheit, Gender, Diversitaet). TECHNIK-Bedingung erfüllt (KI_Sonstige=Ja), SOZIAL-Bedingung erfüllt (Bias_Ungleichheit, Gender, Diversitaet, Feministisch = alle Ja). Include-Schwelle erreicht."
257,THGC3PA2,Kubes (2024),Feministische KI – Künstliche Intelligenz für alle? [Feminist AI – Artificial intelligence for everyone?],Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Nein,Include,,Konzept,0.92,"Paper behandelt substantiell AI Literacy durch feministische Perspektive (Kubes, Gender-kritisch). Analysiert KI-Anwendungen im Kontext Bias (androzentrisch, eurozentrisch, kapitalistisch-patriarchal), Geschlechterperspektive und Inklusion. Praktischer Bezug durch Curriculum und Workshops. Beide Bedingungen erfüllt: TECHNIK (AI_Literacies, KI_Sonstige), SOZIAL (Feministisch, Gender, Bias_Ungleichheit, Diversitaet)."
258,MT54YHER,Zhao et al. (2025),Thinking like a scientist: Can interactive simulations foster critical AI literacy?,Ja,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Empirisch,0.92,"Paper erfüllt TECHNIK-Kriterium durch AI_Literacies (Fokus auf kritische KI-Kompetenzentwicklung), Generative_KI (Erwähnung von Language Models) und SOZIAL-Kriterium durch Bias_Ungleichheit (expliziter Fokus auf Bias-Mechanismen) und Fairness (Verständnis von Fairness als zentrales Element). Empirische Studie mit großer Stichprobe (n=605). Kein Soziale_Arbeit-Bezug erkennbar."
259,T8R8RKX9,Wilson & Caliskan (2024),AI tools show biases in ranking job applicants' names according to perceived race and gender,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Empirisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK (KI_Sonstige: algorithmische Systeme für Recruitment/Resume-Screening) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet: explizite Analyse von Rassismus und Geschlechterdiskriminierung; Feministisch: intersektionale Analyse nach Crenshaw-Logik; Fairness: algorithmic fairness in hiring). Substanzielle empirische Studie mit klarem intersektionalem Fokus."
260,X54V3JMF,Ulnicane (2024),Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Nein,Include,,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: analysiert AI-Systeme wie Voice Assistants, Robots, Hiring Tools). SOZIAL_OK (Bias_Ungleichheit: Diskriminierung durch homogene Dev-Teams; Gender + Diversitaet: mehrere Diskriminierungsformen; Feministisch: explizite Anwendung von Crenshaw's Intersektionalitätstheorie). Substantielle intersektionale Analyse von strukturellen KI-Ungleichheiten."
261,EN6GNKL3,Dilek et al. (2025),AI literacy in teacher education: Empowering educators through critical co-discovery,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Include,,Experimentell,0.85,Paper behandelt substantiell AI Literacy durch teacher education und co-discovery-Ansätze (TECHNIK erfüllt). Expliziter Fokus auf kritische Examination von AI's power dynamics und social justice implications erfüllt Bias_Ungleichheit-Kategorie (SOZIAL erfüllt). Beide Bedingungen der Inklusionskriterien sind erfüllt.
262,QUU8QDPQ,Benjamin (2023),Keynote Summary: The New Jim Code: Reimagining the Default Settings of Technology & Society,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Exclude,Not_relevant_topic,Konzept,0.85,"Das Paper erfüllt die SOZIAL-Bedingung (Bias_Ungleichheit, Diversitaet, Feministisch, Fairness alle Ja) und auch die TECHNIK-Bedingung (KI_Sonstige Ja). Jedoch ist dies eine Keynote-Zusammenfassung von fundamentaler Kritischer Techniktheorie/Philosophie ohne direkten Bezug zu Sozialer Arbeit als Praxis oder Disziplin. Das Literature Review fokussiert auf KI-Anwendungen in der Sozialen Arbeit, nicht auf allgemeine KI-Ethik und Gesellschaftskritik. Benjamin's Werk ist relevant für theoretische Grundlagen, aber nicht spezifisch für das Scopus des Reviews."
263,PQC9G5EU,Wajcman & Young (2023),Feminism Confronts AI: The Gender Relations of Digitalisation,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Nein,Include,,Theoretisch,0.95,Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja: kritische Analyse von KI-Systemen und deren Entwicklung) UND SOZIAL_OK (Bias_Ungleichheit=Ja: Fokus auf Gender Bias in AI; Gender=Ja: expliziter Gender-Fokus; Diversitaet=Ja: Unterrepräsentation von Frauen; Feministisch=Ja: verwendet feministische Theorie und Kritik von Machtstrukturen). Theoretisches Paper mit substantiellem feministischem Anspruch.
264,U75LP6SV,Raji & Buolamwini (2024),The Algorithmic Auditing Landscape: A Social Justice Approach,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,,Theoretisch,0.92,"Paper erfüllt TECHNIK-Kriterium über KI_Sonstige (algorithmische Auditing-Systeme) und SOZIAL-Kriterium über multiple Kategorien: Bias_Ungleichheit (Fokus auf Diskriminierung und marginalisierte Communities), Diversitaet (Multi-Stakeholder-Prozess, Community-Involvement), Feministisch (intersektionale Kritik an Machtstrukturen, Fokus auf strukturelle Benachteiligung), Fairness (algorithmic auditing für gerechtere Systeme). Kritik an technischen Metriken und Betonung von Community-Perspektiven zeigt feministische/kritische Technikforschung."
265,JZ4P8V8S,Thwaites et al. (2024),Operationalizing positive-constructive pedagogy to artificial intelligence: the 3E model for scaffolding AI technology adoption,Ja,Nein,Ja,Nein,Nein,Ja,Nein,Nein,Nein,Nein,Include,,Konzept,0.85,Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies Ja: Pedagogisches Framework für kritische KI-Kompetenzentwicklung; Prompting Ja: Expliziter Fokus auf 'critical prompting' in Explore-Phase). SOZIAL_OK (Bias_Ungleichheit Ja: Substantielle Behandlung von Bias-Erkennung und Sichtbarmachung von Diskriminierung in KI-Outputs). Kein Bezug zu Sozialer Arbeit oder explizit feministischer Theorie; daher nur diese Kategorien Ja.
266,SJLJ2GHC,Ulnicane (2024),Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Theoretisch,0.92,"Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Kritische Analyse von KI-Policy), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch: Explizit intersektionale Analyse mit Fokus auf strukturelle Machtdynamiken; Fairness: Kritik an unzureichenden Fairness-Ansätzen). Die feministische Perspektive ist substantiell durch intersektionale Theoriearbeit (Crenshaw-Rahmen impliziert) verankert."
267,3XMBE43Z,Voutyrakou & Skordoulis (2025),Algorithmic Governance: Gender Bias in AI-Generated Policymaking?,Nein,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Ja,Ja,Include,,Experimentell,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting), SOZIAL_OK (Bias_Ungleichheit + Gender + Feministisch + Fairness). Empirische Analyse von Gender-Bias in GPT-4/Copilot-generierten Policies mit intersektionalem Rahmen. Fokus auf Androzentrizität und strukturelle Biases entspricht feministischer Technikforschung. Keine direkte Soziale-Arbeit-Verbindung, aber Policymaking ist relevant für soziale Systeme."
268,SHDNTZJZ,Toupin (2024),Shaping feminist artificial intelligence,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Nein,Ja,Nein,Include,,Literaturreview,0.95,"Das Paper behandelt substantiell feministische Perspektiven auf KI (KI_Sonstige: Ja), thematisiert Gender-Bias und Machtdynamiken in AI-Systemen (Bias_Ungleichheit: Ja, Gender: Ja) und nutzt explizit feministische Theorie als Analyserahmen (Feministisch: Ja). Erfüllt beide Bedingungen: TECHNIK (KI_Sonstige) und SOZIAL (Bias_Ungleichheit, Gender, Feministisch)."
269,9YCFMPVT,Ulnicane (2024),Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Nein,Include,,Literaturreview,0.92,"Paper analysiert intersektionale Perspektiven in KI-Entwicklung und befasst sich substantiell mit Bias, Geschlechterperspektiven und struktureller Unterrepräsentation. Intersektionalität (Crenshaw-Framework) ist eine feministische Methode. Erfüllt TECHNIK (KI_Sonstige) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch). Kein direkter Soziale_Arbeit-Bezug, aber AI-Systemkritik mit marginalisierten Gruppen im Fokus."
270,C485NKYA,Attard-Frost et al. (2025),AI Countergovernance: Lessons Learned from Canada and Paris,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Nein,Ja,Nein,Include,,Konzept,0.82,"Paper behandelt kritische AI Literacies (TECHNIK) und thematisiert strukturelle Ungleichheiten bezüglich Rasse, Geschlecht und Arbeit (SOZIAL). Expliziter Fokus auf Bias/Ungleichheit und Gender. Kritische Perspektive auf Strukturen deutet auf feministische Ansätze hin. Beide Bedingungen erfüllt → Include."
271,XZ6Z8A9C,Knowles et al. (2023),Trustworthy AI and the Logics of Intersectional Resistance,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Theoretisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Ja - Kritik an AI-Ethics-Frameworks als algorithmische Systeme) und SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness alle Ja). Explizit intersektional-feministischer Ansatz mit Fokus auf marginalisierte Gruppen und strukturelle Gerechtigkeit. Kein Bezug zu Sozialer Arbeit als Disziplin, aber substantielle Behandlung von Fairness und intersektionalen Gerechtigkeitsfragen."
272,GJF776AY,Shukla (2025),Investigating AI systems: examining data and algorithmic bias through hermeneutic reverse engineering,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Theoretisch,0.85,"Paper behandelt Bias in AI-Systemen (KI_Sonstige: Ja) mittels hermeneutischer Reverse Engineering. Fokus auf algorithmischen Bias (Bias_Ungleichheit: Ja), Auswirkungen auf verschiedene soziale Gruppen (Diversitaet: Ja) und Fairness-Aspekte durch participatory design (Fairness: Ja). Beide Bedingungen erfüllt: TECHNIK (KI_Sonstige) + SOZIAL (Bias_Ungleichheit, Diversitaet, Fairness)."
273,PC75954S,Ghosal (2024),An empirical study of structural social and ethical challenges in AI,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Empirisch,0.85,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja, da es sich um Studie zu AI-Development befasst) und SOZIAL_OK (Bias_Ungleichheit=Ja: 'injustices and inequalities'; Fairness=Ja: Responsibility und strukturelle Herausforderungen). Kein Bezug zu Sozialer Arbeit oder feministischer Theorie erkennbar. Empirische Studie mit N=32 Professionals."
274,ZAU6P4BK,Toupin (2024),Shaping feminist artificial intelligence,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Literaturreview,0.92,"Das Paper behandelt substantiell feministische Theorie und Perspektiven im KI-Kontext (KI_Sonstige: Ja). Es adressiert explizit feministische Ansätze, Machtstrukturen und soziale Gerechtigkeit (Feministisch: Ja). Gender und Diversität sind zentral (Gender: Ja, Diversitaet: Ja). Bias/Ungleichheit wird durch die Fokussierung auf Machtrelationen thematisiert (Bias_Ungleichheit: Ja). TECHNIK erfüllt (KI_Sonstige), SOZIAL erfüllt (multiple Kategorien). → Include"
275,Y8J3HI9J,Lin & Chen (2022),Artificial Intelligence in a Structurally Unjust Society,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Include,,Theoretisch,0.85,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja, algorithmische Systeme im Gesundheitskontext), SOZIAL_OK (Bias_Ungleichheit=Ja zu struktureller Ungerechtigkeit, Fairness=Ja zu AI Fairness). Kein direkter Soziale_Arbeit-Bezug, daher keine entsprechende Kategorie."
276,VVEEL68I,Slesinger et al. (2024),Training in Co-Creation as a Methodological Approach to Improve AI Fairness,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,,Empirisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies: Training zu KI-Konzepten; KI_Sonstige: AI Bias Detection/Mitigation Tools; Fairness: Algorithmen-Fairness) + SOZIAL (Bias_Ungleichheit: Fokus auf Bias Detection; Diversitaet: vulnerable/marginalized stakeholder groups, inklusive AI-Entwicklung; Feministisch: Co-creation mit marginalisierten Gruppen, Machtstrukturen in AI-Design). Substantieller Fokus auf Training zur Befähigung von Non-Technical Stakeholders und kritische Reflexion von Inklusion in AI-Systemen."
277,HLBXNWAZ,Sharma et al. (2024),Intersectional analysis of visual generative AI: the case of stable diffusion,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Include,,Empirisch,0.95,"Paper erfüllt beide Bedingungen: (1) TECHNIK: Generative_KI=Ja (Stable Diffusion als visuelles generatives KI-System). (2) SOZIAL: Bias_Ungleichheit=Ja (Analyse von Sexismus, Rassismus, Heteronormativität, Ableismus), Gender=Ja (expliziter Fokus auf Geschlechterstereotype/masculine-presenting standard), Diversitaet=Ja (Repräsentation marginalisierter Gruppen), Feministisch=Ja (intersektionale Analyse nach kritischer Perspektive, Fokus auf Machtstrukturen und Marginalisierung). Empirische Inhaltsanalyse von 180 generierten Bildern."
278,Y6SAPNT2,Vethman et al. (2025),Fairness Beyond the Algorithmic Frame: Actionable Recommendations for an Intersectional Approach,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,,Konzept,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige, Fairness behandeln KI-Systeme und Kompetenzentwicklung). SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Feministisch durch intersektionale Perspektive, expliziter Fokus auf Machtstrukturen und marginalisierte Gruppen). Intersektionalität ist feministische Theorie (Crenshaw). Framework für verantwortungsvolle AI-Entwicklung mit gesellschaftlichem Kontext."
279,XXCDL3A3,Ovalle et al. (2023),Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,,Literaturreview,0.95,"Das Paper behandelt substantiell AI Fairness und Bias/Ungleichheit im KI-Kontext (KI_Sonstige erfüllt Technik-Bedingung). Bias_Ungleichheit, Diversitaet und Fairness sind zentrale Themen. Explizite feministische Perspektive durch Bezug zu Crenshaw's Intersectionality-Framework und kritische Analyse von Machtstrukturen (Feministisch = Ja). Beide Bedingungen (Technik + Sozial) erfüllt → Include."
280,QXDK8Z6I,Himmelreich & Lim (2022),"Artificial Intelligence and Structural Injustice: Foundations for Equity, Values, and Responsibility",Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Theoretisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK durch KI_Sonstige (algorithmische Systeme und AI Bias); SOZIAL_OK durch Bias_Ungleichheit (strukturelle Gerechtigkeit), Diversitaet (Diversity/Equity/Inclusion) und Fairness (Frameworks für faire KI-Governance). Theoretischer Ansatz basierend auf Young's Strukturtheorie, nicht auf feministischer Methode selbst (daher Feministisch=Nein)."
281,LT3D3ZQ2,Siapka (2023),Towards a Feminist Metaethics of AI,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Nein,Ja,Nein,Include,,Theoretisch,0.92,"Das Paper entwickelt einen explizit feministischen theoretischen Rahmen (feminist metaethics) für KI-Ethik und untersucht Machtstrukturen, Kontexte und die Rolle von Akteur:innen. KI_Sonstige=Ja (KI-Ethik als Teilbereich), Feministisch=Ja (explizite feministische Metaethik-Perspektive), Bias_Ungleichheit=Ja (Untersuchung von Machtrelationen und Kontexten), Gender=Ja (Gender-expliziter Fokus). TECHNIK-Bedingung (KI_Sonstige) + SOZIAL-Bedingungen (Feministisch, Bias_Ungleichheit, Gender) erfüllt → Include."
282,D2EGAVKZ,Klein & D'Ignazio (2024),Data Feminism for AI,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Konzept,0.95,"Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Ja - behandelt AI-Systeme im Allgemeinen), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness alle Ja). Explizit feministische Arbeit (D'Ignazio ist Autorin von Data Feminism), intersektionale Perspektive auf KI-Systeme, Fokus auf ungleiche Machtbeziehungen und Gerechtigkeit. Substantielle Behandlung aller relevanten Kategorien."
283,NHIZN4QJ,Wudel & Ehrenberg (2025),What is Feminist AI?,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies: Ja – Framework für KI-Kompetenzen/Werte-Integration; KI_Sonstige: Ja – Systemische Analyse von AI-Biases). SOZIAL_OK (Bias_Ungleichheit: Ja – adressiert Biases und Injustices; Gender: Ja – expliziter Gender-Fokus; Diversitaet: Ja – Inklusion; Feministisch: Ja – intersektionale Feminismus, feministische Theorie; Fairness: Ja – Gerechtigkeit und Equity als zentrale Werte). Praktische Anwendungen (MIRA, EU AI Act) zeigen Substanz."
284,MIT8HTC6,Clemmer et al. (2024),PreciseDebias: An automatic prompt engineering approach for generative AI to mitigate image demographic biases,Nein,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Include,,Experimentell,0.95,"Paper behandelt Generative KI (Bildgeneratoren) mit substantiellem Fokus auf Prompt-Engineering durch automatisierte Prompt-Rewriting. Adressiert demografische Bias-Reduktion explizit durch Gender-, Ethnizität- und Altersrepräsentation. Fairness-Metriken zur Bias-Messung zentral. TECHNIK erfüllt (Generative_KI + Prompting), SOZIAL erfüllt (Bias_Ungleichheit + Gender + Diversitaet + Fairness). Keine feministische Theorie-Nutzung, kein SA-Bezug."
285,YN9JAREI,Gengler et al. (2024),Faires KI-Prompting – Ein Leitfaden für Unternehmen,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Konzept,0.92,"Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies, Generative_KI, Prompting alle Ja) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness alle Ja). Fokus auf feministisches KI-Prompting mit KI-FAIRNESS-Framework für faire, diskriminierungsvermeidende Prompt-Gestaltung. Substantielle Behandlung von Stereotyp-Reduktion und Repräsentation in KI-Outputs."
286,UJ7DXK8Y,Santy et al. (2023),NLPositionality: Characterizing design biases of datasets and models,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Include,,Konzept,0.85,"Paper behandelt substantiell algorithmische Bias und Repräsentationsprobleme in KI-Datasets/Modellen (KI_Sonstige: Ja). Das Konzept der 'Positionality' basiert auf einer kritischen Perspektive, die implizit feministische Technikforschung (d'Ignazio & Klein) widerspiegelt - Fokus auf positionality, marginalisierte Gruppen, strukturelle Machtdynamiken. Adressiert Diversität, Bias und Fairness substantiell. Kein direkter Sozialarbeitsbezug, keine Generative KI. TECHNIK + SOZIAL erfüllt → Include."
287,MJDTRLAI,Shin et al. (2024),Can prompt modifiers control bias? A comparative analysis of text-to-image generative models,Nein,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Empirisch,0.92,"Das Paper erfüllt beide Bedingungen: (1) TECHNIK: Generative_KI (Text-to-Image Modelle), Prompting (Analyse von Prompt-Modifizierern zur Bias-Reduktion) → Ja. (2) SOZIAL: Bias_Ungleichheit (explizit: Analyse gesellschaftlicher Biases), Diversitaet (diversity-reflective prompting), Fairness (ethische KI-Entwicklung) → Ja. Empirische Vergleichsstudie mit substantiellem Fokus auf Prompt-Engineering und algorithmischen Bias."
288,6L85PRUW,Skilton & Cardinal (2024),Inclusive prompt engineering: A methodology for hacking biased AI image generation,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Include,,Experimentell,0.92,"Paper behandelt substantiell Prompting-Strategien zur Bias-Mitigation in generativen KI-Systemen (Bildgeneratoren). TECHNIK erfüllt: AI_Literacies (Kompetenzentwicklung für inklusives Prompting), Generative_KI (Bildgeneratoren), Prompting (Prompt-Engineering als Kernmethodologie). SOZIAL erfüllt: Bias_Ungleichheit (Stereotypen-Analyse), Diversitaet (inklusive Repräsentation), Gender (implizit durch Stereotypen), Fairness (Bias-Mitigation). Beide Bedingungen erfüllt → Include."
289,YLAKP7Z2,Jääskeläinen et al. (2025),Intersectional analysis of visual generative AI: The case of Stable Diffusion,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Nein,Include,,Theoretisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK durch Generative_KI (Stable Diffusion-Analyse). SOZIAL_OK durch Bias_Ungleichheit (Stereotype, Diskriminierung), Gender (Sexismus-Fokus), Diversitaet (Inklusion marginalisierter Gruppen), und Feministisch (explizite feministische intersektionale Methodik nach Crenshaw-Tradition, kritische Machtanalyse). Substantielle qualitative Analyse von AI-Outputs unter sozialkritischer Perspektive."
290,8WBUGXRR,Djeffal (2025),Reflexive prompt engineering: A framework for responsible prompt engineering and AI interaction design,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Konzept,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, Generative_KI, Prompting alle Ja), SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Fairness alle Ja). Framework für verantwortungsvolles Prompt Engineering mit explizitem Fokus auf ethische Prinzipien, Menschenrechte und Diversität. Keine Soziale_Arbeit oder feministischen Theoriebezug identifiziert."
291,IDW7QSYG,Fraile-Rojas et al. (2025),Female perspectives on algorithmic bias: Implications for AI researchers and practitioners,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Empirisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: NLP/ML für Datenanalyse; AI_Literacies: feministische digitale Kompetenzen) und SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness substantiell behandelt). Empirische Analyse von 172K Tweets zu Geschlechter-Perspektiven auf algorithmischen Bias mit explizit feministischem Rahmen."
292,AIZGTQKG,Shah (2025),Gender bias in artificial intelligence: Empowering women through digital literacy,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Nein,Ja,Include,,Literaturreview,0.88,"Paper erfüllt BEIDE Bedingungen: TECHNIK (AI_Literacies: digitale Literacy als Intervention + KI_Sonstige: Bias in Hiring/Healthcare/Finance-Systemen) UND SOZIAL (Gender: expliziter Gender-Bias-Fokus; Bias_Ungleichheit: Underrepresentation von Frauen; Diversitaet: Women-led AI projects; Fairness: algorithmic bias combating). Substantielle Behandlung von Gender und Bias in KI-Systemen. Keine explizit feministische Theorie erkennbar, daher Feministisch=Nein."
293,X7ZGW6CN,Ciston (2024),"Intersectional Artificial Intelligence Is Essential: Polyvocal, Multimodal, Experimental Methods to Save AI",Ja,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies, Generative_KI, KI_Sonstige durch intersektionale AI-Strategien und Chatbot-Beispiel) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch durch explizite intersektionale und queer-feministische Theorie nach Crenshaw; Fairness durch Data Nutrition Label für Bias-Assessment). Substantiell behandelt."
294,CHJQ52DC,Latif et al. (2024),"AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?",Nein,Ja,Nein,Ja,Nein,Ja,Ja,Nein,Nein,Ja,Exclude,Not_relevant_topic,Experimentell,0.85,"Paper erfüllt TECHNIK-Bedingung (Generative_KI, KI_Sonstige) und SOZIAL-Bedingung (Bias_Ungleichheit, Gender, Fairness). ABER: Kein Bezug zu Sozialer Arbeit oder deren Zielgruppen/Praxis. Rein technische Studie zu LLM-Bias ohne Anwendungskontext in Sozialbereich. Für Literature Review Soziale Arbeit nicht relevant."
295,AIGLDZ4C,Articulate (2025),How to Create Inclusive AI Images: A Guide to Bias-Free Prompting,Ja,Ja,Ja,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Include,,Konzept,0.92,"Paper erfüllt TECHNIK-Kriterium (Generative_KI + Prompting + AI_Literacies) und SOZIAL-Kriterium (Bias_Ungleichheit + Gender + Diversitaet + Fairness). Substantieller Fokus auf Prompt-Engineering-Strategien zur Reduktion von Bias in generativen KI-Bildern. Adressiert stereotype Darstellungen und präsentiert konkrete inklusive Prompting-Techniken. Kein explizit feministischer oder sozialarbeiterischer Bezug, daher keine Ja-Vergabe für diese Kategorien."
296,AFDLFCIL,Ahmed (2024),Feminist Perspectives on AI: Ethical Considerations in Algorithmic Decision-Making,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Theoretisch,0.95,"Paper erfüllt TECHNIK-Kriterium durch KI_Sonstige (algorithmische Entscheidungssysteme in Hiring, Healthcare, Law Enforcement). SOZIAL-Kriterium erfüllt durch: Bias_Ungleichheit (Diskriminierung, Bias in automatisierten Systemen), Gender (Unterrepräsentation von Frauen in AI), Diversitaet (marginalisierte Gruppen), Feministisch (explizit feministische Perspektive und Rahmen). Substanzielle Behandlung aller Kategorien im Abstract dokumentiert."
297,FDR5APIU,Ulnicane (2024),Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Literaturreview,0.95,"Paper analysiert Intersektionalität in KI-Systemen explizit mit feministischer Perspektive (Intersektionalitätsagenda nach Crenshaw-Tradition). Thematisiert substantiell: algorithmische Bias/Diskriminierung (KI_Sonstige), strukturelle Ungleichheit (Bias_Ungleichheit), Gender-Perspektive (homogene Gruppen weißer Männer), Diversität in AI-Entwicklung (Diversitaet), feministische Technikforschung (Feministisch) und Fairness-Probleme. Beide Bedingungen erfüllt: Technik (KI_Sonstige=Ja) + Sozial (multiple Kategorien=Ja)."
298,SWB86AYC,Hartshorne & Cohen (2025),Generative AI and the Future of Digital Literacy: Opportunities for Gender Inclusion,Ja,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Nein,Include,,Empirisch,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies: Ja, Generative_KI: Ja); SOZIAL_OK (Bias_Ungleichheit: Ja, Gender: Ja, Diversitaet: Ja). Qualitative empirische Studie zu generativen KI in Bildung mit explizitem Gender-Fokus und Analyse von Bias-Risiken. Kein explizit feministischer Theoriebezug erkannt, daher Feministisch=Nein. Keine Prompting-Fokussierung oder sozialarbeiterischer Bezug."
299,GP4JDSI8,Klein & D'Ignazio (2024),Data Feminism for AI,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK durch AI_Literacies (Framework für KI-Kompetenzen/ethische Prinzipien) und KI_Sonstige (ML-Design, algorithmische Systeme). SOZIAL_OK durch Feministisch (explizite Data Feminism Theorie nach D'Ignazio), Bias_Ungleichheit (Fokus auf diskriminatorische Systeme), Gender (Geschlechterperspektive), Diversitaet (Pluralismus, marginalisierte Gruppen), Fairness (gerechte KI-Entwicklung). Substantielle Behandlung aller Dimensionen."
300,GPSB87RN,Wudel & Ehrenberg (2025),What is Feminist AI?,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK-OK durch KI_Sonstige (Bias- und Fairness-Fokus auf AI-Systeme). SOZIAL-OK durch Bias_Ungleichheit, Gender, Diversitaet, Feministisch (explizit intersektionales Feminist AI Framework) und Fairness. Verwendet feministische Theorie substantiell als Kernrahmen zur AI-Kritik. Keine Soziale_Arbeit-Verbindung erkannt."
301,SHJQQTI6,Gohar & Cheng (2023),A Survey on Intersectional Fairness in Machine Learning: Opportunities and Challenges,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Literaturreview,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: ML-Systeme, Fairness-Metriken); SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch via intersektionale Analyse). Explizit intersektionales Framing (Crenshaw-Tradition) mit Fokus auf marginalisierte Gruppen und Machtstrukturen ist substantiell feministisch."
302,9Y7ZFGI5,Shah (2025),Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Nein,Ja,Include,,Literaturreview,0.95,"Paper erfüllt beide Bedingungen: (1) TECHNIK: AI_Literacies (Digital Literacy Programme), KI_Sonstige (algorithmische Systeme in Recruitment/Healthcare/Finance); (2) SOZIAL: Bias_Ungleichheit (systematische Gender-Biases), Gender (expliziter Gender-Fokus), Diversitaet (Unterrepräsentation von Frauen), Fairness (algorithmische Fairness). Kein feministischer Theorierahmen explizit erwähnt, daher konservativ Nein."
303,9BDIJE9B,UN Women (2024),Artificial Intelligence and gender equality,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Nein,Ja,Include,,Literaturreview,0.92,"TECHNIK_OK: KI_Sonstige=Ja (algorithmische Systeme und deren Auswirkungen). SOZIAL_OK: Bias_Ungleichheit=Ja (Gender-Bias in 44% der KI-Systeme), Gender=Ja (expliziter Gender-Fokus), Fairness=Ja (Policy-Empfehlungen). Kein direkter Soziale_Arbeit-Bezug erkennbar. Nicht feministisch theoretisch fundiert, aber empirisch-analytisch zu Gender-Bias."
304,V4GTLMED,Browne et al. (2024),Tech workers' perspectives on ethical issues in AI development: Foregrounding feminist approaches,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Empirisch,0.95,Paper kombiniert TECHNIK (AI_Literacies: KI-Ethik-Verständnis; KI_Sonstige: algorithmische Bias) mit SOZIAL (Bias_Ungleichheit: Analyse von Bias-Konzepten; Gender + Diversität + Feministisch: expliziter feministischer Ansatz; Fairness: Design Justice). Empirische Studie mit Tech-Worker-Perspektive und Design-Justice-Fokus erfüllt beide Bedingungen eindeutig.
305,YMYHLMFS,Klein & D'Ignazio (2024),Data feminism for AI,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Konzept,0.95,"Paper erfüllt beide Bedingungen: TECHNIK_OK durch KI_Sonstige (allgemeiner KI/ML-Bezug); SOZIAL_OK durch Bias_Ungleichheit, Gender, Diversitaet und Feministisch (explizite Verwendung des Data-Feminism-Frameworks von D'Ignazio, intersektionale feministische Perspektive). Starker konzeptioneller Beitrag zu equitable und ethical AI."
306,QM6L6XLZ,A+ Alliance (2024),Incubating Feminist AI: Executive Summary 2021-2024,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Ja,Ja,Include,,Konzept,0.92,"Das Paper erfüllt beide Bedingungen: TECHNIK (KI_Sonstige: Bias-Detection in NLP, algorithmische Systeme im sozialen Kontext) und SOZIAL (Feministisch: explizit feminist AI; Gender: gender-based violence; Soziale_Arbeit: praktische Anwendungen bei Gewalt und Justiz; Bias_Ungleichheit: Bias-Detection; Diversitaet: intersektionale Analyse, multi-regionale Gemeinschaften). Starker feministischer Rahmen mit substantiellem Fokus auf strukturelle Gerechtigkeit."
307,ZNHUCA4B,UNESCO (2021),Recommendation on the Ethics of Artificial Intelligence,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Nein,Ja,Include,,Konzept,0.85,"UNESCO-Empfehlung behandelt AI Ethics als globale Governance-Frage (KI_Sonstige: Ja). Substantieller Fokus auf Gender Equality, Geschlechterstereotypen und diskriminatorische Biases (Gender: Ja, Bias_Ungleichheit: Ja), sowie equitable participation und Fairness (Diversitaet: Ja, Fairness: Ja). Erfüllt beide Bedingungen: TECHNIK (KI_Sonstige) + SOZIAL (Gender, Bias, Fairness). Nicht-Feministisch, da keine explizite feministische Theorie erkennbar."
308,SQ38TTWQ,An et al. (2025),Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Experimentell,0.95,"Paper kombiniert Technik (Generative_KI: Evaluation von LLMs) mit mehreren sozialen Kategorien. Explizit intersektionale Analyse (Feministisch: Crenshaw'sche Intersektionalität), Gender- und Diversitätsfokus, algorithmischer Bias in Hiring-Kontexten (Bias_Ungleichheit, Fairness). Erfüllt beide Bedingungen klar."
309,ZMW228P6,Ovalle et al. (2023),Factoring the matrix of domination: A critical review and reimagination of intersectionality in AI fairness,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Literaturreview,0.95,"Paper adressiert AI Fairness (KI_Sonstige) mit explizit feministisch-theoretischem Rahmen (Collins, Bilge). Substantielle Behandlung von Intersektionalität (Feministisch, Gender, Diversität, Bias_Ungleichheit). Kritische Analyse algorithmischer Systeme unter intersektionaler Perspektive. TECHNIK (KI_Sonstige) + SOZIAL (Bias, Gender, Diversität, Feministisch, Fairness) erfüllt."
310,XW8NHCIE,Wilson & Caliskan (2024),"Gender, race, and intersectional bias in AI resume screening via language model retrieval",Nein,Ja,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Empirisch,0.95,"Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + KI_Sonstige: LLM-basierte Systeme für Resume Screening), SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch via explizite intersektionale Analyse nach Crenshaw-Theorie, Fairness). Substanzielle empirische Analyse von Diskriminierung in KI-Systemen mit intersektionalem Fokus."
311,ZLM537Z4,Browne et al. (2023),"Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligent Machines",Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Literaturreview,0.95,"Sammlung mit explizit feministischer Perspektive (TECHNIK: KI_Sonstige durch algorithmische Systeme und deren gesellschaftliche Auswirkungen; SOZIAL: Bias_Ungleichheit, Gender, Diversitaet und Feministisch alle substantiell durch Fokus auf Gerechtigkeit, Diskriminierung und feministische Theorie). Beide Bedingungen erfüllt → Include."
312,JZN2I6J5,Chisca et al. (2024),Prompting fairness: Learning prompts for debiasing large language models,Nein,Nein,Ja,Ja,Nein,Ja,Nein,Nein,Nein,Ja,Exclude,"TECHNIK_OK (Prompting + KI_Sonstige erfüllt), aber SOZIAL_OK nicht erfüllt - nur Bias/Fairness im technischen Sinne, kein Bezug zu sozialer Arbeit, Gesellschaft oder marginalisierten Gruppen",Experimentell,0.92,"Paper behandelt Prompt-Tuning für Debiasing von LLMs und adressiert Fairness/Bias-Reduktion. Dies sind technische Kategorien. Allerdings fehlt ein substantieller Bezug zu sozialen Auswirkungen, sozialarbeiterischer Praxis oder marginalisierten Gruppen. Die Bias-Thematisierung ist rein algorithmisch-technisch, nicht sozialwissenschaftlich verankert. Damit erfüllt das Paper die SOZIAL-Bedingung nicht."
313,MS3CNU3S,Gallegos et al. (2024),Bias and fairness in large language models: A survey,Nein,Ja,Nein,Nein,Nein,Ja,Nein,Nein,Nein,Ja,Exclude,Not_relevant_topic,Literaturreview,0.85,"Das Paper erfüllt die TECHNIK-Bedingung (Generative_KI, Bias_Ungleichheit, Fairness alle Ja). Jedoch fehlt die SOZIAL-Bedingung im Sinne des Literature Reviews: Bias_Ungleichheit und Fairness sind zwar vorhanden, aber das Paper behandelt allgemeine KI-Ethik und -Fairness OHNE spezifischen Bezug zu Sozialer Arbeit, sozialen Zielgruppen oder sozialarbeiterischen Kontexten. Die Umfrage konzentriert sich auf technische Metriken und NLP-Debiasing, nicht auf Anwendungen im Sozialbereich. Für den Review erforderlich: KI im Kontext Sozialer Arbeit oder marginalisierten Gruppen in sozialen Handlungsfeldern."
314,7G73H3KM,Small (2023),Generative AI and opportunities for feminist classroom assignments,Ja,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Ja,Nein,Include,,Konzept,0.85,"Paper behandelt Integration generativer KI in Bildung (AI_Literacies: Ja, reflexive Kompetenzentwicklung). Fokus auf generative AI-Tools (Ja). Explizit feministische Perspektive und Epistemologie (Feministisch: Ja). Gender-Fokus durch feministische Theorieentwicklung (Gender: Ja). TECHNIK erfüllt (AI_Literacies + Generative_KI), SOZIAL erfüllt (Feministisch + Gender). Kein direkter Sozialer-Arbeit-Bezug, daher Nein dort."
315,KNQYFQ6B,Sant et al. (2024),The power of prompts: Evaluating and mitigating gender bias in MT with LLMs,Nein,Ja,Ja,Nein,Nein,Ja,Ja,Nein,Nein,Ja,Exclude,"Technik-Kriterien erfüllt (Generative_KI, Prompting), aber kein Bezug zu Sozialer Arbeit, Diversität oder implizit-feministischen Ansätzen. Gender und Bias werden zwar adressiert, aber in einem rein technischen ML-Kontext ohne sozialarbeitsbezogene Anwendung.",Experimentell,0.85,"Das Paper behandelt LLMs (Generative_KI=Ja), Prompt-Engineering (Prompting=Ja), Gender-Bias (Gender=Ja) und Fairness-Metriken (Fairness=Ja). Dies erfüllt die TECHNIK-Bedingung klar. SOZIAL-Bedingung: Bias_Ungleichheit=Ja ist erfüllt. Allerdings liegt der Fokus ausschließlich auf technischer Sprachübersetzung ohne direkten Bezug zu Sozialer Arbeit oder ihren Zielgruppen/Praktiken. Für Inclusion ist substantieller Bezug zu Soziale_Arbeit erforderlich oder starke strukturelle Ungleichheits-Dimension. Hier handelt es sich um ein reines ML/NLP-Paper mit Gender-Bias-Fokus."
316,8BUHU5EP,Derechos Digitales (2023),Feminist reflections for the development of Artificial Intelligence,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Nein,Include,,Konzept,0.92,"Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja, da methodologische Frameworks für AI-Entwicklung), SOZIAL_OK (Feministisch=Ja durch explizite feministische Perspektive; Gender=Ja, Latin American women focus; Diversitaet=Ja, intersektionale Ansätze; Bias_Ungleichheit=Ja, power-balancing und Autonomie). Kein direkter Sozialarbeitsbezug, aber substantielle feminist AI-Entwicklung."
317,IGFYSIV8,Toupin (2024),Shaping feminist artificial intelligence,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Nein,Include,,Literaturreview,0.92,"Das Paper analysiert Feminist AI (FAI) mit explizit feministischer Theorie und Perspektive (erfüllt Feministisch=Ja). Es behandelt algorithmische Systeme und deren gesellschaftliche Auswirkungen (KI_Sonstige=Ja, Bias_Ungleichheit=Ja). Gender und Diversität sind zentral für die Analyse von FAI-Frameworks. TECHNIK erfüllt (KI_Sonstige), SOZIAL erfüllt (Feministisch, Gender, Bias_Ungleichheit, Diversität). Inklusion korrekt."
318,BDI6XU5A,Franken & Mauritz (n.d.),Gender und KI-Anwendungen. Trägt KI zum Genderproblem oder zu seiner Lösung bei?,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Nein,Nein,Nein,Include,,Theoretisch,0.75,"Der Titel signalisiert explizit einen Gender-Fokus im KI-Kontext ('Gender und KI-Anwendungen', Frage zu Genderproblemen). Dies aktiviert KI_Sonstige (allgemeine KI-Anwendungen) und Gender sowie Bias_Ungleichheit (Gender-Bias ist strukturelle Ungleichheit). TECHNIK + SOZIAL erfüllt → Include. Ohne Abstract ist Confidence reduziert; feministischer Theoriebezug unklar."
319,EQV4DNQR,"UNESCO, IRCAI (2024)",Challenging systematic prejudices: an Investigation into Gender Bias in Large Language Models,Nein,Ja,Nein,Nein,Nein,Ja,Ja,Ja,Nein,Ja,Include,,Empirisch,0.95,"Paper untersucht Gender-Bias in LLMs (ChatGPT, GPT-2, Llama 2) – erfüllt TECHNIK-Kriterium (Generative_KI). Expliziter Fokus auf Geschlechter-Stereotypen, soziale Vorurteile und Fairness in generierten Texten erfüllt SOZIAL-Kriterien (Bias_Ungleichheit, Gender, Diversitaet, Fairness). Kein feministischer Theoriebezug. Keine Verbindung zu Sozialer Arbeit, daher nicht relevant für diesen Kontext, aber sachlich korrekt kategorisiert."
320,8NG4ZEWE,Arnold et al. (2023),Intersektionalität,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Nein,Exclude,No_full_text,Unclear,0.95,Keine ausreichende Information vorhanden. Titel allein ('Intersektionalität') ermöglicht keine verlässliche Kategorisierung. Ohne Abstract und Volltext können KI-Technologie-Bezug und spezifisches thematisches Fokus nicht bewertet werden. Restriktives Klassifizieren erfordert Ausschluss bei unzureichender Evidenz.
321,5T55I5Z7,UNESCO (2020),ARTIFICIAL INTELLIGENCE and GENDER EQUALITY,Ja,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Nein,Nein,Include,,Literaturreview,0.85,"Das UNESCO-Paper behandelt substantiell die Themen Gender Equality und AI (Kategorie Gender: Ja). Es adressiert digitale Kompetenzen und Geschlechterkluft (AI_Literacies: Ja), Gender Bias in KI-Systemen (KI_Sonstige: Ja, Bias_Ungleichheit: Ja) sowie Diversität. Beide Bedingungen (Technik + Sozial) sind erfüllt → Include."
322,QUV5DQH3,Smith & Rustagi (2021),When Good Algorithms Go Sexist: Why and How to Advance AI Gender Equity,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Nein,Nein,Ja,Include,,Konzept,0.85,"Das Paper adressiert algorithmen-basierte Systeme (KI_Sonstige) mit explizitem Fokus auf Gender-Bias und Geschlechter-Gerechtigkeit (Gender, Bias_Ungleichheit). Es behandelt konkrete Strategien für faire und gender-sensible KI-Entwicklung (Fairness). Beide Bedingungen erfüllt: TECHNIK (KI_Sonstige=Ja) + SOZIAL (Gender=Ja, Bias_Ungleichheit=Ja, Fairness=Ja). Kein explizit feministischer Theoriebezug erkennbar aus dem Titel/Abstract."
323,4KMMPA6A,"Gengler, & Bodrožić-Brnić, (2024)",Faires KIPrompting – Ein Leitfaden für Unternehmen. BSP Business and Law School – Hochschule für Management und Recht,Ja,Ja,Ja,Nein,Nein,Ja,Nein,Ja,Nein,Ja,Include,,Konzept,0.78,"Das Paper behandelt Kompetenzen im Umgang mit generativer KI (Leitfaden für verantwortungsvolle Nutzung), fokussiert explizit auf Prompting und Fairness (‚faires KI-Prompting'), sowie Diversität und Bias-Mitigation. TECHNIK-Bedingung erfüllt (AI_Literacies, Generative_KI, Prompting). SOZIAL-Bedingung erfüllt (Bias_Ungleichheit, Diversitaet, Fairness). Fehlender Bezug zu Sozialer Arbeit spricht nicht gegen Inklusion, da andere soziale Kategorien substantiell adressiert werden."
324,3ZNMTJ5B,[Author not specified] (n.d.),feminist AI | ACADEMY,Ja,Nein,Nein,Nein,Nein,Ja,Nein,Nein,Ja,Ja,Include,,Konzept,0.72,"Das Material behandelt KI-Kompetenzen/Literacies durch Workshops und Schulungen zur Erstellung von 'equitable AI'. Der Titel 'feminist AI' und der explizite Fokus auf Gerechtigkeit und Fairness deuten auf feministische und Fairness-Perspektiven hin. Es erfüllt TECHNIK (AI_Literacies) und SOZIAL (Bias_Ungleichheit, Feministisch, Fairness). Allerdings: Kein Volltext vorhanden, daher nur Abstract-basierte Bewertung mit moderater Konfidenz."
325,J5EF9W6M,DIVERSIFAIR Project (2024),AI & Intersectionality: A Toolkit For Fairness & Inclusion,Ja,Nein,Ja,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Konzept,0.95,"Das Toolkit adressiert substantiell KI-Kompetenzen (Bewusstsein und Handlungsstrategien für Stakeholder), Prompting-Strategien (intersektionale Szenarien zur Bias-Aufdeckung) und KI-Systeme generell. Die intersektionale Perspektive erfüllt implizit feministische Ansätze (Crenshaw'sche Intersektionalität). Starker Fokus auf Bias_Ungleichheit, Gender, Diversität und Fairness. TECHNIK (AI_Literacies + Prompting + KI_Sonstige) und SOZIAL (Bias_Ungleichheit + Gender + Diversität + Feministisch + Fairness) beide erfüllt → Include."
326,2EBHMYU4,Ulnicane (2024),Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,Nein,Nein,Nein,Ja,Nein,Ja,Ja,Ja,Ja,Ja,Include,,Literaturreview,0.95,"Paper erfüllt beide Bedingungen: (1) TECHNIK_OK: KI_Sonstige=Ja (algorithmische Systeme und ihre sozialen Auswirkungen). (2) SOZIAL_OK: Bias_Ungleichheit=Ja (systematische Diskriminierung durch AI), Gender=Ja (Geschlechterstereotypen), Diversitaet=Ja (Diversitätskrisen in AI-Entwicklung), Feministisch=Ja (explizit intersektionale Analyse nach Crenshaw-Tradition). Substantielle Behandlung aller Kategorien, nicht beiläufig."
