---
title: "Sharma 2024 Intersectional"
original_document: Sharma_2024_Intersectional.md
document_type: Research Paper
research_domain: AI Ethics, AI Bias & Fairness
methodology: Theoretical
keywords: algorithmic bias, workarounds, user agency, HAAII-TIME model, algorithmic literacy
mini_abstract: "This paper develops a theoretical framework explaining how users detect and respond to algorithmic bias through workarounds, integrating information systems and media effects theory across four epistemic categories of bias presence and perception."
target_audience: Researchers, Policymakers, Industry
key_contributions: "Theoretical framework linking user workarounds to algorithmic bias detection"
geographic_focus: Global
publication_year: 2025
related_fields: Human-Computer Interaction, Information Systems, Media Studies
summary_date: 2025-11-07
language: English
ai_model: claude-haiku-4-5
---

# Summary: Sharma 2024 Intersectional

## Overview

This theoretical paper by Overbye-Thompson and Rice addresses a critical gap in algorithmic justice research by shifting focus from technical bias documentation to user-level adaptive responses. While extensive literature demonstrates algorithmic bias across healthcare, hiring, criminal justice, and social media platforms—including smartwatch inaccuracy for darker skin tones, facial recognition misgendering, and healthcare algorithms favoring White patients—little is known about how users actually respond to and navigate these biased systems. The authors propose that users employ strategic workarounds—adaptive behaviors to circumvent or mitigate algorithmic disadvantages—and that these responses vary significantly across four epistemic scenarios: bias that exists and is perceived, exists but is unperceived, doesn't exist but is perceived, and neither exists nor is perceived. By centering human agency alongside technological constraint, the paper contributes to more nuanced understandings of digital equity and algorithmic literacy.

## Main Findings

The paper establishes theoretical propositions predicting differential user responses across four distinct bias scenarios. When bias exists and is perceived, users develop deliberate workarounds to circumvent disadvantage. When bias exists but remains unperceived, users face particular vulnerability lacking awareness to trigger adaptive responses. When bias doesn't exist but is perceived, users may develop unnecessary workarounds that paradoxically reduce system utility. When neither exists nor is perceived, users engage systems normally without adaptation. Key conclusions include: algorithmic literacy and human agency are fundamental mechanisms for mitigating bias effects at the user level; understanding adaptive user strategies complements but does not replace technical bias-mitigation approaches; and user detection and response patterns depend on perception rather than objective bias presence alone. The framework enables prediction of context-dependent workaround behaviors and highlights that effective algorithmic accountability requires understanding user navigation strategies in everyday contexts.

## Methodology/Approach

The paper employs a theoretical framework integrating three distinct literatures: critical algorithm studies, information systems research, and media effects theory. It applies the "workarounds" concept from information systems—established mechanisms through which users adapt to, circumvent, or resist system constraints—combined with the Human-AI Interaction Theory of Interactive Media Effects (HAAII-TIME). HAAII-TIME explains user detection through "cue routes" (perceptual mechanisms by which users identify algorithmic outputs and potential bias signals) and strategy development through "action routes" (behavioral processes through which users implement adaptive responses). These frameworks intersect in a 2×2 matrix crossing actual bias presence with user perception, generating four distinct scenarios with predicted differential workaround patterns. This theoretical architecture enables systematic analysis of user responses without requiring empirical data collection, establishing propositions for future empirical validation.

## Relevant Concepts

**Algorithmic Bias**: Systematic disadvantaging of particular demographic groups through automated decision-making systems, documented across facial recognition, hiring algorithms, healthcare systems, and criminal justice applications.

**Workarounds**: User-initiated adaptive strategies to circumvent, resist, or mitigate technological constraints and system disadvantages; established concept from information systems literature.

**Algorithmic Literacy**: Users' capacity to understand, detect, and respond critically to algorithmic processes and their potential biases; essential for developing adaptive strategies.

**Cue Routes**: Perceptual mechanisms through which users detect and interpret algorithmic outputs, system behaviors, and potential bias signals in media environments.

**Action Routes**: Behavioral processes through which users develop, deliberate, and implement adaptive strategies in response to perceived algorithmic bias.

**Four-Category Epistemic Framework**: Matrix distinguishing scenarios where bias exists/doesn't exist crossed with user perception/non-perception, predicting distinct workaround behaviors in each quadrant.

## Significance

This work advances algorithmic justice scholarship by centering user agency and adaptive capacity rather than positioning users as passive victims of biased systems. By theorizing how users detect and respond to bias across different epistemic conditions, the paper provides crucial insights for developing more inclusive technologies and fostering algorithmic literacy. The framework has practical implications: technology designers should anticipate user workarounds and design systems that support rather than obstruct adaptive strategies; policymakers should recognize that algorithmic accountability requires user-level interventions alongside technical solutions; and digital equity initiatives must prioritize algorithmic literacy to enable user detection and response. Importantly, it highlights that unperceived bias represents a critical vulnerability requiring proactive disclosure mechanisms. This humanistic approach enriches ongoing debates about technological constraint and human autonomy in digital societies, positioning users as active negotiators rather than passive recipients of algorithmic mediation.
