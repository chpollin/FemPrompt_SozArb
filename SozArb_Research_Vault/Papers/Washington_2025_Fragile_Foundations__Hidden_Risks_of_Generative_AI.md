---
title: "Fragile Foundations: Hidden Risks of Generative AI"
zotero_key: BALHXHPD
author_year: "Washington (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: en
doi: "10.11586/2025078"
url: "https://www.bertelsmann-stiftung.de/doi/10.11586/2025078"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Washington_2025_Fragile.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Fragile Foundations: Hidden Risks of Generative AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ⭐ Low |
| Vulnerable Groups & Digital Equity | 2/3 | ⭐⭐ Medium |
| Bias & Discrimination Analysis | 3/3 | ⭐⭐⭐ High |
| Practical Implementation | 1/3 | ⭐ Low |
| Professional/Social Work Context | 1/3 | ⭐ Low |


## Abstract

Foundation models are the backbone of generative AI and thus central to applications such as ChatGPT, Gemini, or Copilot. However, their use comes with risks: from randomly compiled training data and opaque processes to profit-driven business models. The new report Fragile Foundations: Hidden Risks of Generative AI by the Bertelsmann Stiftung shows why mission-driven organizations in particular should critically question the foundations of AI. It highlights the systemic weaknesses of foundation models, the dangers they pose to vulnerable groups, and the possible alternatives. This report illustrates why it is critical to scrutinize foundation models. It thus offers a starting point and impetus for decision-makers and practitioners in mission-driven organizations, as well as anyone committed to a responsible digital future. Using generative AI meaningfully in the service of the common good requires a clear understanding of the technology’s foundations –and of the questions these raise for mission-driven organizations.


## AI Summary

## Overview

"Fragile Foundations: Hidden Risks of Generative AI" is a critical policy analysis by Dr. Anne L. Washington (Duke University), published by Bertelsmann Stiftung in September 2025. The document systematically examines structural vulnerabilities in foundation models—large-scale AI systems powering ChatGPT, Gemini, and similar applications. Washington argues that risks stem not from technical limitations alone but from systemic failures in data quality, business models, and computational design. The analysis addresses a critical governance gap by demonstrating how economic incentives and social biases become embedded in generative AI systems at scale, ultimately shaping digital infrastructure affecting billions of users. The work bridges academic AI ethics with actionable policy recommendations for policymakers and practitioners.

## Main Findings

Washington identifies four interconnected risk categories. **Data quality problems** reveal that training datasets contain unaddressed biases, poor curation, and representation gaps that propagate through AI systems with amplified consequences. **Business model constraints** demonstrate that proprietary development prioritizes rapid deployment and profit over safety and harm prevention. **False certainty** describes systematic user overestimation of AI reliability despite inherent limitations. **Structural barriers**—including resource-intensive computing, data monocultures, and recycled historical errors—create systemic obstacles to responsible development. Critically, foundation models amplify existing biases and automate cultural associations at unprecedented scale while externalizing environmental and computational costs. Washington proposes four solution categories: (1) **computational alternatives** improving efficiency; (2) **participatory alternatives** enabling inclusive design; (3) **source alternatives** ensuring diverse, deliberate representation; (4) **collaboration alternatives** supporting open development. Governance should emulate public libraries: transparent, deliberately curated, continuously improved, and publicly accessible rather than proprietary and static.

## Methodology/Approach

The document employs critical policy analysis synthesizing existing literature on AI risks and governance. Washington systematically categorizes structural barriers across data, business, computational, and cultural dimensions through comparative analysis contrasting current practices with proposed alternatives. The theoretical framework draws from critical AI studies, emphasizing how technical systems embed social and economic structures. Rather than conducting original empirical research, the analysis maps problems to solutions across four distinct frameworks. This methodology prioritizes accessibility for non-academic audiences while maintaining analytical rigor, bridging scholarship with implementation guidance.

## Relevant Concepts

**Foundation models:** Large-scale AI systems trained on vast datasets, versatile across applications

**Data monocultures:** Homogeneous training datasets lacking diversity, systematically amplifying embedded biases

**Deliberate representation:** Intentional, curated inclusion of diverse perspectives rather than passive diversity

**Data recycling:** Perpetuation of historical errors through retraining on contaminated datasets

**Cultural associations:** Automated encoding of social biases and stereotypes within model outputs

**Structural barriers:** Systemic obstacles—not individual failures—preventing responsible AI development

**False certainty:** User overconfidence in AI reliability despite inherent uncertainties and limitations

**Public-interest models:** Alternative governance frameworks prioritizing transparency, accountability, and continuous improvement over profit maximization

## Significance

This work significantly advances critical AI governance discourse by reframing foundation model risks as structural rather than technical problems requiring systemic reform. It challenges the assumption that algorithmic improvements ensure responsible AI, instead advocating regulatory frameworks and alternative development models. The library-based governance metaphor provides concrete institutional alternatives to proprietary approaches. By synthesizing fragmented concerns into coherent analysis with explicit problem-solution mapping, Washington advances emerging consensus that responsible AI demands social and economic transformation alongside technological innovation. The policy-oriented approach makes critical scholarship accessible to decision-makers, potentially influencing regulatory development and corporate governance. Its emphasis on participatory design, deliberate representation, and collaborative development reflects recognition that foundation model risks require structural change, not merely technical optimization.


## Links & Resources

- **DOI:** [10.11586/2025078](https://doi.org/10.11586/2025078)
- **URL:** https://www.bertelsmann-stiftung.de/doi/10.11586/2025078
- **Zotero:** [Open in Zotero](zotero://select/items/BALHXHPD)

## Related Concepts

- [[Concepts/Foundation_models|Foundation models]]
- [[Concepts/Data_monocultures|Data monocultures]]
- [[Concepts/Deliberate_representation|Deliberate representation]]
- [[Concepts/Data_recycling|Data recycling]]
- [[Concepts/Cultural_associations|Cultural associations]]
- [[Concepts/Structural_barriers|Structural barriers]]
- [[Concepts/False_certainty|False certainty]]
- [[Concepts/Public_interest_models|Public-interest models]]

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

