---
title: "Explainable Artificial Intelligence"
zotero_key: CE3C2JNF
author_year: "European Data Protection Supervisor (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: report
language: nan
doi: "nan"
url: "https://www.edps.europa.eu/system/files/2023-11/23-11-16_techdispatch_xai_en.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 2
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_European_Data_Protection_Supervisor_2023_Explainab.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Explainable Artificial Intelligence

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ⭐ Low |
| Vulnerable Groups & Digital Equity | 2/3 | ⭐⭐ Medium |
| Bias & Discrimination Analysis | 2/3 | ⭐⭐ Medium |
| Practical Implementation | 2/3 | ⭐⭐ Medium |
| Professional/Social Work Context | 1/3 | ⭐ Low |


## Abstract

Emphasizes the necessity of Explainable AI (XAI) to ensure human-centered, transparent, and ethical AI systems. By increasing transparency and comprehensibility of algorithmic decisions, XAI enables individuals—including those from marginalized groups—to participate meaningfully in digital decision-making and challenge unjust outcomes.


## AI Summary

## Overview

The EDPS TechDispatch addresses a critical governance challenge in contemporary AI deployment: the "black box" problem. This regulatory document, issued by the European Data Protection Supervisor, examines why artificial intelligence systems' opacity poses unacceptable risks in automated decision-making contexts. The document's central concern is that despite rapid AI adoption across healthcare, finance, transportation, and manufacturing sectors, many AI systems—including large language models like ChatGPT and text-to-image generators like Stable Diffusion—operate in ways that remain opaque to providers, deployers, and affected individuals alike. Crucially, the document distinguishes between general technological opacity (where users need not understand underlying mechanisms) and AI opacity in decision-making contexts (where transparency and accountability are legal imperatives). The document argues that explainability is not merely a technical preference but a mandatory legal and ethical requirement, particularly when AI systems influence consequential decisions affecting individuals' rights and opportunities.

## Main Findings

The analysis reveals several critical findings. First, AI opacity is fundamentally distinct from opacity in other technologies because AI systems make autonomous decisions affecting individuals, whereas traditional opaque technologies (like automatic transmissions) do not. Second, the document establishes that current AI systems cannot guarantee explainability even by their creators—developers themselves often cannot trace how millions of interacting parameters produce specific outputs, a technical constraint inherent to machine learning and deep learning architectures. Third, opacity directly enables two distinct harms: discrimination and misplaced trust/over-reliance. The document demonstrates discrimination through concrete examples, such as hiring algorithms inadvertently discriminating against candidates from certain demographics due to biased training data. Fourth, opacity masks systemic deficiencies including bias, inaccuracies, and "hallucinations" (AI-generated false information presented as factual). Finally, the analysis concludes that transparency and accountability are legal requirements in most jurisdictions under data protection frameworks, making opacity legally unacceptable in automated decision-making contexts, particularly for public authorities.

## Methodology/Approach

The document employs a normative-analytical approach rather than empirical research methodology. It utilizes conceptual analysis to define and contextualize the "black box effect," comparative reasoning through technology analogies to distinguish AI from other opaque systems, and systematic risk-based argumentation to establish why opacity poses unacceptable dangers. The framework identifies three distinct stakeholder groups—providers (AI developers), deployers (organizations implementing AI), and affected individuals (those subject to AI decisions)—establishing accountability relationships across the AI ecosystem. The approach implicitly references EU data protection regulatory frameworks, grounding arguments in existing legal requirements rather than proposing new ones. The document distinguishes between "automated decision-making" (fully autonomous systems) and "decision support" (human-assisted systems), recognizing different transparency requirements for each context.

## Relevant Concepts

**Black Box Effect:** The phenomenon where AI systems' decision-making processes remain opaque to all stakeholders, including developers, even when systems produce accurate outputs, resulting from complex interactions among millions of parameters.

**Explainability:** The capacity to understand and articulate how AI systems reach specific decisions, tracing the relationship between inputs and outputs—a legal requirement distinct from mere system functionality.

**Transparency:** The disclosure of AI system logic, training data sources, and decision-making processes to enable meaningful oversight and accountability.

**Automated Decision-Making:** Systems that make consequential decisions affecting individuals with minimal or no human intervention, particularly in public authority contexts, requiring mandatory transparency.

**Algorithmic Bias:** Systematic discrimination embedded in training data that causes AI systems to produce discriminatory outcomes against protected groups, often undetectable without explainability.

**Hallucinations:** AI-generated false information or inaccurate outputs presented with confidence, representing a specific category of opacity-masked deficiency.

**Training Data:** The datasets used to develop AI models, whose quality and representativeness directly determine bias propagation and system reliability.

## Significance

This document holds substantial significance for AI governance and regulatory development. It establishes explainability as a non-negotiable prerequisite for legitimate AI deployment, particularly in public sector contexts where accountability is mandatory. By positioning explainability as a legal requirement rather than optional enhancement, the EDPS contributes to the emerging regulatory consensus reflected in the EU AI Act framework. The document bridges technical constraints (inherent opacity in ML/DL systems) with legal imperatives (accountability requirements), demonstrating that innovation cannot proceed at the expense of fundamental rights protection. Its prescriptive stance prioritizes accountability over innovation flexibility, establishing a regulatory precedent that influences global AI governance discussions, institutional policy development, and organizational compliance frameworks. The document's emphasis on distinguishing AI decision-making from other opaque technologies provides a principled basis for regulatory differentiation and targeted governance approaches.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.edps.europa.eu/system/files/2023-11/23-11-16_techdispatch_xai_en.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/CE3C2JNF)

## Related Concepts

- [[Concepts/Black_Box_Effect|Black Box Effect]]
- [[Concepts/Explainability|Explainability]]
- [[Concepts/Transparency|Transparency]]
- [[Concepts/Automated_Decision_Making|Automated Decision-Making]]
- [[Concepts/Algorithmic_Bias|Algorithmic Bias]]
- [[Concepts/Hallucinations|Hallucinations]]
- [[Concepts/Training_Data|Training Data]]

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

