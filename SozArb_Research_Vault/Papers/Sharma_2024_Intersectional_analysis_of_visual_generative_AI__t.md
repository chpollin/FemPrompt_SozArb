---
title: "Intersectional analysis of visual generative AI: the case of stable diffusion"
zotero_key: 8TVALKIV
author_year: "Sharma (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://link.springer.com/article/10.1007/s00146-025-02498-1"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Sharma_2024_Intersectional.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Intersectional analysis of visual generative AI: the case of stable diffusion

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ⭐ Low |
| Vulnerable Groups & Digital Equity | 3/3 | ⭐⭐⭐ High |
| Bias & Discrimination Analysis | 3/3 | ⭐⭐⭐ High |
| Practical Implementation | 1/3 | ⭐ Low |
| Professional/Social Work Context | 1/3 | ⭐ Low |


## Abstract

This study conducts a critical intersectional analysis of Stable Diffusion, a widely used visual generative AI tool. The examination of 180 generated images shows how the system perpetuates prevailing power systems such as sexism, racism, heteronormativity, and ableism, assuming white, physically healthy, masculine-presenting individuals as the standard. The study identifies three levels of analysis: (1) the aesthetics of AI images (micro-level), (2) institutional contexts (meso-level), and (3) intersections between power systems (macro-level). The authors argue for a reparative, socially just approach to visual generative AI.


## AI Summary

## Overview

This theoretical paper by Overbye-Thompson and Rice addresses a critical gap in algorithmic justice research by shifting focus from technical bias documentation to user-level adaptive responses. While extensive literature demonstrates algorithmic bias across healthcare, hiring, criminal justice, and social media platforms—including smartwatch inaccuracy for darker skin tones, facial recognition misgendering, and healthcare algorithms favoring White patients—little is known about how users actually respond to and navigate these biased systems. The authors propose that users employ strategic workarounds—adaptive behaviors to circumvent or mitigate algorithmic disadvantages—and that these responses vary significantly across four epistemic scenarios: bias that exists and is perceived, exists but is unperceived, doesn't exist but is perceived, and neither exists nor is perceived. By centering human agency alongside technological constraint, the paper contributes to more nuanced understandings of digital equity and algorithmic literacy.

## Main Findings

The paper establishes theoretical propositions predicting differential user responses across four distinct bias scenarios. When bias exists and is perceived, users develop deliberate workarounds to circumvent disadvantage. When bias exists but remains unperceived, users face particular vulnerability lacking awareness to trigger adaptive responses. When bias doesn't exist but is perceived, users may develop unnecessary workarounds that paradoxically reduce system utility. When neither exists nor is perceived, users engage systems normally without adaptation. Key conclusions include: algorithmic literacy and human agency are fundamental mechanisms for mitigating bias effects at the user level; understanding adaptive user strategies complements but does not replace technical bias-mitigation approaches; and user detection and response patterns depend on perception rather than objective bias presence alone. The framework enables prediction of context-dependent workaround behaviors and highlights that effective algorithmic accountability requires understanding user navigation strategies in everyday contexts.

## Methodology/Approach

The paper employs a theoretical framework integrating three distinct literatures: critical algorithm studies, information systems research, and media effects theory. It applies the "workarounds" concept from information systems—established mechanisms through which users adapt to, circumvent, or resist system constraints—combined with the Human-AI Interaction Theory of Interactive Media Effects (HAAII-TIME). HAAII-TIME explains user detection through "cue routes" (perceptual mechanisms by which users identify algorithmic outputs and potential bias signals) and strategy development through "action routes" (behavioral processes through which users implement adaptive responses). These frameworks intersect in a 2×2 matrix crossing actual bias presence with user perception, generating four distinct scenarios with predicted differential workaround patterns. This theoretical architecture enables systematic analysis of user responses without requiring empirical data collection, establishing propositions for future empirical validation.

## Relevant Concepts

**Algorithmic Bias**: Systematic disadvantaging of particular demographic groups through automated decision-making systems, documented across facial recognition, hiring algorithms, healthcare systems, and criminal justice applications.

**Workarounds**: User-initiated adaptive strategies to circumvent, resist, or mitigate technological constraints and system disadvantages; established concept from information systems literature.

**Algorithmic Literacy**: Users' capacity to understand, detect, and respond critically to algorithmic processes and their potential biases; essential for developing adaptive strategies.

**Cue Routes**: Perceptual mechanisms through which users detect and interpret algorithmic outputs, system behaviors, and potential bias signals in media environments.

**Action Routes**: Behavioral processes through which users develop, deliberate, and implement adaptive strategies in response to perceived algorithmic bias.

**Four-Category Epistemic Framework**: Matrix distinguishing scenarios where bias exists/doesn't exist crossed with user perception/non-perception, predicting distinct workaround behaviors in each quadrant.

## Significance

This work advances algorithmic justice scholarship by centering user agency and adaptive capacity rather than positioning users as passive victims of biased systems. By theorizing how users detect and respond to bias across different epistemic conditions, the paper provides crucial insights for developing more inclusive technologies and fostering algorithmic literacy. The framework has practical implications: technology designers should anticipate user workarounds and design systems that support rather than obstruct adaptive strategies; policymakers should recognize that algorithmic accountability requires user-level interventions alongside technical solutions; and digital equity initiatives must prioritize algorithmic literacy to enable user detection and response. Importantly, it highlights that unperceived bias represents a critical vulnerability requiring proactive disclosure mechanisms. This humanistic approach enriches ongoing debates about technological constraint and human autonomy in digital societies, positioning users as active negotiators rather than passive recipients of algorithmic mediation.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://link.springer.com/article/10.1007/s00146-025-02498-1
- **Zotero:** [Open in Zotero](zotero://select/items/8TVALKIV)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

