---
title: "Coded injustice: Surveillance and discrimination in Denmark's automated welfare state"
zotero_key: C2MFBS5K
author_year: "Amnesty International (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://www.amnesty.org/en/documents/eur18/8709/2024/en/"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 2
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-prof-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Amnesty_International_2024_Coded.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Coded injustice: Surveillance and discrimination in Denmark's automated welfare state

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ⭐ Low |
| Vulnerable Groups & Digital Equity | 3/3 | ⭐⭐⭐ High |
| Bias & Discrimination Analysis | 3/3 | ⭐⭐⭐ High |
| Practical Implementation | 1/3 | ⭐ Low |
| Professional/Social Work Context | 2/3 | ⭐⭐ Medium |


## Abstract

Critical human rights investigation documenting how Denmark's AI-powered welfare fraud detection systems risk discriminating against people with disabilities, low-income individuals, migrants, refugees, and marginalized racial groups. Report examines up to 60 algorithmic models used to flag individuals for fraud investigations, revealing mass surveillance practices violating privacy rights and creating atmospheres of fear among welfare recipients. Key findings demonstrate how automated systems paired with extensive data collection from multiple government agencies create what approaches prohibited social scoring. Investigation reveals harmful psychological tolls on surveilled populations and argues automation exacerbates pre-existing structural inequalities rather than creating fair or efficient systems.


## AI Summary

![[summary_Amnesty_International_2024_Coded.md]]


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.amnesty.org/en/documents/eur18/8709/2024/en/
- **Zotero:** [Open in Zotero](zotero://select/items/C2MFBS5K)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

