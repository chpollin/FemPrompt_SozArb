---
title: "Challenging systematic prejudices: an Investigation into Gender Bias in Large Language Models"
zotero_key: WD9KQG9J
author_year: "UNESCO (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://discovery.ucl.ac.uk/id/eprint/10188772/1/Unesco_results.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 7

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_UNESCO_2024_Challenging.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Challenging systematic prejudices: an Investigation into Gender Bias in Large Language Models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **7/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | — None |
| Vulnerable Groups & Digital Equity | 2/3 | ⭐⭐ Medium |
| Bias & Discrimination Analysis | 3/3 | ⭐⭐⭐ High |
| Practical Implementation | 1/3 | ⭐ Low |
| Professional/Social Work Context | 1/3 | ⭐ Low |


## Abstract

This study explores biases in three significant large language models (LLMs): OpenAI’s GPT-2 and ChatGPT, along with Meta’s Llama 2, highlighting their role in both advanced decision-making systems and as user-facing conversational agents. Across multiple studies, the brief reveals how biases emerge in the text generated by LLMs, through gendered word associations, positive or negative regard for gendered subjects, or diversity in text generated by gender and culture. The research uncovers persistent social biases within these state-of-the-art language models, despite ongoing efforts to mitigate such issues. The findings underscore the critical need for continuous research and policy intervention to address the biases that exacerbate as these technologies are integrated across diverse societal and cultural landscapes. The emphasis on GPT-2 and Llama 2 being open-source foundational models is particularly noteworthy, as their widespread adoption underlines the urgent need for scalable, objective methods to assess and correct biases, ensuring fairness in AI systems globally.


## AI Summary

## Overview

This UNESCO-IRCAI study, funded by the European Union's Horizon 2020 programme and published in 2024, systematically investigates gender bias within three major large language models: OpenAI's GPT-2 and ChatGPT, and Meta's Llama 2. The research directly addresses UNESCO's Ethics of AI Recommendation, which mandates that AI actors minimize discriminatory outcomes throughout system lifecycles. The study fundamentally reframes bias not as a technical anomaly but as systematic discrimination embedded within training data, model architectures, and deployment mechanisms. Published as open-access research, the work bridges academic investigation and policy implementation, establishing gender bias as central to responsible AI governance rather than peripheral concern.

## Main Findings

The research documents a critical paradox: despite substantial industry mitigation efforts, all three examined LLMs persistently embed and amplify gender-based discrimination. Bias manifests through three primary mechanisms. First, gendered word associations reveal systematic patterns linking women and girls to stereotypical roles, occupations, and attributes within model outputs. Second, sentiment analysis demonstrates differential valuation of gendered subjects—models exhibit measurably different positive or negative regard depending on gender framing of identical scenarios. Third, cross-cultural analysis reveals that gender bias operates intersectionally, with text generation varying significantly across cultural contexts, indicating that bias compounds across identity dimensions. Critically, the study concludes that current mitigation strategies remain fundamentally insufficient, indicating bias is structural rather than incidental. The findings suggest that technical approaches alone cannot address discrimination embedded at foundational levels of model training and architecture.

## Methodology/Approach

The study employs a rigorous multi-dimensional empirical framework integrating quantitative and qualitative analysis across three complementary dimensions. Gendered word association analysis examines semantic relationships and co-occurrence patterns within model outputs, identifying systematic linguistic patterns. Sentiment analysis measures differential treatment through natural language processing techniques evaluating positive/negative valuation of gendered subjects. Cross-cultural diversity analysis assesses how text generation varies across gender and cultural categories, capturing intersectional bias patterns. This methodology deliberately bridges technical AI analysis with social science perspectives, enabling identification of meaningful harms beyond statistical patterns. The theoretical framework explicitly positions bias mitigation as governance imperative requiring policy intervention, regulatory frameworks, and systemic redesign—not merely technical optimization.

## Relevant Concepts

**Algorithmic bias**: Systematic discrimination embedded in AI systems through training data, model design, or deployment contexts producing disparate outcomes for protected groups.

**Systemic bias**: Discrimination foundational to AI architecture and training processes, requiring comprehensive intervention rather than incremental technical fixes.

**Multi-level harm**: Gender bias consequences operating simultaneously at individual level (personalized discrimination), collective level (group stereotyping), and societal level (reinforcing structural inequalities).

**Intersectionality**: Recognition that gender bias operates differently across cultural contexts and compounds with other identity dimensions, producing amplified discrimination.

**Persistent bias paradox**: The phenomenon where contemporary LLMs continue embedding discrimination despite ongoing mitigation efforts, indicating current approaches are fundamentally inadequate.

**Normative AI governance**: Policy frameworks establishing ethical requirements for AI development and deployment, positioning fairness as regulatory obligation rather than optional enhancement.

## Significance

This research holds substantial significance for AI governance, policy development, and corporate accountability. By documenting persistent bias in widely-deployed systems affecting millions of users, the study challenges assumptions that technical sophistication ensures fairness. The UNESCO/IRCAI collaboration provides institutional authority elevating findings into policy-relevant territory, with implications for international AI regulation. The interdisciplinary authorship spanning machine learning, ethics, social science, and development studies demonstrates that addressing algorithmic bias requires integrated expertise beyond computer science. The open-access publication model maximizes research impact across academic, policy, and practitioner communities. Most critically, the work establishes gender bias as central to responsible AI governance, providing evidence-based justification for regulatory intervention, mandatory bias auditing, and comprehensive mitigation strategies extending beyond current industry self-regulation practices. The research contributes to growing consensus that algorithmic fairness requires systemic solutions rather than incremental technical improvements.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://discovery.ucl.ac.uk/id/eprint/10188772/1/Unesco_results.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/WD9KQG9J)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

