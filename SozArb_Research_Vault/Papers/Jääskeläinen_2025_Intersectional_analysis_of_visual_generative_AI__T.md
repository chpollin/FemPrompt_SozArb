---
title: "Intersectional analysis of visual generative AI: The case of Stable Diffusion"
zotero_key: 9G6AQ3AA
author_year: "Jääskeläinen (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1007/s00146-025-02207-y"
url: "https://link.springer.com/article/10.1007/s00146-025-02207-y"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: true
summary_file: "summary_Jääskeläinen_2025_Intersectional.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Intersectional analysis of visual generative AI: The case of Stable Diffusion

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ⭐ Low |
| Vulnerable Groups & Digital Equity | 3/3 | ⭐⭐⭐ High |
| Bias & Discrimination Analysis | 3/3 | ⭐⭐⭐ High |
| Practical Implementation | 1/3 | ⭐ Low |
| Professional/Social Work Context | 1/3 | ⭐ Low |


## Abstract

This open-access paper provides a feminist intersectional critique of Stable Diffusion through qualitative visual analysis of 180 AI-generated images. Authors examined how power systems like racism, sexism, heteronormativity, ableism, colonialism, and capitalism are reflected in AI outputs. Found that default outputs frequently perpetuate harmful stereotypes and assume a "white, able-bodied, masculine-presenting" default subject position. Advocates for social justice-oriented approach to AI by acknowledging cultural-aesthetic biases and engaging in reparative strategies.


## AI Summary

## Overview

This paper by Jääskeläinen, Sharma, Pallett, and Åsberg presents a critical examination of Stable Diffusion (SD), a widely-adopted open-source visual generative AI tool launched in August 2022 that has generated over 12 billion images with 10 million daily active users. The research fundamentally challenges the prevailing assumption that vGenAI technologies operate as culturally neutral tools, instead arguing that these systems actively encode, reflect, and perpetuate existing societal power structures and inequalities. By analyzing 180 deliberately-prompted SD-generated images through an intersectional lens, the authors provide empirical evidence that generative AI is not neutral but functions as an active agent reproducing harmful visual politics that systematically advantage dominant groups while marginalizing others.

## Main Findings

The analysis reveals three critical and interconnected findings. First, SD-generated imagery perpetuates systemic inequalities including sexism, racism, heteronormativity, and ableism through consistent aesthetic choices and representational patterns—demonstrating that the technology actively reproduces rather than merely reflects pre-existing biases. Second, SD exhibits a persistent "default individual" assumption, with images predominantly featuring white, able-bodied, masculine-presenting subjects, indicating embedded cultural hierarchies within training data and design architecture. Third, outputs demonstrate pronounced Euro- and North America-centric cultural representations, directly traceable to the institutional origins, development contexts, and training datasets of these tools. Critically, the authors document how SD produces harmful and violent imagery targeting marginalized groups, establishing that vGenAI cannot achieve cultural or aesthetic neutrality without deliberate intervention in training data, institutional practices, and design choices.

## Methodology/Approach

The research employs qualitative, interpretative visual analysis examining 180 SD-generated images deliberately prompted across multiple intersecting axes of privilege and disadvantage—including wealth/poverty distinctions, citizen/immigrant statuses, and other social positioning variables. This systematic prompting strategy enables investigation of how the technology differentially represents social groups. The theoretical framework integrates three complementary disciplinary perspectives: Feminist Science and Technology Studies (examining power dynamics in technological development and design), visual media studies (analyzing aesthetic and representational politics), and intersectional critical theory (understanding how multiple systems of oppression simultaneously operate and reinforce one another). This interdisciplinary integration moves beyond isolated bias identification toward comprehensive understanding of intersecting power systems within visual outputs.

## Relevant Concepts

**Visual Generative AI (vGenAI)**: AI systems trained on large image datasets that generate novel photorealistic images from text prompts, including Stable Diffusion, DALL-E, and Midjourney.

**Intersectionality**: Analytical framework examining how multiple systems of oppression (race, gender, class, ability, citizenship) simultaneously operate and interact within social structures and technologies.

**Algorithmic bias**: Systematic prejudices embedded within AI systems through training data selection, design choices, and institutional contexts that disadvantage specific social groups.

**Cultural-aesthetic politics**: Ideological dimensions embedded within aesthetic choices and visual representations that reflect and reinforce particular worldviews, hierarchies, and power arrangements.

**Algorithmic reparation**: Restorative justice approach acknowledging harms caused by biased AI systems and implementing material and symbolic interventions to mend injustices against affected social groups.

**Default individual assumption**: Implicit design assumption that treats particular demographic characteristics (whiteness, able-bodiedness, masculinity) as unmarked or universal, rendering other identities as marked or exceptional.

## Significance

This paper makes substantial contributions to critical AI scholarship by centering visual and intersectional analysis—dimensions frequently overlooked in technical AI audits and fairness frameworks. It bridges humanities-centered critique with technology studies, offering essential counterweight to techno-optimist narratives claiming AI neutrality. The work advances beyond problem identification toward constructive solutions through its explicit reparative justice framework, representing an emerging normative turn in critical AI studies. The research has immediate relevance for policymakers, technologists, and researchers developing generative AI systems, providing empirical evidence that algorithmic neutrality is impossible without deliberate interventions. Most significantly, by demonstrating how vGenAI actively reproduces harmful visual politics rather than passively reflecting them, the paper establishes urgent grounds for implementing algorithmic reparation strategies, restorative justice approaches, and institutional reforms in AI development and deployment. The authors' interdisciplinary positioning—spanning media technology, environmental sciences, data science, and gender studies—strengthens the credibility of their humanities-centered critique within technology discourse.


## Links & Resources

- **DOI:** [10.1007/s00146-025-02207-y](https://doi.org/10.1007/s00146-025-02207-y)
- **URL:** https://link.springer.com/article/10.1007/s00146-025-02207-y
- **Zotero:** [Open in Zotero](zotero://select/items/9G6AQ3AA)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

