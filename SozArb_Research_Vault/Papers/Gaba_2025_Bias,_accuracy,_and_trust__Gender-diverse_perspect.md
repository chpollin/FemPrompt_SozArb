---
title: "Bias, accuracy, and trust: Gender-diverse perspectives on large language models"
zotero_key: AGNLCTFB
author_year: "Gaba (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://arxiv.org/abs/2506.21898"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Gaba_2025_Bias.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Bias, accuracy, and trust: Gender-diverse perspectives on large language models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ⭐ Low |
| Vulnerable Groups & Digital Equity | 2/3 | ⭐⭐ Medium |
| Bias & Discrimination Analysis | 3/3 | ⭐⭐⭐ High |
| Practical Implementation | 1/3 | ⭐ Low |
| Professional/Social Work Context | 1/3 | ⭐ Low |


## Abstract

Qualitative study with 25 interviews exploring how users of different gender identities perceive bias and trustworthiness in ChatGPT. Found perceived biases in LLM outputs undermine user trust, with non-binary participants encountering stereotyped responses that eroded confidence. Trust levels varied by group with male participants reporting higher trust. Participants recommended bias mitigation strategies including diversifying training data and implementing clarifying follow-up prompts to check biases.


## AI Summary

## Overview

This empirical study examines how gender-diverse populations perceive bias, accuracy, and trustworthiness in Large Language Models, specifically ChatGPT. Conducted by researchers across multiple institutions and submitted to ACM for publication in the CSCW/HCI field, the work addresses a critical gap in AI development by centering the experiences of non-binary, transgender, male, and female users. The research operates from the premise that LLMs, like other machine learning systems, reflect and perpetuate societal inequalities, particularly regarding gender representation. By investigating differential responses to gendered versus neutral prompts and analyzing user evaluations across gender identities, the study provides empirical evidence that gender identity materially shapes how individuals interact with and trust AI systems. This qualitative investigation contributes to broader conversations about responsible AI development and the necessity of inclusive design practices in technology development.

## Main Findings

The research reveals several significant patterns across three gender identity categories. Gendered prompts—those explicitly referencing or implying gender identity—consistently elicited identity-specific responses from ChatGPT, with non-binary participants experiencing particularly problematic outputs characterized by condescension and stereotyping. This vulnerability represents a critical equity concern. Perceived accuracy remained relatively consistent across gender groups, though errors concentrated in technical topics and creative tasks—suggesting domain-specific rather than gender-specific accuracy issues. Trustworthiness demonstrated substantial gender variation: male participants reported higher overall trust levels, while non-binary participants showed elevated performance-based trust, and female participants occupied intermediate positions. Participants offered actionable recommendations including diversifying training data, ensuring equitable response depth across gender categories, and implementing clarifying questions to reduce ambiguity. These findings suggest that gender bias in LLMs operates through multiple mechanisms affecting user experience and trust formation differently across populations.

## Methodology/Approach

The study employed qualitative methodology through 25 in-depth interviews with participants stratified across three gender identity categories: non-binary/transgender, male, and female. This approach allowed researchers to capture nuanced user experiences and evaluative processes. Participants systematically evaluated LLM responses to both gendered prompts (explicitly referencing gender identity) and neutral prompts (without gender references), enabling direct comparison of how prompt framing influenced outputs. The analysis was grounded in Human-Computer Interaction (HCI) and Computer-Supported Cooperative Work (CSCW) theoretical frameworks, positioning the research within established traditions of user-centered technology evaluation. The gender-critical theoretical lens situated LLM bias within broader patterns of technological discrimination, moving beyond purely technical analyses to incorporate social and contextual dimensions.

## Relevant Concepts

**Algorithmic bias:** Systematic errors in AI systems reflecting and amplifying existing societal inequalities, particularly regarding marginalized groups.

**Gender identity:** Individual's internal sense of gender, encompassing cisgender, transgender, and non-binary identities—distinct from biological sex.

**Gendered prompts:** User inputs explicitly referencing or implying gender identity, used to test whether LLMs produce identity-specific responses.

**Trust in AI:** Multi-dimensional construct encompassing reliability, competence, and performance-based confidence in automated systems.

**Inclusive design:** Development approach prioritizing diverse user perspectives and needs from inception rather than as afterthoughts.

**Responsible AI:** Framework emphasizing fairness, transparency, accountability, and stakeholder involvement in AI system development.

## Significance

This research advances algorithmic fairness scholarship by extending beyond demographic representation to examine user perception and trust formation across gender identities. By centering non-binary and transgender experiences—historically marginalized in HCI research—the study challenges normative assumptions about gender in technology design and highlights specific vulnerabilities of non-binary users to stereotypical AI outputs. The work bridges HCI/CSCW and AI ethics literatures, providing empirical evidence supporting calls for inclusive development practices. Practically, findings inform LLM improvement strategies and broader AI governance discussions. Theoretically, the study reinforces recognition that algorithmic bias requires integrated technical and social solutions, emphasizing that meaningful progress requires incorporating diverse stakeholder perspectives throughout development cycles rather than treating inclusion as a compliance requirement. The research contributes to growing recognition within the CSCW/HCI field that gender-diverse perspectives are essential for developing trustworthy AI systems.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/abs/2506.21898
- **Zotero:** [Open in Zotero](zotero://select/items/AGNLCTFB)

## Related Concepts

- [[Concepts/Algorithmic_bias|Algorithmic bias]]
- [[Concepts/Gender_identity|Gender identity]]
- [[Concepts/Gendered_prompts|Gendered prompts]]
- [[Concepts/Trust_in_AI|Trust in AI]]
- [[Concepts/Inclusive_design|Inclusive design]]
- [[Concepts/Responsible_AI|Responsible AI]]

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

