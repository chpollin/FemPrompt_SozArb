TY  - CONF
AU  - Gohar, U.
AU  - Cheng, L.
PY  - 2023
TI  - A Survey on Intersectional Fairness in Machine Learning: Opportunities and Challenges
JO  - Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23), Survey Track
DO  - 10.24963/ijcai.2023/742
AB  - Comprehensive survey examining intersectional fairness in machine learning systems beyond traditional binary fairness approaches. Presents taxonomy of intersectional fairness concepts showing how multiple sensitive attributes interact to create unique discrimination forms. Identifies challenges including data sparsity for intersectional groups and bias mitigation complexity. Demonstrates applications in NLP systems, ranking algorithms, and image recognition. Shows traditional fairness metrics fail for intersectional identities as Black women experience different discrimination than Black people or women separately.
N1  - Quality: High - Peer-reviewed in high-ranking IJCAI conference, methodologically rigorous with systematic taxonomy development, comprehensive literature analysis with 100+ references, high relevance with strong theoretical and practical contributions.
ER  -

TY  - RPRT
AU  - Wudel, A.
AU  - Ehrenberg, A.
PY  - 2025
TI  - What is Feminist AI?
JO  - Friedrich-Ebert-Stiftung Competence Centre on the Future of Work Analysis Paper
UR  - https://library.fes.de/pdf-files/bueros/bruessel/21888-20250304.pdf
AB  - Defines Feminist AI (FAI) as framework using intersectional feminism to address bias and inequalities in AI systems. Emphasizes interdisciplinary collaboration, systemic power analysis, and iterative theory-practice loops. Embeds feminist values of equality, freedom, and justice to transform AI development. Includes practical applications like FemAI advocacy for feminist perspectives in EU AI Act and MIRA diagnostic platform aligning AI tools with social justice goals. Distinguishes FAI from traditional "Responsible AI" approaches through focus on structural power inequalities rather than individual "bad actors."
N1  - Quality: High - Current 2025 publication, theoretically grounded with practical application examples, systematic methodology, connection to political decision processes (EU AI Act), high relevance for feminist AI research.
ER  -

TY  - JOUR
AU  - Shah, S. S.
PY  - 2025
TI  - Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy
JO  - Premier Journal of Artificial Intelligence
DO  - 10.70389/PJAI.1000088
AB  - Narrative review examining interplay between gender bias in AI systems and digital literacy potential for empowering women in technology. Synthesizes research from 2010-2024 analyzing systematic gender biases in AI applications across recruitment, healthcare, and financial services. These biases stem from women's underrepresentation in AI development teams (only 22% globally), biased training data, and algorithmic design decisions. Digital literacy programs show promise as intervention fostering critical awareness of AI bias, encouraging women toward AI careers, and catalyzing growth of women-led AI projects.
N1  - Quality: Medium-High - Peer-reviewed journal, systematic literature search with thematic analysis, high practical relevance, however relatively new journal with unclear impact factor, methodologically sound narrative review.
ER  -

TY  - CONF
AU  - Klein, L.
AU  - D'Ignazio, C.
PY  - 2024
TI  - Data Feminism for AI
JO  - Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT '24)
DO  - 10.1145/3630106.3658543
AB  - Presents intersectional feminist principles for just, ethical, and sustainable AI research. Extends seven Data Feminism principles to AI contexts: examine power, challenge power, rethink binaries and hierarchies, elevate emotion and embodiment, consider context, embrace pluralism, and make work visible. Proposes two additional principles on environmental impacts and consent. Framework helps identify and mitigate predictable harms before releasing discriminatory systems. Practical applications include participatory ML design processes and analysis of online advertising systems.
N1  - Quality: High - Peer-reviewed in high-ranking FAccT conference, renowned authors with established expertise, theoretically grounded with practical applications, high citation numbers expected, methodologically rigorous.
ER  -

TY  - CONF
AU  - Hartshorne, R.
AU  - Cohen, J.
PY  - 2025
TI  - Generative AI and the Future of Digital Literacy: Opportunities for Gender Inclusion
JO  - Proceedings of Society for Information Technology & Teacher Education International Conference 2025
AB  - Examines generative AI potential for promoting female inclusion in primary and secondary education while addressing inherent risks of reinforcing gender-specific biases. Qualitative research analyzes systemic effects of generative AI tools on teaching and learning through interviews and focus groups with students and teachers. Results show complex interactions between generative AI technologies and gender dynamics. Key factors for effective AI-supported learning environments include personalized learning experiences, bias-aware content generation, and teachers' role as mediators of AI interactions.
N1  - Quality: Medium-High - Peer-reviewed conference contribution, qualitative methodology, highly current 2025 publication, practical education relevance, however limited methodological details in summary.
ER  -

TY  - JOUR
AU  - Ulnicane, I.
PY  - 2024
TI  - Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action
JO  - Social Inclusion
DO  - 10.17645/si.v12.7543
AB  - Analyzes emerging intersectionality agenda in AI through examination of four high-level reports on this topic (2019-2021). Research shows how these documents frame problems and formulate recommendations for addressing inequalities. AI systems often amplify and exacerbate human biases and stereotypes, leading to discrimination and marginalization. Analysis reveals systematic problems including diversity crises in AI development where founders and employees mainly come from homogeneous groups of white men, and reinforcement of existing power relationships through AI systems.
N1  - Quality: High - Peer-reviewed in established Social Inclusion journal, systematic analysis of multiple reports, methodologically grounded, high relevance for intersectionality research in AI, well-documented methodology.
ER  -

TY  - ELEC
AU  - Articulate
PY  - 2025
TI  - How to Create Inclusive AI Images: A Guide to Bias-Free Prompting
JO  - Articulate Blog
UR  - https://www.articulate.com/blog/how-to-create-inclusive-ai-images-a-guide-to-bias-free-prompting/
AB  - Practical guide examining prompt engineering strategies for creating inclusive AI-generated images and avoiding biased default outputs. Analysis shows AI image generators often produce stereotypical representations (white, male, slim, young, physically able) due to training on unbalanced internet datasets. Presents concrete techniques for inclusive prompt engineering: specifying visible identity characteristics (age, race, gender, body size, visible disabilities), using diversity-promoting terms like "multicultural" and "gender-diverse," and providing additional context to break stereotypical associations.
N1  - Quality: Medium - Practically relevant guide with concrete application examples, however no peer-review, limited scientific methodology, high practical relevance for prompt engineering, current 2025 publication.
ER  -

TY  - UNPB
AU  - Latif, E.
AU  - Zhai, X.
AU  - Liu, L.
PY  - 2024
TI  - AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?
JO  - arXiv preprint
UR  - https://arxiv.org/html/2312.10833v4
AB  - Empirical study examining gender bias in large language models through analysis of over 6000 evaluated student responses from 70 male and 70 female participants. Research uses fine-tuned BERT models and GPT-3.5 to evaluate various training data configurations: gender-specific versus mixed datasets. Three evaluation metrics applied: Scoring Accuracy Difference for bias assessment, Mean Score Gaps (MSG) for gender disparities, and Equalized Odds (EO) for fairness measurement. Results show mixed-gender trained models produce significantly better results than gender-specific models with reduced MSG and fairer predictions.
N1  - Quality: Medium-High - Methodologically rigorous with large sample, empirically grounded, however arXiv preprint without peer-review, innovative methodology for bias measurement, high practical relevance for AI training.
ER  -

TY  - JOUR
AU  - Ahmed, U.
PY  - 2024
TI  - Feminist Perspectives on AI: Ethical Considerations in Algorithmic Decision-Making
JO  - Research Corridor Journal of Gender Studies and Intersectionality
UR  - https://www.researchcorridor.org/index.php/jgsi/article/download/330/314
AB  - Explores ethical implications of algorithmic decision-making from feminist perspective, examining data bias, discrimination in automated systems, and underrepresentation of women in AI development. Algorithmic biases disproportionately affect marginalized groups and reinforce societal inequalities in areas like hiring, healthcare, and law enforcement. Feminist ethical framework emphasizes transparency, fairness, and inclusivity while questioning patriarchal and corporate-driven narratives in AI research and policy. Analysis shows AI ethics must go beyond technical solutions to address systemic power imbalances and cultural biases in data.
N1  - Quality: Medium - Theoretically grounded with feminist perspective, however published in less established journal, good conceptual analysis, limited empirical data, high theoretical relevance.
ER  -

TY  - JOUR
AU  - Ciston, S.
PY  - 2024
TI  - Intersectional Artificial Intelligence Is Essential: Polyvocal, Multimodal, Experimental Methods to Save AI
JO  - Journal of Science and Technology of the Arts
DO  - 10.7559/CITARJ.V11I2.665
AB  - Arguments for applying intersectional strategies to AI at all levels - from data through design to implementation. Intersectionality, integrating institutional power analysis and queer-feminist plus critical race theories, can contribute to AI reconceptualization. Intersectional framework enables analysis of existing AI biases and uncovering alternative ethics from counter-narratives. Research presents intersectional strategies as polyvocal, multimodal, and experimental, with community-focused and artistic practices helping explore AI's intersectional possibilities. Practical examples include Data Nutrition Label for bias assessment in datasets and experimental projects like "ladymouth," a chatbot explaining feminism.
N1  - Quality: Medium-High - Peer-reviewed in specialized journal, innovative theoretical approaches, practical application examples, however specialized journal with unclear impact, strong conceptual contributions.
ER  -