
# **Feministische Interventionen im Algorithmus: Wie KI-Kompetenz und diversitätsreflektierendes Prompting intersektionale Gerechtigkeit fördern**

## **I. Einleitung: Die Soziotechnische Verwobenheit von KI und Diskriminierung**

### **Kontextualisierung des Problems**

Künstliche Intelligenz (KI) ist zu einer transformativen Kraft in nahezu allen Lebensbereichen geworden, von der Gesundheitsversorgung und Bildung bis hin zur Strafverfolgung und Personalbeschaffung.1 Während das Potenzial zur Verbesserung von Effizienz und zur Lösung komplexer Probleme unbestreitbar ist, geht die zunehmende Integration von KI-Systemen in gesellschaftliche Kernprozesse mit erheblichen Risiken einher. Ein zentrales Problem, das die ethische Debatte dominiert, ist die Fähigkeit dieser Technologien, bestehende soziale Ungleichheiten nicht nur abzubilden, sondern aktiv zu reproduzieren, zu verstärken und zu perpetuieren.1

Die weit verbreitete Vorstellung, KI-Systeme seien per se objektiv oder neutral, erweist sich bei genauerer Betrachtung als ein gefährlicher Mythos.1 Als von Menschen geschaffene Technologien sind KI-Systeme tief in den Werten, Annahmen und unbewussten Vorurteilen ihrer Entwickler\*innen verankert.1 Sie lernen aus Daten, die die Welt nicht so abbilden, wie sie sein sollte, sondern wie sie ist – mit all ihren historischen und strukturellen Ungerechtigkeiten. Der Grundsatz "Garbage in, garbage out" 5 verdeutlicht, dass die Qualität der Ergebnisse eines KI-Modells direkt von der Qualität der Eingabedaten abhängt. Wenn diese Daten historische Diskriminierungsmuster enthalten – etwa in Einstellungsentscheidungen oder Kreditvergaben –, wird das KI-System lernen, genau diese Muster als Norm zu betrachten und für zukünftige Entscheidungen zu reproduzieren.2

Diese Erkenntnis führt zu einer fundamentalen Neubewertung des Problems. Algorithmischer Bias ist nicht zwangsläufig eine technische Fehlfunktion oder ein "Bug", der einfach behoben werden kann. Vielmehr kann er ein logisches Ergebnis eines Systems sein, das darauf optimiert ist, Muster in historisch ungleichen Daten zu erkennen und zu extrapolieren. In diesem Sinne ist Diskriminierung durch KI weniger ein Bruch mit der gesellschaftlichen Norm als vielmehr deren algorithmische Fortsetzung. Das Problem ist somit nicht rein technischer, sondern fundamental soziotechnischer Natur. Es erfordert daher Interventionen, die über die reine Datenbereinigung oder algorithmische Anpassungen hinausgehen und die tieferen Machtstrukturen und Wissensformen adressieren, die in die Technologie eingeschrieben sind.

### **Von einfachen Biases zu intersektionalen Schäden**

Die öffentliche und wissenschaftliche Diskussion über "Bias in AI" konzentriert sich häufig auf einzelne Diskriminierungsachsen wie Geschlecht oder ethnische Zugehörigkeit.1 So wurde beispielsweise nachgewiesen, dass Gesichtserkennungssysteme bei weißen Männern deutlich besser funktionieren als bei Schwarzen Frauen oder dass KI-gestützte Recruiting-Tools systematisch weibliche Bewerberinnen benachteiligen.1 Obwohl diese Erkenntnisse wichtig sind, greifen sie zu kurz, wenn sie die Komplexität menschlicher Identitäten und Diskriminierungserfahrungen ignorieren.

An dieser Stelle wird das von der Rechtswissenschaftlerin Kimberlé Crenshaw geprägte Konzept der **Intersektionalität** zu einem unverzichtbaren analytischen Werkzeug. Intersektionalität beschreibt, wie verschiedene soziale Kategorien und Diskriminierungsformen – wie Rassismus, (Cis-)Sexismus, Ableismus, Klassismus oder Kolonialismus – nicht einfach additiv nebeneinander bestehen, sondern sich überschneiden und ineinandergreifen. Dadurch entstehen einzigartige, komplexe und sich gegenseitig verstärkende Formen der Unterdrückung und Benachteiligung.1 Eine Schwarze Frau erfährt beispielsweise nicht nur die Summe aus Rassismus und Sexismus, sondern eine spezifische Form der Diskriminierung ("Misogynoir"), die sich von den Erfahrungen Schwarzer Männer oder weißer Frauen unterscheidet.1

Wenn KI-Systeme diese intersektionalen Realitäten nicht berücksichtigen, können sie verheerende Schäden anrichten, insbesondere für bereits marginalisierte Gruppen. Zwei prominente Fallbeispiele verdeutlichen die realen Konsequenzen:

1. **Der niederländische Kinderbetreuungsskandal (2021):** Ein algorithmisches System zur Betrugserkennung bei der Vergabe von Kinderbetreuungszuschüssen stufte Tausende von Eltern fälschlicherweise als Betrüger ein. Besonders betroffen waren Familien mit Migrationshintergrund oder doppelter Staatsbürgerschaft.1 Der Algorithmus hatte gelernt, Merkmale, die mit Migration assoziiert sind, als Risikofaktoren zu werten, was zu massiven Rückforderungen, finanziellem Ruin und der Zerstörung von Familien führte. Dieser Fall zeigt eindrücklich, wie intersektionaler Bias in staatlichen Entscheidungssystemen Existenzen vernichten kann.1  
2. **Amazons KI-Recruiting-Tool (2017):** Amazon entwickelte ein KI-Tool, um Bewerbungen für technische Positionen zu sichten. Das System wurde mit den Lebensläufen der letzten zehn Jahre trainiert, die überwiegend von Männern stammten. Infolgedessen lernte die KI, Lebensläufe, die Begriffe wie "Frauen" (z.B. in "Frauen-Schachclub") enthielten, oder Absolventinnen von reinen Frauen-Colleges abzuwerten.1 Selbst nach mehreren Versuchen, diesen Gender-Bias zu korrigieren, konnte das Unternehmen nicht sicherstellen, dass das System nicht andere, subtilere Formen der Diskriminierung erlernte. Amazon entschied sich schließlich, das Tool nicht weiter zu verwenden.3 Dieser Fall demonstriert nicht nur die Perpetuierung von Sexismus, sondern auch die Grenzen rein technischer Lösungsansätze und unterstreicht die Notwendigkeit, in manchen Fällen die Entwicklung bestimmter Systeme von vornherein in Frage zu stellen.4

Diese Beispiele zeigen, dass ein unkritischer Einsatz von KI das Potenzial hat, die Rechte und Chancen von Frauen und LGBTQI+-Personen zu untergraben und bestehende Diskriminierungsformen zu zementieren.3

### **Problemstellung und Gliederung**

Die tiefgreifende und oft unsichtbare Natur von intersektionalem Bias in KI-Systemen erfordert proaktive und korrigierende Maßnahmen, die über technische Anpassungen hinausgehen. Sie erfordern eine Veränderung in der Art und Weise, wie wir über KI denken, wie wir mit ihr interagieren und wer an ihrer Gestaltung beteiligt ist. Dieser Bericht untersucht vor diesem Hintergrund die folgende zentrale Forschungsfrage: *Wie können die gezielten Interventionen von (1) feministischer Digital- und KI-Kompetenz und (2) diversitätsreflektierendem Prompting als korrigierende und präventive Maßnahmen gegen diese tief verwurzelten intersektionalen Diskriminierungsformen wirken?*

Um diese Frage umfassend zu beantworten, gliedert sich der Bericht wie folgt:

* **Abschnitt II** legt die theoretischen Fundamente, indem er Schlüsselkonzepte aus der feministischen Theorie und den Gender Studies vorstellt, die als analytische Werkzeuge zur Kritik von KI dienen.  
* **Abschnitt III** analysiert die erste Intervention: die Entwicklung einer feministisch informierten KI-Kompetenz, die über technisches Wissen hinausgeht und auf kritische Handlungsfähigkeit abzielt.  
* **Abschnitt IV** untersucht die zweite Intervention: die Praxis des diversitätsreflektierenden Promptings als direkten und ethischen Eingriffspunkt in der Interaktion mit generativer KI.  
* **Abschnitt V** führt die beiden Interventionsstränge zusammen, zeigt deren synergetisches Zusammenspiel auf und leitet daraus konkrete, strategische Handlungsempfehlungen für verschiedene Akteure ab.  
* **Abschnitt VI** schließt mit einem Fazit, das die zentralen Argumente zusammenfasst und einen Ausblick auf zukünftige Herausforderungen und Forschungsfelder gibt.

## **II. Theoretische Fundamente: Feministische Perspektiven auf Künstliche Intelligenz**

Um die Komplexität von Bias und Diskriminierung in KI-Systemen zu durchdringen, bedarf es eines robusten theoretischen Instrumentariums. Technische Ansätze zur Fairness stoßen oft an ihre Grenzen, weil sie die sozialen, politischen und historischen Kontexte, in denen Technologie entsteht und wirkt, vernachlässigen. Feministische Theorien und Konzepte aus den Gender Studies bieten hier entscheidende analytische Werkzeuge. Sie ermöglichen es, die verborgenen Annahmen, Machtstrukturen und normativen Setzungen in KI-Systemen aufzudecken und alternative, gerechtere Gestaltungsprinzipien zu formulieren. Dieser Abschnitt stellt drei zentrale theoretische Stränge vor, die das Fundament für die nachfolgende Analyse feministischer Interventionen bilden.

### **Situiertes Wissen und Standpunkttheorie (Situated Knowledge & Standpoint Theory)**

Eine der einflussreichsten feministischen Kritiken an traditionellen Wissenschaftsauffassungen stammt von der Biologin und Wissenschaftstheoretikerin Donna Haraway. In ihrem wegweisenden Aufsatz "Situated Knowledges" argumentiert sie gegen die Illusion einer gottgleichen, körperlosen Objektivität – eines "Blicks von nirgendwo" ("view from nowhere").7 Haraway postuliert, dass alles Wissen fundamental

**situiert** ist. Das bedeutet, es ist immer partiell und entsteht aus einem spezifischen materiellen, verkörperten, kulturellen, soziopolitischen und historischen Kontext.7 Anstelle einer wertneutralen, universellen Objektivität schlägt sie eine "Objektivität als positionierte Rationalität" vor, die die eigene Position reflektiert und für sie Rechenschaft ablegt.7

Die **feministische Standpunkttheorie**, die eng mit dem situierten Wissen verbunden ist, geht noch einen Schritt weiter. Sie argumentiert, dass Wissen nicht nur aus spezifischen sozialen Positionen heraus produziert wird, sondern dass die Standpunkte von marginalisierten und unterdrückten Gruppen einen epistemisch privilegierten Einblick in die Funktionsweise von Macht- und Herrschaftsstrukturen bieten.7 Wer am Rande der Gesellschaft steht, sieht die Strukturen, die für die im Zentrum Stehenden unsichtbar oder selbstverständlich sind, oft klarer.

Für die Entwicklung und Bewertung von KI-Systemen hat dieser Ansatz tiefgreifende Konsequenzen. Er entlarvt die Vorstellung, ein KI-Modell könne eine "objektive" oder "neutrale" Sicht auf die Welt liefern, als Fiktion. Das in einem Algorithmus kodifizierte "Wissen" ist immer das situierte Wissen seiner Entwickler\*innen und der Daten, auf denen es trainiert wurde – historisch gesehen überwiegend der Standpunkt weißer, westlicher, männlicher und privilegierter Gruppen.3 Die feministische Standpunkttheorie liefert somit die theoretische Begründung dafür, warum die Diversifizierung von Entwicklungsteams und die partizipatorische Einbeziehung marginalisierter Gemeinschaften nicht nur eine Frage der sozialen Gerechtigkeit oder der symbolischen Repräsentation ist, sondern eine epistemische Notwendigkeit. Die aktive Zentrierung marginalisierter Perspektiven ist eine Voraussetzung für die Schaffung robusterer, umfassenderer und letztlich "objektiverer" KI-Systeme, die Ungleichheiten nicht nur vermeiden, sondern aktiv zu deren Abbau beitragen.7

### **Performativität von Gender und Identität (Judith Butler)**

Ein weiteres wirkmächtiges Konzept aus den Gender Studies, das für die KI-Kritik fruchtbar gemacht werden kann, ist die Theorie der **Gender-Performativität** von Judith Butler. Wie von Forschenden wie Gabriele Nino und Francesca A. Lisi dargelegt, bietet dieser Ansatz eine tiefere Analyseebene als die reine Betrachtung von Gender als statischem Datenpunkt.6 Butler argumentiert, dass Gender keine feste, angeborene Eigenschaft einer Person ist, sondern ein sozialer Prozess, der durch die ständige Wiederholung von Handlungen, Gesten, Sprechakten und gesellschaftlichen Zuschreibungen performativ hergestellt und aufrechterhalten wird.6 Gender ist etwas, das man

*tut*, nicht etwas, das man *ist*.

Wendet man diese Theorie auf KI an, wird deutlich, dass algorithmische Systeme nicht nur passive Beobachter sozialer Realitäten sind, sondern aktive Teilnehmer am performativen Prozess der Herstellung von Identitätskategorien.3 Ein KI-System, das auf Anfrage stereotype Bilder von "Krankenschwestern" (weiblich) und "Ingenieuren" (männlich) generiert, bildet nicht einfach nur eine bestehende Realität ab. Es

*tut* Gender. Es zitiert, wiederholt und verfestigt patriarchale Normen und trägt so aktiv zur Stabilisierung dieser gesellschaftlichen Konfigurationen bei.6 Die Klassifikations-, Vorhersage- und Generierungsfunktionen von KI sind somit performative Akte, die soziale Realität mitgestalten.

Der berüchtigte Fall des **COMPAS-Systems**, das in der US-Justiz zur Vorhersage der Rückfallwahrscheinlichkeit von Angeklagten eingesetzt wurde, kann durch diese Linse analysiert werden. Das System sagte für Schwarze Angeklagte systematisch höhere Rückfallquoten voraus als für weiße Angeklagte mit ähnlicher krimineller Vorgeschichte.3 Aus einer performativen Perspektive hat der Algorithmus hier nicht einfach eine objektive "Gefährlichkeit" gemessen. Vielmehr hat er die soziale Kategorie "Kriminalität" performativ mit "Schwarzsein" verknüpft und damit rassistische Stereotype algorithmisch reproduziert und legitimiert. Die performative Theorie zeigt, dass die entscheidende Frage nicht nur lautet,

*ob* ein Algorithmus fair ist, sondern *was* ein Algorithmus *tut* – welche sozialen Realitäten er durch seine Operationen hervorbringt und verfestigt. Dies erfordert, die Dimension des Geschlechts und anderer Identitätsmerkmale nicht als individuelle Eigenschaft, sondern als dynamischen, relationalen und performativen Prozess zu verstehen.6

### **Ethik der Sorge (Ethics of Care) und kritischer Posthumanismus**

Als Alternative zu traditionellen ethischen Frameworks, die oft auf abstrakten Regeln (Deontologie) oder der Maximierung von Nutzen (Konsequentialismus) basieren, bietet die feministisch geprägte **Ethik der Sorge (Ethics of Care)** einen relationalen und kontextsensitiven Ansatz.9 Anstatt universelle Prinzipien anzuwenden, lenkt die Sorgeethik den Fokus auf die konkreten Beziehungen, Abhängigkeiten und Verantwortlichkeiten innerhalb eines Netzwerks. Die zentralen Fragen sind nicht "Ist diese Regel eingehalten?" oder "Was ist der größte Nutzen für die größte Zahl?", sondern "Wer wird von dieser Technologie betroffen?", "Wessen Bedürfnisse werden berücksichtigt und wessen nicht?", "Wer trägt die Verantwortung?" und "Wie wird Sorge praktiziert?".11 In der Mensch-Computer-Interaktion (HCI) und KI-Ethik bedeutet dies, den Fokus von der reinen Systemfunktionalität auf die Auswirkungen für die betroffenen Menschen und Gemeinschaften zu verlagern und Empathie, Verantwortung und Nachhaltigkeit in den Designprozess zu integrieren.5

Der **kritische Posthumanismus**, wie er von Denkerinnen wie Rosi Braidotti und Donna Haraway vertreten wird, erweitert diese Perspektive.5 Er fordert eine Abkehr von der anthropozentrischen Weltsicht, die den Menschen als Krone der Schöpfung und die Natur als zu beherrschende Ressource betrachtet. Stattdessen wird KI nicht als kontrollierendes Werkzeug des Menschen, sondern als integraler Bestandteil eines größeren, komplexen und ökologisch bewussten Gefüges verstanden.5 Dieser Ansatz dezentriert den Menschen im Designprozess und stellt die oft extraktiven und ausbeuterischen Logiken in Frage, die der Entwicklung von KI zugrunde liegen (z.B. der massive Ressourcenverbrauch für das Training großer Modelle oder die Ausbeutung von "Clickworkern" für die Datenannotation). Er fordert eine inklusivere und verantwortungsvollere Interaktion mit unserer Umwelt und stellt traditionelle anthropozentrische Narrative in Frage.5

Zusammengenommen zeigen diese theoretischen Ansätze eine fundamentale Spannung auf. Während technische Ansätze zur "KI-Fairness" oft nach universellen, quantifizierbaren Metriken suchen, wie z.B. der demografischen Parität 12, demonstriert das feministische Denken, dass Fairness niemals universell, sondern immer kontextuell, relational und politisch ist. Ein System kann eine mathematische Fairness-Metrik erfüllen und dennoch schädliche Stereotype reproduzieren (und damit eine performative Ungerechtigkeit begehen) oder die Bedürfnisse einer verletzlichen Gruppe ignorieren (und damit eine Verletzung der Sorgeethik darstellen). Dies führt zu der Schlussfolgerung, dass das Ziel nicht die Entwicklung eines einzigen, universell "fairen Algorithmus" sein kann. Vielmehr muss das Ziel darin bestehen, Prozesse, Praktiken und Werkzeuge – wie feministische KI-Kompetenz und diversitätsreflektierendes Prompting – zu schaffen, die eine fortlaufende, situierte und verhandelbare Aushandlung dessen ermöglichen, was Fairness in einem spezifischen Kontext bedeutet. Der Fokus verschiebt sich von einer statischen "Lösung" hin zu einer dynamischen "Praxis".

Die folgende Tabelle fasst die vorgestellten theoretischen Rahmenwerke und ihre Relevanz für die KI-Kritik zusammen.

**Tabelle 1: Feministische theoretische Rahmenwerke für die KI-Kritik**

| Theoretischer Rahmen | Zentrale Denker\*innen | Kernthese | Anwendung auf KI |
| :---- | :---- | :---- | :---- |
| **Situiertes Wissen & Standpunkttheorie** | Donna Haraway, Sandra Harding, Patricia Hill Collins | Wissen ist immer partiell und aus einer spezifischen sozialen Position heraus geformt. Marginalisierte Perspektiven bieten kritische, oft überlegene Einsichten in Machtstrukturen. | Entlarvt den Mythos der "neutralen" KI. Begründet die Notwendigkeit diverser Entwicklungsteams und partizipatorischer Designmethoden über symbolische Repräsentation hinaus, als epistemische Voraussetzung für gerechtere Systeme.7 |
| **Gender-Performativität** | Judith Butler | Gender und andere Identitätskategorien sind keine festen Eigenschaften, sondern werden durch wiederholte soziale und diskursive Akte performativ hergestellt. | KI-Systeme sind keine passiven Beobachter, sondern aktive Akteure, die durch Klassifikation und Generierung soziale Normen (z.B. Genderstereotype) reproduzieren und verfestigen. Die Analyse muss fragen, was die KI *tut*.3 |
| **Ethik der Sorge (Ethics of Care)** | Carol Gilligan, Maria Puig de la Bellacasa, Joan Tronto | Ethik basiert auf Beziehungen, Verantwortung und der Erfüllung konkreter Bedürfnisse, nicht auf abstrakten Regeln. Der Fokus liegt auf der Pflege von Beziehungen und der Vermeidung von Schaden. | Verlagert den Fokus von reiner Systemfunktionalität und abstrakten Fairness-Metriken auf die konkreten Auswirkungen für betroffene Individuen und Gemeinschaften. Fordert Empathie und Nachhaltigkeit im Designprozess.5 |
| **Kritischer Posthumanismus** | Rosi Braidotti, Donna Haraway | Kritisiert die anthropozentrische Weltsicht und fordert eine Dezentrierung des Menschen. Betrachtet Technologie als Teil eines größeren, vernetzten ökologischen und ethischen Gefüges. | Stellt die extraktiven Logiken der KI-Entwicklung (Ressourcenverbrauch, Ausbeutung von Arbeitskräften) in Frage. Fördert eine Sichtweise, in der KI zu einer inklusiveren und verantwortungsvolleren Interaktion mit der (Um-)Welt beiträgt.5 |

## **III. Erste Intervention: Die Entwicklung feministischer Digital- und KI-Kompetenzen**

Aufbauend auf den theoretischen Grundlagen analysiert dieser Abschnitt die erste konkrete Interventionsstrategie: die Kultivierung einer spezifisch feministisch informierten Digital- und KI-Kompetenz. Es wird argumentiert, dass ein tiefgreifendes, kritisches Verständnis der soziotechnischen Natur von KI die Voraussetzung für jede wirksame Auseinandersetzung mit algorithmischer Diskriminierung ist. Diese Form der Kompetenz zielt nicht nur auf Wissen, sondern auf Ermächtigung und Handlungsfähigkeit.

### **Neudefinition von KI-Kompetenz (AI Literacy)**

Im allgemeinen Sprachgebrauch wird KI-Kompetenz (AI Literacy) oft auf zwei Dimensionen reduziert: erstens auf die technischen Fähigkeiten zur Nutzung oder Entwicklung von KI-Anwendungen und zweitens auf ein grundlegendes konzeptionelles Verständnis der Funktionsweise von Algorithmen und maschinellem Lernen.1 Obwohl diese Aspekte wichtig sind, greifen sie zu kurz, da sie die politischen, sozialen und ethischen Dimensionen von KI weitgehend ausblenden. Sie laufen Gefahr, den Mythos der KI als neutrales Werkzeug zu perpetuieren, dessen Beherrschung lediglich eine Frage technischer Fertigkeiten sei.

Eine **feministisch informierte KI-Kompetenz** stellt diesem reduktionistischen Verständnis ein umfassenderes, kritischeres Konzept gegenüber. Sie ist fundamental eine **kritische und politische Kompetenz**. Ihr Kern besteht nicht nur darin zu wissen, *was* KI ist und *wie* sie funktioniert, sondern vor allem darin zu verstehen, *was KI tut* – welche gesellschaftlichen Auswirkungen sie hat, wessen Interessen sie dient und welche Machtstrukturen sie stützt oder untergräbt.1 Konkret umfasst eine solche Kompetenz:

* **Ein Bewusstsein für Intersektionalität:** Die Fähigkeit zu erkennen, wie KI-Systeme verschiedene Diskriminierungsachsen miteinander verknüpfen und komplexe, sich überschneidende Benachteiligungen für Menschen mit multiplen marginalisierten Identitäten schaffen können.1  
* **Die Dekonstruktion von Mythen:** Das aktive Hinterfragen und Entlarven von Narrativen wie dem der KI-Neutralität, der reinen Datengetriebenheit oder der unweigerlichen technologischen Fortschrittslogik.1  
* **Ein Verständnis der soziotechnischen Verwobenheit:** Die Erkenntnis, dass KI-Technologie untrennbar mit den sozialen, politischen, ökonomischen und historischen Kontexten verbunden ist, in denen sie entwickelt und eingesetzt wird, wie in Abschnitt II dargelegt.

Das Ziel einer feministisch informierten KI-Kompetenz ist es, Individuen und Organisationen zu befähigen, die gesellschaftlichen Auswirkungen von KI-Technologien kritisch zu analysieren und fundierte Entscheidungen über deren Einsatz und Gestaltung zu treffen.1

### **Von der Erklärbarkeit (XAI) zur Handlungsfähigkeit (Response-Ability)**

Ein zentrales Feld innerhalb der KI-Forschung, das sich mit der Transparenz von KI-Systemen beschäftigt, ist die **Explainable AI (XAI)**, also die erklärbare KI.3 Das Ziel von XAI ist es, die interne Funktionsweise von komplexen "Black Box"-Modellen (wie tiefen neuronalen Netzen) nachvollziehbar zu machen. Sie soll Antworten auf Fragen geben wie: "Warum hat das Modell diese spezifische Entscheidung getroffen?" oder "Auf Basis welcher Merkmale wurde diese Klassifikation vorgenommen?". XAI wird oft als Schlüssel zur Förderung von Vertrauen, Fairness und Rechenschaftspflicht angesehen.3

Aus einer feministischen Perspektive, wie sie insbesondere von Goda Klumbytė, Hannah Piehl und Claude Draude formuliert wird, ist reine Erklärbarkeit jedoch notwendig, aber bei weitem nicht hinreichend.7 Die Kritik an konventionellen XAI-Ansätzen lautet, dass sie oft in einem rationalistischen, kognitiven und universalistischen Modus verharren. Sie gehen von einem idealisierten, oft männlich kodierten Experten-Nutzer aus und nehmen an, dass eine logische Erklärung des Systemverhaltens automatisch zu einem fairen Ergebnis führt. Dabei wird übersehen, dass westliche Formen der Rationalität selbst kulturell spezifisch sind und dass Erklärungen auf vielfältige, auch affektive oder narrative Weisen, erfolgen können.7

Als Weiterentwicklung und Korrektiv schlagen Klumbytė et al. das Konzept der **Response-Ability** vor, das im Deutschen treffend als **Ver-Antwortungs-Fähigkeit** übersetzt werden kann.7 Dieser Begriff beschreibt die Fähigkeit von Betroffenen, auf KI-Systeme nicht nur mit Verständnis, sondern mit einer kritischen Bewertung, mit Widerspruch und mit einer fundierten Reaktion zu antworten. Es geht um die Ermächtigung der Nutzer\*innen, insbesondere derjenigen aus marginalisierten Gruppen, die Deutungshoheit über die Auswirkungen eines Systems nicht allein den Entwickler\*innen zu überlassen.

Dieser konzeptionelle Wandel von "Erklärbarkeit" zu "Handlungsfähigkeit" ist mehr als nur eine semantische Verschiebung; er repräsentiert einen fundamentalen Machtwechsel. Während traditionelle XAI die Macht bei den Entwickler\*innen belässt, die definieren, was eine "gute" Erklärung ist, verlagert Response-Ability die Macht zu den von der Technologie betroffenen Gemeinschaften. Ihre gelebte Erfahrung und ihre kritische Bewertung der Systemauswirkungen werden zu einer legitimen Quelle der Validierung oder Falsifizierung der ethischen Angemessenheit eines Systems. Ein System ist demnach nicht ethisch, weil es transparent ist, sondern weil es rechenschaftspflichtig gegenüber denen ist, die es betrifft. Die Förderung von Response-Ability erfordert daher mehr als nur verbesserte Benutzeroberflächen für Erklärungen. Sie verlangt nach formalen Mechanismen für Widerspruch, Anfechtung und partizipative Governance – und verbindet so das Konzept der Kompetenz direkt mit den Forderungen nach organisatorischen und rechtlichen Rechenschaftsstrukturen.1

### **Strategien zur Kompetenzförderung: Der "Feminist Toolbox"-Ansatz**

Die Entwicklung einer solch kritischen, feministischen KI-Kompetenz geschieht nicht von selbst. Sie erfordert gezielte Bildungsstrategien und die Bereitstellung von zugänglichen Werkzeugen und Methoden. Eine Gruppe von Forscherinnen im Bereich der Feminist Human-Computer Interaction (HCI) hat daher die Entwicklung eines **"Feminist Toolbox"** vorgeschlagen.11 Die Idee ist, einen Werkzeugkasten zu schaffen, der Forschende, Designer\*innen, Aktivist\*innen und die breite Öffentlichkeit mit den zentralen Konzepten, Methoden und epistemologischen Ansätzen der feministischen Theorie ausstattet, um ihre "feministische Lesekompetenz" (feminist literacy) im Umgang mit Technologie zu stärken.11 Ein solcher Werkzeugkasten könnte Inventare feministischer Theorien, Methoden zur Machtanalyse, Leitfäden für partizipatives Design und normkritische Perspektiven enthalten.11

Initiativen wie das europäische Projekt **DIVERSIFAIR** setzen solche Strategien bereits in die Praxis um.1 Das Projekt bringt Partner aus Wissenschaft, Industrie und Zivilgesellschaft zusammen, um das Bewusstsein für intersektionalen Bias in der KI zu schärfen. Ein zentrales Ergebnis sind Toolkits und Trainingsmaterialien, die sich an verschiedene Zielgruppen richten – Industrie, politische Entscheidungsträger\*innen und die Zivilgesellschaft.1 Diese Materialien bieten konkrete Schulungen zu KI-Ethik, Intersektionalität und den gesellschaftlichen Auswirkungen von KI an und zielen darauf ab, Wissenslücken durch eine multidisziplinäre Perspektive zu schließen.1 Solche Initiativen sind entscheidend, um die abstrakte Forderung nach feministischer KI-Kompetenz in greifbare Bildungsangebote zu übersetzen und eine neue Generation von KI-Expert\*innen auszubilden, die sowohl über technische Fähigkeiten als auch über ein tiefes Verständnis für die Identifizierung und Bekämpfung von intersektionalen Biases verfügen.1

## **IV. Zweite Intervention: Diversitätsreflektierendes Prompting als ethische Praxis**

Während feministische KI-Kompetenz die grundlegende Fähigkeit zur kritischen Analyse schafft, bietet das Prompting – die Formulierung von Anweisungen für generative KI-Modelle – einen direkten und unmittelbaren Interventionspunkt. Dieser Abschnitt untersucht, wie eine bewusste und diversitätsreflektierende Gestaltung von Prompts als praktische Anwendung kritischer KI-Kompetenz dienen kann, um stereotype und diskriminierende Ergebnisse von KI-Systemen sichtbar zu machen und aktiv zu reduzieren.

### **Die Macht des Prompts: Die Eingabeaufforderung als kognitive und ethische Rahmung**

Mit dem Aufkommen großer Sprachmodelle (Large Language Models, LLMs) wie GPT-4, Claude 3.5 oder Llama 3.1 12 ist das "Prompt Engineering" zu einer Schlüsselkompetenz geworden. Ein Prompt ist weit mehr als eine simple technische Anweisung oder eine Frage. Er fungiert als die kognitive und kommunikative Schnittstelle zwischen menschlicher Absicht und maschineller Ausgabe.18 Die Art und Weise, wie ein Prompt formuliert ist, rahmt das Problem für die KI, lenkt ihre "Aufmerksamkeit" auf bestimmte Aspekte der Daten, auf denen sie trainiert wurde, und beeinflusst maßgeblich die Art, den Ton und den Inhalt der generierten Antwort.

Da Prompts inhärent menschliche Schöpfungen sind, tragen sie unweigerlich die bewussten und unbewussten Annahmen, Werte und Vorurteile ihrer Ersteller\*innen in sich.18 Ein unreflektierter Prompt kann daher unbeabsichtigt stereotype Assoziationen im Modell aktivieren. Das aufstrebende Feld des

**"Reflexive Prompt Engineering"** fordert daher einen bewussten und kritischen Ansatz, der über die reine Optimierung auf technische Funktionalität hinausgeht. Es zielt darauf ab, ethische, rechtliche und soziale Überlegungen direkt in die Interaktion mit der KI einzubetten und erfordert ein Gleichgewicht zwischen technischer Präzision und ethischem Bewusstsein.20

### **Mechanismen der Bias-Reproduktion durch "Value-Laden Prompts"**

Die Forschung zeigt deutlich, dass Bias nicht nur aus den Trainingsdaten, sondern auch direkt aus der Interaktionsphase mit dem Modell stammen kann. Der Grundsatz "Garbage in, garbage out" 5 gilt somit nicht nur für die Daten, sondern auch für die Prompts. Sogenannte

**"value-laden prompts"** (wertgeladene Eingabeaufforderungen) sind ein primärer Mechanismus, durch den dies geschieht.

Eine Studie zu KI-gestützten Entscheidungen in Lieferketten illustriert diesen Mechanismus eindrücklich.18 Wenn eine KI mit einem neutralen Prompt wie "Liste potenzielle Lieferanten für Bauteil X auf" konfrontiert wird, könnte sie eine Liste basierend auf verschiedenen Kriterien wie Preis, Standort und Zuverlässigkeit erstellen. Ein wertgeladener Prompt wie "Finde die kostengünstigsten Lieferanten für Bauteil X" weist das Modell jedoch an, ein einziges Kriterium – den Preis – über alle anderen zu stellen. Infolgedessen könnte die KI Lieferanten mit höheren ethischen oder ökologischen Standards, die aber geringfügig teurer sind, systematisch ignorieren oder herabstufen.18

Noch problematischer sind Prompts, die implizite soziale Vorurteile enthalten. Ein Prompt wie "Erstelle eine Liste der zuverlässigsten Lieferanten aus Industrieländern" könnte dazu führen, dass fähige und wettbewerbsfähige Anbieter aus Schwellenländern von vornherein ausgeschlossen werden, selbst wenn die zugrunde liegenden Daten ihre Zuverlässigkeit belegen würden.18 Die sprachliche Rahmung im Prompt kann somit die KI dazu verleiten, diskriminierende Entscheidungen zu treffen, die über die in den Daten vorhandenen Muster hinausgehen oder diese verstärken. Dies unterstreicht, dass Prompting keine neutrale Handlung ist, sondern ein Akt der ethischen Rahmung mit weitreichenden Konsequenzen.

### **Techniken des diversitätsreflektierenden und ethischen Promptings**

Aufbauend auf einer kritischen, feministischen KI-Kompetenz lassen sich konkrete Techniken formulieren, um den beschriebenen Problemen entgegenzuwirken. Diese Techniken zielen darauf ab, die KI bewusst dazu anzuleiten, stereotype Assoziationen zu durchbrechen und inklusivere, vielfältigere und weniger voreingenommene Ergebnisse zu generieren.

* **Rollen- und Perspektivübernahme (Role-Playing / Persona Prompting):** Eine der effektivsten Methoden ist es, die KI anzuweisen, eine bestimmte Rolle oder Perspektive einzunehmen. Anstatt einer neutralen Anfrage kann man das Modell anweisen: "Antworte aus der Perspektive einer Soziologin, die auf Intersektionalität spezialisiert ist" oder "Verhalte dich wie ein Historiker, der sich der Dekolonisierung verschrieben hat".19 Diese Technik zwingt das Modell, auf spezifischere, oft weniger stereotype Teile seines Trainingsdatensatzes zuzugreifen und Antworten zu generieren, die die Nuancen und den kritischen Rahmen der zugewiesenen Rolle widerspiegeln.  
* **Iterative Verfeinerung und Feedbackschleifen:** Anstatt einen perfekten ersten Prompt zu erwarten, ist ein iterativer Prozess oft erfolgreicher. Man beginnt mit einer breiten Anfrage, bewertet die Antwort auf Stereotype und Vorurteile und verfeinert den Prompt dann schrittweise in nachfolgenden Anfragen. Zum Beispiel: "Die vorherige Antwort enthielt stereotype Darstellungen. Bitte erstelle eine neue Version, die diese vermeidet und stattdessen die Vielfalt innerhalb der beschriebenen Gruppe hervorhebt".19 Dieser Prozess ist eine Form des Dialogs mit der Maschine, der die eigenen kritischen Fähigkeiten schärft.  
* **Einsatz von expliziten Constraints und deontologischen Rahmen:** Man kann dem Modell klare positive und negative Einschränkungen (Constraints) vorgeben. Negative Constraints verbieten bestimmte Inhalte ("Erstelle eine Beschreibung ohne die Verwendung von ethnischen oder geschlechtsspezifischen Stereotypen"). Positive Constraints fordern explizit Vielfalt ein ("Stelle sicher, dass die generierten Charaktere eine Vielfalt an Hautfarben, Körpertypen und Fähigkeiten aufweisen"). Darüber hinaus kann man deontologische, also pflichtbasierte, ethische Rahmen vorgeben, die das Modell anleiten, bestimmte moralische Regeln als Selbstzweck zu befolgen, unabhängig von potenziellen Ergebnissen ("Handle stets nach dem Prinzip der Gleichbehandlung und Nicht-Diskriminierung").19  
* **Nutzung von modellgenerierten Erklärungen zur Bias-Reduktion:** Ein besonders innovativer Ansatz wurde in einer Studie von Martha Otisi Dimgba (2025) erprobt. Anstatt nur die Ausgabe zu steuern, wurde das KI-Modell im Prompt aufgefordert, eine Erklärung für seine potenziellen Biases zu liefern, *bevor* es die eigentliche Aufgabe (das Schreiben einer Erzählung über einen Beruf) ausführte. Allein die Einbeziehung dieser selbsterzeugten Reflexion in den Prompting-Prozess führte zu einer signifikanten Reduzierung von Gender- und Ethno-Bias in den finalen Narrativen um 2 % bis 20 % über verschiedene Modelle hinweg.12 Dies deutet darauf hin, dass das Aktivieren der "Reflexionsfähigkeit" eines Modells dessen Verhalten in eine gerechtere Richtung lenken kann.

Die Praxis des diversitätsreflektierenden Promptings ist jedoch mehr als nur ein "Trick" für einzelne Nutzer\*innen, um bessere Ergebnisse zu erzielen. Sie dient auch als leistungsfähiges Diagnosewerkzeug. Wenn ein Modell trotz eines sorgfältig formulierten, diversitätsreflektierenden Prompts weiterhin stereotype oder diskriminierende Inhalte produziert, offenbart dies eine tiefe, strukturelle Voreingenommenheit in seinen Trainingsdaten oder seiner Architektur. Das Scheitern des Prompts wird so zu einem wertvollen Datenpunkt für Entwickler\*innen. Systematisch eingesetzt, kann eine Suite von diversitätsreflektierenden Prompts als eine Form des "ethischen Stresstests" oder Audits dienen, um die blinden Flecken und Schwachstellen eines Modells in Bezug auf Fairness und Intersektionalität aufzudecken. Damit wird das Prompting von einer reinen Nutzerinteraktion zu einem integralen Bestandteil des gesamten KI-Lebenszyklus, einschließlich Auditierung, Debugging und ethischem Red-Teaming, bevor ein Modell überhaupt bereitgestellt wird.

Die folgende Tabelle bietet eine praktische Übersicht über diese Techniken mit konkreten Beispielen.

**Tabelle 2: Techniken für diversitätsreflektierendes Prompting**

| Technik | Beschreibung | Beispiel für einen problematischen Prompt | Beispiel für einen diversitätsreflektierenden Prompt |
| :---- | :---- | :---- | :---- |
| **Persona Prompting** | Die KI wird angewiesen, eine spezifische Rolle oder Perspektive einzunehmen, um stereotype Assoziationen zu vermeiden. | "Schreibe eine Geschichte über eine Familie." | "Du bist eine Autorin, die sich auf die Darstellung nicht-traditioneller Familienstrukturen spezialisiert hat. Schreibe eine Geschichte über eine Familie mit zwei Vätern und einem adoptierten Kind." |
| **Explizite Constraints** | Dem Modell werden klare Anweisungen gegeben, was es tun und was es unterlassen soll, um Vielfalt zu gewährleisten und Stereotype zu vermeiden. | "Erstelle ein Bild von einer Gruppe von Wissenschaftlern im Labor." | "Erstelle ein Bild von einer Gruppe von Wissenschaftlern im Labor. Stelle sicher, dass die Gruppe geschlechtlich und ethnisch divers ist, eine Person im Rollstuhl sitzt und verschiedene Altersgruppen vertreten sind." |
| **Iterative Verfeinerung** | Die Antwort der KI wird genutzt, um den Prompt in einem schrittweisen Dialog zu verbessern und unerwünschte Ergebnisse zu korrigieren. | *Initial:* "Beschreibe einen typischen Tag eines Programmierers." *KI-Antwort enthält Klischees (männlich, isoliert, Pizza).* | *Folge-Prompt:* "Deine Antwort war sehr klischeehaft. Beschreibe den Tag erneut, aber konzentriere dich diesmal auf eine Programmiererin, die Teamarbeit schätzt und eine gesunde Work-Life-Balance hat." |
| **Reflexionsaufforderung** | Das Modell wird aufgefordert, über seine eigenen potenziellen Biases nachzudenken, bevor es die Hauptaufgabe ausführt. | "Schreibe eine fiktive Biografie eines erfolgreichen Unternehmers." | "Bevor du beginnst: Reflektiere über gängige Stereotype in Bezug auf erfolgreiche Unternehmer (z.B. bezüglich Geschlecht, Ethnie, Herkunft). Schreibe nun eine fiktive Biografie, die diese Stereotype bewusst vermeidet." |

## **V. Synthese und strategische Empfehlungen für die Praxis**

Nachdem die theoretischen Grundlagen gelegt und die beiden zentralen Interventionsstrategien – feministische KI-Kompetenz und diversitätsreflektierendes Prompting – analysiert wurden, führt dieser Abschnitt die Fäden zusammen. Er zeigt auf, wie diese beiden Ansätze in einem synergetischen Verhältnis zueinander stehen und leitet daraus konkrete, an verschiedene Akteure gerichtete Handlungsempfehlungen ab. Ziel ist es, eine strategische Roadmap für die Implementierung von Fairness und intersektionaler Gerechtigkeit im gesamten KI-Ökosystem zu skizzieren.

### **Das synergetische Zusammenspiel von Kompetenz und Prompting**

Die beiden vorgestellten Interventionen sind keine isolierten Strategien, sondern bedingen und verstärken sich gegenseitig in einem positiven Kreislauf. Effektives, nuanciertes und diversitätsreflektierendes Prompting ist ohne eine zugrundeliegende feministische KI-Kompetenz kaum denkbar. Die Kompetenz liefert das kritische Bewusstsein – das "Warum" –, um überhaupt zu erkennen, an welchen Stellen Stereotype lauern, welche Fragen gestellt werden müssen und welche alternativen Narrative erforderlich sind. Ohne das Wissen um Intersektionalität, Performativität und situierte Perspektiven bleibt das Prompting an der Oberfläche und wird zu einer reinen Technik-Übung. Die Kompetenz ist der Kompass, der die Richtung für die Intervention vorgibt.

Umgekehrt ist die Praxis des Promptings ein hervorragendes Werkzeug, um die eigene kritische Kompetenz zu schärfen und zu vertiefen. Durch das gezielte Testen der Grenzen eines KI-Modells, das Analysieren seiner Antworten auf subtile Vorurteile und das iterative Verfeinern von Anweisungen entsteht ein aktiver Lernprozess. Jede Interaktion mit der KI wird zu einer kleinen Fallstudie über die Funktionsweise von algorithmischem Bias. Dieser Zyklus aus kritischer Reflexion und praktischer Anwendung macht die abstrakten theoretischen Konzepte greifbar und erfahrbar. Kompetenz und Prompting bilden somit eine untrennbare Einheit: Die Kompetenz informiert die Praxis, und die Praxis schärft die Kompetenz.

Es besteht jedoch eine erhebliche Spannung zwischen der Ermächtigung einzelner Nutzer\*innen (Bottom-up-Ansatz) und der Notwendigkeit von organisatorischer und regulatorischer Kontrolle (Top-down-Ansatz). Eine einseitige Betonung der Nutzerkompetenz birgt die Gefahr des sogenannten "Digital Washing" 1: Organisationen könnten die Verantwortung für faire Ergebnisse auf die Anwender\*innen abwälzen ("Wir stellen das Werkzeug zur Verfügung; es liegt am Nutzer, es richtig zu prompten") und sich so ihrer eigenen Verantwortung für die Entwicklung eines gerechten Systems entziehen. Eine ganzheitliche Strategie muss daher diese beiden Ansätze integrieren und eine Kultur der geteilten Verantwortung schaffen, in der Bottom-up-Kritik und Top-down-Rechenschaftspflicht sich gegenseitig stärken, anstatt sich zu ersetzen.

### **Handlungsempfehlungen für verschiedene Akteure**

Basierend auf den Erkenntnissen aus der Forschung und den praktischen Empfehlungen von Initiativen wie dem DIVERSIFAIR-Projekt 1 lassen sich spezifische Handlungsempfehlungen für verschiedene Stakeholder-Gruppen ableiten.

**Für Entwickler\*innen und Design-Teams:**

* **Partizipation und Co-Design:** Beziehen Sie marginalisierte und von der Technologie potenziell betroffene Gemeinschaften von Beginn an und während des gesamten Entwicklungsprozesses aktiv mit ein. Dies sollte über reine Feedback-Runden hinausgehen und echte Mitgestaltungsmacht umfassen.7  
* **Intersektionale Audits und ethisches Red-Teaming:** Führen Sie vor der Bereitstellung von KI-Systemen verpflichtende Audits durch, die explizit auf intersektionale Fairness prüfen. Nutzen Sie diversitätsreflektierendes Prompting als systematische Methode des "Stresstests", um Schwachstellen aufzudecken.1  
* **Aufbau epistemisch diverser Teams:** Sorgen Sie für Vielfalt in Ihren Teams nicht nur aus Gründen der Repräsentation, sondern als epistemische Notwendigkeit. Unterschiedliche gelebte Erfahrungen und Standpunkte (siehe Abschnitt II) sind entscheidend, um "blinde Flecken" im Designprozess zu erkennen.5

**Für Führungskräfte und Organisationen:**

* **Rechenschafts- und Transparenzmechanismen:** Implementieren Sie klare und transparente Berichtspflichten über die Funktionsweise und die Auswirkungen Ihrer KI-Systeme. Richten Sie unabhängige Aufsichts- und Ethikgremien ein, die die Einhaltung der Fairness-Standards überwachen und mit echten Vetorechten ausgestattet sind.1  
* **Entwicklung ethischer Risikorahmenwerke:** Erstellen Sie verbindliche "ethical risk frameworks", die nicht nur technische, sondern auch soziale und ethische Risiken bewerten und klare Handlungsanweisungen für den Umgang mit Machtdynamiken und potenziellem Schaden geben.1  
* **Investition in unternehmensweite Kompetenz:** Bieten Sie verpflichtende und kontinuierliche Schulungen zu kritischer, feministischer KI-Kompetenz für alle Mitarbeiter\*innen an, von der Entwicklung über das Management bis hin zum Marketing. Ziel muss es sein, ein gemeinsames Bewusstsein für Ethik, Intersektionalität und die gesellschaftlichen Auswirkungen von KI zu schaffen.1

**Für politische Entscheidungsträger\*innen und die Zivilgesellschaft:**

* **Förderung kritischer Medien- und KI-Kompetenz:** Unterstützen Sie breit angelegte Bildungsinitiativen in Schulen, Universitäten und der Erwachsenenbildung, die eine kritische Auseinandersetzung mit KI fördern und Mythen wie den der Neutralität dekonstruieren.1  
* **Schaffung robuster Regulierungsrahmen:** Entwickeln Sie Gesetze und Vorschriften (ähnlich dem EU AI Act), die Transparenz, Rechenschaftspflicht und unabhängige Audits für KI-Systeme, insbesondere im öffentlichen Sektor und in Hochrisikobereichen, vorschreiben.1  
* **Stärkung der Zivilgesellschaft:** Fördern Sie zivilgesellschaftliche Organisationen, Forschungsinstitute und journalistische Initiativen, die als unabhängige "Watchdogs" fungieren, KI-Systeme auf Diskriminierung untersuchen und die Öffentlichkeit über Missstände informieren.

Die folgende Tabelle fasst diese Empfehlungen zusammen und ordnet sie den relevanten Stakeholder-Gruppen zu.

**Tabelle 3: Stakeholder-spezifische Empfehlungen zur Minderung von intersektionalem Bias**

| Stakeholder-Gruppe | Strategische Priorität | Konkrete Maßnahmen | Begründung/Ziel |
| :---- | :---- | :---- | :---- |
| **Entwickler\*innen & Design-Teams** | **Gerechtigkeit durch Design (Justice by Design)** | \- Partizipatorische Methoden anwenden \- Verpflichtende Intersektionalitäts-Audits \- Diversitätsreflektierendes Prompting als Testmethode nutzen \- Diverse Teams aufbauen | Einbeziehung marginalisierter Perspektiven von Beginn an; Proaktive Identifizierung von Bias, nicht reaktive Reparatur; Nutzung von Vielfalt als epistemische Ressource.1 |
| **Führungskräfte & Organisationen** | **Organisatorische Rechenschaftspflicht** | \- Unabhängige Aufsichtsgremien einrichten \- Ethische Risikorahmenwerke entwickeln \- Transparenzberichte veröffentlichen \- Unternehmensweite Kompetenztrainings durchführen | Schaffung von verbindlichen Kontrollmechanismen; Verankerung von Ethik als Kernwert der Unternehmensstrategie; Dekonstruktion des Neutralitätsmythos auf allen Ebenen.1 |
| **HR-Fachleute** | **Faire und inklusive Prozesse** | \- KI-gestützte Recruiting-Tools auf Bias auditieren \- DIVERSIFAIR Job Ad Debiasing Tool nutzen \- Intersektionalitätsbewusstsein in Einstellungs- und Beförderungsrichtlinien fördern | Vermeidung der algorithmischen Reproduktion von Diskriminierung im Personalwesen; Sicherstellung von Chancengleichheit; Reduzierung von Fluktuation durch diskriminierungsbedingte Kündigungen.1 |
| **Politische Entscheidungsträger\*innen** | **Regulatorischer Rahmen & öffentliche Bildung** | \- KI-Kompetenz in Lehrpläne integrieren \- Strenge Vorschriften für Hochrisiko-KI erlassen \- Unabhängige zivilgesellschaftliche Forschung fördern \- Iterative Testphasen für KI im öffentlichen Dienst vorschreiben | Stärkung der "Response-Ability" der gesamten Gesellschaft; Schutz der Bürger\*innen vor algorithmischem Schaden; Sicherstellung, dass KI dem öffentlichen Interesse dient.1 |

### **Die ultimative Intervention: Die Entscheidung gegen eine KI-Anwendung**

In der Diskussion über die Minderung von Bias wird oft übersehen, dass die ethischste und verantwortungsvollste Entscheidung manchmal darin besteht, ein bestimmtes KI-System **überhaupt nicht zu entwickeln oder einzusetzen**. Expert\*innen und Organisationen wie die UNESCO weisen darauf hin, dass es Anwendungen gibt, deren Schadenspotenzial so inhärent und hoch ist, dass eine "Reparatur" oder "Entzerrung" von Bias prinzipiell unmöglich oder unzureichend ist.4

Dies gilt insbesondere für hochproblematische Anwendungsfälle wie die Vorhersage von "Kriminalität" oder "Arbeitsleistung" anhand von biometrischen Merkmalen oder die Erkennung von Emotionen oder sexueller Orientierung anhand von Gesichtsbildern. Solche Systeme basieren oft auf pseudowissenschaftlichen Annahmen und haben ein enormes Potenzial, soziale Kontrolle auszuüben und diskriminierende Praktiken unter dem Deckmantel der wissenschaftlichen Objektivität zu legitimieren.4 Der bereits erwähnte Fall des gescheiterten Amazon-Recruiting-Tools ist ein Paradebeispiel: Selbst nach wiederholten Korrekturversuchen konnte nicht ausgeschlossen werden, dass das System neue, subtilere Wege der Diskriminierung findet. Die endgültige Entscheidung, das System aufzugeben, war die einzig verantwortungsvolle.4 Eine kritische feministische KI-Kompetenz muss daher immer auch die Fähigkeit beinhalten, eine rote Linie zu ziehen und zu erkennen, wann die Nicht-Anwendung von Technologie die bessere Option ist.

## **VI. Fazit: Auf dem Weg zu gerechteren soziotechnischen Zukünften**

### **Zusammenfassung der zentralen Argumente**

Dieser Bericht hat untersucht, wie feministische Digital- und KI-Kompetenzen sowie diversitätsreflektierendes Prompting dazu beitragen können, intersektionale Diskriminierungsformen in KI-Technologien sichtbar zu machen und zu reduzieren. Die Analyse hat gezeigt, dass ein tiefgreifender Wandel in der Herangehensweise an KI-Ethik erforderlich ist, der über rein technische Lösungen hinausgeht.

Die zentralen Argumente lassen sich wie folgt zusammenfassen:

1. **KI als soziotechnisches System:** Algorithmischer Bias ist kein reiner Programmfehler, sondern oft die logische Konsequenz von Systemen, die in einer von Ungleichheit geprägten Welt auf Basis historischer Daten lernen. Die Anerkennung dieser soziotechnischen Verwobenheit ist die Grundlage für jede sinnvolle Intervention.  
2. **Die Unverzichtbarkeit feministischer Theorien:** Konzepte wie situiertes Wissen, Performativität und die Ethik der Sorge bieten ein unverzichtbares analytisches Instrumentarium. Sie entlarven den Mythos der Neutralität, decken auf, wie KI soziale Realitäten performativ mitgestaltet, und verlagern den Fokus von abstrakten Regeln auf konkrete Verantwortung und die Bedürfnisse der Betroffenen.  
3. **Von Erklärbarkeit zu Handlungsfähigkeit (Response-Ability):** Der entscheidende Schritt zur Ermächtigung liegt in der Verschiebung von reiner Transparenz (Explainability) hin zur Fähigkeit der Betroffenen, KI-Systeme kritisch zu bewerten und auf sie zu reagieren (Response-Ability). Dies demokratisiert die KI-Kritik und fordert eine grundlegende Umverteilung von Deutungsmacht.  
4. **Die Synergie von Kompetenz und Praxis:** Feministische KI-Kompetenz und diversitätsreflektierendes Prompting sind zwei Seiten derselben Medaille. Die Kompetenz liefert das kritische Bewusstsein, um zu wissen, *warum* und *wo* interveniert werden muss, während das Prompting das praktische Werkzeug für die Intervention im Kleinen darstellt. Gemeinsam bilden sie einen Kreislauf des kritischen Lernens und Handelns.

### **Ausblick und offene Forschungsfragen**

Die Arbeit an einer gerechten, inklusiven und feministischen KI ist kein Projekt mit einem definierten Endpunkt, sondern ein kontinuierlicher Prozess des Aushandelns, des Lernens und der kritischen Auseinandersetzung. Während dieser Bericht konkrete Interventionsstrategien aufgezeigt hat, bleiben wichtige Fragen für zukünftige Forschung und Praxis offen:

* **Skalierbarkeit und Implementierung:** Wie können die hier beschriebenen kritischen und oft zeitintensiven Praktiken (z.B. partizipatives Design, iterative Reflexion) in kommerziellen, agilen und schnelllebigen Entwicklungsumgebungen implementiert werden, ohne ihre kritische Substanz zu verlieren und zu einer reinen Checklisten-Ethik zu verkommen?  
* **Integration nicht-westlicher Perspektiven:** Wie können dekoloniale, postkoloniale und indigene feministische Theorien und Wissensformen noch stärker in den Diskurs und die Gestaltung von KI integriert werden? Die aktuelle Debatte ist noch stark von westlichen, eurozentrischen Perspektiven geprägt, was die Gefahr birgt, neue Formen epistemischer Hegemonie zu schaffen.7  
* **Messbarkeit und Evaluation:** Wie lässt sich der Erfolg von Interventionen, die auf Konzepte wie "Response-Ability" oder "Sorge" abzielen, jenseits traditioneller quantitativer Fairness-Metriken evaluieren? Es bedarf neuer, qualitativer und kontextsensitiver Methoden, um die tatsächlichen Auswirkungen auf das Wohlergehen von marginalisierten Gemeinschaften zu erfassen.

### **Schlusswort**

Letztlich erfordert die Gestaltung gerechterer soziotechnischer Zukünfte mehr als nur bessere Algorithmen oder umfassendere Datensätze. Sie erfordert eine Haltung, die die Forscherin Sara Ahmed als **"feministische Neugier" ("feminist wonder")** bezeichnet hat.11 Es ist eine Haltung, die von Affekt, Neugier und dem tiefen Wunsch angetrieben wird, bestehende Strukturen und Selbstverständlichkeiten immer wieder zu destabilisieren und zu hinterfragen. Es ist die Weigerung, sich mit der Antwort "So ist es eben" zufriedenzugeben, und stattdessen beharrlich zu fragen: "Warum ist es so? Und wie könnte es anders sein?". Diese unermüdliche, kritische und zugleich hoffnungsvolle Neugier ist der eigentliche Motor für die Entwicklung einer Künstlichen Intelligenz, die nicht die Ungerechtigkeiten der Vergangenheit fortschreibt, sondern aktiv zu einer gerechteren und inklusiveren Zukunft für alle beiträgt.

#### **Works cited**

1. AI & Intersectionality \- A TOOLKIT FOR FAIRNESS & INCLUSION \- DIVERSIFAIR project, accessed on July 30, 2025, [https://diversifair-project.eu/wp-content/uploads/2025/01/Copie-de-INDUSTRY-Educ-Kits-A4-V2.pdf](https://diversifair-project.eu/wp-content/uploads/2025/01/Copie-de-INDUSTRY-Educ-Kits-A4-V2.pdf)  
2. Addressing Bias and Fairness in AI- Enabled Hiring and Financial Systems \- Preprints.org, accessed on July 30, 2025, [https://www.preprints.org/frontend/manuscript/34e4139c588c3f24b78bb3d795fb3361/download\_pub](https://www.preprints.org/frontend/manuscript/34e4139c588c3f24b78bb3d795fb3361/download_pub)  
3. Exploring the Question of Bias in AI through a Gender Performative Approach \- Diamond Open, accessed on July 30, 2025, [https://diamondopen.com/journals/index.php/sgsj/article/download/735/309/5871](https://diamondopen.com/journals/index.php/sgsj/article/download/735/309/5871)  
4. AI & Intersectionality \- A TOOLKIT FOR FAIRNESS & INCLUSION \- DIVERSIFAIR project, accessed on July 30, 2025, [https://diversifair-project.eu/wp-content/uploads/2025/01/POLICY-Educ-Kits-A4-V2.pdf](https://diversifair-project.eu/wp-content/uploads/2025/01/POLICY-Educ-Kits-A4-V2.pdf)  
5. Imagine AI for Dignity – Feminine Visions for Regeneration \- Squarespace, accessed on July 30, 2025, [https://static1.squarespace.com/static/62496c6cea7721585910d2d8/t/67b31b78dbb5cb6ff149e337/1739791238540/Feminine+AI+Presentation\_web.pdf](https://static1.squarespace.com/static/62496c6cea7721585910d2d8/t/67b31b78dbb5cb6ff149e337/1739791238540/Feminine+AI+Presentation_web.pdf)  
6. Rethinking Bias and Fairness in AI Through the ... \- CEUR-WS.org, accessed on July 30, 2025, [https://ceur-ws.org/Vol-3881/paper3.pdf](https://ceur-ws.org/Vol-3881/paper3.pdf)  
7. Towards Feminist Intersectional XAI: From Explainability to ... \- arXiv, accessed on July 30, 2025, [https://arxiv.org/pdf/2305.03375](https://arxiv.org/pdf/2305.03375)  
8. Towards Feminist Intersectional XAI: From Explainability to Response-Ability \- ResearchGate, accessed on July 30, 2025, [https://www.researchgate.net/publication/370593944\_Towards\_Feminist\_Intersectional\_XAI\_From\_Explainability\_to\_Response-Ability](https://www.researchgate.net/publication/370593944_Towards_Feminist_Intersectional_XAI_From_Explainability_to_Response-Ability)  
9. The Ethics of Care and Participatory Design: A Situated Exploration \- University of Texas at Austin, accessed on July 30, 2025, [https://repositories.lib.utexas.edu/bitstreams/257017aa-8ff8-48ff-9804-4da591d99f35/download](https://repositories.lib.utexas.edu/bitstreams/257017aa-8ff8-48ff-9804-4da591d99f35/download)  
10. Caring Trouble and Musical AI: Considerations towards a ... \- arXiv, accessed on July 30, 2025, [https://arxiv.org/pdf/2311.08120](https://arxiv.org/pdf/2311.08120)  
11. MIT Open Access Articles A Toolbox of Feminist Wonder, accessed on July 30, 2025, [https://dspace.mit.edu/bitstream/handle/1721.1/152614/3584931.3611295.pdf?sequence=1\&isAllowed=y](https://dspace.mit.edu/bitstream/handle/1721.1/152614/3584931.3611295.pdf?sequence=1&isAllowed=y)  
12. Model Explanations for Gender and Ethnicity Bias Mitigation in AI-Generated Narratives \- PDXScholar, accessed on July 30, 2025, [https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=7888\&context=open\_access\_etds](https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=7888&context=open_access_etds)  
13. Impact of Artificial Intelligence on Women Empowerment \- IJNRD, accessed on July 30, 2025, [https://www.ijnrd.org/papers/IJNRD2306462.pdf](https://www.ijnrd.org/papers/IJNRD2306462.pdf)  
14. Trustworthy Machine Learning: Mitigating Bias and Promoting Fairness in Automated Decision Systems \- Computational Intelligence Group, accessed on July 30, 2025, [https://cig.fi.upm.es/wp-content/uploads/TFM\_STEPHAN\_WOLTERS.pdf](https://cig.fi.upm.es/wp-content/uploads/TFM_STEPHAN_WOLTERS.pdf)  
15. \[2305.03375\] Towards Feminist Intersectional XAI: From Explainability to Response-Ability, accessed on July 30, 2025, [https://arxiv.org/abs/2305.03375](https://arxiv.org/abs/2305.03375)  
16. A Toolbox of Feminist Wonder \- DSpace@MIT, accessed on July 30, 2025, [https://dspace.mit.edu/handle/1721.1/152614](https://dspace.mit.edu/handle/1721.1/152614)  
17. Toolkit for Industry: AI & Intersectionality \- DIVERSIFAIR project, accessed on July 30, 2025, [https://diversifair-project.eu/courses/a-toolkit-for-fairness-inclusion-industry-2/](https://diversifair-project.eu/courses/a-toolkit-for-fairness-inclusion-industry-2/)  
18. (PDF) Ethical Considerations and Bias in Prompt-Driven Supply ..., accessed on July 30, 2025, [https://www.researchgate.net/publication/393723059\_Ethical\_Considerations\_and\_Bias\_in\_Prompt-Driven\_Supply\_Chain\_Decisions](https://www.researchgate.net/publication/393723059_Ethical_Considerations_and_Bias_in_Prompt-Driven_Supply_Chain_Decisions)  
19. What is Prompt Engineering? A Detailed Guide For 2025 \- DataCamp, accessed on July 30, 2025, [https://www.datacamp.com/blog/what-is-prompt-engineering-the-future-of-ai-communication](https://www.datacamp.com/blog/what-is-prompt-engineering-the-future-of-ai-communication)  
20. (PDF) Reflexive Prompt Engineering: A Framework for Responsible ..., accessed on July 30, 2025, [https://www.researchgate.net/publication/392947686\_Reflexive\_Prompt\_Engineering\_A\_Framework\_for\_Responsible\_Prompt\_Engineering\_and\_AI\_Interaction\_Design](https://www.researchgate.net/publication/392947686_Reflexive_Prompt_Engineering_A_Framework_for_Responsible_Prompt_Engineering_and_AI_Interaction_Design)  
21. Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques \- arXiv, accessed on July 30, 2025, [https://arxiv.org/html/2506.21584v1](https://arxiv.org/html/2506.21584v1)