# Systematic Literature Review: Feminist Digital/AI Competencies and Diversity-Reflective Prompting for AI Bias Mitigation

## Feminist AI literacy and pedagogical frameworks

**Toupin, S. (2024). Shaping feminist artificial intelligence. *New Media & Society*, 26(4), 1875-1894. https://journals.sagepub.com/doi/full/10.1177/14614448221150776**

This comprehensive article examines feminist artificial intelligence (FAI) through historical analysis and contemporary typology development. Toupin provides a detailed framework categorizing FAI as: (1) model, (2) design, (3) policy, (4) culture, (5) discourse, and (6) science. The research traces FAI's evolution from Alison Adam's foundational work to contemporary initiatives like the f<A+I>er network's $2 million CAD project. Key findings include tensions between commercialized "Feminist AIâ„¢" branding and community-oriented approaches, demonstrating how FAI functions as both response to AI bias and broader political project. The work provides crucial theoretical frameworks for understanding feminist interventions in AI development while highlighting material conditions and power relations shaping feminist technology initiatives.

**Quality Assessment: High** - Published in *New Media & Society* (Impact Factor: 5.9), demonstrates rigorous theoretical framework combining historical analysis with contemporary case studies, establishes clear typology with significant pedagogical implications for feminist AI literacy education.

**Small, S. F. (2023). Generative AI and opportunities for feminist classroom assignments. *Feminist Pedagogy*, 3(5), Article 10. https://digitalcommons.calpoly.edu/feministpedagogy/vol3/iss5/10/**

This pedagogical article directly addresses how educators can integrate generative AI into feminist classroom assignments to develop reflexivity and feminist epistemology. Small challenges prevalent concerns about ChatGPT, proposing intentional feminist approaches where students collaborate with AI to develop deeper understanding of feminist knowledge production. The work demonstrates how feminist pedagogical principles can transform AI from educational threat into tool for critical learning about power, knowledge construction, and epistemology. Small's approach emphasizes that educational responses to generative AI can maintain critical feminist consciousness through intentional teaching practices, providing concrete assignment models for feminist AI literacy curricula.

**Quality Assessment: High** - Published in peer-reviewed *Feminist Pedagogy* journal, grounded in feminist pedagogical theory with practical classroom applications, directly addresses feminist AI literacy pedagogy implementation.

**Derechos Digitales. (2023). Feminist reflections for the development of Artificial Intelligence. Latin America and Caribbean Hub, f<A+I>er project. https://www.derechosdigitales.org/fair-2023-en/**

This practical guide synthesizes conversations among Latin American women developing AI systems, providing concrete methodological frameworks for feminist AI development based on four real-world projects including conversational agents for indigenous language interpretation and gender-inclusive data science. Key contributions include rethinking fundamental AI concepts through feminist lenses, identifying systemic challenges in data representation and algorithmic bias, and proposing specific principles for feminist AI development. The guide emphasizes co-design methodologies, digital autonomy, data sovereignty, and intersectional approaches, demonstrating how feminist epistemologies can inform both AI literacy education and system development through community agreements and power-balancing strategies.

**Quality Assessment: High** - Organizational publication with academic rigor supported by IDRC funding, based on systematic practitioner conversations with clear methodological frameworks, provides practical pedagogical applications for feminist AI literacy.

## Diversity-reflective prompting and technical bias mitigation

**Gallegos, I. O., Rossi, R. A., Barrow, J., Tanjim, M. M., Kim, S., Dernoncourt, F., Yu, T., Zhang, R., & Ahmed, N. K. (2024). Bias and fairness in large language models: A survey. *Computational Linguistics*, 50(3), 1097-1179. https://doi.org/10.1162/coli_a_00524**

This comprehensive survey consolidates notions of social bias and fairness in NLP, defining distinct facets of harm and introducing desiderata to operationalize fairness for LLMs. The authors propose three taxonomies: metrics for bias evaluation (embedding-based, probability-based, generated text-based), datasets for bias evaluation (counterfactual inputs/prompts), and techniques for bias mitigation classified by intervention timing. The survey includes extensive discussion of prompt-based evaluation methods and identifies how prompting strategies reveal and mitigate biases. Analysis covers over 200 studies and provides practical guidance for researchers implementing bias detection through prompting, establishing taxonomies that have become standard references for bias evaluation.

**Quality Assessment: High** - Published in top-tier *Computational Linguistics* (MIT Press), represents most comprehensive survey with rigorous methodology, extensive peer review, and significant research community impact.

**Sant, A., Escolano, C., Mash, A., De Luca Fornaciari, F., & Melero, M. (2024). The power of prompts: Evaluating and mitigating gender bias in MT with LLMs. In *Proceedings of the 5th Workshop on Gender Bias in Natural Language Processing (GeBNLP)* (pp. 94-139). https://doi.org/10.18653/v1/2024.gebnlp-1.7**

This empirical study examines gender bias in machine translation through LLMs using four widely-used test sets, comparing base LLMs against Neural Machine Translation models for English-Catalan and English-Spanish translations. The research reveals pervasive gender bias across all models, with base LLMs showing higher bias than NMT systems. Most significantly, the authors develop specific prompting engineering techniques that reduce gender bias by up to 12% on WinoMT evaluation dataset. They identify optimal prompt structures incorporating explicit fairness instructions and context-aware guidelines, providing systematic evaluation of different prompt formulations and their effectiveness in bias reduction while maintaining translation quality.

**Quality Assessment: High** - Published at ACL's premier gender bias workshop, provides concrete empirical validation of prompting techniques with quantitative bias reduction results, reproducible experimental design with practical applications.

**Chisca, A.-V., Rad, A.-C., & Lemnaru, C. (2024). Prompting fairness: Learning prompts for debiasing large language models. In *Proceedings of the Fourth Workshop on Language Technology for Equality, Diversity, Inclusion* (pp. 52-62). https://aclanthology.org/2024.ltedi-1.6/**

This paper introduces novel prompt-tuning method for reducing biases in encoder models like BERT and RoBERTa through training small sets of additional reusable token embeddings that can be concatenated to any input sequence to reduce bias in outputs. The authors particularize their approach to gender bias by providing template sets for training prompts. Evaluation on two benchmarks demonstrates state-of-the-art performance while maintaining minimal impact on language modeling capabilities. The technique represents parameter-efficient approach to bias mitigation applicable across different models and tasks without requiring full model retraining.

**Quality Assessment: High** - Peer-reviewed at specialized ACL workshop focusing on equality and inclusion, provides innovative technical contributions with empirical validation, demonstrates clear technical advancement over existing methods.

## Intersectional discrimination and compound bias in AI systems

**Wilson, K., & Caliskan, A. (2024). Gender, race, and intersectional bias in AI resume screening via language model retrieval. *Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society*, 7(1), 1578-1590. https://doi.org/10.1609/aies.v7i1.31748**

This groundbreaking empirical study analyzes bias in large language models used for resume screening, examining over 550 job descriptions and 550 resumes across multiple demographic combinations. The research reveals significant intersectional discrimination, finding that Black male candidates faced most severe disadvantages (selected 0% of the time compared to white male candidates), while bias patterns were not simply additive combinations of race and gender effects. The study validates three key hypotheses of intersectionality theory, demonstrating that Black men experience unique forms of discrimination that cannot be understood by examining race or gender in isolation. Methodology employs massive text embedding models to evaluate systematic bias patterns across over 40,000 comparisons.

**Quality Assessment: High** - Strong intersectionality theoretical framework drawing from Crenshaw's foundational work, robust empirical methodology with large-scale data analysis, direct policy relevance for employment discrimination, published in premier AI ethics venue.

**Ovalle, A., Subramonian, A., Gautam, V., Gee, G., & Chang, K. W. (2023). Factoring the matrix of domination: A critical review and reimagination of intersectionality in AI fairness. *Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society*, 704-716. https://dl.acm.org/doi/abs/10.1145/3600211.3604705**

This comprehensive critical review examines how intersectionality is conceptualized and operationalized within AI fairness research, analyzing 30 papers from the field. The authors identify significant gaps between intersectionality theory and its application in technical contexts, finding that researchers predominantly reduce intersectionality to subgroup fairness optimization rather than engaging with power structures and systemic oppression. The work proposes six key tenets for properly applying intersectionality in AI research, drawn from Patricia Hill Collins and Sirma Bilge's framework. Study reveals that most AI fairness research fails to address social context of discrimination and inadequately considers power dynamics within and beyond AI systems.

**Quality Assessment: High** - Rigorous theoretical grounding in critical intersectionality literature, systematic review methodology examining gaps in current practice, provides actionable recommendations addressing fundamental theoretical issues in the field.

**An, J., Huang, D., Lin, C., & Tai, M. (2025). Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation. *PNAS Nexus*, 4(3), pgaf089. https://doi.org/10.1093/pnasnexus/pgaf089**

This large-scale experimental study examines bias across five major LLMs using over 361,000 randomized resumes. The research reveals complex intersectional patterns where LLMs favor female candidates but discriminate against Black male candidates, with bias effects translating to 1-3 percentage point differences in hiring probabilities. The study validates intersectionality theory through three key findings: gender-ethnic stereotypes contain unique elements beyond additive combinations, racial stereotypes align more with male group members than females, and gendered race prototypes exacerbate bias against Black men. Methodology employs rigorous experimental design with randomized characteristics and comprehensive robustness testing.

**Quality Assessment: High** - Massive scale empirical study with cutting-edge LLMs, strong intersectionality theoretical framework, direct economic impact quantification, comprehensive methodological rigor, published in prestigious multidisciplinary venue.

## Critical feminist perspectives on AI technologies

**Browne, J., Cave, S., Drage, E., & McInerney, K. (Eds.). (2023). *Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligent Machines*. Oxford University Press. https://doi.org/10.1093/oso/9780192889898.001.0001**

This groundbreaking edited volume represents the first comprehensive collection bringing together leading feminist thinkers across disciplines to examine AI's societal impact. The book features 21 chapters covering topics from techno-racial capitalism to AI's military applications, examining how feminist scholarship can hold the AI sector accountable for designing systems that further social justice. Contributors span computer science, political theory, anthropology, and literature, providing diverse feminist approaches including intersectional analysis, posthuman feminism, and decolonial perspectives. The volume addresses power dynamics in AI development, algorithmic bias through feminist epistemologies, and proposes feminist frameworks for ethical AI design.

**Quality Assessment: High** - Published by prestigious Oxford University Press as open access, featuring preeminent scholars in feminist STS and AI ethics, demonstrates exceptional theoretical rigor through interdisciplinary feminist frameworks, represents most comprehensive feminist AI scholarship to date.

**Klein, L., & D'Ignazio, C. (2024). Data feminism for AI. In *Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency* (pp. 540-551). https://doi.org/10.1145/3630106.3658543**

This paper extends the influential "Data Feminism" framework to AI research, presenting intersectional feminist principles for conducting equitable, ethical, and sustainable AI research. The authors rearticulate their original seven data feminism principles specifically for AI contexts and introduce two new principles addressing environmental impact and consent. The work challenges power inequalities in AI development through feminist epistemological frameworks, emphasizing situated knowledge, challenging binary thinking, and foregrounding marginalized voices. It provides concrete methodological guidance for implementing feminist approaches in AI research and development, addressing both algorithmic bias and broader systemic inequalities.

**Quality Assessment: High** - Published at premier FAccT venue by leading scholars Catherine D'Ignazio and Lauren Klein, builds on influential "Data Feminism" work with rigorous theoretical foundations and practical applications, high policy relevance.

**Browne, J., Drage, E., & McInerney, K. (2024). Tech workers' perspectives on ethical issues in AI development: Foregrounding feminist approaches. *Big Data & Society*, 11(1). https://doi.org/10.1177/20539517231221780**

This empirical study examines tech workers' understanding of ethical AI development through feminist lens, revealing critical gaps in current AI ethics approaches. The research finds that the term "bias" creates confusion among tech workers, undermining AI ethics initiatives. Workers don't necessarily connect diversity, equality, and inclusion (DEI) agendas with AI development, suggesting significant challenges for feminist AI literacy implementation. The authors argue for moving beyond diversity narratives toward "design justice" that centers marginalized voices. For AI literacy education, this suggests need for pedagogical frameworks that explicitly connect feminist theory with technical practice, helping learners understand how gender perspectives inform both AI system development and workplace culture transformation.

**Quality Assessment: High** - Published in *Big Data & Society*, prestigious peer-reviewed journal, empirical study with clear feminist methodological framework, directly relevant to feminist AI literacy education and workplace implementation.

## Policy frameworks and global governance approaches

**UNESCO. (2021, updated 2023-2024). *Recommendation on the Ethics of Artificial Intelligence*. UNESCO Publishing. https://unesdoc.unesco.org/ark:/48223/pf0000380455**

UNESCO's first global standard on AI ethics, adopted by 193 member states, establishes comprehensive policy frameworks addressing gender equality in AI development and deployment. The recommendation explicitly addresses gender stereotyping, discriminatory biases, and need for equitable participation across the AI lifecycle. Recent implementation work includes the Women4Ethical AI platform, systematic bias studies in large language models, and policy action areas specifically targeting gender equality. The document provides concrete policy guidance for governments on data governance, education, health applications, and environment considerations with gender-sensitive approaches. Multiple follow-up reports document persistent gender biases in AI systems and propose scalable solutions for bias detection and mitigation.

**Quality Assessment: High** - Global policy standard adopted by 193 countries, comprehensive implementation framework, significant real-world policy impact, ongoing research and monitoring programs.

**UN Women. (2024). *Artificial Intelligence and gender equality* [Policy brief series]. UN Women. https://www.unwomen.org/en/articles/explainer/artificial-intelligence-and-gender-equality**

This comprehensive policy brief series analyzes how AI systems perpetuate gender inequalities while highlighting pathways for more equitable development. The analysis reveals that 44% of AI systems show gender bias, with 25% exhibiting both gender and racial bias. The briefs document systematic underrepresentation of women in AI development (only 30% of AI workforce) and provide evidence of discriminatory outcomes across hiring, healthcare, and financial services. Key policy recommendations include increased women's participation in STEM/ICT education, diverse development teams, and integration of gender expertise in AI system design. The work connects individual-level interventions with structural approaches to addressing gender bias in AI.

**Quality Assessment: High** - Authoritative UN agency source, comprehensive policy analysis with evidence-based recommendations, significant influence on international AI governance discussions.

**A+ Alliance. (2024). *Incubating Feminist AI: Executive Summary 2021-2024*. A+ Alliance. https://aplusalliance.org/incubatingfeministai2024/**

This comprehensive report documents outcomes from the $2 million CAD Feminist AI Research Network project funded by IDRC across Latin America, Middle East, and Asia. The initiative developed 12 feminist AI prototypes addressing gender-based violence, transit safety, bias detection in NLP systems, and judicial transparency. Key innovations include SafeHER (AI-driven transit safety app for Manila), E.D.I.A (bias detection tool winning Mozilla Data Futures Lab Award), and AymurAI (gender violence documentation system). The project demonstrates practical applications of feminist AI principles through community-centered design, data sovereignty approaches, and intersectional analysis capabilities, with pilots reaching thousands of users and integration into institutional systems.

**Quality Assessment: High** - Significant funding and scope, documented real-world implementations with measurable impact outcomes, innovative technical solutions, policy influence through UNESCO integration.

## Key findings and synthesis

This systematic literature review reveals several critical developments in feminist digital/AI competencies and diversity-reflective prompting for AI bias mitigation. The research demonstrates growing academic sophistication in applying feminist theoretical frameworks to understand and reshape AI development, moving beyond simple bias detection to structural analyses of power relations in technological systems.

**Theoretical advancement**: Recent scholarship shows increasing sophistication in applying intersectionality theory to AI systems, moving beyond additive models of bias to understand compound and unique forms of discrimination. Feminist epistemologies are being successfully operationalized into technical frameworks for AI development and evaluation.

**Methodological innovation**: Scholars are developing new approaches including participatory methods, intersectional auditing frameworks, and differential fairness metrics that better capture complex bias patterns. Diversity-reflective prompting techniques show measurable bias reduction while maintaining system performance.

**Global perspectives**: The literature increasingly incorporates Global South voices and decolonial approaches, challenging Western-centric assumptions about AI development and providing alternative models for feminist AI implementation.

**Policy integration**: Feminist approaches are gaining traction in international governance frameworks, with concrete implementation through UNESCO, UN Women, and other institutions demonstrating scalability and real-world impact.

**Pedagogical applications**: Emerging frameworks for feminist AI literacy education provide concrete tools for educators and practitioners, emphasizing critical digital competencies that connect technical skills with social justice awareness.

The evidence suggests that feminist digital/AI competencies and diversity-reflective prompting represent promising approaches for making bias visible and reducing intersectional discrimination in AI technologies, though significant challenges remain in scaling these approaches within existing technological and institutional structures.