ID,Zotero_Key,Author_Year,Title,DOI,Item_Type,Publication_Year,Language,Source_Tool,Abstract,URL,Decision,Exclusion_Reason,AI_Literacies,Generative_KI,Prompting,KI_Sonstige,Soziale_Arbeit,Bias_Ungleichheit,Gender,Diversitaet,Feministisch,Fairness,Studientyp,Notes
1,EJEFPZGA,[Author not specified] (2025),"Navigating the Nexus of Trust: Prompt Engineering, Professional Judgment, and the Integration of Large Language Models in Social Work",,report,2025.0,,Gemini,"Comprehensive analysis examining how prompt engineering strategies designed to enhance transparency and mitigate bias influence trust that social work professionals place in LLM-generated case recommendations. Synthesizes literature from computer science, social work, ethics, and psychology to construct understanding of complex interplay between technology, human psychology, and professional practice. Develops framework for calibrated trust through responsible prompt engineering, positioning professionals as active directors rather than passive consumers of AI outputs. Proposes that trust emerges from three-way interaction between intentional prompting strategies, professional disposition and expertise, and perceived system trustworthiness across ability, benevolence, and integrity dimensions.",,,,,,,,,,,,,,,
2,QM6L6XLZ,A+ Alliance (2024),Incubating Feminist AI: Executive Summary 2021-2024,,report,2024.0,,Claude,"Documents outcomes from $2 million CAD Feminist AI Research Network project across Latin America, Middle East, and Asia. Developed 12 feminist AI prototypes addressing gender-based violence, transit safety, bias detection in NLP systems, and judicial transparency, demonstrating practical applications of feminist AI principles through community-centered design and intersectional analysis capabilities.",https://aplusalliance.org/incubatingfeministai2024/,,,,,,,,,,,,,,
3,AFDLFCIL,Ahmed (2024),Feminist Perspectives on AI: Ethical Considerations in Algorithmic Decision-Making,,journalArticle,2024.0,,Perplexity,"Explores ethical implications of algorithmic decision-making from feminist perspective, examining data bias, discrimination in automated systems, and underrepresentation of women in AI development. Algorithmic biases disproportionately affect marginalized groups and reinforce societal inequalities in areas like hiring, healthcare, and law enforcement. Feminist ethical framework emphasizes transparency, fairness, and inclusivity while questioning patriarchal and corporate-driven narratives in AI research and policy. Analysis shows AI ethics must go beyond technical solutions to address systemic power imbalances and cultural biases in data.",https://www.researchcorridor.org/index.php/jgsi/article/download/330/314,,,,,,,,,,,,,,
4,5G2QTZYD,Ahmed (2024),Feminist perspectives on AI: Ethical considerations in algorithmic decision-making,,journalArticle,2024.0,,Perplexity,"Theoretische Arbeit zur feministischen KI-Ethik. Argumentiert, dass KI strukturell eingebettete Diskriminierung reproduziert und plädiert für partizipative, intersektionale Entwicklungspraxis.",https://www.researchcorridor.org/index.php/jgsi/article/download/330/314,,,,,,,,,,,,,,
5,CY5IMM6G,Ahn (2025),Artificial Intelligence (AI) literacy for social work: Implications for core competencies,10.1086/735187,journalArticle,2025.0,,Claude,"Comprehensive framework integrating AI literacy into CSWE's nine core competencies, arguing AI understanding is essential for recognizing algorithmic bias perpetuation of social inequalities, contributing to ethical governance, and thoughtfully integrating technology into practice. Emphasizes AI literacy enables social workers to recognize algorithmic bias, advocate for individuals navigating AI-driven systems, and address emerging vulnerabilities including digital divides and discriminatory practices across housing, hiring, and service delivery.",,,,,,,,,,,,,,,
6,ZJS2J7E7,Ahn (2025),Artificial Intelligence (AI) Literacy for Social Work: Implications for Core Competencies,10.1086/735187,journalArticle,2025.0,,ChatGPT,"Conceptual paper arguing that AI literacy should be a core social work competency. It links “algorithmic literacy” to existing educational standards and details how AI systems can amplify inequities without critical oversight. The authors propose integrating critical evaluation of AI tools, bias detection, and ethical safeguards into curricula and practice to protect vulnerable populations and uphold social justice.",https://doi.org/10.1086/735187](https://doi.org/10.1086/735187,,,,,,,,,,,,,,
7,THVKEETD,Ahn (2025),Artificial Intelligence (AI) literacy for social work,10.1086/735187,journalArticle,2025.0,,Perplexity,"This work explores how AI literacy, defined as the knowledge and skills to understand, use, and critically evaluate AI systems, can enhance social workers' ability to navigate technological integration while maintaining professional values. It emphasizes that embedding AI literacy into core competencies enables social workers to better address emerging challenges and promote equity in an AI-influenced society. The author argues that AI literacy must go beyond technical skills to include critical evaluation of AI's social implications and potential for bias, advocating for comprehensive professional development that prepares social workers to engage with AI tools ethically and effectively.",https://doi.org/10.1086/735187,,,,,,,,,,,,,,
8,JHRVDXSD,Ahn (2025),Artificial Intelligence (AI) Literacy for Social Work: Implications for Core Competencies,10.1086/735187,journalArticle,2025.0,en,Manual,,https://www.journals.uchicago.edu/doi/10.1086/735187,,,,,,,,,,,,,,
9,DJEVSR8D,Ahrweiler (2025),AI FORA – Artificial Intelligence for Assessment: Fairness bei der Verteilung öffentlicher sozialer Leistungen,,report,2025.0,,Perplexity,"This international research project examined AI-supported social assessments across nine countries on four continents. The study demonstrates that justice criteria for receiving state benefits are culture- and context-dependent. A central finding is that deploying a standardized AI system globally is insufficient; instead, flexible, dynamic, and adaptive systems are required. Development of such systems depends on contributions from all societal actors, including vulnerable groups, for designing participatory, context-specific, and fair AI.",https://nachrichten.idw-online.de/2025/03/18/wie-koennte-kuenstliche-intelligenz-weltweit-fairness-bei-der-verteilung-oeffentlicher-sozialer-leistungen-erhoehen,,,,,,,,,,,,,,
10,PYN6HB3E,Alam (2025),"Social work in the age of artificial intelligence: A rights-based framework for evidence-based practice through social psychology, group dynamics, and institutional analysis",10.1080/26408066.2025.2547219,journalArticle,2025.0,,Perplexity,"This study develops a comprehensive rights-based framework for navigating AI integration in social work practice while addressing ethical implications across micro, meso, and macro practice levels. The framework bridges social work theory with interdisciplinary insights, demonstrating that AI systems profoundly impact vulnerable populations by mediating interpersonal relationships and constructing meaning in AI-mediated environments. It provides evidence-based guidance for practitioners to harness AI's potential while safeguarding core social work values of human dignity, self-determination, and social justice, offering concrete strategies for social work education and research methodologies that center community voices.",https://doi.org/10.1080/26408066.2025.2547219,,,,,,,,,,,,,,
11,5F7D9PEB,Alvarez (2024),Policy advice and best practices on bias and fairness in AI,10.1007/s10676-024-09746-w,journalArticle,2024.0,,ChatGPT,"This open-access paper provides a comprehensive overview of fairness in AI, bridging technical bias mitigation methods with legal and policy considerations. Alvarez et al. survey the state-of-the-art in fair AI techniques and review major policy initiatives and standards on algorithmic bias. A key contribution is the NoBIAS architecture introduced in the paper, which comprises a “Legal Layer” (focusing on EU non-discrimination law and human rights requirements) and a “Bias Management Layer” (covering bias understanding, mitigation, and accountability). The authors note that AI systems have produced real-world harms, including illegal discrimination against protected groups, and they highlight challenges such as intersectional discrimination that current EU law does not explicitly address. By organizing existing knowledge and best practices, the article guides researchers and practitioners in aligning technical solutions with ethical and legal norms – underscoring that managing AI bias requires not just algorithmic techniques but also adherence to equality principles and governance frameworks.",https://link.springer.com/article/10.1007/s10676-024-09746-w,,,,,,,,,,,,,,
12,LXDG4KQK,Amnesty International (2024),Coded injustice: Surveillance and discrimination in Denmark's automated welfare state,,report,2024.0,,Claude,"Critical human rights investigation documenting how Denmark's AI-powered welfare fraud detection systems risk discriminating against people with disabilities, low-income individuals, migrants, refugees, and marginalized racial groups. Report examines up to 60 algorithmic models used to flag individuals for fraud investigations, revealing mass surveillance practices violating privacy rights and creating atmospheres of fear among welfare recipients. Key findings demonstrate how automated systems paired with extensive data collection from multiple government agencies create what approaches prohibited social scoring. Investigation reveals harmful psychological tolls on surveilled populations and argues automation exacerbates pre-existing structural inequalities rather than creating fair or efficient systems.",https://www.amnesty.org/en/documents/eur18/8709/2024/en/,,,,,,,,,,,,,,
13,QDUGSBPC,An (2025),Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation,10.1093/pnasnexus/pgaf089,journalArticle,2025.0,,ChatGPT,"Examines intersectional bias in LLM-based decision-making using AI-driven resume screening context. Multiple recent LLMs scored ~361,000 synthetic entry-level job resumes with systematically varied demographic attributes. Revealed significant biases: female candidates received higher competence scores than equally qualified males, while Black male candidates received markedly lower scores, demonstrating intersectional effects.",https://doi.org/10.1093/pnasnexus/pgaf089,,,,,,,,,,,,,,
14,SQ38TTWQ,An (2025),Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation,10.1093/pnasnexus/pgaf089,journalArticle,2025.0,,Claude,"Large-scale experimental study examining bias across five major LLMs using over 361,000 randomized resumes. Reveals complex intersectional patterns where LLMs favor female candidates but discriminate against Black male candidates, with bias effects translating to 1-3 percentage point differences in hiring probabilities, validating intersectionality theory through three key findings.",https://doi.org/10.1093/pnasnexus/pgaf089,,,,,,,,,,,,,,
15,SY8LNID7,Arias López (2023),Digital literacy as a new determinant of health: A scoping review,10.1371/journal.pdig.0000279,journalArticle,2023.0,en,Manual,"Introduction
              Harnessing new digital technologies can improve access to health care but can also widen the health divide for those with poor digital literacy. This scoping review aims to assess the current situation of low digital health literacy in terms of its definition, reach, impact on health and interventions for its mitigation.
            
            
              Methods
              A comprehensive literature search strategy was composed by a qualified medical librarian. Literature databases [Medline (Ovid), Embase (Ovid), Scopus, and Google Scholar] were queried using appropriate natural language and controlled vocabulary terms along with hand-searching and citation chaining. We focused on recent and highly cited references published in English. Reviews were excluded. This scoping review was conducted following the methodological framework of Arksey and O’Malley.
            
            
              Results
              A total of 268 articles were identified (263 from the initial search and 5 more added from the references of the original papers), 53 of which were finally selected for full text analysis. Digital health literacy is the most frequently used descriptor to refer to the ability to find and use health information with the goal of addressing or solving a health problem using technology. The most utilized tool to assess digital health literacy is the eHealth literacy scale (eHEALS), a self-reported measurement tool that evaluates six core dimensions and is available in various languages. Individuals with higher digital health literacy scores have better self-management and participation in their own medical decisions, mental and psychological state and quality of life. Effective interventions addressing poor digital health literacy included education/training and social support.
            
            
              Conclusions
              Although there is interest in the study and impact of poor digital health literacy, there is still a long way to go to improve measurement tools and find effective interventions to reduce the digital health divide.",https://dx.plos.org/10.1371/journal.pdig.0000279,,,,,,,,,,,,,,
16,AIGLDZ4C,Articulate (2025),How to Create Inclusive AI Images: A Guide to Bias-Free Prompting,,webpage,2025.0,,Perplexity,"Practical guide examining prompt engineering strategies for creating inclusive AI-generated images and avoiding biased default outputs. Analysis shows AI image generators often produce stereotypical representations (white, male, slim, young, physically able) due to training on unbalanced internet datasets. Presents concrete techniques for inclusive prompt engineering: specifying visible identity characteristics (age, race, gender, body size, visible disabilities), using diversity-promoting terms like ""multicultural"" and ""gender-diverse,"" and providing additional context to break stereotypical associations.",https://www.articulate.com/blog/how-to-create-inclusive-ai-images-a-guide-to-bias-free-prompting/,,,,,,,,,,,,,,
17,GUMWKBN6,Asseri (2024),Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,10.48550/arXiv.2506.18199,journalArticle,2024.0,,Perplexity,"Systematic review following PRISMA guidelines analyzing prompt-engineering techniques to reduce cultural bias against Arabs and Muslims in LLMs, evaluating eight empirical studies (2021-2024). Identifies five primary approaches: Cultural Prompting, Affective Priming, Self-Debiasing techniques, structured multi-stage pipelines, and parameter-optimized continuous prompts. Structured multi-stage pipelines most effective with up to 87.7% bias reduction. Emphasizes accessibility of prompt engineering for bias mitigation and importance of transparent methodology for trust building.",https://doi.org/10.48550/arXiv.2506.18199,,,,,,,,,,,,,,
18,25XSMXKT,Asseri (2024),Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,10.48550/arXiv.2506.18199,journalArticle,2024.0,,ChatGPT,"Systematic review following PRISMA guidelines analyzing prompt-engineering techniques to reduce cultural bias against Arabs and Muslims in LLMs, evaluating eight empirical studies (2021-2024). Identifies five primary approaches: Cultural Prompting, Affective Priming, Self-Debiasing techniques, structured multi-stage pipelines, and parameter-optimized continuous prompts. Structured multi-stage pipelines most effective with up to 87.7% bias reduction. Emphasizes accessibility of prompt engineering for bias mitigation and importance of transparent methodology for trust building.",https://doi.org/10.48550/arXiv.2506.18199,,,,,,,,,,,,,,
19,Y4BMCI2J,Asseri (2024),Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,10.48550/arXiv.2506.18199,journalArticle,2024.0,,Gemini,"Systematic review following PRISMA guidelines analyzing prompt-engineering techniques to reduce cultural bias against Arabs and Muslims in LLMs, evaluating eight empirical studies (2021-2024). Identifies five primary approaches: Cultural Prompting, Affective Priming, Self-Debiasing techniques, structured multi-stage pipelines, and parameter-optimized continuous prompts. Structured multi-stage pipelines most effective with up to 87.7% bias reduction. Emphasizes accessibility of prompt engineering for bias mitigation and importance of transparent methodology for trust building.",https://doi.org/10.48550/arXiv.2506.18199,,,,,,,,,,,,,,
20,EHQBHVYV,Asseri (2025),Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,,report,2025.0,,ChatGPT,"This systematic review of 8 studies (2021–2024) identifies five prompt engineering approaches to mitigate bias against Arabs and Muslims: self-debiasing, cultural context prompting, affective priming, structured multi-step pipelines, and continuous prompt tuning. Multi-step pipelines were most effective, reducing biased content by up to ~88%, while simpler methods like cultural prompts showed ~71–81% improvement. The review concludes that while prompt engineering can mitigate biases without retraining, deep-seated biases may persist, and fixes can be superficial.",https://arxiv.org/html/2506.18199,,,,,,,,,,,,,,
21,C485NKYA,Attard-Frost (2025),AI Countergovernance: Lessons Learned from Canada and Paris,,report,2025.0,,ChatGPT,"Argues against superficial ""AI literacy"" programs, promoting instead grassroots critical AI literacies that engage directly with structural inequalities related to race, gender, and labor. Stresses collectiv",https://techpolicy.press/ai-countergovernance-lessons-learned-from-canada-and-paris/,,,,,,,,,,,,,,
22,38E5FZDV,Bai (2025),Explicitly unbiased large language models still form biased associations,10.1073/pnas.2416228122,journalArticle,2025.0,,ChatGPT,"Demonstrates that even when LLMs are aligned to avoid overt bias, they can still harbor implicit biases. Introduces novel evaluation methods inspired by psychology: LLM Word Association Test (LLM-WAT) and LLM Relative Decision Test (LLM-RDT) to probe automatic associations and subtle discrimination. Finds pervasive stereotype-consistent biases across multiple domains in eight state-of-the-art, value-aligned models.",https://doi.org/10.1073/pnas.2416228122,,,,,,,,,,,,,,
23,AK7K4P97,Baker (2025),Artificial intelligence in social work: An EPIC model for practice,10.1080/0312407X.2025.2488345,journalArticle,2025.0,,Claude,"Presents EPIC model for integrating AI into social work consisting of four components: Ethics and justice, Policy development and advocacy, Intersectoral collaboration, and Community engagement and empowerment. Following comprehensive literature review, examines AI's influence on social work including opportunities to advance socially just outcomes and challenges risking ethical practice. Emphasizes community-based initiatives promoting AI digital literacy and partnerships with local organizations to improve technology access for vulnerable populations.",,,,,,,,,,,,,,,
24,GCQ8J9XF,Barman (2024),Beyond transparency and explainability: On the need for adequate and contextualized user guidelines for LLM use,10.1007/s10676-024-09778-2,journalArticle,2024.0,,ChatGPT,"Argues for user-centered approach to governing AI systems, contending that transparency alone is insufficient. Proposes contextualized guidelines and training for users including clear instructions on LLM reliability, diversity-sensitive prompting techniques, and iterative query refinement. Emphasizes shifting focus from AI's internal workings to human-AI interaction context.",https://doi.org/10.1007/s10676-024-09778-2,,,,,,,,,,,,,,
25,5UAHQESQ,Basseri (2025),Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review,,report,2025.0,,Perplexity,"This systematic review identifies five major prompt engineering strategies to reduce cultural and intersectional bias in LLMs. Structured multi-step pipelines were most effective but complex, while cultural prompting offered a practical balance. Results show varying mitigation success depending on stereotype type, and emphasize trade-offs between bias reduction and performance.",https://arxiv.org/html/2506.18199v1,,,,,,,,,,,,,,
26,QUU8QDPQ,Benjamin (2023),Keynote Summary: The New Jim Code: Reimagining the Default Settings of Technology & Society,10.1145/3589139,journalArticle,2023.0,,Gemini,"In this summary of her foundational work, Ruha Benjamin introduces the concept of the ""New Jim Code,"" which describes how new technologies, including AI, can reproduce and even deepen existing racial hierarchies and discrimination under a veneer of neutrality and progress. She argues that discrimination becomes embedded in the very architecture of these systems. This framework is crucial for understanding how various forms of discrimination are co-constituted in AI, not as accidental bugs, but as features of a system designed within a society that has not resolved its structural inequalities. The concept inherently critiques individual competence, showing how even well-intentioned developers can create discriminatory systems if the underlying societal ""default settings"" are not challenged.",https://dl.acm.org/doi/10.1145/3589139,,,,,,,,,,,,,,
27,VP6SXQHY,Benlian (2025),The AI literacy development canvas: Assessing and building AI literacy in organizations,10.1016/j.bushor.2025.10.001,journalArticle,2025.0,en,Manual,,https://linkinghub.elsevier.com/retrieve/pii/S0007681325001673,,,,,,,,,,,,,,
28,J9IZDVTW,Biagini (2024),"Less knowledge, more trust? Exploring potentially uncritical attitudes towards AI in higher education",10.17471/2499-4324/1337,journalArticle,2024.0,eng,Manual,"L'intelligenza artificiale (IA) ha il potenziale per trasformare vari aspetti delle nostre vite, ma il suo sviluppo è stato accompagnato da numerose preoccupazioni sociali ed etiche. Per comprendere le implicazioni e i meccanismi sottostanti, è essenziale acquisire una comprensione ampia dei suoi benefici e svantaggi. A questo scopo, l'alfabetizzazione all'IA è un fattore fondamentale per promuovere atteggiamenti più consapevoli verso lo sviluppo dell'IA e delle sue implicazioni. Tuttavia, la ricerca sulla literacy all'IA è ancora agli esordi. Per contribuire ai progressi del settore, questo articolo presenta i risultati di uno studio volto a valutare l'alfabetizzazione all'IA degli studenti nel contesto dell'istruzione universitaria, concentrandosi su dei dottorandi. L’indagine sulla loro literacy all’IA è stata condotta su quattro dimensioni: cognitiva, operativa, critica ed etica. I risultati mostrano che, sebbene i partecipanti avessero poca conoscenza dell'IA, erano eccessivamente fiduciosi nelle capacità della tecnologia. Lo studio evidenzia la necessità di un approccio più completo all'alfabetizzazione all'IA, che includa una comprensione più profonda delle sue implicazioni etiche, sociali ed economiche.",https://doi.org/10.17471/2499-4324/1337,,,,,,,,,,,,,,
29,9832ZJB7,Biegelbauer (2023),"Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden für KI in der Verwaltung, Version 1.0",,report,2023.0,,Perplexity,"This guideline defines AI literacy as the ability to understand and use AI, emphasizing that safe, self-determined, and responsible use requires sufficient understanding of the technology's functioning, possibilities, and challenges. It identifies automation bias as a central risk and emphasizes competency building and training as the foundation for all further measures, recommending the creation of educational standards for AI procurement and application.",https://oeffentlicherdienst.gv.at/wp-content/uploads/2023/11/Leitfaden-Digitale-Verwaltung-Ethik.pdf,,,,,,,,,,,,,,
30,GCN42PAM,Birru (2024),Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development,,journalArticle,2024.0,,Gemini,"The increasing popularity of large language models (LLMs) in digital platforms elevates the urgency to address inherent biases, particularly age-related biases, which can significantly skew the model’s fairness and performance. This paper introduces a novel two-stage bias mitigation approach utilizing LLM’s empathy ability, reinforcement learning, and human-in-the-loop mechanisms to identify and correct age-related biases without altering model parameters. There are two modes for our bias mitigation strategy. Self-bias mitigation in the loop allows LLMs to self-assess and adjust their outputs autonomously, promoting inherent bias awareness and correction. Alternatively, cooperative bias mitigation in the loop leverages collaborative filtering among multiple LLMs to debate and mitigate biases through consensus. Furthermore, we introduce the empathetic perspective exchange strategy, which can further refine the answers by changing the perspective in the context information given to the LLM. In this way, more suitable responses applicable to different ages are generated. Our comprehensive evaluation across several data sets demonstrates that our trained model, FairLLM, significantly reduces age bias, outperforming existing techniques in fairness metrics. These findings underscore the effectiveness of our proposed framework in fostering the development of more equitable artificial intelligence systems, potentially benefiting a broader demographic spectrum by reducing digital ageism.",https://pubsonline.informs.org/doi/10.1287/ijoc.2024.0645,,,,,,,,,,,,,,
31,U9ACKGB4,Bisconti (2024),A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization,,journalArticle,2024.0,,Perplexity,"Develops formal conceptualization of AI trustworthiness connecting intrinsic and perceived trustworthiness in operationalizable schema. Argues that trustworthiness extends beyond inherent AI system capabilities to include significant influences from observer perceptions such as perceived transparency, agency locus, and human oversight. Identifies three central perceived characteristics: transparency (access to information about functionality), agency locus (perception of action autonomy source), and human oversight (presence of human control). Mathematical formalization defines overall trustworthiness as function of discrepancy between intrinsic and perceived trustworthiness.",https://philarchive.org/archive/BISAFA,,,,,,,,,,,,,,
32,PH7JBBC8,Bisconti (2024),A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization,,journalArticle,2024.0,,Gemini,"Develops formal conceptualization of AI trustworthiness connecting intrinsic and perceived trustworthiness in operationalizable schema. Argues that trustworthiness extends beyond inherent AI system capabilities to include significant influences from observer perceptions such as perceived transparency, agency locus, and human oversight. Identifies three central perceived characteristics: transparency (access to information about functionality), agency locus (perception of action autonomy source), and human oversight (presence of human control). Mathematical formalization defines overall trustworthiness as function of discrepancy between intrinsic and perceived trustworthiness.",https://philarchive.org/archive/BISAFA,,,,,,,,,,,,,,
33,XJCEMM3D,Bisconti (2024),A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization,,journalArticle,2024.0,,ChatGPT,"Develops formal conceptualization of AI trustworthiness connecting intrinsic and perceived trustworthiness in operationalizable schema. Argues that trustworthiness extends beyond inherent AI system capabilities to include significant influences from observer perceptions such as perceived transparency, agency locus, and human oversight. Identifies three central perceived characteristics: transparency (access to information about functionality), agency locus (perception of action autonomy source), and human oversight (presence of human control). Mathematical formalization defines overall trustworthiness as function of discrepancy between intrinsic and perceived trustworthiness.",https://philarchive.org/archive/BISAFA,,,,,,,,,,,,,,
34,GXZWBNJU,Boetto (2025),Artificial Intelligence in Social Work: An EPIC Model for Practice,10.1080/0312407X.2025.2488345,journalArticle,2025.0,,ChatGPT,"Narrative review proposing the EPIC model—Ethics & justice, Policy, Intersectoral collaboration, Community engagement—to guide ethical AI integration. Balances efficiency opportunities with risks of bias and value erosion, advocating structured, human-centered adoption aligned with social justice.",https://doi.org/10.1080/0312407X.2025.2488345,,,,,,,,,,,,,,
35,7QQV7R9S,British Association of Social Workers (2025),Generative AI & social work practice guidance,,report,2025.0,,Claude,"BASW's initial practice guidance specifically addressing generative AI use in social work, offering reflection points for practitioners relating to ethical considerations under BASW Code of Ethics. Warns that AI tools are prone to replicating racist/sexist assumptions from training datasets, generating misleading information (hallucinations), and risking data privacy breaches. Emphasizes generative AI should create capacity for relationship-based practice rather than justify increased caseloads or redundancies. Key recommendations include avoiding entry of sensitive personal information into generic tools without explicit consent, conducting data protection assessments, maintaining human oversight and critical assessment of AI outputs.",https://basw.co.uk/policy-and-practice/resources/generative-ai-social-work-practice-guidance,,,,,,,,,,,,,,
36,ZLM537Z4,Browne (2023),"Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligent Machines",,book,2023.0,,Claude,"First comprehensive collection bringing together leading feminist thinkers across disciplines to examine AI's societal impact. Features 21 chapters covering topics from techno-racial capitalism to AI's military applications, examining how feminist scholarship can hold the AI sector accountable for designing systems that further social justice through diverse feminist approaches.",https://doi.org/10.1093/oso/9780192889898.001.0001,,,,,,,,,,,,,,
37,V4GTLMED,Browne (2024),Tech workers' perspectives on ethical issues in AI development: Foregrounding feminist approaches,10.1177/20539517231221780,journalArticle,2024.0,,Claude,"Empirical study examining tech workers' understanding of ethical AI development through feminist lens, revealing critical gaps in current AI ethics approaches. Finds that the term ""bias"" creates confusion among tech workers, undermining AI ethics initiatives, and argues for moving beyond diversity narratives toward ""design justice"" that centers marginalized voices.",https://doi.org/10.1177/20539517231221780,,,,,,,,,,,,,,
38,HJ7BHX8J,Browne (2024),Engineers on responsibility: feminist approaches to who's responsible for ethical AI,10.1007/s10676-023-09739-1,journalArticle,2024.0,,Claude,"Through interviews with AI practitioners interpreted via feminist political thought, reimagines responsibility in AI development beyond individualized approaches. Critiques current AI responsibility frameworks focused on individual competency and technical solutions, proposing instead ""responsibility as the product of work cultures that enable tech workers to be responsive and answerable for their products."" Moves beyond ""individual competency approaches"" toward understanding responsibility as embedded in structural power relations and organizational cultures.",,,,,,,,,,,,,,,
39,UFE85SCV,Casal-Otero (2023),AI literacy in K-12: a systematic literature review,10.1186/s40594-023-00418-7,journalArticle,2023.0,en,Manual,"Abstract
            The successful irruption of AI-based technology in our daily lives has led to a growing educational, social, and political interest in training citizens in AI. Education systems now need to train students at the K-12 level to live in a society where they must interact with AI. Thus, AI literacy is a pedagogical and cognitive challenge at the K-12 level. This study aimed to understand how AI is being integrated into K-12 education worldwide. We conducted a search process following the systematic literature review method using Scopus. 179 documents were reviewed, and two broad groups of AI literacy approaches were identified, namely learning experience and theoretical perspective. The first group covered experiences in learning technical, conceptual and applied skills in a particular domain of interest. The second group revealed that significant efforts are being made to design models that frame AI literacy proposals. There were hardly any experiences that assessed whether students understood AI concepts after the learning experience. Little attention has been paid to the undesirable consequences of an indiscriminate and insufficiently thought-out application of AI. A competency framework is required to guide the didactic proposals designed by educational institutions and define a curriculum reflecting the sequence and academic continuity, which should be modular, personalized and adjusted to the conditions of the schools. Finally, AI literacy can be leveraged to enhance the learning of disciplinary core subjects by integrating AI into the teaching process of those subjects, provided the curriculum is co-designed with teachers.",https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00418-7,,,,,,,,,,,,,,
40,4LY3SA4E,Charlesworth (2024),Flexible intersectional stereotype extraction (FISE): Analyzing intersectional biases in large language models,,journalArticle,2024.0,,Perplexity,Studie entwickelt FISE-Methode zur Messung intersektionaler Repräsentationsverzerrungen. Zeigt massive Dominanz weißer Männer in Internettexten und Ableitung entsprechender LLM-Biases.,https://news.northwestern.edu/stories/2024/03/kellogg-study-suggests-that-some-intersectional-groups-are-more-represented-than-others-in-internet-text/,,,,,,,,,,,,,,
41,P82X89Q4,Chee (2025),A Competency Framework for AI Literacy: Variations by Different Learner Groups and an Implied Learning Pathway,10.1111/bjet.13556,journalArticle,2025.0,en,Manual,"This study aims to develop a comprehensive competency framework for artificial intelligence (AI) literacy, delineating essential competencies and sub‐competencies. This framework and its potential variations, tailored to different learner groups (by educational level and discipline), can serve as a crucial reference for designing and implementing AI curricula. However, the research on AI literacy by target learners is still in its infancy, and the findings of several existing studies provide inconsistent guidelines for educational practices. Following the 2020 PRISMA guidelines, we searched the Web of Science, Scopus, and ScienceDirect databases to identify relevant studies published between January 2012 and October 2024. The quality of the included studies was evaluated using QualSyst. A total of 29 studies were identified, and their research findings were synthesized. Results show that at the K‐12 level, the required competencies include basic AI knowledge, device usage, and AI ethics. For higher education, the focus shifts to understanding data and algorithms, problem‐solving, and career‐related competencies. For general workforce, emphasis is placed on the interpretation and utilization of data and AI tools for specific careers, along with error detection and AI‐based decision‐making. This study connects the progression of specific learning objectives, which should be intensively addressed at each stage, to propose an AI literacy education pathway. We discuss the findings, potentials, and limitations of the derived competency framework for AI literacy, including its theoretical and practical implications and future research suggestions.
            
            
              
              
                
                  
                    Practitioner notes
                  
                  
                    What is already known about this topic
                    
                      
                        AI literacy is becoming increasingly important as AI technologies are integrated into various aspects of life and work.
                      
                      
                        Research on AI literacy competencies across diverse learner groups and disciplines remains fragmented and inconsistent to guide educational practices.
                      
                      
                        Studies providing a coherent pathway for AI literacy development throughout educational and working life are lacking.
                      
                    
                  
                  
                    What this paper adds
                    
                      
                        A comprehensive AI literacy competency framework consisting of 8 competencies and 18 sub‐competencies.
                      
                      
                        Variations in AI literacy competencies with tailored configuration and prioritization across different learner groups by school levels and disciplines.
                      
                      
                        A proposed pathway for developing AI literacy from K‐12 to higher education and workforce levels.
                      
                    
                  
                  
                    Implications for practice and policy
                    
                      
                        The framework can guide the design and implementation of AI curricula tailored to different learner characteristics and needs.
                      
                      
                        Education should shift focus from teaching how to use AI to fostering competencies for critical, strategic, responsible and ethical integration of AI.
                      
                      
                        Policies are needed to support a systematic pathway for lifelong AI literacy development from K‐12 education to workforce training.",https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13556,,,,,,,,,,,,,,
42,VEJBIZRR,Chen (2024),Exploring complex mental health symptoms via classifying social media data with explainable LLMs,,report,2024.0,,Manual,,https://www.arxivdaily.com/thread/62478,,,,,,,,,,,,,,
43,79TL6HSB,Chen (2025),Social work and artificial intelligence: Collaboration and challenges,,journalArticle,2025.0,,Perplexity,"This qualitative study explores current AI applications in social work through interviews with professionals, AI developers, and policymakers, identifying challenges including insufficient decision-making transparency, gaps in ethical frameworks, and inadequate technical literacy among professionals. The research reveals that while ninety percent of social work professionals acknowledge AI's auxiliary function in daily operations, concerns persist about automation bias and the potential undermining of professional autonomy. The study proposes educational training and policy recommendations while advocating for explainable AI systems and strengthened ethical governance to foster harmonious development between technological applications and humanistic values.",https://dmjr-journals.com/assets/article/1755892935-SOCIAL_WORK_&_ARTIFICIAL_INTELLIGENCE-_COLLABORATION_AND_CHALLENGES.pdf,,,,,,,,,,,,,,
44,XQM6WRU2,Cher (2024),Exploring machine learning to support decision-making for placement stabilization and preservation in child welfare,10.1007/s10826-024-02993-x,journalArticle,2024.0,,Claude,"Statewide study analyzed 12,621 child welfare cases in large Midwestern state (2017-2020) to develop machine learning models predicting placement disruption risk. Goal was to identify youth who could benefit from placement stabilization services to prevent unnecessary residential care under Family First Prevention Services Act. Random forest models were compared with conventional logistic regression for predicting placement disruption and referral to stabilization programs. ML models demonstrated moderate predictive validity with practical implications for proactive service allocation. Results showed ML could support but not replace caseworker judgment in placement decisions. Study used administrative child welfare data and evaluated model fairness considerations.",,,,,,,,,,,,,,,
45,JZN2I6J5,Chisca (2024),Prompting fairness: Learning prompts for debiasing large language models,,conferencePaper,2024.0,,Claude,Introduces novel prompt-tuning method for reducing biases in encoder models like BERT and RoBERTa through training small sets of additional reusable token embeddings. Demonstrates state-of-the-art performance while maintaining minimal impact on language modeling capabilities through parameter-efficient approach applicable across different models and tasks.,https://aclanthology.org/2024.ltedi-1.6/,,,,,,,,,,,,,,
46,ACDF4FL9,Chisca (2024),Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,,report,2024.0,,Gemini,"Dual process theory posits that human cognition arises via two systems. System 1, which is a quick, emotional, and intuitive process, which is subject to cognitive biases, and System 2, is a slow, onerous, and deliberate process. NLP researchers often compare zero-shot prompting in LLMs to System 1 reasoning and chain-of-thought (CoT) prompting to System 2. In line with this interpretation, prior research has found that using CoT prompting in LLMs leads to reduced gender bias. We investigate the relationship between bias, CoT prompting, a debiasing prompt, and dual process theory in LLMs directly. We compare zero-shot CoT, debiasing, and a variety of dual process theory-based prompting strategies on two bias datasets spanning nine different social bias categories. We incorporate human and machine personas to determine whether the effects of dual process theory in LLMs exist independent of explicit persona models or are based on modeling human cognition. We find that a human persona, debiasing, System 2, and CoT prompting all tend to reduce social biases in LLMs, though the best combination of features depends on the exact model and bias category—resulting in up to a 19 percent drop in stereotypical judgments by an LLM",https://arxiv.org/html/2404.17218v3,,,,,,,,,,,,,,
47,I78CL6R5,Chiu (2024),What are artificial intelligence literacy and competency? A comprehensive framework to support them,10.1016/j.caeo.2024.100171,journalArticle,2024.0,en,Manual,,https://linkinghub.elsevier.com/retrieve/pii/S2666557324000120,,,,,,,,,,,,,,
48,KPSF8EZE,Chiu (2025),"AI literacy and competency: definitions, frameworks, development and future research directions",10.1080/10494820.2025.2514372,journalArticle,2025.0,en,Manual,,https://www.tandfonline.com/doi/full/10.1080/10494820.2025.2514372,,,,,,,,,,,,,,
49,ED6C8LD2,Choudhury (2024),Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals,10.2196/56764,journalArticle,2024.0,,Gemini,"Peer-reviewed viewpoint discussing integration of LLMs into healthcare requiring balance between trust and skepticism. Warns that ""blind trust"" in LLM recommendations can lead to automation bias and confirmation bias as clinicians may accept AI outputs uncritically. Argues that prompting for transparency in LLM reasoning enhances professionals' trust by making AI reasoning visible and verifiable. Emphasizes need for human oversight and bias mitigation to ensure LLMs complement rather than compromise expert decision-making.",https://www.jmir.org/2024/1/e56764/,,,,,,,,,,,,,,
50,WAYCKUZ8,Choudhury (2024),Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals,10.2196/56764,journalArticle,2024.0,,ChatGPT,"Peer-reviewed viewpoint discussing integration of LLMs into healthcare requiring balance between trust and skepticism. Warns that ""blind trust"" in LLM recommendations can lead to automation bias and confirmation bias as clinicians may accept AI outputs uncritically. Argues that prompting for transparency in LLM reasoning enhances professionals' trust by making AI reasoning visible and verifiable. Emphasizes need for human oversight and bias mitigation to ensure LLMs complement rather than compromise expert decision-making.",https://www.jmir.org/2024/1/e56764/,,,,,,,,,,,,,,
51,X7ZGW6CN,Ciston (2024),"Intersectional Artificial Intelligence Is Essential: Polyvocal, Multimodal, Experimental Methods to Save AI",10.7559/CITARJ.V11I2.665,journalArticle,2024.0,,Perplexity,"Arguments for applying intersectional strategies to AI at all levels - from data through design to implementation. Intersectionality, integrating institutional power analysis and queer-feminist plus critical race theories, can contribute to AI reconceptualization. Intersectional framework enables analysis of existing AI biases and uncovering alternative ethics from counter-narratives. Research presents intersectional strategies as polyvocal, multimodal, and experimental, with community-focused and artistic practices helping explore AI's intersectional possibilities. Practical examples include Data Nutrition Label for bias assessment in datasets and experimental projects like ""ladymouth,"" a chatbot explaining feminism.",,,,,,,,,,,,,,,
52,MIT8HTC6,Clemmer (2024),PreciseDebias: An automatic prompt engineering approach for generative AI to mitigate image demographic biases,,conferencePaper,2024.0,,Gemini,"This paper presents a technical solution for reducing demographic bias in AI image generators through ""PreciseDebias,"" an automated approach that rewrites simple prompts into more complex, diversity-reflective prompts. The system analyzes model bias and strategically adds attributes like ethnicity, gender, or age to enforce fairer representation distributions, demonstrating that diversity-reflective prompting can be automated at the system level.",https://openaccess.thecvf.com/content/WACV2024/html/Clemmer_PreciseDebias_An_Automatic_Prompt_Engineering_Approach_for_Generative_AI_WACV_2024_paper.html,,,,,,,,,,,,,,
53,7IY7AX7D,Colombatto (2025),The influence of mental state attributions on trust in large language models,10.1038/s44271-025-00262-1,journalArticle,2025.0,,ChatGPT,"Empirical study examining how users' beliefs about LLM's ""mind"" affect trust in advice. Preregistered experiment (N=410) showing attributing intelligence to LLM strongly increased trust, whereas attributing human-like consciousness did not increase trust. Higher ascriptions of sentience correlated with less advice-taking. Participants trusted AI for perceived analytical competence, not for having feelings. Suggests prompt-engineering highlighting competence and reliability more effective than anthropomorphism for building appropriate trust.",https://www.nature.com/articles/s44271-025-00262-1,,,,,,,,,,,,,,
54,8JFZMD5G,Colombatto (2025),The influence of mental state attributions on trust in large language models,10.1038/s44271-025-00262-1,journalArticle,2025.0,,Gemini,"Empirical study examining how users' beliefs about LLM's ""mind"" affect trust in advice. Preregistered experiment (N=410) showing attributing intelligence to LLM strongly increased trust, whereas attributing human-like consciousness did not increase trust. Higher ascriptions of sentience correlated with less advice-taking. Participants trusted AI for perceived analytical competence, not for having feelings. Suggests prompt-engineering highlighting competence and reliability more effective than anthropomorphism for building appropriate trust.",https://www.nature.com/articles/s44271-025-00262-1,,,,,,,,,,,,,,
55,JI24FQPV,Creswell Báez (2025),Clinical Social Workers’ Perceptions of Large Language Models in Practice: Resistance to Automation and Prospects for Integration,10.1080/26408066.2025.2542450,journalArticle,2025.0,,ChatGPT,"Qualitative study (interviews, reflexive thematic analysis) of clinicians exposed to LLM-supported consultation. Identifies efficiency and documentation support alongside concerns over confidentiality, loss of nuance, and reduced empathy. Concludes AI should augment—not replace—clinical judgment, requiring training and ethics.",https://doi.org/10.1080/26408066.2025.2542450,,,,,,,,,,,,,,
56,ZLMLP53P,Cvoelcker (2023),Queer in AI: A case study in community-led participatory AI,,conferencePaper,2023.0,,Perplexity,"Fallstudie zu Queer in AI, dokumentiert Schäden durch KI-Systeme an queeren Menschen und beschreibt community-geleitete Strategien für partizipative, faire KI.",https://cvoelcker.de/assets/pdf/paper_queer_in_ai.pdf,,,,,,,,,,,,,,
57,UFJ7ERFF,D'Ignazio (2024),Data Feminism for AI,,report,2024.0,,Claude,"Comprehensive feminist framework directly critiques individualized approaches to AI ethics, challenging the ""liberal framework of making algorithms unbiased and inclusive"" in favor of structural ""remediation"" addressing ""systemic and structural dimensions of discrimination."" Examines how AI research is captured by ""racial, gendered capitalism"" and proposes nine principles focusing on structural power analysis including examining power, challenging power, and making labor visible. Explicitly positions against technical solutions ignoring power structures.",https://arxiv.org/html/2405.01286v1,,,,,,,,,,,,,,
58,F7WNRWIC,De Duro (2025),Measuring and identifying factors of individuals' trust in large language models,10.48550/arXiv.2502.21028,journalArticle,2025.0,,ChatGPT,"Study developing ""Trust-In-LLMs Index (TILLMI)"" psychometric scale to quantify trust in conversational AI. Survey of ~1,000 U.S. adults identified two trust dimensions: affective component (emotional closeness) and cognitive component (reliance on competence). Found significant individual differences with personality factors influencing trust. Prior LLM experience increased trust while those with no direct use were more skeptical. Highlights need for prompt-engineering and system design to account for user differences.",https://arxiv.org/html/2502.21028v1,,,,,,,,,,,,,,
59,5ERYSQCK,De Duro (2025),Measuring and identifying factors of individuals' trust in large language models,10.48550/arXiv.2502.21028,journalArticle,2025.0,,Gemini,"Study developing ""Trust-In-LLMs Index (TILLMI)"" psychometric scale to quantify trust in conversational AI. Survey of ~1,000 U.S. adults identified two trust dimensions: affective component (emotional closeness) and cognitive component (reliance on competence). Found significant individual differences with personality factors influencing trust. Prior LLM experience increased trust while those with no direct use were more skeptical. Highlights need for prompt-engineering and system design to account for user differences.",https://arxiv.org/html/2502.21028v1,,,,,,,,,,,,,,
60,2YS85B49,Debnath (2024),Can LLMs reason about trust? A pilot study,,journalArticle,2024.0,,Perplexity,"Pilot study investigating LLMs' ability to reason about trust between individuals by capturing complex mental states that exceed traditional symbolic approaches. Evaluates whether LLMs can analyze conversations and assess trust levels based on willingness, competence, and security factors. Demonstrates that LLMs can identify individual goals and develop improvement suggestions for trust building, as well as generate strategic action plans for trust promotion. Suggests LLMs have potential to bridge the gap between computational and socio-cognitive trust models.",https://arxiv.org/html/2507.21075v1,,,,,,,,,,,,,,
61,3GB9B4IJ,Debnath (2024),Can LLMs reason about trust? A pilot study,,journalArticle,2024.0,,ChatGPT,"Pilot study investigating LLMs' ability to reason about trust between individuals by capturing complex mental states that exceed traditional symbolic approaches. Evaluates whether LLMs can analyze conversations and assess trust levels based on willingness, competence, and security factors. Demonstrates that LLMs can identify individual goals and develop improvement suggestions for trust building, as well as generate strategic action plans for trust promotion. Suggests LLMs have potential to bridge the gap between computational and socio-cognitive trust models.",https://arxiv.org/html/2507.21075v1,,,,,,,,,,,,,,
62,BCBWSU3Z,Debnath (2024),Can LLMs reason about trust? A pilot study,,journalArticle,2024.0,,Gemini,"Pilot study investigating LLMs' ability to reason about trust between individuals by capturing complex mental states that exceed traditional symbolic approaches. Evaluates whether LLMs can analyze conversations and assess trust levels based on willingness, competence, and security factors. Demonstrates that LLMs can identify individual goals and develop improvement suggestions for trust building, as well as generate strategic action plans for trust promotion. Suggests LLMs have potential to bridge the gap between computational and socio-cognitive trust models.",https://arxiv.org/html/2507.21075v1,,,,,,,,,,,,,,
63,S8WGUVQT,Dencik (2024),Automated government benefits and welfare surveillance,,journalArticle,2024.0,,Claude,"Critical surveillance studies analysis examining digital welfare state historically, presently, and prospectively, focusing on AI-driven welfare surveillance systems. Authors argue problems posed by AI in public administration are often misattributed to technological novelty when they actually represent historically familiar patterns of surveillance and control. Drawing on bureaucracy, welfare state, and automation scholarship, demonstrates how algorithmic fraud detection and chatbot assistance systems extend long-standing practices of scrutinizing and disciplining marginalized populations. Critical insights include analysis of how automation enables unprecedented scale of surveillance while maintaining opacity through algorithmic systems' auditable veneer. Examines cases from Netherlands and other jurisdictions showing how automated welfare systems amplify existing power asymmetries and inequality.",https://ojs.library.queensu.ca/index.php/surveillance-and-society/article/view/16107,,,,,,,,,,,,,,
64,8BUHU5EP,Derechos Digitales (2023),Feminist reflections for the development of Artificial Intelligence,,report,2023.0,,Claude,"Synthesizes conversations among Latin American women developing AI systems, providing methodological frameworks for feminist AI development based on four real-world projects. Emphasizes co-design methodologies, digital autonomy, data sovereignty, and intersectional approaches through community agreements and power-balancing strategies.",https://www.derechosdigitales.org/fair-2023-en/,,,,,,,,,,,,,,
65,YDW5TX8M,Deuze (2022),"Imagination, Algorithms and News: Developing AI Literacy for Journalism",10.1080/21670811.2022.2119152,journalArticle,2022.0,en,Manual,,https://www.tandfonline.com/doi/full/10.1080/21670811.2022.2119152,,,,,,,,,,,,,,
66,EN6GNKL3,Dilek (2025),AI literacy in teacher education: Empowering educators through critical co-discovery,10.1177/00224871251325083,journalArticle,2025.0,,Claude,"Implements critical co-discovery approaches within AI teacher education to move beyond technical automation toward critical pedagogical engagement. Through co-discovery activities, educators developed understanding of AI concepts, ethical considerations, and context-specific applications while co-constructing knowledge. Emphasizes that prolonged engagement with AI literacy integrated into teacher education programs enables educators to critically navigate AI systems and examine broader pedagogical and ethical implications, prioritizing critical examination of AI's power dynamics and social justice implications.",,,,,,,,,,,,,,,
67,KG8JRLRQ,Dixon (2018),Measuring and mitigating unintended bias in text data,,conferencePaper,2018.0,,Manual,,,,,,,,,,,,,,,,
68,8WBUGXRR,Djeffal (2025),Reflexive prompt engineering: A framework for responsible prompt engineering and AI interaction design,10.1145/3715275.3732118,conferencePaper,2025.0,,ChatGPT,"This paper proposes ""Reflexive Prompt Engineering"" as a comprehensive framework to embed ethical and inclusive principles into prompt crafting for generative AI. Framework consists of five components: prompt design, system selection, system configuration, performance evaluation, and prompt management, each considered from social responsibility perspective. Positions responsible prompt engineering as essential component of AI literacy, bridging gap between AI development and deployment by aligning AI behavior with human rights and diversity values.",,,,,,,,,,,,,,,
69,K2KL8WH8,Djiberou Mahamadou (2024),Revisiting Technical Bias Mitigation Strategies,,report,2024.0,,Perplexity,"Diese systematische Überprüfung identifiziert praktische Limitationen technischer Bias-Mitigation-Strategien im Gesundheitswesen entlang fünf Schlüsseldimensionen: wer Bias und Fairness definiert, welche Mitigation-Strategie zu verwenden und zu priorisieren ist, wann in den KI-Entwicklungsstadien die Lösungen am effektivsten sind, für welche Populationen und in welchem Kontext die Lösungen entworfen sind. Die Studie zeigt mathematische Inkonsistenzen und Inkompatibilitäten zwischen verschiedenen Fairness-Metriken auf und diskutiert, wie werteorientierte KI stakeholder-bezogene Ansätze zur Bias-Mitigation ermöglichen kann.",https://arxiv.org/abs/2410.17433,,,,,,,,,,,,,,
70,DGLWM93D,Engelhardt (2025),Voll (dia)logisch? Ein Werkstattbericht über den Einsatz von generativer KI in der Hochschulbildung für Soziale Arbeit – Curriculare Überlegungen und veränderte Akteurskonstellationen,,bookSection,2025.0,,ChatGPT,"Werkstattbericht zur curricularen Integration generativer KI. Positioniert Prompting als metakognitive Schlüsselkompetenz und diskutiert Rollenwandel von Lehr-/Lernakteuren; fordert reflektierte, ethisch eingebettete Nutzung mit Fokus auf kritische Validierung von KI-Ergebnissen.",https://www.pedocs.de/volltexte/2025/33133/pdf/Engelhardt_Ley_2025_Voll_dia_logisch.pdf,,,,,,,,,,,,,,
71,XCS4YCQH,European Commission. Joint Research Centre. (2017),DigComp 2.1: the digital competence framework for citizens with eight proficiency levels and examples of use.,,book,2017.0,eng,Manual,,https://data.europa.eu/doi/10.2760/38842,,,,,,,,,,,,,,
72,QFPTW4VL,European Data Protection Supervisor (2023),Explainable Artificial Intelligence,,report,2023.0,,Perplexity,"Emphasizes the necessity of Explainable AI (XAI) to ensure human-centered, transparent, and ethical AI systems. By increasing transparency and comprehensibility of algorithmic decisions, XAI enables individuals—including those from marginalized groups—to participate meaningfully in digital decision-making and challenge unjust outcomes.",https://www.edps.europa.eu/system/files/2023-11/23-11-16_techdispatch_xai_en.pdf,,,,,,,,,,,,,,
73,MITLDF9S,Feist-Ortmanns (2025),KI-basiertes Assistenzsystem im Kinderschutzverfahren,,bookSection,2025.0,,ChatGPT,"Praxisnaher Bericht zu einem KI-Assistenzsystem für Gefährdungseinschätzung. Betont Human-in-the-Loop, Datenschutz, Organisationsentwicklung und Schulungsbedarf; zeigt Potenziale (präventive Hinweise) und Risiken (Bias, Fehlklassifikationen) im sensiblen Kinderschutzkontext.",https://afet-ev.de/assets/themenplattform/KI-in-der-Kinder--und-Jugendhilfe-Naher-Grasshoff-Forum-2.pdf,,,,,,,,,,,,,,
74,MITLDF9S,Feist-Ortmanns (2025),KI-basiertes Assistenzsystem im Kinderschutzverfahren,,bookSection,2025.0,,ChatGPT,"Praxisnaher Bericht zu einem KI-Assistenzsystem für Gefährdungseinschätzung. Betont Human-in-the-Loop, Datenschutz, Organisationsentwicklung und Schulungsbedarf; zeigt Potenziale (präventive Hinweise) und Risiken (Bias, Fehlklassifikationen) im sensiblen Kinderschutzkontext.",https://afet-ev.de/assets/themenplattform/KI-in-der-Kinder--und-Jugendhilfe-Naher-Grasshoff-Forum-2.pdf,,,,,,,,,,,,,,
75,IDW7QSYG,Fraile-Rojas (2025),Female perspectives on algorithmic bias: Implications for AI researchers and practitioners,10.1108/MD-04-2024-0884,journalArticle,2025.0,,ChatGPT,"This study uses NLP and machine learning to analyze 172,041 tweets from female users discussing gender inequality in AI. It identifies prominent themes including the future of AI technologies and women's active role in ensuring gender-balanced systems. Findings show that algorithmic bias directly affects women's experiences, prompting engagement in online discourse about injustices. Women lead constructive conversations and create entrepreneurial solutions when faced with bias, demonstrating how feminist digital literacies can make AI biases visible and push for their reduction.",https://colab.ws/articles/10.1108%2Fmd-04-2024-0884,,,,,,,,,,,,,,
76,EB7PZUZZ,Freinhofer (2025),Prompten nach Plan: Das PCRR-Framework als pädagogisches Werkzeug für den Einsatz von Künstlicher Intelligenz.,10.21243/mi-01-25-26,journalArticle,2025.0,de,Gemini,"Die rasante Entwicklung generativer Künstlicher Intelligenz (KI) macht Prompt Engineering zu einer Schlüsselkompetenz für den kompetenten Umgang mit KI-Modellen. Während zahlreiche Prompting-Techniken und -Frameworks existieren, fehlt bislang eine systematische Integration in den schulischen Kontext. Diese Publikation stellt das PCRR-Framework (Plan – Create – Review – Reflect) vor, das als ganzheitlicher Ansatz für den Einsatz von Prompt Engineering im Unterricht dient. Basierend auf Erfahrungen aus dem Hochschullehrgang „Künstliche Intelligenz im IT-Unterricht der Berufsbildung“ (PH Tirol und Hochschule für Agrar- und Umweltpädagogik) wurde das Framework iterativ weiterentwickelt und in drei Praxisbeispielen erprobt. Die Ergebnisse zeigen, dass das PCRR-Framework die Effizienz und Qualität der Prompterstellung steigern kann und die Schüler:innen beim Umgang mit Sprachmodellen (Large Language Models, LLMs) unterstützt. Gleichzeitig wurden Herausforderungen deutlich, insbesondere hinsichtlich der methodischen Vergleichbarkeit der Ergebnisse sowie der Akzeptanz bestimmter Prompting-Techniken. Das Paper diskutiert diese Erkenntnisse, methodischen Limitationen und Verbesserungspotenziale und bietet einen Ausblick auf zukünftige Forschungsarbeiten. Neben der Weiterentwicklung des PCRR-Frameworks wird die Notwendigkeit betont, AI Literacy systematisch in Lehrpläne und Lehramtsausbildungen zu integrieren, um eine nachhaltige und verantwortungsbewusste Nutzung von KI im Bildungsbereich zu ermöglichen.",https://journals.univie.ac.at/index.php/mp/article/view/9274,,,,,,,,,,,,,,
77,A776TPGG,Friedrich-Ebert-Stiftung (2025),The EU artificial intelligence act through a gender lens,,report,2025.0,,Perplexity,Politikanalyse des EU AI Acts mit Fokus auf Geschlechtergerechtigkeit. Identifiziert Potenziale und Lücken im Gesetzestext und gibt konkrete Empfehlungen zur Umsetzung.,https://library.fes.de/pdf-files/bueros/bruessel/21887-20250304.pdf,,,,,,,,,,,,,,
78,QKFG72IT,Fujii (2024),Bildungsteilhabe - Flucht - Digitalisierung: Eine multilokale Ethnografie im (digitalen) Alltag junger Geflüchteter,,book,2024.0,,Claude,"Presents ethnographic research on how digital media shapes educational participation for young refugees, examining ambivalent role of digital technologies: enabling connection to educational resources and transnational networks while simultaneously creating new forms of exclusion through surveillance, documentation requirements, and digital skill barriers. Reveals digital access alone does not guarantee educational participation—digital literacy, linguistic competence, and social support remain crucial. Documents how digitalization intersects with migration status, creating specific vulnerabilities related to data collection, surveillance, and documentation requirements.",https://www.beltz.de/fachmedien/sozialpaedagogik_soziale_arbeit/produkte/details/53146,,,,,,,,,,,,,,
79,MUBZ8XJL,Gaba (2025),"Bias, accuracy, and trust: Gender-diverse perspectives on large language models",,journalArticle,2025.0,,ChatGPT,"Qualitative study with 25 interviews exploring how users of different gender identities perceive bias and trustworthiness in ChatGPT. Found perceived biases in LLM outputs undermine user trust, with non-binary participants encountering stereotyped responses that eroded confidence. Trust levels varied by group with male participants reporting higher trust. Participants recommended bias mitigation strategies including diversifying training data and implementing clarifying follow-up prompts to check biases.",https://arxiv.org/abs/2506.21898,,,,,,,,,,,,,,
80,G53MCF3W,Gaba (2025),"Bias, accuracy, and trust: Gender-diverse perspectives on large language models",,journalArticle,2025.0,,Gemini,"Qualitative study with 25 interviews exploring how users of different gender identities perceive bias and trustworthiness in ChatGPT. Found perceived biases in LLM outputs undermine user trust, with non-binary participants encountering stereotyped responses that eroded confidence. Trust levels varied by group with male participants reporting higher trust. Participants recommended bias mitigation strategies including diversifying training data and implementing clarifying follow-up prompts to check biases.",https://arxiv.org/abs/2506.21898,,,,,,,,,,,,,,
81,WNY526GN,Gallegos (2024),Bias and fairness in large language models: A survey,10.1162/coli_a_00524,journalArticle,2024.0,,ChatGPT,"This comprehensive survey consolidates recent research on social biases in large language models (LLMs) and methods to mitigate them. The authors formalize key concepts of bias and fairness in NLP, presenting three taxonomies: metrics for bias evaluation organized by model level, datasets for bias evaluation categorized by structure and target social groups, and bias mitigation techniques classified by intervention stage.",https://doi.org/10.1162/coli_a_00524,,,,,,,,,,,,,,
82,MS3CNU3S,Gallegos (2024),Bias and fairness in large language models: A survey,10.1162/coli_a_00524,journalArticle,2024.0,,Claude,"Comprehensive survey consolidating notions of social bias and fairness in NLP, defining distinct facets of harm and introducing desiderata to operationalize fairness for LLMs. Proposes three taxonomies: metrics for bias evaluation, datasets for bias evaluation, and techniques for bias mitigation classified by intervention timing.",https://doi.org/10.1162/coli_a_00524,,,,,,,,,,,,,,
83,ST9UCTJE,Garg (2019),Counterfactual fairness in text classification through robustness,,conferencePaper,2019.0,,Manual,,,,,,,,,,,,,,,,
84,UENFDPH9,Garkisch (2024),Considering a unified model of artificial intelligence enhanced social work: A systematic review,10.1007/s41134-024-00326-y,journalArticle,2024.0,,Claude,"Systematic review mapping research landscape of social work AI scholarship, analyzing 67 articles using qualitative analytic approaches to explore how social work researchers investigate AI. Identified themes consistent with Staub-Bernasconi's triple mandate covering profession level, social agencies/organizations, and clients. Emphasizes importance of enhancing computational thinking, AI literacy, and data literacy, and developing skills for evaluating automated systems. Stresses that professionals must be educated and sensitized to data and AI literacy with regular training opportunities.",,,,,,,,,,,,,,,
85,YN9JAREI,Gengler (2024),Faires KI-Prompting – Ein Leitfaden für Unternehmen,,report,2024.0,,Gemini,"This practical guide argues that feminist AI competency results in conscious prompt design to actively reduce stereotypes and discrimination in AI-generated content. It presents the ""KI-FAIRNESS"" framework for diversity-reflective prompting and demonstrates how specific, context-rich prompts lead to fairer and more representative results by compensating for AI's ""blind spots"" through targeted instructions.",https://www.digitalzentrum-zukunftskultur.de/wp-content/uploads/2024/05/Faires-KI-Prompting-Ein-Leitfaden-fuer-Unternehmen.pdf,,,,,,,,,,,,,,
86,PC75954S,Ghosal (2024),An empirical study of structural social and ethical challenges in AI,,journalArticle,2024.0,,Perplexity,"This empirical study examines how professionals (N=32) in AI development perceive structural ethical challenges such as injustices and inequalities. The research identifies three main themes: (1) barriers to responsibility in a changing ecosystem, (2) the need for holistic consideration of AI products and their harms, and (3) structural obstacles that prevent engineers from taking personal responsibility.",https://link.springer.com/article/10.1007/s00146-025-02207-y,,,,,,,,,,,,,,
87,EMZ33KFH,Ghosal (2025),Unequal voices: How LLMs construct constrained queer narratives,,report,2025.0,,ChatGPT,"Investigates how large language models represent queer individuals in generated narratives, uncovering tendencies toward stereotyped and narrow portrayals. Identifies phenomena including narrow topic range, discursive othering, and identity foregrounding. Shows LLMs unconsciously reinforce divide where marginalized groups are not afforded same breadth of narrative roles as others.",https://arxiv.org/abs/2507.15585,,,,,,,,,,,,,,
88,UBYTNGNV,Giannoni Adielsson (2024),"The AI Act, gender equality and non-discrimination: what role for the AI Office?",10.1007/s12027-024-00785-w,journalArticle,2024.0,,Perplexity,"Diese Analyse bewertet, ob der EU AI Act Fragen der Geschlechtergerechtigkeit und Nichtdiskriminierung ausreichend adressiert. Die substantiellen Bestimmungen des AI Acts werden durch die Linse von Gleichstellungs- und Antidiskriminierungsrecht analysiert, wobei vorgeschlagene Tools wie grundrechtliche Folgenabschätzungen und Bias-Audits zur Reduzierung von Geschlechterverzerrungen und Diskriminierungsrisiken hervorgehoben werden. Die Rolle des AI Office und seine Kooperation mit nationalen, europäischen und internationalen Stellen zur Durchsetzung der Geschlechtergerechtigkeit wird diskutiert.",https://doi.org/10.1007/s12027-024-00785-w,,,,,,,,,,,,,,
89,MPCZVZEW,Goellner (2025),Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room,,report,2025.0,,Gemini,"Identifies nine persistent challenges undermining responsible use of AI in education, including neglect of key learning processes, lack of stakeholder involvement, and use of unreliable XAI methods. Proposes hybrid human-AI methods, specifically neural-symbolic AI (NSAI), which integrates expert domain knowledge with data-driven approaches. This hybrid architecture allows for built-in transparency, stakeholder engagement, and modeling of complex pedagogically-grounded principles.",https://arxiv.org/pdf/2504.16148,,,,,,,,,,,,,,
90,VFD9ENG6,Gohar (2023),"A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges",,conferencePaper,2023.0,,Claude,"Comprehensive survey provides the first taxonomy of intersectional fairness notions in machine learning, explicitly grounded in Crenshaw's legal intersectionality framework. Demonstrates how multiple sensitive attributes interact to create distinct algorithmic bias forms that traditional single-axis approaches cannot capture. Shows how intersectional identities amplify biases not present in constituent groups, revealing co-constitutive discrimination mechanisms. Systematically reviews mitigation techniques and identifies key challenges.",https://arxiv.org/abs/2305.06969,,,,,,,,,,,,,,
91,SHJQQTI6,Gohar (2023),A Survey on Intersectional Fairness in Machine Learning: Opportunities and Challenges,10.24963/ijcai.2023/742,conferencePaper,2023.0,,Perplexity,"Comprehensive survey examining intersectional fairness in machine learning systems beyond traditional binary fairness approaches. Presents taxonomy of intersectional fairness concepts showing how multiple sensitive attributes interact to create unique discrimination forms. Identifies challenges including data sparsity for intersectional groups and bias mitigation complexity. Demonstrates applications in NLP systems, ranking algorithms, and image recognition. Shows traditional fairness metrics fail for intersectional identities as Black women experience different discrimination than Black people or women separately.",,,,,,,,,,,,,,,
92,RAV6DAFQ,Goldkind (2023),The End of the World as We Know It? ChatGPT and Social Work,10.1093/sw/swad044,journalArticle,2023.0,,ChatGPT,"Brief commentary marking ChatGPT as a pivotal moment for social work. Encourages proactive engagement to steer AI toward just, human-centered outcomes and warns that non-engagement risks value misalignment and inequity.",https://doi.org/10.1093/sw/swad044,,,,,,,,,,,,,,
93,L48P8FBG,Goldkind (2024),Artificial intelligence in social work: An EPIC model for practice,10.1080/0312407X.2025.2488345,journalArticle,2024.0,,Perplexity,"Develops EPIC model (Ethics and justice, Policy development and advocacy, Intersectoral collaboration, Community engagement and empowerment) as structured approach for AI integration in social work. Emphasizes that AI integration must align with social work mission and values through four overlapping components. Ethics and justice aspect addresses bias mitigation and transparent use, while policy development prioritizes client protection. Warns against excessive calibration of AI systems toward trustworthiness that could impair utility, emphasizing importance of transparency in both models and underlying technologies.",https://doi.org/10.1080/0312407X.2025.2488345,,,,,,,,,,,,,,
94,4GYXUS9Y,Goldkind (2024),The end of the world as we know it? ChatGPT and social work,10.1093/sw/swad044,journalArticle,2024.0,,Claude,"Editorial in flagship journal Social Work providing critical reflection on ChatGPT's introduction and implications for social work practice. Addresses how ChatGPT, built on natural language processing, responds to prompts and generates text responses. Notes social work's historical reluctance to embrace new technologies and positions ChatGPT as opportunity to reflect on strategies promoting just technology use. Urges social workers to join cross-disciplinary conversations about AI evolution and advocate for fair use. Calls for profession to move beyond fear and awe toward critical, reflective engagement with AI systems through thoughtful prompting practices.",,,,,,,,,,,,,,,
95,ZGM7K3H6,Goldkind (2024),Artificial intelligence in social work: An EPIC model for practice,10.1080/0312407X.2025.2488345,journalArticle,2024.0,,Gemini,"Develops EPIC model (Ethics and justice, Policy development and advocacy, Intersectoral collaboration, Community engagement and empowerment) as structured approach for AI integration in social work. Emphasizes that AI integration must align with social work mission and values through four overlapping components. Ethics and justice aspect addresses bias mitigation and transparent use, while policy development prioritizes client protection. Warns against excessive calibration of AI systems toward trustworthiness that could impair utility, emphasizing importance of transparency in both models and underlying technologies.",https://doi.org/10.1080/0312407X.2025.2488345,,,,,,,,,,,,,,
96,RR5MJRBZ,Goldkind (2024),Artificial intelligence in social work: An EPIC model for practice,10.1080/0312407X.2025.2488345,journalArticle,2024.0,,ChatGPT,"Develops EPIC model (Ethics and justice, Policy development and advocacy, Intersectoral collaboration, Community engagement and empowerment) as structured approach for AI integration in social work. Emphasizes that AI integration must align with social work mission and values through four overlapping components. Ethics and justice aspect addresses bias mitigation and transparent use, while policy development prioritizes client protection. Warns against excessive calibration of AI systems toward trustworthiness that could impair utility, emphasizing importance of transparency in both models and underlying technologies.",https://doi.org/10.1080/0312407X.2025.2488345,,,,,,,,,,,,,,
97,HZKE8T8I,Gravelmann (2024),Künstliche Intelligenz in der Sozialen Arbeit – Zwischen Bedenken und Optionen,,journalArticle,2024.0,,Perplexity,"Gravelmann analyzes the impact of AI on the social work profession, identifying both opportunities and risks. Potential benefits include AI as communication aid, documentation tool, and data analysis instrument. Critical concerns include the danger of decision delegation to AI systems, potentially reducing professionals to executors. The author warns against automated AI-based procedures that massively intervene in people's lives, especially in child protection. Ethical problems are identified in the lack of objectivity in AI data, which reflects societal power relations and reproduces discriminatory value systems.",https://content-select.com/de/portal/media/view/66472512-3a7c-4fef-b8e6-a5d3ac1b0002,,,,,,,,,,,,,,
98,D4FY2S3R,Gravelmann (2024),Künstliche Intelligenz in der Sozialen Arbeit – Zwischen Bedenken und Optionen,,journalArticle,2024.0,,ChatGPT,"Überblicksbeitrag zu Status, Risiken und Potenzialen von KI in Praxisfeldern der Sozialen Arbeit. Betont Spannungen zwischen Effizienzgewinnen und Werten wie Menschenwürde; skizziert Handlungsfelder und fordert professionellen Diskurs sowie reflektierte Implementationsstrategien.",https://www.beltz.de/fachmedien/sozialpaedagogik_soziale_arbeit/zeitschriften/theorie_und_praxis_der_sozialen_arbeit/artikel/53521-kuenstliche-intelligenz-in-der-sozialen-arbeit-zwischen-bedenken-und-optionen.html,,,,,,,,,,,,,,
99,2565PPJW,Guerra (2023),Feminist reflections for the development of artificial intelligence,,report,2023.0,,Claude,"Comprehensive synthesis of Latin American women's conversations developing AI under feminist frameworks establishes methodological commitments for co-design with communities, gender perspective integration in data science projects, and strategies for women crowd workers. Key feminist AI principles include building diverse intersectional teams, establishing community collaboration agreements, choosing technology based on context rather than consumption, and protecting autonomy through strong anonymity criteria.",https://www.derechosdigitales.org/fair-2023-en/,,,,,,,,,,,,,,
100,8MDXCTA6,Hall (2024),"A systematic review of sophisticated predictive and prescriptive analytics in child welfare: Accuracy, equity, and bias",10.1007/s10560-023-00931-2,journalArticle,2024.0,,Claude,"Systematic review examining how researchers address ethics, equity, bias, and model performance in predictive and prescriptive algorithms used in child welfare settings. Analyzing 67 articles published 2010-2020, reveals inconsistent approaches to measuring and mitigating algorithmic fairness in child welfare applications. Identifies that most predictive models use administrative data reflecting surveillance biases rather than true child maltreatment incidence, leading to discrimination against low-income families and communities of color. Highlights many tools fail to address how automation bias and dehumanization affect social workers' professional judgment.",,,,,,,,,,,,,,,
101,SWB86AYC,Hartshorne (2025),Generative AI and the Future of Digital Literacy: Opportunities for Gender Inclusion,,conferencePaper,2025.0,,Perplexity,"Examines generative AI potential for promoting female inclusion in primary and secondary education while addressing inherent risks of reinforcing gender-specific biases. Qualitative research analyzes systemic effects of generative AI tools on teaching and learning through interviews and focus groups with students and teachers. Results show complex interactions between generative AI technologies and gender dynamics. Key factors for effective AI-supported learning environments include personalized learning experiences, bias-aware content generation, and teachers' role as mediators of AI interactions.",,,,,,,,,,,,,,,
102,YTDMY5W4,Hauck (2025),A framework for the learning and teaching of Critical AI Literacy skills (Version 0.1),,report,2025.0,,Perplexity,"This framework defines Critical AI Literacy as expanding beyond traditional AI literacy to examine how Large Language Models contribute to ongoing epistemic injustices that can lead to significant social and personal harm. It applies equality, diversity, inclusion, and accessibility principles to AI use, emphasizing the importance of critically evaluating AI-generated outputs and engaging in equitable and inclusive prompting practices. Critical AI Literacy is conceptualized as context-specific and treats literacy as a social practice rather than a possession. At advanced levels, it examines AI's potential to shift power relationships and explores how AI contributes to inequalities while considering ways it could help redress power balances.",https://www.open.ac.uk/blogs/learning-design/wp-content/uploads/2025/01/OU-Critical-AI-Literacy-framework-2025-external-sharing.pdf,,,,,,,,,,,,,,
103,6L6WSDC8,Hayati (2024),How Far Can We Extract Diverse Perspectives from Large Language Models?,,conferencePaper,2024.0,,Perplexity,"Systematically evaluates prompting strategies to extract diverse perspectives from LLMs and mitigate dominant group bias in outputs. Measuring subjective tasks such as argumentation and hate speech labeling, the study finds that diversity prompting increases perspective variety and reduces monocultural output tendencies.",https://aclanthology.org/2024.emnlp-main.306.pdf,,,,,,,,,,,,,,
104,RM833D5N,He (2024),On the steerability of large language models,,conferencePaper,2024.0,,Gemini,"Building pluralistic AI requires designing models that are able to be shaped to represent a wide
range of value systems and cultures. Achieving this requires first being able to evaluate
the degree to which a given model is capable
of reflecting various personas. To this end,
we propose a benchmark for evaluating the
steerability of model personas as a function of
prompting. Our design is based on a formal definition of prompt steerability, which analyzes
the degree to which a model’s joint behavioral
distribution can be shifted from its baseline.
By defining steerability indices and inspecting how these indices change as a function of
steering effort, we can estimate the steerability
of a model across various persona dimensions
and directions. Our benchmark reveals that
the steerability of many current models is limited — due to both a skew in their baseline
behavior and an asymmetry in their steerability
across many persona dimensions. We release
an implementation of our benchmark at https:
//github.com/IBM/prompt-steering.",https://aclanthology.org/2025.naacl-long.400.pdf,,,,,,,,,,,,,,
105,E4G328PD,Heinz (2025),Clinical trial of an LLM-based conversational AI psychotherapy,,journalArticle,2025.0,,Claude,"Groundbreaking study representing first randomized controlled trial of generative AI-powered therapy chatbot, Therabot. Trial included 106 participants across United States diagnosed with major depressive disorder, generalized anxiety disorder, or eating disorders. Participants interacted with Therabot via smartphone app over 4-8 weeks. Results showed clinically significant symptom improvements: 51% reduction in depression symptoms, 31% reduction in anxiety symptoms, and 19% reduction in eating disorder concerns, with outcomes comparable to traditional outpatient therapy. Participants reported therapeutic alliance levels comparable to human therapists and engaged average of 6 hours (equivalent to 8 therapy sessions).",https://ai.nejm.org/doi/full/10.1056/AIoa2400802,,,,,,,,,,,,,,
106,WGVC7FQ5,Hermann (2022),Artificial intelligence and mass personalization of communication content—An ethical and literacy perspective,10.1177/14614448211022702,journalArticle,2022.0,en,Manual,"Artificial intelligence (AI) is (re)shaping communication and contributes to (commercial and informational) need satisfaction by means of mass personalization. However, the substantial personalization and targeting opportunities do not come without ethical challenges. Following an AI-for-social-good perspective, the authors systematically scrutinize the ethical challenges of deploying AI for mass personalization of communication content from a multi-stakeholder perspective. The conceptual analysis reveals interdependencies and tensions between ethical principles, which advocate the need of a basic understanding of AI inputs, functioning, agency, and outcomes. By this form of AI literacy, individuals could be empowered to interact with and treat mass-personalized content in a way that promotes individual and social good while preventing harm.",https://journals.sagepub.com/doi/10.1177/14614448211022702,,,,,,,,,,,,,,
107,QXDK8Z6I,Himmelreich (2022),"Artificial Intelligence and Structural Injustice: Foundations for Equity, Values, and Responsibility",,journalArticle,2022.0,,Perplexity,"This work develops a structural injustice approach to AI governance based on Iris Marion Young's theory of structural injustice. The authors argue that structural injustice is a powerful conceptual tool that enables researchers and practitioners to identify, articulate, and potentially even anticipate AI bias. The approach includes both an analytical component (structural explanations) and an evaluative component (justice theory) and provides methodological and normative foundations for diversity, equity, and inclusion values.",https://arxiv.org/abs/2205.02389,,,,,,,,,,,,,,
108,H59BNSX8,James (2023),Algorithmic decision-making in social work practice and pedagogy: confronting the competency/critique dilemma,10.1080/02615479.2023.2195425,journalArticle,2023.0,,Gemini,The world is experiencing an accelerating digital transformation. One aspect of this is the implementation of...[source](https://www.google.com/search?q=https://www.aminer.cn/pub/645d0410d68f896efa94d024/algorithmic-decision-making-in-social-work-practice-and-pedagogy-confronting-the-competency),https://www.tandfonline.com/doi/full/10.1080/02615479.2023.2195425,,,,,,,,,,,,,,
109,CLKAD87H,James (2025),Responsible prompting recommendation: Fostering responsible AI practices in prompting-time,10.1145/3706598.3713365,conferencePaper,2025.0,,Claude,"Presents insights from interviews and user studies with IT professionals exploring prompting practices and develops open-source responsible prompting recommender system. Research reveals responsible prompt recommendations can support novice prompt engineers and raise awareness about Responsible AI in prompting-time, helping people reflect on responsible practices before LLM content generation. Demonstrates that finding right balance between adding social values to prompts and removing potentially harmful content is critical for recommendation systems. Framework highly relevant for social services as it addresses how to design systems encouraging reflective, values-based prompting practices.",,,,,,,,,,,,,,,
110,QG8IAB53,Jarke (2024),Who cares about data? Data care arrangements in everyday organisational practice,10.1080/1369118X.2024.2320917,journalArticle,2024.0,,Claude,"Introduces data care arrangements concept to understand mundane data work in organizations. Demonstrates through empirical research in educational and social service organizations how data care work is distributed across organizational members with different, often conflicting care obligations. Reveals data quality maintenance involves complex sociomaterial configurations of people, infrastructures, routines, and practices. Shows data care work is frequently backgrounded and assumed effortless despite being essential for datafied organizations.",,,,,,,,,,,,,,,
111,NQEUZQF9,Jarke (2025),Datafied ageing futures: Regimes of anticipation and participatory futuring,10.1177/20539517241306363,journalArticle,2025.0,,Claude,"Challenges regimes of anticipation suggesting datafied futures are inevitable, arguing futures are actively made through sociotechnical imaginaries promoted by powerful actors. Explores how to democratize futures-making regarding aging populations, critiquing how current anticipations around data-driven systems and ageist assumptions dominate discussions without adequate participation from affected populations. Develops participatory futuring methodology enabling diverse stakeholders, particularly older adults, to imagine and design alternative futures beyond dominant techno-solutionist narratives.",,,,,,,,,,,,,,,
112,7IVS7X63,Jiang (2022),Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia,,report,2022.0,,Gemini,"Recent studies have separately highlighted significant biases within foundational large language models (LLMs)
against certain nationalities and stigmatized social groups. This research investigates the ethical implications of
these biases intersecting with outputs of widely-used GPT-3.5/4/4o LLMS. Through structured prompt series, we
evaluate model responses to several scenarios involving American and North Korean nationalities with various
mental disabilities. Findings reveal significant discrepancies in empathy levels with North Koreans facing greater
negative bias, particularly when mental disability is also a factor. This underscores the need for improvements in
LLMs designed with a nuanced understanding of intersectional identity.",https://arxiv.org/pdf/2505.17045,,,,,,,,,,,,,,
113,JMAWNUEV,Jin (2025),GLAT: The generative AI literacy assessment test,10.1016/j.caeai.2025.100436,journalArticle,2025.0,en,Manual,,https://linkinghub.elsevier.com/retrieve/pii/S2666920X25000761,,,,,,,,,,,,,,
114,YLAKP7Z2,Jääskeläinen (2025),Intersectional analysis of visual generative AI: The case of Stable Diffusion,10.1007/s00146-025-02207-y,journalArticle,2025.0,,ChatGPT,"This open-access paper provides a feminist intersectional critique of Stable Diffusion through qualitative visual analysis of 180 AI-generated images. Authors examined how power systems like racism, sexism, heteronormativity, ableism, colonialism, and capitalism are reflected in AI outputs. Found that default outputs frequently perpetuate harmful stereotypes and assume a ""white, able-bodied, masculine-presenting"" default subject position. Advocates for social justice-oriented approach to AI by acknowledging cultural-aesthetic biases and engaging in reparative strategies.",https://link.springer.com/article/10.1007/s00146-025-02207-y,,,,,,,,,,,,,,
115,R3VJVFCE,Kamruzzaman (2024),Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,,report,2024.0,,ChatGPT,"Explores how different prompt engineering strategies can mitigate social biases in LLM outputs by analogizing model's reasoning to human cognitive processes. Leverages dual-process cognition theory (System 1 vs System 2) to design prompts that encourage deliberative reasoning. Finds certain prompting techniques significantly reduce biased responses, with up to 13% reduction in stereotypes.",https://arxiv.org/abs/2404.17218,,,,,,,,,,,,,,
116,7W3RGSSG,Kamruzzaman (2024),Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,10.48550/arXiv.2404.17218,journalArticle,2024.0,,Gemini,"Study evaluates 12 prompt-engineering strategies to reduce social biases in Large Language Models using dual-process theory. Findings show Chain-of-Thought prompting follows System 1 patterns rather than expected System 2 reasoning. Human Persona combined with System 2 prompting proved most effective, reducing stereotypical responses by up to 13%. Demonstrates that transparency in prompt design and systematic bias-mitigation approaches contribute significantly to building trust in LLM recommendations.",https://doi.org/10.48550/arXiv.2404.17218,,,,,,,,,,,,,,
117,RARE5UFC,Kamruzzaman (2024),Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,,report,2024.0,,ChatGPT,"This study evaluates 12 prompt strategies across five LLMs, finding that instructing a model to adopt a System 2 (deliberative) reasoning style and a ""human persona"" most effectively reduces stereotypes. Combining these two strategies yielded up to a 13% reduction in stereotypical responses. Contrary to prior assumptions, Chain-of-Thought (CoT) prompting alone was not as effective, showing bias levels similar to a default prompt. The results suggest that prompts encouraging careful, human-like reasoning are key for mitigating bias.",https://arxiv.org/html/2404.17218v1,,,,,,,,,,,,,,
118,Z4YXX9PZ,Kamruzzaman (2024),Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,10.48550/arXiv.2404.17218,journalArticle,2024.0,,Perplexity,"Study evaluates 12 prompt-engineering strategies to reduce social biases in Large Language Models using dual-process theory. Findings show Chain-of-Thought prompting follows System 1 patterns rather than expected System 2 reasoning. Human Persona combined with System 2 prompting proved most effective, reducing stereotypical responses by up to 13%. Demonstrates that transparency in prompt design and systematic bias-mitigation approaches contribute significantly to building trust in LLM recommendations.",https://doi.org/10.48550/arXiv.2404.17218,,,,,,,,,,,,,,
119,UIIDCXLB,Kamruzzaman (2024),Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,10.48550/arXiv.2404.17218,journalArticle,2024.0,,ChatGPT,"Study evaluates 12 prompt-engineering strategies to reduce social biases in Large Language Models using dual-process theory. Findings show Chain-of-Thought prompting follows System 1 patterns rather than expected System 2 reasoning. Human Persona combined with System 2 prompting proved most effective, reducing stereotypical responses by up to 13%. Demonstrates that transparency in prompt design and systematic bias-mitigation approaches contribute significantly to building trust in LLM recommendations.",https://doi.org/10.48550/arXiv.2404.17218,,,,,,,,,,,,,,
120,55ZR64VU,Kaneko (2024),Debiasing prompts for gender bias in large language models,,report,2024.0,,Gemini,"Dual process theory posits that human cognition arises via two systems. System 1, which is a quick, emotional, and intuitive process, which is subject to cognitive biases, and System 2, is a slow, onerous, and deliberate process. NLP researchers often compare zero-shot prompting in LLMs to System 1 reasoning and chain-of-thought (CoT) prompting to System 2. In line with this interpretation, prior research has found that using CoT prompting in LLMs leads to reduced gender bias. We investigate the relationship between bias, CoT prompting, a debiasing prompt, and dual process theory in LLMs directly. We compare zero-shot CoT, debiasing, and a variety of dual process theory-based prompting strategies on two bias datasets spanning nine different social bias categories. We incorporate human and machine personas to determine whether the effects of dual process theory in LLMs exist independent of explicit persona models or are based on modeling human cognition. We find that a human persona, debiasing, System 2, and CoT prompting all tend to reduce social biases in LLMs, though the best combination of features depends on the exact model and bias category—resulting in up to a 19 percent drop in stereotypical judgments by an LLM",https://arxiv.org/html/2404.17218v3,,,,,,,,,,,,,,
121,CYZQ6XPK,Kaneko (2024),Evaluating gender bias in large language models via chain-of-thought prompting,,report,2024.0,,ChatGPT,"This study investigates if Chain-of-Thought (CoT) prompting reduces implicit gender bias in LLMs. Using a synthetic task of counting gendered words, the authors found that without step-by-step prompting, models made biased errors. CoT prompting, which forced the model to explicitly label each word's gender before counting, significantly reduced these mistakes. This suggests that guiding the model through an explicit reasoning process makes it rely more on logic than on stereotypes, thereby mitigating bias.",https://arxiv.org/abs/2401.15585,,,,,,,,,,,,,,
122,7L93JBLR,Karagianni (2025),Gender in a stereo-(gender)typical EU AI law: A feminist reading of the AI Act,10.1017/cfl.2025.12,journalArticle,2025.0,,ChatGPT,"This peer-reviewed article offers a feminist critique of the European Union’s AI Act, arguing that the law’s current bias mitigation provisions only superficially address gendered risks and fail to confront deeply embedded structural biases. Using frameworks like hermeneutical injustice and feminist legal theory, Karagianni shows that AI systems can perpetuate historical power imbalances and systemic discrimination against women and marginalized groups. The AI Act’s pre-market and post-market measures are analyzed to reveal how they often reinforce, rather than challenge, algorithmic discrimination. The author concludes that effective AI governance must go beyond technical fixes, incorporating an intersectional perspective and substantive equality principles. She calls for feminist-informed revisions to the AI Act – emphasizing gender inclusivity, intersectionality, and accountability – to ensure AI regulation actively dismantles (instead of inadvertently upholding) existing power asymmetries.",https://www.cambridge.org/core/journals/cambridge-forum-on-ai-law-and-governance/article/gender-in-astereogendertypical-eu-ai-law-a-feminist-reading-of-the-ai-act/E9DEFC1E114CBE577D737EC616610921,,,,,,,,,,,,,,
123,3QUWCYVW,Kattnig (2024),Assessing trustworthy AI: Technical and legal perspectives of fairness in AI,10.1016/j.clsr.2024.106053,journalArticle,2024.0,,ChatGPT,"Kattnig et al. examine the disconnect between existing AI fairness techniques and the requirements of non-discrimination law, highlighting that many algorithmic bias mitigation methods do not meet legal standards for equality. Focusing on the EU context (with particular attention to the forthcoming AI Act and established non-discrimination directives), the authors review state-of-the-art bias mitigation approaches – from pre-processing data fixes to in-processing algorithms – and evaluate them against legal concepts of fairness and equality. They discuss how ambiguous legal frameworks and the difficulty of defining “fairness” pose challenges: for instance, fairness has multiple interpretations (individual vs. group fairness, formal vs. substantive equality) and is understood differently across disciplines. The paper argues for an interdisciplinary legal methodology to complement technical solutions. In practice, this means moving beyond purely quantitative parity metrics and ensuring AI systems comply with human rights and equality principles (e.g. ensuring de facto non-discrimination for all data subjects). By contrasting algorithms with legal norms, the study underlines that trustworthy AI requires more than technical robustness – it demands alignment with justice and accountability frameworks.",https://graz.elsevierpure.com/ws/portalfiles/portal/89954869/1-s2.0-S0267364924001195-main.pdf,,,,,,,,,,,,,,
124,JC7X3MM7,Kawakami (2022),"Improving human-AI partnerships in child welfare: Understanding worker practices, challenges, and desires for algorithmic decision support",10.1145/3491102.3517439,conferencePaper,2022.0,,Claude,"Empirical study examining how child maltreatment hotline workers interact with Allegheny Family Screening Tool, an AI-based decision support system. Through interviews and contextual inquiries, found workers' reliance on algorithmic predictions guided by four key factors: knowledge of contextual information beyond AI model capabilities, beliefs about system limitations, organizational pressures around tool use, and awareness of misalignments between algorithmic predictions and their own decision-making objectives. Reveals discrimination risks stemming from workers lacking adequate training on tool's data sources and limitations.",,,,,,,,,,,,,,,
125,DZ39L7GC,Kim (2021),Why and What to Teach: AI Curriculum for Elementary School,10.1609/aaai.v35i17.17833,journalArticle,2021.0,,Manual,"With the rapid technological change of society with Artificial Intelligence, elementary schools' goal should be to prepare the next generations according to competencies. We propose an AI curriculum to cultivate students' AI literacy to answer the question of ‘why and what to teach’ on AI. The proposed AI curriculum focuses on achieving AI literacy based on three competencies: AI Knowledge, AI Skill, and AI Attitude. We anticipate that the proposed curriculum will equip students with core competencies for the future with AI.",https://ojs.aaai.org/index.php/AAAI/article/view/17833,,,,,,,,,,,,,,
126,GP4JDSI8,Klein (2024),Data Feminism for AI,10.1145/3630106.3658543,conferencePaper,2024.0,,Perplexity,"Presents intersectional feminist principles for just, ethical, and sustainable AI research. Extends seven Data Feminism principles to AI contexts: examine power, challenge power, rethink binaries and hierarchies, elevate emotion and embodiment, consider context, embrace pluralism, and make work visible. Proposes two additional principles on environmental impacts and consent. Framework helps identify and mitigate predictable harms before releasing discriminatory systems. Practical applications include participatory ML design processes and analysis of online advertising systems.",,,,,,,,,,,,,,,
127,YMYHLMFS,Klein (2024),Data feminism for AI,10.1145/3630106.3658543,conferencePaper,2024.0,,Claude,"Extends the influential ""Data Feminism"" framework to AI research, presenting intersectional feminist principles for conducting equitable, ethical, and sustainable AI research. Rearticulates original seven data feminism principles specifically for AI contexts and introduces two new principles addressing environmental impact and consent, providing concrete methodological guidance.",https://doi.org/10.1145/3630106.3658543,,,,,,,,,,,,,,
128,D2EGAVKZ,Klein (2024),Data Feminism for AI,,conferencePaper,2024.0,,Perplexity,"This work extends the seven principles of Data Feminism to the AI context and introduces two additional principles on environmental impacts and consent. The authors argue that feminist perspectives are essential for understanding and combating the unequal, undemocratic, extractive, and exclusionary forces in AI research. Their intersectional feminist principles aim to: (1) identify unequal power relations in AI systems, (2) mitigate predictable harms proactively, and (3) inspire creative, collective approaches for a more just world.",https://facctconference.org/static/papers24/facct24-7.pdf,,,,,,,,,,,,,,
129,QIQR449A,Klinge (2024),A sociolinguistic approach to stereotype assessment in large language models,,conferencePaper,2024.0,,Gemini,"Social categories and stereotypes embedded in language can introduce data bias into the training of Large Language Models (LLMs).
Despite safeguards, these biases often persist in model behavior,
potentially leading to representational harm in outputs. While sociolinguistic research provides valuable insights into the formation
and spread of stereotypes, NLP approaches for bias evaluation rarely
draw on this foundation and often lack objectivity, precision, and interpretability. To fill this gap, we propose a new approach to assess
stereotypes by detecting and quantifying the linguistic indication
of a stereotype. We derive linguistic indicators from the Social Category and Stereotype Communication (SCSC) framework indicating
strong social category formulation and stereotyping in language,
and use them to build a categorization scheme. We use in-context
learning to instruct LLMs to examine the linguistic properties of a
sentence containing stereotypes, providing a basis for a fine-grained
stereotype assessment. We develop a scoring function to measure
linguistic indicators of stereotypes based on empirical evaluation.
Our annotations of stereotyped sentences reveal that these linguistic indicators explain the strength of a stereotype. The models perform well in detecting and classifying linguistic indicators used to
denote a category, but sometimes struggle with accurately evaluating the described associations. The use of more few-shot examples
significantly improves the performance. Model performance increases with size, as Llama-3.3-70B-Instruct and GPT-4 achieve
comparable results that surpass those of Mixtral-8x7B-Instruct,
GPT-4-mini and Llama-3.1-8B-Instruct_4bit. Code and annotations can be found in https://github.com/r-goerge/DetectingLinguistic-Indicators-for-Stereotype-Assessment-with-LLMs.",https://facctconference.org/static/docs/facct2025-206archivalpdfs/facct2025-final1618-acmpaginated.pdf,,,,,,,,,,,,,,
130,XZ6Z8A9C,Knowles (2023),Trustworthy AI and the Logics of Intersectional Resistance,10.1145/3593013.3593986,conferencePaper,2023.0,,ChatGPT,"Critically examines mainstream ""Trustworthy AI"" frameworks from an intersectional feminist perspective, arguing that traditional AI ethics often privilege dominant groups and fail marginalized communities. Suggests reframing trustworthy AI principles to incorporate stewardship, care, humility, and empowerment, addressing intersectional injustices through power-sharing and structural change.",https://doi.org/10.1145/3593013.3593986,,,,,,,,,,,,,,
131,H3STST88,Kojima (2022),Large language models are zero-shot reasoners,,conferencePaper,2022.0,,Manual,,,,,,,,,,,,,,,,
132,7VFNS5R3,Kong (2021),Evaluation of an artificial intelligence literacy course for university students with diverse study backgrounds,10.1016/j.caeai.2021.100026,journalArticle,2021.0,en,Manual,,https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000205,,,,,,,,,,,,,,
133,Q4LK53XW,Kong (2022),"Are ""Intersectionally Fair"" AI Algorithms Really Fair to Women of Color? A Philosophical Analysis",10.1145/3531146.3533074,conferencePaper,2022.0,,Perplexity,"Diese philosophische Analyse identifiziert drei fundamentale Probleme mit der dominanten Interpretation intersektionaler Fairness in der KI: Die Fokussierung auf Identitätskategorien statt Unterdrückungsstrukturen, ein Dilemma zwischen statistischer Parität und substanzieller Gerechtigkeit, sowie die Vernachlässigung historischer Kontexte. Kong argumentiert, dass echte intersektionale Fairness über statistische Metriken hinausgehen und strukturelle Unterdrückungssysteme (Rassismus, Sexismus) direkt adressieren muss. Die Arbeit fordert eine Verschiebung von rein technischen zu kritisch-theoretischen Ansätzen.",https://doi.org/10.1145/3531146.3533074,,,,,,,,,,,,,,
134,BBLJ4RG3,Kong (2024),Developing an artificial intelligence literacy framework: Evaluation of a literacy course for senior secondary students using a project-based learning approach,10.1016/j.caeai.2024.100214,journalArticle,2024.0,en,Manual,,https://linkinghub.elsevier.com/retrieve/pii/S2666920X24000158,,,,,,,,,,,,,,
135,FJQ6XYHH,Kong (2025),Artificial Intelligence (AI) literacy – an argument for AI literacy in education,10.1080/14703297.2024.2332744,journalArticle,2025.0,en,Manual,,https://www.tandfonline.com/doi/full/10.1080/14703297.2024.2332744,,,,,,,,,,,,,,
136,THGC3PA2,Kubes (2024),Feministische KI – Künstliche Intelligenz für alle? [Feminist AI – Artificial intelligence for everyone?],,webpage,2024.0,,Claude,"Innovative interdisciplinary seminar teaches students to critically analyze everyday AI applications from sociotechnical feminist perspectives across four domains: love, robots, work, and creativity. Students analyze AI within androcentric, Eurocentric, anthropocentric, and capitalist-patriarchal structures. Curriculum combines theoretical foundations with practical application through ""queerbot"" design workshops that reimagine AI beyond normative dichotomies, demonstrating concrete pedagogical approaches for implementing feminist AI literacy education.",https://blogs.fu-berlin.de/toolbox/2024/04/08/feministische-ki-kuenstliche-intelligenz-fuer-alle/,,,,,,,,,,,,,,
137,BR2LG8LD,Kumar (2024),How AI hype impacts the LGBTQ+ community,10.1007/s43681-024-00423-8,journalArticle,2024.0,,Perplexity,"Die Studie analysiert, wie der Hype um KI heteronormative Annahmen verstärkt. Sie führt Fallstudien zur Gesichtserkennung, Content-Moderation und Geschlechtsklassifikation durch und zeigt auf, wie queere Identitäten algorithmisch marginalisiert werden.",https://doi.org/10.1007/s43681-024-00423-8,,,,,,,,,,,,,,
138,HT9ZSI9U,Kutscher (2020),Handbuch Soziale Arbeit und Digitalisierung,,book,2020.0,,Gemini,"Dieses umfassende Handbuch mit über 50 Beiträgen behandelt erstmals systematisch Digitalisierung in Bezug auf Disziplin und Praxis der Sozialen Arbeit. Das 658-seitige Werk beleuchtet aus verschiedenen disziplinären Perspektiven gesellschaftliche Entwicklungen, Diskurse, digitalisierte Formen der Dienstleistungserbringung, Profession, Organisation und Handlungsfelder sowie neue Herausforderungen für Forschung. Zentrale Themen umfassen Mediatisierung, Akteur-Netzwerk-Theorie, ethische Fragen, informationelle Selbstbestimmung, Datenschutz, Social Media, E-Government, digitalisierte Kinder- und Jugendhilfe, Medienpädagogik und Sozialwirtschaft. Das Handbuch verfolgt einen über technisches Verständnis hinausgehenden Begriff von Digitalität und fokussiert auf soziotechnische Arrangements sowie deren Folgen für Akteure, Formen und Rahmenbedingungen Sozialer Arbeit. Mit Perspektiven auf Organisation, Fachkräfte, Adressat*innen und Erbringungsformen werden Möglichkeiten, Risiken und offene Fragestellungen diskutiert.",https://www.beltz.de/fileadmin/beltz/leseproben/978-3-7799-5258-9.pdf,,,,,,,,,,,,,,
139,6Y5EPQRR,Kutscher (2023),"Positionings, challenges, and ambivalences in children's and parents' perspectives in digitalized familial contexts",,bookSection,2023.0,,Claude,"Examines family dynamics in digitalized contexts, analyzing tensions between children's digital participation rights and parental protection responsibilities. Presents research revealing ambivalences in both parental and children's perspectives: parents struggle between enabling children's digital competence and protecting them from risks; children experience tension between desire for autonomy and need for guidance. Addresses sharenting (parents sharing children's images/information online), examining conflicts between parental expression rights and children's privacy rights.",,,,,,,,,,,,,,,
140,BKSU66QB,Kutscher (2024),Digitalität und Digitalisierung als Gegenstand der Sozialen Arbeit,,bookSection,2024.0,,Claude,"Differentiates between digitalization (technical processes of making things digital) and digitality (sociotechnical transformations of social practices and relations), arguing social work must engage with both technological changes and their social implications. Demonstrates how algorithms, data-driven systems, and digital platforms reshape professional practice, client relationships, and social inequalities. Argues digitalization fundamentally transforms social contexts where social work operates rather than merely adopting new tools, requiring critical engagement examining power relations, surveillance mechanisms, and social justice implications.",,,,,,,,,,,,,,,
141,DUV4TUG3,Lahoti (2023),Improving diversity of demographic representation in people entities in Large Language Models,,conferencePaper,2023.0,,Perplexity,Introduces the Collective-Critique and Self-Voting (CCSV) prompting method to systematically enhance demographic diversity in LLM outputs. The approach leverages LLMs' internal capacity for diversity reasoning and combines critique and self-voting mechanisms to iteratively improve output balance while maintaining model performance.,https://aclanthology.org/2023.emnlp-main.643/,,,,,,,,,,,,,,
142,Z9BIKVS3,Laine (2025),Avoiding Catastrophe Through Intersectionality in Global AI Governance,,report,2025.0,,Perplexity,"Dieses Working Paper nutzt einen feministischen Policy-Analyse-Rahmen, der auf fünf thematischen Bereichen basiert: Intersektionalität, Kontext, Neutralität, Macht und Gerechtigkeit. Die Forschung schlägt einen feministischen KI-Policy-Rahmen vor, der Entscheidungsträger und Stakeholder ermutigt, potenzielle KI-Sicherheitsprojekte in Übereinstimmung mit vier Zielen zu bewerten: Förderung der Intersektionalität, Bereitstellung diverser Kontexte, Bekämpfung der Neutralität und transformative Gerechtigkeit. Der Ansatz wächst aus Ansätzen zur feministischen KI-Governance, die die Notwendigkeit betonen, KI als Produkt struktureller Ungleichheiten zu sehen.",https://www.cigionline.org/publications/avoiding-catastrophe-through-intersectionality-in-global-ai-governance/,,,,,,,,,,,,,,
143,AXEIVEW3,Lanzetta (2024),Artificial Intelligence Competence Needs for Youth Workers,10.5281/ZENODO.11525357,journalArticle,2024.0,en,Manual,"The rapid developments in AI technology and the rise of accessible AI-powered tools are transforming the way we live, work and learn. While young people have already warmly embraced these solutions, with Gen Z being the most active users and experimenters of Generative AI (Microsoft, 2024), there is a sense of confusion and fear among youth workers about the future of how AI tools are going to be used in the youth sector, mixed with diverse emotions and viewpoints ranging from apprehension, scepticism, resistance to feelings of enthusiasm and recognition of the significance of AI's role in the field (Pawluczuk, 2023).

This study aims to advance knowledge on the specific competencies required by youth workers to effectively integrate AI into their professional activities, as well as picture the current and potential use of AI for youth professionals.  

The publication is part of the Artificial Intelligence for Youth Work (AI4YouthWork)project, a pioneering initiative under the Erasmus+ programme, co-funded by the European Union, dedicated to enhancing the youth sector across Europe through the integration of artificial intelligence (AI). The project unites four organisations - Lascò from Italy, TEAM4Excellence from Romania, Kyttaro Enallaktikon Anazitiseon Neon from Greece, and Contextos from Portugal -, aspiring to contribute to increasing youth professionals' capacity to harness AI's potential to enhance the quality, attractiveness and effectiveness of their work, and prepare young people to thrive in AI-powered environments. 





Chapter 1 introduces the project, highlighting the steps and methodological approaches to achieving the main objectives and the expected results.




Chapter 2, dedicated to the research methodology, outlines the approach and techniques used to conduct this study. It includes the research design, data collection methods through systematic review, focus groups and interviews, data analysis procedures, as well as limitations and criteria for ensuring the validity and reliability of the findings.




Chapter 3 presents the results of the desk research conducted by the consortium partners to explore the intersections of artificial intelligence, youth and youth work.  The chapter is divided into four main sections, addressing an introduction to AI, the impact of AI on youth, the role of youth workers in the AI revolution, and practical applications of AI in youth work settings.




Chapter 4 outlines the needs, challenges, and tasks involved in integrating AI into youth work, presenting the results of focus group discussions which have been conducted in each partner country.




Chapter 5 sets out the publication's conclusions, formulating recommendations for the development of an AI Competence Framework for Youth Workers, and enhancing the capacity of youth professionals to harness AI in their work.",https://zenodo.org/doi/10.5281/zenodo.11525357,,,,,,,,,,,,,,
144,CSJS9JGH,Latif (2023),"AI gender bias, disparities, and fairness: Does training data matter?",,report,2023.0,,Perplexity,"Empirische Analyse von Geschlechterbias in Bewertungssystemen mit BERT und GPT-3.5. Mixed-gender Trainingsdaten reduzierten Bias, aber verstärkten Unterschiede. Drei Bias-Metriken angewendet.",https://arxiv.org/html/2312.10833v2,,,,,,,,,,,,,,
145,CHJQ52DC,Latif (2024),"AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?",,journalArticle,2024.0,,Perplexity,"Empirical study examining gender bias in large language models through analysis of over 6000 evaluated student responses from 70 male and 70 female participants. Research uses fine-tuned BERT models and GPT-3.5 to evaluate various training data configurations: gender-specific versus mixed datasets. Three evaluation metrics applied: Scoring Accuracy Difference for bias assessment, Mean Score Gaps (MSG) for gender disparities, and Equalized Odds (EO) for fairness measurement. Results show mixed-gender trained models produce significantly better results than gender-specific models with reduced MSG and fairer predictions.",https://arxiv.org/html/2312.10833v4,,,,,,,,,,,,,,
146,LBLF9BCW,Lau (2023),Dipper: Diversity in Prompts for Producing Large Language Model Outputs,,conferencePaper,2023.0,,Perplexity,"Presents 'Dipper', an LLM prompting ensemble framework that systematically deploys a diverse set of prompts in parallel to improve the breadth of generated perspectives, including those of minority or marginalized groups. This training-free technique enhances demographic and perspective diversity without performance degradation.",https://www.comp.nus.edu.sg/~greglau/assets/pdf/dipper_neurips_mint.pdf,,,,,,,,,,,,,,
147,4BRSDIPP,Laupichler (2023),Development of the “Scale for the assessment of non-experts’ AI literacy” – An exploratory factor analysis,10.1016/j.chbr.2023.100338,journalArticle,2023.0,en,Manual,,https://linkinghub.elsevier.com/retrieve/pii/S2451958823000714,,,,,,,,,,,,,,
148,Y8J3HI9J,Lin (2022),Artificial Intelligence in a Structurally Unjust Society,,journalArticle,2022.0,,Perplexity,"This study argues that AI bias represents a form of structural injustice that arises when AI systems interact with other social factors to exacerbate existing social inequalities. Using the example of AI in healthcare, the authors show that the goal of AI fairness is to strive for a more just social structure through appropriate development and use of AI systems. They argue that all actors involved in the unjust social structure bear shared responsibility to join collective actions for structural reform and offer practical recommendations for various social positions.",https://ojs.lib.uwo.ca/index.php/fpq/article/download/14191/12139/38443,,,,,,,,,,,,,,
149,MA3LBJS6,Linnemann (2023),Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit: Eine exemplarische arbeitsfeldübergreifende Betrachtung des Natural Language Processing (NLP),10.1007/s12592-023-00455-7,journalArticle,2023.0,,Claude,"Examines Natural Language Processing technologies' significance for social work practice using Staub-Bernasconi's action theory and media equation theory. Analyzes how NLP implementation creates opportunities (enhanced participation, low-threshold access, broader data analysis) and risks (modularized separation of social work activities, potential dehumanization). Emphasizes critical examination of whether AI supports or displaces authentic social work functions.",,,,,,,,,,,,,,,
150,JNLPSHD5,Linnemann (2023),Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit: Eine exemplarische arbeitsfeldübergreifende Betrachtung des Natural Language Processing (NLP),10.1007/s12592-023-00455-7,journalArticle,2023.0,,ChatGPT,Theoriegeleitete Analyse zu Chancen und Risiken von NLP für die Soziale Arbeit. Diskutiert mögliche Auslagerung sozialarbeiterischer Tätigkeiten vs. Potenziale für Teilhabe und Zugänge; ordnet KI entlang handlungstheoretischer Konzepte ein und plädiert für kritische Auseinandersetzung im Einklang mit professionsethischen Werten.,https://link.springer.com/article/10.1007/s12592-023-00455-7,,,,,,,,,,,,,,
151,7FBB2KUC,Linnemann (2023),Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit,10.1007/s12592-023-00455-7,journalArticle,2023.0,de,Gemini,"Die Bedeutung des Einsatzes von Verfahren, die unter dem Begriff der Künstlichen Intelligenz (KI) zusammenzufassen sind, wird sowohl für gesellschaftliche Prozesse als auch den Auftrag an die Soziale Arbeit zunehmend erkannt und diskutiert. Mit diesem Artikel wird ein Beitrag zum Diskurs geleistet, indem vertieft der Bereich der Sprachverarbeitung durch KI, das Natural Language Processing (NLP), in den Blick genommen wird. Verarbeitung natürlicher Sprache ist aufgrund der hohen Bedeutung kommunikativer Prozesse für die Praxis der Sozialen Arbeit von besonderer Relevanz, zugleich wird die Profession der Sozialen Arbeit tangiert. Bezugnehmend auf Staub-Bernasconis Handlungstheorie werden Implikationen und Diskussionspunkte von NLP identifiziert und diskutiert. Zudem werden mögliche Gratifikationen für Klient*innen herausgearbeitet, die sich u. a. aus der Wirkung und sozialen Interaktion ergeben. Hier wird die Media-Equation-Theorie von Nass und Reeves als Erkenntnisfolie herangezogen. Vor diesen Perspektiven ergeben sich sowohl Risiken (u. a. die Gefahr einer modularisierten Herauslösung genuin sozialarbeiterischer Tätigkeit) als auch Chancen (u. a. Teilhabe, niederschwelliger Zugang, Zugriff auf breitere Datenbasis).",https://doi.org/10.1007/s12592-023-00455-7,,,,,,,,,,,,,,
152,L6PH7GDL,Linnemann (2025),Künstliche Intelligenz in der Sozialen Arbeit: Grundlagen für Theorie und Praxis,,book,2025.0,,Claude,"First major German-language systematic treatment of AI in social work from multiple perspectives. Bridges technological progress and ethics, treating AI theoretically and in practice-oriented applications. Addresses technical AI basics for social work, ethical and legal frameworks, bias and discrimination in training data, automation bias risks, development of AI competencies in education and organizations, and specific application fields. Emphasizes responsible, reflective engagement with AI enriching social work without losing sight of human integrity and professional responsibility.",https://doi.org/10.3262/978-3-7799-8562-4,,,,,,,,,,,,,,
153,64DQYVVB,Liu (2025),More or less wrong: A benchmark for directional bias in LLM comparative reasoning,,report,2025.0,,Gemini,"Large language models (LLMs) are known to be sensitive to input phrasing, but the mechanisms by which semantic cues shape reasoning remain poorly understood. We investigate this phenomenon in the context of comparative math problems with objective ground truth, revealing a consistent and directional framing bias: logically equivalent questions containing the words “more”, “less”, or “equal” systematically steer predictions in the direction of the framing term. To study this effect, we introduce MathComp, a controlled benchmark of 300 comparison scenarios, each evaluated under 14 prompt variants across three LLM families. We find that model errors frequently reflect linguistic steering—systematic shifts toward the comparative term present in the prompt. Chain-of-thought prompting reduces these biases, but its effectiveness varies: free-form reasoning is more robust, while structured formats may preserve or reintroduce directional drift. Finally, we show that including demographic identity terms (e.g., “a woman”, “a Black person”) in input scenarios amplifies directional drift, despite identical underlying quantities, highlighting the interplay between semantic framing and social referents. These findings expose critical blind spots in standard evaluation and motivate framing-aware benchmarks for diagnosing reasoning robustness and fairness in LLMs.",https://arxiv.org/html/2506.03923v1,,,,,,,,,,,,,,
154,UHEM78MX,Long (2020),What is AI Literacy? Competencies and Design Considerations,10.1145/3313831.3376727,conferencePaper,2020.0,en,Manual,,https://dl.acm.org/doi/10.1145/3313831.3376727,,,,,,,,,,,,,,
155,QZJL6KBZ,Lund (2025),"Algorithms, artificial intelligence and discrimination",,report,2025.0,,Perplexity,"Dieser norwegische Regierungsbericht überprüft Schlüsselelemente des norwegischen Gleichstellungs- und Antidiskriminierungsgesetzes mit primärem Fokus auf algorithmische Diskriminierung. Der Bericht diskutiert die mögliche Einführung spezifischer Definitionen direkter und indirekter algorithmischer Diskriminierung und schlägt die Schaffung einer spezifischen Bestimmung zu rechtmäßiger algorithmischer Differenzialbehandlung vor. Die Komplexität algorithmischer Systeme erschwert die Unterscheidung zwischen direkter und indirekter Diskriminierung, was neue rechtliche Ansätze erfordert.",https://ldo.no/content/uploads/2025/05/Algorithms-artificial-intelligence-and-discrimination-report.pdf,,,,,,,,,,,,,,
156,HMDFMBV3,Ma (2023),Intersectional Stereotypes in Large Language Models: Dataset and Analysis,10.18653/v1/2023.findings-emnlp.575,conferencePaper,2023.0,,Perplexity,"This EMNLP paper introduces a dataset for studying intersectional stereotypes and applies it to three LLMs. Results reveal emergent stereotypes not predictable from single-attribute analysis. Prompt engineering reduces but does not eliminate such patterns, highlighting persistent biases in generated narratives.",https://aclanthology.org/2023.findings-emnlp.575.pdf,,,,,,,,,,,,,,
157,7WMD5JQ6,Maeda (2025),"Toward Agency‐Centered <span style=""font-variant:small-caps;"">AI</span> Literacy: A Scoping Review",10.1002/pra2.1472,journalArticle,2025.0,en,Manual,"ABSTRACT
            Digital literacy is well‐studied across disciplines, with established attention to core competencies and social inequalities. However, artificial intelligence (AI) literacy remains underexplored. To address this gap, we conducted a scoping review on AI literacy to: (1) consolidate current definitions and pinpoint conceptual gaps, (2) evaluate methodological approaches and their relevance in practice, and (3) examine how social inequalities are considered in AI literacy studies. Definitions of AI literacy are inconsistent across and within disciplines, and most studies do not consider social factors. Most definitions focus on knowledge and skill acquisition, framing AI literacy as a suite of acquired competencies. We argue that current understandings of AI literacy need to expand to include informed decision‐making, critical engagement, and resistance to technological coercion by taking an agency‐driven approach. These insights can guide researchers, educators, and policymakers in fostering an agency‐centered AI literacy that empowers individuals in an increasingly AI‐mediated world.",https://asistdl.onlinelibrary.wiley.com/doi/10.1002/pra2.1472,,,,,,,,,,,,,,
158,2SLISKSW,McCrory (2024),Avoiding catastrophe through intersectionality in global AI governance,,report,2024.0,,ChatGPT,"In this working paper, McCrory argues that prevailing global AI governance efforts (especially those focused on “AI safety” and existential risks) lack an adequate intersectional, feminist perspective, which is needed to avert not just hypothetical future catastrophes but ongoing injustices. The paper analyzes seven prominent international AI policy initiatives (e.g. global AI accords and principles) against criteria derived from feminist theory, such as intersectionality, context, and power dynamics. McCrory finds that these high-level policies often remain techno-centric: they invoke abstract risks or neutrality, but fail to engage with how AI harms are unevenly distributed along lines of gender, race, and class. For example, current AI “safety” pledges seldom consider the lived experiences of marginalized communities or the way existing structural inequalities are mirrored in AI systems. The author contends that treating AI governance as a purely technical, top-down process is misguided; instead, governance should include meaningful participation from under-represented groups and incorporate feminist insights about power and oppression. The paper’s recommendations call for centering intersectionality in AI policy: explicitly addressing how AI-related risks and harms intersect with social identity and historical injustices, and ensuring that any frameworks for AI risk management or ethics actively involve those who have been marginalized by past technological developments.",https://www.cigionline.org/documents/3375/DPH-paper-Laine_McCrory.pdf,,,,,,,,,,,,,,
159,YPYQ2TCL,Mei (2023),Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia,,report,2023.0,,Manual,,https://arxiv.org/pdf/2505.17045,,,,,,,,,,,,,,
160,BTLTEA6Y,Meilvang (2024),Decision support and algorithmic support: The construction of algorithms and professional discretion in social work,,bookSection,2024.0,,Claude,"Critical analysis examining three decision-support algorithms developed for Danish municipalities in child and family social work, analyzing how they affect professional discretion despite claims to merely support professionals. Demonstrates how algorithmic systems designed to minimize subjective judgment and promote efficiency actually embody positivist assumptions that professional discretion can and should be eliminated. Key findings reveal how political actors favor standardized, automated approaches to avoid high-profile cases, effectively negating professional judgment central to ethical social work practice. Drawing on street-level bureaucracy and digitalization literature, argues that framing algorithms as neutral decision support obscures their role in fundamentally restructuring professional autonomy, expertise, and nature of care relationships.",,,,,,,,,,,,,,,
161,PI5H2LZ2,Moreau (2024),"Failing our youngest: On the biases, pitfalls, and risks in a decision support algorithm used for child protection",10.1145/3630106.3658906,conferencePaper,2024.0,,Claude,"Critical empirical study examining child protection decision support algorithm deployed in Danish municipalities, analyzing its biases and implementation challenges. Using real administrative data from Denmark's child welfare system, evaluated algorithm's predictions against actual case outcomes and found significant biases including disproportionate impacts on immigrant families and systematic errors in risk assessment. Results revealed concerning patterns of false positives for marginalized communities and questioned algorithm's validity for high-stakes decision-making. Documents actual harms from deployed systems.",,,,,,,,,,,,,,,
162,NUVZI357,Mosene (2023),Feministische Netzpolitik und Künstliche Intelligenz in der politischen Bildung [Feminist network politics and artificial intelligence in political education],,webpage,2023.0,,Claude,"Examines intersections of feminist network politics and AI within political education, emphasizing intersectional feminist perspectives on digital technologies. Argues feminist network politics involves supporting AI researchers and activists working to eliminate bias in development and outcomes. Discusses how traditional gender roles are reinforced through AI systems and advocates for political education helping users understand how technologies function, emerged, which societal ideas they reflect, and where critical discourse is needed.",https://www.politische-medienkompetenz.de/debatte/ki-und-intersektionalitaet/,,,,,,,,,,,,,,
163,7L78MV2V,Navigli (2023),"Biases in large language models: Origins, inventory and discussion",10.1145/3597307,journalArticle,2023.0,,ChatGPT,"Provides an overview of various social biases manifested by large language models and discusses their root causes. Examines how training data selection leads to bias and surveys different types of biases including gender, racial/ethnic, sexual orientation, age, religious and cultural biases. Compiles an inventory of biased behaviors and discusses emerging approaches to measure and mitigate such biases.",https://doi.org/10.1145/3597307,,,,,,,,,,,,,,
164,CLADZZKS,Ng (2021),Conceptualizing AI literacy: An exploratory review,10.1016/j.caeai.2021.100041,journalArticle,2021.0,en,Manual,"Highlights

    •
    Research on AI literacy has not been widely published in educational journals.
    •
    An exploratory review was conducted to conceptualize the newly emerging concept “AI literacy” to define, teach and evaluate AI literacy.
    •
    This review proposed four aspects (i.e., know and understand, use, evaluate and ethical issues) for fostering AI literacy based on the adaptation of classic literacies.

Abstract
Artificial Intelligence (AI) has spread across industries (e.g., business, science, art, education) to enhance user experience, improve work efficiency, and create many future job opportunities. However, public understanding of AI technologies and how to define AI literacy is under-explored. This vision poses upcoming challenges for our next generation to learn about AI. On this note, an exploratory review was conducted to conceptualize the newly emerging concept “AI literacy”, in search for a sound theoretical foundation to define, teach and evaluate AI literacy. Grounded in literature on 30 existing peer-reviewed articles, this review proposed four aspects (i.e., know and understand, use and apply, evaluate and create, and ethical issues) for fostering AI literacy based on the adaptation of classic literacies. This study sheds light on the consolidated definition, teaching, and ethical concerns on AI literacy, establishing the groundwork for future research such as competency development and assessment criteria on AI literacy.",https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000357,,,,,,,,,,,,,,
165,3AQ2K3BX,Ng (2021),Conceptualizing AI literacy: An exploratory review,10.1016/j.caeai.2021.100041,journalArticle,2021.0,en,Manual,"Highlights

    •
    Research on AI literacy has not been widely published in educational journals.
    •
    An exploratory review was conducted to conceptualize the newly emerging concept “AI literacy” to define, teach and evaluate AI literacy.
    •
    This review proposed four aspects (i.e., know and understand, use, evaluate and ethical issues) for fostering AI literacy based on the adaptation of classic literacies.

Abstract
Artificial Intelligence (AI) has spread across industries (e.g., business, science, art, education) to enhance user experience, improve work efficiency, and create many future job opportunities. However, public understanding of AI technologies and how to define AI literacy is under-explored. This vision poses upcoming challenges for our next generation to learn about AI. On this note, an exploratory review was conducted to conceptualize the newly emerging concept “AI literacy”, in search for a sound theoretical foundation to define, teach and evaluate AI literacy. Grounded in literature on 30 existing peer-reviewed articles, this review proposed four aspects (i.e., know and understand, use and apply, evaluate and create, and ethical issues) for fostering AI literacy based on the adaptation of classic literacies. This study sheds light on the consolidated definition, teaching, and ethical concerns on AI literacy, establishing the groundwork for future research such as competency development and assessment criteria on AI literacy.",https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000357,,,,,,,,,,,,,,
166,SE579V7B,Ng (2021),Conceptualizing AI literacy: An exploratory review,10.1016/j.caeai.2021.100041,journalArticle,2021.0,en,Manual,"Highlights

    •
    Research on AI literacy has not been widely published in educational journals.
    •
    An exploratory review was conducted to conceptualize the newly emerging concept “AI literacy” to define, teach and evaluate AI literacy.
    •
    This review proposed four aspects (i.e., know and understand, use, evaluate and ethical issues) for fostering AI literacy based on the adaptation of classic literacies.

Abstract
Artificial Intelligence (AI) has spread across industries (e.g., business, science, art, education) to enhance user experience, improve work efficiency, and create many future job opportunities. However, public understanding of AI technologies and how to define AI literacy is under-explored. This vision poses upcoming challenges for our next generation to learn about AI. On this note, an exploratory review was conducted to conceptualize the newly emerging concept “AI literacy”, in search for a sound theoretical foundation to define, teach and evaluate AI literacy. Grounded in literature on 30 existing peer-reviewed articles, this review proposed four aspects (i.e., know and understand, use and apply, evaluate and create, and ethical issues) for fostering AI literacy based on the adaptation of classic literacies. This study sheds light on the consolidated definition, teaching, and ethical concerns on AI literacy, establishing the groundwork for future research such as competency development and assessment criteria on AI literacy.",https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000357,,,,,,,,,,,,,,
167,6JAZXKEB,Ng (2022),Using digital story writing as a pedagogy to develop AI literacy among primary students,10.1016/j.caeai.2022.100054,journalArticle,2022.0,en,Manual,,https://linkinghub.elsevier.com/retrieve/pii/S2666920X22000091,,,,,,,,,,,,,,
168,PAZJHB8J,Nuwasiima (2024),The role of artificial intelligence (AI) and machine learning in social work practice,10.30574/wjarr.2024.24.1.2998,journalArticle,2024.0,,Perplexity,"This comprehensive review identifies algorithmic bias as a critical challenge in social work AI implementation, noting that algorithms trained on historical data may perpetuate existing inequalities by replicating racial, gender, and socio-economic disparities. The authors document specific cases where predictive analytics tools disproportionately flagged families of color for child welfare interventions despite lacking substantial evidence of higher abuse rates. The study emphasizes that addressing algorithmic bias requires multi-faceted approaches including diverse and inclusive datasets, ongoing evaluation and auditing of AI systems, and involvement of social workers and community members in AI tool development.",https://doi.org/10.30574/wjarr.2024.24.1.2998,,,,,,,,,,,,,,
169,TNLGELEQ,Näscher (2025),ReflectAI: Design and evaluation of an AI coach to support public servants' self-reflection,10.1007/978-3-032-02515-9_7,conferencePaper,2025.0,,Claude,"Design science research presenting ReflectAI, an LLM-based AI coach designed to support public servants in developing self-reflection competencies—critical skill for digital transformation in public administration. Two-week user study with seven public servants revealed three key benefits: increased awareness of self-reflection opportunities, improved thought structure, and valuable conversation documentation. Demonstrates how conversational AI can facilitate reflective practice through structured prompting and dialogue. Shows AI coaching for personality-related competencies, demonstrating how prompt-based interactions can support professional development in human services contexts closely aligned with social services.",,,,,,,,,,,,,,,
170,IW32JGWV,OECD (2023),Advancing Accountability in AI,,report,2023.0,,Perplexity,"Delivers a multi-level review of AI accountability, focusing on transparency, fairness, and privacy. Discusses trade-offs in adopting explainability and transparency measures while mitigating algorithmic bias and upholding fairness, framed within legal, social, and ethical requirements for inclusive, trustworthy AI.",https://www.oecd.org/content/dam/oecd/en/publications/reports/2023/02/advancing-accountability-in-ai_753bf8c8/2448f04b-en.pdf,,,,,,,,,,,,,,
171,T83KNEQZ,Ovalle (2023),Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness,10.1145/3600211.3604705,journalArticle,2023.0,,Perplexity,"Diese kritische Literaturanalyse von 30 Arbeiten zur intersektionalen KI-Fairness deckt eine fundamentale Diskrepanz zwischen der Konzeptualisierung und Operationalisierung von Intersektionalität auf. Die Autoren zeigen, dass Forscher Intersektionalität überwiegend auf die Optimierung von Fairness-Metriken über demografische Untergruppen reduzieren, dabei aber die strukturellen Machtverhältnisse und den historischen Kontext vernachlässigen. Die Studie demonstriert, dass tiefgreifendes Engagement mit Intersektionalität eine systematische Analyse von Machtstrukturen über den gesamten KI-Pipeline hinweg erfordert, nicht nur technische Anpassungen. Arbeiten, die systemische (statt nur statistische) Quellen von Bias berücksichtigen und interdisziplinäre Synergien aufweisen, zeigen signifikant höhere Abdeckung intersektionaler Prinzipien.",https://doi.org/10.1145/3600211.3604705,,,,,,,,,,,,,,
172,XXCDL3A3,Ovalle (2023),Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness,,conferencePaper,2023.0,,Perplexity,"This critical review examines how intersectionality is discussed in 30 works of AI fairness literature. The study shows that researchers predominantly reduce intersectionality to optimizing fairness metrics across demographic subgroups while neglecting social context and power structures. The authors develop a framework for re-conceptualizing intersectionality in AI fairness based on relationality, social power, and structural analysis. They argue that genuine intersectional approaches must consider the interweaving of various systems of oppression rather than treating them as separate, additive categories.",https://www.lsv.uni-saarland.de/wp-content/uploads/2023/12/Ovalle-et-al.-2023-Factoring-the-Matrix-of-Domination-A-Critical-Rev.pdf,,,,,,,,,,,,,,
173,ZMW228P6,Ovalle (2023),Factoring the matrix of domination: A critical review and reimagination of intersectionality in AI fairness,10.1145/3600211.3604705,conferencePaper,2023.0,,Claude,"Critical review examining how intersectionality is conceptualized and operationalized within AI fairness research, analyzing 30 papers from the field. Identifies significant gaps between intersectionality theory and technical applications, proposing six key tenets for properly applying intersectionality in AI research drawn from Patricia Hill Collins and Sirma Bilge's framework.",https://dl.acm.org/doi/abs/10.1145/3600211.3604705,,,,,,,,,,,,,,
174,DWS4KXBW,Ovalle (2024),Towards Substantive Equality in Artificial Intelligence: Transformative AI Policy for Gender Equality and Diversity,,report,2024.0,,Perplexity,"Dieser GPAI-Bericht, basierend auf Konsultationen mit über 200 Teilnehmern aus mehr als 50 Ländern, entwickelt einen menschenrechtsbasierten Rahmen für substantielle Gleichberechtigung in der KI. Der Bericht betont, dass KI ohne Intervention das Risiko birgt, gesellschaftliche Verzerrungen zu perpetuieren und zu verstärken, insbesondere gegen historisch marginalisierte Gruppen. Die Empfehlungen zielen darauf ab, die strukturellen Wurzeln der Ungleichheit zu bekämpfen und transformative Veränderungen zu fördern, die substantielle Gleichberechtigung in der KI erreichen. Solche Politiken verbessern die Effektivität, Fairness und Nutzbarkeit von KI-Systemen.",https://datapopalliance.org/publications/towards-real-diversity-and-gender-equality-in-ai/,,,,,,,,,,,,,,
175,MQJEES7J,Pan (2025),AI literacy and trust: A multi-method study of Human-GAI team collaboration,10.1016/j.chbah.2025.100162,journalArticle,2025.0,en,Manual,,https://linkinghub.elsevier.com/retrieve/pii/S2949882125000465,,,,,,,,,,,,,,
176,JPUCNHNU,Pan (2025),LIBRA: Measuring bias of large language model from a local context,,report,2025.0,,Gemini,"Critiques the U.S.-centricity of existing LLM bias evaluation methods. Proposes the Local Integrated Bias Recognition and Assessment (LIBRA) framework and develops dataset of over 360,000 test cases specific to New Zealand context. Results show models like BERT and GPT-2 struggle with local context, while Llama-3 responds better to different cultural contexts despite exhibiting larger bias overall.",https://www.researchgate.net/publication/388686547_LIBRA_Measuring_Bias_of_Large_Language_Model_from_a_Local_Context,,,,,,,,,,,,,,
177,ZHQMHHPQ,Park (2025),"AI algorithm transparency, pipelines for trust not prisms: mitigating general negative attitudes and enhancing trust toward AI",10.1057/s41599-025-05116-z,journalArticle,2025.0,,ChatGPT,"Peer-reviewed study examining how algorithmic transparency overcomes users' predisposed distrust in AI. 2×2 experiment showing transparency significantly increases trust, especially for those with initially negative AI attitudes. Making AI workings visible mitigated negative relationship between user AI-skepticism and organizational trust. Transparency acts as ""signal of trustworthiness,"" reducing skepticism and uncertainty. Emphasizing transparency in prompts and system design can reassure wary professionals by demonstrating accountability.",https://www.nature.com/articles/s41599-025-05116-z,,,,,,,,,,,,,,
178,JKF6VAQB,Park (2025),"AI algorithm transparency, pipelines for trust not prisms: mitigating general negative attitudes and enhancing trust toward AI",10.1057/s41599-025-05116-z,journalArticle,2025.0,,Gemini,"Peer-reviewed study examining how algorithmic transparency overcomes users' predisposed distrust in AI. 2×2 experiment showing transparency significantly increases trust, especially for those with initially negative AI attitudes. Making AI workings visible mitigated negative relationship between user AI-skepticism and organizational trust. Transparency acts as ""signal of trustworthiness,"" reducing skepticism and uncertainty. Emphasizing transparency in prompts and system design can reassure wary professionals by demonstrating accountability.",https://www.nature.com/articles/s41599-025-05116-z,,,,,,,,,,,,,,
179,QY6P4RGQ,Parrish (2022),BBQ: A hand-built bias benchmark for question answering,,conferencePaper,2022.0,,Manual,,,,,,,,,,,,,,,,
180,YUVR5YNQ,Parrish (2025),Self-debiasing large language models: Zero-shot recognition and reduction of stereotypes,,conferencePaper,2025.0,,Gemini,"Large language models (LLMs) have shown
remarkable advances in language generation
and understanding but are also prone to exhibiting harmful social biases. While recognition of
these behaviors has generated an abundance of
bias mitigation techniques, most require modifications to the training data, model parameters,
or decoding strategy, which may be infeasible
without access to a trainable model. In this
work, we leverage the zero-shot capabilities of
LLMs to reduce stereotyping in a technique
we introduce as zero-shot self-debiasing. With
two approaches, self-debiasing via explanation
and self-debiasing via reprompting, we show
that self-debiasing can significantly reduce the
degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations
correctly identifying invalid assumptions and
reprompting delivering the greatest reductions
in bias. We hope this work opens inquiry into
other zero-shot techniques for bias mitigation.",https://aclanthology.org/2025.naacl-short.74.pdf,,,,,,,,,,,,,,
181,LGYFN6JK,Patton (2023),ChatGPT for Social Work Science: Ethical Challenges and Opportunities,10.1086/726042,journalArticle,2023.0,,ChatGPT,"Ethical framework for using LLMs in social work research. Recommends transparency, verification, authorship integrity, anti-plagiarism, and inclusion/social justice to counter bias. Positions LLMs as assistive tools requiring critical human oversight.",https://doi.org/10.1086/726042,,,,,,,,,,,,,,
182,HTXCWNQ9,Peng (2022),A Literature Review of Digital Literacy over Two Decades,10.1155/2022/2533413,journalArticle,2022.0,en,Manual,"The COVID-19 pandemic has forced online learning to be a “new normal” during the past three years, which highly emphasizes students’ improved digital literacy. This study aims to present a literature review of students’ digital literacy. Grounded on about twenty journal articles and other related publications from the Web of Science Core Collection, this paper focused on the definition of digital literacy; the factors affecting students’ digital literacy (age, gender, family socioeconomic status, and parent’s education level); the relationship between students’ digital literacy and their self-control, technostress, and engagement; and the three approaches to gauge the level of students’ digital literacy. The study also provided some advice for educators and policymakers. Finally, the limitations and implications were presented.",https://www.hindawi.com/journals/edri/2022/2533413/,,,,,,,,,,,,,,
183,22XEFRWP,Petzel (2025),Prejudiced interactions with large language models (LLMs) reduce trustworthiness and behavioral intentions among members of stigmatized groups,10.1016/j.chb.2025.108563,journalArticle,2025.0,,ChatGPT,"Investigates how biased or prejudiced content in LLM responses affects user trust and willingness to use the system, particularly for users from marginalized communities. Through three preregistered experiments, finds that when AI responses exhibited prejudice, participants from marginalized groups reported significantly lower trust and decreased intentions to continue using the system.",https://doi.org/10.1016/j.chb.2025.108563,,,,,,,,,,,,,,
184,3K9RDWLY,Pinski (2023),AI Literacy - Towards Measuring Human Competency in Artificial Intelligence,10.24251/HICSS.2023.021,conferencePaper,2023.0,,Manual,,http://hdl.handle.net/10125/102649,,,,,,,,,,,,,,
185,7G78SADI,Pinski (2024),"AI literacy for users – A comprehensive review and future research directions of learning methods, components, and effects",10.1016/j.chbah.2024.100062,journalArticle,2024.0,en,Manual,,https://linkinghub.elsevier.com/retrieve/pii/S2949882124000227,,,,,,,,,,,,,,
186,H8WRNPP7,Pinski (2024),AI Literacy for the top management: An upper echelons perspective on corporate AI orientation and implementation ability,10.1007/s12525-024-00707-1,journalArticle,2024.0,en,Manual,"Abstract
            We draw on upper echelons theory to examine whether the AI literacy of a firm’s top management team (i.e., TMT AI literacy) has an effect on two firm characteristics paramount for value generation with AI—a firm’s AI orientation, enabling it to identify AI value potentials, and a firm’s AI implementation ability, empowering it to realize these value potentials. Building on the notion that TMT effects are contingent upon firm contexts, we consider the moderating influence of a firm’s type (i.e., startups vs. incumbents). To investigate these relationships, we leverage observational literacy data of 6986 executives from a professional social network (LinkedIn.com) and firm data from 10-K statements. Our findings indicate that TMT AI literacy positively affects AI orientation as well as AI implementation ability and that AI orientation mediates the effect of TMT AI literacy on AI implementation ability. Further, we show that the effect of TMT AI literacy on AI implementation ability is stronger in startups than in incumbent firms. We contribute to upper echelons literature by introducing AI literacy as a skill-oriented perspective on TMTs, which complements prior role-oriented TMT research, and by detailing AI literacy’s role for the upper echelons-based mechanism that explains value generation with AI.",https://link.springer.com/10.1007/s12525-024-00707-1,,,,,,,,,,,,,,
187,6MJYP7ZX,Prakash (2023),Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,,journalArticle,2023.0,,Gemini,"Large language models have demonstrated remarkable capabilities across various domains, yet concerns about cultural bias - particularly towards Arabs and Muslims - pose significant ethical challenges by perpetuating harmful stereotypes and marginalization. Despite growing recognition of bias in LLMs, prompt engineering strategies specifically addressing Arab and Muslim representation remain understudied. This mixed-methods systematic review examines such techniques, offering evidence-based guidance for researchers and practitioners. Following PRISMA guidelines and Kitchenham's systematic review methodology, we analyzed 8 empirical studies published between 2021-2024 investigating bias mitigation strategies. Our findings reveal five primary prompt engineering approaches: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts. Although all approaches show potential for reducing bias, effectiveness varied substantially across studies and bias types. Evidence suggests that certain bias types may be more resistant to prompt-based mitigation than others. Structured multi-step pipelines demonstrated the highest overall effectiveness, achieving up to 87.7% reduction in bias, though they require greater technical expertise. Cultural prompting offers broader accessibility with substantial effectiveness. These results underscore the accessibility of prompt engineering for mitigating cultural bias without requiring access to model parameters. The limited number of studies identified highlights a significant research gap in this critical area. Future research should focus on developing culturally adaptive prompting techniques, creating Arab and Muslim-specific evaluation resources, and integrating prompt engineering with complementary debiasing methods to address deeper stereotypes while maintaining model utility.",https://www.researchgate.net/publication/392942981_Prompt_Engineering_Techniques_for_Mitigating_Cultural_Bias_Against_Arabs_and_Muslims_in_Large_Language_Models_A_Systematic_Review,,,,,,,,,,,,,,
188,MTMU9UPJ,Qiu (2025),DR.GAP: Mitigating bias in large language models using gender-aware prompting with demonstration and reasoning,,report,2025.0,,Perplexity,"DR.GAP ist eine prompting-basierte Methode zur Bias-Reduktion in LLMs. Sie nutzt Beispielfälle und strukturierte Reasoning-Schritte, um gendergerechtere Antworten zu erzielen.",https://arxiv.org/html/2502.11603v1,,,,,,,,,,,,,,
189,UY65LIAI,"Quaid-i-Azam University, Islamabad, Pakistan (2025)",Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy,10.70389/PJAI.1000088,journalArticle,2025.0,,Manual,"Purpose

This narrative review investigates the interplay between gender bias in artificial intelligence (AI) systems and the potential of digital literacy to empower women in technology. By synthesising research from 2010 to 2024, the study examines how gender bias manifests in AI, its impact on women’s participation in technology, and the effectiveness of digital literacy initiatives in addressing these disparities.

Methods

A systematic literature search was conducted across major academic databases, including Web of Science, Scopus, IEEE Xplore, and Google Scholar. The review focused on peer-reviewed articles, reports, and case studies published between 2010 and 2024 that addressed gender bias in AI, women’s participation in technology, and digital literacy initiatives. A thematic analysis framework was employed to identify and synthesise recurring themes and patterns.

Results

The findings reveal systemic gender biases embedded in AI applications across diverse domains, such as recruitment, healthcare, and financial services. These biases stem from factors including the under-representation of women in AI development teams, biased training datasets, and algorithmic design choices. Digital literacy programs emerge as a promising intervention, fostering a critical awareness of AI bias, encouraging women to pursue AI careers, and catalysing growth in women-led AI projects.

Conclusions Although gender bias in AI poses significant challenges, this review highlights digital literacy as a transformative tool for achieving gender equity in AI development and application. The study highlights the importance of inclusive AI design, gender-responsive education policies, and sustained research efforts to mitigate bias and promote equity.",https://premierscience.com/pjai-24-524/,,,,,,,,,,,,,,
190,BFG8VUK3,"Quaid-i-Azam University, Islamabad, Pakistan (2025)",Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy,10.70389/PJAI.1000088,journalArticle,2025.0,,Manual,"Purpose

This narrative review investigates the interplay between gender bias in artificial intelligence (AI) systems and the potential of digital literacy to empower women in technology. By synthesising research from 2010 to 2024, the study examines how gender bias manifests in AI, its impact on women’s participation in technology, and the effectiveness of digital literacy initiatives in addressing these disparities.

Methods

A systematic literature search was conducted across major academic databases, including Web of Science, Scopus, IEEE Xplore, and Google Scholar. The review focused on peer-reviewed articles, reports, and case studies published between 2010 and 2024 that addressed gender bias in AI, women’s participation in technology, and digital literacy initiatives. A thematic analysis framework was employed to identify and synthesise recurring themes and patterns.

Results

The findings reveal systemic gender biases embedded in AI applications across diverse domains, such as recruitment, healthcare, and financial services. These biases stem from factors including the under-representation of women in AI development teams, biased training datasets, and algorithmic design choices. Digital literacy programs emerge as a promising intervention, fostering a critical awareness of AI bias, encouraging women to pursue AI careers, and catalysing growth in women-led AI projects.

Conclusions Although gender bias in AI poses significant challenges, this review highlights digital literacy as a transformative tool for achieving gender equity in AI development and application. The study highlights the importance of inclusive AI design, gender-responsive education policies, and sustained research efforts to mitigate bias and promote equity.",https://premierscience.com/pjai-24-524/,,,,,,,,,,,,,,
191,U75LP6SV,Raji (2024),The Algorithmic Auditing Landscape: A Social Justice Approach,10.1145/3630659.3630671,conferencePaper,2024.0,,Gemini,"Raji and Buolamwini, pioneers of algorithmic auditing, argue for an approach rooted in social justice. They critique audits that focus solely on technical metrics, advocating instead for methods that center the lived experiences of marginalized communities. This involves a multi-stakeholder process, transparency, and a focus on real-world harms. Such an audit practice inherently makes the co-constitution of discrimination visible by investigating not just the algorithm's output, but the entire socio-technical system in which it is embedded. The paper implicitly highlights the limits of individual competence (e.g., a single auditor's skill) by emphasizing the need for collective, community-involved processes to challenge structural power.",https://dl.acm.org/doi/10.1145/3630659.3630671,,,,,,,,,,,,,,
192,HN7KKNYV,Reamer (2023),Artificial intelligence in social work: Emerging ethical issues,10.55521/10-020-205,journalArticle,2023.0,,Perplexity,"Systematic analysis of ethical challenges in AI use within social work, developing comprehensive framework for ethical AI implementation. Identifies eight central ethical areas including informed consent, data privacy, transparency, misdiagnosis, client neglect, surveillance, scientific misconduct, and algorithmic bias. Emphasizes transparency as fundamental principle requiring social workers to inform clients about AI use. Develops eight-step protocol for ethical AI implementation including ethics committees and continuous peer-review processes.",https://doi.org/10.55521/10-020-205,,,,,,,,,,,,,,
193,CF6T2RD7,Reamer (2023),Artificial intelligence in social work: Emerging ethical issues,,journalArticle,2023.0,,Perplexity,"This comprehensive analysis identifies key ethical challenges in social work's adoption of AI, including informed consent and client autonomy, privacy and confidentiality, transparency, client misdiagnosis, algorithmic bias and unfairness, and the need for evidence-based AI tools. The author proposes a concrete ethics-informed protocol for AI implementation, advocating for digital ethics steering committees and transparent, auditable methodologies. The work stresses that social workers must familiarize themselves with AI protocols and ensure compliance with ethical standards while incorporating AI literacy into social work education curricula.",https://jswve.org/volume-20/issue-2/item-05/,,,,,,,,,,,,,,
194,IUN7Z56I,Reamer (2023),Artificial intelligence in social work: Emerging ethical issues,10.55521/10-020-205,journalArticle,2023.0,,ChatGPT,"Systematic analysis of ethical challenges in AI use within social work, developing comprehensive framework for ethical AI implementation. Identifies eight central ethical areas including informed consent, data privacy, transparency, misdiagnosis, client neglect, surveillance, scientific misconduct, and algorithmic bias. Emphasizes transparency as fundamental principle requiring social workers to inform clients about AI use. Develops eight-step protocol for ethical AI implementation including ethics committees and continuous peer-review processes.",https://doi.org/10.55521/10-020-205,,,,,,,,,,,,,,
195,EXVG7MQR,Reamer (2023),Artificial intelligence in social work: Emerging ethical issues,10.55521/10-020-205,journalArticle,2023.0,,Gemini,"Systematic analysis of ethical challenges in AI use within social work, developing comprehensive framework for ethical AI implementation. Identifies eight central ethical areas including informed consent, data privacy, transparency, misdiagnosis, client neglect, surveillance, scientific misconduct, and algorithmic bias. Emphasizes transparency as fundamental principle requiring social workers to inform clients about AI use. Develops eight-step protocol for ethical AI implementation including ethics committees and continuous peer-review processes.",https://doi.org/10.55521/10-020-205,,,,,,,,,,,,,,
196,X2QQ4JH6,Reamer (2023),Artificial Intelligence in Social Work: Emerging Ethical Issues,,journalArticle,2023.0,,ChatGPT,"Comprehensive ethical analysis of AI in social work covering consent, autonomy, privacy, transparency, algorithmic bias, and professional competence. Maps issues to the NASW Code and urges policies, training, and client options to opt out of AI-mediated services.",https://jswve.org/wp-content/uploads/2023/10/10-020-205-IJSWVE-2023.pdf,,,,,,,,,,,,,,
197,TRAN2GJU,Ricaurte (2024),How can feminism inform AI governance in practice?,,report,2024.0,,Perplexity,"Diese UNESCO-Publikation definiert feministische KI-Governance als einen aufkommenden Bereich von Politik, Forschung und Entwicklung, der darauf abzielt, KI-Systeme gerecht, gleichberechtigt und inklusiv zu gestalten. Feministische KI-Governance zielt darauf ab, Machtungleichgewichte im KI-Ökosystem zu adressieren und strukturelle Ungleichheiten, koloniale Vermächtnisse und multidimensionale Schäden zu berücksichtigen, die überproportional Gemeinschaften der globalen Mehrheit betreffen. Der Ansatz versteht Algorithmen als kontextuell und durch soziale Beziehungen geformt, nicht als rein mathematische Entitäten.",https://www.unesco.org/en/articles/how-can-feminism-inform-ai-governance-practice,,,,,,,,,,,,,,
198,XIYX5HJS,Ricaurte Quijano (2024),Towards substantive equality in artificial intelligence: Transformative AI policy for gender equality and diversity,,report,2024.0,,ChatGPT,"This extensive report – developed through the Global Partnership on AI (GPAI) with contributors from academia and policy – sets out a vision for “substantive equality” in AI as opposed to mere formal equality. It recognizes that AI systems can replicate and even amplify societal power imbalances (“algorithmic discrimination”), thus requiring proactive governance to ensure historically marginalized groups are not left behind in the AI era. The report argues that purely technical bias mitigation is insufficient; instead, transformative policies should intervene at multiple points in the AI lifecycle to address structural inequities. Key principles advocated include: anchoring AI development in human rights and intersectional gender analysis, improving inclusive representation in data and AI design, and imposing stronger transparency and accountability obligations on AI systems to prevent harm. It provides practical policy recommendations (e.g., mandating diverse datasets and impact assessments, establishing a “right to information” about AI algorithms, and supporting community-led AI initiatives) to achieve de facto equality of outcomes – meaning AI should actively help reduce societal inequalities rather than reinforce them. Overall, the report positions digital equity as an extension of social justice: ensuring not only fairness within AI outputs but also equitable access, participation, and empowerment in shaping AI technology.",https://wp.oecd.ai/app/uploads/2025/05/towards-substantive-equality-in-artificial-intelligence_Transformative-AI-policy-for-gender-equality-and-diversity.pdf,,,,,,,,,,,,,,
199,YRBP6IEJ,Rodriguez (2024),Introducing Generative Artificial Intelligence into the MSW Curriculum: A Proposal for the 2029 Educational Policy and Accreditation Standards,10.1080/10437797.2024.2340931,journalArticle,2024.0,,ChatGPT,"Proposal to add an explicit AI competency to MSW accreditation. Outlines benefits and risks of generative AI, recommends curricular content on ethics, bias, transparency, and responsible use, and frames AI literacy as essential for safeguarding client dignity and equity while leveraging innovation.",https://doi.org/10.1080/10437797.2024.2340931,,,,,,,,,,,,,,
200,9WIGR47Y,Rodríguez-Martínez (2024),Ethical issues related to the use of technology in social work practice: A systematic review,10.1177/21582440241274842,journalArticle,2024.0,,Claude,"Systematic literature review examining ethical tensions arising from technology integration in social work practice. Review identifies three main categories of ethical challenges: effects of digitization on professional practice (tensions between efficiency and human connection), education, research and engagement challenges (digital literacy requirements conflicting with traditional social work training), and ethical challenges in digital professional practice (boundary issues, dual relationships, informed consent in digital contexts). Key value conflicts include technology's potential to erode professional relationships and trust; digital divide creating social justice concerns; automated systems undermining client self-determination and participation; and privacy violations threatening human dignity.",,,,,,,,,,,,,,,
201,K3YCLBXK,Ruiz (2024),"AI Literacy: A Framework to Understand, Evaluate, and Use Emerging Technology",,report,2024.0,,Manual,"To enable all who participate in educational settings to leverage AI tools for powerful learning, this paper describes a framework and strategies for educational leaders to design and implement a clear approach to AI Literacy for their specific audiences (e.g. learners, teachers, or others) that are safe and effective. The first part of the paper describes a framework that identifies essential components of AI Literacy and connects them to existing initiatives. The second part of the paper identifies strategies and illustrative examples as guidance for educational leaders to integrate AI Literacy in PK–12 education and adapt to their unique contexts.",https://hdl.handle.net/20.500.12265/218,,,,,,,,,,,,,,
202,M7AGB7LI,Salecha (2025),Model explanations for gender and ethnicity bias mitigation in AI-generated narratives,,thesis,2025.0,,Gemini,"Large Language Models (LLMs) are increasingly utilized in diverse applications, ranging
from professional content creation to decision-making systems. However, their outputs often
amplify the biases present in their training data, perpetuating stereotypes and reinforcing societal inequities, particularly regarding gender and ethnicity. Such biases can cause tangible
harm, especially for underrepresented groups, and require awareness and effective mitigation
strategies.
This work explores gender and ethnicity representation in narratives created by generative AI describing 25 occupational fields defined by the U.S. Bureau of Labor Statistics. We
examine three large language models (LLMs)—Llama 3.1 70B Instruct, Claude 3.5 Sonnet,
and GPT 4.0 Turbo. Employing a novel approach that leverages model-generated explanations, we assess bias before and after mitigation using two metrics: Demographic Parity
Ratio (DPR) and Total Variation Distance (TVD).
Our findings reveal that incorporating model explanations significantly improves demographic representation, reducing biases by 2%–20% across different models and occupations.
Qualitative analysis of the generated stories indicates high levels of creativity, coherence,
and inclusivity, demonstrating the potential of targeted interventions to produce equitable
narratives. This research contributes a robust dataset of occupational narratives and a systematic framework for bias mitigation, advancing the understanding of LLM behavior and
i
promoting ethical AI development. By aligning explainability with equity, this work underscores the critical role of transparency and accountability in the deployment of generative
AI systems.",https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=7888&context=open_access_etds,,,,,,,,,,,,,,
203,YMABYPKF,Salinas (2025),What’s in a name? Auditing large language models for race and gender bias,,report,2025.0,,ChatGPT,"This interdisciplinary audit of GPT-4 and other LLMs reveals systematic intersectional biases based on names signaling race and gender. Prompts with names suggesting a Black woman received less favorable advice compared to those with white male names. This disparity was robust across 42 prompt templates. The study found that adding quantitative anchors (facts, numbers) to the prompt largely eliminated this bias, whereas adding qualitative descriptive details had inconsistent effects and sometimes amplified stereotypes.",https://arxiv.org/html/2402.14875v3,,,,,,,,,,,,,,
204,KNQYFQ6B,Sant (2024),The power of prompts: Evaluating and mitigating gender bias in MT with LLMs,10.18653/v1/2024.gebnlp-1.7,conferencePaper,2024.0,,Claude,Examines gender bias in machine translation through LLMs using four widely-used test sets. Develops specific prompting engineering techniques that reduce gender bias by up to 12% on WinoMT evaluation dataset. Identifies optimal prompt structures incorporating explicit fairness instructions and context-aware guidelines.,https://doi.org/10.18653/v1/2024.gebnlp-1.7,,,,,,,,,,,,,,
205,YV53DKI2,Santos (2024),Explainability through systematicity: The hard systematicity challenge,,journalArticle,2024.0,,Gemini,"This philosophical paper argues that the pursuit of ""explainability"" in AI is too narrow. It proposes a richer ideal called ""systematicity,"" which demands that an AI's reasoning be consistent, coherent, comprehensive, and principled, akin to an integrated body of human thought. The author distinguishes this ""hard systematicity challenge"" from the historical Fodorian debate on connectionism and explores how the demand for AI to be systematic should be regulated by different rationales.",https://pmc.ncbi.nlm.nih.gov/articles/PMC12307450/,,,,,,,,,,,,,,
206,99QJDBSV,Santos (2025),How large language models judge cooperation,,report,2025.0,,Gemini,"This study investigates how 21 state-of-the-art LLMs make social and moral judgments about cooperative behavior. Using an evolutionary game-theory model and a dataset of 43,200 prompts, the authors find significant variation in how different models assign reputations, particularly when judging interactions with ""ill-reputed"" actors. Demonstrates that LLM social norms are highly malleable and can be consistently steered by different types of prompt interventions.",https://arxiv.org/pdf/2507.00088,,,,,,,,,,,,,,
207,UJ7DXK8Y,Santy (2023),NLPositionality: Characterizing design biases of datasets and models,,conferencePaper,2023.0,,Gemini,"This study provides a theoretical and methodological framework for making bias visible in AI models and datasets. It introduces the concept of ""positionality"" - the assumption that developers' social, cultural, and political perspectives inevitably flow into AI artifacts. The authors develop a method to quantify design biases by analyzing which dialects, demographic groups, or social contexts are under- or over-represented in datasets.",https://aclanthology.org/2023.acl-long.530/,,,,,,,,,,,,,,
208,QLXLEUCG,Schneider (2018),Der Einfluss der Algorithmen: Neue Qualitäten durch Big Data Analytics und Künstliche Intelligenz,10.1007/s12054-018-0046-y,journalArticle,2018.0,,Claude,"Examines how algorithmic decision-making affects professional judgment formation and discretionary decision-making space for practitioners. Analyzes automation bias where professionals may over-rely on algorithmic recommendations without adequate critical evaluation. Stresses social work requires debate about what forms of knowledge new technologies can generate, where limits lie, and how it can meaningfully be incorporated into professional reflection and decision-making practices.",,,,,,,,,,,,,,,
209,M2FYV58I,Schneider (2022),Exploring opportunities and risks in decision support technologies for social workers: An empirical study in the field of disabled people's services,10.1093/bjsw/bcab262,journalArticle,2022.0,,Claude,"Empirical study investigating German social workers' perspectives on decision support systems in disability services through practitioner interviews. Identifies both opportunities (consistency across cases, evidence-based practice, administrative time-saving) and significant risks (deprofessionalization, data protection concerns, reduced professional autonomy, loss of holistic assessment capabilities). Social workers express ambivalence: recognizing potential for reducing subjective bias and improving resource allocation transparency while worrying about losing relational aspects of assessment and client trust.",,,,,,,,,,,,,,,
210,2WHGF83D,Schneider (2024),"AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?",10.14512/tatup.33.1.08,journalArticle,2024.0,,Claude,"Comprehensive examination of AI-based decision support systems across healthcare, legal systems, and border control. Provides critical analysis of technical, ethical, legal, and societal challenges when machines make or support decisions previously made by humans. Reviews regulatory attempts including EU AI Act. Examines key issues: opacity of algorithmic systems creating black box problems for accountability, professional deskilling risks when practitioners defer to AI, and potential for discrimination embedded in training data and algorithmic logic.",,,,,,,,,,,,,,,
211,44AA6ETH,Schneider (2024),Das verflixte Problem mit Klassifikationen: Zum Einfluss der Digitalisierung auf die Soziale Diagnostik in der Sozialen Arbeit,,bookSection,2024.0,,Claude,"Examines how digitalization affects social diagnostics and assessment practices in social work. Analyzes fundamental problem of classification systems: tension between necessary categorization for resource allocation and profession's commitment to individualized, contextual understanding of clients. Digital systems intensify this tension by requiring standardized data inputs that may not capture social complexity or unique circumstances. Explores how algorithmic decision support systems rely on predefined categories that risk reifying social problems and overlooking contextual factors.",,,,,,,,,,,,,,,
212,LMW8DZ78,Schneider (2025),"Indecision on the use of artificial intelligence in healthcare: A qualitative study of patient perspectives on trust, responsibility and self-determination using AI-CDSS",10.1186/s12910-024-01143-5,journalArticle,2025.0,,Claude,"Empirical qualitative study exploring patient perspectives on AI-based clinical decision support systems (AI-CDSS) in healthcare, revealing significant ambivalence about AI use in medical decision-making. Through interviews examining trust, responsibility distribution, and self-determination when AI systems assist physicians, findings show patients worry about decreased human interaction, loss of holistic care perspectives, and unclear accountability when AI makes errors. Vulnerable populations express particular concerns about algorithmic systems making decisions affecting their wellbeing.",,,,,,,,,,,,,,,
213,R5QQTD95,Schönauer (2025),Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen Arbeit – Eine empirische Untersuchung aus der Perspektive von Berufseinsteiger:innen,10.1007/s12054-025-00783-3,journalArticle,2025.0,,ChatGPT,Empirischer Kurzbeitrag zu Einstellungen von Berufseinsteiger:innen: überwiegend kritische Haltung gegenüber KI in direkter Klient:innenarbeit; Datenschutz- und Empathiefragen zentral. Akzeptanz steigt mit digitaler Kompetenz; Empfehlung: Aus- und Weiterbildung für kritische KI-Literacy und partizipative Technikgestaltung.,https://doi.org/10.1007/s12054-025-00783-3,,,,,,,,,,,,,,
214,6N2E242K,Schönauer (2025),Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen Arbeit,10.1007/s12054-025-00783-3,journalArticle,2025.0,de,Gemini,"Der Artikel untersucht die Akzeptanz und organisationalen Rahmenbedingungen digitaler Technologien, insbesondere Künstlicher Intelligenz (KI), in der Praxis der Sozialen Arbeit aus Sicht der Fachkräfte. Während digitale Technologien im administrativen Bereich bereits weit verbreitet sind, zeigt sich bei der Nutzung digitaler Technologien in der direkten Arbeit mit den Klient:innen noch Zurückhaltung. Durch die Entwicklungen im Bereich KI ergeben sich zunehmend neue Möglichkeiten digitale Technologien auch im Bereich der Interaktionsarbeit mit den Klient:innen zu nutzen. Zugleich stellt dies die Fachkräfte vor ethische und professionelle Herausforderungen. Die empirische Untersuchung unter Berufseinsteiger:innen zeigt, dass der Einsatz von KI im beruflichen Kontext kritisch bewertet wird. Bedenken bestehen insbesondere in Bezug auf Datenschutz und die Unersetzbarkeit menschlicher Empathie durch die KI. Die Akzeptanz von KI und digitalen Technologien hängt von den digitalen Kompetenzen und den Erfahrungen der Fachkräfte beim Einsatz digitaler Technologien im beruflichen Kontext ab, die in diesem Bereich noch ausbaufähig sind. Vor dem Hintergrund einer zunehmend digitalisierten Gesellschaft fordert der Artikel eine kritische Auseinandersetzung mit den technischen Entwicklungen und betont die Notwendigkeit von Fort- und Weiterbildungen sowie einer partizipativen Technikgestaltung, um die Praxis der Sozialen Arbeit nachhaltig zu gestalten.",https://doi.org/10.1007/s12054-025-00783-3,,,,,,,,,,,,,,
215,9Y7ZFGI5,Shah (2025),Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy,10.70389/PJAI.1000088,journalArticle,2025.0,,Perplexity,"Narrative review examining interplay between gender bias in AI systems and digital literacy potential for empowering women in technology. Synthesizes research from 2010-2024 analyzing systematic gender biases in AI applications across recruitment, healthcare, and financial services. These biases stem from women's underrepresentation in AI development teams (only 22% globally), biased training data, and algorithmic design decisions. Digital literacy programs show promise as intervention fostering critical awareness of AI bias, encouraging women toward AI careers, and catalyzing growth of women-led AI projects.",,,,,,,,,,,,,,,
216,AIZGTQKG,Shah (2025),Gender bias in artificial intelligence: Empowering women through digital literacy,10.70389/PJAI.1000088,journalArticle,2025.0,,ChatGPT,"This narrative review examines how systemic gender biases are embedded in AI systems across domains (e.g. hiring, healthcare, finance) and explores digital literacy as a tool to combat these biases. Key findings indicate that biases arise from underrepresentation of women in AI development, biased training data, and algorithmic design choices. Digital literacy programs for women are highlighted as a promising intervention that raises critical awareness of AI bias, encourages women's participation in AI careers, and fosters women-led AI projects.",https://premierscience.com/pjai-24-524/,,,,,,,,,,,,,,
217,HLBXNWAZ,Sharma (2024),Intersectional analysis of visual generative AI: the case of stable diffusion,,journalArticle,2024.0,,Perplexity,"This study conducts a critical intersectional analysis of Stable Diffusion, a widely used visual generative AI tool. The examination of 180 generated images shows how the system perpetuates prevailing power systems such as sexism, racism, heteronormativity, and ableism, assuming white, physically healthy, masculine-presenting individuals as the standard. The study identifies three levels of analysis: (1) the aesthetics of AI images (micro-level), (2) institutional contexts (meso-level), and (3) intersections between power systems (macro-level). The authors argue for a reparative, socially just approach to visual generative AI.",https://link.springer.com/article/10.1007/s00146-025-02498-1,,,,,,,,,,,,,,
218,MJDTRLAI,Shin (2024),Can prompt modifiers control bias? A comparative analysis of text-to-image generative models,,report,2024.0,,ChatGPT,"This preprint investigates whether explicit prompt modifiers can reduce societal biases in text-to-image generative AI models. Authors evaluated three models (Stable Diffusion, DALL·E 3, Adobe Firefly) comparing baseline versus bias-mitigating prompts. Analysis revealed notable biases across all models, with inconsistent effectiveness of prompt modifiers. While diversity-reflective prompting can expose hidden biases and sometimes nudge outputs towards inclusivity, it is not a comprehensive fix and must be combined with broader ethical AI development efforts.",https://arxiv.org/abs/2406.05602,,,,,,,,,,,,,,
219,W8DFWR9L,Shin (2025),Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development,,journalArticle,2025.0,,Manual,,https://pubsonline.informs.org/doi/10.1287/ijoc.2024.0645,,,,,,,,,,,,,,
220,GJF776AY,Shukla (2025),Investigating AI systems: examining data and algorithmic bias through hermeneutic reverse engineering,,journalArticle,2025.0,,Perplexity,"This work presents hermeneutic reverse engineering as a framework for investigating bias in AI systems. The approach considers AI systems as boundary objects and analyzes cultural meanings and assumptions embedded in techno-cultural objects. The study proposes three research perspectives: (1) comparative exploration of algorithmic bias, (2) investigation of impacts on various social groups, and (3) participatory approaches to include users in AI design.",https://www.frontiersin.org/journals/communication/articles/10.3389/fcomm.2025.1380252/full,,,,,,,,,,,,,,
221,LT3D3ZQ2,Siapka (2023),Towards a Feminist Metaethics of AI,,journalArticle,2023.0,,Perplexity,"This work develops a research agenda for a feminist metaethics of AI. Unlike traditional metaethics that reflects on moral judgments non-normatively, feminist metaethics expands its scope to ask not only what ethics is, but also how our approach to it should be. The author argues that a feminist metaethics of AI should investigate four areas: (1) continuity between theory and action in AI ethics, (2) real-world impacts of AI ethics, (3) the role and profile of those involved in AI ethics, and (4) AI's impacts on power relations through methods that consider context, emotions, and narrative.",https://arxiv.org/abs/2311.14700,,,,,,,,,,,,,,
222,3B87U5LN,Siddals (2024),"""It happened to be the perfect thing"": Experiences of generative AI chatbots for mental health",10.1038/s44184-024-00097-4,journalArticle,2024.0,,Claude,"Qualitative study using semi-structured interviews with 19 individuals from 8 countries who used generative AI chatbots (primarily Pi, ChatGPT) for mental health support in real-world settings. Participants reported high engagement and positive impacts including improved relationships, healing from trauma and loss, and improved mood. Four themes emerged: emotional sanctuary (non-judgmental, always-available support), insightful guidance particularly for relationships, joy of connection, and comparisons with human therapy. Some participants described life-changing impacts. Identified challenges including frustrating safety guardrails disrupting emotional sanctuary, limited memory capabilities, and inability to lead therapeutic process.",,,,,,,,,,,,,,,
223,DA6T4Z5B,Sinders (2017),Feminist Data Set,,webpage,2017.0,,Claude,"Multi-year art-research project directly addresses critical prompting practices by interrogating every AI development step—data collection, labeling, training, algorithm selection, and chatbot design—through feminist and intersectional lenses. Conducts public workshops to collaboratively build feminist datasets. Represents concrete critical prompting practice through community-based data creation as protest against biased AI systems, demonstrating practical approaches to feminist prompting by creating alternative training data.",https://carolinesinders.com/feminist-data-set/,,,,,,,,,,,,,,
224,VICS443I,Singer (2023),AI Creates the Message: Integrating AI Language Learning Models into Social Work Education and Practice,10.1080/10437797.2023.2189878,journalArticle,2023.0,,ChatGPT,"Commentary advocating cautious integration of LLMs in teaching and practice. Describes pedagogical uses (idea generation, material tailoring) and warns of bias, factual errors, confidentiality risks, and plagiarism. Emphasizes transparency policies and that engagement with AI is ethically preferable to avoidance.",https://doi.org/10.1080/10437797.2023.2189878,,,,,,,,,,,,,,
225,BDBYDLVK,Singh (2025),A reparative turn in AI,,report,2025.0,,Gemini,"Argues for a ""reparative turn"" in AI governance, moving beyond harm prevention to focus on remedying harm after it occurs. Based on thematic analysis of 1,060 real-world AI harm incidents, proposes taxonomy of reparative actions around four goals: acknowledging harm, attributing responsibility, providing remedies, and enabling systemic change. Finds significant ""accountability gap"" with most corporate responses limited to symbolic acknowledgments.",https://arxiv.org/pdf/2506.05687,,,,,,,,,,,,,,
226,6L85PRUW,Skilton (2024),Inclusive prompt engineering: A methodology for hacking biased AI image generation,10.1145/3641237.3691655,conferencePaper,2024.0,,ChatGPT,"This conference paper introduces ""inclusive prompt engineering"" as a strategy to probe and mitigate biases in generative AI image systems. Authors developed methodology to systematically modify prompts and provide tools for generating more diverse outputs. User studies revealed that when participants encountered stereotypical outputs, they tried adding negative qualifiers but models often failed to obey these negations. Findings underscore need for improved prompt interfaces that actively promote inclusive representation.",https://www.researchgate.net/publication/385325948_Inclusive_Prompt_Engineering_A_Methodology_for_Hacking_Biased_AI_Image_Generation,,,,,,,,,,,,,,
227,VVEEL68I,Slesinger (2024),Training in Co-Creation as a Methodological Approach to Improve AI Fairness,,journalArticle,2024.0,,Perplexity,"This study examines the integration of training components in co-creation processes with vulnerable and marginalized stakeholder groups as part of developing AI bias detection and mitigation tools. The research shows that training on AI definitions, terminology, and socio-technical impacts is necessary to enable non-technical stakeholders to clearly articulate their insights on AI fairness. The authors emphasize the importance of critical reflection on appropriate use of training in co-creation approaches and their design and implementation for a truly more inclusive approach to AI system design.",https://cris.unibo.it/retrieve/707c849f-3aeb-4ca3-8d1f-2817960064bd/societies-14-00259%20(1).pdf,,,,,,,,,,,,,,
228,7G73H3KM,Small (2023),Generative AI and opportunities for feminist classroom assignments,,journalArticle,2023.0,,Claude,"Addresses integration of generative AI into feminist classroom assignments to develop reflexivity and feminist epistemology. Proposes intentional feminist approaches where students collaborate with AI to develop understanding of feminist knowledge production, transforming AI from educational threat into tool for critical learning about power and epistemology.",https://digitalcommons.calpoly.edu/feministpedagogy/vol3/iss5/10/,,,,,,,,,,,,,,
229,7U29SIC8,Sperling (2024),In search of artificial intelligence (AI) literacy in teacher education: A scoping review,10.1016/j.caeo.2024.100169,journalArticle,2024.0,en,Manual,,https://linkinghub.elsevier.com/retrieve/pii/S2666557324000107,,,,,,,,,,,,,,
230,Q8YPNNKL,Srinivasan (2025),Mitigating trust-induced inappropriate reliance on AI assistance,,journalArticle,2025.0,,ChatGPT,"Investigates trust-adaptive interventions to reduce inappropriate reliance on AI recommendations in decision-support tasks. Argues that AI assistants should adapt behavior based on user trust to mitigate both under- and over-trust. In two decision scenarios shows that providing supportive explanations for low trust and counter-explanations for high trust leads to 38% reduction in inappropriate reliance and 20% improvement in decision accuracy. Demonstrates that adaptive insertion of forced pauses to promote deliberation reduces over-trust, opening new paths for improved human-AI collaboration.",https://arxiv.org/pdf/2502.13321.pdf,,,,,,,,,,,,,,
231,7AMRV4DT,Srinivasan (2025),Mitigating trust-induced inappropriate reliance on AI assistance,,journalArticle,2025.0,,Gemini,"Investigates trust-adaptive interventions to reduce inappropriate reliance on AI recommendations in decision-support tasks. Argues that AI assistants should adapt behavior based on user trust to mitigate both under- and over-trust. In two decision scenarios shows that providing supportive explanations for low trust and counter-explanations for high trust leads to 38% reduction in inappropriate reliance and 20% improvement in decision accuracy. Demonstrates that adaptive insertion of forced pauses to promote deliberation reduces over-trust, opening new paths for improved human-AI collaboration.",https://arxiv.org/pdf/2502.13321.pdf,,,,,,,,,,,,,,
232,7AS5MAU9,Srinivasan (2025),Mitigating trust-induced inappropriate reliance on AI assistance,,journalArticle,2025.0,,Perplexity,"Investigates trust-adaptive interventions to reduce inappropriate reliance on AI recommendations in decision-support tasks. Argues that AI assistants should adapt behavior based on user trust to mitigate both under- and over-trust. In two decision scenarios shows that providing supportive explanations for low trust and counter-explanations for high trust leads to 38% reduction in inappropriate reliance and 20% improvement in decision accuracy. Demonstrates that adaptive insertion of forced pauses to promote deliberation reduces over-trust, opening new paths for improved human-AI collaboration.",https://arxiv.org/pdf/2502.13321.pdf,,,,,,,,,,,,,,
233,N7J2ZRFP,Srivastava (2024),Algorithmic Governance and the International Politics of Big Tech,,journalArticle,2024.0,,Claude,"Structural analysis examines how Big Tech corporations exercise ""entrepreneurial private authority"" through algorithmic governance systems challenging state sovereignty. Moves beyond individual-focused approaches to analyze how technology firms exercise instrumental, structural, and discursive power globally. Critiques approaches focusing on individual user agency or technical fixes, examining instead how ""Big Tech's algorithmic governance incentivizes 'information pollution'"" and creates systemic power imbalances.",https://www.cambridge.org/core/journals/perspectives-on-politics/article/algorithmic-governance-and-the-international-politics-of-big-tech/3C04908735A5F2EE8A70AFED647741FB,,,,,,,,,,,,,,
234,4JN7NIS4,Steiner (2022),"Künstliche Intelligenz in der Sozialen Arbeit: Grundlagen, Entwicklungen, Herausforderungen",10.1007/s12054-022-00546-4,journalArticle,2022.0,,Claude,"Systematically analyzes two key AI application scenarios: Predictive Risk Modeling (PRM) and chatbots in counseling. Discusses neural networks' black box problem, dangers of case labeling through standardization, ethical questions of responsibility and liability when AI predictions diverge from professional judgment, and algorithmic bias risks perpetuating social inequalities. Uses Jonas' ethical theory of responsibility to emphasize ethical responsibility as foundational to all AI implementation decisions.",,,,,,,,,,,,,,,
235,KRUQB7L2,Steyvers (2025),What large language models know and what people think they know,10.1038/s42256-024-00976-7,journalArticle,2025.0,,ChatGPT,"High-impact study investigating mismatch between LLMs' confidence and users' trust. Found users often over-trust LLM answers, especially with explanations. Longer, detailed explanations increased user confidence even when answers weren't more accurate. Demonstrated that prompt engineering to convey uncertainty accurately helps users calibrate trust better. Aligning explanation with model's true confidence narrowed ""calibration gap,"" improving accuracy in judging when to trust AI.",https://www.nature.com/articles/s42256-024-00976-7,,,,,,,,,,,,,,
236,RXNXJA8W,Steyvers (2025),What large language models know and what people think they know,10.1038/s42256-024-00976-7,journalArticle,2025.0,,Gemini,"High-impact study investigating mismatch between LLMs' confidence and users' trust. Found users often over-trust LLM answers, especially with explanations. Longer, detailed explanations increased user confidence even when answers weren't more accurate. Demonstrated that prompt engineering to convey uncertainty accurately helps users calibrate trust better. Aligning explanation with model's true confidence narrowed ""calibration gap,"" improving accuracy in judging when to trust AI.",https://www.nature.com/articles/s42256-024-00976-7,,,,,,,,,,,,,,
237,QVNJIQJG,Strauß (2024),CAIL – Critical AI Literacy: Kritische Technikkompetenz für konstruktiven Umgang mit KI-basierter Technologie in Betrieben,,report,2024.0,,Perplexity,"This study develops a Critical AI Literacy framework defined as critical competency for assessing practical utility and limitations of AI applications in specific contexts. It emphasizes that AI-based automation is more complex, dynamic, and volatile than classical automation forms, creating new challenges. A central finding is that critical AI competency becomes part of knowledge work, as interpretation and verification of AI results remains an essential human task. The project identifies deep automation bias as a meta-risk of AI deployment.",https://wien.arbeiterkammer.at/service/digifonds/gefoerderte-projekte/CAIL_Bericht-FINAL.pdf,,,,,,,,,,,,,,
238,JRG3B3LE,Struppek (2024),Homoglyph unlearning: A novel approach to bias mitigation,,report,2024.0,,Gemini,"It has been shown that many generative models inherit and amplify societal biases. To date, there is no uniform/systematic agreed standard to control/adjust for these biases. This study examines the presence and manipulation of societal biases in leading text-to-image models: Stable Diffusion, DALL·E 3, and Adobe Firefly. Through a comprehensive analysis combining base prompts with modifiers and their sequencing, we uncover the nuanced ways these AI technologies encode biases across gender, race, geography, and region/culture. Our findings reveal the challenges and potential of prompt engineering in controlling biases, highlighting the critical need for ethical AI development promoting diversity and inclusivity.

This work advances AI ethics by not only revealing the nuanced dynamics of bias in text-to-image generation models but also by offering a novel framework for future research in controlling bias. Our contributions—spanning comparative analyses, the strategic use of prompt modifiers, the exploration of prompt sequencing effects, and the introduction of a bias sensitivity taxonomy—lay the groundwork for the development of common metrics and standard analyses for evaluating whether and how future AI models exhibit and respond to requests to adjust for inherent biases.",https://arxiv.org/html/2406.05602v1,,,,,,,,,,,,,,
239,LXKXPD5H,Studeny (2025),Digitale Werkzeuge und Machtasymmetrien?,,conferencePaper,2025.0,,Perplexity,"Studeny analyzes power asymmetries in digital social work, emphasizing that digital tools and algorithms create new, often invisible forms of power and control. AI decisions remove influence from both professionals and clients while responsibility remains unclear. Algorithms reinforce discrimination as they work with biased data. The author demands that social work critically reflects on digital technologies, demands transparency, and ensures that technology serves people.",https://www.ogsa.at/wp-content/uploads/2025/03/ogsaTAGUNG2025_AG-Digitalisierung_Handout-Machtasymmetrien-Susanne-Studeny.pdf,,,,,,,,,,,,,,
240,H7E3N6VR,Sūna (2024),Diskriminierung durch Algorithmen – Überlegungen zur Stärkung KI-bezogener Kompetenzen,,bookSection,2024.0,,ChatGPT,"Konzeptioneller Beitrag zu Ursachen und Formen algorithmischer Diskriminierung und zur Förderung kritischer KI-Kompetenzen. Plädiert für Aufklärung zu Datenbias, reflexive Nutzung und partizipative Trainings, um Benachteiligungen zu erkennen und digitale Teilhabe zu stärken.",https://www.gmk-net.de/wp-content/uploads/2024/12/gmk60_suna_hoffmann_mollen.pdf,,,,,,,,,,,,,,
241,T9KEZN3G,Taeihagh (2025),Governance of generative AI: A comprehensive framework for navigating challenges and opportunities,,journalArticle,2025.0,,Gemini,"Provides comprehensive overview of governance challenges posed by generative AI, including bias amplification, privacy violations, misinformation, and exacerbation of power imbalances. Critiques inadequacy of voluntary self-regulation and proposes comprehensive governance framework that is proactive, adaptive, and participatory. Recommends improving data governance, mandating independent audits, enhancing public engagement, and fostering international cooperation.",https://academic.oup.com/policyandsociety/article/44/1/1/7997395,,,,,,,,,,,,,,
242,GFXYER4F,Takaoka (2022),AI implementation science for social issues: Pitfalls and tips,10.2188/jea.JE20210380,journalArticle,2022.0,,Claude,"Case study documenting four-stage social implementation of AI system (AiCAN - Assistant of Intelligence for Child Abuse and Neglect) in Japanese Child Guidance Centers from 2012-2020. System uses machine learning to predict child abuse recurrence and Bayesian networks for real-time probabilistic inference to guide temporary protection decisions. Data from over 6,000 cases (2014-2018) were used to develop gradient boosting algorithms with AUROC >0.70. Implementation involved iterative stakeholder engagement, workflow redesign, training field staff, and addressing organizational resistance. Emphasizes critical importance of building consensus with practitioners, designing for field usability, ensuring data quality through validated scales, and employing eXplainable AI for transparency.",,,,,,,,,,,,,,,
243,9MDIBETZ,Tang (2024),GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models,10.1145/3658644.3670284,conferencePaper,2024.0,en,Manual,,https://dl.acm.org/doi/10.1145/3658644.3670284,,,,,,,,,,,,,,
244,JZ4P8V8S,Thwaites (2024),Operationalizing positive-constructive pedagogy to artificial intelligence: the 3E model for scaffolding AI technology adoption,10.3389/feduc.2024.1293235,journalArticle,2024.0,,Gemini,"This article proposes the ""3E model"" (Expose, Explore, Exploit) as a pedagogical framework for developing critical AI literacy. The model aims to move students beyond passive use of AI to a more critical engagement. The ""Expose"" phase involves revealing the underlying mechanisms and biases of AI systems. ""Explore"" encourages students to test AI boundaries and critically question its outputs, a practice akin to critical prompting. ""Exploit"" focuses on using AI for creative and novel purposes. While not explicitly feminist, the framework provides a practical method for developing the critical literacies needed to identify and question the co-constitution of discrimination in AI outputs, thereby making biases visible.",https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2024.1293235/full,,,,,,,,,,,,,,
245,43CRXRRT,Tinmaz (2022),A systematic review on digital literacy,10.1186/s40561-022-00204-y,journalArticle,2022.0,en,Manual,"Abstract
            The purpose of this study is to discover the main themes and categories of the research studies regarding digital literacy. To serve this purpose, the databases of WoS/Clarivate Analytics, Proquest Central, Emerald Management Journals, Jstor Business College Collections and Scopus/Elsevier were searched with four keyword-combinations and final forty-three articles were included in the dataset. The researchers applied a systematic literature review method to the dataset. The preliminary findings demonstrated that there is a growing prevalence of digital literacy articles starting from the year 2013. The dominant research methodology of the reviewed articles is qualitative. The four major themes revealed from the qualitative content analysis are: digital literacy, digital competencies, digital skills and digital thinking. Under each theme, the categories and their frequencies are analysed. Recommendations for further research and for real life implementations are generated.",https://slejournal.springeropen.com/articles/10.1186/s40561-022-00204-y,,,,,,,,,,,,,,
246,WLV8L8PM,Tint (2025),"Guardrails, not guidance: Understanding responses to LGBTQ+ language in large language models",,conferencePaper,2025.0,,ChatGPT,"Examines how large language models respond to prompts involving LGBTQ+ terminology and how current safety measures handle such content. Finds disparity where LLMs invoke safety guardrails for overtly heteronormative prompts but exhibit subtle biases when handling queer slang or informal LGBTQ+ language, responding with more negative emotional tone without triggering content filters.",https://aclanthology.org/2025.queerinai-main.2.pdf,,,,,,,,,,,,,,
247,ZAU6P4BK,Toupin (2024),Shaping feminist artificial intelligence,,journalArticle,2024.0,,Perplexity,"This study examines the historical and contemporary shaping of feminist AI (FAI) through a typology of six approaches: FAI as model, design, politics, culture, discourse, and science. Toupin analyzes how feminist perspectives are implemented in various areas of AI development and identifies both potentials and limitations of feminist approaches. The work shows that FAI is not only a technological concept but also a movement that aims to transform power relations in AI development and promote social justice.",,,,,,,,,,,,,,,
248,SHDNTZJZ,Toupin (2024),Shaping feminist artificial intelligence,10.1177/14614448221150776,journalArticle,2024.0,,ChatGPT,"Toupin reviews historical and conceptual developments of feminist AI, identifying six key forms—model, design, policy, culture, discourse, and science. She demonstrates how feminist initiatives historically and currently surface hidden biases and power dynamics in AI. Advocates for integrating feminist perspectives directly into AI discourse and development.",https://doi.org/10.1177/14614448221150776,,,,,,,,,,,,,,
249,IGFYSIV8,Toupin (2024),Shaping feminist artificial intelligence,10.1177/14614448221150776,journalArticle,2024.0,,Claude,"Comprehensive examination of feminist artificial intelligence through historical analysis and contemporary typology development. Provides detailed framework categorizing FAI as: model, design, policy, culture, discourse, and science. Traces FAI's evolution from foundational work to contemporary initiatives, analyzing tensions between commercialized approaches and community-oriented methods.",https://journals.sagepub.com/doi/full/10.1177/14614448221150776,,,,,,,,,,,,,,
250,BJZYGNEE,Toupin (2024),Shaping feminist artificial intelligence,10.1177/14614448221150776,journalArticle,2024.0,,Claude,"Comprehensive typology establishes six ways feminism and AI intersect: model, design, policy, culture, discourse, and science. The ""design"" category most directly addresses prompting practices, exploring how feminist approaches emphasize participatory design, community involvement, and challenging masculine-coded AI systems. Traces feminist AI from 1990s theoretical foundations through contemporary projects, establishing frameworks for feminist human-AI interaction informing critical prompting practices.",https://journals.sagepub.com/doi/full/10.1177/14614448221150776,,,,,,,,,,,,,,
251,GFHALQS2,Tun (2025),Trust in artificial intelligence–based clinical decision support systems among health care workers: Systematic review,10.2196/69678,journalArticle,2025.0,,ChatGPT,"Systematic review synthesizing 27 studies on clinicians' trust in AI decision-support tools. Identifies eight key factors shaping professional trust, notably system transparency as primary enabler. Lack of transparency was recurring barrier with ""black-box"" algorithms undermining trust. Improving transparency, providing explainable recommendations, and addressing bias/fairness issues recommended to bolster trust. Concludes transparent, explainable AI with proper training and ethical safeguards crucial for practitioner trust.",https://www.jmir.org/2025/1/e69678,,,,,,,,,,,,,,
252,ERTJZW5M,Tun (2025),Trust in artificial intelligence–based clinical decision support systems among health care workers: Systematic review,10.2196/69678,journalArticle,2025.0,,Gemini,"Systematic review synthesizing 27 studies on clinicians' trust in AI decision-support tools. Identifies eight key factors shaping professional trust, notably system transparency as primary enabler. Lack of transparency was recurring barrier with ""black-box"" algorithms undermining trust. Improving transparency, providing explainable recommendations, and addressing bias/fairness issues recommended to bolster trust. Concludes transparent, explainable AI with proper training and ethical safeguards crucial for practitioner trust.",https://www.jmir.org/2025/1/e69678,,,,,,,,,,,,,,
253,WS4KQPWN,Ulnicane (2024),Intersectionality in artificial intelligence: Framing concerns and recommendations for action,10.17645/si.v12i1.7543,journalArticle,2024.0,,Perplexity,"Die Arbeit analysiert vier Berichte zur Intersektionalität in KI, zeigt wie mangelnde Diversität zu biased KI-Systemen führt, und dokumentiert Vorurteile in Robotern, Sprachassistenten und HR-Tools.",https://doi.org/10.17645/si.v12i1.7543,,,,,,,,,,,,,,
254,FDR5APIU,Ulnicane (2024),Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,10.17645/si.v12.7543,journalArticle,2024.0,,Perplexity,"Analyzes emerging intersectionality agenda in AI through examination of four high-level reports on this topic (2019-2021). Research shows how these documents frame problems and formulate recommendations for addressing inequalities. AI systems often amplify and exacerbate human biases and stereotypes, leading to discrimination and marginalization. Analysis reveals systematic problems including diversity crises in AI development where founders and employees mainly come from homogeneous groups of white men, and reinforcement of existing power relationships through AI systems.",,,,,,,,,,,,,,,
255,SSH3LVN6,Ulnicane (2024),Artificial Intelligence and Intersectionality,,journalArticle,2024.0,,Perplexity,"Diese Analyse untersucht, wie KI-Dokumente Bedenken über Bias und Ungleichheit in KI rahmen und Empfehlungen zur Bekämpfung formulieren. Mittels intersektionaler Linse wird die Interaktion multipler Identitäten (Geschlecht, Rasse, Klasse) hervorgehoben, die zu Marginalisierung und Diskriminierung bestimmter sozialer Gruppen führt. Die Studie unterscheidet zwischen technischen und sozio-technischen Framings von KI-Bias und zeigt auf, dass technische Frames KI oft als objektiv und neutral darstellen, während sozio-technische Ansätze die sozialen, politischen und historischen Dimensionen von Bias anerkennen.",https://ecpr.eu/news/news/details/749,,,,,,,,,,,,,,
256,8U23BX2J,Ulnicane (2024),Intersectionality in artificial intelligence: Framing concerns and recommendations for action,10.17645/si.v12.7543,journalArticle,2024.0,,ChatGPT,"Ulnicane’s article investigates how intersectionality – the overlapping of gender, race, class and other social inequalities – is being addressed in debates about AI bias. Through an analysis of four high-profile reports on AI and discrimination, the study finds that AI is often incorrectly portrayed as neutral, whereas in reality it amplifies existing societal biases, leading to discriminatory outcomes. A core issue identified is the tech sector’s “diversity crisis”: a homogenous AI workforce (primarily white, male developers) embeds its biases into AI systems, creating a feedback loop of inequality. The reviewed reports frame this situation as urgent – noting that past superficial diversity initiatives have failed – and call for holistic, structural solutions. Rather than just adding token diversity, they emphasize changing organizational culture and power dynamics in AI development, and giving marginalized groups more influence in shaping AI. Ulnicane highlights a concern that discussions of intersectional fairness risk being siloed as special-interest issues “for women and minorities” instead of being treated as core to AI innovation. The article concludes with recommendations: centering intersectionality and equity in AI policy, ensuring that efforts to fix AI bias focus on underlying power structures and systemic change, not merely on technical tweaks or increased headcounts.",https://www.cogitatiopress.com/socialinclusion/article/download/7543/3744,,,,,,,,,,,,,,
257,X54V3JMF,Ulnicane (2024),Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,10.17645/si.7543,journalArticle,2024.0,,Claude,"Applies Crenshaw's intersectionality theory to examine four high-profile AI policy reports, revealing how diversity crises in AI workforce create ""negative feedback loops"" where homogeneous development teams embed biases into systems. Demonstrates how multiple forms of discrimination co-constitute each other through voice assistants, robots, and hiring tools, showing that intersectional experiences cannot be reduced to single identity categories. Argues for moving beyond simple diversity initiatives toward addressing culture, power, and structural inequalities.",,,,,,,,,,,,,,,
258,SJLJ2GHC,Ulnicane (2024),Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,10.17645/si.7543,journalArticle,2024.0,,Gemini,"This article critically examines how intersectionality is understood and applied in European Union AI policy documents. Ulnicane argues that current policy approaches often reduce intersectionality to a mere ""multi-category"" perspective, focusing on adding more diversity variables without addressing the underlying structural power dynamics that create inequalities. The study reveals that while policies acknowledge bias, they fail to adequately address the systemic nature of discrimination co-constituted at the intersections of gender, race, and other identity markers. The author recommends moving beyond simplistic diversity initiatives towards a more holistic, systemic, and transformative approach to AI ethics and governance, which explicitly confronts structural power imbalances.",https://www.cogitatiopress.com/socialinclusion/article/view/7543,,,,,,,,,,,,,,
259,9YCFMPVT,Ulnicane (2024),Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,10.17645/si.v12i2.7543,journalArticle,2024.0,,ChatGPT,"Ulnicane analyzes four high-profile reports on intersectionality in AI, identifying a “vicious cycle” of bias perpetuated by a homogeneous AI workforce. She argues previous diversity initiatives have largely failed and advocates a holistic approach to altering power structures and cultures within AI development. Intersectional perspectives must move from the periphery to challenge core AI agendas.",https://doi.org/10.17645/si.v12i2.7543,,,,,,,,,,,,,,
260,9BDIJE9B,UN Women (2024),Artificial Intelligence and gender equality,,report,2024.0,,Claude,"Comprehensive policy brief series analyzing how AI systems perpetuate gender inequalities while highlighting pathways for more equitable development. Documents that 44% of AI systems show gender bias, with 25% exhibiting both gender and racial bias, and provides evidence-based policy recommendations for addressing systematic underrepresentation and discriminatory outcomes.",https://www.unwomen.org/en/articles/explainer/artificial-intelligence-and-gender-equality,,,,,,,,,,,,,,
261,ZNHUCA4B,UNESCO (2021),Recommendation on the Ethics of Artificial Intelligence,,report,2021.0,,Claude,"First global standard on AI ethics adopted by 193 member states, establishing comprehensive policy frameworks addressing gender equality in AI development and deployment. Explicitly addresses gender stereotyping, discriminatory biases, and need for equitable participation across the AI lifecycle, with recent implementation including Women4Ethical AI platform and systematic bias studies.",https://unesdoc.unesco.org/ark:/48223/pf0000380455,,,,,,,,,,,,,,
262,6QPLNNQK,UNESCO (2024),Bias against women and girls in large language models: A UNESCO study,,report,2024.0,,Perplexity,"Die Studie analysiert LLMs (z. B. GPT-3.5, Llama 2) und dokumentiert signifikante Stereotypisierungen gegen Frauen und queere Menschen in generierten Texten. Besondere Auffälligkeit bei Open-Source-Modellen.",https://www.unesco.org/en/articles/generative-ai-unesco-study-reveals-alarming-evidence-regressive-gender-stereotypes,,,,,,,,,,,,,,
263,TSYJ3Y57,UNESCO (2024),Women4Ethical AI: Global cooperation for gender-inclusive AI,,report,2024.0,,Perplexity,"UNESCO-Initiative zur Förderung genderinklusiver KI-Entwicklung. Fokus auf globale Zusammenarbeit, Menschenrechtsprinzipien und Expertinnenbeteiligung in allen Phasen.",https://www.unesco.org/en/articles/unescos-women4ethical-ai-urges-global-cooperation-gender-inclusive-ai,,,,,,,,,,,,,,
264,E35KJFXB,Unknown (),,,journalArticle,,,Manual,,,,,,,,,,,,,,,,
265,2FUXNFZS,Unknown (2024),"RETHINKING SOCIAL SERVICES WITH ARTIFICIAL INTELLIGENCE: OPPORTUNITIES, RISKS, AND FUTURE PERSPECTIVES",,bookSection,2024.0,,Gemini,"This chapter explores the transformative potential of artificial intelligence (AI) in the field of social services. It highlights how AI—through data analysis, predictive modeling, and administrative automation—can enhance the effectiveness, accessibility, and efficiency of social work practice. The chapter also presents significant ethical concerns, including risks of algorithmic bias, loss of human connection, and violations of privacy. The author emphasizes that while AI can complement social work, it cannot replace the human-centered values at the core of the profession. It concludes by urging institutions and educational programs to prepare social workers for ethical and effective use of AI and calls for multidisciplinary collaboration to develop guidelines that ensure AI integration supports social justice, equality, and human rights.",https://www.researchgate.net/publication/393509196_RETHINKING_SOCIAL_SERVICES_WITH_ARTIFICIAL_INTELLIGENCE_OPPORTUNITIES_RISKS_AND_FUTURE_PERSPECTIVES,,,,,,,,,,,,,,
266,KQ5A8D6E,Unknown (2024),AI competency framework for students,,book,2024.0,,Manual,,https://unesdoc.unesco.org/ark:/48223/pf0000391105,,,,,,,,,,,,,,
267,SS5HTYY6,Unknown (2025),"Artificial Intelligence in Social Sciences and Social Work: Bridging Technology and Humanity to Revolutionize Research, Policy, and Human Services",,journalArticle,2025.0,,Gemini,"This review explores the transformative role of artificial intelligence (AI) in the fields of social sciences and social work, with a focus on developments from 2022 to 2025. It examines how AI technologies—such as machine learning, natural language processing—enhance the analysis of complex social phenomena, support real-time forecasting, and inform data-driven policymaking. Within social work and human services, AI-driven tools facilitate case management, mental health interventions, crisis response, and resource allocation. While acknowledging AI's potential to improve equity and access, the article critically engages with ethical concerns around algorithmic bias, privacy, surveillance, and the erosion of human-centered care. Drawing on recent policy frameworks like the EU AI Act and UNESCO's AI Ethics Guidelines, the review calls for interdisciplinary collaboration to ensure the ethical, inclusive, and accountable integration of AI in social contexts.",https://www.multispecialityjournal.com/uploads/archives/20250920153430_MCR-2025-5-004.1.pdf,,,,,,,,,,,,,,
268,HSQW48VE,Unknown (2025),Artificial Intelligence in Social Work: An EPIC Model for Practice,10.1080/0312407X.2025.2488345,journalArticle,2025.0,,Gemini,"As artificial intelligence (AI) permeates the workplace environments of social workers, there is a need to understand the risks and benefits posed to the mission and values of the profession. This article examines the influence of artificial intelligence on the profession, including opportunities to advance socially just outcomes and challenges that risk ethical practice. A comprehensive review of literature was conducted to examine existing research on the intersection of AI and social work. Drawing on insights from this review, an EPIC model for integrating artificial intelligence into the profession is presented, consisting of four components: (E) ethics and justice; (P) policy development and advocacy; (I) intersectoral collaboration; and (C) community engagement and empowerment. The author contends that augmenting the benefits of artificial intelligence in social work requires a proactive and ethical approach towards a more secure, safe, transparent, and socially just future.",https://www.tandfonline.com/doi/full/10.1080/0312407X.2025.2488345,,,,,,,,,,,,,,
269,7D3ICY7Z,van Toorn (2024),"Introduction to the digital welfare state: Contestations, considerations and entanglements",10.1177/14407833241260890,journalArticle,2024.0,,Claude,"Special issue introduction providing critical sociological analysis of digital welfare state, examining how datafication and automation amplify existing trends of surveillance and control over marginalized populations. Authors argue that contrary to neutral efficiency narratives, digital welfare technologies are embedded in fiscal austerity politics and criminalization of poverty. Employs power relations and human agency frameworks to demonstrate how algorithmic systems increase scrutiny of welfare recipients, migrants, and undeserving populations while prioritizing cost-cutting over meeting social needs. Critiques framing of AI as unprecedented innovation, situating digital welfare within historical dynamics of social control. Key themes include erosion of professional discretion, surveillance assemblages, and automated decision-making reproducing structural inequalities.",,,,,,,,,,,,,,,
270,Y6SAPNT2,Vethman (2025),Fairness Beyond the Algorithmic Frame: Actionable Recommendations for an Intersectional Approach,,conferencePaper,2025.0,,Perplexity,"This study develops actionable recommendations for AI experts to implement intersectional approaches beyond purely technical solutions. The authors criticize the reduction of intersectionality to algorithmic bias measurements between subgroups and develop a framework that emphasizes interdisciplinary teams, community participation, and analysis of power structures in social context. Their approach includes six core recommendations: responsible AI expert role, multidisciplinary team building, reflection on societal positioning, participatory community engagement, power and context analysis, and data-sensitive metrics.",https://facctconference.org/static/docs/facct2025-206archivalpdfs/facct2025-final2348-acmpaginated.pdf,,,,,,,,,,,,,,
271,9YYPYEGY,Victor (2023),Recommendations for social work researchers and journal editors on the use of generative AI and large language models,10.1086/726021,journalArticle,2023.0,,ChatGPT,"Develops ""disruptive-disrupting"" framework for analyzing generative AI in social work research with concrete recommendations for researchers and journal editors. Emphasizes transparency and accountability requiring researchers to fully document AI use, disclose prompts, and take responsibility for AI-generated content. Argues that continuous education about AI construction, limitations, and ethical considerations is essential for developing appropriate trust in LLM systems.",https://doi.org/10.1086/726021,,,,,,,,,,,,,,
272,4PK8UN82,Victor (2023),Recommendations for social work researchers and journal editors on the use of generative AI and large language models,10.1086/726021,journalArticle,2023.0,,Gemini,"Develops ""disruptive-disrupting"" framework for analyzing generative AI in social work research with concrete recommendations for researchers and journal editors. Emphasizes transparency and accountability requiring researchers to fully document AI use, disclose prompts, and take responsibility for AI-generated content. Argues that continuous education about AI construction, limitations, and ethical considerations is essential for developing appropriate trust in LLM systems.",https://doi.org/10.1086/726021,,,,,,,,,,,,,,
273,U3AJIXAJ,Victor (2023),Recommendations for social work researchers and journal editors on the use of generative AI and large language models,10.1086/726021,journalArticle,2023.0,,Perplexity,"Develops ""disruptive-disrupting"" framework for analyzing generative AI in social work research with concrete recommendations for researchers and journal editors. Emphasizes transparency and accountability requiring researchers to fully document AI use, disclose prompts, and take responsibility for AI-generated content. Argues that continuous education about AI construction, limitations, and ethical considerations is essential for developing appropriate trust in LLM systems.",https://doi.org/10.1086/726021,,,,,,,,,,,,,,
274,3XMBE43Z,Voutyrakou (2025),Algorithmic Governance: Gender Bias in AI-Generated Policymaking?,10.1007/s44230-025-00109-2,journalArticle,2025.0,,ChatGPT,"Examines whether gender-specific needs are reflected in AI-generated policies, demonstrating through GPT-4 and Copilot experiments that AI tends to overlook female-specific needs unless explicitly prompted. Highlights androcentric biases, advocating intersectionally-informed prompting to surface hidden biases but recognizing the limits of individual prompt-based solutions in addressing structural AI biases.",https://doi.org/10.1007/s44230-025-00109-2,,,,,,,,,,,,,,
275,GZ9B3GPD,Waag (2023),Rationalisierung durch Digitalisierung?,10.1007/s12592-023-00472-6,journalArticle,2023.0,,Claude,"Contributes labor sociology and interaction sociology perspectives (particularly Luhmann's interaction theory) to digitalization analyses in social work. Examines potential advantages and disadvantages from multiple stakeholder perspectives (professionals, service users, organizations), revealing that fears and hopes regarding rationalization through digitalization are overly simplistic. Highlights irreducible complexity of professional helping relationships and fundamental limitations of applying rationalization logic to social work contexts.",,,,,,,,,,,,,,,
276,PQC9G5EU,Wajcman (2023),Feminism Confronts AI: The Gender Relations of Digitalisation,,bookSection,2023.0,,Gemini,"Wajcman and Young provide a feminist critique of AI, arguing that the technology is not neutral but deeply embedded in existing gendered power structures. They highlight the severe underrepresentation of women in AI development as a key source of bias, leading to the creation of systems that reflect and amplify a masculine worldview. The authors contend that simply adding more women to the field is insufficient. Instead, they call for a fundamental shift in the culture of technology production, challenging the technical-social dualism and integrating feminist perspectives into the very design of AI. This requires addressing the structural power asymmetries that shape technological development and moving beyond individualistic solutions.",https://academic.oup.com/book/55103/chapter/423909956,,,,,,,,,,,,,,
277,23Y3627L,Wang (2023),Measuring user competence in using artificial intelligence: validity and reliability of artificial intelligence literacy scale,10.1080/0144929X.2022.2072768,journalArticle,2023.0,en,Manual,,https://www.tandfonline.com/doi/full/10.1080/0144929X.2022.2072768,,,,,,,,,,,,,,
278,NVZA58ML,Wang (2024),A survey on fairness in large language models,,report,2024.0,,Gemini,,https://file.mixpaper.cn/paper_store/2023/5ddea1cd-c031-433f-a09b-14e7754f7826.pdf,,,,,,,,,,,,,,
279,XK6G84V7,Wang (2024),Algorithmic discrimination: examining its types and regulatory measures with emphasis on US legal practices,10.3389/frai.2024.1320277,journalArticle,2024.0,,Perplexity,"Diese umfassende Systematik identifiziert fünf primäre Typen algorithmischer Diskriminierung: Bias durch algorithmische Agenten, diskriminierende Merkmalsselektion, Proxy-Diskriminierung, disparate Auswirkungen und gezielte Werbung. Die Analyse der US-Rechtslandschaft offenbart einen mehrstufigen Regulierungsansatz aus prinzipieller Regulierung, präventiven Kontrollen, konsequenter Haftung und Selbstregulierung. Zentral ist die Erkenntnis, dass unbeabsichtigte Diskriminierung durch scheinbar neutrale Algorithmen besonders schwer zu erkennen und zu regulieren ist, da sie strukturelle historische Ungleichheiten perpetuiert. Die Studie betont die Notwendigkeit interdisziplinärer Forschung und proaktiver Politikentwicklung.",https://doi.org/10.3389/frai.2024.1320277,,,,,,,,,,,,,,
280,3AHQEHDF,Wang (2024),Multilingual Prompting for Improving LLM Generation Diversity,,report,2024.0,,Perplexity,This paper introduces multilingual and multicultural prompting as methods to enhance the demographic and cultural diversity of Large Language Model outputs. The authors demonstrate these approaches outperform established diversity methods across multiple LLM architectures. Results indicate that prompting in culturally and linguistically aligned languages reduces hallucinated outputs and supports more representative generation.,https://arxiv.org/html/2505.15229v1,,,,,,,,,,,,,,
281,6BZ5353S,Wang (2025),Multilingual Prompting for Improving LLM Generation Diversity,,report,2025.0,,Perplexity,"This study introduces multilingual prompting as a strategy to enhance narrative diversity in LLM outputs. By using prompts with diverse languages and cultural cues, models produced outputs with improved demographic and opinion diversity. Compared to temperature-based and persona prompting, multilingual prompting was more effective and reduced cultural hallucinations.",https://arxiv.org/html/2505.15229v1,,,,,,,,,,,,,,
282,LPMP8QAY,Weber (2023),Messung von AI Literacy – Empirische Evidenz und Implikationen,,journalArticle,2023.0,,Manual,,https://aisel.aisnet.org/wi2023/3,,,,,,,,,,,,,,
283,RZ4QFXQI,West (2023),"Discriminating Systems: Gender, Race, and Power in AI",,report,2023.0,,Perplexity,"Diese einflussreiche Studie argumentiert, dass die Diversitätskrise im KI-Sektor und Bias in KI-Systemen zwei Manifestationen desselben Problems sind und gemeinsam angegangen werden müssen. Die Autoren zeigen, dass rein technische Ansätze zur Bias-Behebung unzureichend sind, da sie die systemischen Machtverhältnisse ignorieren, die sowohl Arbeitsplätze als auch Technologien formen. Das ""Pipeline-Problem""-Narrativ wird als zu eng kritisiert, da es tieferliegende Probleme mit Arbeitsplatzkultur, Machtasymmetrien und struktureller Diskriminierung nicht adressiert. Die Studie fordert eine Verschiebung von technischer ""Debiasing"" zu breiterer sozialer Analyse.",https://ainowinstitute.org/wp-content/uploads/2023/04/discriminatingsystems.pdf,,,,,,,,,,,,,,
284,T8R8RKX9,Wilson (2024),AI tools show biases in ranking job applicants' names according to perceived race and gender,,conferencePaper,2024.0,,Claude,Large-scale empirical study using over 550 resumes and 3+ million comparisons reveals that intersectional patterns of bias in AI resume screening cannot be understood as additive combinations of single-axis discrimination. Discovered unique harm against Black men invisible when examining race or gender independently—Black male names were never preferred over white male names (0% selection rate). Demonstrates co-constitutive nature of multiple discrimination where intersection of Blackness and masculinity creates distinct exclusion patterns.,https://ojs.aaai.org/index.php/AIES/article/view/31748,,,,,,,,,,,,,,
285,XW8NHCIE,Wilson (2024),"Gender, race, and intersectional bias in AI resume screening via language model retrieval",10.1609/aies.v7i1.31748,conferencePaper,2024.0,,Claude,"Analyzes bias in large language models used for resume screening, examining over 550 job descriptions and 550 resumes across multiple demographic combinations. Reveals significant intersectional discrimination with Black male candidates facing most severe disadvantages, validating three key hypotheses of intersectionality theory through over 40,000 comparisons.",https://doi.org/10.1609/aies.v7i1.31748,,,,,,,,,,,,,,
286,R2LPR3VD,Wong (2020),Broadening artificial intelligence education in K-12: where to start?,10.1145/3381884,journalArticle,2020.0,en,Manual,,https://dl.acm.org/doi/10.1145/3381884,,,,,,,,,,,,,,
287,C325Y32P,World Economic Forum (2024),AI for impact: The PRISM framework for responsible AI in social innovation,,report,2024.0,,Claude,"Institutional report introducing PRISM framework specifically designed for social innovators, impact enterprises, and intermediaries working in social services sectors. Building on Presidio Framework of AI Governance Alliance, PRISM provides adoption pathways through which organizations can filter their impact mission, capabilities, and risks against AI technology use. Framework includes AI-enabled readiness assessment matrix enabling organizations to evaluate current practices and develop actionable roadmaps for AI integration both internally and externally. Emphasizes ethical adoption of AI aligned with social impact missions.",https://www.weforum.org/publications/ai-for-impact-the-prism-framework-for-responsible-ai-in-social-innovation/,,,,,,,,,,,,,,
288,JGZDWMN3,Wu (2025),Bias in decision-making for AI's ethical dilemmas: A comparative study of ChatGPT and Claude,,report,2025.0,,Gemini,"Recent advances in Large Language Models (LLMs) have enabled human-like responses across various tasks, raising questions about their ethical decision-making capabilities and potential biases. This study investigates protected attributes in LLMs through systematic evaluation of their responses to ethical dilemmas. Using two prominent models - GPT-3.5 Turbo and Claude 3.5 Sonnet - we analyzed their decision-making patterns across multiple protected attributes including age, gender, race, appearance, and disability status. Through 11,200 experimental trials involving both single and intersectional protected attribute combinations, we evaluated the models’ ethical preferences, sensitivity, stability, and clustering of preferences. Our findings reveal significant protected attributes in both models, with consistent preferences for certain features (e.g., “good-looking”) and systematic neglect of others. Notably, while GPT-3.5 Turbo showed stronger preferences aligned with traditional power structures, Claude 3.5 Sonnet demonstrated more diverse protected attribute choices. We also found that ethical sensitivity significantly decreases in more complex scenarios involving multiple protected attributes. Additionally, linguistic referents heavily influence the models’ ethical evaluations, as demonstrated by differing responses to racial descriptors (e.g., “Yellow” versus “Asian”). These findings highlight critical concerns about the potential impact of LLM biases in autonomous decision-making systems and emphasize the need for careful consideration of protected attributes in AI development. Our study contributes to the growing body of research on AI ethics by providing a systematic framework for evaluating protected attributes in LLMs’ ethical decision-making capabilities.",https://arxiv.org/html/2501.10484v2,,,,,,,,,,,,,,
289,NHIZN4QJ,Wudel (2025),What is Feminist AI?,,report,2025.0,,Perplexity,"This publication examines Feminist Artificial Intelligence (FAI) as a framework that utilizes intersectional feminism to address biases and injustices in AI systems. FAI emphasizes interdisciplinary collaboration, systemic power analysis, and iterative theory-practice loops. By embedding feminist values (justice, freedom, and equity), FAI aims to transform AI development to ensure inclusivity and social sustainability. Practical applications include FemAI's advocacy for feminist perspectives in the EU AI Act and the MIRA diagnostic platform that aligns AI tools with social justice.",https://library.fes.de/pdf-files/bueros/bruessel/21888-20250304.pdf,,,,,,,,,,,,,,
290,GPSB87RN,Wudel (2025),What is Feminist AI?,,report,2025.0,,Perplexity,"Defines Feminist AI (FAI) as framework using intersectional feminism to address bias and inequalities in AI systems. Emphasizes interdisciplinary collaboration, systemic power analysis, and iterative theory-practice loops. Embeds feminist values of equality, freedom, and justice to transform AI development. Includes practical applications like FemAI advocacy for feminist perspectives in EU AI Act and MIRA diagnostic platform aligning AI tools with social justice goals. Distinguishes FAI from traditional ""Responsible AI"" approaches through focus on structural power inequalities rather than individual ""bad actors.""",https://library.fes.de/pdf-files/bueros/bruessel/21888-20250304.pdf,,,,,,,,,,,,,,
291,NBYNRKBL,Wudel (2025),What is Feminist AI?,,report,2025.0,,Perplexity,"Das Papier entwickelt einen Rahmen für Feminist AI (FAI), der intersektionale feministische Methodologie zur Adressierung von Bias und Ungleichheit in KI-Systemen nutzt. FAI betont interdisziplinäre Zusammenarbeit, systematische Machtanalyse und iterative Theorie-Praxis-Schleifen. Durch die Einbettung feministischer Werte (Gleichberechtigung, Freiheit, Gerechtigkeit) zielt FAI darauf ab, KI-Entwicklung zu transformieren und Inklusivität sowie soziale Nachhaltigkeit sicherzustellen. Praktische Anwendungen umfassen FemAI's Advocacy für feministische Perspektiven im EU AI Act und die MIRA-Diagnose-Plattform. FAI markiert eine kritische Abkehr von traditioneller KI durch die Bekämpfung struktureller Ungleichheiten.",https://library.fes.de/pdf-files/bueros/bruessel/21888-20250304.pdf,,,,,,,,,,,,,,
292,NUYCHW2T,Xu (2023),Transparency enhances positive perceptions of social artificial intelligence,10.1155/2023/5550418,journalArticle,2023.0,,ChatGPT,"Experimental study with 914 participants testing how transparent explanations about chatbot workings affect user perceptions. Results show transparency modestly improved trust-related perceptions: users found chatbot ""less creepy,"" felt more affinity, and perceived it as more socially intelligent when transparent explanation provided. Transparency's impact on perceived intelligence stronger for users with low prior AI knowledge. Suggests prompt-engineering for greater transparency can foster user comfort and trust.",,,,,,,,,,,,,,,
293,NFW58AU8,Xu (2023),Transparency enhances positive perceptions of social artificial intelligence,10.1155/2023/5550418,journalArticle,2023.0,,Gemini,"Experimental study with 914 participants testing how transparent explanations about chatbot workings affect user perceptions. Results show transparency modestly improved trust-related perceptions: users found chatbot ""less creepy,"" felt more affinity, and perceived it as more socially intelligent when transparent explanation provided. Transparency's impact on perceived intelligence stronger for users with low prior AI knowledge. Suggests prompt-engineering for greater transparency can foster user comfort and trust.",,,,,,,,,,,,,,,
294,2SNYUZG4,Yan (2024),Promises and challenges of generative artificial intelligence for human learning,10.1038/s41562-024-02004-5,journalArticle,2024.0,en,Manual,,https://www.nature.com/articles/s41562-024-02004-5,,,,,,,,,,,,,,
295,Y6M97SWQ,Yu (2025),Algorithmic-Assisted Decision-Making Tools in Child Welfare Practice: A Systematic Review,10.1177/10497315251350933,journalArticle,2025.0,,ChatGPT,"PRISMA-guided review of algorithmic tools in child welfare. Finds potential for consistency and early risk identification but significant concerns about bias, transparency, practitioner training, and stakeholder inclusion. Recommends audits, participatory design, and ethical guidelines; highlights evidence gaps.",https://doi.org/10.1177/10497315251350933,,,,,,,,,,,,,,
296,22KJL3PC,Yuan (2025),The cultural stereotype and cultural bias of ChatGPT,10.1177/18344909251355673,journalArticle,2025.0,,ChatGPT,"This article examines cultural biases in ChatGPT-3.5 and GPT-4. Study 1 measures alignment with human cultural values. Study 2 finds clear cultural stereotypes in GPT-3.5 but fewer in GPT-4. Study 3 tests four diversity-sensitive prompts (emphasizing individuality, fairness, egalitarian futures, or multiculturalism). All four strategies eliminated cultural stereotypes in GPT-3.5's outputs. For GPT-4, bias mitigation was more nuanced, requiring task-specific prompts. This indicates that while various prompts can reduce stereotypes, newer models may need more targeted strategies.",https://www.researchgate.net/publication/393408216_The_cultural_stereotype_and_cultural_bias_of_ChatGPT,,,,,,,,,,,,,,
297,UDZLIJWX,Yunusov (2024),MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models,,report,2024.0,,Perplexity,"This empirical study introduces a corpus of 1,500 personalized short stories generated with LLMs, incorporating identity features like gender, ethnicity, and age. Human judges rated these stories higher in engagement, diversity, and personalness. Narrative personalization increased textual diversity without harming moral comprehension. However, biases persist, such as preferential engagement for certain identities. The paper illustrates both potential and limitations of diversity-sensitive prompting.",https://arxiv.org/html/2409.13935v1,,,,,,,,,,,,,,
298,49PPZJ7Z,Zakharova (2024),Tensions in digital welfare states: Three perspectives on care and control,10.1177/14407833241226800,journalArticle,2024.0,,Claude,"Examines tensions between care and control in digital welfare states, analyzing how welfare services increasingly rely on digital technologies and data systems. Develops three analytical perspectives: datafied care practices, algorithmic governance, and digitalized welfare encounters. Demonstrates how digitalization reshapes welfare provision by intensifying surveillance while potentially enabling new forms of care. Reveals fundamental contradictions where care logics and control logics coexist uneasily.",,,,,,,,,,,,,,,
299,K6JQ7SVA,Zannone (2023),Intersectional Fairness: A Fractal Approach,,report,2023.0,,Perplexity,"Diese Studie rahmt intersektionale Fairness in einem geometrischen Setting und projiziert Daten auf einen Hyperkubus. Die Autoren beweisen mathematisch, dass Fairness ""nach oben"" propagiert - die Sicherstellung von Fairness für alle Untergruppen auf der niedrigsten intersektionalen Ebene führt notwendigerweise zu Fairness auf allen höheren Ebenen. Sie definieren eine Familie von Metriken zur Erfassung intersektionaler Verzerrung und schlagen vor, Fairness als ""fraktales"" Problem zu betrachten, bei dem Muster auf der kleinsten Skala auf größeren Skalen wiederholt werden. Dieser Bottom-up-Ansatz führt zur natürlichen Entstehung fairer KI.",https://arxiv.org/abs/2302.12683,,,,,,,,,,,,,,
300,UB9NK8KI,Zayed (2024),Scaling implicit bias analysis across transformer-based language models through embedding association test and prompt engineering,,journalArticle,2024.0,,Gemini,"In the evolving field of machine learning, deploying fair and transparent models remains a formidable challenge. This study builds on earlier research, demonstrating that neural architectures exhibit inherent biases by analyzing a broad spectrum of transformer-based language models from base to x-large configurations. This article investigates movie reviews for genre-based bias, which leverages the Word Embedding Association Test (WEAT), revealing that scaling models up tends to mitigate bias, with larger models showing up to a 29% reduction in prejudice. Alternatively, this study also underscores the effectiveness of prompt-based learning, a facet of prompt engineering, as a practical approach to bias mitigation, as this technique reduces genre bias in reviews by more than 37% on average. This suggests that the refinement of development practices should include the strategic use of prompts in shaping model outputs, highlighting the crucial role of ethical AI integration to weave fairness seamlessly into the core functionality of transformer models. Despite the basic nature of the prompts employed in this research, this highlights the possibility of embracing structured prompt engineering to create AI systems that are ethical, equitable, and more responsible for their actions.",https://www.mdpi.com/2076-3417/14/8/3483,,,,,,,,,,,,,,
301,XY2WVKBY,Zeng (2025),"Governing discriminatory content in conversational AI: A cross-system, cross-lingual, and cross-topic audit",10.1080/1369118X.2025.2537803,journalArticle,2025.0,,Gemini,"Conducts mixed-method audit of how major conversational AI systems respond to and regulate discriminatory content. Analysis is cross-system, cross-lingual, and cross-topic, revealing that refusal sensitivity and answering strategies vary significantly across all three axes. Discusses value alignment process through reinforcement learning with human feedback and implementation of guardrails, highlighting tensions when tech platforms become arbiters of morality.",https://doi.org/10.1080/1369118X.2025.2537803,,,,,,,,,,,,,,
302,2MVSL3UV,Zhang (2025),Learning About AI: A Systematic Review of Reviews on AI Literacy,10.1177/07356331251342081,journalArticle,2025.0,en,Manual,"Given the ubiquity of artificial intelligence (AI), it is essential to empower students to become creators, designers, and producers of AI technologies, rather than limiting them to the role of informed consumers. To achieve this, learners need to be equipped with AI knowledge and concepts and develop AI literacy. Paradoxically, it is largely unclear what AI literacy is and how we should learn and teach it. We address both of these questions through a systematic review of systematic reviews, also known as an umbrella review, to gain a comprehensive understanding of AI literacy. After searching the literature, we critically examine the results of 17 reviews focusing on AI literacy and the teaching and learning of AI concepts. Our analysis revealed several encouraging developments: a general consensus on the definition of AI literacy, the availability of teaching tools and materials that support AI learning without prior programming experience, and effective pedagogical approaches that have shown positive effects on students' understanding and engagement. In addition, we identified several areas needing attention in the field: an interdisciplinary pedagogical approach, integration of ethical considerations in AI education, discussions on AI policy, and standardized, content-validated, reliable assessments across educational levels and cultures.",https://journals.sagepub.com/doi/10.1177/07356331251342081,,,,,,,,,,,,,,
303,MT54YHER,Zhao (2025),Thinking like a scientist: Can interactive simulations foster critical AI literacy?,10.1007/978-3-031-98417-4_5,conferencePaper,2025.0,,Claude,"Empirical study with 605 participants demonstrates that interactive simulations enhance critical AI literacy by engaging learners in scientific thinking processes including hypothesis testing and direct observation of AI behavior. Reveals that critical AI literacy requires understanding of fairness, dataset representativeness, and bias mechanisms in language models beyond technical knowledge. Establishes that effective AI literacy education must move beyond static instruction toward experiential engagement that fosters deep conceptual understanding of power structures embedded in AI systems.",,,,,,,,,,,,,,,
