#!/usr/bin/env python3
"""
Generate Static Site from Obsidian Vault
Converts FemPrompt_Vault to HTML for GitHub Pages
"""

import json
import re
from pathlib import Path
from datetime import datetime

# Paths
VAULT_DIR = Path(__file__).parent.parent / "FemPrompt_Vault"
DOCS_DIR = Path(__file__).parent.parent / "docs"
PAPERS_DIR = VAULT_DIR / "Papers"
CONCEPTS_DIR = VAULT_DIR / "Concepts"

def parse_yaml_frontmatter(content):
    """Extract YAML frontmatter from markdown"""
    match = re.match(r'^---\n(.*?)\n---\n', content, re.DOTALL)
    if not match:
        return {}, content

    yaml_content = match.group(1)
    body = content[match.end():]

    metadata = {}
    for line in yaml_content.split('\n'):
        if ':' in line:
            key, value = line.split(':', 1)
            metadata[key.strip()] = value.strip()

    return metadata, body

def convert_obsidian_links(content):
    """Convert [[link]] to HTML links"""
    # Convert [[summary_Paper_Name]] to <a href="../papers/paper_name.html">Paper Name</a>
    content = re.sub(
        r'\[\[summary_(.*?)\]\]',
        lambda m: f'<a href="../papers/{m.group(1).lower()}.html">{m.group(1).replace("_", " ")}</a>',
        content
    )

    # Convert [[Concept_Name]] to <a href="../concepts/concept_name.html">Concept Name</a>
    content = re.sub(
        r'\[\[(.*?)\]\]',
        lambda m: f'<a href="../concepts/{m.group(1).lower().replace(" ", "-")}.html">{m.group(1)}</a>',
        content
    )

    return content

def markdown_to_html(content):
    """Simple markdown to HTML conversion"""
    # Headers
    content = re.sub(r'^### (.*?)$', r'<h3>\1</h3>', content, flags=re.MULTILINE)
    content = re.sub(r'^## (.*?)$', r'<h2>\1</h2>', content, flags=re.MULTILINE)
    content = re.sub(r'^# (.*?)$', r'<h1>\1</h1>', content, flags=re.MULTILINE)

    # Bold
    content = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', content)

    # Italic
    content = re.sub(r'\*(.*?)\*', r'<em>\1</em>', content)

    # Lists
    lines = content.split('\n')
    in_list = False
    result = []
    for line in lines:
        if line.strip().startswith('- '):
            if not in_list:
                result.append('<ul>')
                in_list = True
            result.append(f'<li>{line.strip()[2:]}</li>')
        else:
            if in_list:
                result.append('</ul>')
                in_list = False
            result.append(line)

    if in_list:
        result.append('</ul>')

    content = '\n'.join(result)

    # Paragraphs
    paragraphs = []
    for para in content.split('\n\n'):
        para = para.strip()
        if para and not para.startswith('<'):
            para = f'<p>{para}</p>'
        paragraphs.append(para)

    return '\n'.join(paragraphs)

def generate_html_template(title, content, breadcrumb=""):
    """Generate full HTML page"""
    return f'''<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>{title} | Social AI Literature Vault</title>
  <link rel="stylesheet" href="../css/style.css">
</head>
<body>
  <header class="header">
    <div class="header-content">
      <div class="header-title">
        <a href="../index.html">Social AI Literature Vault</a>
      </div>
      <nav class="nav">
        <a href="../papers/index.html">Papers</a>
        <a href="../concepts/index.html">Concepts</a>
        <a href="../graph.html">Graph</a>
        <a href="../search.html">Search</a>
      </nav>
    </div>
  </header>

  <main class="main">
    {breadcrumb}
    {content}
  </main>

  <footer class="footer">
    <p>
      Social AI Literature Vault &copy; 2025 |
      Generated by automated research pipeline
    </p>
  </footer>
</body>
</html>'''

def generate_papers():
    """Generate HTML pages for all papers"""
    print("Generating paper pages...")
    papers_output = DOCS_DIR / "papers"
    papers_output.mkdir(exist_ok=True)

    papers = []

    for paper_file in PAPERS_DIR.glob("*.md"):
        content = paper_file.read_text(encoding='utf-8')
        metadata, body = parse_yaml_frontmatter(content)

        # Convert to HTML
        html_body = convert_obsidian_links(body)
        html_body = markdown_to_html(html_body)

        # Extract title
        title = metadata.get('title', paper_file.stem.replace('summary_', '').replace('_', ' '))

        # Generate page
        breadcrumb = '<div class="text-muted" style="margin-bottom: 1rem;"><a href="../index.html">Home</a> / <a href="index.html">Papers</a> / ' + title + '</div>'

        paper_html = f'''
        <article class="paper-header">
          <h1>{title}</h1>
          <div class="paper-metadata">
            <span class="paper-metadata-item">Published: {metadata.get('date', 'Unknown')}</span>
            <span class="paper-metadata-item">Authors: {metadata.get('authors', 'Unknown')}</span>
          </div>
        </article>
        <div class="paper-content">
          {html_body}
        </div>
        '''

        full_html = generate_html_template(title, paper_html, breadcrumb)

        output_file = papers_output / f"{paper_file.stem.lower()}.html"
        output_file.write_text(full_html, encoding='utf-8')

        papers.append({
            'title': title,
            'filename': output_file.name,
            'date': metadata.get('date', ''),
            'authors': metadata.get('authors', '')
        })

    # Generate papers index
    papers_list_html = '<ul class="list">'
    for paper in sorted(papers, key=lambda x: x['title']):
        papers_list_html += f'''
        <li class="list-item">
          <a href="{paper['filename']}">
            <div class="list-item-title">{paper['title']}</div>
            <div class="list-item-meta">{paper['authors']} • {paper['date']}</div>
          </a>
        </li>
        '''
    papers_list_html += '</ul>'

    index_html = f'''
    <h1>Research Papers</h1>
    <p class="text-muted">{len(papers)} papers in the knowledge vault</p>
    {papers_list_html}
    '''

    full_index = generate_html_template('Papers', index_html)
    (papers_output / "index.html").write_text(full_index, encoding='utf-8')

    print(f"Generated {len(papers)} paper pages")
    return papers

def generate_concepts():
    """Generate HTML pages for all concepts"""
    print("Generating concept pages...")
    concepts_output = DOCS_DIR / "concepts"
    concepts_output.mkdir(exist_ok=True)

    concepts = []

    for category_dir in CONCEPTS_DIR.iterdir():
        if not category_dir.is_dir():
            continue

        category = category_dir.name
        category_output = concepts_output / category.lower().replace('_', '-')
        category_output.mkdir(exist_ok=True)

        for concept_file in category_dir.glob("*.md"):
            content = concept_file.read_text(encoding='utf-8')
            metadata, body = parse_yaml_frontmatter(content)

            # Convert to HTML
            html_body = convert_obsidian_links(body)
            html_body = markdown_to_html(html_body)

            title = metadata.get('title', concept_file.stem.replace('_', ' '))
            frequency = metadata.get('frequency', '0')
            papers_count = metadata.get('papers', '0')

            # Generate page
            breadcrumb = f'<div class="text-muted" style="margin-bottom: 1rem;"><a href="../../index.html">Home</a> / <a href="../index.html">Concepts</a> / <a href="index.html">{category}</a> / {title}</div>'

            concept_html = f'''
            <article class="concept-header">
              <span class="badge badge-{category.lower().replace("_", "-")}">{category.replace("_", " ")}</span>
              <h1>{title}</h1>
              <div class="concept-frequency">{frequency}</div>
              <div class="concept-frequency-label">mentions across {papers_count} papers</div>
            </article>
            <div class="paper-content">
              {html_body}
            </div>
            '''

            full_html = generate_html_template(title, concept_html, breadcrumb)

            output_file = category_output / f"{concept_file.stem.lower().replace(' ', '-')}.html"
            output_file.write_text(full_html, encoding='utf-8')

            concepts.append({
                'title': title,
                'category': category,
                'frequency': int(frequency) if frequency.isdigit() else 0,
                'papers': int(papers_count) if str(papers_count).isdigit() else 0,
                'filename': f"{category.lower().replace('_', '-')}/{output_file.name}"
            })

    # Generate concepts index
    concepts_list_html = '<h2>Bias Types</h2><ul class="list">'
    for concept in sorted([c for c in concepts if 'Bias' in c['category']], key=lambda x: -x['frequency']):
        concepts_list_html += f'''
        <li class="list-item">
          <a href="{concept['filename']}">
            <div class="list-item-title">{concept['title']}</div>
            <div class="list-item-meta">{concept['frequency']} mentions • {concept['papers']} papers</div>
          </a>
        </li>
        '''
    concepts_list_html += '</ul>'

    concepts_list_html += '<h2 class="mt-lg">Mitigation Strategies</h2><ul class="list">'
    for concept in sorted([c for c in concepts if 'Mitigation' in c['category']], key=lambda x: -x['frequency']):
        concepts_list_html += f'''
        <li class="list-item">
          <a href="{concept['filename']}">
            <div class="list-item-title">{concept['title']}</div>
            <div class="list-item-meta">{concept['frequency']} mentions • {concept['papers']} papers</div>
          </a>
        </li>
        '''
    concepts_list_html += '</ul>'

    index_html = f'''
    <h1>Key Concepts</h1>
    <p class="text-muted">{len(concepts)} concepts extracted from research papers</p>
    {concepts_list_html}
    '''

    full_index = generate_html_template('Concepts', index_html)
    (concepts_output / "index.html").write_text(full_index, encoding='utf-8')

    print(f"Generated {len(concepts)} concept pages")
    return concepts

def generate_search_index(papers, concepts):
    """Generate JSON search index"""
    print("Generating search index...")

    search_data = {
        'papers': papers,
        'concepts': concepts
    }

    data_dir = DOCS_DIR / "data"
    data_dir.mkdir(exist_ok=True)

    with open(data_dir / "search-index.json", 'w', encoding='utf-8') as f:
        json.dump(search_data, f, indent=2, ensure_ascii=False)

    # Generate stats
    stats = {
        'papers': len(papers),
        'concepts': len(concepts),
        'connections': sum(c['papers'] for c in concepts),
        'generated': datetime.now().isoformat()
    }

    with open(data_dir / "stats.json", 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2)

    print("Search index and stats generated")

def main():
    print("="*60)
    print("Generating Static Site from Obsidian Vault")
    print("="*60)

    if not VAULT_DIR.exists():
        print(f"Error: Vault directory not found at {VAULT_DIR}")
        return

    papers = generate_papers()
    concepts = generate_concepts()
    generate_search_index(papers, concepts)

    print("="*60)
    print("Static site generation complete!")
    print(f"Output: {DOCS_DIR}")
    print(f"Papers: {len(papers)}")
    print(f"Concepts: {len(concepts)}")
    print("="*60)

if __name__ == "__main__":
    main()
