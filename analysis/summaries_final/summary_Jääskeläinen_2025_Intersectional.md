---
title: "Jääskeläinen 2025 Intersectional"
original_document: Jääskeläinen_2025_Intersectional.md
document_type: Empirical Study
research_domain: AI Ethics, AI Bias & Fairness, Generative AI
methodology: Qualitative, Case Study, Visual Analysis
keywords: Stable Diffusion, intersectionality, visual bias, algorithmic reparation, generative AI
mini_abstract: "This paper critically analyzes Stable Diffusion through intersectional visual analysis of 180 generated images, demonstrating how the technology perpetuates systemic inequalities including racism, sexism, and ableism rather than producing culturally neutral outputs. The authors propose reparative and social justice-oriented approaches to address embedded power systems in visual generative AI."
target_audience: Researchers, Policymakers, Industry, Practitioners
key_contributions: "Intersectional visual analysis framework for generative AI bias assessment"
geographic_focus: Global
publication_year: 2025
related_fields: Feminist Science and Technology Studies, Visual Media Studies, Critical Theory
summary_date: 2025-10-31
language: English
ai_model: claude-haiku-4-5
---

# Summary: Jääskeläinen 2025 Intersectional

## Overview

This paper by Jääskeläinen, Sharma, Pallett, and Åsberg presents a critical examination of Stable Diffusion (SD), a widely-adopted open-source visual generative AI tool launched in August 2022 that has generated over 12 billion images with 10 million daily active users. The research fundamentally challenges the prevailing assumption that vGenAI technologies operate as culturally neutral tools, instead arguing that these systems actively encode, reflect, and perpetuate existing societal power structures and inequalities. By analyzing 180 deliberately-prompted SD-generated images through an intersectional lens, the authors provide empirical evidence that generative AI is not neutral but functions as an active agent reproducing harmful visual politics that systematically advantage dominant groups while marginalizing others.

## Main Findings

The analysis reveals three critical and interconnected findings. First, SD-generated imagery perpetuates systemic inequalities including sexism, racism, heteronormativity, and ableism through consistent aesthetic choices and representational patterns—demonstrating that the technology actively reproduces rather than merely reflects pre-existing biases. Second, SD exhibits a persistent "default individual" assumption, with images predominantly featuring white, able-bodied, masculine-presenting subjects, indicating embedded cultural hierarchies within training data and design architecture. Third, outputs demonstrate pronounced Euro- and North America-centric cultural representations, directly traceable to the institutional origins, development contexts, and training datasets of these tools. Critically, the authors document how SD produces harmful and violent imagery targeting marginalized groups, establishing that vGenAI cannot achieve cultural or aesthetic neutrality without deliberate intervention in training data, institutional practices, and design choices.

## Methodology/Approach

The research employs qualitative, interpretative visual analysis examining 180 SD-generated images deliberately prompted across multiple intersecting axes of privilege and disadvantage—including wealth/poverty distinctions, citizen/immigrant statuses, and other social positioning variables. This systematic prompting strategy enables investigation of how the technology differentially represents social groups. The theoretical framework integrates three complementary disciplinary perspectives: Feminist Science and Technology Studies (examining power dynamics in technological development and design), visual media studies (analyzing aesthetic and representational politics), and intersectional critical theory (understanding how multiple systems of oppression simultaneously operate and reinforce one another). This interdisciplinary integration moves beyond isolated bias identification toward comprehensive understanding of intersecting power systems within visual outputs.

## Relevant Concepts

**Visual Generative AI (vGenAI)**: AI systems trained on large image datasets that generate novel photorealistic images from text prompts, including Stable Diffusion, DALL-E, and Midjourney.

**Intersectionality**: Analytical framework examining how multiple systems of oppression (race, gender, class, ability, citizenship) simultaneously operate and interact within social structures and technologies.

**Algorithmic bias**: Systematic prejudices embedded within AI systems through training data selection, design choices, and institutional contexts that disadvantage specific social groups.

**Cultural-aesthetic politics**: Ideological dimensions embedded within aesthetic choices and visual representations that reflect and reinforce particular worldviews, hierarchies, and power arrangements.

**Algorithmic reparation**: Restorative justice approach acknowledging harms caused by biased AI systems and implementing material and symbolic interventions to mend injustices against affected social groups.

**Default individual assumption**: Implicit design assumption that treats particular demographic characteristics (whiteness, able-bodiedness, masculinity) as unmarked or universal, rendering other identities as marked or exceptional.

## Significance

This paper makes substantial contributions to critical AI scholarship by centering visual and intersectional analysis—dimensions frequently overlooked in technical AI audits and fairness frameworks. It bridges humanities-centered critique with technology studies, offering essential counterweight to techno-optimist narratives claiming AI neutrality. The work advances beyond problem identification toward constructive solutions through its explicit reparative justice framework, representing an emerging normative turn in critical AI studies. The research has immediate relevance for policymakers, technologists, and researchers developing generative AI systems, providing empirical evidence that algorithmic neutrality is impossible without deliberate interventions. Most significantly, by demonstrating how vGenAI actively reproduces harmful visual politics rather than passively reflecting them, the paper establishes urgent grounds for implementing algorithmic reparation strategies, restorative justice approaches, and institutional reforms in AI development and deployment. The authors' interdisciplinary positioning—spanning media technology, environmental sciences, data science, and gender studies—strengthens the credibility of their humanities-centered critique within technology discourse.
