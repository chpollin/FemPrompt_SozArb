---
source_file: Barman_2024_Beyond.pdf
conversion_date: 2025-11-07T10:37:51.773958
---

<!-- image -->

DPC/G13.1is provided by the OCIO Policy Lead after approvals]

ACROSS GOVERNMENT GUIDELINE

## Guideline for the use of Generative Artificial Intelligence and Large Language Model Tools

## Purpose

This guideline covers the limitations and risks associated with the use of generative artificial intelligence (AI) and Large Language Model (LLM) tools within the South Australian (SA) Government, and outlines guidance to assist SA Government agencies (agencies) when considering the safe usage of this technology in their agency. It also provides some practical do's and don'ts in the interim as long-term implications are considered.

## Scope

This guideline applies to:

- All SA Government agencies and personnel operating on behalf of the agencies.
- Non-government suppliers and personnel that access SA Government information and resources.

## Guidance detail

Generative AI and LLM tools, such as OpenAI's ChatGPT and Google Bard, have highlighted the need to consider the safe use of such technologies within the SA Government context. The potential benefits of these tools need to be balanced against the limitations of the products and the risks of their use.

LLMs, a type of generative AI designed to generative text-based content, are first trained on associations within texts to create content similar to its training data. A key difference between traditional AI and generative AI is that the output of generative AI (including LLMs) is not designed to be 'accurate.' It is designed to be similar to content the model has 'seen' before, which can often be true to the original information or can be entirely different (and not factual).

<!-- image -->

## Benefits

Generative AI and LLM tools have the potential to automate a wide variety of tasks that may take a human days or weeks to perform, freeing up employees to focus on higher value work and to lift productivity.

The capabilities of these technologies can be applied to nearly all aspects of business. For instance, LLM tools can generate reports, create content marketing materials such as email campaigns and social media posts, create and debug code, create presentations, generate ideas, and help with brainstorming.

## Risk Management

While the potential of AI applications is extensive and growing, the technology is rapidly advancing and changing. Like every technology or tool, SA Government employees must remain responsible for their work and continue to meet all their obligations including those relating to record keeping, privacy, confidentiality, and integrity, ensuring the security of public sector data and responsible handling of personal information.

Prior to authorising the use of these tools, SA Government agencies should conduct their own risk assessments, balancing potential benefits and risk, while being aware that at this stage there are unknown and unknowable risks 1 .

The adoption and use of generative AI and LLM tools may magnify existing information, cyber security and privacy risks and create new ones. SA Government agencies should assess the maturity of their existing security and privacy programs prior to implementation.

## Limitations and risks

There are several major limitations inherent with the use of generative AI and LLM tools, which must be considered and mitigated for acceptable SA Government use.

Refer to the table below for guideline details.

1  Forrester: Build your own business case for Microsoft 365 Copilot

## OFFICIAL

<!-- image -->

## Limitations and Risks

## General

While generative AI and LLM tools are still evolving, comprehensive guidance is required to ensure appropriate use within agencies.

## Information confidentiality

Inputs into consumer oriented and open-source generative AI and LLM tools should automatically be considered as available in the public domain.

Employees can easily expose sensitive and proprietary government data in the questions and prompts they provide to generative AI and LLM tools. Any data provided into these tools are potentially stored indefinitely, are likely stored outside of Australia, and may not be stored securely.

Data provided may be used to train third-party models in the future or used to generate responses to external users. Additionally, information provided to these technologies can potentially be viewed by service provider employees and subcontractors, future users of the system and hackers.

## Data integrity and accuracy

LLM AI tools are trained using immense data sets of internet-sourced data, including text scraped from unverified sources such as X, Wikipedia and Reddit.

Subpar training data can lead to responses that are insufficient, obsolete or contain sensitive information and biases, leading to biased, prohibited, or incorrect responses.

## OFFICIAL

## Mitigating Controls

- Consider developing an agency strategy for the safe adoption of generative AI and LLM tools.
- Establish an AI governance and compliance framework for agency use.
- Consider the National framework for the assurance of artificial intelligence in government and ISO/IEC 42001:2023 Information Technology-Artificial Intelligence-Management Standard.
- Properly identify and document the risks associated with the use of AI technologies.
- Conduct awareness campaigns for the safe and ethical use of AI tools.
- Define standard business use cases based on business needs and security risks.
- Instruct employees via clearly understood and widely disseminated policies to not use prompts that expose official government or personal information. For example, such policies may include:
- Disallow any cut-and-paste of enterprise content, such as emails, reports and chat logs into prompts.
- Disallow any inputs that include customer data.
- Remind staff of their obligations under the Code of Ethics for the SA Public Sector and the SA Government Information Privacy Principles.
- Consider a Data Loss Prevention (DLP) solution to monitor or control the egress information flows which contain sensitive information. Establish a self-hosted AI tenancy that is segregated from public networks and users (refer to Commercial Generative AI and LLM Services below).
- Enforce by policy a manual review model to spot incorrect or misinformed results.
- Manually review all model output and use it only as a first draft tool. Inaccurate or biased outputs could lead to liability, reputation loss or harm.

<!-- image -->

## OFFICIAL

One of the most widespread and hard to spot problems in the AI context are 'hallucinations' - these are confident responses by AI that are not justified by its training data, where it makes up the answer and reports it as fact.

- Use in a controlled setting where generated text can be properly evaluated and tuned.

It has been shown that AI not only hallucinates, but also can amplify hallucinations.

Generally, manufacturers of consumer oriented and open-source AI tools ensure that their terms of use make no warranties as to the accuracy or quality of the outputs, and the data it draws upon may not be current.

Agencies must remain accountable for the accuracy of all sources, including AI generated outputs, and should research and verify sources as required before they are used in their work.

Agencies should also ensure that any attribution obligations are met, with some AI tool manufacturers requiring users attribute the role it has played in drafting or editing content.

## Legal and privacy

Copyright:

Users should be aware of the potential copyright violations with the use of generative AI  and  LLM  tools.  Terms  of  use  usually  put  the  responsibility  to  identify  potential copyright violations in the output on the user. In some instances, manufacturers have also been accused of using copyrighted data for training models.

Legal Risks:

Given the new and evolving nature of the AI technology, agencies should be mindful of broader legal risks, including in relation to any terms of use that apply. For example, it is usual for the terms of use to require users to indemnify the manufacturer for all losses resulting from misuse.

Privacy:

The privacy policies and terms of use of these technologies are unlikely to meet the requirements of the SA Government to sufficiently protect personal or government information.

## Ethics and fairness

Relying on vast amounts of algorithms and data carries the inherent risk of bias or discriminatory content in outputs, and the algorithms used to generate outputs are largely unknown to general users.

- Implement standard processes for configuration and change control among business operations and systems.
- If used for developing or checking code, ensure that secure software development principles are adhered to.
- Remain responsible and accountable for your work, including to critically assess all AI generated outputs and validate quality and accuracy with other sources.
- Thoroughly understand terms of use and seek legal advice as necessary.
- Consider fine tuning models using specific internal data sources.
- Consider and monitor non-government suppliers use of AI when used in the provision of services of government. Seek assurance on their generative AI and LLM output validation processes.
- Seek legal advice and conduct a compliance analysis and document the applicable regulatory and contractual requirements.
- Review the privacy policy / terms of use issued by the AI service provider regularly.
- Conduct privacy impact analyses on the AI business use cases.
- Establish or update the agency's privacy policies to accommodate the use of AI tools to meet applicable obligations - including in relation to integrity, privacy, security, human rights, anti-discrimination, administrative and other laws.
- Remind staff of their obligations under the Code of Ethics for the SA Public Sector and the SA Government Information Privacy Principles.

Consider commercial products that include intellectual property indemnification coverage to copyright claims relating to the output of AI-powered products (for example, Microsoft commercial Copilot services, Adobe's Firefly and Google's Vertex and Duet AI systems).

- Regularly monitor and review use of AI tools to ensure they are being used responsibly ethically, safely, and according to law and government policy. Establish an incident reporting channel and investigation procedure to identify and handle the misuse of AI tools.

<!-- image -->

## OFFICIAL

The risk is particularly high where outputs may inform government decision making that have the potential to harm or impact adversely on individuals, and in the case of some manufacturers, the terms of use note that use in government context carries a greater risk of potential harm.AI systems have the potential to be expressions of cultural and social frameworks, representing the socially dominant concepts and normative ideas of the system designers. This can lead to inbuilt and amplified biases that could be particularly damaging for Aboriginal and Torres Strait Islander peoples and minority groups.

- Where applicable, consider the use of generative AI and LLM tools through the lens of specific cultures, Aboriginal and Torres Strait Islander peoples and minority groups. Be wary of inherent biases that may be inbuilt and develop strategies to counter them. Where generative AI and LLM tools are used in circumstances that may impact Aboriginal and Torres Strait Islander peoples, agencies must consider the risk of negative bias in that instance and consult with Aboriginal and Torres Strait Islander peoples. Be vigilant and report on the identified events that violate the ethics principles.

Data used for AI technology development and training could be insufficient, obsolete or contain sensitive information and biases, leading to biased, prohibited, or incorrect responses. Given these AI tools can also adopt different personas through text, there is also a potential risk of fraud or misuse:

## Deepfakes:

These  outputs  generated  by  AI  could  appear  realistic  but  be  fake  content.  SA Government  employees  must  be  vigilant  to  identify  fake  news,  misinformation, impersonations, or efforts to manipulate public opinion.

## Fraud and abuse:

Malicious actors may use AI chatbots for writing fake reviews, spamming, phishing, and social engineering. Moreover, researchers have found many ways in which some platforms can aid in the development of malware.

## Commercial Generative AI and LLM Services

To reduce the information confidentiality risk associated with the use of consumer oriented and publicly available open-sourced generative AI and LLM tools or 'bring your own AI (BYOAI), agencies should consider commercial generative AI and LLM products or privately hosted open-source tools, particularly where there are strict requirements that data must remain inside the M365 tenant or geographic boundaries .

All the other limitations and risks listed in these guidelines apply equally to commercial generative AI and LLM AI Services.

- Consider the potential exploitation and attacks of the use of AI tools from external and internal malicious actors.
- Continually monitor the security practices of the commercial generative AI and LLM services.
- Create information classification policies for the commercial generative AI and LLM services.
- Consider the data sovereignty risks:
- Create limited use cases for out of region access.
- Ensure the general, data integrity and accuracy, legal and privacy, and ethics and fairness risks outlined in this guideline are properly considered.
- Depending on the use case, consider commercial generative AI and LLM products that offer sufficient protection for government information based on risk:
- Establish a self-hosted AI tenancy that is segregated from public networks and users.

<!-- image -->

## OFFICIAL

|                                                                                                                                                                                            | • Establish commercial generative AI and LLM services with enterprise security considerations as an alternative to consumer oriented generative AI and LLM services. Consider potential data leakage risks. • Assess the maturity of the agency information security program prior to implementation.                                                                                              |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Service Availability                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                    |
| Agencies should identify the availability requirements of their AI services based on defined use cases and assess the impact to business operations should the service become unavailable. | • Conduct business impact analysis (BIA) on the AI services and document in business continuity plans (BCPs). • Standardise the procedures of using the generative AI and LLM tools and implement alternative methods and workaround for service interruptions. • Where appropriate, use generative AI and LLM tools as one output to your work (i.e., to supplement and accelerate, not replace). |

<!-- image -->

## Dos and Don'ts

SA Government staff must not input any Personal Information or any official government information that is not suitable for public consumption into consumer oriented and open-source AI tools 2 .

| DO                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | DON'T                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Assume that inputs into online generative AI and LLM tools will automatically be considered as available in the public domain. Ensure you consult with your legal team before use to ensure you can comply with any terms of use which may change at any time. Remain responsible and accountable for your work, including critically assessing all AI generated outputs and validating quality and accuracy with other sources. Thoroughly research and source reliable references for content where necessary. Meet all your existing employment obligations- including in relation to handling official information, privacy, security, human rights, anti-discrimination, administrative and other laws. Where required, attribute content that has resulted from the use of these tools. Regularly monitor and review uses of these tools to ensure they are being used responsibly ethically, safely, and according to law and government policy. | Enter any information (questions or prompts) that shouldn't be in the public domain, including any information that is confidential. Enter any personal information about any person. Enter any health information about any person. Enter inputs or use outputs in a way that may breach another person's intellectual property rights. Use these tools for any query that is complex or sensitive, or where local context and nuance is critical. Ask these tools to answer a question you don't already know the answer to or cannot validate the answer to - you need the knowledge to decide whether it can be trusted or contains bias. Use these tools to replace your own research, analysis and content development or embed them into your work in a way that means it cannot be done without them. 'Copy and paste' sections of AI-generated content into your work. If you do copy and paste any AI generated content, consider what your obligations are in relation to attribution and intellectual property. |

2  Contact your agency ICT Service Desk and IT Security Advisor (ITSA) for advice regarding the suitability of AI tools and utilities for your intended use.

<!-- image -->

## OFFICIAL

## Aboriginal Impact Statement

The needs and interests of Aboriginal and Torres Strait Islander peoples have been considered in the development of this guideline. As this guideline presents general guidance only, where generative AI and LLM tools are used in circumstances that may impact Aboriginal and Torres Strait Islander peoples, agencies must assess and consider the risk of negative bias and consult with Aboriginal and Torres Strait Islander peoples.

## Related documents

- Australia's AI Ethics Principles
- National framework for the assurance of artificial intelligence in government
- SA Government Information Privacy Principles
- Code of Ethics for the SA Public Sector
- South Australian Cyber Security Framework
- South Australian Protective Security Framework
- PC 030 - Protective Security in the Government of South Australia
- Protective Security Policy Framework
- Deploying AI Systems Securely | Cyber.gov.au

## DOCUMENT CONTROL

| Approved by: CIO Steering Committee Contact: Office of the Chief Information   | Email: CIOAdministrator@sa.gov.au      |
|--------------------------------------------------------------------------------|----------------------------------------|
| Review number: v1.3                                                            | Compliance: Optional                   |
| Next review date: 14 August 2026                                               | Date of original approval: 31 May 2023 |
|                                                                                | Date of last approval: 14 August 2024  |

## Licence

<!-- image -->

With the exception of the Government of South Australia brand, logos and any images, this work is

With the exception of the Government of South Australia brand, logos and any images, this work is licensed under a Creative Commons Attribution (CC BY) 4.0 Licence. To attribute this material, cite the Office of the Chief Information Officer, Department of the Treasury and Finance, Government of South Australia, 2024.

<!-- image -->

## OFFICIAL