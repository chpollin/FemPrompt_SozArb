[
  {
    "key": "MIT8HTC6",
    "itemType": "conferencePaper",
    "title": "PreciseDebias: An automatic prompt engineering approach for generative AI to mitigate image demographic biases",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "C.",
        "lastName": "Clemmer"
      },
      {
        "creatorType": "author",
        "firstName": "J.",
        "lastName": "Ding"
      },
      {
        "creatorType": "author",
        "firstName": "Y.",
        "lastName": "Feng"
      }
    ],
    "date": "2024",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://openaccess.thecvf.com/content/WACV2024/html/Clemmer_PreciseDebias_An_Automatic_Prompt_Engineering_Approach_for_Generative_AI_WACV_2024_paper.html",
    "abstractNote": "This paper presents a technical solution for reducing demographic bias in AI image generators through \"PreciseDebias,\" an automated approach that rewrites simple prompts into more complex, diversity-reflective prompts. The system analyzes model bias and strategically adds attributes like ethnicity, gender, or age to enforce fairer representation distributions, demonstrating that diversity-reflective prompting can be automated at the system level.",
    "tags": [],
    "dateAdded": "2025-07-31T07:31:42Z",
    "dateModified": "2025-07-31T07:31:42Z"
  },
  {
    "key": "YN9JAREI",
    "itemType": "report",
    "title": "Faires KI-Prompting – Ein Leitfaden für Unternehmen",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "E.",
        "lastName": "Gengler"
      },
      {
        "creatorType": "author",
        "firstName": "A.",
        "lastName": "Kraus"
      },
      {
        "creatorType": "author",
        "firstName": "K.",
        "lastName": "Bodrožić-Brnić"
      }
    ],
    "date": "2024",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://www.digitalzentrum-zukunftskultur.de/wp-content/uploads/2024/05/Faires-KI-Prompting-Ein-Leitfaden-fuer-Unternehmen.pdf",
    "abstractNote": "This practical guide argues that feminist AI competency results in conscious prompt design to actively reduce stereotypes and discrimination in AI-generated content. It presents the \"KI-FAIRNESS\" framework for diversity-reflective prompting and demonstrates how specific, context-rich prompts lead to fairer and more representative results by compensating for AI's \"blind spots\" through targeted instructions.",
    "tags": [],
    "dateAdded": "2025-07-31T07:31:42Z",
    "dateModified": "2025-07-31T07:31:42Z"
  },
  {
    "key": "UJ7DXK8Y",
    "itemType": "conferencePaper",
    "title": "NLPositionality: Characterizing design biases of datasets and models",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "S.",
        "lastName": "Santy"
      },
      {
        "creatorType": "author",
        "firstName": "A.",
        "lastName": "O'Connor"
      },
      {
        "creatorType": "author",
        "firstName": "E.",
        "lastName": "Shi"
      },
      {
        "creatorType": "author",
        "firstName": "A.",
        "lastName": "Wang"
      },
      {
        "creatorType": "author",
        "firstName": "J.",
        "lastName": "Dai"
      },
      {
        "creatorType": "author",
        "firstName": "D.",
        "lastName": "Klein"
      }
    ],
    "date": "2023",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://aclanthology.org/2023.acl-long.530/",
    "abstractNote": "This study provides a theoretical and methodological framework for making bias visible in AI models and datasets. It introduces the concept of \"positionality\" - the assumption that developers' social, cultural, and political perspectives inevitably flow into AI artifacts. The authors develop a method to quantify design biases by analyzing which dialects, demographic groups, or social contexts are under- or over-represented in datasets.",
    "tags": [],
    "dateAdded": "2025-07-31T07:31:42Z",
    "dateModified": "2025-07-31T07:31:42Z"
  },
  {
    "key": "MJDTRLAI",
    "itemType": "report",
    "title": "Can prompt modifiers control bias? A comparative analysis of text-to-image generative models",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "P. W.",
        "lastName": "Shin"
      },
      {
        "creatorType": "author",
        "firstName": "J. J.",
        "lastName": "Ahn"
      },
      {
        "creatorType": "author",
        "firstName": "W.",
        "lastName": "Yin"
      },
      {
        "creatorType": "author",
        "firstName": "J.",
        "lastName": "Sampson"
      },
      {
        "creatorType": "author",
        "firstName": "V.",
        "lastName": "Narayanan"
      }
    ],
    "date": "2024",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://arxiv.org/abs/2406.05602",
    "abstractNote": "This preprint investigates whether explicit prompt modifiers can reduce societal biases in text-to-image generative AI models. Authors evaluated three models (Stable Diffusion, DALL·E 3, Adobe Firefly) comparing baseline versus bias-mitigating prompts. Analysis revealed notable biases across all models, with inconsistent effectiveness of prompt modifiers. While diversity-reflective prompting can expose hidden biases and sometimes nudge outputs towards inclusivity, it is not a comprehensive fix and must be combined with broader ethical AI development efforts.",
    "tags": [],
    "dateAdded": "2025-07-31T07:01:43Z",
    "dateModified": "2025-07-31T07:01:43Z"
  },
  {
    "key": "6L85PRUW",
    "itemType": "conferencePaper",
    "title": "Inclusive prompt engineering: A methodology for hacking biased AI image generation",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "R.",
        "lastName": "Skilton"
      },
      {
        "creatorType": "author",
        "firstName": "A.",
        "lastName": "Cardinal"
      }
    ],
    "date": "2024",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "10.1145/3641237.3691655",
    "url": "https://www.researchgate.net/publication/385325948_Inclusive_Prompt_Engineering_A_Methodology_for_Hacking_Biased_AI_Image_Generation",
    "abstractNote": "This conference paper introduces \"inclusive prompt engineering\" as a strategy to probe and mitigate biases in generative AI image systems. Authors developed methodology to systematically modify prompts and provide tools for generating more diverse outputs. User studies revealed that when participants encountered stereotypical outputs, they tried adding negative qualifiers but models often failed to obey these negations. Findings underscore need for improved prompt interfaces that actively promote inclusive representation.",
    "tags": [],
    "dateAdded": "2025-07-31T07:01:43Z",
    "dateModified": "2025-07-31T07:01:43Z"
  },
  {
    "key": "YLAKP7Z2",
    "itemType": "journalArticle",
    "title": "Intersectional analysis of visual generative AI: The case of Stable Diffusion",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "P.",
        "lastName": "Jääskeläinen"
      },
      {
        "creatorType": "author",
        "firstName": "N. K.",
        "lastName": "Sharma"
      },
      {
        "creatorType": "author",
        "firstName": "H.",
        "lastName": "Pallett"
      },
      {
        "creatorType": "author",
        "firstName": "C.",
        "lastName": "Åsberg"
      }
    ],
    "date": "2025",
    "publicationTitle": "AI & Society",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "10.1007/s00146-025-02207-y",
    "url": "https://link.springer.com/article/10.1007/s00146-025-02207-y",
    "abstractNote": "This open-access paper provides a feminist intersectional critique of Stable Diffusion through qualitative visual analysis of 180 AI-generated images. Authors examined how power systems like racism, sexism, heteronormativity, ableism, colonialism, and capitalism are reflected in AI outputs. Found that default outputs frequently perpetuate harmful stereotypes and assume a \"white, able-bodied, masculine-presenting\" default subject position. Advocates for social justice-oriented approach to AI by acknowledging cultural-aesthetic biases and engaging in reparative strategies.",
    "tags": [],
    "dateAdded": "2025-07-31T07:01:43Z",
    "dateModified": "2025-07-31T07:01:43Z"
  },
  {
    "key": "8WBUGXRR",
    "itemType": "conferencePaper",
    "title": "Reflexive prompt engineering: A framework for responsible prompt engineering and AI interaction design",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "C.",
        "lastName": "Djeffal"
      }
    ],
    "date": "2025",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "10.1145/3715275.3732118",
    "url": "",
    "abstractNote": "This paper proposes \"Reflexive Prompt Engineering\" as a comprehensive framework to embed ethical and inclusive principles into prompt crafting for generative AI. Framework consists of five components: prompt design, system selection, system configuration, performance evaluation, and prompt management, each considered from social responsibility perspective. Positions responsible prompt engineering as essential component of AI literacy, bridging gap between AI development and deployment by aligning AI behavior with human rights and diversity values.",
    "tags": [],
    "dateAdded": "2025-07-31T07:01:43Z",
    "dateModified": "2025-07-31T07:01:43Z"
  },
  {
    "key": "IDW7QSYG",
    "itemType": "journalArticle",
    "title": "Female perspectives on algorithmic bias: Implications for AI researchers and practitioners",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "B.",
        "lastName": "Fraile-Rojas"
      },
      {
        "creatorType": "author",
        "firstName": "C.",
        "lastName": "De-Pablos-Heredero"
      },
      {
        "creatorType": "author",
        "firstName": "M.",
        "lastName": "Méndez-Suárez"
      }
    ],
    "date": "2025",
    "publicationTitle": "Management Decision",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "10.1108/MD-04-2024-0884",
    "url": "https://colab.ws/articles/10.1108%2Fmd-04-2024-0884",
    "abstractNote": "This study uses NLP and machine learning to analyze 172,041 tweets from female users discussing gender inequality in AI. It identifies prominent themes including the future of AI technologies and women's active role in ensuring gender-balanced systems. Findings show that algorithmic bias directly affects women's experiences, prompting engagement in online discourse about injustices. Women lead constructive conversations and create entrepreneurial solutions when faced with bias, demonstrating how feminist digital literacies can make AI biases visible and push for their reduction.",
    "tags": [],
    "dateAdded": "2025-07-31T07:01:43Z",
    "dateModified": "2025-07-31T07:01:43Z"
  },
  {
    "key": "AIZGTQKG",
    "itemType": "journalArticle",
    "title": "Gender bias in artificial intelligence: Empowering women through digital literacy",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "S. S.",
        "lastName": "Shah"
      }
    ],
    "date": "2025",
    "publicationTitle": "Premier Journal of Artificial Intelligence",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "10.70389/PJAI.1000088",
    "url": "https://premierscience.com/pjai-24-524/",
    "abstractNote": "This narrative review examines how systemic gender biases are embedded in AI systems across domains (e.g. hiring, healthcare, finance) and explores digital literacy as a tool to combat these biases. Key findings indicate that biases arise from underrepresentation of women in AI development, biased training data, and algorithmic design choices. Digital literacy programs for women are highlighted as a promising intervention that raises critical awareness of AI bias, encourages women's participation in AI careers, and fosters women-led AI projects.",
    "tags": [],
    "dateAdded": "2025-07-31T07:01:43Z",
    "dateModified": "2025-07-31T07:01:43Z"
  },
  {
    "key": "X7ZGW6CN",
    "itemType": "journalArticle",
    "title": "Intersectional Artificial Intelligence Is Essential: Polyvocal, Multimodal, Experimental Methods to Save AI",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "S.",
        "lastName": "Ciston"
      }
    ],
    "date": "2024",
    "publicationTitle": "Journal of Science and Technology of the Arts",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "10.7559/CITARJ.V11I2.665",
    "url": "",
    "abstractNote": "Arguments for applying intersectional strategies to AI at all levels - from data through design to implementation. Intersectionality, integrating institutional power analysis and queer-feminist plus critical race theories, can contribute to AI reconceptualization. Intersectional framework enables analysis of existing AI biases and uncovering alternative ethics from counter-narratives. Research presents intersectional strategies as polyvocal, multimodal, and experimental, with community-focused and artistic practices helping explore AI's intersectional possibilities. Practical examples include Data Nutrition Label for bias assessment in datasets and experimental projects like \"ladymouth,\" a chatbot explaining feminism.",
    "tags": [],
    "dateAdded": "2025-07-31T06:57:46Z",
    "dateModified": "2025-07-31T06:57:46Z"
  },
  {
    "key": "CHJQ52DC",
    "itemType": "journalArticle",
    "title": "AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "E.",
        "lastName": "Latif"
      },
      {
        "creatorType": "author",
        "firstName": "X.",
        "lastName": "Zhai"
      },
      {
        "creatorType": "author",
        "firstName": "L.",
        "lastName": "Liu"
      }
    ],
    "date": "2024",
    "publicationTitle": "arXiv preprint",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://arxiv.org/html/2312.10833v4",
    "abstractNote": "Empirical study examining gender bias in large language models through analysis of over 6000 evaluated student responses from 70 male and 70 female participants. Research uses fine-tuned BERT models and GPT-3.5 to evaluate various training data configurations: gender-specific versus mixed datasets. Three evaluation metrics applied: Scoring Accuracy Difference for bias assessment, Mean Score Gaps (MSG) for gender disparities, and Equalized Odds (EO) for fairness measurement. Results show mixed-gender trained models produce significantly better results than gender-specific models with reduced MSG and fairer predictions.",
    "tags": [],
    "dateAdded": "2025-07-31T06:57:46Z",
    "dateModified": "2025-07-31T06:57:46Z"
  },
  {
    "key": "AIGLDZ4C",
    "itemType": "webpage",
    "title": "How to Create Inclusive AI Images: A Guide to Bias-Free Prompting",
    "creators": [
      {
        "creatorType": "author",
        "name": "Articulate"
      }
    ],
    "date": "2025",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://www.articulate.com/blog/how-to-create-inclusive-ai-images-a-guide-to-bias-free-prompting/",
    "abstractNote": "Practical guide examining prompt engineering strategies for creating inclusive AI-generated images and avoiding biased default outputs. Analysis shows AI image generators often produce stereotypical representations (white, male, slim, young, physically able) due to training on unbalanced internet datasets. Presents concrete techniques for inclusive prompt engineering: specifying visible identity characteristics (age, race, gender, body size, visible disabilities), using diversity-promoting terms like \"multicultural\" and \"gender-diverse,\" and providing additional context to break stereotypical associations.",
    "tags": [],
    "dateAdded": "2025-07-31T06:57:46Z",
    "dateModified": "2025-07-31T06:57:46Z"
  },
  {
    "key": "AFDLFCIL",
    "itemType": "journalArticle",
    "title": "Feminist Perspectives on AI: Ethical Considerations in Algorithmic Decision-Making",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "U.",
        "lastName": "Ahmed"
      }
    ],
    "date": "2024",
    "publicationTitle": "Research Corridor Journal of Gender Studies and Intersectionality",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://www.researchcorridor.org/index.php/jgsi/article/download/330/314",
    "abstractNote": "Explores ethical implications of algorithmic decision-making from feminist perspective, examining data bias, discrimination in automated systems, and underrepresentation of women in AI development. Algorithmic biases disproportionately affect marginalized groups and reinforce societal inequalities in areas like hiring, healthcare, and law enforcement. Feminist ethical framework emphasizes transparency, fairness, and inclusivity while questioning patriarchal and corporate-driven narratives in AI research and policy. Analysis shows AI ethics must go beyond technical solutions to address systemic power imbalances and cultural biases in data.",
    "tags": [],
    "dateAdded": "2025-07-31T06:57:46Z",
    "dateModified": "2025-07-31T06:57:46Z"
  },
  {
    "key": "FDR5APIU",
    "itemType": "journalArticle",
    "title": "Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "I.",
        "lastName": "Ulnicane"
      }
    ],
    "date": "2024",
    "publicationTitle": "Social Inclusion",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "10.17645/si.v12.7543",
    "url": "",
    "abstractNote": "Analyzes emerging intersectionality agenda in AI through examination of four high-level reports on this topic (2019-2021). Research shows how these documents frame problems and formulate recommendations for addressing inequalities. AI systems often amplify and exacerbate human biases and stereotypes, leading to discrimination and marginalization. Analysis reveals systematic problems including diversity crises in AI development where founders and employees mainly come from homogeneous groups of white men, and reinforcement of existing power relationships through AI systems.",
    "tags": [],
    "dateAdded": "2025-07-31T06:57:45Z",
    "dateModified": "2025-07-31T06:57:45Z"
  },
  {
    "key": "SWB86AYC",
    "itemType": "conferencePaper",
    "title": "Generative AI and the Future of Digital Literacy: Opportunities for Gender Inclusion",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "R.",
        "lastName": "Hartshorne"
      },
      {
        "creatorType": "author",
        "firstName": "J.",
        "lastName": "Cohen"
      }
    ],
    "date": "2025",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "",
    "abstractNote": "Examines generative AI potential for promoting female inclusion in primary and secondary education while addressing inherent risks of reinforcing gender-specific biases. Qualitative research analyzes systemic effects of generative AI tools on teaching and learning through interviews and focus groups with students and teachers. Results show complex interactions between generative AI technologies and gender dynamics. Key factors for effective AI-supported learning environments include personalized learning experiences, bias-aware content generation, and teachers' role as mediators of AI interactions.",
    "tags": [],
    "dateAdded": "2025-07-31T06:57:45Z",
    "dateModified": "2025-07-31T06:57:45Z"
  },
  {
    "key": "GP4JDSI8",
    "itemType": "conferencePaper",
    "title": "Data Feminism for AI",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "L.",
        "lastName": "Klein"
      },
      {
        "creatorType": "author",
        "firstName": "C.",
        "lastName": "D'Ignazio"
      }
    ],
    "date": "2024",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "10.1145/3630106.3658543",
    "url": "",
    "abstractNote": "Presents intersectional feminist principles for just, ethical, and sustainable AI research. Extends seven Data Feminism principles to AI contexts: examine power, challenge power, rethink binaries and hierarchies, elevate emotion and embodiment, consider context, embrace pluralism, and make work visible. Proposes two additional principles on environmental impacts and consent. Framework helps identify and mitigate predictable harms before releasing discriminatory systems. Practical applications include participatory ML design processes and analysis of online advertising systems.",
    "tags": [],
    "dateAdded": "2025-07-31T06:57:45Z",
    "dateModified": "2025-07-31T06:57:45Z"
  },
  {
    "key": "GPSB87RN",
    "itemType": "report",
    "title": "What is Feminist AI?",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "A.",
        "lastName": "Wudel"
      },
      {
        "creatorType": "author",
        "firstName": "A.",
        "lastName": "Ehrenberg"
      }
    ],
    "date": "2025",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://library.fes.de/pdf-files/bueros/bruessel/21888-20250304.pdf",
    "abstractNote": "Defines Feminist AI (FAI) as framework using intersectional feminism to address bias and inequalities in AI systems. Emphasizes interdisciplinary collaboration, systemic power analysis, and iterative theory-practice loops. Embeds feminist values of equality, freedom, and justice to transform AI development. Includes practical applications like FemAI advocacy for feminist perspectives in EU AI Act and MIRA diagnostic platform aligning AI tools with social justice goals. Distinguishes FAI from traditional \"Responsible AI\" approaches through focus on structural power inequalities rather than individual \"bad actors.\"",
    "tags": [],
    "dateAdded": "2025-07-31T06:57:45Z",
    "dateModified": "2025-07-31T06:57:45Z"
  },
  {
    "key": "SHJQQTI6",
    "itemType": "conferencePaper",
    "title": "A Survey on Intersectional Fairness in Machine Learning: Opportunities and Challenges",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "U.",
        "lastName": "Gohar"
      },
      {
        "creatorType": "author",
        "firstName": "L.",
        "lastName": "Cheng"
      }
    ],
    "date": "2023",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "10.24963/ijcai.2023/742",
    "url": "",
    "abstractNote": "Comprehensive survey examining intersectional fairness in machine learning systems beyond traditional binary fairness approaches. Presents taxonomy of intersectional fairness concepts showing how multiple sensitive attributes interact to create unique discrimination forms. Identifies challenges including data sparsity for intersectional groups and bias mitigation complexity. Demonstrates applications in NLP systems, ranking algorithms, and image recognition. Shows traditional fairness metrics fail for intersectional identities as Black women experience different discrimination than Black people or women separately.",
    "tags": [],
    "dateAdded": "2025-07-31T06:57:45Z",
    "dateModified": "2025-07-31T06:57:45Z"
  },
  {
    "key": "9Y7ZFGI5",
    "itemType": "journalArticle",
    "title": "Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "S. S.",
        "lastName": "Shah"
      }
    ],
    "date": "2025",
    "publicationTitle": "Premier Journal of Artificial Intelligence",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "10.70389/PJAI.1000088",
    "url": "",
    "abstractNote": "Narrative review examining interplay between gender bias in AI systems and digital literacy potential for empowering women in technology. Synthesizes research from 2010-2024 analyzing systematic gender biases in AI applications across recruitment, healthcare, and financial services. These biases stem from women's underrepresentation in AI development teams (only 22% globally), biased training data, and algorithmic design decisions. Digital literacy programs show promise as intervention fostering critical awareness of AI bias, encouraging women toward AI careers, and catalyzing growth of women-led AI projects.",
    "tags": [],
    "dateAdded": "2025-07-31T06:57:45Z",
    "dateModified": "2025-07-31T06:57:45Z"
  },
  {
    "key": "9BDIJE9B",
    "itemType": "report",
    "title": "Artificial Intelligence and gender equality",
    "creators": [
      {
        "creatorType": "author",
        "name": "UN Women"
      }
    ],
    "date": "2024",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://www.unwomen.org/en/articles/explainer/artificial-intelligence-and-gender-equality",
    "abstractNote": "Comprehensive policy brief series analyzing how AI systems perpetuate gender inequalities while highlighting pathways for more equitable development. Documents that 44% of AI systems show gender bias, with 25% exhibiting both gender and racial bias, and provides evidence-based policy recommendations for addressing systematic underrepresentation and discriminatory outcomes.",
    "tags": [],
    "dateAdded": "2025-07-31T06:51:30Z",
    "dateModified": "2025-07-31T06:51:30Z"
  },
  {
    "key": "V4GTLMED",
    "itemType": "journalArticle",
    "title": "Tech workers' perspectives on ethical issues in AI development: Foregrounding feminist approaches",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "J.",
        "lastName": "Browne"
      },
      {
        "creatorType": "author",
        "firstName": "E.",
        "lastName": "Drage"
      },
      {
        "creatorType": "author",
        "firstName": "K.",
        "lastName": "McInerney"
      }
    ],
    "date": "2024",
    "publicationTitle": "Big Data & Society",
    "volume": "11",
    "issue": "1",
    "pages": "",
    "DOI": "10.1177/20539517231221780",
    "url": "https://doi.org/10.1177/20539517231221780",
    "abstractNote": "Empirical study examining tech workers' understanding of ethical AI development through feminist lens, revealing critical gaps in current AI ethics approaches. Finds that the term \"bias\" creates confusion among tech workers, undermining AI ethics initiatives, and argues for moving beyond diversity narratives toward \"design justice\" that centers marginalized voices.",
    "tags": [],
    "dateAdded": "2025-07-31T06:51:30Z",
    "dateModified": "2025-07-31T06:51:30Z"
  },
  {
    "key": "YMYHLMFS",
    "itemType": "conferencePaper",
    "title": "Data feminism for AI",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "L.",
        "lastName": "Klein"
      },
      {
        "creatorType": "author",
        "firstName": "C.",
        "lastName": "D'Ignazio"
      }
    ],
    "date": "2024",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "540-551",
    "DOI": "10.1145/3630106.3658543",
    "url": "https://doi.org/10.1145/3630106.3658543",
    "abstractNote": "Extends the influential \"Data Feminism\" framework to AI research, presenting intersectional feminist principles for conducting equitable, ethical, and sustainable AI research. Rearticulates original seven data feminism principles specifically for AI contexts and introduces two new principles addressing environmental impact and consent, providing concrete methodological guidance.",
    "tags": [],
    "dateAdded": "2025-07-31T06:51:30Z",
    "dateModified": "2025-07-31T06:51:30Z"
  },
  {
    "key": "QM6L6XLZ",
    "itemType": "report",
    "title": "Incubating Feminist AI: Executive Summary 2021-2024",
    "creators": [
      {
        "creatorType": "author",
        "name": "A+ Alliance"
      }
    ],
    "date": "2024",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://aplusalliance.org/incubatingfeministai2024/",
    "abstractNote": "Documents outcomes from $2 million CAD Feminist AI Research Network project across Latin America, Middle East, and Asia. Developed 12 feminist AI prototypes addressing gender-based violence, transit safety, bias detection in NLP systems, and judicial transparency, demonstrating practical applications of feminist AI principles through community-centered design and intersectional analysis capabilities.",
    "tags": [],
    "dateAdded": "2025-07-31T06:51:30Z",
    "dateModified": "2025-07-31T06:51:30Z"
  },
  {
    "key": "ZNHUCA4B",
    "itemType": "report",
    "title": "Recommendation on the Ethics of Artificial Intelligence",
    "creators": [
      {
        "creatorType": "author",
        "name": "UNESCO"
      }
    ],
    "date": "2021",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://unesdoc.unesco.org/ark:/48223/pf0000380455",
    "abstractNote": "First global standard on AI ethics adopted by 193 member states, establishing comprehensive policy frameworks addressing gender equality in AI development and deployment. Explicitly addresses gender stereotyping, discriminatory biases, and need for equitable participation across the AI lifecycle, with recent implementation including Women4Ethical AI platform and systematic bias studies.",
    "tags": [],
    "dateAdded": "2025-07-31T06:51:30Z",
    "dateModified": "2025-07-31T06:51:30Z"
  },
  {
    "key": "SQ38TTWQ",
    "itemType": "journalArticle",
    "title": "Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "J.",
        "lastName": "An"
      },
      {
        "creatorType": "author",
        "firstName": "D.",
        "lastName": "Huang"
      },
      {
        "creatorType": "author",
        "firstName": "C.",
        "lastName": "Lin"
      },
      {
        "creatorType": "author",
        "firstName": "M.",
        "lastName": "Tai"
      }
    ],
    "date": "2025",
    "publicationTitle": "PNAS Nexus",
    "volume": "4",
    "issue": "3",
    "pages": "pgaf089",
    "DOI": "10.1093/pnasnexus/pgaf089",
    "url": "https://doi.org/10.1093/pnasnexus/pgaf089",
    "abstractNote": "Large-scale experimental study examining bias across five major LLMs using over 361,000 randomized resumes. Reveals complex intersectional patterns where LLMs favor female candidates but discriminate against Black male candidates, with bias effects translating to 1-3 percentage point differences in hiring probabilities, validating intersectionality theory through three key findings.",
    "tags": [],
    "dateAdded": "2025-07-31T06:51:30Z",
    "dateModified": "2025-07-31T06:51:30Z"
  },
  {
    "key": "ZMW228P6",
    "itemType": "conferencePaper",
    "title": "Factoring the matrix of domination: A critical review and reimagination of intersectionality in AI fairness",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "A.",
        "lastName": "Ovalle"
      },
      {
        "creatorType": "author",
        "firstName": "A.",
        "lastName": "Subramonian"
      },
      {
        "creatorType": "author",
        "firstName": "V.",
        "lastName": "Gautam"
      },
      {
        "creatorType": "author",
        "firstName": "G.",
        "lastName": "Gee"
      },
      {
        "creatorType": "author",
        "firstName": "K. W.",
        "lastName": "Chang"
      }
    ],
    "date": "2023",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "704-716",
    "DOI": "10.1145/3600211.3604705",
    "url": "https://dl.acm.org/doi/abs/10.1145/3600211.3604705",
    "abstractNote": "Critical review examining how intersectionality is conceptualized and operationalized within AI fairness research, analyzing 30 papers from the field. Identifies significant gaps between intersectionality theory and technical applications, proposing six key tenets for properly applying intersectionality in AI research drawn from Patricia Hill Collins and Sirma Bilge's framework.",
    "tags": [],
    "dateAdded": "2025-07-31T06:51:30Z",
    "dateModified": "2025-07-31T06:51:30Z"
  },
  {
    "key": "XW8NHCIE",
    "itemType": "conferencePaper",
    "title": "Gender, race, and intersectional bias in AI resume screening via language model retrieval",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "K.",
        "lastName": "Wilson"
      },
      {
        "creatorType": "author",
        "firstName": "A.",
        "lastName": "Caliskan"
      }
    ],
    "date": "2024",
    "publicationTitle": "",
    "volume": "7",
    "issue": "",
    "pages": "1578-1590",
    "DOI": "10.1609/aies.v7i1.31748",
    "url": "https://doi.org/10.1609/aies.v7i1.31748",
    "abstractNote": "Analyzes bias in large language models used for resume screening, examining over 550 job descriptions and 550 resumes across multiple demographic combinations. Reveals significant intersectional discrimination with Black male candidates facing most severe disadvantages, validating three key hypotheses of intersectionality theory through over 40,000 comparisons.",
    "tags": [],
    "dateAdded": "2025-07-31T06:51:30Z",
    "dateModified": "2025-07-31T06:51:30Z"
  },
  {
    "key": "ZLM537Z4",
    "itemType": "book",
    "title": "Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligent Machines",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "J.",
        "lastName": "Browne"
      },
      {
        "creatorType": "author",
        "firstName": "S.",
        "lastName": "Cave"
      },
      {
        "creatorType": "author",
        "firstName": "E.",
        "lastName": "Drage"
      },
      {
        "creatorType": "author",
        "firstName": "K.",
        "lastName": "McInerney"
      }
    ],
    "date": "2023",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://doi.org/10.1093/oso/9780192889898.001.0001",
    "abstractNote": "First comprehensive collection bringing together leading feminist thinkers across disciplines to examine AI's societal impact. Features 21 chapters covering topics from techno-racial capitalism to AI's military applications, examining how feminist scholarship can hold the AI sector accountable for designing systems that further social justice through diverse feminist approaches.",
    "tags": [],
    "dateAdded": "2025-07-31T06:51:30Z",
    "dateModified": "2025-07-31T06:51:30Z"
  },
  {
    "key": "JZN2I6J5",
    "itemType": "conferencePaper",
    "title": "Prompting fairness: Learning prompts for debiasing large language models",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "A.-V.",
        "lastName": "Chisca"
      },
      {
        "creatorType": "author",
        "firstName": "A.-C.",
        "lastName": "Rad"
      },
      {
        "creatorType": "author",
        "firstName": "C.",
        "lastName": "Lemnaru"
      }
    ],
    "date": "2024",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "52-62",
    "DOI": "",
    "url": "https://aclanthology.org/2024.ltedi-1.6/",
    "abstractNote": "Introduces novel prompt-tuning method for reducing biases in encoder models like BERT and RoBERTa through training small sets of additional reusable token embeddings. Demonstrates state-of-the-art performance while maintaining minimal impact on language modeling capabilities through parameter-efficient approach applicable across different models and tasks.",
    "tags": [],
    "dateAdded": "2025-07-31T06:51:30Z",
    "dateModified": "2025-07-31T06:51:30Z"
  },
  {
    "key": "MS3CNU3S",
    "itemType": "journalArticle",
    "title": "Bias and fairness in large language models: A survey",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "I. O.",
        "lastName": "Gallegos"
      },
      {
        "creatorType": "author",
        "firstName": "R. A.",
        "lastName": "Rossi"
      },
      {
        "creatorType": "author",
        "firstName": "J.",
        "lastName": "Barrow"
      },
      {
        "creatorType": "author",
        "firstName": "M. M.",
        "lastName": "Tanjim"
      },
      {
        "creatorType": "author",
        "firstName": "S.",
        "lastName": "Kim"
      },
      {
        "creatorType": "author",
        "firstName": "F.",
        "lastName": "Dernoncourt"
      },
      {
        "creatorType": "author",
        "firstName": "T.",
        "lastName": "Yu"
      },
      {
        "creatorType": "author",
        "firstName": "R.",
        "lastName": "Zhang"
      },
      {
        "creatorType": "author",
        "firstName": "N. K.",
        "lastName": "Ahmed"
      }
    ],
    "date": "2024",
    "publicationTitle": "Computational Linguistics",
    "volume": "50",
    "issue": "3",
    "pages": "1097-1179",
    "DOI": "10.1162/coli_a_00524",
    "url": "https://doi.org/10.1162/coli_a_00524",
    "abstractNote": "Comprehensive survey consolidating notions of social bias and fairness in NLP, defining distinct facets of harm and introducing desiderata to operationalize fairness for LLMs. Proposes three taxonomies: metrics for bias evaluation, datasets for bias evaluation, and techniques for bias mitigation classified by intervention timing.",
    "tags": [],
    "dateAdded": "2025-07-31T06:51:30Z",
    "dateModified": "2025-07-31T06:51:30Z"
  },
  {
    "key": "7G73H3KM",
    "itemType": "journalArticle",
    "title": "Generative AI and opportunities for feminist classroom assignments",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "S. F.",
        "lastName": "Small"
      }
    ],
    "date": "2023",
    "publicationTitle": "Feminist Pedagogy",
    "volume": "3",
    "issue": "5",
    "pages": "Article 10",
    "DOI": "",
    "url": "https://digitalcommons.calpoly.edu/feministpedagogy/vol3/iss5/10/",
    "abstractNote": "Addresses integration of generative AI into feminist classroom assignments to develop reflexivity and feminist epistemology. Proposes intentional feminist approaches where students collaborate with AI to develop understanding of feminist knowledge production, transforming AI from educational threat into tool for critical learning about power and epistemology.",
    "tags": [],
    "dateAdded": "2025-07-31T06:51:30Z",
    "dateModified": "2025-07-31T06:51:30Z"
  },
  {
    "key": "KNQYFQ6B",
    "itemType": "conferencePaper",
    "title": "The power of prompts: Evaluating and mitigating gender bias in MT with LLMs",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "A.",
        "lastName": "Sant"
      },
      {
        "creatorType": "author",
        "firstName": "C.",
        "lastName": "Escolano"
      },
      {
        "creatorType": "author",
        "firstName": "A.",
        "lastName": "Mash"
      },
      {
        "creatorType": "author",
        "firstName": "F.",
        "lastName": "De Luca Fornaciari"
      },
      {
        "creatorType": "author",
        "firstName": "M.",
        "lastName": "Melero"
      }
    ],
    "date": "2024",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "94-139",
    "DOI": "10.18653/v1/2024.gebnlp-1.7",
    "url": "https://doi.org/10.18653/v1/2024.gebnlp-1.7",
    "abstractNote": "Examines gender bias in machine translation through LLMs using four widely-used test sets. Develops specific prompting engineering techniques that reduce gender bias by up to 12% on WinoMT evaluation dataset. Identifies optimal prompt structures incorporating explicit fairness instructions and context-aware guidelines.",
    "tags": [],
    "dateAdded": "2025-07-31T06:51:30Z",
    "dateModified": "2025-07-31T06:51:30Z"
  },
  {
    "key": "8BUHU5EP",
    "itemType": "report",
    "title": "Feminist reflections for the development of Artificial Intelligence",
    "creators": [
      {
        "creatorType": "author",
        "name": "Derechos Digitales"
      }
    ],
    "date": "2023",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://www.derechosdigitales.org/fair-2023-en/",
    "abstractNote": "Synthesizes conversations among Latin American women developing AI systems, providing methodological frameworks for feminist AI development based on four real-world projects. Emphasizes co-design methodologies, digital autonomy, data sovereignty, and intersectional approaches through community agreements and power-balancing strategies.",
    "tags": [],
    "dateAdded": "2025-07-31T06:51:30Z",
    "dateModified": "2025-07-31T06:51:30Z"
  },
  {
    "key": "IGFYSIV8",
    "itemType": "journalArticle",
    "title": "Shaping feminist artificial intelligence",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "S.",
        "lastName": "Toupin"
      }
    ],
    "date": "2024",
    "publicationTitle": "New Media & Society",
    "volume": "26",
    "issue": "4",
    "pages": "1875-1894",
    "DOI": "10.1177/14614448221150776",
    "url": "https://journals.sagepub.com/doi/full/10.1177/14614448221150776",
    "abstractNote": "Comprehensive examination of feminist artificial intelligence through historical analysis and contemporary typology development. Provides detailed framework categorizing FAI as: model, design, policy, culture, discourse, and science. Traces FAI's evolution from foundational work to contemporary initiatives, analyzing tensions between commercialized approaches and community-oriented methods.",
    "tags": [],
    "dateAdded": "2025-07-31T06:51:30Z",
    "dateModified": "2025-07-31T06:51:30Z"
  },
  {
    "key": "BDI6XU5A",
    "itemType": "report",
    "title": "Gender und KI-Anwendungen. Trägt KI zum Genderproblem oder zu seiner Lösung bei?",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "Swetlana",
        "lastName": "Franken"
      },
      {
        "creatorType": "author",
        "firstName": "Nina",
        "lastName": "Mauritz"
      }
    ],
    "date": "",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://www.fh-bielefeld.de/multimedia/Fachbereiche/Wirtschaft+und+Gesundheit/ Forschung/Denkfabrik+Digitalisierte+Arbeitswelt/geki_Abschlussbericht.pdf",
    "abstractNote": "",
    "tags": [],
    "dateAdded": "2025-07-31T06:25:25Z",
    "dateModified": "2025-07-31T06:26:19Z"
  },
  {
    "key": "EQV4DNQR",
    "itemType": "document",
    "title": "Challenging systematic prejudices: an Investigation into Gender Bias in Large Language Models",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "",
        "lastName": "UNESCO, IRCAI"
      }
    ],
    "date": "2024",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://discovery.ucl.ac.uk/id/eprint/10188772/1/Unesco_results.pdf",
    "abstractNote": "This study explores biases in three significant large language models (LLMs): OpenAI’s GPT-2\nand ChatGPT, along with Meta’s Llama 2, highlighting their role in both advanced decision-making\nsystems and as user-facing conversational agents. Across multiple studies, the brief reveals how\nbiases emerge in the text generated by LLMs, through gendered word associations, positive or\nnegative regard for gendered subjects, or diversity in text generated by gender and culture.\nThe research uncovers persistent social biases within these state-of-the-art language models,\ndespite ongoing efforts to mitigate such issues. The findings underscore the critical need for\ncontinuous research and policy intervention to address the biases that exacerbate as these\ntechnologies are integrated across diverse societal and cultural landscapes. The emphasis on\nGPT-2 and Llama 2 being open-source foundational models is particularly noteworthy, as their\nwidespread adoption underlines the urgent need for scalable, objective methods to assess and\ncorrect biases, ensuring fairness in AI systems globally.",
    "tags": [],
    "dateAdded": "2025-07-31T05:28:23Z",
    "dateModified": "2025-07-31T06:25:09Z"
  },
  {
    "key": "8NG4ZEWE",
    "itemType": "bookSection",
    "title": "Intersektionalität",
    "creators": [
      {
        "creatorType": "editor",
        "firstName": "Rolf",
        "lastName": "Arnold"
      },
      {
        "creatorType": "editor",
        "firstName": "Ekkehard",
        "lastName": "Nuissl"
      },
      {
        "creatorType": "editor",
        "firstName": "Josef",
        "lastName": "Schrader"
      },
      {
        "creatorType": "author",
        "firstName": "Katharina",
        "lastName": "Walgenbach"
      }
    ],
    "date": "2023-04-03",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "236-237",
    "DOI": "",
    "url": "https://wb-erwachsenenbildung.net/intersektionalitaet/",
    "abstractNote": "",
    "tags": [],
    "dateAdded": "2025-07-31T06:00:51Z",
    "dateModified": "2025-07-31T06:00:51Z"
  },
  {
    "key": "5T55I5Z7",
    "itemType": "report",
    "title": "ARTIFICIAL INTELLIGENCE and GENDER EQUALITY",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "",
        "lastName": "UNESCO"
      }
    ],
    "date": "2020",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "unesdoc.unesco.org/in/rest/annotationSVC/DownloadWatermarkedAttachment/attach_import_ab07646d-c784-4a4e-96a1-3be7855b6f76?_=374174eng.pdf&to=54&from=1",
    "abstractNote": "The present report builds on UNESCO’s previous\nwork on gender equality and AI and aims to continue\nthe conversation on this topic with a select group\nof experts from key stakeholder groups. In March\n2019, UNESCO published a groundbreaking report,\nI’d Blush if I Could: closing gender divides in digital\nskills through education, based on research funded\nby the German Federal Ministry for Economic\nCooperation and Development. This report featured\nrecommendations on actions to overcome\nglobal gender gaps in digital skills, with a special\nexamination of the impact of gender biases coded\ninto some of the most prevalent AI applications.",
    "tags": [],
    "dateAdded": "2025-07-31T05:46:45Z",
    "dateModified": "2025-07-31T05:55:56Z"
  },
  {
    "key": "QUV5DQH3",
    "itemType": "journalArticle",
    "title": "When Good Algorithms Go Sexist: Why and How to Advance AI Gender Equity",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "Genevieve",
        "lastName": "Smith"
      },
      {
        "creatorType": "author",
        "firstName": "Ishita",
        "lastName": "Rustagi"
      }
    ],
    "date": "2021",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "10.48558/A179-B138",
    "url": "https://ssir.org/articles/entry/when_good_algorithms_go_sexist_why_and_how_to_advance_ai_gender_equity",
    "abstractNote": "Seven actions social change leaders and machine learning developers can take to build gender-smart artificial intelligence for a more just world.",
    "tags": [],
    "dateAdded": "2025-07-31T05:50:54Z",
    "dateModified": "2025-07-31T05:50:54Z"
  },
  {
    "key": "4KMMPA6A",
    "itemType": "report",
    "title": "Faires KIPrompting – Ein Leitfaden für Unternehmen. BSP Business and Law School – Hochschule für Management und Recht",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "E.",
        "lastName": "Gengler,"
      },
      {
        "creatorType": "author",
        "firstName": "K.",
        "lastName": "Bodrožić-Brnić,"
      }
    ],
    "date": "2024",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://www.businessschool-berlin.de/files/Business-School-Berlin/Publikation/Faires-KI-Prompting-Ein-Leitfaden-fuer-Unternehmen%202024.pdf",
    "abstractNote": "Der vorliegende Leitfaden möchte Sie auf eine Reise durch die Welt der Generativen KI mitnehmen und Ihnen Werkzeuge an die Hand geben, um diese Technolo-\ngien verantwortungsvoll und bewusst zu nutzen. Wir möchten Verständnis für die positive wie negative Wirkung von Generativer KI schaffen, zugleich aber auch den Weg für einen diversen und fairen Einsatz ebnen. Dieser Guide kann Ihr Kompass sein, um nicht nur zu navigieren, sondern die digitale Zukunft mitzugestalten",
    "tags": [],
    "dateAdded": "2025-07-31T05:40:00Z",
    "dateModified": "2025-07-31T05:42:34Z"
  },
  {
    "key": "3ZNMTJ5B",
    "itemType": "webpage",
    "title": "feminist AI | ACADEMY",
    "creators": [],
    "date": "",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://www.feminist-ai.com/academy",
    "abstractNote": "Get ready for transforming power! We enable organizations to create more equitable AI through education. We hold workshops, give training, and provide learning material to raise awareness, build knowledge, and ease your creation of equitable AI.",
    "tags": [],
    "dateAdded": "2025-07-31T05:36:42Z",
    "dateModified": "2025-07-31T05:36:42Z"
  },
  {
    "key": "J5EF9W6M",
    "itemType": "report",
    "title": "AI & Intersectionality: A Toolkit For Fairness & Inclusion",
    "creators": [
      {
        "creatorType": "author",
        "name": "DIVERSIFAIR Project"
      }
    ],
    "date": "2024",
    "publicationTitle": "",
    "volume": "",
    "issue": "",
    "pages": "",
    "DOI": "",
    "url": "https://diversifair-project.eu/wp-content/uploads/2025/01/Copie-de-INDUSTRY-Educ-Kits-A4-V2.pdf",
    "abstractNote": "Das DIVERSIFAIR-Toolkit ist eine praktische Ressource, die sich an politische Entscheidungsträger*innen, die Industrie und die Zivilgesellschaft richtet. Es zielt darauf ab, ein Bewusstsein für intersektionale Diskriminierung in KI-Systemen zu schaffen und konkrete Handlungsstrategien zur Risikominderung anzubieten. Das Toolkit betont die Notwendigkeit, über einzelne Diskriminierungsachsen (wie Geschlecht oder Herkunft) hinauszudenken und deren Verschränkungen zu analysieren. Es fördert eine KI-Kompetenz, die es Stakeholdern ermöglicht, KI-Systeme über ihren gesamten Lebenszyklus hinweg – von der Datensammlung über das Design bis zur Anwendung – auf intersektionale Risiken zu prüfen. Für das Prompting bedeutet dies, gezielt Szenarien zu entwerfen, die marginalisierte Identitäten an der Schnittstelle mehrerer Merkmale repräsentieren, um blinde Flecken und stereotype Assoziationen in KI-Modellen aufzudecken.",
    "tags": [],
    "dateAdded": "2025-07-30T15:49:07Z",
    "dateModified": "2025-07-30T15:49:07Z"
  },
  {
    "key": "2EBHMYU4",
    "itemType": "journalArticle",
    "title": "Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action",
    "creators": [
      {
        "creatorType": "author",
        "firstName": "Inga",
        "lastName": "Ulnicane"
      }
    ],
    "date": "2024/04/18",
    "publicationTitle": "Social Inclusion",
    "volume": "12",
    "issue": "0",
    "pages": "",
    "DOI": "10.17645/si.7543",
    "url": "https://www.cogitatiopress.com/socialinclusion/article/view/7543",
    "abstractNote": "Diese Studie analysiert die aufkommende Agenda zu Intersektionalität in AI durch Untersuchung von vier hochrangigen Berichten zu diesem Thema (2019-2021). Die Forschung zeigt, wie diese Dokumente Probleme rahmen und Empfehlungen zur Adressierung von Ungleichheiten formulieren. AI-Systeme verstärken und verschärfen oft menschliche Verzerrungen und Stereotypen, was zu Diskriminierung und Marginalisierung führt. Die Analyse deckt systematische Probleme auf: Diversitätskrisen in AI-Entwicklung, wo Gründer und Mitarbeiter hauptsächlich aus homogenen Gruppen weißer Männer stammen, sowie die Verstärkung bestehender Machtbeziehungen durch AI-Systeme. Die Studie betont die Notwendigkeit intersektionaler Ansätze, die multiple Ungleichheiten berücksichtigen, die aus der Interaktion verschiedener sozialer Identitäten entstehen. Empfehlungen umfassen partizipative AI-Design-Prinzipien und die Einbindung diverser Stakeholder in AI-Governance-Prozesse.",
    "tags": [],
    "dateAdded": "2025-07-30T15:26:37Z",
    "dateModified": "2025-07-30T15:26:58Z"
  }
]