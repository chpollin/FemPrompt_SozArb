---
title: "Himmelreich 2022 Artificial"
original_document: Himmelreich_2022_Artificial.md
document_type: Research Paper
research_domain: AI Ethics
methodology: Theoretical
keywords: Structural injustice, AI governance, algorithmic bias, Iris Marion Young, equity
mini_abstract: "This chapter proposes structural injustice as a conceptual framework for AI governance, arguing that AI systems perpetuate existing social inequalities when deployed in unjust contexts. The authors contend that structural analysis provides superior methodological and normative foundations compared to harm-benefit approaches."
target_audience: Researchers, Policymakers, Practitioners
key_contributions: "Applies structural injustice theory to AI governance systematically"
geographic_focus: Global
publication_year: Unknown
related_fields: Political philosophy, Applied ethics, Social justice studies
summary_date: 2025-11-07
language: English
ai_model: claude-haiku-4-5
---

# Summary: Himmelreich 2022 Artificial

## Overview

This academic work by Himmelreich and Lim presents a philosophical framework for understanding and addressing AI bias through structural injustice theory, derived from philosopher Iris Marion Young's work. The authors argue that AI systems perpetuate pre-existing unjust social structures rather than creating bias independently. Emerging from contemporary global awareness of systemic racism, colonialism, and inequality across the Global North (US, Europe), the work positions AI governance as inseparable from broader justice concerns. The central claim is that structural injustice theory provides superior analytical and normative foundations for AI ethics compared to conventional harm-benefit analyses or abstract value statements.

## Main Findings

The authors establish several critical findings. First, structural injustice operates through both analytical components (explaining systemic mechanisms) and evaluative components (normative justice theory), providing dual explanatory and moral frameworks. Second, AI biases emerge from interaction between well-intentioned systems and fundamentally unjust social contexts, not individual malice. Third, structural injustice functions as a diagnostic tool for identifying, articulating, and anticipating AI biases before deployment. Fourth, the framework provides rigorous philosophical grounding for Diversity, Equity, and Inclusion initiatives, moving beyond rhetorical commitments. Fifth, responsibility for addressing structural injustice is distributed across individuals and organizations, independent of causal responsibility for injustice's existence. Finally, an open theoretical question remains: whether AI itself is becoming constitutive of society's structural fabric, potentially amplifying injustice systematically.

## Methodology/Approach

The authors employ comparative interdisciplinary philosophical analysis, explicitly contrasting the structural injustice approach against alternative governance frameworks (harm-benefit analyses, value-based approaches). The methodology integrates social science structural explanations with political philosophy and applied ethics, grounding abstract theory in concrete case studies of racial bias in AI systems. The approach draws from sociology, philosophy, and policy studies, examining how institutional arrangements and systemic processes illuminate AI's role in perpetuating injustice while simultaneously applying justice theory to evaluate these structures' legitimacy.

## Relevant Concepts

**Structural Injustice:** Injustice embedded in institutional practices and social structures through both analytical mechanisms (how systems operate) and evaluative dimensions (normative justice standards); operates through systemic patterns affecting particular groups.

**Iris Marion Young's Theory:** Foundational philosophical framework distinguishing structural from individual injustice, emphasizing how institutions perpetuate inequality through normal operations.

**Dual-Component Framework:** Structural injustice analysis combining analytical explanation (social science mechanisms) with evaluative judgment (justice theory).

**Algorithmic Bias:** Systematic errors in AI systems disproportionately harming particular groups, understood as emerging from structural rather than purely technical sources.

**Distributed Responsibility:** Moral obligation to address injustice shared across individuals and organizations, distinct from causal responsibility for creating injustice.

**Status Quo Injustice:** Recognition that existing arrangements embed historical inequalities (racism, colonialism, gender/class disparities) that AI risks perpetuating.

## Significance

This work represents a paradigm shift in AI ethics discourse, moving from individualistic to systemic responsibility frameworks. It challenges narrow technical approaches by demonstrating that algorithmic "objectivity" cannot overcome structural injustice. The significance extends globally: it provides policy-makers, practitioners, and researchers with conceptual tools for recognizing how AI governance connects to broader social justice movements addressing systemic racism and colonialism. By grounding DEI initiatives in rigorous philosophical theory, the framework offers methodological rigor to equity-focused governance. The work ultimately argues that responsible AI deployment requires understanding and addressing the unjust structures into which AI is embedded, while acknowledging that the extent of AI's constitutive role in society's structure remains an open theoretical question requiring further investigation.
