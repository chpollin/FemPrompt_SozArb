---
title: "Colombatto 2025 influence"
original_document: Colombatto_2025_influence.md
document_type: Empirical Study
research_domain: AI Ethics
methodology: Experimental
keywords: mental state attribution, trust in AI, large language models, consciousness, advice-taking behavior
mini_abstract: "A preregistered experiment (N=410) reveals that attributions of consciousness to LLMs do not increase advice-taking, while intelligence attributions strongly predict trust, challenging assumptions about anthropomorphism's universal positive effects on AI trust."
target_audience: Researchers, Policymakers, Industry
key_contributions: "Differentiates mental state attribution dimensions predicting AI trust"
geographic_focus: Not Applicable
publication_year: Unknown
related_fields: Human-Computer Interaction, Philosophy of Mind, AI Safety
summary_date: 2025-11-07
language: English
ai_model: claude-haiku-4-5
---

# Summary: Colombatto 2025 influence

## Overview

This Nature Portfolio research addresses a critical disconnect in human-AI interaction: while public surveys reveal that most people attribute consciousness and mental states to large language models, the actual relationship between these attributions and trust in AI advice remains unclear. The study challenges the prevailing assumption in anthropomorphism research that assigning human-like qualities to AI systems uniformly increases user reliance. Using a preregistered experimental design, the authors demonstrate that different types of mental state attributions have divergent—and sometimes opposing—effects on trust. Crucially, consciousness attributions show no positive effect and may even reduce advice-taking, while intelligence attributions strongly predict trust. This investigation addresses an urgent gap as widespread public misattribution of consciousness to LLMs could lead to inappropriate reliance or unjustified skepticism, with significant implications for AI governance and user safety.

## Main Findings

The preregistered experiment (N=410) produced three striking empirical results that fundamentally contradict prior anthropomorphism research. First, **Bayesian analyses revealed strong evidence against a positive correlation between consciousness attributions and advice-taking**—directly contradicting prior studies showing anthropomorphism increases trust in robots, chatbots, and AI systems. Second, **mental state dimensions related to subjective experience (emotions, consciousness) showed negative relationships with advice acceptance**, suggesting users may distrust systems perceived as sentient or emotionally aware. Third, **attributions of intelligence (reasoning, planning capabilities) demonstrated strong positive correlations with advice acceptance**, indicating users calibrate trust based on functional competence rather than perceived consciousness. These findings reveal users employ sophisticated, differentiated reasoning: they distinguish between consciousness and capability when evaluating AI trustworthiness, contrary to the assumption that anthropomorphism uniformly enhances reliance.

## Methodology/Approach

The study employed a rigorous preregistered experimental design combining measurement and behavioral components, strengthening credibility through pre-commitment to analysis protocols. Participants first rated LLM capacity across multiple mental state dimensions—both experience-related (consciousness, emotions, subjective awareness) and intelligence-related (reasoning, planning, problem-solving). Subsequently, participants engaged in a consequential decision-making task where they could revise initial choices based on LLM advice, operationalizing trust through actual behavioral choice revision rather than self-report measures. This behavioral operationalization strengthens ecological validity by measuring consequential trust rather than stated attitudes. The analytical framework utilized Bayesian statistics to assess correlations between specific mental state attributions and advice acceptance, enabling nuanced examination of how different attribution types independently predict trust while controlling for confounds.

## Relevant Concepts

**Folk psychology vs. scientific consensus**: Lay attributions of mental states to AI systems often diverge from expert opinion regarding whether current AI systems actually possess consciousness or mentality.

**Anthropomorphism**: The tendency to assign human-like qualities, intentions, and mental states to non-human entities; prior research suggested this uniformly increases trust.

**Mental state attribution dimensions**: Distinguishing between experience-related attributions (consciousness, emotions, subjective awareness) and intelligence-related attributions (reasoning, planning, problem-solving)—with different behavioral consequences.

**Trust calibration**: Aligning confidence in AI systems with their actual capabilities and limitations, avoiding both dangerous over-reliance and unjustified skepticism.

**Trust resilience**: The tendency to maintain trust in anthropomorphic agents even after observing errors—a potential risk factor for inappropriate reliance.

## Significance

This research fundamentally challenges AI ethics and human-computer interaction frameworks by demonstrating that anthropomorphism effects are not uniformly positive. The findings have immediate practical implications for AI interface design, user education, and governance policy. Rather than encouraging anthropomorphic features to increase engagement, designers may need to emphasize functional intelligence while minimizing consciousness-related cues to promote appropriate trust calibration. The work bridges philosophy of mind, psychology, and AI safety, providing essential empirical evidence during a critical period of LLM deployment. By revealing that public consciousness attributions may actually undermine appropriate trust, the research highlights the importance of aligning user beliefs with scientific reality—a prerequisite for responsible AI governance and user safety as AI capabilities expand.
