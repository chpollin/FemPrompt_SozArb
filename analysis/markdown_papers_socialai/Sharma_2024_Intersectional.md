---
source_file: Sharma_2024_Intersectional.pdf
conversion_date: 2025-11-02T17:39:33.541745
---

## OPEN FORUM

## Understanding how users may work around algorithmic bias

Hannah Overbye-Thompson 1  · Ronald E. Rice 1

Received: 26 May 2025 / Accepted: 14 July 2025 © The Author(s) 2025

## Abstract

Algorithms increasingly mediate critical aspects of daily life across healthcare, hiring, and social media, shaping user experiences through automated decision-making processes. Yet, algorithmic bias, the systematic disadvantaging of certain groups through automated systems, has been widely documented across a variety of algorithms. Thus this study addresses the gap in understanding how users may respond to four epistemic categories of algorithm bias, depending on whether it exists or not, and is perceived or not. We apply the information systems concept of workarounds to characterize potential user responses to these categories of algorithmic bias. Then, we apply the Human-AI Interaction Theory of Interactive Media Effects to understand how users may detect bias through cue routes and develop workaround strategies through action routes. Our theoretical framework proposes how users' detection and workarounds may vary based on the four categories of bias. Understanding these adaptive strategies provides crucial insights for developing inclusive technologies and fostering algorithmic literacy, ultimately enhancing the ongoing negotiation between human agency and technological constraint in digital societies.

Keywords Algorithmic bias · Algorithms · HAAII-TIME model · Media platforms · Workarounds

## 1    Understanding how users may work around algorithmic bias

Algorithms, at their most basic level, are instructions for solving or performing a problem or task, typically repeatedly (Pew Research Center 2017). For the purpose of this paper, the term algorithm(s) refers to the computer code containing a set of instructions that are employed to make a particular technology work . Artificial Intelligence (AI) applies algorithms to perform specific kinds of intelligence, such as language processing, content creation, human learning, or planning. From hiring (Kuncel et al. 2013) and parole sentencing (Laqueur and Copus 2022) to social media feeds and search engines (Google Search Central 2023), algorithms are increasingly created with the goal of making our lives safer and easier.

However,  algorithms,  even  when  widely  accessible, do not benefit everyone equally. For example, many

* Ronald E. Rice

rrice@comm.ucsb.edu Hannah Overbye-Thompson

hoverbye@umail.ucsb.edu

1 Department of Communication, University of California, Santa Barbara, Santa Barbara, USA

smartwatches have been found to provide less accurate data, when using physiological sensors, to those with dark skin (Ajmal et al. 2021; Ray et al. 2021). This is an instance of algorithmic bias -where a device is using algorithm's advantages or disadvantages of certain groups of users or data over others (Friedman and Nissenbaum 1996; OverbyeThompson et al. 2024). Algorithm bias is a widely recognized problem. Research reveals widespread algorithmic bias across multiple sectors: facial recognition systems more frequently misgender individuals with darker skin (Zou and Schiebinger 2018); search engines reinforce racial and ethnic stereotypes (Noble 2018; Sweeney 2013); and even basic technology like optically activated devices fails to detect darker skin tones (Ren and Heacock 2022). Lee et al. (2019) also summarize examples of algorithmic biases (e.g., online recruitment, word associations, online ads, facial recognition, financial credit, criminal justice algorithms, rental or housing purchase offerings, bias indicator proxies, and public safety), describe a variety of bias detection approaches, and recommend some bias-mitigating policies. In critical public services, healthcare algorithms systematically favor White patients over Black patients (Obermeyer et al. 2019), while justice algorithms more often label women as high recidivism risks when determining parole (Hamilton 2019).

<!-- image -->

<!-- image -->

One little-explored aspect of algorithmic bias is how users react or adapt to such real or perceived biases, such as modifying the algorithm service or product, combining the algorithm with other technologies, reinterpreting the purpose of the algorithm, or deciding not to use it. For example, because many image recognition algorithms privilege those with light skin (Buolamwini and Gebru 2018; Ren and Heacock 2022), people with lighter skin tones may not need to react or adapt as often as those with dark skin. If people with darker skin tones do respond to the bias, it may mitigate some of the negative effects of algorithmic bias as they are still able to reap the benefits of a technology while minimizing the drawbacks (Overbye-Thompson et al. 2024). On the other hand, if users with darker skin do not perceive such biases, or do not adapt the settings, applications, or use of the algorithm when they do, they can either use the technology as designed despite the deficiencies, or choose not to use the technology. In the first case, they may not receive the full benefits or even suffer from bias effects; in the second, they will not obtain any of the purported benefits.

The following sections describe the nature(s) of algorithmic bias, suggest ways of conceptualizing responses to algorithmic bias, identify workarounds as an appropriate conceptualization of such responses, explain how the HAAI-TIME theoretical model provides a framework for understanding the process of algorithmic bias workarounds, apply the model and algorithmic bias workarounds to the four epistemic categories of algorithmic bias, and end with a conclusion.

Table 1 Domains of algorithmic bias

| Domain              | Example                                                                                                                                                                               |
|---------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Criminal justice    | Wrongful arrests due to skin tone in facial recognition systems (Johnson 2022)                                                                                                        |
| Facial recognition  | NIST audit finding Western facial recognition algorithms perform worse for Asian features; most facial algorithms preform worse for women and people with dark skin tones (NIST 2019) |
| Finance             | Biased credit scoring methods (O'Neil 2016)                                                                                                                                           |
| Healthcare          | Synthetic healthcare data amplifying existing demographic disparities (Bhanot et al. 2022)                                                                                            |
| Hiring              | Hiring algorithms discriminating based on gender, race, and personality (Chen 2023)                                                                                                   |
| Housing             | Algorithms discriminating against Black and Hispanic tenants in housing applications (Bhuiyan 2024)                                                                                   |
| Image recognition   | Facebook's AI-generated prompt asking users if they wanted to 'keep seeing videos about primates' on video featuring Black men (AP News 2021)                                         |
| Language processing | Sentiment analysis tools misinterpreting African American English (AAE) as more negative or toxic (Resende et al. 2024)                                                               |
| Online advertising  | Searches for 'Black-sounding names' generating advertisements related to incarceration (Sweeney 2013)                                                                                 |
| Online search       | Image search results underrepresenting women and people of color in professional occupations (Metaxa et al. 2021)                                                                     |
| Social welfare      | Algorithms leading to widespread rejection of welfare benefit claims for low-income applicants (Eubanks 2018)                                                                         |

For more examples of algorithmic bias, see https://  incid  entda  tabase.  ai/

## 2    The nature(s) of algorithmic bias

Research, policy, and technology authors have described a range of domains, types, sources, interdependencies, contexts, intentionality, and perceptions of algorithmic biases (see Table 1). Lee et al. (2019) summarize examples of algorithmic biases (e.g., online recruitment, word associations, online ads, facial recognition, financial credit, criminal justice algorithms, rental or housing purchase offerings, bias indicator proxies, and public safety), describe a variety of bias detection approaches, and recommend some biasmitigating policies.

Algorithmic bias can arise at multiple stages of algorithmic development and use: (1) data selection, (2) feature engineering (selecting variables for the model) (Carrell et al. 2024), and (3) model deployment or applications. Fazelpour and Danks (2021) propose an overlapping set of stages: problem specification, data, modeling and validation, and deployment. Within each stage, bias may emerge from three primary sources: (1) biased data, (2) biased people making decisions, and (3) biased algorithmic structures themselves. While the vast landscape of algorithms prohibits a comprehensive examination of every source of bias, Table 2 highlights key ways in which bias can be introduced across different stages of algorithm development and sources.

We provide four additional contingencies to this enumeration of types of bias. First, the various ways in which bias is introduced into users' encounters with algorithms are often interconnected, reinforcing, and may produce similar as well as unique outcomes. In this way, algorithmic bias can foster a digital Matthew effect (Merton 1968; a core principle of digital divide research; see van Dijk 2020), whereby those already well represented in training

| Types Type of bias                                               | Algorithm development stage   | Source of bias   | Description                                                                                                                                                                                                                              |
|------------------------------------------------------------------|-------------------------------|------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Selecting data                                                   |                               |                  |                                                                                                                                                                                                                                          |
| Population bias (Olteanu et al. 2019)                            |                               | Data             | Systematic distortions in demographics between dataset and target population (e.g., facial recognition systems trained predominantly on light-skinned faces perform- ing worse on darker-skinned individuals; Buolamwini and Gebru 2018) |
| Temporal bias (Olteanu et al. 2019)                              |                               | Data             | Systematic variations in data collected across different time periods, including population drifts, behavioral drifts, and system drifts (e.g., Amazon's AI recruiting tool favoring male candidates; Dastin 2018)                       |
| Redundancy bias (Olteanu et al. 2019)                            |                               | Data             | Over-representation of certain data points or patterns in the dataset (e.g., over-policed neighborhoods leading to higher predicted crime rates; O'Neil 2016)                                                                            |
| Biased labels (Das et al. 2021)                                  |                               | People           | Human-generated labels incorporating stereotypes or cultural biases (e.g., sentiment analysis tools misin- terpreting African American English; Resende et al. 2024)                                                                     |
| Synthetic data bias (Das et al. 2021)                            |                               | People           | Artificially generated datasets that contain the biases of their creators (Bhanot et al. 2022; Das et al. 2021)                                                                                                                          |
| Algorithm-generated bias (Stinson 2022)                          |                               | Algorithms       | Algorithms creating or amplifying biases through data collection and feedback loops (e.g., search algorithms reinforcing popular content; Pelly 2025)                                                                                    |
| Feature engineering                                              |                               |                  |                                                                                                                                                                                                                                          |
| Curation bias (Das et al. 2021)                                  |                               | People           | Unintentional selection of variables favoring specific populations (e.g., COMPAS algorithm's 'gender- neutral' factors correlating differently across genders; Hamilton 2019)                                                            |
| Algorithmic focus bias (also proxy bias) (Danks and London 2017) |                               | People           | Selected variables becoming accidental proxies for protected characteristics (e.g., zip codes serving as proxies for race in loan eligibility; Datta et al. 2017)                                                                        |
| Model collapse (also homogenization bias) (Das et al. 2021)      |                               | Algorithms       | Degenerative process affecting successive generations of models, leading to increasingly homogenized out- puts and excluding minority perspectives (Shumailov et al. 2024; Stinson 2022)                                                 |
| Model deployment                                                 |                               |                  |                                                                                                                                                                                                                                          |
| Transfer context bias (Danks and London 2017)                    |                               | People           | Application of algorithms in contexts different from their intended use (e.g., diagnostic models trained on one demographic performing poorly on others; Celeste et al. 2023)                                                            |
| Interpretation bias (Danks and London 2017)                      |                               | People           | Misinterpretation of algorithmic outputs by users (e.g., treating probabilistic predictions as definitive diagno- ses; Hastie and Dawes 2010)                                                                                            |

data, features, and model outputs become further advantaged, while marginalized groups experience increasingly disparate and automated outcomes that reinforce their systematic exclusion, stereotyping, prejudice, and discrimination as illustrated in the next section. Second, not all potential effects of algorithmic bias will manifest uniformly across different technological system contexts. The varied architectures, purposes, and implementation contexts can create distinct pathways for bias to emerge and propagate (Stinson 2022). Table 3 summarizes some implications of four contexts: search engines, social media platforms, image recognition systems, and high-risk organizational algorithms. Third, while this analysis focuses on unintentional sources of algorithmic bias emerging through various stages of development and application, intentional bias can also occur through deliberate selection of data and structuring of the algorithm to produce discriminatory outcomes (Adams-Prassl et al. 2023). However, Lee et al. (2019) note the difficulties and ambiguities in determining and detecting what is intended or unintended biased

<!-- image -->

Table 3 Comparison of algorithmic bias across selected contexts, by visibility and user workaround agency

| Technological context                                                     | Visibility of bias                                                                                         | User agency                                                                                                                                                       |
|---------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Search engines                                                            | Results are visible but ranking criteria are opaque                                                        | Users can only modify queries but cannot directly influence ranking algorithms-limited user ability to develop workarounds                                        |
| Social media platforms                                                    | Algorithmic curation visibility likely depends on algorithm awareness and the specific platform being used | Users can modify feeds through implicit and explicit actions, but algorithms still control visibility- enables some user workarounds through strategic engagement |
| Image recognition systems                                                 | Classification processes are fully opaque; only outcomes are visible                                       | Users typically cannot influence how they are rec- ognized or classified-limited opportunity for user workarounds                                                 |
| High-risk organizational algo- rithms (e.g., hiring, lending, healthcare) | Proprietary systems with minimal transparency or explainability                                            | Users often unaware of algorithmic decision-making and have no input-almost no opportunity for user workarounds                                                   |

or unbiased content. Further, perceptions of discrimination and bias in AI or algorithmic decision-making are themselves influenced by socio-demographics of the user, such as gender, and by occupation or economic contexts (Kim et al. 2024).

Fourth, algorithms, involving a wide variety of programs, technologies, services, uses, and users are fundamentally socio-technical systems (Fazelpour and Danks 2021; Riesen 2025). Correspondingly, an 'algorithmic bias' has both an objective aspect and a subjective aspect, which interact. Therefore, algorithmic bias involves (at least) two conceptually independent bases: more or less existing (an actual, technically objective bias) and more or less perceived (an interpreted, socially subjective bias). While each of these dimensions is a continuum, depending on definitions, system characteristics, and user experiences, we may simplify the intersection into four possible categories: (1) bias that exists but is not perceived, (2) bias that exists and is perceived, (3) bias that is perceived but does not exist, and (4) bias that is not perceived and does not exist. We return to this important distinction later. Table 4 summarizes the categories, with examples.

Table 4 Existing vs. perceived algorithmic bias categories

| Exists   | Algorithmic bias                                                                                                                                                             | Algorithmic bias                                                                                                               |
|----------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------|
|          | Perceived                                                                                                                                                                    | Perceived                                                                                                                      |
|          | Yes                                                                                                                                                                          | No                                                                                                                             |
| Yes      | True positive : Users correctly identify existing bias Ex: Facial recognition system shows measurable bias against certain demograph- ics; users notice and report this bias | False negative : Users fail to notice existing bias Ex: Hiring algorithm discriminates but bias is subtle or hidden from users |
| No       | False positive : Users perceive bias where none exists Ex: Users attribute random variations or technical limitations to systematic bias                                     | True negative : Users correctly do not perceive bias Ex: System operates fairly and users recognize this                       |

<!-- image -->

## 3    Conceptualizing responses to algorithmic bias

Algorithmic bias may affect perceptions of fairness, and outcomes such as acceptance of recommendations, appreciation of and attitudes toward algorithms, and adoption or rejection of the associated systems (Kordzadeh and Ghasemaghaei 2022). The actual or perceived harmful effects of any of the above types of algorithmic bias on diverse user groups may stimulate both individuals and communities to respond, resist, or adapt to, or counter, these biases. Kordzadeh and Ghasemaghaei (2022) note that in spite of considerable research on algorithmic bias, there is little literature on how those biases lead to subsequent decisions or behaviors. While there is currently no standardized terminology to describe such efforts, scholars have proposed several relevant concepts in other domains (e.g., information systems or technology) for analyzing how users non-conform to technological systems. These include appropriation (DeSanctis and Poole 1994), hacking (Herskovitz et al. 2023), reinvention (Rogers 2003, p. 180), and workarounds (Wei et al. 2023) (among others).

## 3.1    Appropriation

Technological appropriation is a socio-technical process where users actively adopt, adapt, and integrate technologies into their practices, often in ways that differ from the technology's intended design or purpose (Bar et al. 2016; DeSanctis and Poole 1994). The theoretical foundations of appropriation, particularly within Adaptive Structuration Theory (DeSanctis and Poole 1994), emphasize the reciprocal relationship between human agency and technological structures-users adapt and reinvent system features based on their needs and contexts, while simultaneously being shaped by the affordances and limitations inherent in the system's design. This mutual structuring occurs through ongoing cycles of interpretation, adaptation, and routinization as users (attempt to) integrate systems into their work practices. Technological appropriation has been used to explain how users often modify or repurpose system features to better align with their specific needs, work routines, and social environments, but such appropriation may also lead to misalignment, derailment, misuse, or failure (Orlikowski 2000). While technological appropriation shares some conceptual similarities with efforts to counter algorithmic bias, such as the capacity for users to modify or adapt technology in ways that diverge from its original design, it ultimately fails to capture a critical nuance of addressing bias in algorithmic systems. The goal is not primarily to modify systems for personal advantage or alternative applications, but to correct, overcome, or bypass biases (existing or perceived) that prevent the technology from functioning correctly or fairly. While this process may yield improved usability as a side effect, its primary purpose is restorative.

## 3.2    Hacking

The term hacking is used to describe various forms of (positive or negative) creative engagement with technology. For example, in disability literature, hacking has been used to describe finding creative ways to adapt assistive tools to both function as intended and to work beyond their intended functions (Herskovitz et al. 2023). In computer science and cyber security literature, the term hacking is used to describe making changes to computer software that may or may not be malicious or outside of the scope of that software (Palmer 2001). These various definitions of 'hacking' may be particularly apt for describing how users circumvent algorithmic bias, as they all involve creatively manipulating software-based systems to better serve user needs. However, the term hacking falls short in multiple respects when used to describe the process of users countering algorithmic bias. First, while hacking can have positive connotations in technical communities, in common usage, the term often implies malicious intent-someone trying to break into or damage a system for nefarious purposes. This negative connotation fails to capture the legitimate and often necessary ways that users adapt and respond to algorithmic systems that exhibit bias. Second, hacking typically, although not always, implies direct modification or intrusion into a system's code, whereas most user adaptations to algorithmic bias occur externally from the code itself. Users rarely have access to or understand the internal workings of proprietary algorithmic systems, particularly in cases involving black-box AI models (O'Neil 2016). Instead of modifying the underlying code, users likely develop strategies to circumvent biases through indirect means, such as altering input data, revising prompts, modifying their behaviors to influence algorithmic outcomes, or strategically interpreting results to mitigate bias effects. Finally, and similarly to technological appropriation, hacking implies a deliberate intent to adapt a system or its uses to improve it or exploit it for unwarranted gain or to improve optimal functionality, whereas many responses to algorithmic bias emerge out of necessity (and preference) rather than intrusion.

## 3.3    Reinvention

Reinvention (a component of diffusion of innovations theory) refers to the adaptation or modification of an innovation after it has been adopted (Rogers 2003). Rather than accepting (or rejecting) an innovation as is, users often engage in reinvention to better suit their particular needs (Rogers 2003). This adaptive process allows users to selectively modify some components or applications of a technology while potentially retaining others (Rice and Rogers 1980). For instance, a social media user may try to reinvent a feed curation algorithm through behaviors such as selectively 'liking' content to influence the algorithm to display more similar content (Burrell et al. 2019). Here, users are modifying the algorithm's original purpose of optimizing engagement and platform revenue into a tool for personal content curation, actively reshaping how the innovation functions to better serve their own goals rather than (primarily) those of the platform. Like technological appropriation and hacking, reinvention captures the idea that users modify technologies to better fit their needs, often through behavioral adjustments, reconceptualizations, or alternative uses. However, when users encounter biased algorithms, their primary motivation is typically to find ways to restore the system's (perceived) intended functionality or avoid specific biases, rather than to personalize or optimize the algorithm for their own, or their group's or organization's benefit. Unlike many cases of reinvention, where users make voluntary (though not necessarily approved) modifications to improve an innovation's usefulness within a particular context (Rice and Rogers 1980; Rogers 2003), responses to algorithmic bias are likely reactive rather than proactive.

<!-- image -->

## 3.4    Workarounds

Workarounds represent strategic, goal-oriented practices through which individuals creatively navigate and overcome systemic barriers or constraints (Alter 2014; Ejnefjäll and Ågerfalk 2019; Spierings et al. 2017). In organizational and information technology contexts, workarounds emerge as intentional adaptations that enable individuals to accomplish tasks or restore system functionality when standard procedures are ineffective, blocked, or misaligned with practical work requirements. Workarounds include internal workarounds, which involve adapting existing (internal) systems in unanticipated ways, and external workarounds, which entail utilizing alternative (external) methods or technologies to achieve the goals (Wei et al. 2023). These adaptive strategies are driven by user motivations to improve efficiency and effectiveness, reduce transaction costs, and overcome systemic limitations (Spierings et al. 2017). While workarounds can be viewed as forms of resistance or improvisation, they also represent critical mechanisms through which individuals actively negotiate and reshape technological environments to better suit their needs (Ejnefjäll and Ågerfalk 2019). This emphasis on overcoming a barrier aligns well with what users are doing when they try to respond to or overcome algorithmic biases. For instance, they might actively seek out underrepresented content to counteract algorithmic amplification biases (Noble 2018). Workarounds may not only be responses to actual biases in algorithmic outcomes but also to users' perceptions of bias (Gruber and Hargittai 2023). These adaptive strategies reflect a broader effort by users to reclaim agency over algorithmic systems and ensure more equitable (among other) outcomes.

## 3.5    Algorithmic bias workarounds

Based on the above comparisons , workarounds is the most relevant term for describing how users respond to algorithmic bias. More specifically, algorithmic bias workarounds are goal-driven attempts by users to overcome, bypass, or reduce perceived or existing algorithmic bias .  These attempts can be internal (modifying aspects of algorithmic systems to circumvent bias) or external (using alternatives instead of the intended system, but as part of the original process or goal) (Alter 2014; Wei et al. 2023). Importantly, workarounds are intentional and not accidental. To identify a workaround in the context of algorithmic bias, several key observations are required. First, there must be evidence of users encountering either existing algorithmic bias (e.g., demonstrable technical or moral systematic discrimination) or perceived algorithmic bias (e.g., beliefs about systematic favoritism). Second, there must be attempts by users to circumvent or overcome these biases through either internal system modifications or in combination with external

<!-- image -->

alternatives. Following Swart's (2021) distinction, internal workarounds can be categorized into explicit actions (such as manually adjusting platform personalization tools) and implicit actions (such as subtly modifying browsing behaviors to counteract algorithmic bias). External workarounds, in contrast, can be systematically identified through observable user behaviors such as turning to alternative platforms, engaging in  manual  information-seeking  processes,  or deliberately circumventing algorithmic recommendation systems to restore perceived fairness. Third, these attempts must be at least aimed at restoring fair functionality rather than merely customizing the system for personal preference, although these customization attempts can, and likely do, serve both to restore fair functionality and to modify the system to work better for the user, such as avoiding certain instances of algorithmic bias. However, as with workarounds in general, such adaptations may create other, subsequent problems, errors, or biases.

## 4    Applying the HAAI-TIME theoretical model for understanding the process of algorithmic bias workarounds

One theoretical model that can illuminate the antecedents to and effects of such algorithmic bias workarounds is the Human-AI Interaction Theory of Interactive Media Effects (HAII-TIME) (Chen and Sundar 2024; Sundar 2020; Sundar et al. 2015). HAII-TIME provides a framework for understanding how technological affordances of interactive media, powered by Artificial Intelligence (AI), influence user psychology and behavior through two primary routes: the cue route and the action route . The cue route examines how interface manifestations of AI trigger cognitive heuristics that shape user perceptions of the AI system, while the action route explores how direct user engagement with AI affects their experience of AI. HAII-TIME emphasizes how machine agency must be balanced with human agency for optimal user experience. In this context, AI refers to users' perceptions of a system as AI-driven, regardless of the extent to which it truly employs artificial intelligence. What matters is that users believe the system utilizes algorithms to perform tasks traditionally carried out by humans, such as image and speech recognition, reasoning, problem-solving, and platform navigation (De Freitas et al. 2023).

## 4.1    Cue route

According to HAII-TIME, when users identify a system as algorithmic or AI-driven, they activate cognitive heuristics related to their perceptions of machines. These heuristics can manifest in various psychological responses, including the 'machine heuristic'-a cognitive shortcut whereby

individuals quickly form judgments about technological systems based on stored mental representations of machine characteristics (Sundar 2020; Yang and Sundar 2024). Such heuristics may trigger phenomena such as automation bias, where users overly trust machine recommendations (Goddard et al. 2012), or algorithm aversion, characterized by a tendency to resist algorithmic decision-making after experiencing an error (Dietvorst et al. 2015). The outcomes of the cue route encompass perceptions of the source, interface, and content. Source perception involves evaluating the trustworthiness of the AI system, interface perception focuses on determining whether the source appears organized or user-friendly, and content perception centers on assessing the credibility and likability of the information presented. Taken together, the cue route suggests that presence of various features triggers heuristics, and collectively shapes users' overall perception of the AI medium. Specifically, in the context of algorithmic bias, the cue route facilitates users' ability to recognize, interpret, and form judgments about potential biases (existing and/or perceived) embedded within the algorithmic system.

## 4.2    Action route

The action route in HAII-TIME examines how users' direct engagement with AI system features shapes the psychological experience of human-AI interaction. Unlike the cue route's focus on perceptual and cognitive responses, the action route explores the substantive interactions between users and AI technologies, investigating how collaborative actions mediate user trust and experience (Sundar 2020). Specifically, in the context of algorithmic bias, the action route provides a mechanism for users to actively develop, test, and implement workarounds to circumvent perceived or real algorithmic biases detected through the cue route. This route is fundamentally concerned with four key dimensions: interaction, agency, social exchange, and mutual augmentation (Sundar 2020). In HAII-TIME, interaction is the direct engagement between users and AI systems, involving user-initiated actions, inputs, and manipulations of AI interfaces that enable communication, customization, and collaborative exploration of the various options allowed by a particular technology. Agency is the negotiation and degree of user control and autonomy within AI-mediated systems, encompassing users' ability to make choices, modify settings, and exert influence over algorithmic processes and outcomes. The ability of a user to exert their agency over a system depends on the affordances of that system as well as user characteristics, such as technical literacy and algorithm awareness. Social exchange is a framework analyzing the perceived costs and benefits of human-AI interactions, where users evaluate the utility, convenience, and value gained from engaging with AI technologies against the cognitive or time-based investments required to do so. Mutual augmentation is a collaborative process where humans and AI systems enhance each other's capabilities, with AI extending human cognitive abilities through data processing and recommendations, while humans provide nuanced context, intuitive understanding, and ethical guidance that refine and improve AI performance. For instance, in medical diagnostics, AI systems can assist in detecting potential anomalies, but human doctors interpret these findings within the broader context of a patient's history, symptoms, and emotional needs, leading to more accurate diagnoses (Kim et al. 2025).

## 4.3    Outcomes

The outcomes of the cue route are trust, credibility, and overall user experience with the system, whereas the outcomes of the action route are the various ways that users engage with the system, which of course in turn may generate their own outcomes. When combined, the cue and the action route ultimately influence the broad user experience with an AIdriven system as well as the overall trust of the medium (Sundar 2020). It is important to note that perceptions of AI through the cue route also shape user actions and engagement through the action route, as users may be evaluating the cues generated by the algorithmic system, which then influence subsequent actions. For example, consider a job recruitment platform where a user notices through the cue route that the AI screening algorithm seems to consistently rank male candidates higher than equally qualified female candidates. This initial perception of algorithmic bias triggers the user's machine heuristic about algorithm bias. In the action route, the user might respond by engaging in algorithmic bias workarounds, such as deliberately diversifying their profile information, adding more gender-neutral language, or seeking alternative platforms that promise more equitable screening processes.

## 4.4    Strengths and limitations

HAII-TIME's strength lies in explaining, through the cue and action pathways, the immediate cognitive and behavioral responses to algorithmic systems (see Table 5). By directly considering algorithmic systems and examining their technological affordances, HAII-TIME provides a good framework for understanding human-algorithm interaction patterns in regard to perceived or existing algorithmic bias, through both cue and action routes. This dual-pathway approach (Sundar 2020) effectively captures both the perceptual and behavioral dimensions of human responses to algorithmic systems, making it useful for understanding how users not only recognize bias but also actively modify their behavior to engage in workarounds. HAII-TIME aligns

Table 5 Summary of HAIITIME as framework for understanding algorithmic bias workarounds

| Dimension         | HAII-TIME                                                                    |
|-------------------|------------------------------------------------------------------------------|
| Theoretical focus | Individual psychological mechanisms and human-AI interaction                 |
| Level of analysis | Individual user experiences and behaviors                                    |
| Temporal scale    | Immediate/real-time interactions                                             |
| Key mechanisms    | Cue route (perceptual) and action route (behavioral)                         |
| Main strengths    | Explains cognitive processes and user responses                              |
| Key limitations   | Limited explanation of social aspects; new framework with limited validation |
| Research methods  | Experimental and user experience studies                                     |
| Suited for        | Studying initial development and implementation of workarounds               |
| Unit of analysis  | Individual user interactions with AI systems                                 |

well with the research objectives of understanding how users first perceive algorithmic bias and subsequently develop and apply workaround strategies.

However, HAII-TIME does not explicitly propose that marginalized groups may experience and respond to algorithmic bias differently based on their social identities and lived experiences (although of course, this could be a salient moderator). Furthermore, the model's focus on individual-level perceptions and behaviors, rather than systematic patterns of bias across subgroups or populations, limits its ability to explain how structural inequalities influence both the occurrence of algorithmic bias and users' strategies for circumventing it. Additionally, the framework's emphasis on technological affordances and interface cues may overlook important social and contextual factors that shape whether and how different user groups develop workarounds in response to perceived or existing bias. For example, marginalized users on social media platforms may adapt their behavior by developing 'folk theories' about how algorithms work and altering their content, such as using coded language, avoiding certain keywords, or changing posting strategies, to navigate perceived biases and reduce the risk of suppression (Mayworm et al. 2024). Notably, these are shaped by collective experience and not necessarily individual technological cues. Finally, although a minor critique, the model is new, and while it has received some empirical validation (Chen and Sundar 2024; Jang et al. 2024; Lee et al. 2023), it has not been thoroughly evaluated.

## 5    Applying the HAII-TIME model and algorithmic bias workarounds to the four categories of algorithmic bias

Given that we have briefly reviewed the nature, prevalence, and types of algorithmic bias, identified four categories from two epistemic bases of algorithmic bias (exists vs. perceived), identified a relevant concept for understanding how users may respond to algorithmic bias (via algorithmic bias workarounds), and presented an appropriate theoretical

<!-- image -->

framework for considering how and when workarounds may be influenced, generated, and have outcomes (HAII-TIME), we can now suggest how the cue route, action route, and user outcomes manifest in each of the four categories. Table 6 provides examples.

## 5.1    Category: exists and perceived

## 5.1.1    Cue route

When algorithmic bias exists and is perceived, HAII-TIME suggests that users encounter various interface cues that trigger cognitive heuristics about the presence of bias in the algorithm. For example, users may notice patterns in content recommendations or search results that appear to systematically disadvantage certain groups (esp. their own), as demonstrated by Noble (2018). This perception can activate the 'machine heuristic' (Sundar 2020). Additionally, certain cues in a biased system may further shape users' detection of bias. For instance, demographic skews in search results may trigger cognitive dissonance if the users already have a competing representativeness heuristic that does not align with what they see in the search results, leading users to resolve this dissonance by concluding that the algorithm mirrors and perpetuates societal inequalities rather than reflecting neutral data processing (Tversky and Kahneman 1974). Similarly, when users repeatedly experience unfavorable outcomes, such as job applicants from marginalized groups consistently receiving lower-ranked recommendations, the availability heuristic may make these instances easier to recall, reinforcing perceptions of systematic discrimination. Once users recognize such bias, they may assume that the algorithm will continue to produce similarly biased outputs. This belief can undermine trust in algorithmic decision-making, prompting users to take steps to engage in workarounds. When applied to algorithmic bias, the cue route is likely the process by which users detect and interpret bias through observable patterns in the system's behavior, while the action route is activated to determine if and how to engage with workarounds.

Table 6 Application of HAII-TIME framework to algorithmic bias scenarios

| User experience outcome   | Varies based on workaround effectiveness: either empowerment and technological resilience with successful workarounds, or helplessness and   | diminished trust when workarounds fail Technological naivety where users continue interac- tion believing in algorithmic neutrality, while sys- temic biases operate beneath conscious perception   | Unnecessary friction and diminished benefits; a technological 'nocebo effect' where negative expectations lead to negative experiences despite absence of objective harm                                 | Technological harmony-users leverage system capabilities without wasting resources on unneces- sary vigilance, leading to higher satisfaction and efficiency   |
|---------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| route                     | Users develop workarounds through four path- ways: interaction, agency, social exchange, mutual augmentation                                 | Normal use without adaptation; unconscious delegation of decision-mak- social exchange: asymmetrical but perceived mutual augmentation: false premise of collaboration                              | unnecessary modifications to interac- patterns; agency: excessive control-asserting social exchange: defensive, adver- stance; mutual augmentation: 'Corrective' that interfere with algorithm function- | straightforward, purposeful engage- agency: appropriate balance of control; exchange: view of AI as reliable partner; augmentation: full potential realized    |
| Action                    | through interface cues that trig- heuristics                                                                                                 | not strong enough or users' in heuristic activation Interaction: agency: ing; as fair; equal                                                                                                        | of interface elements as indicat- Interaction: tion behaviors; sarial behaviors                                                                                                                          | of neutral algorithm func- Interaction: ment; social mutual                                                                                                    |
| Cue route                 | Exists and perceived Users detect bias ger cognitive                                                                                         | Bias cues are either individual differences prevent recognition                                                                                                                                     | Misinterpretation ing bias                                                                                                                                                                               | perceived Correct interpretation tioning                                                                                                                       |
| Scenario                  |                                                                                                                                              | Exists and not perceived                                                                                                                                                                            | Does not exist and perceived                                                                                                                                                                             | Does not exist and not                                                                                                                                         |

## 5.1.2    Action route

Through the action route, when algorithmic bias is perceived, users engage in direct behavioral responses by experimenting with different inputs and observing the algorithm's reactions, gradually building up various workarounds through trial and error. This experiential learning process aligns with research showing that users primarily develop their understanding of algorithmic systems through direct interaction and experience (Cotter and Reisdorf 2020; Siles et al. 2024). These interactions occur through the four pathways of the action route (Sundar 2020).

Interaction:  Many  algorithms  depend  on  interaction (e.g., prompts, or provided content) from the user to create responses and to generate content (Sundar 2020). For example, the TikTok algorithm relies on user action to select content that keeps them engaged (Gerbaudo 2024; Siles et al. 2024). If a user wished to develop a workaround for a particular algorithm through interaction, they could engage in implicit content curation, whereby they try to change the algorithm via mechanisms such as likes, and time spent viewing content, or they could engage in explicit workarounds such as consciously changing the system settings (Swart 2021). Some examples of interaction workarounds might be using virtual private networks (VPNs), incognito mode, and anonymous messaging apps to limit algorithmic tracking/demographic profiling, or strategically liking/ commenting on content to 'train' recommendation algorithms (Siles et al. 2024). However, despite awareness of these potential workarounds, users may not engage in them, depending on how much efficacy or time they have engaging with the system (Swart 2021), and choose to engage in external workarounds, looking for a different tool to accomplish their goal.

Agency: Agency refers to the broader level of control and decision-making power that users choose to exercise in their relationship with AI systems (Sundar 2020). Users can attempt to adjust the level of agency that they want to have over an algorithm by choosing either to passively allow the algorithm to generate content for them or to take an active role and try to customize available settings (Sundar 2020), or through iterative revised uses. When algorithmic bias is perceived, users are likely to try to exert more agency over the algorithm, being more likely to change their settings to combat the bias. Some agency workarounds users may engage in include creating personal rules/boundaries for when to accept vs. override algorithmic recommendations, and choosing manual over algorithmic system options when available.

Social  exchange:  Users  interact  with  AI  systems  as social actors, applying human-like expectations to their interactions. They may perceive AI as a cooperative partner, expecting reciprocal behavior, such as personalized

<!-- image -->

responses or memory of past interactions, which influences trust and engagement (Sundar 2020). When users do not perceive AI as a cooperative actor, such as when they accurately perceive that they are dealing with a biased algorithm, they would likely view it as an unequal exchange and adjust their interactions accordingly. For example, a job recruitment algorithm that systematically disadvantages women (Dastin 2018) violates the perceived fairness of the interaction. Women might view this as an unequal exchange where the AI system fails to provide the promised neutral and objective service, consequently decreasing their trust, and viewing the algorithm as an untrustworthy partner that does not honor the implicit social contract of equitable treatment. This is reflected in findings by Pew Research Center that women are more skeptical of the use of AI hiring algorithms than men (Rainie et al. 2023). Some examples of social exchange workarounds may include selectively sharing information based on the level of perceived bias in the algorithmic system, limiting information exchange until bias is no longer detected within the algorithm, or finding a different job recruitment system all together.

Mutual augmentation: Users and AI systems can enhance each other's capabilities through collaboration, where AI assists users by providing recommendations or automating tasks, while users refine AI outputs by offering feedback or adjustments. This reciprocal interaction allows both human and machine to improve over time, leading to more effective and personalized experiences (Sundar 2020). When algorithmic bias exists and is perceived, the potential for mutual augmentation becomes compromised. Users may become hesitant to provide feedback or training data if they believe that the system is fundamentally biased, while simultaneously feeling greater pressure to correct or compensate for perceived biases or find other technologies. For example, Dietvorst et al. (2015) found that people were less likely to trust an AI MBA admission algorithm once they perceived that it had made errors. This reluctance to trust and engage with the system disrupts the collaborative cycle necessary for mutual augmentation, as users withdraw their participation, limiting the AI's ability to learn and adapt. This type of withdrawal represents an external workaround , as the user stops engaging with the system and looks for differing alternatives. While this reduces the individual's negative outcomes, it can also prevent the system from adapting and improving.

## 5.1.3    User experience outcome

The user experience outcome of the HAII-TIME model will likely vary based on whether algorithm users develop effective  workarounds.  When users perceive bias but fail to create successful workarounds, they may experience a sense of helplessness and diminished trust in the

<!-- image -->

technological system. These users may become increasingly frustrated, feeling that the algorithm's biased outputs are inescapable and that their individual agency is severely limited. In contrast, users who successfully develop workarounds may experience a sense of empowerment and technological resilience, eventually becoming better users of biased algorithms.

## 5.2    Category: exists and not perceived

When algorithm bias exists and is not perceived, HAIITIME would suggest that users are unlikely to engage in workarounds, because they do not recognize the need for intervention. Algorithmic bias must first be detected through interface cues or direct experience before users will initiate corrective actions. Without this awareness, users lack the motivation to question system outputs or develop alternative strategies for achieving their goals.

## 5.2.1    Cue route

Although their may be cues from the system that indicate bias exists, they are either not strong enough to be perceived, the user may have insufficient AI literacy, or there are individual differences in which heuristics are activated to cause users to fail to recognize the bias. Users with limited literacy about how algorithms function and make decisions may struggle to identify bias in these systems (Gran et al. 2021; Hargittai et al. 2020) or properly attribute problematic outcomes to algorithmic bias. Trust in technology also may also play a role in algorithmic bias detection. Users experiencing the machine heuristic or exhibiting high automation bias (Goddard et al. 2012; Sundar 2020) may be less likely to question algorithmic decisions. Users' prior experiences significantly shape their perceptions as well. Previous interactions with similar systems create expectations that influence how users interpret algorithmic outcomes (Gagrčin et al. 2024). If users have primarily encountered unbiased systems or systems with consistent bias patterns, they may normalize these patterns and fail to recognize them as problematic. Finally, positionality likely influences bias detection. Algorithmic bias is considerably more difficult to identify for individuals who are not directly disadvantaged by it. For instance, facial recognition algorithms with lower accuracy rates for darker skin tones (Buolamwini and Gebru 2018) may not register as problematic to users with lighter skin tones who never experience these failures, making systematic bias invisible to those who benefit from or remain unaffected by it.

## 5.2.2    Action route

In scenarios where algorithmic bias exists but is not perceived, the action route of HAII-TIME reveals a passive engagement characterized by uncritical interaction. Users continue to interact with systems without recognizing the potential inequities embedded in their design.

Interaction: User interaction is likely to look like normal use, with individuals continuing to provide data and engage with platforms under the assumption of neutral technological mediation. The algorithm continues to generate content and make decisions without user awareness of its potential discriminatory mechanisms (Sundar 2020). A lack of revised prompts or alternative use strategies due to lack of perceived bias also reinforces the underlying algorithm processes, sustaining or strengthening subsequent bias (Foka et al. 2025).

Agency: Users' agency remains constrained by their lack of awareness, with individuals effectively surrendering decision-making power to algorithmic systems without understanding the possible biases. This unconscious delegation of agency allows systemic biases to operate uncontested, potentially leading to inappropriate or erroneous decisions and behaviors.

Social exchange: The social exchange remains fundamentally asymmetrical, though users are unaware of this imbalance. They continue to view the AI system as a cooperative partner, as reciprocal and fair, while unknowingly participating in, or even reinforcing, potentially discriminatory interactions.

Mutual augmentation: Without perception of bias, mutual augmentation continues under a false premise of equal collaboration. Users provide data and feedback, believing that they are improving the system, while the underlying biased mechanisms remain unchallenged, unchanged, or even strengthened.

## 5.2.3    User experience outcome

The user experience in this scenario is characterized by a state of technological naivety. Individuals continue to interact with algorithmic systems believing in their neutrality, experiencing a form of algorithmic invisibility where systemic biases operate beneath the threshold of conscious perception. Making decisions or applying information from such biased results may lead to negative or harmful outcomes to the immediate user, as well as to other members of their subgroup, to the organization providing the algorithmic tool, and to society as a whole.

## 5.3    Category: does not exist and perceived

When algorithm bias does not exist yet is perceived as such, HAII-TIME would suggest that users develop workarounds to counter an imagined bias, potentially creating inefficiencies or unintended consequences in their interactions with the system. Such an outcome is one of the negative aspects of workarounds; not only may the workaround not actually be based on an accurate understanding of the current system, but also, because of this, users end up implementing actions that generate downstream and displaced obstacles, biases, and workarounds for them and for other users (Rice and Cooper 2010). This misperception could stem from various sources including prior negative experiences with different algorithms (innovation negativism; Rogers 2003), broader skepticism about technology such as algorithm aversion (Dietvorst et al. 2015; Jones-Jang and Park 2023), incorrect prompts or input data, or misattribution of random errors or technological features to bias.

## 5.3.1    Cue route

Through the cue route, users may misinterpret certain interface elements or system behaviors as indicators of bias when none exists. This can occur when users lack direct insight into how the algorithm processes information, leading them to infer discriminatory intent from ambiguous or unfavorable outcomes. For instance, if a hiring algorithm ranks a user's application lower than the user expected, the user may attribute this result to systemic bias rather than considering alternative explanations, such as insufficient qualifications, competition, or model design constraints. If users have high algorithm aversion (Dietvorst et al. 2015; Jones-Jang and Park 2023), this false perception might arise from confirmation bias, where they selectively attend to outcomes that reinforce their preexisting suspicions about algorithmic discrimination while ignoring counterevidence. Misinterpretations may be more pronounced among users with higher algorithm awareness or but limited technical understanding (algorithmic literacy) of how specific algorithms function. Algorithm designers, implementors, and training should invest considerable efforts in designing the tools and increasing users' understanding, to minimize such false perceptions.

## 5.3.2    Action route

In this category, the action route manifests as unnecessary compensatory workarounds. These may be the same types of workarounds that manifest when users accurately perceive algorithm bias; however, they will be ineffective (or, as noted above, harmful both locally and distally, in time, user, and process) as there is no real bias to be compensated for.

## 5.3.3    User experience outcome

The user experience outcome in this scenario is characterized by unnecessary friction and diminished benefits. Users

expend cognitive and behavioral resources (in the form of algorithmic bias workarounds) to combat an imaginary problem, potentially experiencing heightened stress, decreased efficiency, reduced satisfaction, and reinforcement of existing perceptions of bias. Their corrective attempts may introduce inefficiencies or biases into an otherwise neutral or fair system through their modified patterns of engagement. This scenario represents a technological 'nocebo effect' where negative expectations lead to negative experiences despite the absence of any objective flaw or harm in the technology itself (Colloca and Barsky 2020).

## 5.4    Category: does not exist and not perceived

When algorithm bias does not exist and is not perceived, HAII-TIME would suggest that users engage with the system without compensatory behaviors or workarounds, representing an appropriate matching of system reality and user perception. Through the cue route, users correctly interpret the neutral functioning of the algorithm, fostering appropriate levels of trust and reliance. The absence of actual or perceived bias cues allows users to focus on the intended purposes and benefits of the technology rather than being distracted by concerns about potential discrimination or unfairness. In the action route, presuming informed use and appropriate algorithmic literacy, this accurate perception facilitates straightforward, purposeful interaction with the system. Users maintain appropriate agency, neither overdelegating decision-making authority to the algorithm nor unnecessarily restricting, adapting, or bypassing its capabilities. Social exchange follows a pattern where users view the AI system as a reliable partner providing fair, objective service. Mutual augmentation can reach its full potential, with users providing quality input that helps improve algorithmic performance while benefiting from increasingly refined outputs. The resulting user experience is characterized by technological harmony as users can leverage the system's capabilities without wasting cognitive resources on unnecessary vigilance or corrective behaviors, leading to outcomes, such as higher satisfaction, efficiency, or perceived value from the technology.

## 6    Conclusion

Understanding workarounds to algorithmic bias provides insight into the ongoing negotiation between human agency and technological constraint in digital societies. We have examined  how  users  may  respond  to  algorithmic  bias through the lens of the HAAI-TIME model, providing a framework for understanding both the existence and perception of, and responses to, such biases. By identifying 'workarounds' as the most appropriate conceptualization of user responses to algorithmic bias, we have shown how users strategically navigate technological systems to overcome perceived or existing barriers to fair functionality. The analysis of four epistemic categories of algorithmic bias, existing and perceived, existing but not perceived, perceived but not existing, and neither existing nor perceived, demonstrates that cues, actions, and outcomes likely vary based on both the reality and perception of bias. When bias exists and is perceived, users engage in active workarounds that may enhance technological resilience or avoid discriminatory experiences. When bias exists but remains unperceived, users unknowingly participate in systems that may perpetuate inequities. Conversely, when bias is perceived but does not exist, users may create unnecessary inefficiencies and inappropriate algorithm uses through misguided workarounds. The ideal, if perhaps infrequent, situation is when algorithmic bias does not exist and is correctly not perceived.

The framework has important implications for the design, implementation, use, and evaluation of more equitable algorithmic systems. System designers and developers should consider how interface cues might better signal potential biases while avoiding false positives that could trigger unnecessary workarounds. They can leverage insights from workaround patterns to improve algorithmic transparency and user agency, for instance, by implementing clearer feedback mechanisms that help users distinguish between actual bias and random variation, and feedback channels to enable users to indicate when they perceive bias and when or even how they intend to workaround that, or by designing interfaces that facilitate appropriate workarounds when bias does exist while preventing unnecessary ones when it does not. More generally, identifying workarounds and their causes can foster system adaptation and improvement (Wiesche et al. 2024).

Auditors can use workaround behaviors as diagnostic signals, treating patterns of user adaptation as possible indicators of underlying bias. The information systems literature provides considerable guidance on using system data and usage logs, referred to as process mining, along with other methods, to identify some kinds of workarounds (Bartelheimer et al. 2025). Auditors and designers may also run simulations to detect biased results for different socio-economic categories (Lee et al. 2019). Auditors should also attempt to develop methods to assess whether workarounds reflect genuine algorithmic problems or stem from misperceptions that could be addressed through better user education and awareness, especially algorithmic literacy (OeldorfHirsch and Neubaum 2025).

Regulators face the challenge of protecting users from both actual algorithmic bias and the costs of unnecessary workarounds. This might involve recommending algorithmic literacy programs to reduce false and increase accurate perceptions

of bias, requiring transparency measures that enable users to make informed decisions about when workarounds are warranted. Fostering algorithmic literacy among users may help ensure that workarounds are appropriately targeted when bias truly exists, and avoid inappropriate workarounds when bias does not. Regulators can establish standards for explainability that help users understand algorithmic behavior without overwhelming them with technical details, and implementing formal and automated non-discrimination computational and social norms (Criado and Such 2019).

Algorithm operators and associations could create bias impact statements, and regularly apply the associated questions to the development, production, and updating of their systems, and to uses and impacts as assessed by users and other stakeholders (Lee et al. 2019; see their Table 1). The European Union has moved more quickly and comprehensively than the U.S. in developing oversight and criteria for AI governance and trustworthiness (Laux 2024). Our work suggests also including explicit consideration of the identification, uses, and implications of AI bias workarounds in such statements. Importantly, intervention by system designers, auditors, and regulators must be calibrated to avoid undermining legitimate user agency and adaptation strategies while simultaneously addressing systemic issues that individual workarounds cannot solve.

Future research could also explore how collective workaround practices might emerge in different and cross-cultural communities, including with groups who have different perceptions of bias in the same system, and how these adaptive strategies could inform the development of more inclusive algorithmic systems (Malaurent and Karanasios 2020). As algorithmic systems continue to mediate critical aspects of everyday life, recognizing and theorizing how users adapt to bias are essential for designing more equitable technologies and for advancing a fuller, more dynamic understanding of human-AI interaction.

Author contributions All authors contributed to the study conception and design. The first draft of the manuscript was written by H O-T and all authors commented on previous versions of the manuscript. All authors read, discussed, revised, and approved the final manuscript.

Data availability No datasets were generated or analyzed during the current study.

## Declarations

Competing interests The authors declare no competing interests.

Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.

## References

Ajmal A, Boonya-Ananta T, Rodriguez AJ, Du Le VN, RamellaRoman JC (2021) Monte Carlo analysis of optical heart rate sensors in commercial wearables: the effect of skin tone and obesity on the photoplethysmography (PPG) signal. Biomed Opt Express 12(12):7445-7457. https://  doi.  org/  10.  1364/  BOE. 439893

- Alter S (2014) Theory of workarounds. Commun Assoc Inf Syst 34:1041-1066. https://  doi.  org/  10.  17705/  1CAIS.  03455
- AP News (2021) Facebook sorry for 'primates' label on video of Black men. AP News. https://  apnews.  com/  artic  le/  techn  ologybusin  ess-  prima  tes-  5b043  69dea  ecea6  7ba59  a1004  d03a5  f4
- Bar F, Weber MS, Pisani F (2016) Mobile technology appropriation in a distant mirror: baroquization, creolization, and cannibalism. New Media Soc 18(4):617-636. https://  doi.  org/  10.  1177/ 14614  44816  629474
- Bartelheimer  C,  Löhr  B,  Reineke  M,  Aßbrock  A,  Beverungen D (2025) Workarounds as a cause of mismatches in business processes. Bus Inf Syst Eng. https://  doi.  org/  10.  1007/ s12599-  025-  00943-5
- Bhanot K, Baldini I, Wei D, Zeng J, Bennett KP (2022) Downstream fairness caveats with synthetic healthcare data. arXiv:  2203.  04462. arXiv. https://  doi.  org/  10.  48550/  arXiv.  2203.  04462
- Bhuiyan J (2024) She didn't get an apartment because of an AI-generated score - and sued to help others avoid the same fate. The Guardian. https://  www.  thegu  ardian.  com/  techn  ology/  2024/  dec/  14/ safer  ent-  ai-  tenant-  scree  ning-  lawsu  it
- Buolamwini J, Gebru T (2018) Gender shades: intersectional accuracy disparities in commercial gender classification. Proc Mach Learn Res 81(15):77-91
- Burrell J, Kahn Z, Jonas A, Griffin D (2019) When users control the algorithms: values expressed in practices on Twitter. Proc ACM Human-Comput Interac 3(CSCW):138, 1-20. https://  doi.  org/  10. 1145/  33592  40
- Carrell DS, Floyd JS, Gruber S, Hazlehurst BL, Heagerty PJ, Nelson JC, Williamson BD, Ball R (2024) A general framework for developing computable clinical phenotype algorithms. J Am Med Inform Assoc 31(8):1785-1796. https://  doi.  org/  10.  1093/  jamia/ ocae1  21
- Celeste C, Ming D, Broce J, Ojo DP, Drobina E, Louis-Jacques AF, Gilbert JE, Fang R, Parker IK (2023) Ethnic disparity in diagnosing asymptomatic bacterial vaginosis using machine learning. NPJ Digit Med 6(1):1-10. https://  doi.  org/  10.  1038/ s41746-  023-  00953-1

Chen Z (2023) Ethics and discrimination in artificial intelligence-enabled recruitment practices. Humanit Soc Sci Commun 10(1):1-12. https://  doi.  org/  10.  1057/  s41599-  023-  02079-x

Chen C, Sundar SS (2024) Communicating and combating algorithmic bias: effects of data diversity, labeler diversity, performance bias, and user feedback on AI trust. Human Comput Interact. https:// doi.  org/  10.  1080/  07370  024.  2024.  23924  94

Colloca L, Barsky AJ (2020) Placebo and nocebo effects. N Engl J Med 382(6):554-561. https://  doi.  org/  10.  1056/  NEJMr  a1907  805

Cotter K, Reisdorf BC (2020) Algorithmic knowledge gaps: a new horizon of (digital) inequality. Int J Commun 14:21

<!-- image -->

- Criado N, Such JM (2019) Digital discrimination. In: Yeung K, Lodge M (eds) Algorithmic regulation. Oxford University Press, pp 82-91
- Danks D, London AJ (2017) Algorithmic bias in autonomous systems. In: Proceedings of the twenty-sixth international joint conference on artificial intelligence. pp 4691-4697. https://  doi.  org/  10.  24963/ ijcai.  2017/  654
- Das S, Donini M, Gelman J, Haas K, Hardt M, Katzman J, Kenthapadi K, Larroy P, Yilmaz P, Zafar MB (2021) Fairness measures for machine learning in finance. J Financ Data Sci 3(4):33-64. https:// doi.  org/  10.  3905/  jfds.  2021.1.  075
- Dastin J (2018) Amazon scraps secret AI recruiting tool that showed bias against women. Reuters. https://  www.  reute  rs.  com/  artic  le/  usamazon-  com-  jobs-  autom  ation-  insig  ht-  idUSK  CN1MK  08G
- Datta A, Fredrikson M, Ko G, Mardziel P, Sen S (2017) Proxy NonDiscrimination in Data-Driven Systems. arXiv:  1707.  08120. arXiv. https://  doi.  org/  10.  48550/  arXiv.  1707.  08120
- De Freitas J, Agarwal S, Schmitt B, Haslam N (2023) Psychological factors underlying attitudes toward AI tools. Nat Hum Behav 7(11):1845-1854. https://  doi.  org/  10.  1038/  s41562-  023-  01734-2
- DeSanctis  G,  Poole  MS  (1994)  Capturing  the  complexity  in advanced technology use: adaptive structuration theory. Organ Sci 5(2):121-147. https://www.jstor.org/stable/2635011
- Dietvorst BJ, Simmons JP, Massey C (2015) Algorithm aversion: people erroneously avoid algorithms after seeing them err. J Exp Psychol Gen 144(1):114-126. https://  doi.  org/  10.  1037/ xge00  00033
- Ejnefjäll T, Ågerfalk PJ (2019) Conceptualizing workarounds: meanings and manifestations in information systems research. Commun Assoc Inf Syst 45(1):340-363. https://  doi.  org/  10.  17705/  1CAIS. 04520
- Eubanks V (2018) Automating inequality: how high-tech tools profile, police, and punish the poor. St. Martin's Press
- Fazelpour S, Danks D (2021) Algorithmic bias: senses, sources, solutions. Philos Compass 16(8):e12760. https://  doi.  org/  10.  1111/ phc3.  12760
- Foka A, Griffin G, Ortiz Pablo D, Rajkowski P, Badri S (2025) Tracing the bias loop: AI, cultural heritage and bias-mitigating in practice. AI Soc. https://  doi.  org/  10.  1007/  s00146-  025-  02349-z
- Friedman B, Nissenbaum H (1996) Bias in computer systems. ACM Trans Inf Syst 14(3):330-347. https://  doi.  org/  10.  1145/  230538. 230561
- Gagrčin E, Naab TK, Grub MF (2024) Algorithmic media use and algorithm literacy: an integrative literature review. New Media Soc. https://  doi.  org/  10.  1177/  14614  44824  12911  37
- Gerbaudo P (2024) TikTok and the algorithmic transformation of social media publics: from social networks to social interest clusters. New Media Soc. https://  doi.  org/  10.  1177/  14614  44824  13041  06
- Goddard K, Roudsari A, Wyatt JC (2012) Automation bias: a systematic review of frequency, effect mediators, and mitigators. J Am Med Inform Assoc 19(1):121-127. https://  doi.  org/  10.  1136/  amiaj nl-  2011-  000089
- Google Search Central (2023) Google. https://  devel  opers.  google.  com/ search/  docs/  funda  menta  ls/  how-  search-  works. Accessed 28 Feb 2023
- Gran AB, Booth P, Bucher T (2021) To be or not to be algorithm aware: a question of a new digital divide? Information, Communication &amp; Society 24(12):1779-1796. https://  doi.  org/  10.  1080/  13691  18X. 2020.  17361  24
- Gruber J, Hargittai E (2023) The importance of algorithm skills for informed internet use. Big Data Soc 10(1):20539517231168100. https://  doi.  org/  10.  1177/  20539  51723  11681  00
- Hamilton M (2019) The sexist algorithm. Behav Sci Law 37(2):145157. https://  doi.  org/  10.  1002/  bsl.  2406
- Hastie R, Dawes RM (2010) Rational choice in an uncertain world: the psychology of judgment and decision making, 2nd edn. Sage Publications Inc.
- Hargittai E, Gruber J, Djukaric T, Fuchs J, Brombach L. (2020) Black box measures? how to study people's algorithm skills. Information, Communication &amp; Society 23(5):764-775. https://  doi.  org/ 10.  1080/  13691  18X.  2020.  17138  46
- Herskovitz J, Xu A, Alharbi R, Guo A (2023) Hacking, switching, combining: understanding and supporting DIY assistive technology design by blind people. In: Proceedings of the 2023 CHI conference on human factors in computing systems. pp 1-17. https://  doi. org/  10.  1145/  35445  48.  35812  49
- Jang W(, Kwak DH, Bucy E (2024) Knowledge of automated journalism moderates evaluations of algorithmically generated news. New Media Soc 26(10):5898-5922. https://  doi.  org/  10.  1177/  14614 44822  11425  34
- Johnson K (2022) How wrongful arrests based on AI derailed 3 men's lives. Wired. https://  www.  wired.  com/  story/  wrong  ful-  arres  ts-  aiderai  led-3-  mens-  lives/. Accessed 6 Mar 2025
- Jones-Jang SM, Park YJ (2023) How do people react to AI failure? automation bias, algorithmic aversion, and perceived controllability. J Comput-Mediat Commun 28(1):zmac029. https://  doi.  org/  10. 1093/  jcmc/  zmac0  29
- Kim S, Oh P, Lee J (2024) Algorithmic gender bias: investigating perceptions of discrimination in automated decision-making. Behav Inf Technol 43(16):4208-4221. https://doi.org/10.1080/01449 29X.2024.2306484
- Kim SH, Wihl J, Schramm S, Berberich C, Rosenkranz E, Schmitzer L, Serguen K, Klenk C, Lenhart N, Zimmer C, Wiestler B, Hedderich DM (2025) Human-AI collaboration in large language modelassisted brain MRI differential diagnosis: a usability study. Eur Radiol. https://  doi.  org/  10.  1007/  s00330-  025-  11484-6
- Kordzadeh N, Ghasemaghaei M (2022) Algorithmic bias: review, synthesis, and future research directions. Eur J Inf Syst 31(3):388409. https://doi.org/10.1080/0960085X.2021.1927212
- Kuncel NR, Klieger DM, Connelly BS, Ones DS (2013) Mechanical versus clinical data combination in selection and admissions decisions: a meta-analysis. J Appl Psychol 98(6):1060-1072. https:// doi.  org/  10.  1037/  a0034  156
- Laqueur HS, Copus RW (2022) An algorithmic assessment of parole decisions. J Quant Criminol 40:151-188. https://  doi.  org/  10.  1007/ s10940-  022-  09563-8
- Laux J (2024) Institutionalised distrust and human oversight of artificial intelligence: towards a democratic design of AI governance under the European Union AI Act. AI Soc 39(6):2853-2866
- Lee S, Moon W-K, Lee J-G, Sundar SS (2023) When the machine learns from users, is it helping or snooping? Comput Hum Behav 138:107427. https://  doi.  org/  10.  1016/j.  chb.  2022.  107427
- Lee NT, Resnick P, Barton G (2019) Algorithmic bias detection and mitigation: best practices and policies to reduce consumer harms. The Brookings Institute. https://  www.  brook  ings.  edu/  artic  les/  algor ithmic-  bias-  detec  tion-  and-  mitig  ation-  best-  pract  ices-  and-  polic  iesto-  reduce-  consu  mer-  harms/
- Malaurent J, Karanasios S (2020) Learning from workaround practices: the challenge of enterprise system implementations in multinational corporations. Inf Syst J 30(4):639-663. https://doi. org/10.1111/isj.12272
- Mayworm S, DeVito MA, Delmonaco D, Thach H, Haimson OL (2024) Content moderation folk theories and perceptions of platform spirit among marginalized social media users. ACM Trans Soc Comput 7(1-4):1-27. https://  doi.  org/  10.  1145/  36327  41
- Merton  RK  (1968)  The  Matthew  effect  in  science.  Science 159(3810):56-63. https://doi.org/10.1126/science.159.3810.5
- Metaxa D, Gan MA, Goh S, Hancock J, Landay JA (2021) An image of society: gender and racial representation and impact in image

<!-- image -->

- search results for occupations. Proc ACM Human Comput Interact 5(CSCW1):1-23. https://  doi.  org/  10.  1145/  34491  00
- National Institute of Standards and Technology (NIST) (2019) Face recognition vendor test (FRVT) Part 3: demographic effects (NISTIR 8280). U.S. Department of Commerce. https://  doi.  org/ 10.  6028/  NIST.  IR.  8280
- Noble SU (2018) Algorithms of oppression: how search engines reinforce racism. New York University Press, New York. https://  doi. org/  10.  18574/  nyu/  97814  79833  641.  001.  0001
- O'Neil C (2016) Weapons of math destruction: how big data increases inequality and threatens democracy. Crown Publishing Group
- Obermeyer Z, Powers B, Vogeli C, Mullainathan S (2019) Dissecting racial bias in an algorithm used to manage the health of populations. Science 366(6464):447-453. https://  doi.  org/  10.  1126/  scien ce.  aax23  42
- Oeldorf-Hirsch A, Neubaum G (2025) What do we know about algorithmic literacy? the status quo and a research agenda for a growing field. New Media Soc 27(2):681-701. https://doi. org/10.1177/14614448231182662
- Olteanu A, Castillo C, Diaz F, Kıcıman E (2019) Social data: biases, methodological pitfalls, and ethical boundaries. Front Big Data. https://  doi.  org/  10.  3389/  fdata.  2019.  00013
- Orlikowski WJ (2000) Using technology and constituting structures: a practice lens for studying technology in organizations. Organ Sci 11(4):404-428. https://  doi.  org/  10.  1287/  orsc.  11.4.  404.  14600
- Overbye-Thompson H, Hamilton KA, Mastro D (2024) Reinvention mediates impacts of skin tone bias in algorithms: implications for technology diffusion. J Comput-Mediat Commun 29(5):zmae016. https://  doi.  org/  10.  1093/  jcmc/  zmae0  16
- Palmer CC (2001) Ethical hacking. IBM Syst J 40(3):769-780. https:// doi.  org/  10.  1147/  sj.  403.  0769
- Pelly L (2025) Mood machine: the rise of Spotify and the costs of the perfect playlist. First One Signal Publishers/Atria Books hardcover edition. New York: One Signal Publishers/Atria.
- Pew Research Center (2017) Code-dependent: pros and cons of the algorithm age. https://  www.  pewre  search.  org/  inter  net/  2017/  02/  08/ code-  depen  dent-  pros-  and-  cons-  of-  the-  algor  ithm-  age/
- Rainie L, Anderson M, McClain C, Vogels EA, Gelles-Watnick R (2023) 1. Americans' views on use of AI in hiring. Pew Research Center. https://  www.  pewre  search.  org/  inter  net/  2023/  04/  20/  ameri cans-  views-  on-  use-  of-  ai-  in-  hiring/
- Ray I, Liaqat D, Gabel M, de Lara E (2021) Skin tone, confidence, and data quality of heart rate sensing in WearOS smartwatches. In: 2021 IEEE international conference on pervasive computing and communications workshops and other affiliated events (PerCom workshops. pp 213-219. https://  doi.  org/  10.  1109/  PerCo  mWork shops  51409.  2021.  94311  20
- Ren Q, Heacock H (2022) Sensitivity of infrared sensor faucet on different skin colours and how it can potentially effect equity in public health. BCIT Environ Public Health J. https://  doi.  org/  10. 47339/  ephj.  2022.  216
- Resende GH, Nery LF, Benevenuto F, Zannettou S, Figueiredo F (2024) A comprehensive view of the biases of toxicity and sentiment analysis methods towards utterances with African American English expressions. arXiv. https://  doi.  org/  10.  48550/  arXiv.  2401. 12720
- Rice RE, Rogers EM (1980) Reinvention in the innovation process. Knowledge 1(4):499-514. https://  doi.  org/  10.  1177/  10755  47080 00100  402
- Rice R E, &amp; Cooper S. (2010) Organizations and unusual routines: a systems analysis of dysfunctional feedback processes, Cambridge University Press, Cambridge, UK
- Riesen E (2025) A sociotechnological-system approach to AI ethics. AI &amp; Soc. https://  doi.  org/  10.  1007/  s00146-  025-  02335-5
- Rogers EM (2003) Diffusion of innovations. Free Press, New York Shumailov I, Shumaylov Z, Zhao Y, Papernot N, Anderson R, Gal Y (2024) AI models collapse when trained on recursively generated data. Nature 631(8022):755-759. https://  doi.  org/  10.  1038/ s41586-  024-  07566-y
- Siles I, Valerio-Alfaro L, Meléndez-Moran A (2024) Learning to like TikTok … and not: algorithm awareness as process. New Media Soc 26(10):5702-5718. https://  doi.  org/  10.  1177/  14614  44822 11389  73
- Spierings A, Kerr D, Houghton L (2017) Issues that support the creation of ICT workarounds: towards a theoretical understanding of feral information systems. Inf Syst J 27(6):775-794. https://  doi. org/  10.  1111/  isj.  12123
- Stinson  C  (2022)  Algorithms  are  not  neutral:  bias  in  collaborative filtering. AI Ethics 2(4):763-770. https://  doi.  org/  10.  1007/ s43681-  022-  00136-w
- Sundar SS (2020) Rise of machine agency: a framework for studying the psychology of human-AI interaction (HAII). J ComputMediat Comm 25(1):74-88. https://  doi.  org/  10.  1093/  jcmc/  zmz026
- Sundar SS, Jia H, Waddell TF, Huang Y (2015) Toward a theory of interactive media effects (TIME). In: Sundar SS (ed) The Handbook of the psychology of communication technology. John Wiley &amp; Sons Ltd, pp 47-86. https://  doi.  org/  10.  1002/  97811  18426  456. ch3
- Swart J (2021) Experiencing algorithms: how young people understand, feel about, and engage with algorithmic news selection on social media. Soc Media Soc 7(2):20563051211008828. https:// doi.  org/  10.  1177/  20563  05121  10088  28
- Sweeney L (2013) Discrimination in online ad delivery. Commun ACM 56(5):44-54. https://  doi.  org/  10.  1145/  24479  76.  24479  90
- Tversky A, Kahneman D. (1974) Judgment under Uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty. Science 185(4157):1124-1131. https://  doi.  org/  10.  1126/  scien  ce.  185.  4157.  112
- Van Dijk J (2020) The digital divide. John Wiley &amp; Sons
- Wei S, Chen X, Rice RE (2023) We can work it out: a multilevel examination of relationships among group and individual technology workarounds, and performance. J Oper Manag 69(6):1008-1038. https://  doi.  org/  10.  1002/  joom.  1267
- Wiesche M, Böhm N, Schermann M (2024) Digital desire paths: exploring the role of computer workarounds in emergent information systems design. Eur J Inf Syst 33(2):145-160. https://doi. org/10.1080/0960085X.2024.2313537
- Yang H, Sundar SS (2024) Machine heuristic: concept explication and development of a measurement scale. J Comput-Mediat Commun 29(6):zmae019. https://  doi.  org/  10.  1093/  jcmc/  zmae0  19
- Zou J, Schiebinger L (2018) AI can be sexist and racist-it's time to make it fair. Nature 559(7714):324-326. https://  doi.  org/  10.  1038/ d41586-  018-  05707-8

Publisher's  Note Springer  Nature  remains  neutral  with  regard  to jurisdictional claims in published maps and institutional affiliations.

<!-- image -->