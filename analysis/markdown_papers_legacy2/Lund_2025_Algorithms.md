---
source_file: Lund_2025_Algorithms.pdf
conversion_date: 2025-11-02T17:34:47.536950
---

<!-- image -->

<!-- image -->

<!-- image -->

This report was originally written and published in Norwegian. The English version has been produced with the active involvement of the author. Some minor adjustments have been made to ensure that the English version of the report is up to date at the time of its publication. The author would like to thank John Carville for excellent work on copy-editing the report.

The Norwegian version of the report, entitled Algoritmer, kunstig intelligens og diskriminering. En analyse av likestillings- og diskrimineringslovens muligheter og begrensninger , was published in May 2024 and can be found at:

https://ldo.no/content/uploads/2024/06/ldo\_algoritmer\_ki\_og\_diskriminering\_\_elektr onisk\_versjon.pdf.

Vibeke Blaker Strand is a Professor of Law at the Department of Public and International Law of the University of Oslo, Norway. She can be contacted at: v.b.strand@jus.uio.no.

The Equality and Anti-Discrimination Ombud

LDO 2025

ISBN: 978-82-8320 -035-5 English electronic version

Cover of the report

Illustration description: Index finger presses graphics on a transparent screen.

Credit: Incept-G

## Content

| Summary   | Summary                                                                                      | 5                                                                                 |
|-----------|----------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|
| 1         | Introduction                                                                                 | 9                                                                                 |
| 2         | Structure of the report                                                                      | 11                                                                                |
|           | 2.1 Aim                                                                                      | 11                                                                                |
|           | 2.2 Terminology                                                                              | 11                                                                                |
|           | 2.3 Limitations                                                                              | 13                                                                                |
|           | 2.4 Legal sources                                                                            | 14                                                                                |
|           | 2.5 Structure                                                                                | 15                                                                                |
| 3         | Lines from the prohibition against discrimination to the regulation of artificial            | Lines from the prohibition against discrimination to the regulation of artificial |
|           | intelligence and the protection of informational privacy                                     | 16                                                                                |
|           | 3.1 Introduction                                                                             | 16                                                                                |
|           | 3.2 The AI Act                                                                               | 16                                                                                |
|           | 3.3 The General Data Protection Regulation (GDPR)                                            | 18                                                                                |
|           | 3.4 Summary and reflections going forward                                                    | 19                                                                                |
| 4         | The prohibition against direct and indirect discrimination                                   | 21                                                                                |
|           | 4.1 Introduction                                                                             | 21                                                                                |
|           | 4.2 Some starting points related to the prohibition against direct discrimination            | 21                                                                                |
|           | 4.3 Some starting points related to the prohibition against indirect discrimination          | 22                                                                                |
|           | 4.4 Differential treatment                                                                   | 24                                                                                |
|           | 4.5 The connection between the differential treatment and one or more discrimination grounds | 26                                                                                |
|           | 4.6 Lawful differential treatment                                                            | 28                                                                                |
|           | 4.7 Timing of discrimination - particularly in relation to content-based discrimination      | 32                                                                                |
|           | 4.8 Burden of proof in discrimination law                                                    | 33                                                                                |
|           | 4.9 Summary and reflections going forward                                                    | 37                                                                                |

| 5          | The actors: Who are protected against discrimination and who may be held                               |   The actors: Who are protected against discrimination and who may be held |
|------------|--------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------|
|            | liable?                                                                                                |                                                                         41 |
|            | 5.1 Introduction                                                                                       |                                                                         41 |
|            | 5.2 Protected groups                                                                                   |                                                                         41 |
|            | 5.3 Against whom should a discrimination claim be directed?                                            |                                                                         42 |
|            | 5.4 Can the prohibitions against instruction and participation widen the circle of responsible actors? |                                                                         44 |
|            | 5.5 Summary and reflections going forward                                                              |                                                                         47 |
| 6          | Enforcement and sanctions                                                                              |                                                                         49 |
|            | 6.1 Introduction                                                                                       |                                                                         49 |
|            | 6.2 The possibility for organizations to have questions of algorithmic discrimination enforced         |                                                                         49 |
|            | 6.3 The sanctioning competence of the Anti-Discrimination Tribunal and the ordinary courts             |                                                                         51 |
|            | 6.4 Summary and reflections going forward                                                              |                                                                         53 |
| References | References                                                                                             |                                                                         55 |
|            | Works cited                                                                                            |                                                                         55 |
|            | Legal instruments and cases                                                                            |                                                                         57 |

## Summary

To ensure effective protection against discrimination in the face of novel technology, it is essential that relevant legislation evolves to encompass new and emerging issues. This report reviews and analyses key elements of the Norwegian Equality and Anti-Discrimination Act (EAD Act), with a primary emphasis on the prohibition against discrimination. The Act implements EU/EEA anti-discrimination directives into Norwegian law. The closer relation between the EAD Act and the EU/EEA antidiscrimination directives is investigated throughout the report. The point of departure for the report's analysis is that the EAD Act is applicable to cases of algorithmic discrimination. At the same time, however, the report highlights a number of challenges for the current legislation.

As a whole, the report reveals the need for a revision of the Equality and AntiDiscrimination Act with an eye to the implementation of specific regulations that would ensure that algorithmic discrimination is addressed in a more precise and distinctive manner than is currently the case. The introduction of regulation directed specifically at algorithmic discrimination would play an important role in clarifying the possibilities for enforcing the prohibition on discrimination in individual cases. Such regulation might also have a preventive effect, particularly in relation to public and private actors utilizing algorithmic systems as part of their activities.

Chapter 3 of this report highlights a number of important links between the prohibition against discrimination in the Equality and Anti-Discrimination Act, the EU Artificial Intelligence Act (AI Act) and the EU General Data Protection Regulation (GDPR).

- The AI Act specifies that existing EU regulation on discrimination is not impacted by the Act. This means that it will be necessary to analyse and develop the content of the prohibition against discrimination on the basis of the premises of the discrimination legislation itself.
- Article 10(5) of the AI Act ties together the three sets of rules (i.e. the prohibition against discrimination, the AI regulation and the GDPR). Under certain conditions, the provision opens for the processing of special categories of personal data ' for the purpose of ensuring bias detection and correction in relation to … high -risk AI systems'. Going forward, Article 10(5) may become an important tool for preventing discrimination.
- The report discusses the possibility of introducing specific provisions into the Equality and Anti-Discrimination Act regarding activity- and reporting-related duties for actors utilizing automated and decision-support algorithms and AI systems in their activities.

Chapter 4 of the report analyses the Equality and AntiDiscrimination Act's prohibition against direct and indirect discrimination.

- The chapter discusses how algorithmic systems and artificial intelligence challenge the legal distinction between direct and indirect differential treatment. Discrimination may occur through algorithmic systems' emphasis on proxy factors ( ' proxy discrimination ' ). The use of proxy factors riggers new issues, particularly in relation to the prohibition against indirect discrimination.

Also, through its practice, the Court of Justice of the European Union has expanded the framework for what is to be regarded as direct differential treatment. In the light of these developments, the report discusses the possible introduction of specific definitions of direct and indirect algorithmic differential treatment into the Equality and Anti-Discrimination Act.

- The report suggests that consideration might be given to the creation of a specific provision on lawful algorithmic differential treatment. The current provision on lawful differential treatment in Section 9 of the Equality and AntiDiscrimination Act is constructed around the need to provide particularly strong protection against discrimination in working life, and the distinction between direct and indirect differential treatment has implications for the strictness of review that is carried out when the provision is interpreted. Both of these conditions are challenged by algorithmic differential treatment. Algorithmic systems are now used across all areas of society and contribute in various ways to decisions that may be of considerable importance to the individuals they concern. This means that the premise that working life is of particular importance for the prohibition on discrimination may need to be reassessed. Furthermore, the complexity of algorithmic systems complicates the distinction between direct and indirect differential treatment (see also the previous bullet point). In the light of these challenges, the possibility of introducing into the legislation a separate and more specific provision on lawful algorithmic differential treatment is presented.
- The special rule of the burden of proof in cases of discrimination may play a crucial role in efforts to address algorithmic discrimination. Lessons may be learnt from the field of equal pay, as opaque pay setting systems make it difficult to substantiate claims for discrimination. Particular attention should be drawn to Article 18 of the EU Pay Transparency Directive (Directive 2023/970), a provision that integrates pay transparency obligations for employers into the burden of proof rule. This may serve as inspiration to questions of algorithmic discrimination -for instance, in relation to the ' black box ' challenge.

Chapter 5 focuses on the different actors that may be involved in cases of discrimination.

- The report directs attention to the Equality and AntiDiscrimination Act's reliance on a closed list of discrimination grounds and highlights how algorithmic systems may lead to new individuals and groups being subjected to differential treatment. Factors such as social status, health and level of education, for example, all fall outside the EAD Act's current list of discrimination grounds. The issue of algorithmic discrimination therefore highlights the need for discussion on whether the EAD A ct's approach to discrimination grounds may require increased flexibility. The inclusion of a separate provision banning algorithmic discrimination might include both a positive list of discrimination grounds (as in Section 6 of the EAD Act) and the introduction of an open-ended category.

- It is emphasized that individual claims in cases of alleged discrimination should be directed against the private or public actors that have made a particular decision, issued a particular ruling, etc., regarding the individual(s) concerned and have used an algorithmic system as part of their activities.
- The report shows how the Equality and AntiDiscrimination Act's prohibition s against instruction and participation in discrimination may make it possible to expand the circle of actors that might be held liable for algorithmic discrimination -for instance, so that developers could also be held liable in some cases. However, the substance of these two prohibitions is currently underdeveloped.

Chapter 6 discusses issues of enforcement and sanctioning in relation to algorithmic discrimination.

- The analysis highlights the potential importance of the ability of relevant organizations to seek enforcement in cases of algorithmic discrimination, particularly since there may be cases where there are no private individuals seeking to bring a particular case of algorithmic discrimination before the Norwegian Anti-Discrimination Tribunal or the ordinary courts.
- EU/EEA anti-discrimination directives require effective sanctioning in relation to the prohibition against discrimination. This also applies in cases where an organization is the complainant or plaintiff.
- The fact that the Tribunal has limited sanctioning competence in cases outside working life may make it more appropriate to bring cases of algorithmic discrimination before the ordinary courts. This reveals the need for discussion on the role of the Tribunal in relation to algorithmic discrimination.
- The roles of the Norwegian Equality and Anti-Discrimination Ombud (LDO) and relevant interest organizations in ensuring that the prohibition on algorithmic discrimination is effectively enforced and sanctioned will be important in the time ahead. For example, they could take an active role in bringing cases of algorithmic discrimination before the courts or the Anti-Discrimination Tribunal, or they could initiate efforts related to the supervision and monitoring of algorithmic systems and their effects, based on the prohibition against discrimination.

In sum, future efforts to address algorithmic discrimination will need to build on the terms of the substantive prohibition against discrimination as currently regulated by Sections 6 -9 of the EAD Act as well as to explore how the formulation of that prohibition might be adjusted to address the new challenges posed by the use of algorithmic systems.

## 1 Introduction

The Equality and Anti-Discrimination Act (EAD Act) 1  was initially set up to address questions related to unlawful differential treatment in interpersonal relations, irrespective of whether such treatment takes place within the workplace, within an educational facility, in connection with the renting of accommodation or within the social sphere. The use of algorithmic systems and artificial intelligence (AI) challenges the ideas on which the EAD Act was based.

It has been well documented that algorithms and AI can lead to infringements of individuals' fundamental rights, including the right not to be discriminated against. When public and private enterprises employ new technology and questions about possible discrimination arise as a result of the contribution made by algorithms to actions and decisions directed at individuals, the rules and categories of the Equality and Anti-Discrimination Act are challenged.

The prohibition against discrimination is relatively flexible in terms of its structure. This flexibility is related to the fact that the prohibition is applicable across all areas of life and law. Broadly formulated conditions for discrimination have proven useful and have made it possible to integrate contextual factors when the prohibition is enforced. 2  The built-in flexibility in the way in which discrimination is defined has also contributed to the development of the content of the prohibition against discrimination over time. This has enabled the law to respond to new societal challenges. The Court of Justice of the European Union has played a central part in this context -for instance, through its contributions to the development of the protection against indirect discrimination, the protection against discrimination by association and the special rule on the burden of proof in discrimination law.

At the same time, discussion continues to take place on how the prohibition against discrimination should be interpreted. How to delineate between direct and indirect discrimination, for example, has proven to be a challenging issue. How the prohibition might be further developed also continues to be the subject of debate. Within both Norway and the EU, there are ongoing discussions over what discrimination grounds should be included in the prohibition. In a report on algorithms, discrimination and EU non-discrimination law, Gerards and Xenidis note that ' algorithmic discrimination shines a new light on many of the ' traditional ' problems and critiques of EU gender equality and non-discrimination law ' . 3

This statement is apt. The new technologies that are the subject of the current report are encountering a regulatory framework that has been developed and applied over a period of several decades to differential treatment occurring in entirely different

1 Lov om likestilling og forbud mot diskriminering (likestillings- og diskrimineringsloven) [Act Relating to Equality and a Prohibition Against Discrimination (Equality and Anti-Discrimination Act)], 16 June 2017 no. 51 (hereafter 'the Equality and Anti-Discrimination Act' or 'the EAD Act').

2 The prohibition against discrimination can be seen as a consequence of so-called contextual equality; see Wachter et al. (2021).

3 Gerards and Xenidis (2020) p. 9.

arenas, with a particular focus on workplace discrimination. The complexity surrounding the use of algorithmic systems -including AI systems -means that existing regulations on discrimination are being confronted with a range of entirely new issues. As a result, key regulations and conditions within existing discrimination law must be interpreted in a new context.

This report examines the intersection between the prohibition against discrimination as set out under the Equality and Anti-Discrimination Act and issues of discrimination triggered by the use of algorithms and artificial intelligence.

## 2 Structure of the report

## 2.1 Aim

This report seeks to identify and highlight the possibilities and limitations embedded in the Equality and AntiDiscrimination Act's prohibition against discrimination in the light of issues of discrimination triggered by the use of algorithmic systems.

In addition, the report seeks to identify possibilities for the further development of the existing protections against discrimination, either through changes in how they are interpreted or through adjustments to the current legislation. In this way, the report aims to contribute to discussions on how to ensure robust protection against discrimination in the face of rapid technological development.

Interspersed throughout the report are comments on the relationship between Norway's Equality and Anti-Discrimination Act and EU/EEA non-discrimination law. These comments have been included in the report because there are certain differences between the two regulatory frameworks that may be relevant for any attempt to address issues related to algorithms, artificial intelligence and discrimination. It should be noted, however, that the Norwegian Equality and AntiDiscrimination Act was designed to implement the EU/EEA 's anti-discrimination directives into Norwegian law. 4

The report is based on the premise that the prohibition against discrimination constitutes a crucial starting point for any attempt to address algorithmic discrimination. This means not only that knowledge about the content and scope of the prohibition against discrimination will be important in any assessment of whether discrimination has occurred after a system has been put into operation, but also that insight into the content and scope of the protection against discrimination will be important when algorithmic systems (including AI systems) are being developed and tested. 5

## 2.2 Terminology

An algorithm is a set of instructions that describes step by step what should be done to solve a problem or achieve a particular result. 6  Algorithms can be used for many purposes.

A distinction can be made between fixed algorithms and learning algorithms. While fixed algorithms are characterized by being rule-based and based on a specific set of instructions that make the types of conclusions that the system will reach somewhat

4  This includes the Race Equality Directive (2000/43/EC), the Equality Framework Directive (2000/78/EC), the Equal Treatment Directive (2006/54/EC) and the Equal Treatment in Goods and Services Directive (2004/113/EC).

5 The relevance of the prohibition against discrimination also in the pre-deployment phase is established and further developed in Mathias Karlsen Hauglid's doctoral thesis; see Hauglid (2024) pp. 14-15.

6  Strümke (2023) p. 18.

predictable, learning algorithms evolve dynamically. Both fixed and learning algorithms are relevant when addressing questions of discrimination.

The term ' artificial intelligence ' (AI) is a broad one and encompasses many forms of algorithms, machine learning, models and statistical methods. Article 3(1) of the EU Artificial Intelligence Act defines 'AI system' as follows :

'AI system' means a machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments. 7

The expression ' may exhibit adaptiveness after deployment ' refers to systems that are able to change or adapt after being put to use. According to the Preamble to the AI Act, the definition of 'AI system' should be based on a ' key characteristic of AI systems ,' namely, ' their capability to infer ' . The definition in Article 3(1) should distinguish 'AI systems' from ' simpler traditional software systems or programming approaches and should not cover systems that are based on the rules defined solely by natural persons to automatically execute operations ' . 8  The Act thus draws a line between fixed algorithms and learning algorithms, whereby only learning algorithms are included in the AI Act's definition of an ' AI system ' . 9  In this report, I base my understanding of the term ' AI system ' on the definition provided in the EU AI Act.

There are different forms of learning algorithms. In discussions on AI and discrimination, attention is often given to the use of machine learning systems. Such systems are characterized by algorithms being taught to find patterns in large amounts of data and to autonomically adapt, develop and improve on the basis of those data. Another form of learning algorithm is that of so-called deep learning algorithms. These are artificially intelligent systems that are able to connect different technologies and algorithms. 10

In the present report, the term ' algorithmic discrimination ' applies to the entire range of algorithmic systems, meaning that both fixed algorithms and learning algorithms (including AI and machine-learning systems) are encompassed. 11  I also use the term ' algorithmic systems ' in a related fashion. To only include ' AI systems ' within the analysis of the report would be too narrow, as the adoption of such an approach

7  Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 Laying Down Harmonised Rules on Artificial Intelligence and Amending Regulations (EC) No. 300/2008, (EU) No. 167/2013, (EU) No. 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act).

8 Artificial Intelligence Act, Regulation (EU) 2024/1689, Preamble Recital 12.

9 Haram (2021) p. 18.

10 In their presentation of algorithms, Gerards and Xenidis differentiate between 'rule-based algorithms', 'machine-learning algorithms', 'deep learning', and 'enabling technologies and combining algorithms'; see Gerards and Xenidis (2020) pp. 32-36.

11 This term is also used by, for example, Gerards and Xenidis (2020).

would exclude fixed algorithms. The term ' AI systems ' is used where the topic under discussion is particularly relevant to those types of algorithmic systems.

When algorithms influence decisions relevant to individuals, the degree of human involvement in the decision-making process can vary. On one hand, there are fully automated decisions that may derive from both fixed and learning algorithms. 12  On the other hand, there are situations where algorithmic systems are used to support a human who makes the ultimate decision (e.g. a doctor, employer or public official). The use of both fully automated and decision-supporting algorithms and AI systems may trigger questions about discrimination -and both types of use are therefore covered by this report. 13

## 2.3 Limitations

This report will not go into technical aspects of the use of algorithms and AI systems. Nor will it discuss typical examples that illustrate how algorithmic discrimination may occur. Examples of areas where AI brings risks for discrimination are provided by, for example, Borgesius, who identifies and discusses algorithmic systems that are used in the contexts of crime prevention, selection of employees and students, advertising, pricing, image search and analysis, and translation. 14

The development and deployment of algorithms and AI systems involves several different phases, and questions related to discrimination may arise in any of these phases. In 2023, the Norwegian Equality and Anti-Discrimination Ombud issued a guide aimed at the detection and prevention of discrimination in connection with the development and use of machine learning systems. The guide is structured in terms of five phases: planning, training data, model development, testing, and implementation and review. 15  In this report, I do not explicitly delve into the specific types of challenges that are present during each of these different phases. That issue has been addressed both in the Ombud's guide and in other reports on algorithms and AI-related discrimination. 16  The main focus of the present report is the legal content of the prohibition against discrimination. At times, however, as part of the legal analysis of the prohibition against discrimination, lines will be drawn to the testing and development phases.

The report focuses on the right of the individual not to be discriminated against. Structural dimensions of the prohibition against discrimination, including proactive

12  Gerards and Xenidis (2020) pp. 39-40.

13 Similarly, see Borgesius (2018) p. 11. See also Schartum (2019) p. 25, where Schartum differentiates between 'legal decision-making systems' [ rettslige beslutningssystemer ] and 'decision support systems' [ beslutningsstøttesystemer ]. Legal decision-making systems are characterized by 'the result from the system forming the basis of the decision', while a decision support system is characterized by the data systems supporting case processing, but where a human has control over the final content of a decision.

14 Borgesius (2018) pp. 23ff.

15  See Likestillings- og diskrimineringsombudet [the Equality and Anti-Discrimination Ombud] (2023).

16 See, for example, Gerards and Xenidis (2020) pp. 37ff.; Borgesius (2018) pp. 15ff.

duties established by the EAD Act that require active measures, documentation and reporting, are only briefly touched upon. 17

AI systems can lead to discrimination, but such systems may also be an important tool for uncovering systemic inequality and potential discrimination. AI systems may thus serve as tools for combatting discrimination. 18  The opportunities this represents will not be explored in this report.

## 2.4 Legal sources

The prohibition against discrimination in the EAD Act serves as the point of departure for this report. At the time of writing, there are few examples of national and international enforcement practice that directly illuminate the content and meaning of the prohibition against discrimination in the context of algorithm- and AI-related discrimination.

At the domestic level in Norway, the significance of the prohibition against discrimination in relation to algorithms and AI systems has not been addressed in the published academic literature. However, some attention has been devoted to algorithmic discrimination in reports and literature on EU/EEA non-discrimination law. 19  As the EAD Act implements EU/EEA non-discrimination directives into Norwegian law, these works have been of interest for this report.

Mathias Karlsen Hauglid 's 2024 doctoral thesis Bias and Discrimination in Clinical Decision Support Systems Based on Artificial Intelligence has been a major source of inspiration. 20  In the thesis, Hauglid integrates EU/EEA non-discrimination law into his analysis of how issues of bias and potential discrimination might be addressed in the pre-deployment phases of AI systems in the health sector.

In the present report, I draw on the case law of the Court of Justice of the European Union where cases may have implications that could be transferred to issues related to algorithmic discrimination. The same applies to the practice of the Norwegian AntiDiscrimination Tribunal. 21

17  See Section 3.4 below. An introduction to the Norwegian equality and anti-discrimination law regime can be found in Hellum et al. (2024) pp. 130-190.

18 This is also relevant for national bodies working on questions of equality and non-discrimination ('equality bodies'), such as the Equality and Anti-Discrimination Ombud; see Council Directive (EU) 2024/1499 of 7 May 2024 on Standards for Equality Bodies in the Field of Equal Treatment Between Persons Irrespective of Their Racial or Ethnic Origin, Equal Treatment in Matters of Employment and Occupation Between Persons Irrespective of Their Religion or Belief, Disability, Age or Sexual Orientation, Equal Treatment Between Women and Men in Matters of Social Security and in the Access to and Supply of Goods and Services, and Amending Directives 2000/43/EC and 2004/113/EC, Preamble Recital 22; and Directive (EU) 2024/1500 on Standards for Equality Bodies in the Field of Equal Treatment and Equal Opportunities Between Women and Men in Matters of Employment and Occupation, and Amending Directives 2006/54/EC and 2010/41/EU, Preamble Recital 21.

19 See, for instance, Borgesius (2018); Gerards and Xenidis (2020); Wachter et al. (2021); Wachter (2022).

20 I was a member of the committee appointed to evaluate the dissertation.

21 I refer to only one case from the Tribunal; see Section 5.4 below.

As part of the report's effort to identify the links between the prohibition against discrimination, personal data regulation and AI regulation, I include a brief discussion of the EU General Data Protection Regulation (GDPR) and the EU AI Act.

## 2.5 Structure

The remainder of this report is organized into four main chapters:

Chapter 3 deals with the relationships between the prohibition against discrimination, the regulation of artificial intelligence and the regulation of informational privacy.

Chapter 4 concerns the substance of the prohibition against discrimination, with particular emphasis on the prohibitions against direct and indirect discrimination. It discusses the following questions: What do the legal conditions for discrimination mean in the context of algorithmic discrimination? At what point should the assessment of discrimination take place? What role might the prohibition against content-based discrimination have in this context? And can the special rule on the burden of proof in discrimination law play a role in cases of algorithmic and AI-related discrimination?

Chapter 5 focuses on the various actors that might be involved in cases of algorithmic discrimination. An initial key question concerns the subjects of discrimination protection: Which individuals and groups are covered by the prohibition against discrimination? A further question concerns who may be held liable in a situation where discrimination is taking place. There are typically several different actors involved when algorithm and AI systems are developed and deployed. Is it the developer or the user of the system that can be held liable -or both? Can the prohibitions against instruction and participation in discrimination expand the circle of actors who can be held liable?

The topic of Chapter 6 is enforcement and sanctioning. Can organizations seek to have questions of algorithmic discrimination enforced? And what sanctions are applicable? Are there differences between the sanctioning competency of the Norwegian Anti-Discrimination Tribunal and the competency of the ordinary courts?

At the end of each chapter, the report provides a summary of the central themes identified, along with some reflections on possibilities for developing more robust legal protection against algorithmic discrimination.

A list of literature and sources is provided at the end of the report.

## 3 Lines from the prohibition against discrimination to the regulation of artificial intelligence and the protection of informational privacy

## 3.1 Introduction

Discrimination arising from the use of algorithms and AI systems is not explicitly addressed either in the text of the Equality and Anti-Discrimination Act or in its preparatory works. Section 2(1) of the EAD Act, however, does state: ' This Act shall apply in all sectors of society. ' The point of departure for the analysis presented in this report is thus that the prohibition against discrimination is applicable to questions of algorithmic discrimination. It is also assumed that anti-discrimination provisions of EU/EEA law and various human rights conventions also apply to issues of AI-related discrimination. 22

The Equality and Anti-Discrimination Act prohibits discrimination against individuals. It contains legal provisions that delineate the boundary between lawful and unlawful differential treatment. Differential treatment that does not have a sufficient justification constitutes discrimination and is illegal.

However, the Equality and Anti-Discrimination Act does not exist in a legal vacuum, and the effects of algorithms and AI systems on individuals' rights are also a central theme in other legal frameworks. Below, I describe the overarching characteristics of the EU Artificial Intelligence Act and the EU General Data Protection Regulation (GDPR), which are the two central pieces of regulation that intersect with the protection against discrimination, albeit in different ways.

## 3.2 The AI Act

The European Union's AI Act is the first piece of legislation in the world that specifically regulates AI. 23  The Act is primarily a product liability law. It adopts a riskbased approach to the different types of AI systems, and the extent to which the regulations and obligations that it sets out are applicable in specific cases depends on the category into which an AI system is considered to fall. Most of the obligations are placed on providers of AI systems. In this way, the AI Act targets a type of actor that

22 Borgesius (2018) pp. 31ff. This understanding also forms the basis of the study by Gerards and Xenidis (2020). In its guide 'Innebygd diskrimineringsvern' [Built-In Protection Against Discrimination], the Norwegian Equality and Anti-Discrimination Ombud also comes to the same conclusion and states: 'The Equality and Anti-Discrimination Act is technology neutral. This means that the prohibition against discrimination applies regardless of whether the discriminatory practice or decision is made by a person or follows from an artificially intelligent system'; see Likestillings- og diskrimineringsombudet (2023) (author's translation).

23  European Parliament (2023).

cannot as easily be held liable under existing anti-discrimination legislation. 24  Users of AI systems -for example, enterprises that use AI systems in their daily operations -are also subject to some obligations. 25  These users will typically also be duty-bearers under the Equality and Anti-Discrimination Act. 26

The AI Act establishes four main categories of risk for AI systems: unacceptable risk (such systems are prohibited), high risk, limited risk and minimal risk. AI systems that are introduced in predefined areas and that ' pose a high risk of harm to the health and safety or the fundamental rights of persons ' , including those covered by the prohibition against discrimination, are classified as high-risk systems and will be subject to strict product standards. 27  These entail, among other things, a duty to perform a fundamental rights impact assessment prior to use. 28  This issue will not be pursued further in the present report.

In terms of the relationship between the AI Act and other EU law, including legislation on protection against discrimination, Preamble Recital 45 of the AI Act states:

Practices that are prohibited by Union legislation, including data protection law, non-discrimination law, consumer protection law, and competition law, should not be affected by this Regulation.

This is an important point of departure, as it indicates that the contents of the prohibition against discrimination -as understood on the basis of EU antidiscrimination law -should be applied in relation to AI regulation. The Recital highlights the importance of independent analyses of the content and significance of the prohibition against discrimination.

The AI Act will enter into force in a stepwise fashion, with the entirety of the legislation being implemented by August 2027. A process to implement the Act into Norwegian law has already begun, and the Norwegian government has prioritized a speedy implementation process in order to avoid regulatory disharmony in relation to EU member-states. 29

24 See, however, Section 5.4 below for a discussion on whether the prohibitions against instruction and participation may widen the circle of actors that can be held liable.

25 Future of Life Institute (2024).

26  These may additionally be primary responsible parties according to the GDPR; see Section 3.3 below.

27 Artificial Intelligence Act, Regulation (EU) 2024/1689, Article 6(2), which refers to systems referred to in Annex III, Article 6(3) and Preamble Recital 52. Recital 48 of the Preamble lists the following examples of fundamental rights: 'the right to human dignity, respect for private and family life, protection of personal data, freedom of expression and information, freedom of assembly and of association, the right to nondiscrimination, the right to education, consumer protection, workers' rights, the rights of persons with disabilities, gender equality, intellectual property rights, the right to an effective remedy and to a fair trial, the right of defence and the presumption of innocence, and the right to good administration'.

28 Artificial Intelligence Act, Regulation (EU) 2024/1689, Article 27.

29 Regjeringen (2024).

## 3.3 The General Data Protection Regulation (GDPR)

The EU ' s General Data Protection Regulation (GDPR) contains comprehensive legislation on informational privacy and has been implemented within Norwegian law through the Personal Data Act of 2018. 30  Among other things, the GDPR requires that evaluations are carried out prior to the processing of personal data. Under the Regulation, the main duty-bearer is the actor that ' determines the purposes and means of the processing of personal data ' , defined in Article 4(7) of the GDPR as the ' controller ' . In many cases, this will be the actor that is responsible under the Equality and Anti-Discrimination Act -for instance, a public or private employer or a public authority. This actor may also have duties under the AI Act.

Artificial intelligence is not explicitly mentioned in the GDPR, but the Regulation does apply to AI systems ' use of personal data. This has been explored in the report The Impact of the General Data Protection Regulation (GDPR) on Artificial Intelligence published by the Panel for the Future of Science and Technology on behalf of the EU Parliament. 31  The report shows how AI systems may be used in ways that are in line with the requirements of the GDPR, but also highlights the limitations of the GDPR and the need for further development and concretization of the regulations.

The GDPR does not explicitly prohibit discrimination, but it is recognized that privacy infringements may lead to discrimination. 32  In such cases, the regulation of privacy and the regulation of protection against discrimination both come into play. The GDPR contains mechanisms that may contribute to promoting the principle of equal treatment and the protection against discrimination. 33  An example of such mechanisms is the data controller's duty to evaluate privacy rights consequences (i.e. to carry out a data protection impact assessment or DPIA) prior to the processing of personal data.

There is considerable overlap between what constitute special categories of personal data under the GDPR and protected discrimination grounds under the Equality and Anti-Discrimination Act. Article 9(1) of the GDPR states:

Processing of personal data revealing racial or ethnic origin, political opinions, religious or philosophical beliefs, or trade union membership, and the processing of genetic data, biometric data for the purpose of uniquely identifying a natural person, data concerning health or data concerning a natural person's sex life or sexual orientation shall be prohibited.

Article 9(2), however, does open for the processing of such data under certain specified conditions. 34

30  Lov om behandling av personopplysninger (personopplysningsloven) [Act Relating to the Processing of Personal Data (The Personal Data Act)], 15 June 2018 no. 38.

31 Sartor and Lagioia (2020).

32  EU General Data Protection Regulation (GDPR), Preamble Recital 75.

33 Naudts (2019).

34 These conditions will not be considered in the present report.

Discussions have taken place within the legal literature regarding whether the prohibition on the processing of special categories of personal data under Article 9 of the GDPR constitute an obstacle for efforts to address AI-related discrimination. Van Bekkum and Borgesius describe the issue as follows:

To assess whether its AI system harms people with a certain ethnicity, the organisation needs to know the ethnicity of its job applicants. In Europe, however, the organisation may not know the ethnicity because, in principle, the GDPR prohibits the use of 'special categories of data' (article 9). Special categories of data include data on ethnicity, religion, health, and sexual preferences. Hence it appears that the organisation is not allowed to infer, collect, or use the ethnicity of the applicants. 35

Among other things, Van Bekkum and Borgesius discuss whether a specific provision should be introduced in the GDPR to allow the processing of special categories of personal data when the purpose is to prevent AI-related discrimination. They also provide examples of where such exemptions have been introduced in certain countries, for instance the United Kingdom.

The above topic illustrates how privacy regulation and anti-discrimination regulation are not always in alignment. The AI Act, however, includes a provision that addresses the tension outlined. Article 10(5) of that Act states that special categories of personal data as defined, inter alia , in Article 9(1) of the GDPR may be processed for the purposes of ensuring ' bias detection ' and ' correction in relation to the high-risk AI systems ' , in cases where this is ' strictly necessary ' and ' subject to appropriate safeguards for the fundamental rights and freedoms of natural persons ' . The provision contains several requirements that must be met for such processing of special categories of personal data to occur. The purpose of the provision is to protect persons against ' the discrimination that might result from the bias in AI systems ' . 36 Through this provision, then, anti-discrimination regulation, the GDPR and the AI Act are explicitly linked.

In my view, it will be important that attention continues to be paid to discussions related to Article 10(5) of the AI Act concerning the collection of special categories of personal data and how such an approach can be used as a means to prevent discrimination. In this context, steps should be taken to ensure that a correct understanding of the prohibition against discrimination is applied.

## 3.4 Summary and reflections going forward

## Important dimensions addressed in Chapter 3

- Both the AI Act and the GDPR address issues of discrimination, and both sets of regulations include requirements for assessments to be conducted before activities can legally proceed. As a result, their primary focus is procedural. In

35  Van Bekkum and Borgesius (2023); see also Borgesius (2018) p. 45.

36 Artificial Intelligence Act, Regulation (EU) 2024/1689, Preamble Recital 70.

contrast, the Equality and Anti-Discrimination Act 's prohibition against discrimination is centred around the substantive protection against discrimination.

- The AI Act specifies that existing EU regulation on discrimination is not impacted by the Act. This means that it will be necessary to analyse and develop the content of the prohibition against discrimination on the basis of the premises of the discrimination legislation itself.
- It is the AI Act that most clearly imposes obligations on providers (including developers) of AI systems. The obliged subject under both the GDPR and the Equality and Anti-Discrimination Act will primarily be the entity that utilizes an AI system, such as a public or private employer or a public authority. 37
- There exists a degree of tension between, on the one hand, Article 9(1) of the GDPR on special categories of personal data and, on the other, the need to process information on ethnicity, religion, etc., as part of efforts to prevent AIrelated discrimination. This tension has been addressed in Article 10(5) of the AI Act, which allows for the processing of special categories of personal data in connection with ' bias detection ' . The intersection between the prohibition of discrimination, data protection regulations and AI regulation is something that should be closely monitored in the future.

## Reflections going forward

- In addition to regulations concerning the individual 's right not to be discriminated against, the Norwegian Equality and Anti-Discrimination Act contains provisions on active equality duties. The specific activities and reporting required of public authorities and employers are set out in Sections 24, 26 and 26(a) of the Act. These include an obligation to make ' active, targeted and systematic efforts to promote equality and prevent discrimination ' and to report on the work being done in that area.
- Drawing on the structural measures set out in the GDPR and the AI Act, it might be considered whether it would be appropriate to introduce specific obligations related to activities, documentation and reporting for entities that deploy algorithmic systems. The adoption of such an approach could help ensure that the issue of discrimination is not overlooked by facilitating regular examination and evaluation of the impacts of such systems on groups protected against discrimination within the framework of anti-discrimination legislation.

37 However, see Section 5.4 below on the prohibitions against instruction and participation.

## 4 The prohibition against direct and indirect discrimination

## 4.1 Introduction

In principle, the prohibition against both direct and indirect discrimination is applicable to algorithmic discrimination. Sections 6 -9 of the Equality and Anti-Discrimination Act regulate the content of the prohibition against discrimination. To establish discrimination, the following conditions must be met:

- there must have been direct or indirect differential treatment;
- there is a connection between the differential treatment and one or more discrimination grounds; and
- the differential treatment must not have a justification that makes it lawful.

These three conditions are also expressed at the international level -for instance, in the discrimination directives of EU/EEA law. It is not a requirement for discrimination that there was an intention to discriminate.

In the following, I will first discuss the overarching features of the prohibition against direct and indirect discrimination in relation to issues associated with the use of algorithms and AI systems. In addition, the content of the three conditions for discrimination will be discussed, along with questions related to content-based discrimination and the importance of the burden of proof in discrimination law in the face of algorithmic discrimination.

## 4.2 Some starting points related to the prohibition against direct discrimination

Section 7 of the EAD Act defines direct differential treatment as ' treatment of a person that is worse than the treatment that is, has been or would have been afforded to other persons in a corresponding situation, on the basis of factors specified in section 6, first paragraph ' of the Act.

The prohibition is directed against situations where persons are treated worse than others specifically because of their gender, age, religion, etc. If direct differential treatment has a justification that satisfies the conditions of Section 9 of the EAD Act in terms of objectivity, necessity and proportionality, it may be lawful. 38  However, the starting point is that a lot is required for direct differential treatment to be permissible.

38 In the work environment, access to direct differential treatment is particularly narrow, as it is restricted to socalled genuine occupational requirements; see Section 9(2) of the EAD Act. For discrimination based on pregnancy and leave, see Section 10 of the EAD Act.

Direct differential treatment that does not have a sufficient justification constitutes direct discrimination and is prohibited. 39

The prohibition against direct discrimination is directed at differential treatment that directly treats one group worse than others. In order for the prohibition on direct discrimination to apply in a situation involving the use of algorithms and AI, direct differential treatment must occur on two levels. First, direct differential treatment of persons from particular protected groups must be something that the algorithm or AI system has been programmed to do or has taught itself. This means that the direct differential treatment must follow from the algorithm or the AI system itself -as a result of making group-based characteristics such as ethnicity, gender or religion criteria that the system weighs unfavourably. Second, this must result in the system presenting recommendations or carrying out actions that cause individuals to be subject to differential treatment precisely ' on the basis of ' their membership of a protected group (e.g. ethnicity, gender, religion).

The prohibition against direct discrimination could apply, for example, if an algorithm is designed to negatively differentiate specific protected groups and this results in a situation where individuals from these groups are placed at a disadvantage in comparison with others. The prohibition against direct discrimination may also be relevant if negative biases against certain groups have been incorporated into the training data used to develop an AI system and this results in individuals from protected groups being subjected to direct differential treatment when the system is deployed. 40

It is not entirely clear how far the prohibition against direct differential treatment extends. It is assumed that the prohibition includes not just cases where the differential treatment is directly related to a protected group characteristic but also cases where the differential treatment results from the application of criteria that are inextricably linked to protected group characteristics. I will return to the delineation between direct and indirect differential treatment in Section 4.5 below.

## 4.3 Some starting points related to the prohibition against indirect discrimination

Not all group-based differential treatment fits within the structure carved out through the prohibition against direct discrimination. This represents the background for the establishment of a prohibition against indirect discrimination.

Section 8 of the EAD Act defines indirect differential treatment as ' any apparently neutral provision, condition, practice, act or omission that results in persons being put in a worse position than others on the basis of factors specified in section 6, first paragraph ' . Through the prohibition of such treatment, it is acknowledged that rules,

39 For more information on the prohibition against direct discrimination, see Ballangrud and Søbstad (2021) pp. 150ff.; Hellum and Strand (2022) Chapter 6.

40 Gerards and Xenidis (2020) p. 67.

practices, etc., that meet the requirements of formal equality can still result in certain groups being worse off than others. In the prohibition against indirect discrimination, the focus is on differentially treating effects .

Not all indirect differential treatment is prohibited. Indirect differential treatment that satisfies the conditions of objectivity, necessity and proportionality set out in Section 9 of the EAD Act may be permissible. If the indirect differential treatment does not have a sufficient justification, however, it constitutes indirect discrimination and is prohibited.

Unlike the prohibition against direct discrimination, the prohibition against indirect discrimination does not require group-based characteristics in themselves to constitute a basis for differential treatment. The prohibition against indirect discrimination focuses on how group-based differential treatment may result from seemingly neutral actions, practices or provisions. For the prohibition against indirect discrimination to be applicable, it is sufficient that practices, actions, etc., have resulted in (or could result in) differential treatment of persons from a protected group.

There are several characteristics of AI systems that make the prohibition against indirect discrimination potentially easier to apply in practice than the prohibition against direct discrimination. For example, it is known that AI systems may be trained on data containing information that reproduces disadvantages of and stigma towards specific groups. It is, however, not necessarily easy to identify exactly which data are prone to creating bias, and this may make it difficult to apply the prohibition against direct discrimination. The prohibition against indirect discrimination, however, does not require that differential treatment be based on specifically defined group characteristics. What is central is the result produced by the use of an AI system. If the system leads to one or more protected groups being disadvantaged in comparison with others, the prohibition against indirect discrimination may be applicable.

The complexity of AI systems represents a challenge for national regulations that prohibit discrimination. Some systems are constructed in such a complex fashion that humans cannot fully understand what makes the system act in the way it does. This is often referred to as the ' black box problem ' . With such complex and unclear systems, applying the prohibition against direct discrimination would be highly challenging because it is not possible to gain insight into the criteria by which the system operates. 41  In such cases, it may be easier to apply the prohibition against indirect discrimination.

In the literature and reports reviewed for this report, several authors expressed the view that the prohibition against indirect discrimination is more relevant for addressing AI-based discrimination than the prohibition against direct discrimination. 42  I would agree with this claim. However, in order to delve deeper into

41  The development of methods for so-called explainable AI may aid here.

42 See, for example, Gerards and Xenidis (2020) p. 67; Borgesius (2018) p. 36; Wachter et al. (2021) p. 15.

this discussion, the delineation between direct and indirect discrimination should be further problematized (see Section 4.5 below).

## 4.4 Differential treatment

The condition of differential treatment can be said to be fulfilled when the treatment of an individual is worse than the treatment received by others. This point of departure is also applicable in cases of algorithmic discrimination.

Differential treatment may, for instance, be present if a person experiences ' loss of privileges, economic losses or fewer opportunities compared to others in a comparable situation ' . 43  In any attempt to assess whether differential treatment has taken place, the starting point must be an objective evaluation of what is usually considered a negative effect. 44  When assessing the issue of direct or indirect differential treatment, a comparison must be made between the situation of the person(s) claiming discrimination and the situation of other persons.

For indirect differential treatment to be established, a group-level comparison must be made through a comparison of the effects an AI system has on a protected group with its effects on others. In cases of indirect discrimination, it is not always clear what is required for a conclusion that persons belonging to a protected group are worse off than others. Under Norwegian law, however, the threshold for reaching such a conclusion is relatively low. It is sufficient that the worse treatment particularly affects persons belonging to a protected group. 45  Exactly where the line for differential treatment is drawn is typically decided through enforcement -on the basis of the facts of individual cases. The practice of the European Court of Justice is also characterized by a similar approach. Here, there is no clear description regarding the degree of disadvantage that is required to meet the criterion of differential treatment. As Wachter et al. express the matter:

Thresholds are ... flexible and set on a case-by-case basis, if they are explicitly set by the judiciary at all. 46

In practice, difficulties in establishing proof of discrimination may make it challenging to support a claim of discrimination.

In situations involving more ' traditional ' forms of discrimination, mechanisms have been developed to make it easier for complainants/plaintiffs to substantiate a claim of differential treatment. For example, there is no requirement that one must be able to point to actual comparable persons in order to support a claim of direct differential treatment. Under both Norwegian and EU/EEA law, a claim of direct differential treatment can be substantiated through reference to both actual and hypothetical

43 Prop.81 L (2016-2017) [Legislative Proposition], Point 12.2.4.1, p. 101 (author's translation).

44 Prop.81 L (2016-2017) [Legislative Proposition], Point 12.2.4.2, p. 102.

45 In the discrimination directives in EU/EEA law, the term 'particular disadvantage' is used. For more on this, see Hellum and Strand (2022) pp. 236-238.

46 Wachter et al. (2021) p. 29.

comparators. 47  In cases of indirect discrimination, there is considerable latitude regarding how a complainant/plaintiff can substantiate a claim of differential treatment. For example, there is no requirement for the complainant/plaintiff to provide statistics to substantiate a claim of indirect differential treatment. Statistics may be used if they exist, but indirect differential treatment may also be substantiated in other ways. 48

However, substantiating claims of direct or indirect differential treatment caused by algorithms or AI systems in individual cases poses a number of challenges. How can individuals gain insight into the criteria these systems use or their effects so that differential treatment can be established? With regard to the prohibition against direct discrimination, Gerards and Xenidis state that:

Categorising algorithmic discrimination as direct discrimination is ... likely to be a challenge given the opacity of particular algorithms, especially in light of the need to establish a comparator under EU law. Indeed, if the lack of transparency of the functioning of an algorithm prevents the gathering of evidence on how the algorithm has treated or would have treated a group that does not share the protected characteristic at stake (the comparator group), then a finding of direct discrimination might be precluded altogether. 49

Establishing the threshold for what constitutes differential treatment within a context involving the use of algorithms and AI systems is something that should be afforded attention going forward. The burden of proof in discrimination law, however, may provide an important key to this issue, as will be discussed in Section 4.8 below.

We have seen that algorithm- and AI-related disadvantage provides some challenges in relation to the condition of differential treatment. These challenges are partly due to the fact that the material content of the differential treatment condition is already inherently vague. They are also partly due to evidentiary challenges related to gaining insight into and knowledge about the criteria and effects of algorithms and AI systems in order to establish that differential treatment has occurred.

47  Hellum and Strand (2022) pp. 381ff.

48  Hellum and Strand (2022) p. 383.

49 Gerards and Xenidis (2020) p. 69.

## 4.5 The connection between the differential treatment and one or more discrimination grounds

In order for discrimination to be established under the EAD Act, there must be a connection between the differential treatment and one or more protected discrimination grounds. 50

In the context of algorithm- and AI-related discrimination, questions regarding the connection between differential treatment and a protected discrimination ground will be increasingly complex. The problem is not simply that a system ' s negative emphasis on protected group characteristics may lead to (direct) discrimination. In many cases, AI systems do not use protected group characteristics in their processing of data. Instead, they use factors that are so closely related to protected discrimination grounds -such as deta ils about an individual's address, economic situation or education -that the end result is still differential treatment of persons based on one or more protected group characteristics. This is often described as ' proxy discrimination ' . In this regard, Datta et al. define the term ' proxy ' as ' a feature correlated with a protected class whose use in a decision procedure can result in indirect discrimination ' . 51

In Norwegian, we might talk about discrimination through the use of stedfortrederfaktorer , meaning that the system takes a kind of detour through factors that are closely related to protected group characteristics. In such cases, it may not always be easy to ascertain which proxy factors were decisive for the differential treatment that may have occurred in relation to persons from particular protected groups. With its focus on how group-based differential treatment can result from seemingly neutral actions, practices or provisions, the prohibition against indirect discrimination thus provides an avenue through which to address the types of challenges posed by such situations. Gerards and Xenidis argue that the prohibition against indirect discrimination ' provides a safety net for tackling proxy discrimination when there is doubt as to whether the link between a given proxy and a given protected ground is direct enough for direct discrimination to arise ' . 52

In his doctoral thesis, Hauglid examines the European Court of Justice's case law and the distinction between direct and indirect discrimination in cases where differential treatment is not based directly on protected group characteristics but may involve factors that are inextricably linked to protected group characteristics. Hauglid is of the opinion that direct differential treatment may be established if an AI system incorporates ' any protected characteristics or inextricably linked features variables ' . 53 Through his analysis of case law from the European Court of Justice, he demonstrates how the Court views some cases as constituting direct differential treatment because an actor has introduced differentiating criteria that are inextricably linked to protected

50 For a closer discussion on what this condition entails, see Hellum and Strand (2022) pp. 249ff.

51 Datta et al. (2017) p. 1.

52 Gerards and Xenidis (2020) p. 71.

53 Hauglid (2024) p. 214.

group characteristics in a way that exclusively disadvantages a protected group or results in all individuals from a protected group being excluded from obtaining a benefit. 54

In the joined case of Wabe and Müller , the European Court of Justice declared in relation to the content of the prohibition against direct discrimination that ' unequal treatment resulting from a rule or practice which is based on a criterion that is inextricably linked to a protected ground, in the present case religion or belief, must be regarded as being directly based on that ground ' . 55  The case concerned differential treatment in the workplace due to the use of religious clothing and symbols, and through its reasoning the Curt clarified that regulations that are constructed in such a way that certain religions or religious expressions are subject to a ban while other religious manifestations are not, this will constitute direct differential treatment based on religion. The case thus illustrates that direct differential treatment also can be found to have occurred against a group within a larger protected group. In the context of algorithmic discrimination, this could mean that direct differential treatment may be established where the system has a bias that results in exclusively one or a few ethnic minority groups being subjected to differential treatment while other ethnic minority groups are not. 56

The prohibition against discrimination does not simply cover discrimination based on existing characteristics of a person but also discrimination by association and discrimination based on 'assumed, former or future factors'. 57  The inclusion of these issues contributes to a widening of the circle of persons for whom the prohibition against discrimination is relevant -also in the face of algorithmic discrimination -and means, inter alia, that misclassification of individuals in ways that cause them to be subjected to discrimination will also fall under the terms of the prohibition against discrimination. This is probably most relevant if individuals are subjected to direct differential treatment. 58

Section 6 of the Equality and Anti-Discrimination Act 's inclusion of a prohibition against multiple forms of discrimination means that the protection against discrimination provided by the Act is applicable in cases where more than one discrimination ground is involved. In contrast, EU/EEA law is based on a single-factor approach to discrimination and there is no prohibition on multiple discrimination, which means that only one discrimination ground is applied at a time. Xenidis considers this a weakness that ' casts doubt on the adequateness of EU non-

54  Hauglid (2024) pp. 221-228.

55  European Court of Justice, C-804/18 and C-341/19 (joined cases WABE and Müller ) (2021), Premise 73.

56 See also European Court of Justice, C-148/22 (OP) (2023), Premise 25, in which it is declared that 'an internal rule decreed by an employer which prohibits in the workplace only the wearing of conspicuous, large-scale signs of beliefs - philosophical or religious in particular - may constitute direct discrimination on the grounds of religion or belief within the meaning of Article 2(2)(a) of Directive 2000/78 where that criterion is inextricably linked to one or more specific religions or beliefs'.

57  See Section 6(2) and Section 6(3) of the EAD Act.

58 Gerards and Xenidis (2020) p. 68.

discrimination law when it comes to redressing intersectional manifestations of algorithmic discrimination ' . 59

<!-- image -->

## 4.6 Lawful differential treatment

Not all differential treatment that is linked to a protected discrimination ground is prohibited. Sometimes the differential treatment may have a justification that is sufficient for it to be considered permissible. The Equality and Anti-Discrimination Act 's main provision on lawful differential treatment is set out in Section 9(1) of the Act, which states that differential treatment does not violate the prohibition against discrimination when it:

59 Xenidis (2020), particularly pp. 741-745.

- a. has an objective purpose,
- b. is necessary to achieve the purpose, and
- c. does not have a disproportionate negative impact on the person or persons subject to the differential treatment.

The provision involves a broad assessment of proportionality. The three conditions for lawful differential treatment are vaguely formulated. In particular, the requirements that the differential treatment must be necessary for achieving the specified purpose and must not be disproportionately intrusive open up possibilities for a range of different assessments, depending on the facts at hand. Referring to the scope for lawful indirect differential treatment under EU law, Gerards and Xenidis state:

If cases of algorithmic discrimination fall 'by default' into the indirect discrimination category, leading to an open pool of possible justifications, legal certainty for potential victims, developers and users of algorithmic systems will decrease as the appreciation of the validity of potential justifications would exclusively be bestowed upon courts. In particular, the application of the 'necessity' part of the objective justification test by courts poses questions in light of the trade-off between accuracy and performance on the one hand and nondiscrimination on the other that might arise in cases of algorithmic discrimination. 60

As Gerards and Xenidis point out, vague criteria for lawful differential treatment provide courts with a large degree of discretion during enforcement. This may be to the detriment of the different actors' need for predictability. A lack of predictability may in turn prevent the prohibition against discrimination from having a real impact in practice, especially in the AI field.

Section 9 of the EAD Act allows for both lawful direct and lawful indirect differential treatment, but the scope for lawful direct differential treatment is narrower than that for lawful indirect differential treatment. 61  This is reflected in Section 9(2) and Section 10 of the Act, which establish a very limited scope for direct differential treatment in employment and an absolute prohibition against discrimination on the basis of pregnancy. The Equality and Anti-Discrimination Act is to be interpreted in line with EU/EEA law. In cases of indirect discrimination, the EU/EEA anti-discrimination directives include an objective justification clause that open for lawful indirect differential treatment (provided that the issue at hand involves a question that falls within the scope of those directives). Direct differential treatment, however, is generally prohibited unless the directives explicitly allow for lawful direct differential treatment under specifically defined conditions. For example, the EU/EEA directives establish a very limited scope for direct differential treatment in the workplace, which

60 Gerards and Xenidis (2020) p. 73.

61  Hellum and Strand (2022) p. 255.

is implemented at the national level in Norwegian law through Section 9(2) of the EAD Act. 62

Accordingly, two factors set the outer limits for assessing the scope of lawful differential treatment. One is the societal area in which the differential treatment occurs. The other is whether it entails direct or indirect differential treatment.

## Which sectors of society are of particular importance?

That the protection against discrimination is particularly strong in employment-related contexts is rooted in the fact that equality in relation to the right to work is of particular importance for the position of individuals within society. This is the field within which large parts of the legislation on protection against discrimination originated, and it is also particularly within the context of working life that the protection has evolved.

As a result of extensive technological development, however, algorithmic and AI systems have been employed across many sectors of society. These systems are being used within the work sector -for instance, within recruitment -but are also utilized in a range of other arenas. Examples include diagnostics and prioritization within the health sector, prioritizing of individuals to be subjected to checks by police or customs authorities, or risk-mapping conducted by banks, insurance companies, rental firms, etc. A common feature is that the analyses and decision support conducted by algorithmic systems may significantly impact the position of individuals, as they can affect individuals ' access to essential services and rights, such as healthcare, housing or the ability to obtain insurance. In the light of the technology ' s widespread application in areas that are of considerable importance to individuals, the premise that protection against discrimination needs to be strongest in the workplace should be reconsidered. The establishment of common criteria for lawful differential treatment applicable to all algorithm- and AI-initiated disadvantage/differential treatment -regardless of the social sector in which such treatment takes place -should instead be considered.

## On the delineation between direct and indirect differential treatment

We have seen that the distinction between direct and indirect differential treatment influences the scope of what can be considered lawful differential treatment. More is required to deem direct differential treatment lawful than is the case for indirect differential treatment. This premise, however, does not align well with a complex and multifaceted technological reality. As discussed in Section 4.5 above, AI systems often incorporate factors other than the grounds of discrimination explicitly listed in the anti-discrimination legislation. At the same time, the data used may be closely or almost inextricably linked to the protected grounds of discrimination. The already challenging distinction between direct and indirect differential treatment is further complicated by the use of algorithmic systems.

62  Regulations on differential treatment based on genuine and determining occupational requirements are found in Article 4(1) of Directive 2000/78/EC, Article 4 of Directive 2000/43/EC and Article 14(2) of Directive 2006/54/EC.

Consideration should be given to the creation of a specific provision regulating lawful direct and indirect algorithmic differential treatment in the Equality and AntiDiscrimination Act. Such a provision should account for the complexity that may arise, for example, when AI systems emphasize proxy factors. The provision should be more concrete than the main rule in Section 9(1) of the Act, and it should be based on the principle that a particularly strong protection against direct algorithmic discrimination should be established across all sectors of society, not just in the arena of working life.

## Reflections on the design of a specific provision regulating lawful algorithmic differential treatment

In relation to the stipulation in Section 9(1)(a) of the EAD Act that differential treatment must have an 'objective purpose', it could be stipulated that, in order to be permitted, algorithmic differential treatment must play a part in the realization of the purpose of the data system in question. It might also be specified that the algorithmic systems concerned cannot have objectives that contravene national or international law (including Norway's human rights obligations).

Furthermore, a requirement might be established that the differential treatment be suitable and necessary for the realization of the data system 's purpose . In cases where differential treatment results from the system directly emphasizing protected group characteristics -or factors inextricably linked to protected group characteristics -it could be stipulated that the differential treatment must be decisive for the realization of the data system ' s purpose. This would raise the threshold for the permissibility of direct algorithmic differential treatment across all societal domains. Such a formulation would be in line with the current Section 9(2) of the Equality and Anti-Discrimination Act regarding lawful direct differential treatment in employment.

In relation to the requirement that the differential treatment must not have a disproportionate negative impact, it could be specified that the assessment of the differential treatment should pay particular attention to whether it concerns individuals' access to fundamental rights and services such as health, education, banking and insurance services, etc.

In my view, the introduction of a specific provision on lawful algorithmic differential treatment as outlined above would not entail the establishment of a higher threshold for lawful direct and indirect algorithmic differential treatment in Norway than that presumed to apply elsewhere in Europe. The proposed provision is based on a complex body of legal sources on anti-discrimination protection and individuals ' fundamental human rights, which are anchored in, among other things, the EU Charter. A specific provision on lawful algorithmic differential treatment would simply ensure that central elements of the surrounding legal framework are directly reflected in the legislation.

<!-- image -->

## 4.7 Timing of discrimination -particularly in relation to content-based discrimination

The Equality and Anti-Discrimination Act does not require that discriminatory effects have occurred for the prohibitions against direct and indirect discrimination to apply. 63 In the legislative proposition to the Equality and Anti-Discrimination Act, it is stated in relation to the prohibition against direct discrimination that laws, decisions, statutes and contractual terms can be subject to the discrimination prohibition even though no concrete negative effect has occurred, because these are binding conditions that by their nature are expected to be followed. 64

The prohibition against direct discrimination therefore also includes the content of regulations, agreements, etc.

Similarly, for indirect discrimination, it is also not a requirement that differential treatment has actually occurred. This is made explicit in Section 8 of the EAD Act, where indirect differential treatment is defined as including any apparently neutral provision, condition, practice, act or omission that will put individuals at a

63  Hellum and Strand (2022) pp. 242-245.

64 Prop.81 L (2016-2017) [Legislative Proposition] Point 12.9.2.2, p. 111 (author's translation).

disadvantage in comparison with others. 65  Differential treatment effects may, for instance, be embedded in different types of rules and regulations. The Norwegian approach in Section 8 of the EAD Act is in line with the prohibition against indirect discrimination under EU/EEA law.

One may question the significance of the prohibition against content-based discrimination in the face of algorithmic systems. Many data systems go through a long period of testing during which training data are applied and the systems are adjusted and developed before being put into operation. It makes little sense to apply the prohibition against discrimination on algorithms and AI systems while they are undergoing testing, since developers must evaluate and adjust potential negative effects during this phase. However, once a system is operational, the prohibition against discrimination will apply, even before the system has any direct negative effects.

There are likely to be some practical challenges regarding the application of the prohibition against content-based discrimination to algorithmic systems (including AI systems), as knowledge of such systems' way of functioning may be difficult to access. Provided there is knowledge that a system uses discriminatory group-based criteria or has a design that disadvantages individuals from protected groups in comparison with others, the prohibition against content-based direct and/or indirect discrimination would be relevant.

Such a situation could arise if, for instance, individuals think that they are exposed to direct or indirect group-based differential treatment through the way in which a wholly or partially automated data system is applied to them, while at the same time these persons themselves do not want to take active steps to bring a case before the AntiDiscrimination Tribunal or the ordinary courts. In such circumstances, awareness of the system's possible discrimination could be communicated to other individuals with the same group characteristics as those persons who had already been subjected to differential treatment -for instance, individuals belonging to the same faith, religion or ethnicity. Such information could also be communicated to an interest organization (see Section 6.2 below). If there is reason to believe that the system will act in the same way also in relation to other people, it could be relevant to bring a case before the Anti-Discrimination Tribunal or the ordinary courts based on a claim that the system 's design violates the prohibition against content-based discrimination.

## 4.8 Burden of proof in discrimination law

Section 37 of the EAD Act includes a specific provision regarding the burden of proof in cases of alleged discrimination that aims to contribute to making the protection

65 The English translation of Section 8 of the Act is not as clear as the text of the official Norwegian Act. The English version is therefore not cited here.

against discrimination more effective. 66  The provision implements EU/EEA law into Norwegian law but also applies to more sectors of society and more groups than are covered by the corresponding EU/EEA legislation. In Norway, debate is currently ongoing regarding the actual content of the burden-of-proof rule. I will not go into the Norwegian discussion on the burden of proof here.

Within the context of EU/EEA law, the development of the rule on the burden of proof in cases of alleged discrimination played an important role in the realization of the right to equal pay as an individual right. There is therefore a close connection between the rule on the burden of proof and the right to equal pay for equal work or work of equal value. Today, the significance of the burden of proof in questions of equal pay has gained momentum through the adoption of the EU Equal Pay Transparency Directive (Directive 2023/970). This directive applies to EEA countries, and work is currently under way to implement the directive into Norwegian law. 67

Both algorithmic discrimination and pay discrimination are characterized by situations where individuals may lack insight into precisely what it is that causes them to be subjected to differential treatment because the systems being used are opaque. This can make it difficult to substantiate that discrimination has occurred. In my view, valuable insights can be gained from understanding the content of the rule on the burden of proof. 68

The burden of proof in discrimination law has its origin in case law from the European Court of Justice in cases of equal pay between women and men. The rule was formalized in 1997 in the Burden of Proof Directive (Directive 97/80/EC) and subsequently incorporated into various directives in the field of discrimination. 69  The European Court of Justice 's development of a separate rule on the burden of proof in the field of discrimination came about through interpretation of the equal pay provision in Article 119 of the EU Treaty ( today's Article 157 of the Treaty on the Functioning of the European Union) and is also closely related to the development of the protection against indirect discrimination on the basis of sex. Three central cases were the Danfoss case, the Enderby case and the Royal Copenhagen case. 70  What these cases had in common was that they concerned systems of pay that appeared neutral on the surface but had the consequence of making women worse off than men

66 The first part of the provision reads: 'Discrimination shall be assumed to have occurred if circumstances apply that provide grounds for believing that discrimination has occurred and the person responsible fails to substantiate that discrimination did not in fact occur.'

67 See Regjeringen (2023); the deadline for implementation has been set at 7 June 2026.

68 This is also emphasized in Hauglid (2024) p. 328.

69  See Race Equality Directive (2000/43/EC), Article 8; Equality Framework Directive (2000/78/EC), Article 10; Equal Treatment Directive (2006/54/EC), Article 19(1); Equal Treatment in Goods and Services Directive (2004/113/EC), Article 9.

According to this rule: 'Member States shall take such measures as are necessary, in accordance with their national judicial systems, to ensure that, when persons who consider themselves wronged because the principle of equal treatment has not been applied to them establish, before a court or other competent authority, facts from which it may be presumed that there has been direct or indirect discrimination, it shall be for the respondent to prove that there has been no breach of the principle of equal treatment.'

70  C-109/88 ( Danfoss ) (1989); C-127/92 ( Enderby ) (1993); C-400/93 ( Royal Copenhagen ) (1995).

in terms of pay. The cases thus involved the use of the prohibition against discrimination in the face of opaque systems that resulted in gender-related differential treatment. To protect individuals ' legal positions, the European Court of Justice emphasized the importance of effective enforcement of the prohibition against discrimination in these cases. As a result, the court chose to let doubts about the facts of the three cases weigh against the relevant employers.

The Danfoss case involved questions of discrimination related to pay in a company that operated with an opaque pay-determination system. The claimant had access only to average information that revealed the existence of gender-based differences in pay. The European Court of Justice described the situation as follows:

the system of individual supplements applied to basic pay is implemented in such a way that a woman is unable to identify the reasons for a difference between her pay and that of a man doing the same work. Employees do not know what criteria in the matter of supplements are applied to them and how they are applied. They know only the amount of their supplemented pay without being able to determine the effect of the individual criteria. Those who are in a particular wage group are thus unable to compare the various components of their pay with those of the pay of their colleagues who are in the same wage group. 71

There was no information on the reasoning for the determination of pay. The European Court of Justice declared:

where an undertaking applies a system of pay which is totally lacking in transparency, it is for the employer to prove that his practice in the matter of wages is not discriminatory, if a female worker establishes, in relation to a relatively large number of employees, that the average pay for women is less than that for men. 72

The Danfoss case is explicitly mentioned in Preamble Recital 52 of the Pay Transparency Directive (Directive 2023/970):

In accordance with the case-law of the Court of Justice, Directive 2006/54/EC establishes provisions to ensure that the burden of proof shifts to the respondent when there is a prima facie case of discrimination. Nevertheless, it is not always easy for victims and courts to know how to establish even that presumption. In case C-109/88, the Court of Justice held that when a system of pay is totally lacking in transparency, the burden of proof should be shifted to the respondent, irrespective of the worker showing a prima facie case of pay discrimination. Accordingly, the burden of proof should be shifted to the respondent where an employer does not comply with the pay transparency obligations set out in this Directive, for instance by refusing to provide information requested by the workers or not reporting on the gender pay gap, where relevant, save where the employer proves that such an infringement was manifestly unintentional and of a minor character.

71  C-109/88 ( Danfoss ), Para. 10.

72  C-109/88 ( Danfoss ), Point 1 in the Court's conclusion.

Two central topics are pointed out here. The first concerns the burden of proof's premise that it is normally the responsibility of the plaintiff/complainant to establish a presumption of discrimination ( ' a prima facie case of discrimination ' ). 73 The second concerns pay systems that are ' totally lacking in transparency ' , including situations where an employer is not transparent about pay determination in the way required by the Pay Transparency Directive. 74 In such cases, it is no longer the responsibility of the plaintiff/complainant to establish a presumption of discrimination. Instead, the responsibility to establish that there is no pay discrimination is immediately transferred to the employer/responsible party.

These two evidentiary situations are explicitly mentioned in Article 18(1) and Article 18(2) of the Pay Transparency Directive, which state:

1. Member States shall take the appropriate measures, in accordance with their national  judicial systems, to ensure that, when workers who consider themselves wronged  because the principle of equal pay has not been applied to them establish before a  competent authority or national court facts from which it may be presumed that there  has been direct or indirect discrimination, it shall be for the respondent to prove that  there has been no direct or indirect discrimination in relation to pay.
2. Member States shall ensure that, in administrative procedures or court proceedings regarding alleged direct or indirect discrimination in relation to pay, where an employer has not implemented the pay transparency obligations set out in Articles 5, 6, 7, 9 and 10, it is for the employer to prove that there has been no such discrimination.

The first subparagraph of this paragraph shall not apply where the employer proves that the infringement of the obligations set out in Articles 5, 6, 7, 9 and 10 was manifestly unintentional and of a minor character.

Recital 52 and Article 18 of the Directive highlight ways of handling cases of discrimination where differential treatment occurs within the framework of an impenetrable system and where the employer is subject to various structural obligations to ensure pay transparency. This may have relevance to questions of algorithmic discrimination.

The Pay Transparency Directive highlights two possibilities regarding the burden of proof. One possibility is to assume that the burden of proof shifts to the party accused of discrimination if there is sufficient documentation that the use of an algorithmic system results in differential treatment. The other possibility is to take one step

73 In Section 37 of the EAD Act, this is described as the plaintiff's responsibility to show that there are 'grounds for believing that discrimination has occurred'.

74 Article 18(2) of the Directive points to Article 5 of the Directive on 'Pay transparency prior to employment', Article 6 on 'Transparency of pay setting and pay progression policy', Article 7 on the 'Right to information', Article 9 on 'Reporting on pay gap between female and male workers' and Article 10 on 'Joint pay assessment'.

further by bringing the approach laid out in Article 18(2) of the Pay Transparency Directive to the area of algorithms and AI. As I see it, Article 18(2) of the Pay Transparency Directive contributes to a development and concretization of the approach adopted in the Danfoss case. It involves a kind of procedural shift and form of accountability that subsequently plays into the question of the burden of proof in cases regarding alleged direct or indirect discrimination in relation to pay. If requirements for pre-assessments, transparency and reporting are not followed, the Pay Transparency Directive presumes that pay discrimination exists unless the party accused of discrimination can prove that discrimination has not occurred.

In my view, it can be argued that a regulatory framework that builds on a similar understanding regarding proof should be established in relation to algorithmic discrimination. In the event that such an approach is adopted, it might be appropriate to include a specification of the responsibilities and duties imposed on users of algorithmic systems, both under the existing equality and anti-discrimination legislation and under other regulatory frameworks, such as the AI Act and the GDPR. Further exploration of which structural obligations may be relevant might be conducted. What is required to refute a presumption of algorithmic discrimination would also be an interesting question to explore. Systems for AI auditing and the development of methods that can explain what happens within AI systems ( ' explainable AI methods ' ) may be relevant in this context. 75

## 4.9 Summary and reflections going forward

## Important dimensions addressed in Chapter 4

- The complexity of algorithmic systems means that the prohibition against indirect discrimination may be easier to apply than the prohibition against direct discrimination.
- Algorithm- and AI-related disadvantage challenges the requirement of differential treatment. This is partly because the criteria for differential treatment are somewhat vague. It also involves evidentiary challenges related to how to gain insights and knowledge regarding the criteria and effects of algorithmic systems that would make it possible to substantiate claims that differential treatment has occurred.
- The use of proxy factors by AI systems makes it more challenging to establish a connection between differential treatment and one or more discrimination grounds. This makes the prohibition against indirect discrimination, which is directed at the effects of systems, possibly more relevant than the prohibition against direct discrimination in cases concerning discrimination through proxy factors ( ' proxy discrimination ' ).

At the same time, case law from the European Court of Justice shows that the prohibition against direct discrimination does not apply only when differential

75 Strümke et al. (2023).

treatment is explicitly based on a protected discrimination ground. The prohibition also applies if criteria that are inextricably linked to a protected discrimination ground are used.

In the light of the above, consideration should be given to the possible introduction of separate definitions of direct and indirect algorithmic differential treatment into the Equality and Anti-Discrimination Act.

- The prohibition against discrimination does not simply cover discrimination based on existing characteristics of a person. It also covers discrimination by association as well as discrimination based on ' assumed, former or future factors ' . These aspects of the prohibition against discrimination may prove relevant in questions of algorithmic discrimination and may contribute to an expansion of the circle of persons covered by the protection against discrimination.
- Multiple discrimination is covered by the EAD Act's prohibition against discrimination, which makes it easier to apply the prohibition against discrimination in cases involving more than one discrimination ground. The EU/EEA anti-discrimination directives do not prohibit multiple forms of discrimination.
- Section 9 of the EAD Act on lawful differential treatment may prove to be challenging in efforts to address algorithmic discrimination as it offers little predictability. Two factors influence the scope of the current justification clause: the social sector in which the differential treatment occurs and whether it involves direct or indirect differential treatment. The report shows how algorithmic differential treatment challenges both of these fundamental premises. Algorithmic systems are now used across all sectors of society and contribute in various ways to decisions that may be of great importance to individuals. This makes the premise that protection against discrimination should be particularly strict in the work sector in need of nuancing. Furthermore, the complexity of algorithmic systems makes it difficult to draw a clear line between direct and indirect discrimination. In the light of these circumstances, the report discusses the possibility that a separate and more detailed provision regulating lawful algorithmic differential treatment might be introduced into the Equality and Anti-Discrimination Act.
- The report discusses how the prohibition against content-based discrimination may be relevant in cases of algorithmic discrimination.
- The report shows how rules and decisions related to the burden of proof in discrimination law, particularly as specified under Article 18 of the EU Pay Transparency Directive (Directive 2023/970), may play a crucial role in efforts to address algorithmic discrimination. The relevance of this issue stems from the fact that equal pay cases are characterized by a lack of transparency regarding the basis for pay determination, and there is thus a clear parallel with questions of algorithmic discrimination. Article 18(2) of the Pay Transparency Directive expresses a ' procedural turn ', in that the provision requires those responsible for pay to conduct various assessments and to be transparent about the results of those assessments. If these requirements are not met, it is

this party 's responsibility to prove that discrimination has not occurred if a case is brought regarding alleged direct or indirect discrimination in relation to pay. Such an approach includes elements that might be transferable to the enforcement of individual cases concerning algorithmic discrimination.

## Reflections going forward

The chapter has suggested that consideration might be given to the possible introduction of certain clarifications into the Equality and Anti-Discrimination Act. These clarifications could help ensure a more robust prohibition against discrimination. The following topics have been raised:

- The introduction of separate definitions for direct and indirect algorithmic differential treatment might be considered. The definition of direct algorithmic differential treatment should highlight that the prohibition covers both situations where an algorithmic system is directly based on one or more discrimination grounds (see Section 7 of the EAD Act) and situations where the system operates with factors that are inextricably linked to protected group characteristics. The definition of indirect algorithmic differential treatment could build on the current text of Section 8 of the EAD Act, but also specify that the prohibition includes situations of differential treatment of protected groups that has occurred through algorithmic systems ' use of proxy factors, provided that the case in question does not fall under (the proposed) prohibition against direct algorithmic differential treatment.
- The introduction of a separate provision on lawful algorithmic differential treatment might be considered. Such a provision would need to be more concrete than the current formulation of Section 9(1) of the EAD Act. It should take into account the complexity inherent in the delineation between direct and indirect algorithmic differential treatment, and it should be based on the premise that the protection against algorithmic discrimination needs to be equally strong across all sectors of society. In my view, the adoption of such an approach may be considered as derivable from common European rules on protection against discrimination and human rights.

Such a specific provision could establish that, in order to be lawful, differential treatment related to the use of an algorithmic system must:

- o seek to realize an objective purpose, which presumes that the system cannot have purposes contrary to national and international legislation (including Norway's h uman rights obligations); and
- o be suitable and necessary for the realization of the algorithmic system 's objective(s). It could be further specified that if the differential treatment results from the system directly emphasizing protected group characteristics -or factors inextricably linked to protected group characteristics -the differential treatment must be decisive for the realization of the system's objective s. (Such a specification would entail the establishment of a particularly strong protection against direct algorithmic discrimination.)

- o must not have a disproportionate negative impact on the person or persons subject to the differential treatment. It might be specified that emphasis should be placed on whether the differential treatment concerns individuals' access to fundamental rights and services, such as health, education, work, banking and insurance services, etc.
- The introduction of a separate provision regarding the burden of proof on cases concerning algorithmic discrimination might be considered, drawing inspiration from Article 18 of the EU Pay Transparency Directive.

## 5 The actors: Who are protected against discrimination and who may be held liable?

## 5.1 Introduction

This chapter turns the attention toward the actors implicated in the protection against discrimination. Who are covered by the prohibition against discrimination? And who can be held liable for having discriminated? I will also address whether the prohibitions against instruction and participation can expand the circle of actors that can be held liable in cases of discrimination.

## 5.2 Protected groups

Section 6(1) of the EAD Act prohibits discrimination related to ' gender, pregnancy, leave in connection with childbirth or adoption, care responsibilities, ethnicity, religion, belief, disability, sexual orientation, gender identity, gender expression, age or combinations of these factors ' . The provision also specifies that ethnicity includes ' national origin, descent, skin colour and language ' . 76

There are, however, a range of group-based characteristics that are not covered by the legislation's listing of discrimination grounds. There is thus a risk that algorithmic systems may contribute to the creation and/or amplification of disadvantages for groups not currently covered by the Equality and Anti-Discrimination Act. This could happen, for instance, if such systems associate disadvantages with factors such as social status, health, residential address, level of education and income. 77  AI systems may conduct differential treatment on the basis of group characteristics that one may initially think are irrelevant but that are nevertheless shown to lead to disadvantage. An example of this might be if AI systems presume that being a dog owner makes one more receptive to certain types of advertisements. 78

The complexity of algorithmic systems may make it appropriate to discuss the possibility of opening up the existing protection against discrimination to new groups. However, such a move would make it necessary to discuss fundamental questions about the relationship between group characteristics based on historical structural differences, such as gender discrimination, and disadvantages emerging for different groups, across different contexts, in contemporary society.

The challenges that may arise at the intersection of algorithmic discrimination and a legal framework originally based on the adoption of a closed list of discrimination

76 In relation to the employment sector, additional prohibitions have been established against discrimination based on political opinion, membership in a labour organization, part-time work and temporary employment; see Section 13(1) in Lov om arbeidsmiljø, arbeidstid og stillingsvern mv. (arbeidsmiljøloven) [Act Relating to the Working Environment, Working Hours and Employment Protection, etc. (Working Environment Act)], 17 June 2005 no. 62.

77 Gerards and Xenidis (2020) p. 64.

78  Wachter (2022).

grounds have already been put on the agenda in the European context. 79  Among other things, academic articles and reports have discussed whether Article 21 of the EU Charter (which has a more flexible approach to the issue of discrimination grounds) can be used as a mechanism to address situations of discrimination against groups that are currently not protected by the anti-discrimination directives.

Limitations resulting from the adoption of a closed list of discrimination grounds might to a certain extent be addressed through the inclusion of an open-ended category in the specification of discrimination grounds. As we have seen earlier, Section 6 of the EAD Act builds on an explicit and closed approach to the issue of discrimination grounds. At the same time, the purpose clause in the first paragraph of Section 1 of the Act takes an open-ended approach to groups who are vulnerable to discrimination (see the formulation ' or other significant characteristics of a person ') . 80 Lines can be drawn between the purpose clause in the EAD Act and the open-ended prohibitions against discrimination in international human rights conventions -for example, Article 14 of the European Convention on Human Rights, Article 26 of the International Convention on Civil and Political Rights, and Article 2(2) of the International Covenant on Economic, Social and Cultural Rights.

It is somewhat problematic that the EAD Act's purpose clause signals an open-ended approach to the issue of discrimination grounds while the prohibition against discrimination in Section 6(1) is based on a closed approach. This creates a lack of clarity that needs to be addressed by the legislature. Issues related to algorithmic discrimination may provide an opportunity to examine this issue more closely.

It may also be worth considering the introduction of a specific prohibition against algorithmic discrimination -one that relies on a positive enumeration of protected discrimination grounds but also accommodates the need for flexibility through the inclusion of an open-ended category.

## 5.3 Against whom should a discrimination claim be directed?

The development of algorithmic systems (including AI systems) involves several phases. It is not until the implementation phase that such systems are fed new data and can generate answers ( ' output ' ) to achieve the objectives for which the systems were designed. 81  It is also during this phase that individuals may be subjected to discrimination.

As various actors are involved prior to the launch of a system, questions can be raised as to who really can and should be held liable for having discriminated.

79 Gerards and Xenidis (2020).

80 The provision states: 'The purpose of this Act is to promote equality and prevent discrimination on the basis of gender, pregnancy, leave in connection with childbirth or adoption, care responsibilities, ethnicity, religion, belief, disability, sexual orientation, gender identity, gender expression, age or other significant characteristics of a person.'

81 Gerards and Xenidis (2020) p. 39.

The clear point of departure must be that a person claiming to have been discriminated against should direct a claim against the private or public actor that is responsible for a particular decision, ruling, resolution, etc., and that uses an algorithmic system as part of its operations. In the AI Act, this is described as ' the deployer ' . 82  This point of departure must hold regardless of whether the case involves a fully automated algorithmic decision or a situation where an AI system is being used in a decision-support role, with the final decision ultimately being made by a human. A public or private employer, educational institution, health institution, etc., cannot circumvent their obligations under the Equality and Anti-Discrimination Act through the use of systems based on algorithms and AI. 83  Such an understanding is also in line with the duty to ensure effective implementation of the prohibition against discrimination.

The European Court of Human Rights case Glukhin v. Russia builds on a similar understanding. 84  Among other things, the case concerned whether an automated facial recognition system in use on the Moscow Metro was in line with the European Convention on Human Rights (ECHR). It was the Russian authorities' actual use of the automated facial recognition system in relation to the applicant that was at the heart of the case, not the question of which actors had potentially contributed to the development of the data system. The court concluded that the use of the system constituted a breach of the individual right to privacy established under Article 8 of the ECHR.

That claims of algorithmic discrimination are to be directed against the actors utilizing such systems as part of their operations highlights that such actors bear a responsibility both during the procurement phase and throughout the review of the systems. Given the complexity of AI systems, it is essential that these actors implement measures that can prevent individual cases of discrimination. Such measures include systems for assessing the risk of discrimination before a system's deployment, as well as ongoing monitoring systems once the system is deployed. Such structural mechanisms will be able to reduce the risk of discrimination, but individual protection would still need to be maintained through reliance on equality and anti-discrimination legislation.

In situations where an actor using an algorithmic system is found to have subjected individuals to discrimination and must pay damages and/or compensation, questions may arise regarding the possibility of seeking recourse from the entities involved in the system ' s development. This particular topic will not be examined further in this report.

82 Artificial Intelligence Act, Regulation (EU) 2024/1689, Article 3(4).

83 This has a small parallel to the human rights field, where it is well established that a state cannot avoid human rights obligations through leaving certain tasks to private actors.

84  Case no. 11519/20 ( Glukhin v. Russia ) (2023).

## 5.4 Can the prohibitions against instruction and participation widen the circle of responsible actors?

Section 15 of the EAD Act states that it is prohibited to instruct anyone to discriminate. The prohibition against instruction in the EAD Act forms part of the implementation of EU/EEA anti-discrimination directives into Norwegian law but has a wider scope than those directives, as it applies generally within the entire scope of the Equality and Anti-Discrimination Act. The prohibition is meant to cover ' orders, guidelines, exhortations, encouragements, etc. directed at one or more persons to discriminate ' . 85  The rationale for the prohibition is to ensure that it is not just those that directly or indirectly discriminate against someone that can be held liable, but also those who initiate or order discrimination.

The Equality and Anti-Discrimination Act also includes a prohibition against participation in discrimination. This is expressed in Section 16 of the Act, which states:

It is prohibited to participate in discrimination in breach of section 6, harassment in breach of section 13, retaliation in breach of section 14 or the issuing of instructions in breach of section 15.

In connection with the initial introduction of the prohibition against participation as a 2005 amendment to the 1978 Gender Equality Act, 86  the Ministry of Children and Families stated that the prohibition was to ' ensure that acts or omissions that could otherwise fall outside of ' the Gender Equality A ct's prohibition against discrimination would be covered. 87  The Ministry further emphasized that:

the framework for when participation is present cannot be generally defined, but will have to be based on a concrete interpretation that integrates the relation to unlawful differential treatment, instruction, retribution and harassment. Both physical and mental incitements can fall under the prohibition against participation. 88

The EU/EEA anti-discrimination directives do not contain a prohibition against participation. Therefore, the prohibition in the Equality and Anti-Discrimination Act means that Norwegian legislation establishes stricter protection against discrimination than the minimum requirements entailed by the directives. 89

A current issue concerns the potential significance of the prohibitions against instruction and participation in the face of algorithmic discrimination. Can, for instance, actors who have taken part in the development of a discriminating AI system be considered to have participated to the discrimination? Or can actors involved in the

85 Ot.prp. no. 35 (2004-2005) [Legislative Proposition] Point 5.2 (author's translation).

86 Lov om likestilling mellom kjønnene (likestillingsloven) [Act Relating to Gender Equality], 9 June 1978 no. 45 [repealed].

87 Ot.prp. no. 35 (2004-2005) [Legislative Proposition] Point 6.5 (author's translation).

88 Ot.prp. no. 35 (2004-2005) [Legislative Proposition] Point 6.5 (author's translation).

89 Ot.prp. no. 35 (2004-2005) [Legislative Proposition] Point 6.1.

development of knowledge-supporting AI systems that provide recommendations and predictions concerning what decision should be made be covered by the prohibition against instruction in cases where the result is a person being exposed to discrimination?

Gerards and Xenidis point out that the prohibition against instruction is not clearly defined either in the EU directives or through the interpretation of the European Court of Justice. They nevertheless think that ' an innovative interpretation of the notion of ' instruction to discriminate ' could effectively mitigate the substantive and procedural hurdles that arise in the context of algorithmic discrimination ' . 90  They do not rule out the possibility that the prohibition against instruction may be legally interpreted to cover situations where knowledge-supporting AI systems form part of the cause of discriminating decisions. They further point out that the prohibition against instruction may be positive in that it can comprise an addition/supplement to the prohibitions against direct and indirect discrimination (and the difficult delineation between these). Finally, they point out that the prohibition against instruction can function as a means to holding different actors in the value chain accountable -for example, by encouraging those who design algorithms used in AI systems to seek certification to avoid accusations of having instructed someone to discriminate. 91

In my view, the prohibition against participation may also be relevant in cases of algorithmic discrimination. The concept of participation is vague, and what is required in order for an actor to be considered as having participated in discrimination is therefore somewhat unclear. However, the prohibition against participation may possess a degree of flexibility that would make it suitable for addressing discrimination in complex situations. Along with the prohibition against direct and indirect discrimination, the prohibition against participation would make it possible to hold more than one actor responsible for discrimination.

A case handled by the Equality and Anti-Discrimination Tribunal highlights the possibility of holding several actors responsible for the same breach of the Equality and Anti-Discrimination Act. The case involved a pregnant woman (A) who was employed in a 50% position at the employment agency C. 92  She was hired for a temporary assignment at B. After it became known that A was pregnant, the assignment was not extended by B. The explanation provided for this was that, owing to her pending maternity leave, A would not be able to work throughout the coming summer. The employment agency C took no action to help ensure that A could still have her assignment extended with B. The Tribunal made the following statement about the relationship between the three parties:

The Tribunal takes the position that such a tripartite relationship does not mean that the responsibilities of the lessor or the lessee to act in accordance with anti-discrimination legislation are eliminated or reduced. The lessor cannot simply claim that they are not responsible for the

90  Gerards and Xenidis (2020) p. 143.

91  Gerards and Xenidis (2020) pp. 143-144.

92  Anti-Discrimination Tribunal, DIN-2019-115 and DIN-2019-196 (joint hearing).

actions of the lessee. Similarly, the lessee cannot merely point to having a contractual relationship only with the lessor and refer the employee to the lessor as the employer. The discrimination protection stands on its own, and it must be assessed specifically whether the lessee, the lessor or both have acted in a way that means the employee is, in fact, placed in a disadvantaged position compared to employees not covered by the relevant discrimination ground. 93

The Tribunal concluded that the lessee's decision not to extend A's assignment constituted discrimination due to pregnancy and maternity leave contrary to Section 6 of the EAD Act. In considering whether the employment agency had participated in the discrimination, the Tribunal claimed that ' both the employment agency and the lessee have to fulfil their responsibility to prevent the employee from being put in a disadvantaged position due to pregnancy and maternity leave ' . 94 In the Tribunal's view, the employment agency's passivity and failure to act toward the lessee meant that it had contributed to the discrimination in a way that contravened the prohibition against participation set out in Section 16 of the EAD Act. Both the employment agency and the lessee were ordered to pay damages to A. 95

In this case, both the employment agency and the lessee were held responsible for their actions. The prohibitions against discrimination and participation do not, however, require an intention to discriminate, something that will largely be absent in cases of algorithmic discrimination. The example from the Tribunal highlights the possible relevance of the prohibition against participation in cases where an actor may have acted in ways that facilitate discrimination, but where the discrimination itself happens in the relationship between a different set of actors.

In my view, the prohibition against participation might potentially be used in situations of algorithmic discrimination and will also -like the prohibition against instruction -potentially have a disciplining effect on actors working on the development and testing of algorithmic systems. The prohibitions against instruction and participation are likely to overlap in some cases, but it is possible that the prohibition against participation will capture a broader variety of actors than those considered relevant under the prohibition against instruction. The main thing to note in this context, however, is that the similarities between the prohibitions against instruction and participation mean that both are suitable for addressing the contributions of other actors to situations in which the users of an algorithmic system end up discriminating.

93 Author's translation.

94 Author's translation.

95  The damages awarded amounted to NOK 25,000 from the employment agency and NOK 15,000 from the lessee.

## 5.5 Summary and reflections going forward

## Important dimensions addressed in Chapter 5

- The prohibition against discrimination in the Equality and Anti-Discrimination Act is based on a closed list of discrimination grounds. In doing so, it builds on the approach adopted in the discrimination directives of EU/EEA law. However, there is a clear risk that algorithmic differential treatment will also affect groups other than those listed by the EAD Act. For instance, factors such as social status, health and level of education fall outside the list of discrimination grounds specified in the Act. Algorithmic discrimination highlights how it may be necessary to examine whether greater flexibility should be introduced into the law's specification of discrimination grounds to allow other groups to be covered by the prohibition against discrimination.
- The purpose clause of the Equality and Anti-Discrimination Act contains the formulation ' or other significant characteristics of a person ' . There is a tension between the closed listing of discrimination grounds in the prohibition against discrimination in Section 6 of the Act and the more flexible approach in the purpose clause in Section 1. Efforts to address the question of algorithmic discrimination may provide an opportunity to examine this issue further.
- An individual's claim of discrimination is to be directed against the private or public actor that has made a decision, formulated a ruling, etc., in relation to the individual, and that -as part of its operations -has used an algorithmic system.
- The Equality and Anti-Discrimination Act 's prohibitions against instruction and participation may contribute to the application of the prohibition against discrimination to a wider circle of actors, allowing, for example, developers of algorithmic systems' in some cases to be included as responsible parties. The content of the two prohibitions is underdeveloped. This is an area where the protection provided by the Equality and Anti-Discrimination Act is stronger than that entailed by the EU/EEA anti-discrimination directives. The EAD Act prohibits instruction within the entire scope of the Act, which is defined in Section 2(1) as ' all sectors of society' and is thus broader than the scope of the directives. The EU/EEA directives do not contain any prohibition on participation.

## Reflections going forward

This chapter has highlighted the need to consider the way forward for some of the provisions of the Equality and Anti-Discrimination Act.

- It may be necessary to consider the inclusion of an open-ended category in the Act's listing of discrimination grounds. If it is not considered desirable to introduce this at the general level, the inclusion of a specific prohibition against algorithmic discrimination might instead be considered. This prohibition could build on a positive enumeration of protected discrimination grounds while also

- accommodating the need for flexibility through the creation of an open-ended category.
- The prohibitions against instruction and participation should be examined in terms of their implications in the context of algorithmic discrimination.

## 6 Enforcement and sanctions

## 6.1 Introduction

Individuals wishing to make a claim of algorithmic discrimination have the possibility of bringing a case either before the Anti-Discrimination Tribunal or before the ordinary courts.

This chapter addresses two enforcement-related topics that are particularly relevant in the context of algorithmic discrimination. The first concerns whether organizations are also able to bring a case of algorithmic discrimination before the Tribunal or ordinary courts. The second concerns the sanctioning competency of those organs.

## 6.2 The possibility for organizations to have questions of algorithmic discrimination enforced

Individuals may not always want to take on the burden of making a complaint about discrimination by bringing a case either before the Anti-Discrimination Tribunal or before the ordinary courts. This gives rise to the question of whether it is possible for other actors to bring cases of alleged algorithmic discrimination before the Tribunal or the courts. Particularly relevant actors in such a context would include the Equality and Anti-Discrimination Ombud and various NGOs and interest organizations whose purpose it is to work against discrimination. Situations in which such institutions or organizations might want to have questions of algorithmic discrimination enforced might arise when it is known that individuals have been discriminated against as a result of how an algorithmic system has been used but the persons who have experienced this discrimination do not themselves wish to bring a case before the Tribunal or the ordinary courts. Another relevant situation would be cases of so-called content-based discrimination. 96

Section 8 of the Equality and Anti-Discrimination Ombud Act states: 'T he Tribunal processes the cases submitted to it. A party, the Ombud or others with legal standing may submit a case to the Tribunal. ' 97  The provision expressly grants the Ombud the possibility of bringing questions of algorithmic discrimination before the Tribunal. The

96 See Section 4.7 above.

97 In the official Norwegian text of the Act, the relevant passage is as follows: 'Nemnda behandler de sakene som bringes inn for den. En part, ombudet eller andre med rettslig klageinteresse kan bringe en sak inn for nemnda.' In the unofficial English translation of the Act, the second sentence has been translated as 'A party, the Ombud or other persons with legal standing may submit a case to the Tribunal' (emphasis added). The Norwegian phrase 'andre med rettslig klageinteresse', however, does not necessarily refer to persons, and has therefore been rendered here as 'others with legal standing'. See Lov om Likestillings- og diskrimineringsombudet og Diskrimineringsnemnda (diskrimineringsombudsloven) [Act Relating to the Equality and Anti-Discrimination Ombud and the Anti-Discrimination Tribunal (Equality and AntiDiscrimination Ombud Act)], 16 June 2017 no. 50.

provision also opens for interest organizations and NGOs to file a complaint with the Tribunal, provided that they fulfil the requirements for ' legal standing ' . 98

Section 1-4(1) and Section 1-3(2) of the Dispute Act stipulate that an organization or foundation 'may bring an action in its own name in relation to matters that fall within its purpose and normal scope', as long as it has 'a genuine need to have the claim decided against the defendant'. 99  Section 1-4(2) states: ' Public bodies charged with promoting specific interests may, in the same manner, bring an action in order to safeguard such interests. ' Thus, Section 1-4 of the Dispute Act allows both the Equality and Anti-Discrimination Ombud and organizations/NGOs/foundations working to prevent discrimination to file a lawsuit in cases concerning algorithmic discrimination. 100  Instead of filing a separate lawsuit under

Section 1-4, however, it may be possible for an organization to participate in a lawsuit filed by another. 101

The ability of the Equality and Anti-Discrimination Ombud and relevant organizations to bring lawsuits in cases of discrimination is in line with the system set out in the EU/EEA anti-discrimination directives. All of the EU/EEA anti-discrimination directives contain provisions related to the ability of relevant associations and organizations to initiate litigation concerning discrimination on behalf of or along with individuals. For example, Article 9(2) of the Framework Directive (2000/78) reads:

Member States shall ensure that associations, organisations or other legal entities which have, in accordance with the criteria laid down by their national law, a legitimate interest in ensuring that the provisions of this Directive are complied with, may engage, either on behalf or in support of the complainant, with his or her approval, in any judicial and/or administrative procedure provided for the enforcement of obligations under this Directive. 102

In the Accept case, which concerned the protection against direct discrimination of homosexuals, the European Court of Justice stated that the prohibition against direct discrimination in Directive 2000/78 may be enforced even without an ' identifiable

98 The details of this requirement are elaborated on in Hellum and Strand (2022) pp. 686ff.

99 Lov om mekling og rettergang i sivile tvister (tvisteloven) [Act Relating to Mediation and Procedure in Civil Disputes (The Dispute Act)], 17 June 2005 no. 90.

100 For a closer description of organizations' ability to bring suits, see Ot.prp. no. 51 (2004-2005) [Legislative Proposition], Chapter 29, special comments to §§1-4.

101 See Section 15-7 of the Dispute Act on third-party intervention and Section 15-8 on written submissions to highlight public interests. Under Section 15-7(1)(b), third-party intervention is allowed for 'associations, foundations and public bodies charged with promoting specific interests in cases that fall within the purpose and normal scope of the organisation pursuant to Section 1-4' of the Act. Under Section 15-8(1), 'Written submissions to highlight matters of public interest that are at stake in a case may be submitted by: a. organisations and associations within the purpose and normal scope of the organisation, or b. a public body within its area of responsibility.'

102 Similar formulations are found in Article 7(2) of the Racial Discrimination Directive (2000/43), Article 17(2) of the Gender Equality Directive (2006/54) and Article 8(3) of the Gender Equality Directive on Goods and Services (2004/113).

complainant who claims to have been the victim of such discrimination ' . 103  The Court further specified that the Directive in no way precludes a Member State from laying down, in its national law, the right of associations with a legitimate interest in ensuring compliance with that directive to bring legal or administrative proceedings to enforce the obligations resulting therefrom without acting in the name of a specific complainant or in the absence of an identifiable complainant. 104

Section 1-4 of the Dispute Act does not require that specific individuals must be discriminated against and/or involved for the Equality and Anti-Discrimination Ombud or an interest organization to file a lawsuit concerning discrimination. The approach in the Dispute Act therefore corresponds with the signals given by the European Court of Justice in the Accept case.

This is a topic that has practical implications, as there may be situations where it is known that an algorithmic system has discriminating effects even if there is no knowledge of individuals having been discriminated against by the relevant system. For example, privacy regulations may provide an obstacle to gathering information at the individual level. In such cases, it will be important that there is still a pathway for legal enforcement in relation to the question of discrimination.

For the Equality and Anti-Discrimination Act to be effective in the face of AI-related discrimination, the ability of organizations to initiate lawsuits may be central. Section 1-4 of the Dispute Act offers organizations access to legal action regardless of whether discriminated individuals are identified or involved in the lawsuit. However, in practice, such access will largely depend on the resources that relevant organizations, such as the Equality and Anti-Discrimination Ombud, have for prioritizing such cases. There is good reason to focus on the resources available for prioritizing such cases in the times ahead.

## 6.3 The sanctioning competence of the Anti-Discrimination Tribunal and the ordinary courts

In cases of algorithmic discrimination, it must be possible to seek enforcement through the same organs that are used in other cases of discrimination.

When an actor is considering whether to bring a case of algorithmic discrimination before the Anti-Discrimination Tribunal or before the ordinary courts, knowledge of the sanctioning competence of these organs will be highly relevant.

103  C-81/12 ( Accept ) (2013), Para. 36.

104  C-81/12 ( Accept ), Para. 37.

According to the EU/EEA anti-discrimination directives, member-states must ensure that violations of the prohibition against discrimination are subject to effective, proportionate and dissuasive sanctions. 105

Section 38 of the EAD Act seeks to implement this requirement and establishes that a ' person who is the subject of treatment in breach of ' the prohibition against discrimination can demand compensation and damages. In the Feryn case, the European Court of Justice clarified that the requirement for effective sanctioning also applies when an organization files a discrimination lawsuit and there is no individual victim involved:

In a case such as that referred by the national court, where there is no direct victim of discrimination but a body empowered to do so by law seeks a finding of discrimination and the imposition of a penalty, the sanctions which Article 15 of Directive 2000/43 requires to be laid down in national law must also be effective, proportionate and dissuasive. 106

In order to be in line with the relevant EU/EEA directives, Section 38 of the EAD Act would need to be interpreted in such a way that it encompasses situations where an organization brings a case before the Tribunal or a lawsuit before the ordinary courts.

There are no substantive limitations on the competency of the ordinary courts to hear cases on discrimination, and the courts may also award compensation and/or damages in cases across all sectors of society. This means that the courts are a relevant addressee for all cases related to algorithmic discrimination.

With regard to the competency of the Tribunal, however, the picture is a little more complicated. The T ribunal's competency to award compensation and damages is regulated in Section 12 of the Equality and Anti-Discrimination Ombud Act, where it is stated that the Tribunal may only award compensation for non-economic loss [ oppreisning ] ' in the context of an employment relationship ' . Algorithmic discrimination may arise in relation to employment relationships, and in such cases the Tribunal will as a rule be able to award compensation if a violation of the prohibition against discrimination is found to have occurred. In all other areas, compensation would have to be awarded by the ordinary courts.

As for the competency to award damages, the T ribunal's competency is limited to so -called simple cases, meaning situations where ' the only submissions made by the respondent relate to inability [to] pay or other manifestly untenable objections ' . 107

The Tribunal has competency to make decisions on breaches of the prohibition against discrimination in cases across most sectors of society (see Section 11 of the Equality and Anti-Discrimination Ombud Act). There are some limitations to the

105 See Article 15 of the Racial Discrimination Directive (2000/43), Article 17 of the Framework Directive (2000/78), Article 25 of the Gender Equality Directive (2006/54) and Article 14 of the Gender Equality Directive on Goods and Services (2004/113).

106  C-54/07 ( Feryn ) (2008), Premise 38. See also C-81/12 ( Accept ), Premise 62.

107 See the Equality and Anti-Discrimination Ombud Act, Section 12. The unofficial English translation of this section of the Act appears to include a translation error, which has been corrected by the author in the version given here.

substantive competency of the Tribunal, but these are likely of little significance in the context of algorithmic discrimination. 108  This means that if the most important thing is to obtain a finding that discrimination has occurred, the Tribunal could be an important actor. This must also be seen in relation to the T ribunal's role as a free low -threshold enforcement body. At the same time, algorithmic discrimination triggers complicated factual and legal questions, which may lead to questions about whether the current structuring of the Tribunal renders it suitable for handling such cases.

## 6.4 Summary and reflections going forward

## Important dimensions addressed in Chapter 6

- The ability of organizations to enforce issues of algorithmic discrimination may prove important in the future. Both the Equality and Anti-Discrimination Ombud (LDO) and organizations/NGOs/foundations working to prevent discrimination can, according to Section 1-4 of the Dispute Act, file lawsuits in cases concerning algorithmic discrimination. The provision does not require that specific individuals must have been discriminated against and/or must be involved in order for such organizations to be able to initiate a lawsuit. In addition, it follows from Section 8 of the Equality and Anti-Discrimination Ombud Act that the Ombud or organizations etc. with ' legal standing ' may submit a case to the Tribunal.
- The EU/EEA anti-discrimination directives require effective sanctioning of the prohibition against discrimination. This requirement also applies in cases where it is an organization that is the complainant or plaintiff.
- The limited sanctioning competency of the Tribunal in cases outside the employment sector may make it more appropriate to bring cases of algorithmic discrimination before the ordinary courts. This highlights that there may be a need for a discussion on what the role of the Tribunal should be in the context of algorithmic discrimination.

## Reflections going forward

- Awareness of the roles of the Equality and Anti-Discrimination Ombud and various interest organizations will be crucial in ensuring that issues of algorithmic discrimination are enforced and controlled in practice.
- The path forward for the Ombud and various interest organizations could involve a deliberate strategy in relation to the possibility of bringing cases before the courts or the Anti-Discrimination Tribunal. The path forward might also involve efforts to ensure that, in practice, algorithmic systems and their effects are monitored and controlled in relation to the prohibition against discrimination.

108 The limitations of the Tribunal's competency are discussed in Hellum and Strand (2022) pp. 665ff.

- Further work should be based on the substantive prohibition against discrimination as it is currently regulated under Sections 6 -9 of the EAD Act and how it might possibly be adjusted in the light of the new challenges posed by the use of algorithmic systems.

## References

## Works cited

- Ballangrud, Anne Jorun Bolken, and Margrethe Søbstad (2021) Likestillings- og diskrimineringsloven. Lovkommentar [The Equality and Anti-Discrimination Act: Commentary]. Oslo: Universitetsforlaget.
- Borgesius, Frederik Zuiderveen (2018) Discrimination, Artificial Intelligence, and Algorithmic Decision-Making . Strasbourg: Council of Europe. Available at: https://rm.coe.int/discrimination-artificial-intelligence-and-algorithmic-decisionmaking/1680925d73.
- Datta, Anupam, Matt Fredrikson, Gihyuk Ko, Piotr Mardziel and Shayak Sen (2017) ' Proxy Discrimination in Data-Driven Systems '. https://arxiv.org/pdf/1707.08120.
- European Parliament (2023) 'Artificial Intelligence Act: Deal on Comprehensive Rules for Trustworthy AI ' , press release, 9 December 2023. Available at: https://www.europarl.europa.eu/news/en/pressroom/20231206IPR15699/artificial-intelligence-act-deal-on-comprehensive-rulesfor-trustworthy-ai (accessed 23 January 2025).
- Future of Life Institute (2024) 'High -Level Summary of the AI Act' . Available at https://artificialintelligenceact.eu/high-level-summary/ (accessed 23 January 2025).
- Gerards, Janneke, and Raphaële Xenidis (2020) Algorithmic Discrimination in Europe: Challenges and Opportunities for Gender Equality and Non-Discrimination Law. Brussels: European Commission. Available at: https://www.equalitylaw.eu/downloads/5361-algorithmic-discrimination-in-europepdf-1-975.
- Haram, Åse (2021) Forklarbarhet i algoritmisk velferdsforvaltning. Demokrati og rettssikkerhet ved bruk av maskinlæringsmodeller i saksbehandlingen [Explainability in Algorithmic Welfare Administration: Democracy and Legal Safeguards in the Use of Machine Learning Models in Case Processing]. Master 's degree thesis, Faculty of Law, University of Oslo. Available at: https://www.jus.uio.no/ifp/forskning/om/publikasjoner/complex/2021/202106.html.
- Hauglid, Mathias Karlsen (2024) Bias and Discrimination in Clinical Decision Support Systems Based on Artificial Intelligence. PhD dissertation, University of Tromsø.
- Hellum, Anne, and Vibeke Blaker Strand (2022) Likestillings- og diskrimineringsrett [Equality and Non-Discrimination Law]. Oslo: Gyldendal.
- Hellum, Anne, Ingunn Ikdahl and Vibeke Blaker Strand (2024) ' Between Norms and Institutions: Unlocking the Transformative Potential of Norwegian Equality and AntiDiscrimination Law'. In: Anne Hellum, Ingunn Ikdahl, Vibeke Blaker Strand and EvaMaria Svensson (eds) Nordic Equality and Anti-Discrimination Laws in the Throes of Change: Legal Developments in Sweden, Finland, Norway, and Iceland, pp. 130 -190. Abingdon: Routledge. https://doi.org/10.4324/9781003172840 (open access).

- Likestillings- og diskrimineringsombudet [Equality and Anti-Discrimination Ombud] (2023) 'Innebygd diskrimineringsvern' [Built-In Protection Against Discrimination] . Available at: https://ldo.no/aktuelt/innebygd-diskrimineringsvern/ (accessed 23 January 2025).
- Naudts, Laurens (2019) ' How Machine Learning Generates Unfair Inequalities and How Data Protection Instruments May Help in Mitigating Them '. I n Ronald Leenes, Rosamunde van Brakel, Serge Gutwirth and Paul de Hert (eds) Data Protection and Privacy: The Internet of Bodies , pp. 71 -92. Oxford: Hart Publishing.
- Regjeringen [Norwegian Government] (2023) 'Likelønnsdirektivet' [Pay Transparency Directive]. EØS-notatbasen. Available at: https://www.regjeringen.no/no/sub/eosnotatbasen/notatene/2021/mai/eus-likelonnsdirektiv/id2856216/ (accessed 23 January 2025).
- Regjeringen [Norwegian Government] (2024) 'Forordning om kunstig intelligens (KI -forordningen)' [Artificial Intelligence Act]. EØS-notatbasen. Available at: https://www.regjeringen.no/no/sub/eos-notatbasen/notatene/2021/juni/forslag-tilforordning-om-kunstig-intelligens-ki-forordningen/id2884935/ (accessed 23 January 2025).
- Sartor, Giovanni, and Francesca Lagioia (2020) The Impact of the General Data Protection Regulation on Artificial Intelligence . Brussels: European Parliament. https://data.europa.eu/doi/10.2861/293.
- Schartum, Dag Wiese (2019) Digitalisering av offentlig forvaltning. Fra lovtekst til programkode [Digitalization of Public Administration: From Legal Text to Program Code]. Bergen: Fagbokforlaget.
- Strümke, Inga (2023) Maskiner som tenker. Algoritmenes hemmelighet og veien til kunstig intelligens [Machines That Think: The Secret of Algorithms and the Path to Artificial Intelligence]. Oslo: Kagge forlag.
- Strümke, Inga, Marija Slavkovik and Clemens Stachl (2023) ' Against Algorithmic Exploitation of Human Vulnerabilities '. https://arxiv.org/pdf/2301.04993.
- Van Bekkum, Marvin, and Frederik Zuiderveen Borgesius (2023) ' Using Sensitive Data to Prevent Discrimination by Artificial Intelligence: Does the GDPR Need a New Exception? ' Computer Law &amp; Security Review 48: 105770. https://doi.org/10.1016/j.clsr.2022.105770.
- Wachter, Sandra (2022) 'The Theory of Artificial Immutability: Protecting Algorithmic Groups Under Anti-Disc rimination Law'. Tulane Law Review 97(2): 149. http://dx.doi.org/10.2139/ssrn.4099100.
- Wachter, Sandra, Brent Mittelstadt and Chris Russell (2021) ' Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI '. Computer Law &amp; Security Review 41: 105567. https://dx.doi.org/10.2139/ssrn.3547922.
- Xenidis, Raphaële (2020) 'Tuning EU Equality Law to Algorithmic Discrimination: Three Pathways to Resilience'. Maastricht Journal of European and Comparative Law 27(6): 736 -758.

## Legal instruments and cases

## National legislation

Lov om likestilling mellom kjønnene (likestillingsloven) [Act Relating to Gender Equality] 9 June 1978 no. 45 [Repealed].

Lov om arbeidsmiljø, arbeidstid og stillingsvern mv. (arbeidsmiljøloven) [Act Relating to the Working Environment, Working Hours and Employment Protection, etc. (Working Environment Act) 17 June 2005 no. 62. [An unofficial English translation of this act is available at https://lovdata.no/NLE/lov/2005-06-17-62.]

Lov om mekling og rettergang i sivile tvister (tvisteloven) [Act Relating to Mediation and Procedure in Civil Disputes (The Dispute Act)] 17 June 2005 no. 90. [An unofficial English translation of this act is available at https://lovdata.no/NLE/lov/2005-06-17-90.]

Lov om Likestillings- og diskrimineringsombudet og Diskrimineringsnemnda (diskrimineringsombudsloven) [Act Relating to the Equality and Anti-Discrimination Ombud and the Anti-Discrimination Tribunal (Equality and Anti-Discrimination Ombud Act)] 16 June 2017 no. 50. [An unofficial English translation of this act is available at https://lovdata.no/NLE/lov/2017-06-16-50.]

Lov om likestilling og forbud mot diskriminering (likestillings- og diskrimineringsloven) [Act Relating to Equality and a Prohibition Against Discrimination (Equality and AntiDiscrimination Act)] 16 June 2017 no. 51 (referred to in the report as 'the Equality and AntiDiscrimination Act' or 'the EAD Act ). [An unofficial English translation of this act is available at https://lovdata.no/NLE/lov/2017-06-16-51.]

Lov om behandling av personopplysninger (personopplysningsloven) [Act Relating to the Processing of Personal Data (The Personal Data Act)] 15 June 2018 no. 38. [An unofficial English translation of this act is available at https://lovdata.no/NLE/lov/2018-06-15-38.]

## Conventions

Convention for the Protection of Human Rights and Fundamental Freedoms (also known as the European Convention on Human Rights or ECHR), Rome, 4 November 1950.

International Covenant on Civil and Political Rights (ICCPR), 16 December 1966.

International Covenant on Economic, Social and Cultural Rights (ICESCR), 16 December 1966.

## EU/EEA law regulations

Charter of Fundamental Rights of the European Union (EU Charter), adopted 7 December 2000, entry into force 1 December 2009.

Treaty on the Functioning of the European Union (TFEU), adopted 13 December 2007, entry into force 1 December 2009.

- Council Directive 2000/43/EC of 29 June 2000 Implementing the Principle of Equal Treatment Between Persons Irrespective of Racial or Ethnic Origin (EU Racial Discrimination Directive).
- Council Directive 2000/78/EC of 27 November 2000 Establishing a General Framework for Equal Treatment in Employment and Occupation (EU Framework Directive).
- Council Directive 2004/113/EC of 13 December 2004 Implementing the Principle of Equal Treatment Between Men and Women in the Access to and Supply of Goods and Services (Gender Equality Directive on Goods and Services).
- Directive 2006/54/EC of the European Parliament and of the Council of 5 July 2006 on the Implementation of the Principle of Equal Opportunities and Equal Treatment of Men and Women in Matters of Employment and Occupation (recast) (Gender Equality Directive).
- Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the Protection of Natural Persons with Regard to the Processing of Personal Data and on the Free Movement of Such Data, and Repealing Directive 95/46/EC (General Data Protection Regulation, GDPR).
- Directive (EU) 2023/970 of the European Parliament and of the Council of 10 May 2023 to Strengthen the Application of the Principle of Equal Pay for Equal Work or Work of Equal Value Between Men and Women Through Pay Transparency and Enforcement Mechanisms (Pay Transparency Directive).
- Council Directive (EU) 2024/1499 of 7 May 2024 on Standards for Equality Bodies in the Field of Equal Treatment Between Persons Irrespective of Their Racial or Ethnic Origin, Equal Treatment in Matters of Employment and Occupation Between Persons Irrespective of Their Religion or Belief, Disability, Age or Sexual Orientation, Equal Treatment Between Women and Men in Matters of Social Security and in the Access to and Supply of Goods and Services, and Amending Directives 2000/43/EC and 2004/113/EC.
- Directive (EU) 2024/1500 of the European Parliament and of the Council of 14 May 2024 on Standards for Equality Bodies in the Field of Equal Treatment and Equal Opportunities Between Women and Men in Matters of Employment and Occupation, and Amending Directives 2006/54/EC and 2010/41/EU.
- Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 Laying Down Harmonised Rules on Artificial Intelligence and Amending Regulations (EC) No. 300/2008, (EU) No. 167/2013, (EU) No. 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act).

## National preparatory works

- Prop.81 L (2016 -2017) Lov om likestilling og forbud mot diskriminering (likestillingsog diskrimineringsloven) [Act on Equality and Anti-Discrimination]. Available at: https://lovdata.no/static/PROP/prop-201617-081.pdf.

Ot.prp. nr. 35 (2004 -2005) Om lov om endringer i likestillingsloven mv.

(Gjennomføring av Europaparlaments- og rådsdirektiv 2002/73/EF og innarbeiding av FN-konvensjonen om avskaffelse av alle former for diskriminering av kvinner med tilleggsprotokoll i norsk lov) [Regarding the Act on Amendments to the Gender Equality Act, etc.].

Ot.prp. nr. 51 (2004 -2005) Om lov om mekling og rettergang i sivile tvister (tvisteloven) [Regarding the Dispute Act].

## European Court of Human Rights

Application no. 11519/20 ( Glukhin v. Russia ) (2023)

## Court of Justice of the European Union

C-109/88 ( Danfoss ) (1989)

C-127/92 ( Enderby ) (1993)

C-400/93 ( Royal Copenhagen ) (1995)

C-54/07 ( Feryn ) (2008)

C-81/12 ( Accept ) (2013)

C-804/18 and C-341/19 (joined cases: WABE and Müller ) (2021)

C-148/22 ( OP ) (2023)

## The Norwegian Anti-Discrimination Tribunal

Case DIN-2019-115 and DIN-2019-196 (joint hearing), 29 January 2020

<!-- image -->

## The Equality and AntiDiscrimination Ombud

## Contact us!

We provide free legal advice.

Website: htttp://www.LDO.no

Email: post@LDO.no

Telephone: (+47) 23 15 73 00

<!-- image -->

<!-- image -->

<!-- image -->

- facebook.com/mittOmbud

- twitter.com/mittOmbud

- instagram.com/mittOmbud

Page 60 of 60