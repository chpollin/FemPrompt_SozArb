---
title: "Kattnig 2024 Assessing"
original_document: Kattnig_2024_Assessing.md
document_type: Research Paper
research_domain: AI Ethics|AI Bias & Fairness
methodology: Comparative Analysis|Literature Review
keywords: AI fairness, bias mitigation, non-discrimination, AI Act, trustworthy AI
mini_abstract: "This paper investigates the alignment between technical bias mitigation methods and legal requirements for fairness in AI systems, with particular focus on EU regulations and the AI Act. It argues that few existing technical solutions adequately meet legal standards for non-discrimination."
target_audience: Researchers|Policymakers|Industry|Practitioners
key_contributions: "Bridging technical and legal perspectives on AI fairness compliance"
geographic_focus: Europe
publication_year: Unknown
related_fields: AI Governance, Legal Technology, Responsible AI
summary_date: 2025-11-07
language: English
ai_model: claude-haiku-4-5
---

# Summary: Kattnig 2024 Assessing

## Overview

This academic paper addresses a critical contemporary challenge in artificial intelligence: ensuring fairness and non-discrimination in AI systems through integrated technical and legal analysis. Published in *Computer Law & Security Review*, the work by Kattnig et al. from Graz University of Technology examines the fundamental gap between technical bias mitigation methods and legal compliance requirements, particularly within the European Union regulatory framework and the emerging AI Act. The research recognizes that as AI systems increasingly influence consequential decisions affecting human lives—from hiring algorithms to criminal risk assessment—the imperative to ensure fair, unbiased decision-making has become paramount. The paper's central premise challenges the prevailing assumption that technical solutions for fairness automatically satisfy legal standards, arguing instead for an integrated approach that bridges computer science and legal scholarship to establish trustworthy AI governance.

## Main Findings

The research reveals several critical gaps between technical and legal approaches to AI fairness. Most significantly, few existing bias mitigation methods adequately meet legal requirements for non-discrimination under EU regulations and the AI Act. The paper identifies that bias—defined as systematic and unfair behavior in AI systems—often emerges when training data contains historical inequalities, inadvertently perpetuating discrimination against already disadvantaged groups. The COMPAS algorithm case study exemplifies this problem, demonstrating how racial bias becomes embedded in decision-making systems with serious social consequences. Additional findings indicate that bias identification remains technically challenging despite widespread AI deployment, creating operational and compliance risks. The paper reveals that fairness definitions remain contested across technical and legal disciplines, creating conceptual confusion that impedes implementation. Crucially, AI systems lack "common sense" or causal reasoning capabilities, compounding fairness challenges beyond statistical bias mitigation. The authors conclude that comprehensive legal methodology is essential for proper AI fairness assessment, and that technical bias mitigation alone proves insufficient without legal validation and alignment with regulatory requirements.

## Methodology/Approach

The paper employs a comparative analytical framework that systematically reviews state-of-the-art bias mitigation technical methods while contrasting them against legal requirements. The geographic scope is limited to the European Union, with particular emphasis on AI Act compliance and existing legal frameworks. The methodology examines both fairness definitions and measurement approaches, identifying conceptual challenges in operationalizing fairness across disciplines. The framework analyzes both group fairness (equitable treatment across demographic groups) and individual fairness (similar treatment for similar individuals). This comparative approach deliberately bridges disciplinary boundaries, recognizing that neither purely technical optimization nor purely legal analysis suffices independently. The methodology acknowledges that bias identification remains technically challenging, requiring integrated expertise from computer science, statistics, and legal scholarship.

## Relevant Concepts

**Bias**: Systematic and unfair behavior or errors in AI systems leading to discriminatory outcomes and unjust decisions.

**Group Fairness**: Ensuring equitable treatment across demographic groups or protected categories in AI decision-making.

**Individual Fairness**: Ensuring similar individuals receive similar treatment from AI systems regardless of protected characteristics.

**Trustworthy AI**: AI systems operating transparently, safely, and in compliance with legal, ethical, and regulatory standards.

**Bias Mitigation**: Technical methods (pre-processing, in-processing, post-processing) designed to reduce or eliminate discriminatory outcomes.

**Non-discrimination**: Legal principle ensuring AI decisions do not unfairly disadvantage protected groups or individuals.

**Causality**: AI system's ability to infer cause-and-effect relationships, enabling "common sense" reasoning beyond statistical patterns.

**Legal Compliance Gap**: Mismatch between technical fairness metrics and actual legal requirements under EU regulations and AI Act.

**Fairness Metrics**: Quantitative measures assessing whether AI systems meet fairness standards across different definitions.

## Significance

This work significantly advances the emerging field of trustworthy AI governance by challenging disciplinary silos and advocating for integrated expertise. It contributes to responsible AI discourse by emphasizing that legal compliance must guide technical implementation, not follow it. The paper's position as a "bridge-builder" between computer science and legal studies reflects growing recognition that AI regulation requires interdisciplinary collaboration. By examining AI Act requirements specifically, the research provides practical guidance for EU compliance while advancing theoretical understanding of fairness in AI. The work establishes that data subjects' rights to fair, non-discriminatory treatment demand systematic solutions transcending technical optimization alone. For organizations deploying AI systems, the research highlights that achieving fairness requires alignment between technical bias mitigation approaches and legal frameworks, not merely technical excellence. The paper establishes a foundation for future regulatory frameworks, technical standards development, and organizational compliance strategies in trustworthy AI implementation.
