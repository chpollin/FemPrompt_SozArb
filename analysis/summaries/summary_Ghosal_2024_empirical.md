---
title: "Ghosal 2024 empirical"
original_document: Ghosal_2024_empirical.md
document_type: Empirical Study
research_domain: AI Ethics, AI Bias & Fairness, Generative AI
methodology: Qualitative, Case Study, Visual Analysis
keywords: Stable Diffusion, intersectionality, visual bias, algorithmic reparation, generative AI
mini_abstract: "Critical analysis of Stable Diffusion's generated imagery reveals systematic perpetuation of intersecting power systems including racism, sexism, and ableism, with Euro-American cultural hegemony embedded in the technology's outputs."
target_audience: Researchers, Policymakers, Industry, Practitioners
key_contributions: "Intersectional framework for analyzing generative AI visual bias"
geographic_focus: Global
publication_year: 2025
related_fields: Feminist Science and Technology Studies, Visual Media Studies, Critical Theory
summary_date: 2025-11-07
language: English
ai_model: claude-haiku-4-5
---

# Summary: Ghosal 2024 empirical

## Overview

This peer-reviewed paper by an international, multidisciplinary team examines Stable Diffusion, a widely-adopted open-source visual generative AI tool that has generated over 12 billion images since 2022—surpassing 150 years of human photography in just two years. The authors fundamentally challenge the pervasive myth of technological neutrality, demonstrating that vGenAI systems are not culturally or aesthetically neutral but actively function as agents perpetuating systemic inequalities. Through intersectional analysis of 180 deliberately-constructed SD-generated images, the research reveals how these technologies encode and amplify power structures related to race, gender, class, and ability. The paper argues that SD's institutional contexts—rooted in Euro- and North America-centric perspectives—produce a default subject that is white, able-bodied, and masculine-presenting, while systematically marginalizing other social groups through "continual reproduction of harmful and violent imagery."

## Main Findings

The analysis identifies three interconnected dimensions of systematic bias reproduction. First, SD-generated imagery perpetuates aesthetic encoding of power through visual politics that actively mirror and reinforce societal assumptions about categories, values, and aesthetic norms—rather than passively reflecting them. Second, the technology simultaneously reproduces intersecting power systems (sexism, racism, heteronormativity, ableism) in compounded rather than isolated ways, establishing hierarchies of representation across multiple social dimensions. Third, institutional contexts producing SD reflect hegemonic Euro- and North America-centric cultural values, resulting in systematic exclusion and violent misrepresentation of non-Western perspectives and marginalized communities. Critically, the authors demonstrate that SD operates at unprecedented scale—generating 2 million images daily across 10 million users—amplifying these harms exponentially beyond traditional media systems.

## Methodology/Approach

The study employs qualitative, interpretative visual analysis grounded in three integrated theoretical traditions: feminist science and technology studies, visual media studies, and intersectional critical theory. Researchers deliberately constructed prompts generating images along multiple axes of privilege and disadvantage (wealth/poverty, citizen/immigrant status, ability/disability) to systematically examine representational patterns. Analysis operates across three dimensions: aesthetic properties of generated images, institutional contexts producing these tools, and intersections between multiple power systems reflected and perpetuated through visual aesthetics. This approach commits to "rendering visible" the cultural-aesthetic politics embedded in technology, moving beyond technical bias detection toward critical examination of how training data, institutional values, and output aesthetics interconnect to reproduce inequality.

## Relevant Concepts

**Intersectionality**: Framework analyzing how multiple social categories (race, gender, class, ability) interact simultaneously to create compounded discrimination rather than additive effects.

**Algorithmic reparation**: Proposed approach addressing historical harms through technology by acknowledging cultural-aesthetic politics and implementing restorative justice mechanisms.

**Aesthetic neutrality myth**: The false assumption that visual generative AI systems produce culturally neutral outputs, obscuring their active role in perpetuating power structures.

**Visual politics**: Mechanisms through which power structures are encoded, naturalized, and perpetuated through visual representation and aesthetic choices.

**Institutional hegemony**: Dominance of particular cultural values embedded within institutional structures producing technology, resulting in systematic exclusion of alternative perspectives.

**Rendering visible**: Methodological commitment to making explicit the cultural-aesthetic politics typically naturalized or obscured within technological systems.

## Significance

This work makes crucial contributions to critical AI studies by establishing intersectionality as central rather than peripheral to AI critique. The paper's innovative call for "algorithmic reparation" and restorative justice moves beyond conventional bias-detection frameworks toward transformative remediation addressing historical injustices. By emphasizing visual representation's role in perpetuating inequality at massive scale, the authors highlight how generative AI systems function as cultural technologies actively encoding and amplifying existing power structures. The research provides empirical grounding for social justice-oriented approaches to AI development and governance, challenging industry narratives of neutrality. The international, multidisciplinary authorship demonstrates the necessity of bridging technology studies, media studies, and critical social theory to adequately address AI's social implications.
