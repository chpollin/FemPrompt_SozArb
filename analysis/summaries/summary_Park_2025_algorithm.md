---
title: "Park 2025 algorithm"
original_document: Park_2025_algorithm.md
document_type: Empirical Study
research_domain: AI Ethics
methodology: Experimental
keywords: AI transparency, trust-building, algorithm accountability, stakeholder communication, generative AI
mini_abstract: "This experimental study demonstrates that AI algorithm transparency significantly mitigates negative attitudes toward AI and enhances trust in parent companies, particularly among highly involved stakeholders, proposing a knowledge-centric pipeline model over reputation-focused approaches."
target_audience: Researchers, Industry, Policymakers
key_contributions: "Empirical evidence linking AI transparency to trust mitigation"
geographic_focus: Global
publication_year: 2025
related_fields: Corporate Communication, Organizational Trust, AI Governance
summary_date: 2025-11-07
language: English
ai_model: claude-haiku-4-5
---

# Summary: Park 2025 algorithm

## Overview

This empirical study investigates how AI algorithm transparency influences stakeholder trust in AI systems and parent organizations, addressing a critical gap in AI trust literature. Conducted by Park and Yoon, the research responds to widespread public ambivalence toward AI—where only 41% of US adults support AI development while 22% oppose it—by providing evidence-based trust-building strategies. The authors propose a fundamental theoretical reconceptualization: shifting from a "reputation-focused prism model" (transparency as passive information filter) to a "knowledge-centric pipeline model" (transparency as active trust-building mechanism). This paradigm shift positions transparency not merely as disclosure but as a strategic signaling mechanism that reduces uncertainty, enhances knowledge, and demonstrates organizational accountability.

## Main Findings

The experimental results demonstrate that **high AI algorithm transparency significantly mitigates the negative relationship between general negative attitudes toward AI and trust in parent companies**. Critically, this mitigating effect is **strongest among individuals with high issue involvement**—those most engaged with and concerned about AI implications. Conversely, transparency shows weaker effects for low-involvement stakeholders. The research reveals that transparency operates through dual mechanisms: (1) enhancing cognitive understanding of algorithmic processes, and (2) signaling organizational commitment to accountability and ethical practices. These findings validate that transparency can transform skepticism into confidence, particularly among actively engaged stakeholders. The study demonstrates that transparency functions as a credible accountability signal capable of overcoming predisposed distrust toward AI technology itself.

## Methodology/Approach

The study employs a rigorous 2×2 between-subjects experimental design with two manipulated variables: AI algorithm transparency (high versus low conditions) and issue involvement (high versus low engagement levels). Conducted as an online experiment, this design enables causal inference while controlling confounding variables. The dependent variable measures trust in parent companies using AI systems. The theoretical framework integrates trust theory, stakeholder communication literature, and algorithmic accountability discourse, grounding research in established traditions while addressing contemporary concerns about algorithmic bias and radicalization. This methodological approach systematically examines transparency's interactive effects with stakeholder characteristics on trust dynamics.

## Relevant Concepts

**General Negative Attitude toward AI**: Predisposed skepticism and distrust toward AI technology arising from uncertainty, unpredictability, and fear of unknown consequences.

**AI Algorithm Transparency**: The degree to which AI decision-making processes, logic, and outcomes are comprehensible and accessible to stakeholders, functioning as both informational and accountability mechanisms.

**Issue Involvement**: The level of personal engagement, concern, and cognitive investment stakeholders maintain regarding AI technology and its implications.

**Reputation-focused Prism Model**: Traditional framework conceptualizing transparency as a passive filter through which organizational reputation is perceived, emphasizing reputation management.

**Knowledge-centric Pipeline Model**: Proposed framework conceptualizing transparency as an active conduit reducing uncertainty and building trust through knowledge enhancement and accountability signaling.

**Signaling Mechanism**: Organizational communication credibly conveying trustworthiness, ethical commitment, and accountability, reducing information asymmetries between organizations and stakeholders.

## Significance

This research holds substantial practical and theoretical importance. For organizations, it provides empirical validation that transparency investments yield measurable trust benefits, particularly among engaged stakeholders—suggesting differentiated communication strategies based on audience involvement. For academic discourse, it bridges policy frameworks (EU AI Act) with empirical evidence, addressing documented gaps between regulatory emphasis on transparency and practical implementation strategies. The identification of issue involvement as a critical moderating factor refines understanding of when transparency proves most effective. The theoretical contribution—the prism-to-pipeline model shift—reconceptualizes transparency's role in organizational communication. However, limitations include the experimental setting's potential constraints on real-world generalizability, sample characteristics not detailed in available text, and unclear applicability across diverse organizational contexts and stakeholder populations. Future research should examine transparency effects longitudinally and across varied implementation contexts to validate findings' robustness and external validity.
