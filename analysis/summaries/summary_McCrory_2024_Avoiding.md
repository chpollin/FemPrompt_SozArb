---
title: "McCrory 2024 Avoiding"
original_document: McCrory_2024_Avoiding.md
document_type: Policy Document
research_domain: AI Ethics
methodology: Comparative Analysis
keywords: AI safety governance, intersectionality, feminist policy analysis, marginalized communities, existential risk
mini_abstract: "This working paper critiques AI safety governance for overlooking how future AI risks intersect with current harms experienced by marginalized groups, proposing feminist intersectional frameworks to improve policy development and accountability."
target_audience: Policymakers, Researchers, Mixed
key_contributions: "Integrating feminist intersectionality into AI safety governance frameworks"
geographic_focus: Global
publication_year: 2025
related_fields: Science and Technology Studies, Policy Studies, Critical Theory
summary_date: 2025-11-07
language: English
ai_model: claude-haiku-4-5
---

# Summary: McCrory 2024 Avoiding

## Overview

This working paper from CIGI's Digital Policy Hub, supported by Mitacs partnership, addresses a critical gap in artificial intelligence governance by integrating feminist intersectional analysis into AI safety discourse. The research, authored by Laine McCrory, challenges the predominant AI safety movement—which focuses on existential risks and system alignment with human values—for treating these risks as universally impactful while overlooking how marginalized communities already experience disproportionate harms from current AI systems. The paper argues that meaningful AI governance requires moving beyond technocratic approaches that claim neutrality, instead recognizing how future AI risks are fundamentally interconnected with existing structural inequalities and power dynamics. By applying feminist policy analysis frameworks organized around five thematic dimensions (intersectionality, context, neutrality, control, and power), the work bridges traditionally separate scholarly domains to propose more equitable governance pathways.

## Main Findings

The analysis reveals five systematic deficiencies in current global AI safety initiatives:

1. **Limited feminist engagement:** AI safety policies demonstrate insufficient meaningful engagement with feminist principles and accountability mechanisms.

2. **Disconnected temporality:** Governance frameworks fail to establish explicit connections between hypothetical future existential risks and observable present-day harms already experienced by marginalized communities.

3. **Unacknowledged differential impacts:** Current AI systems replicate existing social biases and power hierarchies, yet policy frameworks inadequately recognize how marginalized groups face disproportionate existential risks from these systems.

4. **Exclusionary participation:** Affected communities lack meaningful participation mechanisms in policy development processes, limiting governance legitimacy and effectiveness.

5. **False universalism:** AI safety discourse operates with implicit universalism—assuming risks affect all populations equally—thereby obscuring how current technological harms are distributed unequally across social groups.

The research demonstrates that treating future AI risks as separate from current inequities prevents governance frameworks from achieving genuine accountability or addressing root causes of technological harm.

## Methodology/Approach

The paper employs feminist policy analysis as its primary analytical framework, systematically examining AI safety governance initiatives through five thematic dimensions. **Intersectionality** interrogates how multiple overlapping social identities create compounded disadvantages. **Context** examines how policies reflect specific historical and social circumstances. **Neutrality** questions claims of objectivity in governance. **Control** analyzes who holds decision-making power. **Power** investigates how policies reinforce or challenge existing hierarchies. This multidimensional approach enables evaluation of how initiatives address structural inequalities and whose voices shape policy development. The framework moves beyond surface-level critique to interrogate underlying assumptions about risk, universality, and governance legitimacy, revealing how AI safety policies often inadvertently reinforce existing power imbalances while claiming neutrality.

## Relevant Concepts

**Intersectionality:** Framework recognizing how multiple social identities (race, gender, class, disability, etc.) interact to create distinct, compounded experiences of marginalization and technological risk.

**AI Safety:** Discipline addressing existential threats from artificial intelligence through development of systems aligned with human values, ethics, explainability, and external control mechanisms.

**Feminist Policy Analysis:** Critical approach examining how policies reflect and reinforce power dynamics, whose interests are centered, how structural inequalities are addressed or perpetuated, and what accountability mechanisms exist.

**Existential Risk:** Threats to human civilization or marginalized populations' futures from advanced AI systems operating without adequate value alignment or human oversight.

**Technocratic Governance:** Top-down policy approaches prioritizing technical expertise and universal frameworks while marginalizing affected communities' participation, knowledge, and differential needs.

**Current Harms:** Observable, present-day negative impacts of existing AI systems on marginalized groups, including algorithmic bias, discriminatory outcomes, and exclusion from decision-making.

## Significance

This work holds substantial significance for academic, policy, and affected communities. It provides concrete frameworks for integrating equity considerations into AI safety governance rather than treating them as separate concerns. The research validates AI safety's core concerns about technological risks while demonstrating how universalist framings obscure differential impacts across social groups. For policymakers, the paper offers actionable recommendations: (1) integrate accountability and participation mechanisms into research and policy development, (2) ensure meaningful participation from marginalized communities, (3) explicitly connect future AI risks to current harms, and (4) acknowledge how current biases shape future AI impacts. By bridging AI safety and critical technology studies, the work establishes intersectionality as essential to legitimate, effective governance. This contribution challenges the field to move beyond technocratic approaches toward more inclusive, equitable AI development processes that acknowledge how technological futures are inseparable from present inequalities. The timing is critical: as AI systems rapidly proliferate, governance frameworks established now will determine whether future development perpetuates or disrupts existing power hierarchies.
