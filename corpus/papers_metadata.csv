ID,Zotero_Key,Title,Authors,Year,DOI,URL,Abstract,Item_Type,Journal,Source_Tool
1,BHXDU7VM,How People Use ChatGPT,"Chatterji, Aaron; Cunningham, Thomas; Deming, David J.; Hitzig, Zoe; Ong, Christopher; Shan, Carl Yan; Wadman, Kevin",2025,,https://www.nber.org/papers/w34255,"Founded in 1920, the NBER is a private, non-profit, non-partisan organization dedicated to conducting economic research and to disseminating research findings among academics, public policy makers, and business professionals.",report,,
2,I78CL6R5,What are artificial intelligence literacy and competency? A comprehensive framework to support them,"Chiu, Thomas K.F.; Ahmad, Zubair; Ismailov, Murod; Sanusi, Ismaila Temitayo",2024,10.1016/j.caeo.2024.100171,https://linkinghub.elsevier.com/retrieve/pii/S2666557324000120,,journalArticle,,
3,VP6SXQHY,The AI literacy development canvas: Assessing and building AI literacy in organizations,"Benlian, Alexander; Pinski, Marc",2025,10.1016/j.bushor.2025.10.001,https://linkinghub.elsevier.com/retrieve/pii/S0007681325001673,,journalArticle,,
4,AXEIVEW3,Artificial Intelligence Competence Needs for Youth Workers,"Lanzetta, Miriam; Abbruzzese, Gianluca; ACOMI, OVIDIU; Acomi, Nicoleta; Machado, Jorge; Maravelaki, Sonia Anastasia",2024,10.5281/ZENODO.11525357,https://zenodo.org/doi/10.5281/zenodo.11525357,"The rapid developments in AI technology and the rise of accessible AI-powered tools are transforming the way we live, work and learn. While young people have already warmly embraced these solutions, with Gen Z being the most active users and experimenters of Generative AI (Microsoft, 2024), there is a sense of confusion and fear among youth workers about the future of how AI tools are going to be used in the youth sector, mixed with diverse emotions and viewpoints ranging from apprehension, scepticism, resistance to feelings of enthusiasm and recognition of the significance of AI's role in the field (Pawluczuk, 2023).

This study aims to advance knowledge on the specific competencies required by youth workers to effectively integrate AI into their professional activities, as well as picture the current and potential use of AI for youth professionals.  

The publication is part of the Artificial Intelligence for Youth Work (AI4YouthWork)project, a pioneering initiative under the Erasmus+ programme, co-funded by the European Union, dedicated to enhancing the youth sector across Europe through the integration of artificial intelligence (AI). The project unites four organisations - Lascò from Italy, TEAM4Excellence from Romania, Kyttaro Enallaktikon Anazitiseon Neon from Greece, and Contextos from Portugal -, aspiring to contribute to increasing youth professionals' capacity to harness AI's potential to enhance the quality, attractiveness and effectiveness of their work, and prepare young people to thrive in AI-powered environments. 





Chapter 1 introduces the project, highlighting the steps and methodological approaches to achieving the main objectives and the expected results.




Chapter 2, dedicated to the research methodology, outlines the approach and techniques used to conduct this study. It includes the research design, data collection methods through systematic review, focus groups and interviews, data analysis procedures, as well as limitations and criteria for ensuring the validity and reliability of the findings.




Chapter 3 presents the results of the desk research conducted by the consortium partners to explore the intersections of artificial intelligence, youth and youth work.  The chapter is divided into four main sections, addressing an introduction to AI, the impact of AI on youth, the role of youth workers in the AI revolution, and practical applications of AI in youth work settings.




Chapter 4 outlines the needs, challenges, and tasks involved in integrating AI into youth work, presenting the results of focus group discussions which have been conducted in each partner country.




Chapter 5 sets out the publication's conclusions, formulating recommendations for the development of an AI Competence Framework for Youth Workers, and enhancing the capacity of youth professionals to harness AI in their work.",journalArticle,,
5,UFE85SCV,AI literacy in K-12: a systematic literature review,"Casal-Otero, Lorena; Catala, Alejandro; Fernández-Morante, Carmen; Taboada, Maria; Cebreiro, Beatriz; Barro, Senén",2023,10.1186/s40594-023-00418-7,https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00418-7,"Abstract
            The successful irruption of AI-based technology in our daily lives has led to a growing educational, social, and political interest in training citizens in AI. Education systems now need to train students at the K-12 level to live in a society where they must interact with AI. Thus, AI literacy is a pedagogical and cognitive challenge at the K-12 level. This study aimed to understand how AI is being integrated into K-12 education worldwide. We conducted a search process following the systematic literature review method using Scopus. 179 documents were reviewed, and two broad groups of AI literacy approaches were identified, namely learning experience and theoretical perspective. The first group covered experiences in learning technical, conceptual and applied skills in a particular domain of interest. The second group revealed that significant efforts are being made to design models that frame AI literacy proposals. There were hardly any experiences that assessed whether students understood AI concepts after the learning experience. Little attention has been paid to the undesirable consequences of an indiscriminate and insufficiently thought-out application of AI. A competency framework is required to guide the didactic proposals designed by educational institutions and define a curriculum reflecting the sequence and academic continuity, which should be modular, personalized and adjusted to the conditions of the schools. Finally, AI literacy can be leveraged to enhance the learning of disciplinary core subjects by integrating AI into the teaching process of those subjects, provided the curriculum is co-designed with teachers.",journalArticle,,
6,J9IZDVTW,"Less knowledge, more trust? Exploring potentially uncritical attitudes towards AI in higher education","Biagini, Gabriele; Cuomo, Stefano; Ranieri, Maria",2024,10.17471/2499-4324/1337,https://doi.org/10.17471/2499-4324/1337,"L'intelligenza artificiale (IA) ha il potenziale per trasformare vari aspetti delle nostre vite, ma il suo sviluppo è stato accompagnato da numerose preoccupazioni sociali ed etiche. Per comprendere le implicazioni e i meccanismi sottostanti, è essenziale acquisire una comprensione ampia dei suoi benefici e svantaggi. A questo scopo, l'alfabetizzazione all'IA è un fattore fondamentale per promuovere atteggiamenti più consapevoli verso lo sviluppo dell'IA e delle sue implicazioni. Tuttavia, la ricerca sulla literacy all'IA è ancora agli esordi. Per contribuire ai progressi del settore, questo articolo presenta i risultati di uno studio volto a valutare l'alfabetizzazione all'IA degli studenti nel contesto dell'istruzione universitaria, concentrandosi su dei dottorandi. L’indagine sulla loro literacy all’IA è stata condotta su quattro dimensioni: cognitiva, operativa, critica ed etica. I risultati mostrano che, sebbene i partecipanti avessero poca conoscenza dell'IA, erano eccessivamente fiduciosi nelle capacità della tecnologia. Lo studio evidenzia la necessità di un approccio più completo all'alfabetizzazione all'IA, che includa una comprensione più profonda delle sue implicazioni etiche, sociali ed economiche.",journalArticle,,
7,P82X89Q4,A Competency Framework for AI Literacy: Variations by Different Learner Groups and an Implied Learning Pathway,"Chee, Hyunkyung; Ahn, Solmoe; Lee, Jihyun",2025,10.1111/bjet.13556,https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13556,"This study aims to develop a comprehensive competency framework for artificial intelligence (AI) literacy, delineating essential competencies and sub‐competencies. This framework and its potential variations, tailored to different learner groups (by educational level and discipline), can serve as a crucial reference for designing and implementing AI curricula. However, the research on AI literacy by target learners is still in its infancy, and the findings of several existing studies provide inconsistent guidelines for educational practices. Following the 2020 PRISMA guidelines, we searched the Web of Science, Scopus, and ScienceDirect databases to identify relevant studies published between January 2012 and October 2024. The quality of the included studies was evaluated using QualSyst. A total of 29 studies were identified, and their research findings were synthesized. Results show that at the K‐12 level, the required competencies include basic AI knowledge, device usage, and AI ethics. For higher education, the focus shifts to understanding data and algorithms, problem‐solving, and career‐related competencies. For general workforce, emphasis is placed on the interpretation and utilization of data and AI tools for specific careers, along with error detection and AI‐based decision‐making. This study connects the progression of specific learning objectives, which should be intensively addressed at each stage, to propose an AI literacy education pathway. We discuss the findings, potentials, and limitations of the derived competency framework for AI literacy, including its theoretical and practical implications and future research suggestions.
            
            
              
              
                
                  
                    Practitioner notes
                  
                  
                    What is already known about this topic
                    
                      
                        AI literacy is becoming increasingly important as AI technologies are integrated into various aspects of life and work.
                      
                      
                        Research on AI literacy competencies across diverse learner groups and disciplines remains fragmented and inconsistent to guide educational practices.
                      
                      
                        Studies providing a coherent pathway for AI literacy development throughout educational and working life are lacking.
                      
                    
                  
                  
                    What this paper adds
                    
                      
                        A comprehensive AI literacy competency framework consisting of 8 competencies and 18 sub‐competencies.
                      
                      
                        Variations in AI literacy competencies with tailored configuration and prioritization across different learner groups by school levels and disciplines.
                      
                      
                        A proposed pathway for developing AI literacy from K‐12 to higher education and workforce levels.
                      
                    
                  
                  
                    Implications for practice and policy
                    
                      
                        The framework can guide the design and implementation of AI curricula tailored to different learner characteristics and needs.
                      
                      
                        Education should shift focus from teaching how to use AI to fostering competencies for critical, strategic, responsible and ethical integration of AI.
                      
                      
                        Policies are needed to support a systematic pathway for lifelong AI literacy development from K‐12 education to workforce training.",journalArticle,,
8,KPSF8EZE,"AI literacy and competency: definitions, frameworks, development and future research directions","Chiu, Thomas K. F.",2025,10.1080/10494820.2025.2514372,https://www.tandfonline.com/doi/full/10.1080/10494820.2025.2514372,,journalArticle,,
9,XCS4YCQH,DigComp 2.1: the digital competence framework for citizens with eight proficiency levels and examples of use.,,2017,,https://data.europa.eu/doi/10.2760/38842,,book,,
10,SY8LNID7,Digital literacy as a new determinant of health: A scoping review,"Arias López, Maria Del Pilar; Ong, Bradley A.; Borrat Frigola, Xavier; Fernández, Ariel L.; Hicklent, Rachel S.; Obeles, Arianne J. T.; Rocimo, Aubrey M.; Celi, Leo A.",2023,10.1371/journal.pdig.0000279,https://dx.plos.org/10.1371/journal.pdig.0000279,"Introduction
              Harnessing new digital technologies can improve access to health care but can also widen the health divide for those with poor digital literacy. This scoping review aims to assess the current situation of low digital health literacy in terms of its definition, reach, impact on health and interventions for its mitigation.
            
            
              Methods
              A comprehensive literature search strategy was composed by a qualified medical librarian. Literature databases [Medline (Ovid), Embase (Ovid), Scopus, and Google Scholar] were queried using appropriate natural language and controlled vocabulary terms along with hand-searching and citation chaining. We focused on recent and highly cited references published in English. Reviews were excluded. This scoping review was conducted following the methodological framework of Arksey and O’Malley.
            
            
              Results
              A total of 268 articles were identified (263 from the initial search and 5 more added from the references of the original papers), 53 of which were finally selected for full text analysis. Digital health literacy is the most frequently used descriptor to refer to the ability to find and use health information with the goal of addressing or solving a health problem using technology. The most utilized tool to assess digital health literacy is the eHealth literacy scale (eHEALS), a self-reported measurement tool that evaluates six core dimensions and is available in various languages. Individuals with higher digital health literacy scores have better self-management and participation in their own medical decisions, mental and psychological state and quality of life. Effective interventions addressing poor digital health literacy included education/training and social support.
            
            
              Conclusions
              Although there is interest in the study and impact of poor digital health literacy, there is still a long way to go to improve measurement tools and find effective interventions to reduce the digital health divide.",journalArticle,,
11,43CRXRRT,A systematic review on digital literacy,"Tinmaz, Hasan; Lee, Yoo-Taek; Fanea-Ivanovici, Mina; Baber, Hasnan",2022,10.1186/s40561-022-00204-y,https://slejournal.springeropen.com/articles/10.1186/s40561-022-00204-y,"Abstract
            The purpose of this study is to discover the main themes and categories of the research studies regarding digital literacy. To serve this purpose, the databases of WoS/Clarivate Analytics, Proquest Central, Emerald Management Journals, Jstor Business College Collections and Scopus/Elsevier were searched with four keyword-combinations and final forty-three articles were included in the dataset. The researchers applied a systematic literature review method to the dataset. The preliminary findings demonstrated that there is a growing prevalence of digital literacy articles starting from the year 2013. The dominant research methodology of the reviewed articles is qualitative. The four major themes revealed from the qualitative content analysis are: digital literacy, digital competencies, digital skills and digital thinking. Under each theme, the categories and their frequencies are analysed. Recommendations for further research and for real life implementations are generated.",journalArticle,,
12,HTXCWNQ9,A Literature Review of Digital Literacy over Two Decades,"Peng, Danhua; Yu, Zhonggen",2022,10.1155/2022/2533413,https://www.hindawi.com/journals/edri/2022/2533413/,"The COVID-19 pandemic has forced online learning to be a “new normal” during the past three years, which highly emphasizes students’ improved digital literacy. This study aims to present a literature review of students’ digital literacy. Grounded on about twenty journal articles and other related publications from the Web of Science Core Collection, this paper focused on the definition of digital literacy; the factors affecting students’ digital literacy (age, gender, family socioeconomic status, and parent’s education level); the relationship between students’ digital literacy and their self-control, technostress, and engagement; and the three approaches to gauge the level of students’ digital literacy. The study also provided some advice for educators and policymakers. Finally, the limitations and implications were presented.",journalArticle,,
13,7G78SADI,"AI literacy for users – A comprehensive review and future research directions of learning methods, components, and effects","Pinski, Marc; Benlian, Alexander",2024,10.1016/j.chbah.2024.100062,https://linkinghub.elsevier.com/retrieve/pii/S2949882124000227,,journalArticle,,
14,3AQ2K3BX,Conceptualizing AI literacy: An exploratory review,"Ng, Davy Tsz Kit; Leung, Jac Ka Lok; Chu, Samuel Kai Wah; Qiao, Maggie Shen",2021,10.1016/j.caeai.2021.100041,https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000357,,journalArticle,,
15,7VFNS5R3,Evaluation of an artificial intelligence literacy course for university students with diverse study backgrounds,"Kong, Siu-Cheung; Man-Yin Cheung, William; Zhang, Guo",2021,10.1016/j.caeai.2021.100026,https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000205,,journalArticle,,
16,23Y3627L,Measuring user competence in using artificial intelligence: validity and reliability of artificial intelligence literacy scale,"Wang, Bingcheng; Rau, Pei-Luen Patrick; Yuan, Tianyi",2023,10.1080/0144929X.2022.2072768,https://www.tandfonline.com/doi/full/10.1080/0144929X.2022.2072768,,journalArticle,,
17,4BRSDIPP,Development of the “Scale for the assessment of non-experts’ AI literacy” – An exploratory factor analysis,"Laupichler, Matthias Carl; Aster, Alexandra; Haverkamp, Nicolas; Raupach, Tobias",2023,10.1016/j.chbr.2023.100338,https://linkinghub.elsevier.com/retrieve/pii/S2451958823000714,,journalArticle,,
18,YDW5TX8M,"Imagination, Algorithms and News: Developing AI Literacy for Journalism","Deuze, Mark; Beckett, Charlie",2022,10.1080/21670811.2022.2119152,https://www.tandfonline.com/doi/full/10.1080/21670811.2022.2119152,,journalArticle,,
19,BBLJ4RG3,Developing an artificial intelligence literacy framework: Evaluation of a literacy course for senior secondary students using a project-based learning approach,"Kong, Siu-Cheung; Cheung, Man-Yin William; Tsang, Olson",2024,10.1016/j.caeai.2024.100214,https://linkinghub.elsevier.com/retrieve/pii/S2666920X24000158,,journalArticle,,
20,FJQ6XYHH,Artificial Intelligence (AI) literacy – an argument for AI literacy in education,"Kong, Siu-Cheung; Korte, Satu-Maarit; Burton, Steve; Keskitalo, Pigga; Turunen, Tuija; Smith, David; Wang, Lixun; Lee, John Chi-Kin; Beaton, Mhairi C.",2025,10.1080/14703297.2024.2332744,https://www.tandfonline.com/doi/full/10.1080/14703297.2024.2332744,,journalArticle,,
21,JMAWNUEV,GLAT: The generative AI literacy assessment test,"Jin, Yueqiao; Martinez-Maldonado, Roberto; Gašević, Dragan; Yan, Lixiang",2025,10.1016/j.caeai.2025.100436,https://linkinghub.elsevier.com/retrieve/pii/S2666920X25000761,,journalArticle,,
22,7U29SIC8,In search of artificial intelligence (AI) literacy in teacher education: A scoping review,"Sperling, Katarina; Stenberg, Carl-Johan; McGrath, Cormac; Åkerfeldt, Anna; Heintz, Fredrik; Stenliden, Linnéa",2024,10.1016/j.caeo.2024.100169,https://linkinghub.elsevier.com/retrieve/pii/S2666557324000107,,journalArticle,,
23,3K9RDWLY,AI Literacy - Towards Measuring Human Competency in Artificial Intelligence,"Pinski, Marc; Benlian, Alexander",2023,10.24251/HICSS.2023.021,http://hdl.handle.net/10125/102649,,conferencePaper,,
24,2SNYUZG4,Promises and challenges of generative artificial intelligence for human learning,"Yan, Lixiang; Greiff, Samuel; Teuber, Ziwen; Gašević, Dragan",2024,10.1038/s41562-024-02004-5,https://www.nature.com/articles/s41562-024-02004-5,,journalArticle,,
25,SE579V7B,Conceptualizing AI literacy: An exploratory review,"Ng, Davy Tsz Kit; Leung, Jac Ka Lok; Chu, Samuel Kai Wah; Qiao, Maggie Shen",2021,10.1016/j.caeai.2021.100041,https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000357,,journalArticle,,
26,KI9GRGHB,Theorising algorithmic justice,"Marjanovic, O.; Cecez-Kecmanovic, D.; Vidgen, R.",2022,10.1080/0960085X.2021.1934130,,"Theoretical article developing framework for understanding algorithmic justice in automated decision-making systems with specific application to social welfare services. Authors examine how AI-driven automated algorithmic decision-making threatens core social justice principles in transformative services like welfare. Framework addresses WHAT matters in algorithmic justice (fairness, equity, human rights), WHO counts as subjects (vulnerable populations disproportionately affected), and HOW algorithmic justice is performed (through transparent, accountable, participatory processes). Key ethical tensions identified include datafication reducing humans to data points violating dignity; technological inscribing embedding biases perpetuating discrimination; systemic nature of injustices compounding disadvantage for marginalized groups; and loss of human judgment and participation.",journalArticle,,
27,AMYZFAPH,Recommendations for social work researchers and journal editors on the use of generative AI and large language models,"Perron, B. E.; Victor, B. G.; Hodge, D. R.; Vaughn, M. G.; Salas-Wright, C. P.",2023,10.1086/726021,,"Evidence-based recommendations for social work researchers using generative AI and LLMs, addressing prompt engineering and critical engagement with AI outputs. Examines how LLMs can improve research efficiency through facilitating literature reviews, data analysis, and writing assistance while emphasizing need for critical evaluation of AI-generated content. Discusses concerns about over-reliance on AI potentially diminishing research quality when researchers don't engage in critical thinking or rigorous data evaluation. Stresses that AI tools cannot replace human expertise and judgment.",journalArticle,,
28,VSZM7CT6,"Problematising artificial intelligence in social work education: Challenges, issues and possibilities","Hodgson, D.; Goldingay, S.; Boddy, J.; Nipperess, S.; Watts, L.",2022,10.1093/bjsw/bcab168,,"Critical examination of AI's fourth industrial revolution implications for social work education, questioning what skills and knowledge should be taught to prepare students for digital working lives. Adopts problematizing approach challenging both celebratory and catastrophic narratives about AI. Argues social work education must address fundamental tensions between training for technologically-driven environments and engaging students in critical theories acknowledging unequal power distributions that technological change reinforces. Advocates for developing critical digital literacies beyond technical competence.",journalArticle,,
29,EXRF5629,Predicting successful placements for youth in child welfare with machine learning,"Spaulding, A. D.; Valentine, E. J.; Skinner, S. R.; Gosnell, R. A.; Tran, K.; Hatfield, J.; Johnson, J. K.",2023,10.1016/j.childyouth.2023.107229,,"Study developed machine learning models to predict treatment success for youth in various child welfare placement settings using data from 4,788 youth served by Children's Hope Alliance in North Carolina. Models aimed to distinguish which youth would succeed in high-cost residential psychiatric treatment versus community-based alternatives. Using Treatment Outcome Package (TOP) assessment data and gradient boosting algorithms, models achieved AUROCs >0.70 in predicting placement success. Addresses Family First Prevention Services Act requirement for right level of care determinations. Multiple raters per case improved model accuracy. Demonstrates feasibility of using existing clinical data to inform high-stakes placement decisions.",journalArticle,,
30,NSI6S5QE,"NASW, ASWB, CSWE, & CSWA standards for technology in social work practice",,2017,,https://www.socialworkers.org/Practice/NASW-Practice-Standards-Guidelines/Standards-for-Technology-in-Social-Work-Practice,"Landmark 64-page collaborative document representing unprecedented coordination among four major U.S. social work organizations to establish comprehensive technology standards for the profession. Standards address four main areas: providing information to public, designing and delivering services, gathering/managing/storing/accessing client information, and educating and supervising social workers. Covers practitioner competence, informed consent, privacy and confidentiality, boundaries and dual relationships, records and documentation, collegial relationships, and jurisdictional boundaries. Provides foundational ethical guidance directly applicable to AI technologies.",report,,
31,7FEFMCBZ,How child welfare workers reduce racial disparities in algorithmic decisions,"Cheng, H.-F.; Stapleton, L.; Kawakami, A.; Sivaraman, V.; Cheng, Y.; Qing, D.; Perer, A.; Holstein, K.; Wu, Z. S.; Zhu, H.",2022,10.1145/3491102.3501831,,"Mixed-methods study analyzing four years of child welfare call screening data alongside worker interviews to investigate how human-algorithm collaboration affects racial bias in decision-making. Demonstrates Allegheny Family Screening Tool algorithm alone would have created 20% disparity in screen-in rates between Black and white children, but workers reduced this to 9% through holistic risk assessments and adjustments for algorithmic limitations. Reveals critical discrimination risks: algorithm disproportionately flags Black families for investigation due to biased training data reflecting historical over-policing and surveillance of communities of color.",conferencePaper,,
32,LR8Z3YHP,'Meaningless work': How the datafication of health reconfigures knowledge about work and erodes professional judgement,"Wadmann, S.; Hoeyer, K.",2020,10.1177/0950017020950021,,"Critical ethnographic study examining how datafication and digitalization in Denmark's healthcare sector erode professional judgment and create meaningless work through surveillance and control mechanisms. Authors challenge policy narratives that more data leads to better, evidence-based healthcare decisions, revealing instead how data-intensive practices create Kafkaesque idiocy reconfiguring perceptions of work and undermining goal orientation. Key critical insights demonstrate how dual ambitions of datafication introduce detailed surveillance tools regulating what counts as knowledge, forcing professionals into documentation activities divorced from care relationships. Reveals how data work serves administrative inspection rather than professional expertise, threatening autonomy essential to professional practice.",journalArticle,,
33,P4YQIKJX,Ethics & AI: A systematic review on ethical concerns and related strategies for designing with AI in healthcare,"Li, F.; Ruijs, N.; Lu, Y.",2023,10.3390/ai4010003,,"Systematic literature review (2010-2020) identifying 12 main ethical issues in AI healthcare applications with direct relevance to social services. Critical value tensions identified include justice and fairness (algorithmic bias causing discrimination), freedom and autonomy (control, respecting human autonomy, and informed consent challenges), privacy violations through data-driven systems, transparency conflicts with black-box algorithms, dignity concerns when AI reduces persons to data, and conflicts in decision-making between AI and human judgment. Review analyzed 45 documents and identified 19 ethical sub-issues, providing strategies for each. Emphasizes human-centered approach respecting fundamental rights and European values.",journalArticle,,
34,R7V99ERA,Examining risks of racial biases in NLP tools for child protective services,"Field, A.; Coston, A.; Gandhi, N.; Chouldechova, A.; Putnam-Hornstein, E.; Steier, D.; Tsvetkov, Y.",2023,10.1145/3593013.3594094,,"Empirical study examining racial bias in natural language processing tools used to analyze child protective services case notes and make risk assessments. Demonstrates language models trained on case narratives exhibit systematic biases disadvantaging families of color. Testing multiple NLP architectures on real child welfare text data, finds models consistently predict higher risk scores for cases mentioning dialects or cultural contexts associated with Black and Latinx families, even when case severity is identical. Reveals how linguistic biases in administrative data get encoded into AI systems, creating automated discrimination.",conferencePaper,,
35,4ZL5Q48E,Algorithmic management in a work context,"Jarrahi, M. H.; Newlands, G.; Lee, M. K.; Wolf, C. T.; Kinder, E.; Sutherland, W.",2021,10.1177/20539517211020332,,"Interdisciplinary analysis examining algorithmic management as sociotechnical phenomenon shaped by organizational choices and power structures rather than technological determinism. Authors critically analyze how algorithmic systems in standard work settings redefine pre-existing power dynamics between workers and managers, demanding new competencies while fostering oppositional attitudes. Key critical insights include risk of treating workers as programmable cogs through automation, commodification and alienation in digitally-mediated work, and elimination of middle management functions. Challenges simplified narratives of algorithmic replacement, demonstrating how algorithms emerge through continuous interaction between organizational members and systems. Critical concerns include deskilling, surveillance, loss of discretion, and embedding of managerial control in opaque technical systems.",journalArticle,,
36,ZQHP5G35,Data and rights in the digital welfare state: the case of Denmark,"Jørgensen, R. F.",2023,10.1080/1369118X.2021.1934069,,"Critical analysis examining how surveillance capitalism logic manifests in public sector through Denmark's automated welfare decision-making systems. Argues that unless more human-centric approaches are adopted, digital welfare states advance digital technocracy treating citizens as data points for calculation and prediction rather than individuals with agency and rights. Employs theories of surveillance capitalism, digital-era governance, and data politics to analyze automated decision support deployment by state actors. Key concerns include data maximization, profiling, surveillance, and citizen disempowerment through predictive analytics used to identify fraud and vulnerability. Demonstrates how Danish municipalities' enthusiasm for AI-driven welfare administration mirrors tech giants' commercial data exploitation.",journalArticle,,
37,A2P8MXMY,"Algorithmic justice in child protection: Statistical fairness, social justice and the implications for practice","Keddell, E.",2019,10.3390/socsci8100281,,"Critical analysis examining fairness and justice implications of predictive algorithms in child protection from both statistical and social justice perspectives. Identifies fundamental problems with using child protection system data: biased sample frames reflecting reporting patterns rather than actual abuse incidence, feedback loops amplifying discrimination, and spurious correlations conflating system surveillance with genuine risk. Demonstrates racial and socioeconomic disparities in child welfare data lead to algorithmic tools disproportionately identifying Indigenous, Black, and poor families as high risk even when true abuse rates are similar across groups.",journalArticle,,
38,ZITLBM8A,ChatGPT for social work science: Ethical challenges and opportunities,"Singer, J. B.; Victor, B. G.; Perron, B. E.",2023,10.1086/726042,,"Invited paper exploring opportunities and ethical challenges of deploying ChatGPT and large language models specifically for social work science. Describes potential uses of ChatGPT in social work research while examining critical ethical concerns related to algorithmic bias, data quality, and risk of replacing human expertise with AI-generated content. Offers preliminary recommendations for ethical ChatGPT use in social work research, emphasizing need for researchers to critically evaluate AI outputs rather than accepting them uncritically.",journalArticle,,
39,QI5AYE4V,Algorithmic decision-making in social work practice and pedagogy: Confronting the competency/critique dilemma,"McDonald, C.; Marston, G.; Buckley, A.",2023,10.1080/02615479.2023.2195425,,Explores tensions between training social workers for technologically-driven environments and engaging students in critical theories acknowledging unequal power distributions that technological change reinforces. Introduces algorithmic literacy centered on understanding critical limitations of algorithmic decision-making systems. Argues that adding social work subjects on ADM alone proves insufficient; students need opportunities to develop algorithmic literacy enabling them to ask appropriate questions and understand what is at stake with ADM normalization.,journalArticle,,
40,WTLVG29I,Artificial intelligence in social work: Emerging ethical issues,"Reamer, F. G.",2023,10.55521/10-020-205,,"Seminal examination of critical ethical challenges related to social workers' AI use, addressing informed consent, privacy, transparency, algorithmic bias, client misdiagnosis, surveillance, and evidence-based tools. Provides detailed protocols for ethical AI implementation including establishing ethics-based governing principles, creating digital ethics steering committees, conducting diverse focus groups, and subjecting algorithms to peer review. Emphasizes developing competencies for understanding AI applications across risk assessment, crisis intervention, prevention, education, and service delivery.",journalArticle,,
41,J7V3AAQT,A chatbot for mental health support: Exploring the impact of Emohaa on reducing mental distress in China,"Sabour, S.; Zhang, W.; Xiao, X.; Zhang, Y.; Zheng, Y.; Wen, J.; Zhao, J.; Huang, M.",2023,10.3389/fdgth.2023.1133987,,"Three-arm randomized controlled trial examining Emohaa mental health chatbot system in China with 141 university students experiencing mental distress. Participants assigned to: CBT-Bot only (n=69), Full Emohaa with CBT-Bot and emotional support bot (n=31), or control group (n=41). 8-week intervention used AI-powered conversational agents providing cognitive-behavioral therapy techniques and emotional support. Results showed significant improvements in depression (p<.001, effect size η²p=.05) and anxiety for intervention groups compared to control, with effects maintained at 3-week follow-up for Full Emohaa group. Satisfaction surveys showed 71% would recommend platform.",journalArticle,,
42,9WIGR47Y,Ethical issues related to the use of technology in social work practice: A systematic review,"Rodríguez-Martínez, A.; Amezcua Aguilar, M. T.; Cortés Moreno, J.; Jiménez-Delgado, J. J.",2024,10.1177/21582440241274842,,"Systematic literature review examining ethical tensions arising from technology integration in social work practice. Review identifies three main categories of ethical challenges: effects of digitization on professional practice (tensions between efficiency and human connection), education, research and engagement challenges (digital literacy requirements conflicting with traditional social work training), and ethical challenges in digital professional practice (boundary issues, dual relationships, informed consent in digital contexts). Key value conflicts include technology's potential to erode professional relationships and trust; digital divide creating social justice concerns; automated systems undermining client self-determination and participation; and privacy violations threatening human dignity.",journalArticle,,
43,S8WGUVQT,Automated government benefits and welfare surveillance,"Dencik, L.; Hintz, A.; Redden, J.; Warne, H.",2024,,https://ojs.library.queensu.ca/index.php/surveillance-and-society/article/view/16107,"Critical surveillance studies analysis examining digital welfare state historically, presently, and prospectively, focusing on AI-driven welfare surveillance systems. Authors argue problems posed by AI in public administration are often misattributed to technological novelty when they actually represent historically familiar patterns of surveillance and control. Drawing on bureaucracy, welfare state, and automation scholarship, demonstrates how algorithmic fraud detection and chatbot assistance systems extend long-standing practices of scrutinizing and disciplining marginalized populations. Critical insights include analysis of how automation enables unprecedented scale of surveillance while maintaining opacity through algorithmic systems' auditable veneer. Examines cases from Netherlands and other jurisdictions showing how automated welfare systems amplify existing power asymmetries and inequality.",journalArticle,,
44,LXDG4KQK,Coded injustice: Surveillance and discrimination in Denmark's automated welfare state,,2024,,https://www.amnesty.org/en/documents/eur18/8709/2024/en/,"Critical human rights investigation documenting how Denmark's AI-powered welfare fraud detection systems risk discriminating against people with disabilities, low-income individuals, migrants, refugees, and marginalized racial groups. Report examines up to 60 algorithmic models used to flag individuals for fraud investigations, revealing mass surveillance practices violating privacy rights and creating atmospheres of fear among welfare recipients. Key findings demonstrate how automated systems paired with extensive data collection from multiple government agencies create what approaches prohibited social scoring. Investigation reveals harmful psychological tolls on surveilled populations and argues automation exacerbates pre-existing structural inequalities rather than creating fair or efficient systems.",report,,
45,BTLTEA6Y,Decision support and algorithmic support: The construction of algorithms and professional discretion in social work,"Meilvang, M. L.; Dahler, A. M.",2024,,,"Critical analysis examining three decision-support algorithms developed for Danish municipalities in child and family social work, analyzing how they affect professional discretion despite claims to merely support professionals. Demonstrates how algorithmic systems designed to minimize subjective judgment and promote efficiency actually embody positivist assumptions that professional discretion can and should be eliminated. Key findings reveal how political actors favor standardized, automated approaches to avoid high-profile cases, effectively negating professional judgment central to ethical social work practice. Drawing on street-level bureaucracy and digitalization literature, argues that framing algorithms as neutral decision support obscures their role in fundamentally restructuring professional autonomy, expertise, and nature of care relationships.",bookSection,,
46,7D3ICY7Z,"Introduction to the digital welfare state: Contestations, considerations and entanglements","van Toorn, G.; Henman, P.; Soldatić, K.",2024,10.1177/14407833241260890,,"Special issue introduction providing critical sociological analysis of digital welfare state, examining how datafication and automation amplify existing trends of surveillance and control over marginalized populations. Authors argue that contrary to neutral efficiency narratives, digital welfare technologies are embedded in fiscal austerity politics and criminalization of poverty. Employs power relations and human agency frameworks to demonstrate how algorithmic systems increase scrutiny of welfare recipients, migrants, and undeserving populations while prioritizing cost-cutting over meeting social needs. Critiques framing of AI as unprecedented innovation, situating digital welfare within historical dynamics of social control. Key themes include erosion of professional discretion, surveillance assemblages, and automated decision-making reproducing structural inequalities.",journalArticle,,
47,XQM6WRU2,Exploring machine learning to support decision-making for placement stabilization and preservation in child welfare,"Cher, K. H. B.; Adelson, S. L.; McCrae, J. S.; Barth, R. P.",2024,10.1007/s10826-024-02993-x,,"Statewide study analyzed 12,621 child welfare cases in large Midwestern state (2017-2020) to develop machine learning models predicting placement disruption risk. Goal was to identify youth who could benefit from placement stabilization services to prevent unnecessary residential care under Family First Prevention Services Act. Random forest models were compared with conventional logistic regression for predicting placement disruption and referral to stabilization programs. ML models demonstrated moderate predictive validity with practical implications for proactive service allocation. Results showed ML could support but not replace caseworker judgment in placement decisions. Study used administrative child welfare data and evaluated model fairness considerations.",journalArticle,,
48,E4G328PD,Clinical trial of an LLM-based conversational AI psychotherapy,"Heinz, M. V.; Fernandez-Mendoza, J.; Semien, G.; Brown, R.; Jacobson, N. C.",2025,,https://ai.nejm.org/doi/full/10.1056/AIoa2400802,"Groundbreaking study representing first randomized controlled trial of generative AI-powered therapy chatbot, Therabot. Trial included 106 participants across United States diagnosed with major depressive disorder, generalized anxiety disorder, or eating disorders. Participants interacted with Therabot via smartphone app over 4-8 weeks. Results showed clinically significant symptom improvements: 51% reduction in depression symptoms, 31% reduction in anxiety symptoms, and 19% reduction in eating disorder concerns, with outcomes comparable to traditional outpatient therapy. Participants reported therapeutic alliance levels comparable to human therapists and engaged average of 6 hours (equivalent to 8 therapy sessions).",journalArticle,,
49,GFXYER4F,AI implementation science for social issues: Pitfalls and tips,"Takaoka, K.",2022,10.2188/jea.JE20210380,,"Case study documenting four-stage social implementation of AI system (AiCAN - Assistant of Intelligence for Child Abuse and Neglect) in Japanese Child Guidance Centers from 2012-2020. System uses machine learning to predict child abuse recurrence and Bayesian networks for real-time probabilistic inference to guide temporary protection decisions. Data from over 6,000 cases (2014-2018) were used to develop gradient boosting algorithms with AUROC >0.70. Implementation involved iterative stakeholder engagement, workflow redesign, training field staff, and addressing organizational resistance. Emphasizes critical importance of building consensus with practitioners, designing for field usability, ensuring data quality through validated scales, and employing eXplainable AI for transparency.",journalArticle,,
50,3B87U5LN,"""It happened to be the perfect thing"": Experiences of generative AI chatbots for mental health","Siddals, S.; Torous, J.; Coxon, A.",2024,10.1038/s44184-024-00097-4,,"Qualitative study using semi-structured interviews with 19 individuals from 8 countries who used generative AI chatbots (primarily Pi, ChatGPT) for mental health support in real-world settings. Participants reported high engagement and positive impacts including improved relationships, healing from trauma and loss, and improved mood. Four themes emerged: emotional sanctuary (non-judgmental, always-available support), insightful guidance particularly for relationships, joy of connection, and comparisons with human therapy. Some participants described life-changing impacts. Identified challenges including frustrating safety guardrails disrupting emotional sanctuary, limited memory capabilities, and inability to lead therapeutic process.",journalArticle,,
51,C325Y32P,AI for impact: The PRISM framework for responsible AI in social innovation,,2024,,https://www.weforum.org/publications/ai-for-impact-the-prism-framework-for-responsible-ai-in-social-innovation/,"Institutional report introducing PRISM framework specifically designed for social innovators, impact enterprises, and intermediaries working in social services sectors. Building on Presidio Framework of AI Governance Alliance, PRISM provides adoption pathways through which organizations can filter their impact mission, capabilities, and risks against AI technology use. Framework includes AI-enabled readiness assessment matrix enabling organizations to evaluate current practices and develop actionable roadmaps for AI integration both internally and externally. Emphasizes ethical adoption of AI aligned with social impact missions.",report,,
52,7QQV7R9S,Generative AI & social work practice guidance,,2025,,https://basw.co.uk/policy-and-practice/resources/generative-ai-social-work-practice-guidance,"BASW's initial practice guidance specifically addressing generative AI use in social work, offering reflection points for practitioners relating to ethical considerations under BASW Code of Ethics. Warns that AI tools are prone to replicating racist/sexist assumptions from training datasets, generating misleading information (hallucinations), and risking data privacy breaches. Emphasizes generative AI should create capacity for relationship-based practice rather than justify increased caseloads or redundancies. Key recommendations include avoiding entry of sensitive personal information into generic tools without explicit consent, conducting data protection assessments, maintaining human oversight and critical assessment of AI outputs.",report,,
53,PI5H2LZ2,"Failing our youngest: On the biases, pitfalls, and risks in a decision support algorithm used for child protection","Moreau, T.; Sinatra, R.; Sekara, V.",2024,10.1145/3630106.3658906,,"Critical empirical study examining child protection decision support algorithm deployed in Danish municipalities, analyzing its biases and implementation challenges. Using real administrative data from Denmark's child welfare system, evaluated algorithm's predictions against actual case outcomes and found significant biases including disproportionate impacts on immigrant families and systematic errors in risk assessment. Results revealed concerning patterns of false positives for marginalized communities and questioned algorithm's validity for high-stakes decision-making. Documents actual harms from deployed systems.",conferencePaper,,
54,JC7X3MM7,"Improving human-AI partnerships in child welfare: Understanding worker practices, challenges, and desires for algorithmic decision support","Kawakami, A.; Sivaraman, V.; Cheng, H.-F.; Stapleton, L.; Cheng, Y.; Qing, D.; Perer, A.; Wu, Z. S.; Zhu, H.; Holstein, K.",2022,10.1145/3491102.3517439,,"Empirical study examining how child maltreatment hotline workers interact with Allegheny Family Screening Tool, an AI-based decision support system. Through interviews and contextual inquiries, found workers' reliance on algorithmic predictions guided by four key factors: knowledge of contextual information beyond AI model capabilities, beliefs about system limitations, organizational pressures around tool use, and awareness of misalignments between algorithmic predictions and their own decision-making objectives. Reveals discrimination risks stemming from workers lacking adequate training on tool's data sources and limitations.",conferencePaper,,
55,TNLGELEQ,ReflectAI: Design and evaluation of an AI coach to support public servants' self-reflection,"Näscher, H.-H.; Schaffner, J.; Koddebusch, M.",2025,10.1007/978-3-032-02515-9_7,,"Design science research presenting ReflectAI, an LLM-based AI coach designed to support public servants in developing self-reflection competencies—critical skill for digital transformation in public administration. Two-week user study with seven public servants revealed three key benefits: increased awareness of self-reflection opportunities, improved thought structure, and valuable conversation documentation. Demonstrates how conversational AI can facilitate reflective practice through structured prompting and dialogue. Shows AI coaching for personality-related competencies, demonstrating how prompt-based interactions can support professional development in human services contexts closely aligned with social services.",conferencePaper,,
56,8MDXCTA6,"A systematic review of sophisticated predictive and prescriptive analytics in child welfare: Accuracy, equity, and bias","Hall, S. F.; Sage, M.; Scott, C. F.; Magruder, K.; Powers, J.",2024,10.1007/s10560-023-00931-2,,"Systematic review examining how researchers address ethics, equity, bias, and model performance in predictive and prescriptive algorithms used in child welfare settings. Analyzing 67 articles published 2010-2020, reveals inconsistent approaches to measuring and mitigating algorithmic fairness in child welfare applications. Identifies that most predictive models use administrative data reflecting surveillance biases rather than true child maltreatment incidence, leading to discrimination against low-income families and communities of color. Highlights many tools fail to address how automation bias and dehumanization affect social workers' professional judgment.",journalArticle,,
57,4GYXUS9Y,The end of the world as we know it? ChatGPT and social work,"Goldkind, L.; Wolf, L.; Glennon, A.; Rios, J.; Nissen, L.",2024,10.1093/sw/swad044,,"Editorial in flagship journal Social Work providing critical reflection on ChatGPT's introduction and implications for social work practice. Addresses how ChatGPT, built on natural language processing, responds to prompts and generates text responses. Notes social work's historical reluctance to embrace new technologies and positions ChatGPT as opportunity to reflect on strategies promoting just technology use. Urges social workers to join cross-disciplinary conversations about AI evolution and advocate for fair use. Calls for profession to move beyond fear and awe toward critical, reflective engagement with AI systems through thoughtful prompting practices.",journalArticle,,
58,CLKAD87H,Responsible prompting recommendation: Fostering responsible AI practices in prompting-time,"James, P.; Meeussen, M.; Matthes, I.; Siemon, D.",2025,10.1145/3706598.3713365,,"Presents insights from interviews and user studies with IT professionals exploring prompting practices and develops open-source responsible prompting recommender system. Research reveals responsible prompt recommendations can support novice prompt engineers and raise awareness about Responsible AI in prompting-time, helping people reflect on responsible practices before LLM content generation. Demonstrates that finding right balance between adding social values to prompts and removing potentially harmful content is critical for recommendation systems. Framework highly relevant for social services as it addresses how to design systems encouraging reflective, values-based prompting practices.",conferencePaper,,
59,CY5IMM6G,Artificial Intelligence (AI) literacy for social work: Implications for core competencies,"Ahn, E.; Choi, M.; Fowler, P.; Song, I. H.",2025,10.1086/735187,,"Comprehensive framework integrating AI literacy into CSWE's nine core competencies, arguing AI understanding is essential for recognizing algorithmic bias perpetuation of social inequalities, contributing to ethical governance, and thoughtfully integrating technology into practice. Emphasizes AI literacy enables social workers to recognize algorithmic bias, advocate for individuals navigating AI-driven systems, and address emerging vulnerabilities including digital divides and discriminatory practices across housing, hiring, and service delivery.",journalArticle,,
60,UENFDPH9,Considering a unified model of artificial intelligence enhanced social work: A systematic review,"Garkisch, M.; Langer, P. J.; Becher, T.",2024,10.1007/s41134-024-00326-y,,"Systematic review mapping research landscape of social work AI scholarship, analyzing 67 articles using qualitative analytic approaches to explore how social work researchers investigate AI. Identified themes consistent with Staub-Bernasconi's triple mandate covering profession level, social agencies/organizations, and clients. Emphasizes importance of enhancing computational thinking, AI literacy, and data literacy, and developing skills for evaluating automated systems. Stresses that professionals must be educated and sensitized to data and AI literacy with regular training opportunities.",journalArticle,,
61,AK7K4P97,Artificial intelligence in social work: An EPIC model for practice,"Baker, S.; Garkisch, M.; Robards, F.",2025,10.1080/0312407X.2025.2488345,,"Presents EPIC model for integrating AI into social work consisting of four components: Ethics and justice, Policy development and advocacy, Intersectoral collaboration, and Community engagement and empowerment. Following comprehensive literature review, examines AI's influence on social work including opportunities to advance socially just outcomes and challenges risking ethical practice. Emphasizes community-based initiatives promoting AI digital literacy and partnerships with local organizations to improve technology access for vulnerable populations.",journalArticle,,
62,6Y5EPQRR,"Positionings, challenges, and ambivalences in children's and parents' perspectives in digitalized familial contexts","Kutscher, N.",2023,,,"Examines family dynamics in digitalized contexts, analyzing tensions between children's digital participation rights and parental protection responsibilities. Presents research revealing ambivalences in both parental and children's perspectives: parents struggle between enabling children's digital competence and protecting them from risks; children experience tension between desire for autonomy and need for guidance. Addresses sharenting (parents sharing children's images/information online), examining conflicts between parental expression rights and children's privacy rights.",bookSection,,
63,QKFG72IT,Bildungsteilhabe - Flucht - Digitalisierung: Eine multilokale Ethnografie im (digitalen) Alltag junger Geflüchteter,"Fujii, M. S.; Kutscher, N.; Friedrichs-Liesenkötter, H.; Hüttmann, J.",2024,,https://www.beltz.de/fachmedien/sozialpaedagogik_soziale_arbeit/produkte/details/53146,"Presents ethnographic research on how digital media shapes educational participation for young refugees, examining ambivalent role of digital technologies: enabling connection to educational resources and transnational networks while simultaneously creating new forms of exclusion through surveillance, documentation requirements, and digital skill barriers. Reveals digital access alone does not guarantee educational participation—digital literacy, linguistic competence, and social support remain crucial. Documents how digitalization intersects with migration status, creating specific vulnerabilities related to data collection, surveillance, and documentation requirements.",book,,
64,BKSU66QB,Digitalität und Digitalisierung als Gegenstand der Sozialen Arbeit,"Kutscher, N.",2024,,,"Differentiates between digitalization (technical processes of making things digital) and digitality (sociotechnical transformations of social practices and relations), arguing social work must engage with both technological changes and their social implications. Demonstrates how algorithms, data-driven systems, and digital platforms reshape professional practice, client relationships, and social inequalities. Argues digitalization fundamentally transforms social contexts where social work operates rather than merely adopting new tools, requiring critical engagement examining power relations, surveillance mechanisms, and social justice implications.",bookSection,,
65,M2FYV58I,Exploring opportunities and risks in decision support technologies for social workers: An empirical study in the field of disabled people's services,"Schneider, D.; Maier, A.; Cimiano, P.; Seelmeyer, U.",2022,10.1093/bjsw/bcab262,,"Empirical study investigating German social workers' perspectives on decision support systems in disability services through practitioner interviews. Identifies both opportunities (consistency across cases, evidence-based practice, administrative time-saving) and significant risks (deprofessionalization, data protection concerns, reduced professional autonomy, loss of holistic assessment capabilities). Social workers express ambivalence: recognizing potential for reducing subjective bias and improving resource allocation transparency while worrying about losing relational aspects of assessment and client trust.",journalArticle,,
66,LMW8DZ78,"Indecision on the use of artificial intelligence in healthcare: A qualitative study of patient perspectives on trust, responsibility and self-determination using AI-CDSS","Schneider, D.; Liedtke, W.; Klausen, A. D.; Lipprandt, M.; Funer, F.; Langanke, M.; Heyen, N. B.; Aichinger, H.; Bratan, T.",2025,10.1186/s12910-024-01143-5,,"Empirical qualitative study exploring patient perspectives on AI-based clinical decision support systems (AI-CDSS) in healthcare, revealing significant ambivalence about AI use in medical decision-making. Through interviews examining trust, responsibility distribution, and self-determination when AI systems assist physicians, findings show patients worry about decreased human interaction, loss of holistic care perspectives, and unclear accountability when AI makes errors. Vulnerable populations express particular concerns about algorithmic systems making decisions affecting their wellbeing.",journalArticle,,
67,2WHGF83D,"AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?","Schneider, D.; Weber, K.",2024,10.14512/tatup.33.1.08,,"Comprehensive examination of AI-based decision support systems across healthcare, legal systems, and border control. Provides critical analysis of technical, ethical, legal, and societal challenges when machines make or support decisions previously made by humans. Reviews regulatory attempts including EU AI Act. Examines key issues: opacity of algorithmic systems creating black box problems for accountability, professional deskilling risks when practitioners defer to AI, and potential for discrimination embedded in training data and algorithmic logic.",journalArticle,,
68,44AA6ETH,Das verflixte Problem mit Klassifikationen: Zum Einfluss der Digitalisierung auf die Soziale Diagnostik in der Sozialen Arbeit,"Schneider, D.",2024,,,"Examines how digitalization affects social diagnostics and assessment practices in social work. Analyzes fundamental problem of classification systems: tension between necessary categorization for resource allocation and profession's commitment to individualized, contextual understanding of clients. Digital systems intensify this tension by requiring standardized data inputs that may not capture social complexity or unique circumstances. Explores how algorithmic decision support systems rely on predefined categories that risk reifying social problems and overlooking contextual factors.",bookSection,,
69,QG8IAB53,Who cares about data? Data care arrangements in everyday organisational practice,"Jarke, J.; Büchner, S.",2024,10.1080/1369118X.2024.2320917,,"Introduces data care arrangements concept to understand mundane data work in organizations. Demonstrates through empirical research in educational and social service organizations how data care work is distributed across organizational members with different, often conflicting care obligations. Reveals data quality maintenance involves complex sociomaterial configurations of people, infrastructures, routines, and practices. Shows data care work is frequently backgrounded and assumed effortless despite being essential for datafied organizations.",journalArticle,,
70,49PPZJ7Z,Tensions in digital welfare states: Three perspectives on care and control,"Zakharova, I.; Jarke, J.; Kaun, A.",2024,10.1177/14407833241226800,,"Examines tensions between care and control in digital welfare states, analyzing how welfare services increasingly rely on digital technologies and data systems. Develops three analytical perspectives: datafied care practices, algorithmic governance, and digitalized welfare encounters. Demonstrates how digitalization reshapes welfare provision by intensifying surveillance while potentially enabling new forms of care. Reveals fundamental contradictions where care logics and control logics coexist uneasily.",journalArticle,,
71,NQEUZQF9,Datafied ageing futures: Regimes of anticipation and participatory futuring,"Jarke, J.; Manchester, H.",2025,10.1177/20539517241306363,,"Challenges regimes of anticipation suggesting datafied futures are inevitable, arguing futures are actively made through sociotechnical imaginaries promoted by powerful actors. Explores how to democratize futures-making regarding aging populations, critiquing how current anticipations around data-driven systems and ageist assumptions dominate discussions without adequate participation from affected populations. Develops participatory futuring methodology enabling diverse stakeholders, particularly older adults, to imagine and design alternative futures beyond dominant techno-solutionist narratives.",journalArticle,,
72,L6PH7GDL,Künstliche Intelligenz in der Sozialen Arbeit: Grundlagen für Theorie und Praxis,"Linnemann, G.; Löhe, J.; Rottkemper, B.",2025,,https://doi.org/10.3262/978-3-7799-8562-4,"First major German-language systematic treatment of AI in social work from multiple perspectives. Bridges technological progress and ethics, treating AI theoretically and in practice-oriented applications. Addresses technical AI basics for social work, ethical and legal frameworks, bias and discrimination in training data, automation bias risks, development of AI competencies in education and organizations, and specific application fields. Emphasizes responsible, reflective engagement with AI enriching social work without losing sight of human integrity and professional responsibility.",book,,
73,QLXLEUCG,Der Einfluss der Algorithmen: Neue Qualitäten durch Big Data Analytics und Künstliche Intelligenz,"Schneider, D.; Seelmeyer, U.",2018,10.1007/s12054-018-0046-y,,"Examines how algorithmic decision-making affects professional judgment formation and discretionary decision-making space for practitioners. Analyzes automation bias where professionals may over-rely on algorithmic recommendations without adequate critical evaluation. Stresses social work requires debate about what forms of knowledge new technologies can generate, where limits lie, and how it can meaningfully be incorporated into professional reflection and decision-making practices.",journalArticle,,
74,4JN7NIS4,"Künstliche Intelligenz in der Sozialen Arbeit: Grundlagen, Entwicklungen, Herausforderungen","Steiner, O.; Tschopp, D.",2022,10.1007/s12054-022-00546-4,,"Systematically analyzes two key AI application scenarios: Predictive Risk Modeling (PRM) and chatbots in counseling. Discusses neural networks' black box problem, dangers of case labeling through standardization, ethical questions of responsibility and liability when AI predictions diverge from professional judgment, and algorithmic bias risks perpetuating social inequalities. Uses Jonas' ethical theory of responsibility to emphasize ethical responsibility as foundational to all AI implementation decisions.",journalArticle,,
75,GZ9B3GPD,Rationalisierung durch Digitalisierung?,"Waag, P.",2023,10.1007/s12592-023-00472-6,,"Contributes labor sociology and interaction sociology perspectives (particularly Luhmann's interaction theory) to digitalization analyses in social work. Examines potential advantages and disadvantages from multiple stakeholder perspectives (professionals, service users, organizations), revealing that fears and hopes regarding rationalization through digitalization are overly simplistic. Highlights irreducible complexity of professional helping relationships and fundamental limitations of applying rationalization logic to social work contexts.",journalArticle,,
76,MA3LBJS6,Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit: Eine exemplarische arbeitsfeldübergreifende Betrachtung des Natural Language Processing (NLP),"Linnemann, G. A.; Löhe, J.; Rottkemper, B.",2023,10.1007/s12592-023-00455-7,,"Examines Natural Language Processing technologies' significance for social work practice using Staub-Bernasconi's action theory and media equation theory. Analyzes how NLP implementation creates opportunities (enhanced participation, low-threshold access, broader data analysis) and risks (modularized separation of social work activities, potential dehumanization). Emphasizes critical examination of whether AI supports or displaces authentic social work functions.",journalArticle,,
77,2FUXNFZS,"RETHINKING SOCIAL SERVICES WITH ARTIFICIAL INTELLIGENCE: OPPORTUNITIES, RISKS, AND FUTURE PERSPECTIVES",,2024,,https://www.researchgate.net/publication/393509196_RETHINKING_SOCIAL_SERVICES_WITH_ARTIFICIAL_INTELLIGENCE_OPPORTUNITIES_RISKS_AND_FUTURE_PERSPECTIVES,"This chapter explores the transformative potential of artificial intelligence (AI) in the field of social services. It highlights how AI—through data analysis, predictive modeling, and administrative automation—can enhance the effectiveness, accessibility, and efficiency of social work practice. The chapter also presents significant ethical concerns, including risks of algorithmic bias, loss of human connection, and violations of privacy. The author emphasizes that while AI can complement social work, it cannot replace the human-centered values at the core of the profession. It concludes by urging institutions and educational programs to prepare social workers for ethical and effective use of AI and calls for multidisciplinary collaboration to develop guidelines that ensure AI integration supports social justice, equality, and human rights.",bookSection,,
78,HSQW48VE,Artificial Intelligence in Social Work: An EPIC Model for Practice,,2025,10.1080/0312407X.2025.2488345,https://www.tandfonline.com/doi/full/10.1080/0312407X.2025.2488345,"As artificial intelligence (AI) permeates the workplace environments of social workers, there is a need to understand the risks and benefits posed to the mission and values of the profession. This article examines the influence of artificial intelligence on the profession, including opportunities to advance socially just outcomes and challenges that risk ethical practice. A comprehensive review of literature was conducted to examine existing research on the intersection of AI and social work. Drawing on insights from this review, an EPIC model for integrating artificial intelligence into the profession is presented, consisting of four components: (E) ethics and justice; (P) policy development and advocacy; (I) intersectoral collaboration; and (C) community engagement and empowerment. The author contends that augmenting the benefits of artificial intelligence in social work requires a proactive and ethical approach towards a more secure, safe, transparent, and socially just future.",journalArticle,,
79,SS5HTYY6,"Artificial Intelligence in Social Sciences and Social Work: Bridging Technology and Humanity to Revolutionize Research, Policy, and Human Services",,2025,,https://www.multispecialityjournal.com/uploads/archives/20250920153430_MCR-2025-5-004.1.pdf,"This review explores the transformative role of artificial intelligence (AI) in the fields of social sciences and social work, with a focus on developments from 2022 to 2025. It examines how AI technologies—such as machine learning, natural language processing—enhance the analysis of complex social phenomena, support real-time forecasting, and inform data-driven policymaking. Within social work and human services, AI-driven tools facilitate case management, mental health interventions, crisis response, and resource allocation. While acknowledging AI's potential to improve equity and access, the article critically engages with ethical concerns around algorithmic bias, privacy, surveillance, and the erosion of human-centered care. Drawing on recent policy frameworks like the EU AI Act and UNESCO's AI Ethics Guidelines, the review calls for interdisciplinary collaboration to ensure the ethical, inclusive, and accountable integration of AI in social contexts.",journalArticle,,
80,H59BNSX8,Algorithmic decision-making in social work practice and pedagogy: confronting the competency/critique dilemma,"James, P.; Lal, J.; Liao, A.; Magee, L.; Soldatic, K.",2023,10.1080/02615479.2023.2195425,https://www.tandfonline.com/doi/full/10.1080/02615479.2023.2195425,The world is experiencing an accelerating digital transformation. One aspect of this is the implementation of...[source](https://www.google.com/search?q=https://www.aminer.cn/pub/645d0410d68f896efa94d024/algorithmic-decision-making-in-social-work-practice-and-pedagogy-confronting-the-competency),journalArticle,,
81,RAV6DAFQ,The End of the World as We Know It? ChatGPT and Social Work,"Goldkind, L.; Wolf, L.; Glennon, A.; Rios, J.; Nissen, L.",2023,10.1093/sw/swad044,https://doi.org/10.1093/sw/swad044,"Brief commentary marking ChatGPT as a pivotal moment for social work. Encourages proactive engagement to steer AI toward just, human-centered outcomes and warns that non-engagement risks value misalignment and inequity.",journalArticle,,
82,YRBP6IEJ,Introducing Generative Artificial Intelligence into the MSW Curriculum: A Proposal for the 2029 Educational Policy and Accreditation Standards,"Rodriguez, M. Y.; Goldkind, L.; Victor, B. G.; Hiltz, B. S.; Perron, B. E.",2024,10.1080/10437797.2024.2340931,https://doi.org/10.1080/10437797.2024.2340931,"Proposal to add an explicit AI competency to MSW accreditation. Outlines benefits and risks of generative AI, recommends curricular content on ethics, bias, transparency, and responsible use, and frames AI literacy as essential for safeguarding client dignity and equity while leveraging innovation.",journalArticle,,
83,JI24FQPV,Clinical Social Workers’ Perceptions of Large Language Models in Practice: Resistance to Automation and Prospects for Integration,"Creswell Báez, J.; Ahn, E.; Tamietti, A.; Victor, B. G.; Goldkind, L.",2025,10.1080/26408066.2025.2542450,https://doi.org/10.1080/26408066.2025.2542450,"Qualitative study (interviews, reflexive thematic analysis) of clinicians exposed to LLM-supported consultation. Identifies efficiency and documentation support alongside concerns over confidentiality, loss of nuance, and reduced empathy. Concludes AI should augment—not replace—clinical judgment, requiring training and ethics.",journalArticle,,
84,LGYFN6JK,ChatGPT for Social Work Science: Ethical Challenges and Opportunities,"Patton, D. U.; Landau, A.; Mathiyazhagan, S.",2023,10.1086/726042,https://doi.org/10.1086/726042,"Ethical framework for using LLMs in social work research. Recommends transparency, verification, authorship integrity, anti-plagiarism, and inclusion/social justice to counter bias. Positions LLMs as assistive tools requiring critical human oversight.",journalArticle,,
85,X2QQ4JH6,Artificial Intelligence in Social Work: Emerging Ethical Issues,"Reamer, F. G.",2023,,https://jswve.org/wp-content/uploads/2023/10/10-020-205-IJSWVE-2023.pdf,"Comprehensive ethical analysis of AI in social work covering consent, autonomy, privacy, transparency, algorithmic bias, and professional competence. Maps issues to the NASW Code and urges policies, training, and client options to opt out of AI-mediated services.",journalArticle,,
86,GXZWBNJU,Artificial Intelligence in Social Work: An EPIC Model for Practice,"Boetto, H.",2025,10.1080/0312407X.2025.2488345,https://doi.org/10.1080/0312407X.2025.2488345,"Narrative review proposing the EPIC model—Ethics & justice, Policy, Intersectoral collaboration, Community engagement—to guide ethical AI integration. Balances efficiency opportunities with risks of bias and value erosion, advocating structured, human-centered adoption aligned with social justice.",journalArticle,,
87,ZJS2J7E7,Artificial Intelligence (AI) Literacy for Social Work: Implications for Core Competencies,"Ahn, E.; Choi, M.; Fowler, P.; Song, I.",2025,10.1086/735187,https://doi.org/10.1086/735187](https://doi.org/10.1086/735187,"Conceptual paper arguing that AI literacy should be a core social work competency. It links “algorithmic literacy” to existing educational standards and details how AI systems can amplify inequities without critical oversight. The authors propose integrating critical evaluation of AI tools, bias detection, and ethical safeguards into curricula and practice to protect vulnerable populations and uphold social justice.",journalArticle,,
88,Y6M97SWQ,Algorithmic-Assisted Decision-Making Tools in Child Welfare Practice: A Systematic Review,"Yu, M.-H.; Rose, R. A.",2025,10.1177/10497315251350933,https://doi.org/10.1177/10497315251350933,"PRISMA-guided review of algorithmic tools in child welfare. Finds potential for consistency and early risk identification but significant concerns about bias, transparency, practitioner training, and stakeholder inclusion. Recommends audits, participatory design, and ethical guidelines; highlights evidence gaps.",journalArticle,,
89,VICS443I,AI Creates the Message: Integrating AI Language Learning Models into Social Work Education and Practice,"Singer, J. B.; Creswell Báez, J.; Rios, J. A.",2023,10.1080/10437797.2023.2189878,https://doi.org/10.1080/10437797.2023.2189878,"Commentary advocating cautious integration of LLMs in teaching and practice. Describes pedagogical uses (idea generation, material tailoring) and warns of bias, factual errors, confidentiality risks, and plagiarism. Emphasizes transparency policies and that engagement with AI is ethically preferable to avoidance.",journalArticle,,
90,R5QQTD95,Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen Arbeit – Eine empirische Untersuchung aus der Perspektive von Berufseinsteiger:innen,"Schönauer, A.-L.",2025,10.1007/s12054-025-00783-3,https://doi.org/10.1007/s12054-025-00783-3,Empirischer Kurzbeitrag zu Einstellungen von Berufseinsteiger:innen: überwiegend kritische Haltung gegenüber KI in direkter Klient:innenarbeit; Datenschutz- und Empathiefragen zentral. Akzeptanz steigt mit digitaler Kompetenz; Empfehlung: Aus- und Weiterbildung für kritische KI-Literacy und partizipative Technikgestaltung.,journalArticle,,
91,JNLPSHD5,Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit: Eine exemplarische arbeitsfeldübergreifende Betrachtung des Natural Language Processing (NLP),"Linnemann, G. A.; Löhe, J.; Rottkemper, B.",2023,10.1007/s12592-023-00455-7,https://link.springer.com/article/10.1007/s12592-023-00455-7,Theoriegeleitete Analyse zu Chancen und Risiken von NLP für die Soziale Arbeit. Diskutiert mögliche Auslagerung sozialarbeiterischer Tätigkeiten vs. Potenziale für Teilhabe und Zugänge; ordnet KI entlang handlungstheoretischer Konzepte ein und plädiert für kritische Auseinandersetzung im Einklang mit professionsethischen Werten.,journalArticle,,
92,H7E3N6VR,Diskriminierung durch Algorithmen – Überlegungen zur Stärkung KI-bezogener Kompetenzen,"Sūna, L.; Hoffmann, D.; Mollen, A.",2024,,https://www.gmk-net.de/wp-content/uploads/2024/12/gmk60_suna_hoffmann_mollen.pdf,"Konzeptioneller Beitrag zu Ursachen und Formen algorithmischer Diskriminierung und zur Förderung kritischer KI-Kompetenzen. Plädiert für Aufklärung zu Datenbias, reflexive Nutzung und partizipative Trainings, um Benachteiligungen zu erkennen und digitale Teilhabe zu stärken.",bookSection,,
93,MITLDF9S,KI-basiertes Assistenzsystem im Kinderschutzverfahren,"Feist-Ortmanns, M.; Sauer, A.; Brinkmann, M.",2025,,https://afet-ev.de/assets/themenplattform/KI-in-der-Kinder--und-Jugendhilfe-Naher-Grasshoff-Forum-2.pdf,"Praxisnaher Bericht zu einem KI-Assistenzsystem für Gefährdungseinschätzung. Betont Human-in-the-Loop, Datenschutz, Organisationsentwicklung und Schulungsbedarf; zeigt Potenziale (präventive Hinweise) und Risiken (Bias, Fehlklassifikationen) im sensiblen Kinderschutzkontext.",bookSection,,
94,D4FY2S3R,Künstliche Intelligenz in der Sozialen Arbeit – Zwischen Bedenken und Optionen,"Gravelmann, R.",2024,,https://www.beltz.de/fachmedien/sozialpaedagogik_soziale_arbeit/zeitschriften/theorie_und_praxis_der_sozialen_arbeit/artikel/53521-kuenstliche-intelligenz-in-der-sozialen-arbeit-zwischen-bedenken-und-optionen.html,"Überblicksbeitrag zu Status, Risiken und Potenzialen von KI in Praxisfeldern der Sozialen Arbeit. Betont Spannungen zwischen Effizienzgewinnen und Werten wie Menschenwürde; skizziert Handlungsfelder und fordert professionellen Diskurs sowie reflektierte Implementationsstrategien.",journalArticle,,
95,DGLWM93D,Voll (dia)logisch? Ein Werkstattbericht über den Einsatz von generativer KI in der Hochschulbildung für Soziale Arbeit – Curriculare Überlegungen und veränderte Akteurskonstellationen,"Engelhardt, E.; Ley, T.",2025,,https://www.pedocs.de/volltexte/2025/33133/pdf/Engelhardt_Ley_2025_Voll_dia_logisch.pdf,"Werkstattbericht zur curricularen Integration generativer KI. Positioniert Prompting als metakognitive Schlüsselkompetenz und diskutiert Rollenwandel von Lehr-/Lernakteuren; fordert reflektierte, ethisch eingebettete Nutzung mit Fokus auf kritische Validierung von KI-Ergebnissen.",bookSection,,
96,EB7PZUZZ,Prompten nach Plan: Das PCRR-Framework als pädagogisches Werkzeug für den Einsatz von Künstlicher Intelligenz.,"Freinhofer, Dominik; Schwabl, Gerlinde; Aichinger, Susanne; Breitenberger, Sandra; Steindl, Sandra; Hechenberger, Tanja",2025,10.21243/mi-01-25-26,https://journals.univie.ac.at/index.php/mp/article/view/9274,"Die rasante Entwicklung generativer Künstlicher Intelligenz (KI) macht Prompt Engineering zu einer Schlüsselkompetenz für den kompetenten Umgang mit KI-Modellen. Während zahlreiche Prompting-Techniken und -Frameworks existieren, fehlt bislang eine systematische Integration in den schulischen Kontext. Diese Publikation stellt das PCRR-Framework (Plan – Create – Review – Reflect) vor, das als ganzheitlicher Ansatz für den Einsatz von Prompt Engineering im Unterricht dient. Basierend auf Erfahrungen aus dem Hochschullehrgang „Künstliche Intelligenz im IT-Unterricht der Berufsbildung“ (PH Tirol und Hochschule für Agrar- und Umweltpädagogik) wurde das Framework iterativ weiterentwickelt und in drei Praxisbeispielen erprobt. Die Ergebnisse zeigen, dass das PCRR-Framework die Effizienz und Qualität der Prompterstellung steigern kann und die Schüler:innen beim Umgang mit Sprachmodellen (Large Language Models, LLMs) unterstützt. Gleichzeitig wurden Herausforderungen deutlich, insbesondere hinsichtlich der methodischen Vergleichbarkeit der Ergebnisse sowie der Akzeptanz bestimmter Prompting-Techniken. Das Paper diskutiert diese Erkenntnisse, methodischen Limitationen und Verbesserungspotenziale und bietet einen Ausblick auf zukünftige Forschungsarbeiten. Neben der Weiterentwicklung des PCRR-Frameworks wird die Notwendigkeit betont, AI Literacy systematisch in Lehrpläne und Lehramtsausbildungen zu integrieren, um eine nachhaltige und verantwortungsbewusste Nutzung von KI im Bildungsbereich zu ermöglichen.",journalArticle,,
97,7FBB2KUC,Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit,"Linnemann, Gesa Alena; Löhe, Julian; Rottkemper, Beate",2023,10.1007/s12592-023-00455-7,https://doi.org/10.1007/s12592-023-00455-7,"Die Bedeutung des Einsatzes von Verfahren, die unter dem Begriff der Künstlichen Intelligenz (KI) zusammenzufassen sind, wird sowohl für gesellschaftliche Prozesse als auch den Auftrag an die Soziale Arbeit zunehmend erkannt und diskutiert. Mit diesem Artikel wird ein Beitrag zum Diskurs geleistet, indem vertieft der Bereich der Sprachverarbeitung durch KI, das Natural Language Processing (NLP), in den Blick genommen wird. Verarbeitung natürlicher Sprache ist aufgrund der hohen Bedeutung kommunikativer Prozesse für die Praxis der Sozialen Arbeit von besonderer Relevanz, zugleich wird die Profession der Sozialen Arbeit tangiert. Bezugnehmend auf Staub-Bernasconis Handlungstheorie werden Implikationen und Diskussionspunkte von NLP identifiziert und diskutiert. Zudem werden mögliche Gratifikationen für Klient*innen herausgearbeitet, die sich u. a. aus der Wirkung und sozialen Interaktion ergeben. Hier wird die Media-Equation-Theorie von Nass und Reeves als Erkenntnisfolie herangezogen. Vor diesen Perspektiven ergeben sich sowohl Risiken (u. a. die Gefahr einer modularisierten Herauslösung genuin sozialarbeiterischer Tätigkeit) als auch Chancen (u. a. Teilhabe, niederschwelliger Zugang, Zugriff auf breitere Datenbasis).",journalArticle,,
98,6N2E242K,Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen Arbeit,"Schönauer, Anna-Lena",2025,10.1007/s12054-025-00783-3,https://doi.org/10.1007/s12054-025-00783-3,"Der Artikel untersucht die Akzeptanz und organisationalen Rahmenbedingungen digitaler Technologien, insbesondere Künstlicher Intelligenz (KI), in der Praxis der Sozialen Arbeit aus Sicht der Fachkräfte. Während digitale Technologien im administrativen Bereich bereits weit verbreitet sind, zeigt sich bei der Nutzung digitaler Technologien in der direkten Arbeit mit den Klient:innen noch Zurückhaltung. Durch die Entwicklungen im Bereich KI ergeben sich zunehmend neue Möglichkeiten digitale Technologien auch im Bereich der Interaktionsarbeit mit den Klient:innen zu nutzen. Zugleich stellt dies die Fachkräfte vor ethische und professionelle Herausforderungen. Die empirische Untersuchung unter Berufseinsteiger:innen zeigt, dass der Einsatz von KI im beruflichen Kontext kritisch bewertet wird. Bedenken bestehen insbesondere in Bezug auf Datenschutz und die Unersetzbarkeit menschlicher Empathie durch die KI. Die Akzeptanz von KI und digitalen Technologien hängt von den digitalen Kompetenzen und den Erfahrungen der Fachkräfte beim Einsatz digitaler Technologien im beruflichen Kontext ab, die in diesem Bereich noch ausbaufähig sind. Vor dem Hintergrund einer zunehmend digitalisierten Gesellschaft fordert der Artikel eine kritische Auseinandersetzung mit den technischen Entwicklungen und betont die Notwendigkeit von Fort- und Weiterbildungen sowie einer partizipativen Technikgestaltung, um die Praxis der Sozialen Arbeit nachhaltig zu gestalten.",journalArticle,,
99,HT9ZSI9U,Handbuch Soziale Arbeit und Digitalisierung,"Kutscher, Nadia; Ley, Thomas; Seelmeyer, Udo; Siller, Friederike; Tillmann, Angela; Zorn, Isabel",2020,,https://www.beltz.de/fileadmin/beltz/leseproben/978-3-7799-5258-9.pdf,"Dieses umfassende Handbuch mit über 50 Beiträgen behandelt erstmals systematisch Digitalisierung in Bezug auf Disziplin und Praxis der Sozialen Arbeit. Das 658-seitige Werk beleuchtet aus verschiedenen disziplinären Perspektiven gesellschaftliche Entwicklungen, Diskurse, digitalisierte Formen der Dienstleistungserbringung, Profession, Organisation und Handlungsfelder sowie neue Herausforderungen für Forschung. Zentrale Themen umfassen Mediatisierung, Akteur-Netzwerk-Theorie, ethische Fragen, informationelle Selbstbestimmung, Datenschutz, Social Media, E-Government, digitalisierte Kinder- und Jugendhilfe, Medienpädagogik und Sozialwirtschaft. Das Handbuch verfolgt einen über technisches Verständnis hinausgehenden Begriff von Digitalität und fokussiert auf soziotechnische Arrangements sowie deren Folgen für Akteure, Formen und Rahmenbedingungen Sozialer Arbeit. Mit Perspektiven auf Organisation, Fachkräfte, Adressat*innen und Erbringungsformen werden Möglichkeiten, Risiken und offene Fragestellungen diskutiert.",book,,
100,79TL6HSB,Social work and artificial intelligence: Collaboration and challenges,"Chen, Y.-C.; Lin, C.-C.",2025,,https://dmjr-journals.com/assets/article/1755892935-SOCIAL_WORK_&_ARTIFICIAL_INTELLIGENCE-_COLLABORATION_AND_CHALLENGES.pdf,"This qualitative study explores current AI applications in social work through interviews with professionals, AI developers, and policymakers, identifying challenges including insufficient decision-making transparency, gaps in ethical frameworks, and inadequate technical literacy among professionals. The research reveals that while ninety percent of social work professionals acknowledge AI's auxiliary function in daily operations, concerns persist about automation bias and the potential undermining of professional autonomy. The study proposes educational training and policy recommendations while advocating for explainable AI systems and strengthened ethical governance to foster harmonious development between technological applications and humanistic values.",journalArticle,,
101,PAZJHB8J,The role of artificial intelligence (AI) and machine learning in social work practice,"Nuwasiima, M.; Ahonon, M. P.; Kadiri, C.",2024,10.30574/wjarr.2024.24.1.2998,https://doi.org/10.30574/wjarr.2024.24.1.2998,"This comprehensive review identifies algorithmic bias as a critical challenge in social work AI implementation, noting that algorithms trained on historical data may perpetuate existing inequalities by replicating racial, gender, and socio-economic disparities. The authors document specific cases where predictive analytics tools disproportionately flagged families of color for child welfare interventions despite lacking substantial evidence of higher abuse rates. The study emphasizes that addressing algorithmic bias requires multi-faceted approaches including diverse and inclusive datasets, ongoing evaluation and auditing of AI systems, and involvement of social workers and community members in AI tool development.",journalArticle,,
102,YTDMY5W4,A framework for the learning and teaching of Critical AI Literacy skills (Version 0.1),"Hauck, M.; Moore, E.; Wright, C.",2025,,https://www.open.ac.uk/blogs/learning-design/wp-content/uploads/2025/01/OU-Critical-AI-Literacy-framework-2025-external-sharing.pdf,"This framework defines Critical AI Literacy as expanding beyond traditional AI literacy to examine how Large Language Models contribute to ongoing epistemic injustices that can lead to significant social and personal harm. It applies equality, diversity, inclusion, and accessibility principles to AI use, emphasizing the importance of critically evaluating AI-generated outputs and engaging in equitable and inclusive prompting practices. Critical AI Literacy is conceptualized as context-specific and treats literacy as a social practice rather than a possession. At advanced levels, it examines AI's potential to shift power relationships and explores how AI contributes to inequalities while considering ways it could help redress power balances.",report,,
103,PYN6HB3E,"Social work in the age of artificial intelligence: A rights-based framework for evidence-based practice through social psychology, group dynamics, and institutional analysis","Alam, N.",2025,10.1080/26408066.2025.2547219,https://doi.org/10.1080/26408066.2025.2547219,"This study develops a comprehensive rights-based framework for navigating AI integration in social work practice while addressing ethical implications across micro, meso, and macro practice levels. The framework bridges social work theory with interdisciplinary insights, demonstrating that AI systems profoundly impact vulnerable populations by mediating interpersonal relationships and constructing meaning in AI-mediated environments. It provides evidence-based guidance for practitioners to harness AI's potential while safeguarding core social work values of human dignity, self-determination, and social justice, offering concrete strategies for social work education and research methodologies that center community voices.",journalArticle,,
104,THVKEETD,Artificial Intelligence (AI) literacy for social work,"Ahn, E.",2025,10.1086/735187,https://doi.org/10.1086/735187,"This work explores how AI literacy, defined as the knowledge and skills to understand, use, and critically evaluate AI systems, can enhance social workers' ability to navigate technological integration while maintaining professional values. It emphasizes that embedding AI literacy into core competencies enables social workers to better address emerging challenges and promote equity in an AI-influenced society. The author argues that AI literacy must go beyond technical skills to include critical evaluation of AI's social implications and potential for bias, advocating for comprehensive professional development that prepares social workers to engage with AI tools ethically and effectively.",journalArticle,,
105,CF6T2RD7,Artificial intelligence in social work: Emerging ethical issues,"Reamer, F. G.",2023,,https://jswve.org/volume-20/issue-2/item-05/,"This comprehensive analysis identifies key ethical challenges in social work's adoption of AI, including informed consent and client autonomy, privacy and confidentiality, transparency, client misdiagnosis, algorithmic bias and unfairness, and the need for evidence-based AI tools. The author proposes a concrete ethics-informed protocol for AI implementation, advocating for digital ethics steering committees and transparent, auditable methodologies. The work stresses that social workers must familiarize themselves with AI protocols and ensure compliance with ethical standards while incorporating AI literacy into social work education curricula.",journalArticle,,
106,HZKE8T8I,Künstliche Intelligenz in der Sozialen Arbeit – Zwischen Bedenken und Optionen,"Gravelmann, R.",2024,,https://content-select.com/de/portal/media/view/66472512-3a7c-4fef-b8e6-a5d3ac1b0002,"Gravelmann analyzes the impact of AI on the social work profession, identifying both opportunities and risks. Potential benefits include AI as communication aid, documentation tool, and data analysis instrument. Critical concerns include the danger of decision delegation to AI systems, potentially reducing professionals to executors. The author warns against automated AI-based procedures that massively intervene in people's lives, especially in child protection. Ethical problems are identified in the lack of objectivity in AI data, which reflects societal power relations and reproduces discriminatory value systems.",journalArticle,,
107,LXKXPD5H,Digitale Werkzeuge und Machtasymmetrien?,"Studeny, S.",2025,,https://www.ogsa.at/wp-content/uploads/2025/03/ogsaTAGUNG2025_AG-Digitalisierung_Handout-Machtasymmetrien-Susanne-Studeny.pdf,"Studeny analyzes power asymmetries in digital social work, emphasizing that digital tools and algorithms create new, often invisible forms of power and control. AI decisions remove influence from both professionals and clients while responsibility remains unclear. Algorithms reinforce discrimination as they work with biased data. The author demands that social work critically reflects on digital technologies, demands transparency, and ensures that technology serves people.",conferencePaper,,
108,9832ZJB7,"Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden für KI in der Verwaltung, Version 1.0","Biegelbauer, P.; Lackinger, C.; Schlarb, S.; Subak, E.; Weinlinger, P.",2023,,https://oeffentlicherdienst.gv.at/wp-content/uploads/2023/11/Leitfaden-Digitale-Verwaltung-Ethik.pdf,"This guideline defines AI literacy as the ability to understand and use AI, emphasizing that safe, self-determined, and responsible use requires sufficient understanding of the technology's functioning, possibilities, and challenges. It identifies automation bias as a central risk and emphasizes competency building and training as the foundation for all further measures, recommending the creation of educational standards for AI procurement and application.",report,,
109,DJEVSR8D,AI FORA – Artificial Intelligence for Assessment: Fairness bei der Verteilung öffentlicher sozialer Leistungen,"Ahrweiler, P.",2025,,https://nachrichten.idw-online.de/2025/03/18/wie-koennte-kuenstliche-intelligenz-weltweit-fairness-bei-der-verteilung-oeffentlicher-sozialer-leistungen-erhoehen,"This international research project examined AI-supported social assessments across nine countries on four continents. The study demonstrates that justice criteria for receiving state benefits are culture- and context-dependent. A central finding is that deploying a standardized AI system globally is insufficient; instead, flexible, dynamic, and adaptive systems are required. Development of such systems depends on contributions from all societal actors, including vulnerable groups, for designing participatory, context-specific, and fair AI.",report,,
110,QVNJIQJG,CAIL – Critical AI Literacy: Kritische Technikkompetenz für konstruktiven Umgang mit KI-basierter Technologie in Betrieben,"Strauß, S.",2024,,https://wien.arbeiterkammer.at/service/digifonds/gefoerderte-projekte/CAIL_Bericht-FINAL.pdf,"This study develops a Critical AI Literacy framework defined as critical competency for assessing practical utility and limitations of AI applications in specific contexts. It emphasizes that AI-based automation is more complex, dynamic, and volatile than classical automation forms, creating new challenges. A central finding is that critical AI competency becomes part of knowledge work, as interpretation and verification of AI results remains an essential human task. The project identifies deep automation bias as a meta-risk of AI deployment.",report,,
111,UHEM78MX,What is AI Literacy? Competencies and Design Considerations,"Long, Duri; Magerko, Brian",2020,10.1145/3313831.3376727,https://dl.acm.org/doi/10.1145/3313831.3376727,,conferencePaper,,
112,JHRVDXSD,Artificial Intelligence (AI) Literacy for Social Work: Implications for Core Competencies,"Ahn, Eunhye; Choi, Moon; Fowler, Patrick; Song, In Han",2025,10.1086/735187,https://www.journals.uchicago.edu/doi/10.1086/735187,,journalArticle,,
113,2MVSL3UV,Learning About AI: A Systematic Review of Reviews on AI Literacy,"Zhang, Shan; Ganapathy Prasad, Priyadharshini; Schroeder, Noah L.",2025,10.1177/07356331251342081,https://journals.sagepub.com/doi/10.1177/07356331251342081,"Given the ubiquity of artificial intelligence (AI), it is essential to empower students to become creators, designers, and producers of AI technologies, rather than limiting them to the role of informed consumers. To achieve this, learners need to be equipped with AI knowledge and concepts and develop AI literacy. Paradoxically, it is largely unclear what AI literacy is and how we should learn and teach it. We address both of these questions through a systematic review of systematic reviews, also known as an umbrella review, to gain a comprehensive understanding of AI literacy. After searching the literature, we critically examine the results of 17 reviews focusing on AI literacy and the teaching and learning of AI concepts. Our analysis revealed several encouraging developments: a general consensus on the definition of AI literacy, the availability of teaching tools and materials that support AI learning without prior programming experience, and effective pedagogical approaches that have shown positive effects on students' understanding and engagement. In addition, we identified several areas needing attention in the field: an interdisciplinary pedagogical approach, integration of ethical considerations in AI education, discussions on AI policy, and standardized, content-validated, reliable assessments across educational levels and cultures.",journalArticle,,
114,UKKQKL7I,Fragile Foundations: Hidden Risks of Generative AI,"Washington, Anne L.",2025,10.11586/2025078,https://www.bertelsmann-stiftung.de/doi/10.11586/2025078,"Foundation models are the backbone of generative AI and thus central to applications such as ChatGPT, Gemini, or Copilot. However, their use comes with risks: from randomly compiled training data and opaque processes to profit-driven business models.

The new report Fragile Foundations: Hidden Risks of Generative AI by the Bertelsmann Stiftung shows why mission-driven organizations in particular should critically question the foundations of AI. It highlights the systemic weaknesses of foundation models, the dangers they pose to vulnerable groups, and the possible alternatives.

This report illustrates why it is critical to scrutinize foundation models. It thus offers a starting point and impetus for decision-makers and practitioners in mission-driven organizations, as well as anyone committed to a responsible digital future. Using generative AI meaningfully in the service of the common good requires a clear understanding of the technology’s foundations –and of the questions these raise for mission-driven organizations.",journalArticle,,
115,8MRNK6FX,Research on the application risks and countermeasures of ChatGPT generative artificial intelligence in social work,,2024,10.23977/jaip.2024.070222,https://www.clausiuspress.com/article/12988.html,,journalArticle,,
116,SSF5Q33W,Research on the application risks and countermeasures of ChatGPT generative artificial intelligence in social work,,2024,10.23977/jaip.2024.070222,https://www.clausiuspress.com/article/12988.html,,journalArticle,,
117,LPMP8QAY,Messung von AI Literacy – Empirische Evidenz und Implikationen,"Weber, Patrick; Baum, Lorenz; Pinski, Marc",2023,,https://aisel.aisnet.org/wi2023/3,,journalArticle,,
118,KQ5A8D6E,AI competency framework for students,,2024,,https://unesdoc.unesco.org/ark:/48223/pf0000391105,,book,,
119,K3YCLBXK,"AI Literacy: A Framework to Understand, Evaluate, and Use Emerging Technology","Ruiz, Pati; Mills, Kelly; Lee, Keun-woo; Coenraad, Merijke; Fusco, Judi; Roschelle, Jeremy; Weisgrau, Josh",2024,,https://hdl.handle.net/20.500.12265/218,"To enable all who participate in educational settings to leverage AI tools for powerful learning, this paper describes a framework and strategies for educational leaders to design and implement a clear approach to AI Literacy for their specific audiences (e.g. learners, teachers, or others) that are safe and effective. The first part of the paper describes a framework that identifies essential components of AI Literacy and connects them to existing initiatives. The second part of the paper identifies strategies and illustrative examples as guidance for educational leaders to integrate AI Literacy in PK–12 education and adapt to their unique contexts.",report,,
120,BFG8VUK3,Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy,"Shah, Syed",2025,10.70389/PJAI.1000088,https://premierscience.com/pjai-24-524/,"Purpose

This narrative review investigates the interplay between gender bias in artificial intelligence (AI) systems and the potential of digital literacy to empower women in technology. By synthesising research from 2010 to 2024, the study examines how gender bias manifests in AI, its impact on women’s participation in technology, and the effectiveness of digital literacy initiatives in addressing these disparities.

Methods

A systematic literature search was conducted across major academic databases, including Web of Science, Scopus, IEEE Xplore, and Google Scholar. The review focused on peer-reviewed articles, reports, and case studies published between 2010 and 2024 that addressed gender bias in AI, women’s participation in technology, and digital literacy initiatives. A thematic analysis framework was employed to identify and synthesise recurring themes and patterns.

Results

The findings reveal systemic gender biases embedded in AI applications across diverse domains, such as recruitment, healthcare, and financial services. These biases stem from factors including the under-representation of women in AI development teams, biased training datasets, and algorithmic design choices. Digital literacy programs emerge as a promising intervention, fostering a critical awareness of AI bias, encouraging women to pursue AI careers, and catalysing growth in women-led AI projects.

Conclusions Although gender bias in AI poses significant challenges, this review highlights digital literacy as a transformative tool for achieving gender equity in AI development and application. The study highlights the importance of inclusive AI design, gender-responsive education policies, and sustained research efforts to mitigate bias and promote equity.",journalArticle,,
121,FTJM5R8N,Defeating Nondeterminism in LLM Inference,,,,https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/,"Reproducibility is a bedrock of scientific progress. However, it’s remarkably difficult to get reproducible results out of large language models.
For example, you might observe that asking ChatGPT the same question multiple times provides different results. This by itself is not surprising, since getting a result from a language model involves “sampling”, a process that converts the language model’s output into a probability distribution and probabilistically selects a token.
What might be more surprising is that even when we adjust the temperature down to 0This means that the LLM always chooses the highest probability token, which is called greedy sampling. (thus making the sampling theoretically deterministic), LLM APIs are still not deterministic in practice (see past discussions here, here, or here). Even when running inference on your own hardware with an OSS inference library like vLLM or SGLang, sampling still isn’t deterministic (see here or here).",blogPost,,
122,MPCZVZEW,Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room,"Goellner, J.; Kumar, V.; Aleven, V.",2025,,https://arxiv.org/pdf/2504.16148,"Identifies nine persistent challenges undermining responsible use of AI in education, including neglect of key learning processes, lack of stakeholder involvement, and use of unreliable XAI methods. Proposes hybrid human-AI methods, specifically neural-symbolic AI (NSAI), which integrates expert domain knowledge with data-driven approaches. This hybrid architecture allows for built-in transparency, stakeholder engagement, and modeling of complex pedagogically-grounded principles.",report,,
123,XY2WVKBY,"Governing discriminatory content in conversational AI: A cross-system, cross-lingual, and cross-topic audit","Zeng, J.; van Es, K.",2025,10.1080/1369118X.2025.2537803,https://doi.org/10.1080/1369118X.2025.2537803,"Conducts mixed-method audit of how major conversational AI systems respond to and regulate discriminatory content. Analysis is cross-system, cross-lingual, and cross-topic, revealing that refusal sensitivity and answering strategies vary significantly across all three axes. Discusses value alignment process through reinforcement learning with human feedback and implementation of guardrails, highlighting tensions when tech platforms become arbiters of morality.",journalArticle,,
124,JPUCNHNU,LIBRA: Measuring bias of large language model from a local context,"Pan, B.; Liu, H.; Hou, Y.; Yang, M.",2025,,https://www.researchgate.net/publication/388686547_LIBRA_Measuring_Bias_of_Large_Language_Model_from_a_Local_Context,"Critiques the U.S.-centricity of existing LLM bias evaluation methods. Proposes the Local Integrated Bias Recognition and Assessment (LIBRA) framework and develops dataset of over 360,000 test cases specific to New Zealand context. Results show models like BERT and GPT-2 struggle with local context, while Llama-3 responds better to different cultural contexts despite exhibiting larger bias overall.",report,,
125,YV53DKI2,Explainability through systematicity: The hard systematicity challenge,"Santos, J. D.",2024,,https://pmc.ncbi.nlm.nih.gov/articles/PMC12307450/,"This philosophical paper argues that the pursuit of ""explainability"" in AI is too narrow. It proposes a richer ideal called ""systematicity,"" which demands that an AI's reasoning be consistent, coherent, comprehensive, and principled, akin to an integrated body of human thought. The author distinguishes this ""hard systematicity challenge"" from the historical Fodorian debate on connectionism and explores how the demand for AI to be systematic should be regulated by different rationales.",journalArticle,,
126,BDBYDLVK,A reparative turn in AI,"Singh, R.; Posada, J.; Penney, D.",2025,,https://arxiv.org/pdf/2506.05687,"Argues for a ""reparative turn"" in AI governance, moving beyond harm prevention to focus on remedying harm after it occurs. Based on thematic analysis of 1,060 real-world AI harm incidents, proposes taxonomy of reparative actions around four goals: acknowledging harm, attributing responsibility, providing remedies, and enabling systemic change. Finds significant ""accountability gap"" with most corporate responses limited to symbolic acknowledgments.",report,,
127,T9KEZN3G,Governance of generative AI: A comprehensive framework for navigating challenges and opportunities,"Taeihagh, A.",2025,,https://academic.oup.com/policyandsociety/article/44/1/1/7997395,"Provides comprehensive overview of governance challenges posed by generative AI, including bias amplification, privacy violations, misinformation, and exacerbation of power imbalances. Critiques inadequacy of voluntary self-regulation and proposes comprehensive governance framework that is proactive, adaptive, and participatory. Recommends improving data governance, mandating independent audits, enhancing public engagement, and fostering international cooperation.",journalArticle,,
128,99QJDBSV,How large language models judge cooperation,"Santos, F. P.; Lages, M.; Melo, F. S.",2025,,https://arxiv.org/pdf/2507.00088,"This study investigates how 21 state-of-the-art LLMs make social and moral judgments about cooperative behavior. Using an evolutionary game-theory model and a dataset of 43,200 prompts, the authors find significant variation in how different models assign reputations, particularly when judging interactions with ""ill-reputed"" actors. Demonstrates that LLM social norms are highly malleable and can be consistently steered by different types of prompt interventions.",report,,
129,R3VJVFCE,Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,"Kamruzzaman, M.; Kim, G. L.",2024,,https://arxiv.org/abs/2404.17218,"Explores how different prompt engineering strategies can mitigate social biases in LLM outputs by analogizing model's reasoning to human cognitive processes. Leverages dual-process cognition theory (System 1 vs System 2) to design prompts that encourage deliberative reasoning. Finds certain prompting techniques significantly reduce biased responses, with up to 13% reduction in stereotypes.",report,,
130,GCQ8J9XF,Beyond transparency and explainability: On the need for adequate and contextualized user guidelines for LLM use,"Barman, K. G.; Wood, N.; Pawlowski, P.",2024,10.1007/s10676-024-09778-2,https://doi.org/10.1007/s10676-024-09778-2,"Argues for user-centered approach to governing AI systems, contending that transparency alone is insufficient. Proposes contextualized guidelines and training for users including clear instructions on LLM reliability, diversity-sensitive prompting techniques, and iterative query refinement. Emphasizes shifting focus from AI's internal workings to human-AI interaction context.",journalArticle,,
131,EMZ33KFH,Unequal voices: How LLMs construct constrained queer narratives,"Ghosal, A.; Gupta, A.; Srikumar, V.",2025,,https://arxiv.org/abs/2507.15585,"Investigates how large language models represent queer individuals in generated narratives, uncovering tendencies toward stereotyped and narrow portrayals. Identifies phenomena including narrow topic range, discursive othering, and identity foregrounding. Shows LLMs unconsciously reinforce divide where marginalized groups are not afforded same breadth of narrative roles as others.",report,,
132,WLV8L8PM,"Guardrails, not guidance: Understanding responses to LGBTQ+ language in large language models","Tint, J.",2025,,https://aclanthology.org/2025.queerinai-main.2.pdf,"Examines how large language models respond to prompts involving LGBTQ+ terminology and how current safety measures handle such content. Finds disparity where LLMs invoke safety guardrails for overtly heteronormative prompts but exhibit subtle biases when handling queer slang or informal LGBTQ+ language, responding with more negative emotional tone without triggering content filters.",conferencePaper,,
133,38E5FZDV,Explicitly unbiased large language models still form biased associations,"Bai, X.; Wang, A.; Sucholutsky, I.; Griffiths, T. L.",2025,10.1073/pnas.2416228122,https://doi.org/10.1073/pnas.2416228122,"Demonstrates that even when LLMs are aligned to avoid overt bias, they can still harbor implicit biases. Introduces novel evaluation methods inspired by psychology: LLM Word Association Test (LLM-WAT) and LLM Relative Decision Test (LLM-RDT) to probe automatic associations and subtle discrimination. Finds pervasive stereotype-consistent biases across multiple domains in eight state-of-the-art, value-aligned models.",journalArticle,,
134,WNY526GN,Bias and fairness in large language models: A survey,"Gallegos, I. O.; Rossi, R. A.; Barrow, J.; Tanjim, M. M.; Kim, S.; Dernoncourt, F.; Yu, T.; Zhang, R.; Ahmed, N. K.",2024,10.1162/coli_a_00524,https://doi.org/10.1162/coli_a_00524,"This comprehensive survey consolidates recent research on social biases in large language models (LLMs) and methods to mitigate them. The authors formalize key concepts of bias and fairness in NLP, presenting three taxonomies: metrics for bias evaluation organized by model level, datasets for bias evaluation categorized by structure and target social groups, and bias mitigation techniques classified by intervention stage.",journalArticle,,
135,22XEFRWP,Prejudiced interactions with large language models (LLMs) reduce trustworthiness and behavioral intentions among members of stigmatized groups,"Petzel, Z. W.; Sowerby, M.",2025,10.1016/j.chb.2025.108563,https://doi.org/10.1016/j.chb.2025.108563,"Investigates how biased or prejudiced content in LLM responses affects user trust and willingness to use the system, particularly for users from marginalized communities. Through three preregistered experiments, finds that when AI responses exhibited prejudice, participants from marginalized groups reported significantly lower trust and decreased intentions to continue using the system.",journalArticle,,
136,QDUGSBPC,Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation,"An, J.; Huang, D.; Lin, C.; Tai, M.",2025,10.1093/pnasnexus/pgaf089,https://doi.org/10.1093/pnasnexus/pgaf089,"Examines intersectional bias in LLM-based decision-making using AI-driven resume screening context. Multiple recent LLMs scored ~361,000 synthetic entry-level job resumes with systematically varied demographic attributes. Revealed significant biases: female candidates received higher competence scores than equally qualified males, while Black male candidates received markedly lower scores, demonstrating intersectional effects.",journalArticle,,
137,7L78MV2V,"Biases in large language models: Origins, inventory and discussion","Navigli, R.; Conia, S.; Ross, B.",2023,10.1145/3597307,https://doi.org/10.1145/3597307,"Provides an overview of various social biases manifested by large language models and discusses their root causes. Examines how training data selection leads to bias and surveys different types of biases including gender, racial/ethnic, sexual orientation, age, religious and cultural biases. Compiles an inventory of biased behaviors and discusses emerging approaches to measure and mitigate such biases.",journalArticle,,
138,IW32JGWV,Advancing Accountability in AI,,2023,,https://www.oecd.org/content/dam/oecd/en/publications/reports/2023/02/advancing-accountability-in-ai_753bf8c8/2448f04b-en.pdf,"Delivers a multi-level review of AI accountability, focusing on transparency, fairness, and privacy. Discusses trade-offs in adopting explainability and transparency measures while mitigating algorithmic bias and upholding fairness, framed within legal, social, and ethical requirements for inclusive, trustworthy AI.",report,,
139,LBLF9BCW,Dipper: Diversity in Prompts for Producing Large Language Model Outputs,"Lau, G. K. R.",2023,,https://www.comp.nus.edu.sg/~greglau/assets/pdf/dipper_neurips_mint.pdf,"Presents 'Dipper', an LLM prompting ensemble framework that systematically deploys a diverse set of prompts in parallel to improve the breadth of generated perspectives, including those of minority or marginalized groups. This training-free technique enhances demographic and perspective diversity without performance degradation.",conferencePaper,,
140,QFPTW4VL,Explainable Artificial Intelligence,,2023,,https://www.edps.europa.eu/system/files/2023-11/23-11-16_techdispatch_xai_en.pdf,"Emphasizes the necessity of Explainable AI (XAI) to ensure human-centered, transparent, and ethical AI systems. By increasing transparency and comprehensibility of algorithmic decisions, XAI enables individuals—including those from marginalized groups—to participate meaningfully in digital decision-making and challenge unjust outcomes.",report,,
141,DUV4TUG3,Improving diversity of demographic representation in people entities in Large Language Models,"Lahoti, P.; Kiela, D.; Ahn, S.",2023,,https://aclanthology.org/2023.emnlp-main.643/,Introduces the Collective-Critique and Self-Voting (CCSV) prompting method to systematically enhance demographic diversity in LLM outputs. The approach leverages LLMs' internal capacity for diversity reasoning and combines critique and self-voting mechanisms to iteratively improve output balance while maintaining model performance.,conferencePaper,,
142,6L6WSDC8,How Far Can We Extract Diverse Perspectives from Large Language Models?,"Hayati, S.; de Masson d'Autume, C.; Naradowsky, J.",2024,,https://aclanthology.org/2024.emnlp-main.306.pdf,"Systematically evaluates prompting strategies to extract diverse perspectives from LLMs and mitigate dominant group bias in outputs. Measuring subjective tasks such as argumentation and hate speech labeling, the study finds that diversity prompting increases perspective variety and reduces monocultural output tendencies.",conferencePaper,,
143,3AHQEHDF,Multilingual Prompting for Improving LLM Generation Diversity,"Wang, Q.; Pan, S.; Linzen, T.; Black, E.",2024,,https://arxiv.org/html/2505.15229v1,This paper introduces multilingual and multicultural prompting as methods to enhance the demographic and cultural diversity of Large Language Model outputs. The authors demonstrate these approaches outperform established diversity methods across multiple LLM architectures. Results indicate that prompting in culturally and linguistically aligned languages reduces hallucinated outputs and supports more representative generation.,report,,
144,PH7JBBC8,A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization,"Bisconti, P.; Aquilino, L.; Marchetti, A.; Nardi, D.",2024,,https://philarchive.org/archive/BISAFA,"Develops formal conceptualization of AI trustworthiness connecting intrinsic and perceived trustworthiness in operationalizable schema. Argues that trustworthiness extends beyond inherent AI system capabilities to include significant influences from observer perceptions such as perceived transparency, agency locus, and human oversight. Identifies three central perceived characteristics: transparency (access to information about functionality), agency locus (perception of action autonomy source), and human oversight (presence of human control). Mathematical formalization defines overall trustworthiness as function of discrepancy between intrinsic and perceived trustworthiness.",journalArticle,,
145,7AMRV4DT,Mitigating trust-induced inappropriate reliance on AI assistance,"Srinivasan, T.; Thomason, J.",2025,,https://arxiv.org/pdf/2502.13321.pdf,"Investigates trust-adaptive interventions to reduce inappropriate reliance on AI recommendations in decision-support tasks. Argues that AI assistants should adapt behavior based on user trust to mitigate both under- and over-trust. In two decision scenarios shows that providing supportive explanations for low trust and counter-explanations for high trust leads to 38% reduction in inappropriate reliance and 20% improvement in decision accuracy. Demonstrates that adaptive insertion of forced pauses to promote deliberation reduces over-trust, opening new paths for improved human-AI collaboration.",journalArticle,,
146,ZGM7K3H6,Artificial intelligence in social work: An EPIC model for practice,"Goldkind, L.; Hamama-Raz, Y.; Levitats, Z.; Levin, L.; Ben-Ezra, M.",2024,10.1080/0312407X.2025.2488345,https://doi.org/10.1080/0312407X.2025.2488345,"Develops EPIC model (Ethics and justice, Policy development and advocacy, Intersectoral collaboration, Community engagement and empowerment) as structured approach for AI integration in social work. Emphasizes that AI integration must align with social work mission and values through four overlapping components. Ethics and justice aspect addresses bias mitigation and transparent use, while policy development prioritizes client protection. Warns against excessive calibration of AI systems toward trustworthiness that could impair utility, emphasizing importance of transparency in both models and underlying technologies.",journalArticle,,
147,Y4BMCI2J,Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,"Asseri, B.; Abdelaziz, E.; Al-Wabil, A.",2024,10.48550/arXiv.2506.18199,https://doi.org/10.48550/arXiv.2506.18199,"Systematic review following PRISMA guidelines analyzing prompt-engineering techniques to reduce cultural bias against Arabs and Muslims in LLMs, evaluating eight empirical studies (2021-2024). Identifies five primary approaches: Cultural Prompting, Affective Priming, Self-Debiasing techniques, structured multi-stage pipelines, and parameter-optimized continuous prompts. Structured multi-stage pipelines most effective with up to 87.7% bias reduction. Emphasizes accessibility of prompt engineering for bias mitigation and importance of transparent methodology for trust building.",journalArticle,,
148,4PK8UN82,Recommendations for social work researchers and journal editors on the use of generative AI and large language models,"Victor, B. G.; Sokol, R. L.; Goldkind, L.; Perron, B. E.",2023,10.1086/726021,https://doi.org/10.1086/726021,"Develops ""disruptive-disrupting"" framework for analyzing generative AI in social work research with concrete recommendations for researchers and journal editors. Emphasizes transparency and accountability requiring researchers to fully document AI use, disclose prompts, and take responsibility for AI-generated content. Argues that continuous education about AI construction, limitations, and ethical considerations is essential for developing appropriate trust in LLM systems.",journalArticle,,
149,EXVG7MQR,Artificial intelligence in social work: Emerging ethical issues,"Reamer, F. G.",2023,10.55521/10-020-205,https://doi.org/10.55521/10-020-205,"Systematic analysis of ethical challenges in AI use within social work, developing comprehensive framework for ethical AI implementation. Identifies eight central ethical areas including informed consent, data privacy, transparency, misdiagnosis, client neglect, surveillance, scientific misconduct, and algorithmic bias. Emphasizes transparency as fundamental principle requiring social workers to inform clients about AI use. Develops eight-step protocol for ethical AI implementation including ethics committees and continuous peer-review processes.",journalArticle,,
150,BCBWSU3Z,Can LLMs reason about trust? A pilot study,"Debnath, A.; Cranefield, S.; Lorini, E.; Savarimuthu, B. T. R.",2024,,https://arxiv.org/html/2507.21075v1,"Pilot study investigating LLMs' ability to reason about trust between individuals by capturing complex mental states that exceed traditional symbolic approaches. Evaluates whether LLMs can analyze conversations and assess trust levels based on willingness, competence, and security factors. Demonstrates that LLMs can identify individual goals and develop improvement suggestions for trust building, as well as generate strategic action plans for trust promotion. Suggests LLMs have potential to bridge the gap between computational and socio-cognitive trust models.",journalArticle,,
151,G53MCF3W,"Bias, accuracy, and trust: Gender-diverse perspectives on large language models","Gaba, A.; Wall, E.; Babu, T. R.; Brun, Y.; Hall, K. W.; Xiong, C.",2025,,https://arxiv.org/abs/2506.21898,"Qualitative study with 25 interviews exploring how users of different gender identities perceive bias and trustworthiness in ChatGPT. Found perceived biases in LLM outputs undermine user trust, with non-binary participants encountering stereotyped responses that eroded confidence. Trust levels varied by group with male participants reporting higher trust. Participants recommended bias mitigation strategies including diversifying training data and implementing clarifying follow-up prompts to check biases.",journalArticle,,
152,8JFZMD5G,The influence of mental state attributions on trust in large language models,"Colombatto, C.; Birch, J.; Fleming, S. M.",2025,10.1038/s44271-025-00262-1,https://www.nature.com/articles/s44271-025-00262-1,"Empirical study examining how users' beliefs about LLM's ""mind"" affect trust in advice. Preregistered experiment (N=410) showing attributing intelligence to LLM strongly increased trust, whereas attributing human-like consciousness did not increase trust. Higher ascriptions of sentience correlated with less advice-taking. Participants trusted AI for perceived analytical competence, not for having feelings. Suggests prompt-engineering highlighting competence and reliability more effective than anthropomorphism for building appropriate trust.",journalArticle,,
153,ERTJZW5M,Trust in artificial intelligence–based clinical decision support systems among health care workers: Systematic review,"Tun, H. M.; Abdul Rahman, H.; Naing, L.; Malik, O. A.",2025,10.2196/69678,https://www.jmir.org/2025/1/e69678,"Systematic review synthesizing 27 studies on clinicians' trust in AI decision-support tools. Identifies eight key factors shaping professional trust, notably system transparency as primary enabler. Lack of transparency was recurring barrier with ""black-box"" algorithms undermining trust. Improving transparency, providing explainable recommendations, and addressing bias/fairness issues recommended to bolster trust. Concludes transparent, explainable AI with proper training and ethical safeguards crucial for practitioner trust.",journalArticle,,
154,5ERYSQCK,Measuring and identifying factors of individuals' trust in large language models,"De Duro, E. S.; Veltri, G. A.; Golino, H.; Stella, M.",2025,10.48550/arXiv.2502.21028,https://arxiv.org/html/2502.21028v1,"Study developing ""Trust-In-LLMs Index (TILLMI)"" psychometric scale to quantify trust in conversational AI. Survey of ~1,000 U.S. adults identified two trust dimensions: affective component (emotional closeness) and cognitive component (reliance on competence). Found significant individual differences with personality factors influencing trust. Prior LLM experience increased trust while those with no direct use were more skeptical. Highlights need for prompt-engineering and system design to account for user differences.",journalArticle,,
155,RXNXJA8W,What large language models know and what people think they know,"Steyvers, M.; Tejeda, H.; Kumar, A.; Belem, C.; Karny, S.; Smyth, P.",2025,10.1038/s42256-024-00976-7,https://www.nature.com/articles/s42256-024-00976-7,"High-impact study investigating mismatch between LLMs' confidence and users' trust. Found users often over-trust LLM answers, especially with explanations. Longer, detailed explanations increased user confidence even when answers weren't more accurate. Demonstrated that prompt engineering to convey uncertainty accurately helps users calibrate trust better. Aligning explanation with model's true confidence narrowed ""calibration gap,"" improving accuracy in judging when to trust AI.",journalArticle,,
156,JKF6VAQB,"AI algorithm transparency, pipelines for trust not prisms: mitigating general negative attitudes and enhancing trust toward AI","Park, K.; Yoon, H. Y.",2025,10.1057/s41599-025-05116-z,https://www.nature.com/articles/s41599-025-05116-z,"Peer-reviewed study examining how algorithmic transparency overcomes users' predisposed distrust in AI. 2×2 experiment showing transparency significantly increases trust, especially for those with initially negative AI attitudes. Making AI workings visible mitigated negative relationship between user AI-skepticism and organizational trust. Transparency acts as ""signal of trustworthiness,"" reducing skepticism and uncertainty. Emphasizing transparency in prompts and system design can reassure wary professionals by demonstrating accountability.",journalArticle,,
157,EJEFPZGA,"Navigating the Nexus of Trust: Prompt Engineering, Professional Judgment, and the Integration of Large Language Models in Social Work",,2025,,,"Comprehensive analysis examining how prompt engineering strategies designed to enhance transparency and mitigate bias influence trust that social work professionals place in LLM-generated case recommendations. Synthesizes literature from computer science, social work, ethics, and psychology to construct understanding of complex interplay between technology, human psychology, and professional practice. Develops framework for calibrated trust through responsible prompt engineering, positioning professionals as active directors rather than passive consumers of AI outputs. Proposes that trust emerges from three-way interaction between intentional prompting strategies, professional disposition and expertise, and perceived system trustworthiness across ability, benevolence, and integrity dimensions.",report,,
158,7W3RGSSG,Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,"Kamruzzaman, M.; Kim, G. L.",2024,10.48550/arXiv.2404.17218,https://doi.org/10.48550/arXiv.2404.17218,"Study evaluates 12 prompt-engineering strategies to reduce social biases in Large Language Models using dual-process theory. Findings show Chain-of-Thought prompting follows System 1 patterns rather than expected System 2 reasoning. Human Persona combined with System 2 prompting proved most effective, reducing stereotypical responses by up to 13%. Demonstrates that transparency in prompt design and systematic bias-mitigation approaches contribute significantly to building trust in LLM recommendations.",journalArticle,,
159,NFW58AU8,Transparency enhances positive perceptions of social artificial intelligence,"Xu, Y.; Bradford, N.; Garg, R.",2023,10.1155/2023/5550418,,"Experimental study with 914 participants testing how transparent explanations about chatbot workings affect user perceptions. Results show transparency modestly improved trust-related perceptions: users found chatbot ""less creepy,"" felt more affinity, and perceived it as more socially intelligent when transparent explanation provided. Transparency's impact on perceived intelligence stronger for users with low prior AI knowledge. Suggests prompt-engineering for greater transparency can foster user comfort and trust.",journalArticle,,
160,ED6C8LD2,Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals,"Choudhury, A.; Chaudhry, Z.",2024,10.2196/56764,https://www.jmir.org/2024/1/e56764/,"Peer-reviewed viewpoint discussing integration of LLMs into healthcare requiring balance between trust and skepticism. Warns that ""blind trust"" in LLM recommendations can lead to automation bias and confirmation bias as clinicians may accept AI outputs uncritically. Argues that prompting for transparency in LLM reasoning enhances professionals' trust by making AI reasoning visible and verifiable. Emphasizes need for human oversight and bias mitigation to ensure LLMs complement rather than compromise expert decision-making.",journalArticle,,
161,XJCEMM3D,A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization,"Bisconti, P.; Aquilino, L.; Marchetti, A.; Nardi, D.",2024,,https://philarchive.org/archive/BISAFA,"Develops formal conceptualization of AI trustworthiness connecting intrinsic and perceived trustworthiness in operationalizable schema. Argues that trustworthiness extends beyond inherent AI system capabilities to include significant influences from observer perceptions such as perceived transparency, agency locus, and human oversight. Identifies three central perceived characteristics: transparency (access to information about functionality), agency locus (perception of action autonomy source), and human oversight (presence of human control). Mathematical formalization defines overall trustworthiness as function of discrepancy between intrinsic and perceived trustworthiness.",journalArticle,,
162,RR5MJRBZ,Artificial intelligence in social work: An EPIC model for practice,"Goldkind, L.; Hamama-Raz, Y.; Levitats, Z.; Levin, L.; Ben-Ezra, M.",2024,10.1080/0312407X.2025.2488345,https://doi.org/10.1080/0312407X.2025.2488345,"Develops EPIC model (Ethics and justice, Policy development and advocacy, Intersectoral collaboration, Community engagement and empowerment) as structured approach for AI integration in social work. Emphasizes that AI integration must align with social work mission and values through four overlapping components. Ethics and justice aspect addresses bias mitigation and transparent use, while policy development prioritizes client protection. Warns against excessive calibration of AI systems toward trustworthiness that could impair utility, emphasizing importance of transparency in both models and underlying technologies.",journalArticle,,
163,3GB9B4IJ,Can LLMs reason about trust? A pilot study,"Debnath, A.; Cranefield, S.; Lorini, E.; Savarimuthu, B. T. R.",2024,,https://arxiv.org/html/2507.21075v1,"Pilot study investigating LLMs' ability to reason about trust between individuals by capturing complex mental states that exceed traditional symbolic approaches. Evaluates whether LLMs can analyze conversations and assess trust levels based on willingness, competence, and security factors. Demonstrates that LLMs can identify individual goals and develop improvement suggestions for trust building, as well as generate strategic action plans for trust promotion. Suggests LLMs have potential to bridge the gap between computational and socio-cognitive trust models.",journalArticle,,
164,25XSMXKT,Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,"Asseri, B.; Abdelaziz, E.; Al-Wabil, A.",2024,10.48550/arXiv.2506.18199,https://doi.org/10.48550/arXiv.2506.18199,"Systematic review following PRISMA guidelines analyzing prompt-engineering techniques to reduce cultural bias against Arabs and Muslims in LLMs, evaluating eight empirical studies (2021-2024). Identifies five primary approaches: Cultural Prompting, Affective Priming, Self-Debiasing techniques, structured multi-stage pipelines, and parameter-optimized continuous prompts. Structured multi-stage pipelines most effective with up to 87.7% bias reduction. Emphasizes accessibility of prompt engineering for bias mitigation and importance of transparent methodology for trust building.",journalArticle,,
165,9YYPYEGY,Recommendations for social work researchers and journal editors on the use of generative AI and large language models,"Victor, B. G.; Sokol, R. L.; Goldkind, L.; Perron, B. E.",2023,10.1086/726021,https://doi.org/10.1086/726021,"Develops ""disruptive-disrupting"" framework for analyzing generative AI in social work research with concrete recommendations for researchers and journal editors. Emphasizes transparency and accountability requiring researchers to fully document AI use, disclose prompts, and take responsibility for AI-generated content. Argues that continuous education about AI construction, limitations, and ethical considerations is essential for developing appropriate trust in LLM systems.",journalArticle,,
166,Q8YPNNKL,Mitigating trust-induced inappropriate reliance on AI assistance,"Srinivasan, T.; Thomason, J.",2025,,https://arxiv.org/pdf/2502.13321.pdf,"Investigates trust-adaptive interventions to reduce inappropriate reliance on AI recommendations in decision-support tasks. Argues that AI assistants should adapt behavior based on user trust to mitigate both under- and over-trust. In two decision scenarios shows that providing supportive explanations for low trust and counter-explanations for high trust leads to 38% reduction in inappropriate reliance and 20% improvement in decision accuracy. Demonstrates that adaptive insertion of forced pauses to promote deliberation reduces over-trust, opening new paths for improved human-AI collaboration.",journalArticle,,
167,IUN7Z56I,Artificial intelligence in social work: Emerging ethical issues,"Reamer, F. G.",2023,10.55521/10-020-205,https://doi.org/10.55521/10-020-205,"Systematic analysis of ethical challenges in AI use within social work, developing comprehensive framework for ethical AI implementation. Identifies eight central ethical areas including informed consent, data privacy, transparency, misdiagnosis, client neglect, surveillance, scientific misconduct, and algorithmic bias. Emphasizes transparency as fundamental principle requiring social workers to inform clients about AI use. Develops eight-step protocol for ethical AI implementation including ethics committees and continuous peer-review processes.",journalArticle,,
168,F7WNRWIC,Measuring and identifying factors of individuals' trust in large language models,"De Duro, E. S.; Veltri, G. A.; Golino, H.; Stella, M.",2025,10.48550/arXiv.2502.21028,https://arxiv.org/html/2502.21028v1,"Study developing ""Trust-In-LLMs Index (TILLMI)"" psychometric scale to quantify trust in conversational AI. Survey of ~1,000 U.S. adults identified two trust dimensions: affective component (emotional closeness) and cognitive component (reliance on competence). Found significant individual differences with personality factors influencing trust. Prior LLM experience increased trust while those with no direct use were more skeptical. Highlights need for prompt-engineering and system design to account for user differences.",journalArticle,,
169,GFHALQS2,Trust in artificial intelligence–based clinical decision support systems among health care workers: Systematic review,"Tun, H. M.; Abdul Rahman, H.; Naing, L.; Malik, O. A.",2025,10.2196/69678,https://www.jmir.org/2025/1/e69678,"Systematic review synthesizing 27 studies on clinicians' trust in AI decision-support tools. Identifies eight key factors shaping professional trust, notably system transparency as primary enabler. Lack of transparency was recurring barrier with ""black-box"" algorithms undermining trust. Improving transparency, providing explainable recommendations, and addressing bias/fairness issues recommended to bolster trust. Concludes transparent, explainable AI with proper training and ethical safeguards crucial for practitioner trust.",journalArticle,,
170,MUBZ8XJL,"Bias, accuracy, and trust: Gender-diverse perspectives on large language models","Gaba, A.; Wall, E.; Babu, T. R.; Brun, Y.; Hall, K. W.; Xiong, C.",2025,,https://arxiv.org/abs/2506.21898,"Qualitative study with 25 interviews exploring how users of different gender identities perceive bias and trustworthiness in ChatGPT. Found perceived biases in LLM outputs undermine user trust, with non-binary participants encountering stereotyped responses that eroded confidence. Trust levels varied by group with male participants reporting higher trust. Participants recommended bias mitigation strategies including diversifying training data and implementing clarifying follow-up prompts to check biases.",journalArticle,,
171,7IY7AX7D,The influence of mental state attributions on trust in large language models,"Colombatto, C.; Birch, J.; Fleming, S. M.",2025,10.1038/s44271-025-00262-1,https://www.nature.com/articles/s44271-025-00262-1,"Empirical study examining how users' beliefs about LLM's ""mind"" affect trust in advice. Preregistered experiment (N=410) showing attributing intelligence to LLM strongly increased trust, whereas attributing human-like consciousness did not increase trust. Higher ascriptions of sentience correlated with less advice-taking. Participants trusted AI for perceived analytical competence, not for having feelings. Suggests prompt-engineering highlighting competence and reliability more effective than anthropomorphism for building appropriate trust.",journalArticle,,
172,KRUQB7L2,What large language models know and what people think they know,"Steyvers, M.; Tejeda, H.; Kumar, A.; Belem, C.; Karny, S.; Smyth, P.",2025,10.1038/s42256-024-00976-7,https://www.nature.com/articles/s42256-024-00976-7,"High-impact study investigating mismatch between LLMs' confidence and users' trust. Found users often over-trust LLM answers, especially with explanations. Longer, detailed explanations increased user confidence even when answers weren't more accurate. Demonstrated that prompt engineering to convey uncertainty accurately helps users calibrate trust better. Aligning explanation with model's true confidence narrowed ""calibration gap,"" improving accuracy in judging when to trust AI.",journalArticle,,
173,WAYCKUZ8,Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals,"Choudhury, A.; Chaudhry, Z.",2024,10.2196/56764,https://www.jmir.org/2024/1/e56764/,"Peer-reviewed viewpoint discussing integration of LLMs into healthcare requiring balance between trust and skepticism. Warns that ""blind trust"" in LLM recommendations can lead to automation bias and confirmation bias as clinicians may accept AI outputs uncritically. Argues that prompting for transparency in LLM reasoning enhances professionals' trust by making AI reasoning visible and verifiable. Emphasizes need for human oversight and bias mitigation to ensure LLMs complement rather than compromise expert decision-making.",journalArticle,,
174,UIIDCXLB,Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,"Kamruzzaman, M.; Kim, G. L.",2024,10.48550/arXiv.2404.17218,https://doi.org/10.48550/arXiv.2404.17218,"Study evaluates 12 prompt-engineering strategies to reduce social biases in Large Language Models using dual-process theory. Findings show Chain-of-Thought prompting follows System 1 patterns rather than expected System 2 reasoning. Human Persona combined with System 2 prompting proved most effective, reducing stereotypical responses by up to 13%. Demonstrates that transparency in prompt design and systematic bias-mitigation approaches contribute significantly to building trust in LLM recommendations.",journalArticle,,
175,ZHQMHHPQ,"AI algorithm transparency, pipelines for trust not prisms: mitigating general negative attitudes and enhancing trust toward AI","Park, K.; Yoon, H. Y.",2025,10.1057/s41599-025-05116-z,https://www.nature.com/articles/s41599-025-05116-z,"Peer-reviewed study examining how algorithmic transparency overcomes users' predisposed distrust in AI. 2×2 experiment showing transparency significantly increases trust, especially for those with initially negative AI attitudes. Making AI workings visible mitigated negative relationship between user AI-skepticism and organizational trust. Transparency acts as ""signal of trustworthiness,"" reducing skepticism and uncertainty. Emphasizing transparency in prompts and system design can reassure wary professionals by demonstrating accountability.",journalArticle,,
176,NUYCHW2T,Transparency enhances positive perceptions of social artificial intelligence,"Xu, Y.; Bradford, N.; Garg, R.",2023,10.1155/2023/5550418,,"Experimental study with 914 participants testing how transparent explanations about chatbot workings affect user perceptions. Results show transparency modestly improved trust-related perceptions: users found chatbot ""less creepy,"" felt more affinity, and perceived it as more socially intelligent when transparent explanation provided. Transparency's impact on perceived intelligence stronger for users with low prior AI knowledge. Suggests prompt-engineering for greater transparency can foster user comfort and trust.",journalArticle,,
177,7AS5MAU9,Mitigating trust-induced inappropriate reliance on AI assistance,"Srinivasan, T.; Thomason, J.",2025,,https://arxiv.org/pdf/2502.13321.pdf,"Investigates trust-adaptive interventions to reduce inappropriate reliance on AI recommendations in decision-support tasks. Argues that AI assistants should adapt behavior based on user trust to mitigate both under- and over-trust. In two decision scenarios shows that providing supportive explanations for low trust and counter-explanations for high trust leads to 38% reduction in inappropriate reliance and 20% improvement in decision accuracy. Demonstrates that adaptive insertion of forced pauses to promote deliberation reduces over-trust, opening new paths for improved human-AI collaboration.",journalArticle,,
178,L48P8FBG,Artificial intelligence in social work: An EPIC model for practice,"Goldkind, L.; Hamama-Raz, Y.; Levitats, Z.; Levin, L.; Ben-Ezra, M.",2024,10.1080/0312407X.2025.2488345,https://doi.org/10.1080/0312407X.2025.2488345,"Develops EPIC model (Ethics and justice, Policy development and advocacy, Intersectoral collaboration, Community engagement and empowerment) as structured approach for AI integration in social work. Emphasizes that AI integration must align with social work mission and values through four overlapping components. Ethics and justice aspect addresses bias mitigation and transparent use, while policy development prioritizes client protection. Warns against excessive calibration of AI systems toward trustworthiness that could impair utility, emphasizing importance of transparency in both models and underlying technologies.",journalArticle,,
179,U9ACKGB4,A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization,"Bisconti, P.; Aquilino, L.; Marchetti, A.; Nardi, D.",2024,,https://philarchive.org/archive/BISAFA,"Develops formal conceptualization of AI trustworthiness connecting intrinsic and perceived trustworthiness in operationalizable schema. Argues that trustworthiness extends beyond inherent AI system capabilities to include significant influences from observer perceptions such as perceived transparency, agency locus, and human oversight. Identifies three central perceived characteristics: transparency (access to information about functionality), agency locus (perception of action autonomy source), and human oversight (presence of human control). Mathematical formalization defines overall trustworthiness as function of discrepancy between intrinsic and perceived trustworthiness.",journalArticle,,
180,2YS85B49,Can LLMs reason about trust? A pilot study,"Debnath, A.; Cranefield, S.; Lorini, E.; Savarimuthu, B. T. R.",2024,,https://arxiv.org/html/2507.21075v1,"Pilot study investigating LLMs' ability to reason about trust between individuals by capturing complex mental states that exceed traditional symbolic approaches. Evaluates whether LLMs can analyze conversations and assess trust levels based on willingness, competence, and security factors. Demonstrates that LLMs can identify individual goals and develop improvement suggestions for trust building, as well as generate strategic action plans for trust promotion. Suggests LLMs have potential to bridge the gap between computational and socio-cognitive trust models.",journalArticle,,
181,Z4YXX9PZ,Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,"Kamruzzaman, M.; Kim, G. L.",2024,10.48550/arXiv.2404.17218,https://doi.org/10.48550/arXiv.2404.17218,"Study evaluates 12 prompt-engineering strategies to reduce social biases in Large Language Models using dual-process theory. Findings show Chain-of-Thought prompting follows System 1 patterns rather than expected System 2 reasoning. Human Persona combined with System 2 prompting proved most effective, reducing stereotypical responses by up to 13%. Demonstrates that transparency in prompt design and systematic bias-mitigation approaches contribute significantly to building trust in LLM recommendations.",journalArticle,,
182,GUMWKBN6,Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,"Asseri, B.; Abdelaziz, E.; Al-Wabil, A.",2024,10.48550/arXiv.2506.18199,https://doi.org/10.48550/arXiv.2506.18199,"Systematic review following PRISMA guidelines analyzing prompt-engineering techniques to reduce cultural bias against Arabs and Muslims in LLMs, evaluating eight empirical studies (2021-2024). Identifies five primary approaches: Cultural Prompting, Affective Priming, Self-Debiasing techniques, structured multi-stage pipelines, and parameter-optimized continuous prompts. Structured multi-stage pipelines most effective with up to 87.7% bias reduction. Emphasizes accessibility of prompt engineering for bias mitigation and importance of transparent methodology for trust building.",journalArticle,,
183,U3AJIXAJ,Recommendations for social work researchers and journal editors on the use of generative AI and large language models,"Victor, B. G.; Sokol, R. L.; Goldkind, L.; Perron, B. E.",2023,10.1086/726021,https://doi.org/10.1086/726021,"Develops ""disruptive-disrupting"" framework for analyzing generative AI in social work research with concrete recommendations for researchers and journal editors. Emphasizes transparency and accountability requiring researchers to fully document AI use, disclose prompts, and take responsibility for AI-generated content. Argues that continuous education about AI construction, limitations, and ethical considerations is essential for developing appropriate trust in LLM systems.",journalArticle,,
184,HN7KKNYV,Artificial intelligence in social work: Emerging ethical issues,"Reamer, F. G.",2023,10.55521/10-020-205,https://doi.org/10.55521/10-020-205,"Systematic analysis of ethical challenges in AI use within social work, developing comprehensive framework for ethical AI implementation. Identifies eight central ethical areas including informed consent, data privacy, transparency, misdiagnosis, client neglect, surveillance, scientific misconduct, and algorithmic bias. Emphasizes transparency as fundamental principle requiring social workers to inform clients about AI use. Develops eight-step protocol for ethical AI implementation including ethics committees and continuous peer-review processes.",journalArticle,,
185,UB9NK8KI,Scaling implicit bias analysis across transformer-based language models through embedding association test and prompt engineering,"Zayed, O.",2024,,https://www.mdpi.com/2076-3417/14/8/3483,,journalArticle,,
186,XG7RFFC7,Occupational employment statistics,,2023,,https://www.bls.gov/oes/,,journalArticle,,
187,JRG3B3LE,Homoglyph unlearning: A novel approach to bias mitigation,"Struppek, T.",2024,,https://arxiv.org/html/2406.05602v1,,report,,
188,JGZDWMN3,Bias in decision-making for AI's ethical dilemmas: A comparative study of ChatGPT and Claude,"Wu, Z.",2025,,https://arxiv.org/html/2501.10484v2,,report,,
189,NVZA58ML,A survey on fairness in large language models,"Wang, Z.",2024,,https://file.mixpaper.cn/paper_store/2023/5ddea1cd-c031-433f-a09b-14e7754f7826.pdf,,report,,
190,Z9DNTBFF,Worst of both worlds: A comparative analysis of error in language and vision-language models,"Srinivasan, V.; Bisk, Y.",2024,,https://arxiv.org/html/2405.20152v1,,report,,
191,W8DFWR9L,Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development,"Shin, J.; You, J.; Birhane, A.",2025,,https://pubsonline.informs.org/doi/10.1287/ijoc.2024.0645,,journalArticle,,
192,M7AGB7LI,Model explanations for gender and ethnicity bias mitigation in AI-generated narratives,"Salecha, A.; Srijith, P. K.",2025,,https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=7888&context=open_access_etds,,thesis,,
193,6MJYP7ZX,Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,"Prakash, A.; Lee, S.",2023,,https://www.researchgate.net/publication/392942981_Prompt_Engineering_Techniques_for_Mitigating_Cultural_Bias_Against_Arabs_and_Muslims_in_Large_Language_Models_A_Systematic_Review,,journalArticle,,
194,YUVR5YNQ,Self-debiasing large language models: Zero-shot recognition and reduction of stereotypes,"Parrish, A.; de-Arteaga, M.",2025,,https://aclanthology.org/2025.naacl-short.74.pdf,,conferencePaper,,
195,YPYQ2TCL,Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia,"Mei, K.; Li, J.; Zhao, J.",2023,,https://arxiv.org/pdf/2505.17045,,report,,
196,64DQYVVB,More or less wrong: A benchmark for directional bias in LLM comparative reasoning,"Liu, H.; Sferrazza, C.; Lupu, Y.",2025,,https://arxiv.org/html/2506.03923v1,,report,,
197,RAY6G2R7,SWIFTSAGE: A new dual-module framework for better action planning in complex interactive reasoning tasks,"Lin, S.; Chen, M.; Yih, W.",2024,,https://arxiv.org/html/2404.17218v3,,report,,
198,QY6P4RGQ,BBQ: A hand-built bias benchmark for question answering,"Parrish, A.; Chen, A.; Nangia, N.; Padmakumar, V.; Bowman, S. R.",2022,,,,conferencePaper,,
199,H3STST88,Large language models are zero-shot reasoners,"Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; Iwasawa, Y.",2022,,,,conferencePaper,,
200,RM833D5N,On the steerability of large language models,"He, Z.; Bhargava, P.; D’Amour, A.",2024,,https://aclanthology.org/2025.naacl-long.400.pdf,,conferencePaper,,
201,ACDF4FL9,Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,"Chisca, R.; Saltaformaggio, A.; Verma, G.",2024,,https://arxiv.org/html/2404.17218v3,,report,,
202,VEJBIZRR,Exploring complex mental health symptoms via classifying social media data with explainable LLMs,"Chen, K.; Lim, N.; Lee, C.; Guerzhoy, M.",2024,,https://www.arxivdaily.com/thread/62478,,report,,
203,QIQR449A,A sociolinguistic approach to stereotype assessment in large language models,"Klinge, A.; Kjeldsen, K.",2024,,https://facctconference.org/static/docs/facct2025-206archivalpdfs/facct2025-final1618-acmpaginated.pdf,,conferencePaper,,
204,55ZR64VU,Debiasing prompts for gender bias in large language models,"Kaneko, K.; Bollegala, D.",2024,,https://arxiv.org/html/2404.17218v3,,report,,
205,7IVS7X63,Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia,"Jiang, H.; Kim, B.; Lipton, Z. C.",2022,,https://arxiv.org/pdf/2505.17045,,report,,
206,ST9UCTJE,Counterfactual fairness in text classification through robustness,"Garg, S.; Perot, V.; Lim, T.; Lipton, Z. C.",2019,,,,conferencePaper,,
207,SQYLQFRU,Reasoning towards fairness: Mitigating bias in language models through reasoning-guided fine-tuning,"Furniturewala, A.; Zhang, A.; Chang, K.",2024,,https://powerdrill.ai/discover/summary-reasoning-towards-fairness-mitigating-bias-in-cm9af0g1h7sb507pn7f5qh32q,,report,,
208,KG8JRLRQ,Measuring and mitigating unintended bias in text data,"Dixon, L.; Li, J.; Sorensen, J.; Thain, N.; Vasserman, L.",2018,,,,conferencePaper,,
209,GCN42PAM,Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development,"Birru, G. T.; Shin, J.; You, J.; Birhane, A.",2024,,https://pubsonline.informs.org/doi/10.1287/ijoc.2024.0645,,journalArticle,,
210,CYZQ6XPK,Evaluating gender bias in large language models via chain-of-thought prompting,"Kaneko, M.; Bollegala, D.; Okazaki, N.; Baldwin, T.",2024,,https://arxiv.org/abs/2401.15585,"This study investigates if Chain-of-Thought (CoT) prompting reduces implicit gender bias in LLMs. Using a synthetic task of counting gendered words, the authors found that without step-by-step prompting, models made biased errors. CoT prompting, which forced the model to explicitly label each word's gender before counting, significantly reduced these mistakes. This suggests that guiding the model through an explicit reasoning process makes it rely more on logic than on stereotypes, thereby mitigating bias.",report,,
211,EHQBHVYV,Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review,"Asseri, B.; Abdelaziz, E.; Al-Wabil, A.",2025,,https://arxiv.org/html/2506.18199,"This systematic review of 8 studies (2021–2024) identifies five prompt engineering approaches to mitigate bias against Arabs and Muslims: self-debiasing, cultural context prompting, affective priming, structured multi-step pipelines, and continuous prompt tuning. Multi-step pipelines were most effective, reducing biased content by up to ~88%, while simpler methods like cultural prompts showed ~71–81% improvement. The review concludes that while prompt engineering can mitigate biases without retraining, deep-seated biases may persist, and fixes can be superficial.",report,,
212,22KJL3PC,The cultural stereotype and cultural bias of ChatGPT,"Yuan, H.; Che, Z.; Zhang, Y.; Li, S.; Yuan, X.; Huang, L.; Luo, S.",2025,10.1177/18344909251355673,https://www.researchgate.net/publication/393408216_The_cultural_stereotype_and_cultural_bias_of_ChatGPT,"This article examines cultural biases in ChatGPT-3.5 and GPT-4. Study 1 measures alignment with human cultural values. Study 2 finds clear cultural stereotypes in GPT-3.5 but fewer in GPT-4. Study 3 tests four diversity-sensitive prompts (emphasizing individuality, fairness, egalitarian futures, or multiculturalism). All four strategies eliminated cultural stereotypes in GPT-3.5's outputs. For GPT-4, bias mitigation was more nuanced, requiring task-specific prompts. This indicates that while various prompts can reduce stereotypes, newer models may need more targeted strategies.",journalArticle,,
213,RARE5UFC,Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes,"Kamruzzaman, M.; Kim, G. L.",2024,,https://arxiv.org/html/2404.17218v1,"This study evaluates 12 prompt strategies across five LLMs, finding that instructing a model to adopt a System 2 (deliberative) reasoning style and a ""human persona"" most effectively reduces stereotypes. Combining these two strategies yielded up to a 13% reduction in stereotypical responses. Contrary to prior assumptions, Chain-of-Thought (CoT) prompting alone was not as effective, showing bias levels similar to a default prompt. The results suggest that prompts encouraging careful, human-like reasoning are key for mitigating bias.",report,,
214,YMABYPKF,What’s in a name? Auditing large language models for race and gender bias,"Salinas, A.; Haim, A.; Nyarko, J.",2025,,https://arxiv.org/html/2402.14875v3,"This interdisciplinary audit of GPT-4 and other LLMs reveals systematic intersectional biases based on names signaling race and gender. Prompts with names suggesting a Black woman received less favorable advice compared to those with white male names. This disparity was robust across 42 prompt templates. The study found that adding quantitative anchors (facts, numbers) to the prompt largely eliminated this bias, whereas adding qualitative descriptive details had inconsistent effects and sometimes amplified stereotypes.",report,,
215,ZLMLP53P,Queer in AI: A case study in community-led participatory AI,"Cvoelcker, C.",2023,,https://cvoelcker.de/assets/pdf/paper_queer_in_ai.pdf,"Fallstudie zu Queer in AI, dokumentiert Schäden durch KI-Systeme an queeren Menschen und beschreibt community-geleitete Strategien für partizipative, faire KI.",conferencePaper,,
216,4LY3SA4E,Flexible intersectional stereotype extraction (FISE): Analyzing intersectional biases in large language models,"Charlesworth, T.",2024,,https://news.northwestern.edu/stories/2024/03/kellogg-study-suggests-that-some-intersectional-groups-are-more-represented-than-others-in-internet-text/,Studie entwickelt FISE-Methode zur Messung intersektionaler Repräsentationsverzerrungen. Zeigt massive Dominanz weißer Männer in Internettexten und Ableitung entsprechender LLM-Biases.,journalArticle,,
217,TSYJ3Y57,Women4Ethical AI: Global cooperation for gender-inclusive AI,,2024,,https://www.unesco.org/en/articles/unescos-women4ethical-ai-urges-global-cooperation-gender-inclusive-ai,"UNESCO-Initiative zur Förderung genderinklusiver KI-Entwicklung. Fokus auf globale Zusammenarbeit, Menschenrechtsprinzipien und Expertinnenbeteiligung in allen Phasen.",report,,
218,A776TPGG,The EU artificial intelligence act through a gender lens,,2025,,https://library.fes.de/pdf-files/bueros/bruessel/21887-20250304.pdf,Politikanalyse des EU AI Acts mit Fokus auf Geschlechtergerechtigkeit. Identifiziert Potenziale und Lücken im Gesetzestext und gibt konkrete Empfehlungen zur Umsetzung.,report,,
219,MTMU9UPJ,DR.GAP: Mitigating bias in large language models using gender-aware prompting with demonstration and reasoning,"Qiu, H.; Xu, Y.; Qiu, M.; Wang, W.",2025,,https://arxiv.org/html/2502.11603v1,"DR.GAP ist eine prompting-basierte Methode zur Bias-Reduktion in LLMs. Sie nutzt Beispielfälle und strukturierte Reasoning-Schritte, um gendergerechtere Antworten zu erzielen.",report,,
220,CSJS9JGH,"AI gender bias, disparities, and fairness: Does training data matter?","Latif, E.; Zhai, X.; Liu, L.",2023,,https://arxiv.org/html/2312.10833v2,"Empirische Analyse von Geschlechterbias in Bewertungssystemen mit BERT und GPT-3.5. Mixed-gender Trainingsdaten reduzierten Bias, aber verstärkten Unterschiede. Drei Bias-Metriken angewendet.",report,,
221,BR2LG8LD,How AI hype impacts the LGBTQ+ community,"Kumar, A.; Gartner, M.",2024,10.1007/s43681-024-00423-8,https://doi.org/10.1007/s43681-024-00423-8,"Die Studie analysiert, wie der Hype um KI heteronormative Annahmen verstärkt. Sie führt Fallstudien zur Gesichtserkennung, Content-Moderation und Geschlechtsklassifikation durch und zeigt auf, wie queere Identitäten algorithmisch marginalisiert werden.",journalArticle,,
222,5G2QTZYD,Feminist perspectives on AI: Ethical considerations in algorithmic decision-making,"Ahmed, U.",2024,,https://www.researchcorridor.org/index.php/jgsi/article/download/330/314,"Theoretische Arbeit zur feministischen KI-Ethik. Argumentiert, dass KI strukturell eingebettete Diskriminierung reproduziert und plädiert für partizipative, intersektionale Entwicklungspraxis.",journalArticle,,
223,6QPLNNQK,Bias against women and girls in large language models: A UNESCO study,,2024,,https://www.unesco.org/en/articles/generative-ai-unesco-study-reveals-alarming-evidence-regressive-gender-stereotypes,"Die Studie analysiert LLMs (z. B. GPT-3.5, Llama 2) und dokumentiert signifikante Stereotypisierungen gegen Frauen und queere Menschen in generierten Texten. Besondere Auffälligkeit bei Open-Source-Modellen.",report,,
224,WS4KQPWN,Intersectionality in artificial intelligence: Framing concerns and recommendations for action,"Ulnicane, I.",2024,10.17645/si.v12i1.7543,https://doi.org/10.17645/si.v12i1.7543,"Die Arbeit analysiert vier Berichte zur Intersektionalität in KI, zeigt wie mangelnde Diversität zu biased KI-Systemen führt, und dokumentiert Vorurteile in Robotern, Sprachassistenten und HR-Tools.",journalArticle,,
225,QD2CXE9I,Measuring gender and racial biases in large language models,"An, J.",2025,10.1093/pnasnexus/pgaf089,https://academic.oup.com/pnasnexus/article/4/3/pgaf089/8071848,"This study quantifies intersectional race–gender biases in LLMs, showing non-additive stereotype effects and supporting intersectionality theory. Results recommend layered prompt interventions to address these complex biases in narrative generation.",journalArticle,,
226,HMDFMBV3,Intersectional Stereotypes in Large Language Models: Dataset and Analysis,"Ma, W.; Chiang, B.; Wu, T.; Wang, L.; Vosoughi, S.",2023,10.18653/v1/2023.findings-emnlp.575,https://aclanthology.org/2023.findings-emnlp.575.pdf,"This EMNLP paper introduces a dataset for studying intersectional stereotypes and applies it to three LLMs. Results reveal emergent stereotypes not predictable from single-attribute analysis. Prompt engineering reduces but does not eliminate such patterns, highlighting persistent biases in generated narratives.",conferencePaper,,
227,5UAHQESQ,Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review,"Basseri, K.; Abdelaziz, E.; Al-Wabil, A.",2025,,https://arxiv.org/html/2506.18199v1,"This systematic review identifies five major prompt engineering strategies to reduce cultural and intersectional bias in LLMs. Structured multi-step pipelines were most effective but complex, while cultural prompting offered a practical balance. Results show varying mitigation success depending on stereotype type, and emphasize trade-offs between bias reduction and performance.",report,,
228,UDZLIJWX,MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models,"Yunusov, S.; Sidat, H.; Emami, A.",2024,,https://arxiv.org/html/2409.13935v1,"This empirical study introduces a corpus of 1,500 personalized short stories generated with LLMs, incorporating identity features like gender, ethnicity, and age. Human judges rated these stories higher in engagement, diversity, and personalness. Narrative personalization increased textual diversity without harming moral comprehension. However, biases persist, such as preferential engagement for certain identities. The paper illustrates both potential and limitations of diversity-sensitive prompting.",report,,
229,6BZ5353S,Multilingual Prompting for Improving LLM Generation Diversity,"Wang, Q.; Pan, S.; Linzen, T.; Black, E.",2025,,https://arxiv.org/html/2505.15229v1,"This study introduces multilingual prompting as a strategy to enhance narrative diversity in LLM outputs. By using prompts with diverse languages and cultural cues, models produced outputs with improved demographic and opinion diversity. Compared to temperature-based and persona prompting, multilingual prompting was more effective and reduced cultural hallucinations.",report,,
230,Z9BIKVS3,Avoiding Catastrophe Through Intersectionality in Global AI Governance,"Laine, S.; McCrory, M.",2025,,https://www.cigionline.org/publications/avoiding-catastrophe-through-intersectionality-in-global-ai-governance/,"Dieses Working Paper nutzt einen feministischen Policy-Analyse-Rahmen, der auf fünf thematischen Bereichen basiert: Intersektionalität, Kontext, Neutralität, Macht und Gerechtigkeit. Die Forschung schlägt einen feministischen KI-Policy-Rahmen vor, der Entscheidungsträger und Stakeholder ermutigt, potenzielle KI-Sicherheitsprojekte in Übereinstimmung mit vier Zielen zu bewerten: Förderung der Intersektionalität, Bereitstellung diverser Kontexte, Bekämpfung der Neutralität und transformative Gerechtigkeit. Der Ansatz wächst aus Ansätzen zur feministischen KI-Governance, die die Notwendigkeit betonen, KI als Produkt struktureller Ungleichheiten zu sehen.",report,,
231,SSH3LVN6,Artificial Intelligence and Intersectionality,"Ulnicane, I.",2024,,https://ecpr.eu/news/news/details/749,"Diese Analyse untersucht, wie KI-Dokumente Bedenken über Bias und Ungleichheit in KI rahmen und Empfehlungen zur Bekämpfung formulieren. Mittels intersektionaler Linse wird die Interaktion multipler Identitäten (Geschlecht, Rasse, Klasse) hervorgehoben, die zu Marginalisierung und Diskriminierung bestimmter sozialer Gruppen führt. Die Studie unterscheidet zwischen technischen und sozio-technischen Framings von KI-Bias und zeigt auf, dass technische Frames KI oft als objektiv und neutral darstellen, während sozio-technische Ansätze die sozialen, politischen und historischen Dimensionen von Bias anerkennen.",journalArticle,,
232,QZJL6KBZ,"Algorithms, artificial intelligence and discrimination","Lund, O.",2025,,https://ldo.no/content/uploads/2025/05/Algorithms-artificial-intelligence-and-discrimination-report.pdf,"Dieser norwegische Regierungsbericht überprüft Schlüsselelemente des norwegischen Gleichstellungs- und Antidiskriminierungsgesetzes mit primärem Fokus auf algorithmische Diskriminierung. Der Bericht diskutiert die mögliche Einführung spezifischer Definitionen direkter und indirekter algorithmischer Diskriminierung und schlägt die Schaffung einer spezifischen Bestimmung zu rechtmäßiger algorithmischer Differenzialbehandlung vor. Die Komplexität algorithmischer Systeme erschwert die Unterscheidung zwischen direkter und indirekter Diskriminierung, was neue rechtliche Ansätze erfordert.",report,,
233,UBYTNGNV,"The AI Act, gender equality and non-discrimination: what role for the AI Office?","Giannoni Adielsson, M.; Öberg, J.",2024,10.1007/s12027-024-00785-w,https://doi.org/10.1007/s12027-024-00785-w,"Diese Analyse bewertet, ob der EU AI Act Fragen der Geschlechtergerechtigkeit und Nichtdiskriminierung ausreichend adressiert. Die substantiellen Bestimmungen des AI Acts werden durch die Linse von Gleichstellungs- und Antidiskriminierungsrecht analysiert, wobei vorgeschlagene Tools wie grundrechtliche Folgenabschätzungen und Bias-Audits zur Reduzierung von Geschlechterverzerrungen und Diskriminierungsrisiken hervorgehoben werden. Die Rolle des AI Office und seine Kooperation mit nationalen, europäischen und internationalen Stellen zur Durchsetzung der Geschlechtergerechtigkeit wird diskutiert.",journalArticle,,
234,RZ4QFXQI,"Discriminating Systems: Gender, Race, and Power in AI","West, S.M.; Whittaker, M.; Crawford, K.",2023,,https://ainowinstitute.org/wp-content/uploads/2023/04/discriminatingsystems.pdf,"Diese einflussreiche Studie argumentiert, dass die Diversitätskrise im KI-Sektor und Bias in KI-Systemen zwei Manifestationen desselben Problems sind und gemeinsam angegangen werden müssen. Die Autoren zeigen, dass rein technische Ansätze zur Bias-Behebung unzureichend sind, da sie die systemischen Machtverhältnisse ignorieren, die sowohl Arbeitsplätze als auch Technologien formen. Das ""Pipeline-Problem""-Narrativ wird als zu eng kritisiert, da es tieferliegende Probleme mit Arbeitsplatzkultur, Machtasymmetrien und struktureller Diskriminierung nicht adressiert. Die Studie fordert eine Verschiebung von technischer ""Debiasing"" zu breiterer sozialer Analyse.",report,,
235,K2KL8WH8,Revisiting Technical Bias Mitigation Strategies,"Djiberou Mahamadou, A. J.",2024,,https://arxiv.org/abs/2410.17433,"Diese systematische Überprüfung identifiziert praktische Limitationen technischer Bias-Mitigation-Strategien im Gesundheitswesen entlang fünf Schlüsseldimensionen: wer Bias und Fairness definiert, welche Mitigation-Strategie zu verwenden und zu priorisieren ist, wann in den KI-Entwicklungsstadien die Lösungen am effektivsten sind, für welche Populationen und in welchem Kontext die Lösungen entworfen sind. Die Studie zeigt mathematische Inkonsistenzen und Inkompatibilitäten zwischen verschiedenen Fairness-Metriken auf und diskutiert, wie werteorientierte KI stakeholder-bezogene Ansätze zur Bias-Mitigation ermöglichen kann.",report,,
236,DWS4KXBW,Towards Substantive Equality in Artificial Intelligence: Transformative AI Policy for Gender Equality and Diversity,"Ovalle, A.",2024,,https://datapopalliance.org/publications/towards-real-diversity-and-gender-equality-in-ai/,"Dieser GPAI-Bericht, basierend auf Konsultationen mit über 200 Teilnehmern aus mehr als 50 Ländern, entwickelt einen menschenrechtsbasierten Rahmen für substantielle Gleichberechtigung in der KI. Der Bericht betont, dass KI ohne Intervention das Risiko birgt, gesellschaftliche Verzerrungen zu perpetuieren und zu verstärken, insbesondere gegen historisch marginalisierte Gruppen. Die Empfehlungen zielen darauf ab, die strukturellen Wurzeln der Ungleichheit zu bekämpfen und transformative Veränderungen zu fördern, die substantielle Gleichberechtigung in der KI erreichen. Solche Politiken verbessern die Effektivität, Fairness und Nutzbarkeit von KI-Systemen.",report,,
237,K6JQ7SVA,Intersectional Fairness: A Fractal Approach,"Zannone, S.",2023,,https://arxiv.org/abs/2302.12683,"Diese Studie rahmt intersektionale Fairness in einem geometrischen Setting und projiziert Daten auf einen Hyperkubus. Die Autoren beweisen mathematisch, dass Fairness ""nach oben"" propagiert - die Sicherstellung von Fairness für alle Untergruppen auf der niedrigsten intersektionalen Ebene führt notwendigerweise zu Fairness auf allen höheren Ebenen. Sie definieren eine Familie von Metriken zur Erfassung intersektionaler Verzerrung und schlagen vor, Fairness als ""fraktales"" Problem zu betrachten, bei dem Muster auf der kleinsten Skala auf größeren Skalen wiederholt werden. Dieser Bottom-up-Ansatz führt zur natürlichen Entstehung fairer KI.",report,,
238,NBYNRKBL,What is Feminist AI?,"Wudel, A.; Ehrenberg, A.",2025,,https://library.fes.de/pdf-files/bueros/bruessel/21888-20250304.pdf,"Das Papier entwickelt einen Rahmen für Feminist AI (FAI), der intersektionale feministische Methodologie zur Adressierung von Bias und Ungleichheit in KI-Systemen nutzt. FAI betont interdisziplinäre Zusammenarbeit, systematische Machtanalyse und iterative Theorie-Praxis-Schleifen. Durch die Einbettung feministischer Werte (Gleichberechtigung, Freiheit, Gerechtigkeit) zielt FAI darauf ab, KI-Entwicklung zu transformieren und Inklusivität sowie soziale Nachhaltigkeit sicherzustellen. Praktische Anwendungen umfassen FemAI's Advocacy für feministische Perspektiven im EU AI Act und die MIRA-Diagnose-Plattform. FAI markiert eine kritische Abkehr von traditioneller KI durch die Bekämpfung struktureller Ungleichheiten.",report,,
239,XK6G84V7,Algorithmic discrimination: examining its types and regulatory measures with emphasis on US legal practices,"Wang, X.; Wu, Y. C.; Ji, X.; Fu, H.",2024,10.3389/frai.2024.1320277,https://doi.org/10.3389/frai.2024.1320277,"Diese umfassende Systematik identifiziert fünf primäre Typen algorithmischer Diskriminierung: Bias durch algorithmische Agenten, diskriminierende Merkmalsselektion, Proxy-Diskriminierung, disparate Auswirkungen und gezielte Werbung. Die Analyse der US-Rechtslandschaft offenbart einen mehrstufigen Regulierungsansatz aus prinzipieller Regulierung, präventiven Kontrollen, konsequenter Haftung und Selbstregulierung. Zentral ist die Erkenntnis, dass unbeabsichtigte Diskriminierung durch scheinbar neutrale Algorithmen besonders schwer zu erkennen und zu regulieren ist, da sie strukturelle historische Ungleichheiten perpetuiert. Die Studie betont die Notwendigkeit interdisziplinärer Forschung und proaktiver Politikentwicklung.",journalArticle,,
240,T83KNEQZ,Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness,"Ovalle, A.; Subramonian, A.; Gautam, V.; Gee, G.; Chang, K.-W.",2023,10.1145/3600211.3604705,https://doi.org/10.1145/3600211.3604705,"Diese kritische Literaturanalyse von 30 Arbeiten zur intersektionalen KI-Fairness deckt eine fundamentale Diskrepanz zwischen der Konzeptualisierung und Operationalisierung von Intersektionalität auf. Die Autoren zeigen, dass Forscher Intersektionalität überwiegend auf die Optimierung von Fairness-Metriken über demografische Untergruppen reduzieren, dabei aber die strukturellen Machtverhältnisse und den historischen Kontext vernachlässigen. Die Studie demonstriert, dass tiefgreifendes Engagement mit Intersektionalität eine systematische Analyse von Machtstrukturen über den gesamten KI-Pipeline hinweg erfordert, nicht nur technische Anpassungen. Arbeiten, die systemische (statt nur statistische) Quellen von Bias berücksichtigen und interdisziplinäre Synergien aufweisen, zeigen signifikant höhere Abdeckung intersektionaler Prinzipien.",journalArticle,,
241,TRAN2GJU,How can feminism inform AI governance in practice?,"Ricaurte, P.",2024,,https://www.unesco.org/en/articles/how-can-feminism-inform-ai-governance-practice,"Diese UNESCO-Publikation definiert feministische KI-Governance als einen aufkommenden Bereich von Politik, Forschung und Entwicklung, der darauf abzielt, KI-Systeme gerecht, gleichberechtigt und inklusiv zu gestalten. Feministische KI-Governance zielt darauf ab, Machtungleichgewichte im KI-Ökosystem zu adressieren und strukturelle Ungleichheiten, koloniale Vermächtnisse und multidimensionale Schäden zu berücksichtigen, die überproportional Gemeinschaften der globalen Mehrheit betreffen. Der Ansatz versteht Algorithmen als kontextuell und durch soziale Beziehungen geformt, nicht als rein mathematische Entitäten.",report,,
242,Q4LK53XW,"Are ""Intersectionally Fair"" AI Algorithms Really Fair to Women of Color? A Philosophical Analysis","Kong, Y.",2022,10.1145/3531146.3533074,https://doi.org/10.1145/3531146.3533074,"Diese philosophische Analyse identifiziert drei fundamentale Probleme mit der dominanten Interpretation intersektionaler Fairness in der KI: Die Fokussierung auf Identitätskategorien statt Unterdrückungsstrukturen, ein Dilemma zwischen statistischer Parität und substanzieller Gerechtigkeit, sowie die Vernachlässigung historischer Kontexte. Kong argumentiert, dass echte intersektionale Fairness über statistische Metriken hinausgehen und strukturelle Unterdrückungssysteme (Rassismus, Sexismus) direkt adressieren muss. Die Arbeit fordert eine Verschiebung von rein technischen zu kritisch-theoretischen Ansätzen.",conferencePaper,,
243,XIYX5HJS,Towards substantive equality in artificial intelligence: Transformative AI policy for gender equality and diversity,"Ricaurte Quijano, P.; Prud’homme, B.",2024,,https://wp.oecd.ai/app/uploads/2025/05/towards-substantive-equality-in-artificial-intelligence_Transformative-AI-policy-for-gender-equality-and-diversity.pdf,"This extensive report – developed through the Global Partnership on AI (GPAI) with contributors from academia and policy – sets out a vision for “substantive equality” in AI as opposed to mere formal equality. It recognizes that AI systems can replicate and even amplify societal power imbalances (“algorithmic discrimination”), thus requiring proactive governance to ensure historically marginalized groups are not left behind in the AI era. The report argues that purely technical bias mitigation is insufficient; instead, transformative policies should intervene at multiple points in the AI lifecycle to address structural inequities. Key principles advocated include: anchoring AI development in human rights and intersectional gender analysis, improving inclusive representation in data and AI design, and imposing stronger transparency and accountability obligations on AI systems to prevent harm. It provides practical policy recommendations (e.g., mandating diverse datasets and impact assessments, establishing a “right to information” about AI algorithms, and supporting community-led AI initiatives) to achieve de facto equality of outcomes – meaning AI should actively help reduce societal inequalities rather than reinforce them. Overall, the report positions digital equity as an extension of social justice: ensuring not only fairness within AI outputs but also equitable access, participation, and empowerment in shaping AI technology.",report,,
244,3QUWCYVW,Assessing trustworthy AI: Technical and legal perspectives of fairness in AI,"Kattnig, M.; Angerschmid, A.; Reichel, T.; Kern, R.",2024,10.1016/j.clsr.2024.106053,https://graz.elsevierpure.com/ws/portalfiles/portal/89954869/1-s2.0-S0267364924001195-main.pdf,"Kattnig et al. examine the disconnect between existing AI fairness techniques and the requirements of non-discrimination law, highlighting that many algorithmic bias mitigation methods do not meet legal standards for equality. Focusing on the EU context (with particular attention to the forthcoming AI Act and established non-discrimination directives), the authors review state-of-the-art bias mitigation approaches – from pre-processing data fixes to in-processing algorithms – and evaluate them against legal concepts of fairness and equality. They discuss how ambiguous legal frameworks and the difficulty of defining “fairness” pose challenges: for instance, fairness has multiple interpretations (individual vs. group fairness, formal vs. substantive equality) and is understood differently across disciplines. The paper argues for an interdisciplinary legal methodology to complement technical solutions. In practice, this means moving beyond purely quantitative parity metrics and ensuring AI systems comply with human rights and equality principles (e.g. ensuring de facto non-discrimination for all data subjects). By contrasting algorithms with legal norms, the study underlines that trustworthy AI requires more than technical robustness – it demands alignment with justice and accountability frameworks.",journalArticle,,
245,5F7D9PEB,Policy advice and best practices on bias and fairness in AI,"Alvarez, J. M.; Bringas Colmenarejo, A.; Elobaid, A.; Fabbrizzi, S.; Fahimi, M.; Ferrara, A.; Ruggieri, S.",2024,10.1007/s10676-024-09746-w,https://link.springer.com/article/10.1007/s10676-024-09746-w,"This open-access paper provides a comprehensive overview of fairness in AI, bridging technical bias mitigation methods with legal and policy considerations. Alvarez et al. survey the state-of-the-art in fair AI techniques and review major policy initiatives and standards on algorithmic bias. A key contribution is the NoBIAS architecture introduced in the paper, which comprises a “Legal Layer” (focusing on EU non-discrimination law and human rights requirements) and a “Bias Management Layer” (covering bias understanding, mitigation, and accountability). The authors note that AI systems have produced real-world harms, including illegal discrimination against protected groups, and they highlight challenges such as intersectional discrimination that current EU law does not explicitly address. By organizing existing knowledge and best practices, the article guides researchers and practitioners in aligning technical solutions with ethical and legal norms – underscoring that managing AI bias requires not just algorithmic techniques but also adherence to equality principles and governance frameworks.",journalArticle,,
246,2SLISKSW,Avoiding catastrophe through intersectionality in global AI governance,"McCrory, L.",2024,,https://www.cigionline.org/documents/3375/DPH-paper-Laine_McCrory.pdf,"In this working paper, McCrory argues that prevailing global AI governance efforts (especially those focused on “AI safety” and existential risks) lack an adequate intersectional, feminist perspective, which is needed to avert not just hypothetical future catastrophes but ongoing injustices. The paper analyzes seven prominent international AI policy initiatives (e.g. global AI accords and principles) against criteria derived from feminist theory, such as intersectionality, context, and power dynamics. McCrory finds that these high-level policies often remain techno-centric: they invoke abstract risks or neutrality, but fail to engage with how AI harms are unevenly distributed along lines of gender, race, and class. For example, current AI “safety” pledges seldom consider the lived experiences of marginalized communities or the way existing structural inequalities are mirrored in AI systems. The author contends that treating AI governance as a purely technical, top-down process is misguided; instead, governance should include meaningful participation from under-represented groups and incorporate feminist insights about power and oppression. The paper’s recommendations call for centering intersectionality in AI policy: explicitly addressing how AI-related risks and harms intersect with social identity and historical injustices, and ensuring that any frameworks for AI risk management or ethics actively involve those who have been marginalized by past technological developments.",report,,
247,8U23BX2J,Intersectionality in artificial intelligence: Framing concerns and recommendations for action,"Ulnicane, I.",2024,10.17645/si.v12.7543,https://www.cogitatiopress.com/socialinclusion/article/download/7543/3744,"Ulnicane’s article investigates how intersectionality – the overlapping of gender, race, class and other social inequalities – is being addressed in debates about AI bias. Through an analysis of four high-profile reports on AI and discrimination, the study finds that AI is often incorrectly portrayed as neutral, whereas in reality it amplifies existing societal biases, leading to discriminatory outcomes. A core issue identified is the tech sector’s “diversity crisis”: a homogenous AI workforce (primarily white, male developers) embeds its biases into AI systems, creating a feedback loop of inequality. The reviewed reports frame this situation as urgent – noting that past superficial diversity initiatives have failed – and call for holistic, structural solutions. Rather than just adding token diversity, they emphasize changing organizational culture and power dynamics in AI development, and giving marginalized groups more influence in shaping AI. Ulnicane highlights a concern that discussions of intersectional fairness risk being siloed as special-interest issues “for women and minorities” instead of being treated as core to AI innovation. The article concludes with recommendations: centering intersectionality and equity in AI policy, ensuring that efforts to fix AI bias focus on underlying power structures and systemic change, not merely on technical tweaks or increased headcounts.",journalArticle,,
248,7L93JBLR,Gender in a stereo-(gender)typical EU AI law: A feminist reading of the AI Act,"Karagianni, A.",2025,10.1017/cfl.2025.12,https://www.cambridge.org/core/journals/cambridge-forum-on-ai-law-and-governance/article/gender-in-astereogendertypical-eu-ai-law-a-feminist-reading-of-the-ai-act/E9DEFC1E114CBE577D737EC616610921,"This peer-reviewed article offers a feminist critique of the European Union’s AI Act, arguing that the law’s current bias mitigation provisions only superficially address gendered risks and fail to confront deeply embedded structural biases. Using frameworks like hermeneutical injustice and feminist legal theory, Karagianni shows that AI systems can perpetuate historical power imbalances and systemic discrimination against women and marginalized groups. The AI Act’s pre-market and post-market measures are analyzed to reveal how they often reinforce, rather than challenge, algorithmic discrimination. The author concludes that effective AI governance must go beyond technical fixes, incorporating an intersectional perspective and substantive equality principles. She calls for feminist-informed revisions to the AI Act – emphasizing gender inclusivity, intersectionality, and accountability – to ensure AI regulation actively dismantles (instead of inadvertently upholding) existing power asymmetries.",journalArticle,,
249,NUVZI357,Feministische Netzpolitik und Künstliche Intelligenz in der politischen Bildung [Feminist network politics and artificial intelligence in political education],"Mosene, K.",2023,,https://www.politische-medienkompetenz.de/debatte/ki-und-intersektionalitaet/,"Examines intersections of feminist network politics and AI within political education, emphasizing intersectional feminist perspectives on digital technologies. Argues feminist network politics involves supporting AI researchers and activists working to eliminate bias in development and outcomes. Discusses how traditional gender roles are reinforced through AI systems and advocates for political education helping users understand how technologies function, emerged, which societal ideas they reflect, and where critical discourse is needed.",webpage,,
250,2565PPJW,Feminist reflections for the development of artificial intelligence,"Guerra, J.; Venturini, J.; Castrillón, A.; Porras Sepúlveda, M.J.",2023,,https://www.derechosdigitales.org/fair-2023-en/,"Comprehensive synthesis of Latin American women's conversations developing AI under feminist frameworks establishes methodological commitments for co-design with communities, gender perspective integration in data science projects, and strategies for women crowd workers. Key feminist AI principles include building diverse intersectional teams, establishing community collaboration agreements, choosing technology based on context rather than consumption, and protecting autonomy through strong anonymity criteria.",report,,
251,DA6T4Z5B,Feminist Data Set,"Sinders, C.",2017,,https://carolinesinders.com/feminist-data-set/,"Multi-year art-research project directly addresses critical prompting practices by interrogating every AI development step—data collection, labeling, training, algorithm selection, and chatbot design—through feminist and intersectional lenses. Conducts public workshops to collaboratively build feminist datasets. Represents concrete critical prompting practice through community-based data creation as protest against biased AI systems, demonstrating practical approaches to feminist prompting by creating alternative training data.",webpage,,
252,UFJ7ERFF,Data Feminism for AI,"D'Ignazio, C.; Klein, L.",2024,,https://arxiv.org/html/2405.01286v1,"Comprehensive feminist framework directly critiques individualized approaches to AI ethics, challenging the ""liberal framework of making algorithms unbiased and inclusive"" in favor of structural ""remediation"" addressing ""systemic and structural dimensions of discrimination."" Examines how AI research is captured by ""racial, gendered capitalism"" and proposes nine principles focusing on structural power analysis including examining power, challenging power, and making labor visible. Explicitly positions against technical solutions ignoring power structures.",report,,
253,BJZYGNEE,Shaping feminist artificial intelligence,"Toupin, S.",2024,10.1177/14614448221150776,https://journals.sagepub.com/doi/full/10.1177/14614448221150776,"Comprehensive typology establishes six ways feminism and AI intersect: model, design, policy, culture, discourse, and science. The ""design"" category most directly addresses prompting practices, exploring how feminist approaches emphasize participatory design, community involvement, and challenging masculine-coded AI systems. Traces feminist AI from 1990s theoretical foundations through contemporary projects, establishing frameworks for feminist human-AI interaction informing critical prompting practices.",journalArticle,,
254,N7J2ZRFP,Algorithmic Governance and the International Politics of Big Tech,"Srivastava, S.",2024,,https://www.cambridge.org/core/journals/perspectives-on-politics/article/algorithmic-governance-and-the-international-politics-of-big-tech/3C04908735A5F2EE8A70AFED647741FB,"Structural analysis examines how Big Tech corporations exercise ""entrepreneurial private authority"" through algorithmic governance systems challenging state sovereignty. Moves beyond individual-focused approaches to analyze how technology firms exercise instrumental, structural, and discursive power globally. Critiques approaches focusing on individual user agency or technical fixes, examining instead how ""Big Tech's algorithmic governance incentivizes 'information pollution'"" and creates systemic power imbalances.",journalArticle,,
255,HJ7BHX8J,Engineers on responsibility: feminist approaches to who's responsible for ethical AI,"Browne, J.; Drage, E.; McInerney, K.",2024,10.1007/s10676-023-09739-1,,"Through interviews with AI practitioners interpreted via feminist political thought, reimagines responsibility in AI development beyond individualized approaches. Critiques current AI responsibility frameworks focused on individual competency and technical solutions, proposing instead ""responsibility as the product of work cultures that enable tech workers to be responsive and answerable for their products."" Moves beyond ""individual competency approaches"" toward understanding responsibility as embedded in structural power relations and organizational cultures.",journalArticle,,
256,VFD9ENG6,"A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges","Gohar, U.; Cheng, L.",2023,,https://arxiv.org/abs/2305.06969,"Comprehensive survey provides the first taxonomy of intersectional fairness notions in machine learning, explicitly grounded in Crenshaw's legal intersectionality framework. Demonstrates how multiple sensitive attributes interact to create distinct algorithmic bias forms that traditional single-axis approaches cannot capture. Shows how intersectional identities amplify biases not present in constituent groups, revealing co-constitutive discrimination mechanisms. Systematically reviews mitigation techniques and identifies key challenges.",conferencePaper,,
257,THGC3PA2,Feministische KI – Künstliche Intelligenz für alle? [Feminist AI – Artificial intelligence for everyone?],"Kubes, T.",2024,,https://blogs.fu-berlin.de/toolbox/2024/04/08/feministische-ki-kuenstliche-intelligenz-fuer-alle/,"Innovative interdisciplinary seminar teaches students to critically analyze everyday AI applications from sociotechnical feminist perspectives across four domains: love, robots, work, and creativity. Students analyze AI within androcentric, Eurocentric, anthropocentric, and capitalist-patriarchal structures. Curriculum combines theoretical foundations with practical application through ""queerbot"" design workshops that reimagine AI beyond normative dichotomies, demonstrating concrete pedagogical approaches for implementing feminist AI literacy education.",webpage,,
258,MT54YHER,Thinking like a scientist: Can interactive simulations foster critical AI literacy?,"Zhao, Y.; Michal, A.; Thain, N.; Subramonyam, H.",2025,10.1007/978-3-031-98417-4_5,,"Empirical study with 605 participants demonstrates that interactive simulations enhance critical AI literacy by engaging learners in scientific thinking processes including hypothesis testing and direct observation of AI behavior. Reveals that critical AI literacy requires understanding of fairness, dataset representativeness, and bias mechanisms in language models beyond technical knowledge. Establishes that effective AI literacy education must move beyond static instruction toward experiential engagement that fosters deep conceptual understanding of power structures embedded in AI systems.",conferencePaper,,
259,T8R8RKX9,AI tools show biases in ranking job applicants' names according to perceived race and gender,"Wilson, K.; Caliskan, A.",2024,,https://ojs.aaai.org/index.php/AIES/article/view/31748,Large-scale empirical study using over 550 resumes and 3+ million comparisons reveals that intersectional patterns of bias in AI resume screening cannot be understood as additive combinations of single-axis discrimination. Discovered unique harm against Black men invisible when examining race or gender independently—Black male names were never preferred over white male names (0% selection rate). Demonstrates co-constitutive nature of multiple discrimination where intersection of Blackness and masculinity creates distinct exclusion patterns.,conferencePaper,,
260,X54V3JMF,Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,"Ulnicane, I.",2024,10.17645/si.7543,,"Applies Crenshaw's intersectionality theory to examine four high-profile AI policy reports, revealing how diversity crises in AI workforce create ""negative feedback loops"" where homogeneous development teams embed biases into systems. Demonstrates how multiple forms of discrimination co-constitute each other through voice assistants, robots, and hiring tools, showing that intersectional experiences cannot be reduced to single identity categories. Argues for moving beyond simple diversity initiatives toward addressing culture, power, and structural inequalities.",journalArticle,,
261,EN6GNKL3,AI literacy in teacher education: Empowering educators through critical co-discovery,"Dilek, M.; Baran, E.; Aleman, E.",2025,10.1177/00224871251325083,,"Implements critical co-discovery approaches within AI teacher education to move beyond technical automation toward critical pedagogical engagement. Through co-discovery activities, educators developed understanding of AI concepts, ethical considerations, and context-specific applications while co-constructing knowledge. Emphasizes that prolonged engagement with AI literacy integrated into teacher education programs enables educators to critically navigate AI systems and examine broader pedagogical and ethical implications, prioritizing critical examination of AI's power dynamics and social justice implications.",journalArticle,,
262,QUU8QDPQ,Keynote Summary: The New Jim Code: Reimagining the Default Settings of Technology & Society,"Benjamin, R.",2023,10.1145/3589139,https://dl.acm.org/doi/10.1145/3589139,"In this summary of her foundational work, Ruha Benjamin introduces the concept of the ""New Jim Code,"" which describes how new technologies, including AI, can reproduce and even deepen existing racial hierarchies and discrimination under a veneer of neutrality and progress. She argues that discrimination becomes embedded in the very architecture of these systems. This framework is crucial for understanding how various forms of discrimination are co-constituted in AI, not as accidental bugs, but as features of a system designed within a society that has not resolved its structural inequalities. The concept inherently critiques individual competence, showing how even well-intentioned developers can create discriminatory systems if the underlying societal ""default settings"" are not challenged.",journalArticle,,
263,PQC9G5EU,Feminism Confronts AI: The Gender Relations of Digitalisation,"Wajcman, J.; Young, E.",2023,,https://academic.oup.com/book/55103/chapter/423909956,"Wajcman and Young provide a feminist critique of AI, arguing that the technology is not neutral but deeply embedded in existing gendered power structures. They highlight the severe underrepresentation of women in AI development as a key source of bias, leading to the creation of systems that reflect and amplify a masculine worldview. The authors contend that simply adding more women to the field is insufficient. Instead, they call for a fundamental shift in the culture of technology production, challenging the technical-social dualism and integrating feminist perspectives into the very design of AI. This requires addressing the structural power asymmetries that shape technological development and moving beyond individualistic solutions.",bookSection,,
264,U75LP6SV,The Algorithmic Auditing Landscape: A Social Justice Approach,"Raji, I. D.; Buolamwini, J.",2024,10.1145/3630659.3630671,https://dl.acm.org/doi/10.1145/3630659.3630671,"Raji and Buolamwini, pioneers of algorithmic auditing, argue for an approach rooted in social justice. They critique audits that focus solely on technical metrics, advocating instead for methods that center the lived experiences of marginalized communities. This involves a multi-stakeholder process, transparency, and a focus on real-world harms. Such an audit practice inherently makes the co-constitution of discrimination visible by investigating not just the algorithm's output, but the entire socio-technical system in which it is embedded. The paper implicitly highlights the limits of individual competence (e.g., a single auditor's skill) by emphasizing the need for collective, community-involved processes to challenge structural power.",conferencePaper,,
265,JZ4P8V8S,Operationalizing positive-constructive pedagogy to artificial intelligence: the 3E model for scaffolding AI technology adoption,"Thwaites, A.; Maas, C.; Zhang, Y.",2024,10.3389/feduc.2024.1293235,https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2024.1293235/full,"This article proposes the ""3E model"" (Expose, Explore, Exploit) as a pedagogical framework for developing critical AI literacy. The model aims to move students beyond passive use of AI to a more critical engagement. The ""Expose"" phase involves revealing the underlying mechanisms and biases of AI systems. ""Explore"" encourages students to test AI boundaries and critically question its outputs, a practice akin to critical prompting. ""Exploit"" focuses on using AI for creative and novel purposes. While not explicitly feminist, the framework provides a practical method for developing the critical literacies needed to identify and question the co-constitution of discrimination in AI outputs, thereby making biases visible.",journalArticle,,
266,SJLJ2GHC,Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,"Ulnicane, I.",2024,10.17645/si.7543,https://www.cogitatiopress.com/socialinclusion/article/view/7543,"This article critically examines how intersectionality is understood and applied in European Union AI policy documents. Ulnicane argues that current policy approaches often reduce intersectionality to a mere ""multi-category"" perspective, focusing on adding more diversity variables without addressing the underlying structural power dynamics that create inequalities. The study reveals that while policies acknowledge bias, they fail to adequately address the systemic nature of discrimination co-constituted at the intersections of gender, race, and other identity markers. The author recommends moving beyond simplistic diversity initiatives towards a more holistic, systemic, and transformative approach to AI ethics and governance, which explicitly confronts structural power imbalances.",journalArticle,,
267,3XMBE43Z,Algorithmic Governance: Gender Bias in AI-Generated Policymaking?,"Voutyrakou, D. A.; Skordoulis, C.",2025,10.1007/s44230-025-00109-2,https://doi.org/10.1007/s44230-025-00109-2,"Examines whether gender-specific needs are reflected in AI-generated policies, demonstrating through GPT-4 and Copilot experiments that AI tends to overlook female-specific needs unless explicitly prompted. Highlights androcentric biases, advocating intersectionally-informed prompting to surface hidden biases but recognizing the limits of individual prompt-based solutions in addressing structural AI biases.",journalArticle,,
268,SHDNTZJZ,Shaping feminist artificial intelligence,"Toupin, S.",2024,10.1177/14614448221150776,https://doi.org/10.1177/14614448221150776,"Toupin reviews historical and conceptual developments of feminist AI, identifying six key forms—model, design, policy, culture, discourse, and science. She demonstrates how feminist initiatives historically and currently surface hidden biases and power dynamics in AI. Advocates for integrating feminist perspectives directly into AI discourse and development.",journalArticle,,
269,9YCFMPVT,Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,"Ulnicane, I.",2024,10.17645/si.v12i2.7543,https://doi.org/10.17645/si.v12i2.7543,"Ulnicane analyzes four high-profile reports on intersectionality in AI, identifying a “vicious cycle” of bias perpetuated by a homogeneous AI workforce. She argues previous diversity initiatives have largely failed and advocates a holistic approach to altering power structures and cultures within AI development. Intersectional perspectives must move from the periphery to challenge core AI agendas.",journalArticle,,
270,C485NKYA,AI Countergovernance: Lessons Learned from Canada and Paris,"Attard-Frost, B.; Brandusescu, A.; Widder, D. G.; Tessono, C.",2025,,https://techpolicy.press/ai-countergovernance-lessons-learned-from-canada-and-paris/,"Argues against superficial ""AI literacy"" programs, promoting instead grassroots critical AI literacies that engage directly with structural inequalities related to race, gender, and labor. Stresses collectiv",report,,
271,XZ6Z8A9C,Trustworthy AI and the Logics of Intersectional Resistance,"Knowles, B.; Fledderjohann, J.; Richards, J. T.; Varshney, K. R.",2023,10.1145/3593013.3593986,https://doi.org/10.1145/3593013.3593986,"Critically examines mainstream ""Trustworthy AI"" frameworks from an intersectional feminist perspective, arguing that traditional AI ethics often privilege dominant groups and fail marginalized communities. Suggests reframing trustworthy AI principles to incorporate stewardship, care, humility, and empowerment, addressing intersectional injustices through power-sharing and structural change.",conferencePaper,,
272,GJF776AY,Investigating AI systems: examining data and algorithmic bias through hermeneutic reverse engineering,"Shukla, N.",2025,,https://www.frontiersin.org/journals/communication/articles/10.3389/fcomm.2025.1380252/full,"This work presents hermeneutic reverse engineering as a framework for investigating bias in AI systems. The approach considers AI systems as boundary objects and analyzes cultural meanings and assumptions embedded in techno-cultural objects. The study proposes three research perspectives: (1) comparative exploration of algorithmic bias, (2) investigation of impacts on various social groups, and (3) participatory approaches to include users in AI design.",journalArticle,,
273,PC75954S,An empirical study of structural social and ethical challenges in AI,"Ghosal, A.",2024,,https://link.springer.com/article/10.1007/s00146-025-02207-y,"This empirical study examines how professionals (N=32) in AI development perceive structural ethical challenges such as injustices and inequalities. The research identifies three main themes: (1) barriers to responsibility in a changing ecosystem, (2) the need for holistic consideration of AI products and their harms, and (3) structural obstacles that prevent engineers from taking personal responsibility.",journalArticle,,
274,ZAU6P4BK,Shaping feminist artificial intelligence,"Toupin, S.",2024,,,"This study examines the historical and contemporary shaping of feminist AI (FAI) through a typology of six approaches: FAI as model, design, politics, culture, discourse, and science. Toupin analyzes how feminist perspectives are implemented in various areas of AI development and identifies both potentials and limitations of feminist approaches. The work shows that FAI is not only a technological concept but also a movement that aims to transform power relations in AI development and promote social justice.",journalArticle,,
275,Y8J3HI9J,Artificial Intelligence in a Structurally Unjust Society,"Lin, T.-A.; Chen, P.-H. C.",2022,,https://ojs.lib.uwo.ca/index.php/fpq/article/download/14191/12139/38443,"This study argues that AI bias represents a form of structural injustice that arises when AI systems interact with other social factors to exacerbate existing social inequalities. Using the example of AI in healthcare, the authors show that the goal of AI fairness is to strive for a more just social structure through appropriate development and use of AI systems. They argue that all actors involved in the unjust social structure bear shared responsibility to join collective actions for structural reform and offer practical recommendations for various social positions.",journalArticle,,
276,VVEEL68I,Training in Co-Creation as a Methodological Approach to Improve AI Fairness,"Slesinger, I.; Yalaz, E.; Rizouli, S.; Komninos, E.; Papadopoulos, S.; Gibin, M.",2024,,https://cris.unibo.it/retrieve/707c849f-3aeb-4ca3-8d1f-2817960064bd/societies-14-00259%20(1).pdf,"This study examines the integration of training components in co-creation processes with vulnerable and marginalized stakeholder groups as part of developing AI bias detection and mitigation tools. The research shows that training on AI definitions, terminology, and socio-technical impacts is necessary to enable non-technical stakeholders to clearly articulate their insights on AI fairness. The authors emphasize the importance of critical reflection on appropriate use of training in co-creation approaches and their design and implementation for a truly more inclusive approach to AI system design.",journalArticle,,
277,HLBXNWAZ,Intersectional analysis of visual generative AI: the case of stable diffusion,"Sharma, S.; Ovalle, A.; Subramonian, A.",2024,,https://link.springer.com/article/10.1007/s00146-025-02498-1,"This study conducts a critical intersectional analysis of Stable Diffusion, a widely used visual generative AI tool. The examination of 180 generated images shows how the system perpetuates prevailing power systems such as sexism, racism, heteronormativity, and ableism, assuming white, physically healthy, masculine-presenting individuals as the standard. The study identifies three levels of analysis: (1) the aesthetics of AI images (micro-level), (2) institutional contexts (meso-level), and (3) intersections between power systems (macro-level). The authors argue for a reparative, socially just approach to visual generative AI.",journalArticle,,
278,Y6SAPNT2,Fairness Beyond the Algorithmic Frame: Actionable Recommendations for an Intersectional Approach,"Vethman, S.; Smit, Q. T. S.; van Liebergen, N. M.; Veenman, C. J.",2025,,https://facctconference.org/static/docs/facct2025-206archivalpdfs/facct2025-final2348-acmpaginated.pdf,"This study develops actionable recommendations for AI experts to implement intersectional approaches beyond purely technical solutions. The authors criticize the reduction of intersectionality to algorithmic bias measurements between subgroups and develop a framework that emphasizes interdisciplinary teams, community participation, and analysis of power structures in social context. Their approach includes six core recommendations: responsible AI expert role, multidisciplinary team building, reflection on societal positioning, participatory community engagement, power and context analysis, and data-sensitive metrics.",conferencePaper,,
279,XXCDL3A3,Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness,"Ovalle, A.; Subramonian, A.; Gautam, V.; Gee, G.; Chang, K.-W.",2023,,https://www.lsv.uni-saarland.de/wp-content/uploads/2023/12/Ovalle-et-al.-2023-Factoring-the-Matrix-of-Domination-A-Critical-Rev.pdf,"This critical review examines how intersectionality is discussed in 30 works of AI fairness literature. The study shows that researchers predominantly reduce intersectionality to optimizing fairness metrics across demographic subgroups while neglecting social context and power structures. The authors develop a framework for re-conceptualizing intersectionality in AI fairness based on relationality, social power, and structural analysis. They argue that genuine intersectional approaches must consider the interweaving of various systems of oppression rather than treating them as separate, additive categories.",conferencePaper,,
280,QXDK8Z6I,"Artificial Intelligence and Structural Injustice: Foundations for Equity, Values, and Responsibility","Himmelreich, J.; Lim, D.",2022,,https://arxiv.org/abs/2205.02389,"This work develops a structural injustice approach to AI governance based on Iris Marion Young's theory of structural injustice. The authors argue that structural injustice is a powerful conceptual tool that enables researchers and practitioners to identify, articulate, and potentially even anticipate AI bias. The approach includes both an analytical component (structural explanations) and an evaluative component (justice theory) and provides methodological and normative foundations for diversity, equity, and inclusion values.",journalArticle,,
281,LT3D3ZQ2,Towards a Feminist Metaethics of AI,"Siapka, A.",2023,,https://arxiv.org/abs/2311.14700,"This work develops a research agenda for a feminist metaethics of AI. Unlike traditional metaethics that reflects on moral judgments non-normatively, feminist metaethics expands its scope to ask not only what ethics is, but also how our approach to it should be. The author argues that a feminist metaethics of AI should investigate four areas: (1) continuity between theory and action in AI ethics, (2) real-world impacts of AI ethics, (3) the role and profile of those involved in AI ethics, and (4) AI's impacts on power relations through methods that consider context, emotions, and narrative.",journalArticle,,
282,D2EGAVKZ,Data Feminism for AI,"Klein, L.; D'Ignazio, C.",2024,,https://facctconference.org/static/papers24/facct24-7.pdf,"This work extends the seven principles of Data Feminism to the AI context and introduces two additional principles on environmental impacts and consent. The authors argue that feminist perspectives are essential for understanding and combating the unequal, undemocratic, extractive, and exclusionary forces in AI research. Their intersectional feminist principles aim to: (1) identify unequal power relations in AI systems, (2) mitigate predictable harms proactively, and (3) inspire creative, collective approaches for a more just world.",conferencePaper,,
283,NHIZN4QJ,What is Feminist AI?,"Wudel, A.; Ehrenberg, A.",2025,,https://library.fes.de/pdf-files/bueros/bruessel/21888-20250304.pdf,"This publication examines Feminist Artificial Intelligence (FAI) as a framework that utilizes intersectional feminism to address biases and injustices in AI systems. FAI emphasizes interdisciplinary collaboration, systemic power analysis, and iterative theory-practice loops. By embedding feminist values (justice, freedom, and equity), FAI aims to transform AI development to ensure inclusivity and social sustainability. Practical applications include FemAI's advocacy for feminist perspectives in the EU AI Act and the MIRA diagnostic platform that aligns AI tools with social justice.",report,,
284,MIT8HTC6,PreciseDebias: An automatic prompt engineering approach for generative AI to mitigate image demographic biases,"Clemmer, C.; Ding, J.; Feng, Y.",2024,,https://openaccess.thecvf.com/content/WACV2024/html/Clemmer_PreciseDebias_An_Automatic_Prompt_Engineering_Approach_for_Generative_AI_WACV_2024_paper.html,"This paper presents a technical solution for reducing demographic bias in AI image generators through ""PreciseDebias,"" an automated approach that rewrites simple prompts into more complex, diversity-reflective prompts. The system analyzes model bias and strategically adds attributes like ethnicity, gender, or age to enforce fairer representation distributions, demonstrating that diversity-reflective prompting can be automated at the system level.",conferencePaper,,
285,YN9JAREI,Faires KI-Prompting – Ein Leitfaden für Unternehmen,"Gengler, E.; Kraus, A.; Bodrožić-Brnić, K.",2024,,https://www.digitalzentrum-zukunftskultur.de/wp-content/uploads/2024/05/Faires-KI-Prompting-Ein-Leitfaden-fuer-Unternehmen.pdf,"This practical guide argues that feminist AI competency results in conscious prompt design to actively reduce stereotypes and discrimination in AI-generated content. It presents the ""KI-FAIRNESS"" framework for diversity-reflective prompting and demonstrates how specific, context-rich prompts lead to fairer and more representative results by compensating for AI's ""blind spots"" through targeted instructions.",report,,
286,UJ7DXK8Y,NLPositionality: Characterizing design biases of datasets and models,"Santy, S.; O'Connor, A.; Shi, E.; Wang, A.; Dai, J.; Klein, D.",2023,,https://aclanthology.org/2023.acl-long.530/,"This study provides a theoretical and methodological framework for making bias visible in AI models and datasets. It introduces the concept of ""positionality"" - the assumption that developers' social, cultural, and political perspectives inevitably flow into AI artifacts. The authors develop a method to quantify design biases by analyzing which dialects, demographic groups, or social contexts are under- or over-represented in datasets.",conferencePaper,,
287,MJDTRLAI,Can prompt modifiers control bias? A comparative analysis of text-to-image generative models,"Shin, P. W.; Ahn, J. J.; Yin, W.; Sampson, J.; Narayanan, V.",2024,,https://arxiv.org/abs/2406.05602,"This preprint investigates whether explicit prompt modifiers can reduce societal biases in text-to-image generative AI models. Authors evaluated three models (Stable Diffusion, DALL·E 3, Adobe Firefly) comparing baseline versus bias-mitigating prompts. Analysis revealed notable biases across all models, with inconsistent effectiveness of prompt modifiers. While diversity-reflective prompting can expose hidden biases and sometimes nudge outputs towards inclusivity, it is not a comprehensive fix and must be combined with broader ethical AI development efforts.",report,,
288,6L85PRUW,Inclusive prompt engineering: A methodology for hacking biased AI image generation,"Skilton, R.; Cardinal, A.",2024,10.1145/3641237.3691655,https://www.researchgate.net/publication/385325948_Inclusive_Prompt_Engineering_A_Methodology_for_Hacking_Biased_AI_Image_Generation,"This conference paper introduces ""inclusive prompt engineering"" as a strategy to probe and mitigate biases in generative AI image systems. Authors developed methodology to systematically modify prompts and provide tools for generating more diverse outputs. User studies revealed that when participants encountered stereotypical outputs, they tried adding negative qualifiers but models often failed to obey these negations. Findings underscore need for improved prompt interfaces that actively promote inclusive representation.",conferencePaper,,
289,YLAKP7Z2,Intersectional analysis of visual generative AI: The case of Stable Diffusion,"Jääskeläinen, P.; Sharma, N. K.; Pallett, H.; Åsberg, C.",2025,10.1007/s00146-025-02207-y,https://link.springer.com/article/10.1007/s00146-025-02207-y,"This open-access paper provides a feminist intersectional critique of Stable Diffusion through qualitative visual analysis of 180 AI-generated images. Authors examined how power systems like racism, sexism, heteronormativity, ableism, colonialism, and capitalism are reflected in AI outputs. Found that default outputs frequently perpetuate harmful stereotypes and assume a ""white, able-bodied, masculine-presenting"" default subject position. Advocates for social justice-oriented approach to AI by acknowledging cultural-aesthetic biases and engaging in reparative strategies.",journalArticle,,
290,8WBUGXRR,Reflexive prompt engineering: A framework for responsible prompt engineering and AI interaction design,"Djeffal, C.",2025,10.1145/3715275.3732118,,"This paper proposes ""Reflexive Prompt Engineering"" as a comprehensive framework to embed ethical and inclusive principles into prompt crafting for generative AI. Framework consists of five components: prompt design, system selection, system configuration, performance evaluation, and prompt management, each considered from social responsibility perspective. Positions responsible prompt engineering as essential component of AI literacy, bridging gap between AI development and deployment by aligning AI behavior with human rights and diversity values.",conferencePaper,,
291,IDW7QSYG,Female perspectives on algorithmic bias: Implications for AI researchers and practitioners,"Fraile-Rojas, B.; De-Pablos-Heredero, C.; Méndez-Suárez, M.",2025,10.1108/MD-04-2024-0884,https://colab.ws/articles/10.1108%2Fmd-04-2024-0884,"This study uses NLP and machine learning to analyze 172,041 tweets from female users discussing gender inequality in AI. It identifies prominent themes including the future of AI technologies and women's active role in ensuring gender-balanced systems. Findings show that algorithmic bias directly affects women's experiences, prompting engagement in online discourse about injustices. Women lead constructive conversations and create entrepreneurial solutions when faced with bias, demonstrating how feminist digital literacies can make AI biases visible and push for their reduction.",journalArticle,,
292,AIZGTQKG,Gender bias in artificial intelligence: Empowering women through digital literacy,"Shah, S. S.",2025,10.70389/PJAI.1000088,https://premierscience.com/pjai-24-524/,"This narrative review examines how systemic gender biases are embedded in AI systems across domains (e.g. hiring, healthcare, finance) and explores digital literacy as a tool to combat these biases. Key findings indicate that biases arise from underrepresentation of women in AI development, biased training data, and algorithmic design choices. Digital literacy programs for women are highlighted as a promising intervention that raises critical awareness of AI bias, encourages women's participation in AI careers, and fosters women-led AI projects.",journalArticle,,
293,X7ZGW6CN,"Intersectional Artificial Intelligence Is Essential: Polyvocal, Multimodal, Experimental Methods to Save AI","Ciston, S.",2024,10.7559/CITARJ.V11I2.665,,"Arguments for applying intersectional strategies to AI at all levels - from data through design to implementation. Intersectionality, integrating institutional power analysis and queer-feminist plus critical race theories, can contribute to AI reconceptualization. Intersectional framework enables analysis of existing AI biases and uncovering alternative ethics from counter-narratives. Research presents intersectional strategies as polyvocal, multimodal, and experimental, with community-focused and artistic practices helping explore AI's intersectional possibilities. Practical examples include Data Nutrition Label for bias assessment in datasets and experimental projects like ""ladymouth,"" a chatbot explaining feminism.",journalArticle,,
294,CHJQ52DC,"AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?","Latif, E.; Zhai, X.; Liu, L.",2024,,https://arxiv.org/html/2312.10833v4,"Empirical study examining gender bias in large language models through analysis of over 6000 evaluated student responses from 70 male and 70 female participants. Research uses fine-tuned BERT models and GPT-3.5 to evaluate various training data configurations: gender-specific versus mixed datasets. Three evaluation metrics applied: Scoring Accuracy Difference for bias assessment, Mean Score Gaps (MSG) for gender disparities, and Equalized Odds (EO) for fairness measurement. Results show mixed-gender trained models produce significantly better results than gender-specific models with reduced MSG and fairer predictions.",journalArticle,,
295,AIGLDZ4C,How to Create Inclusive AI Images: A Guide to Bias-Free Prompting,,2025,,https://www.articulate.com/blog/how-to-create-inclusive-ai-images-a-guide-to-bias-free-prompting/,"Practical guide examining prompt engineering strategies for creating inclusive AI-generated images and avoiding biased default outputs. Analysis shows AI image generators often produce stereotypical representations (white, male, slim, young, physically able) due to training on unbalanced internet datasets. Presents concrete techniques for inclusive prompt engineering: specifying visible identity characteristics (age, race, gender, body size, visible disabilities), using diversity-promoting terms like ""multicultural"" and ""gender-diverse,"" and providing additional context to break stereotypical associations.",webpage,,
296,AFDLFCIL,Feminist Perspectives on AI: Ethical Considerations in Algorithmic Decision-Making,"Ahmed, U.",2024,,https://www.researchcorridor.org/index.php/jgsi/article/download/330/314,"Explores ethical implications of algorithmic decision-making from feminist perspective, examining data bias, discrimination in automated systems, and underrepresentation of women in AI development. Algorithmic biases disproportionately affect marginalized groups and reinforce societal inequalities in areas like hiring, healthcare, and law enforcement. Feminist ethical framework emphasizes transparency, fairness, and inclusivity while questioning patriarchal and corporate-driven narratives in AI research and policy. Analysis shows AI ethics must go beyond technical solutions to address systemic power imbalances and cultural biases in data.",journalArticle,,
297,FDR5APIU,Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,"Ulnicane, I.",2024,10.17645/si.v12.7543,,"Analyzes emerging intersectionality agenda in AI through examination of four high-level reports on this topic (2019-2021). Research shows how these documents frame problems and formulate recommendations for addressing inequalities. AI systems often amplify and exacerbate human biases and stereotypes, leading to discrimination and marginalization. Analysis reveals systematic problems including diversity crises in AI development where founders and employees mainly come from homogeneous groups of white men, and reinforcement of existing power relationships through AI systems.",journalArticle,,
298,SWB86AYC,Generative AI and the Future of Digital Literacy: Opportunities for Gender Inclusion,"Hartshorne, R.; Cohen, J.",2025,,,"Examines generative AI potential for promoting female inclusion in primary and secondary education while addressing inherent risks of reinforcing gender-specific biases. Qualitative research analyzes systemic effects of generative AI tools on teaching and learning through interviews and focus groups with students and teachers. Results show complex interactions between generative AI technologies and gender dynamics. Key factors for effective AI-supported learning environments include personalized learning experiences, bias-aware content generation, and teachers' role as mediators of AI interactions.",conferencePaper,,
299,GP4JDSI8,Data Feminism for AI,"Klein, L.; D'Ignazio, C.",2024,10.1145/3630106.3658543,,"Presents intersectional feminist principles for just, ethical, and sustainable AI research. Extends seven Data Feminism principles to AI contexts: examine power, challenge power, rethink binaries and hierarchies, elevate emotion and embodiment, consider context, embrace pluralism, and make work visible. Proposes two additional principles on environmental impacts and consent. Framework helps identify and mitigate predictable harms before releasing discriminatory systems. Practical applications include participatory ML design processes and analysis of online advertising systems.",conferencePaper,,
300,GPSB87RN,What is Feminist AI?,"Wudel, A.; Ehrenberg, A.",2025,,https://library.fes.de/pdf-files/bueros/bruessel/21888-20250304.pdf,"Defines Feminist AI (FAI) as framework using intersectional feminism to address bias and inequalities in AI systems. Emphasizes interdisciplinary collaboration, systemic power analysis, and iterative theory-practice loops. Embeds feminist values of equality, freedom, and justice to transform AI development. Includes practical applications like FemAI advocacy for feminist perspectives in EU AI Act and MIRA diagnostic platform aligning AI tools with social justice goals. Distinguishes FAI from traditional ""Responsible AI"" approaches through focus on structural power inequalities rather than individual ""bad actors.""",report,,
301,SHJQQTI6,A Survey on Intersectional Fairness in Machine Learning: Opportunities and Challenges,"Gohar, U.; Cheng, L.",2023,10.24963/ijcai.2023/742,,"Comprehensive survey examining intersectional fairness in machine learning systems beyond traditional binary fairness approaches. Presents taxonomy of intersectional fairness concepts showing how multiple sensitive attributes interact to create unique discrimination forms. Identifies challenges including data sparsity for intersectional groups and bias mitigation complexity. Demonstrates applications in NLP systems, ranking algorithms, and image recognition. Shows traditional fairness metrics fail for intersectional identities as Black women experience different discrimination than Black people or women separately.",conferencePaper,,
302,9Y7ZFGI5,Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy,"Shah, S. S.",2025,10.70389/PJAI.1000088,,"Narrative review examining interplay between gender bias in AI systems and digital literacy potential for empowering women in technology. Synthesizes research from 2010-2024 analyzing systematic gender biases in AI applications across recruitment, healthcare, and financial services. These biases stem from women's underrepresentation in AI development teams (only 22% globally), biased training data, and algorithmic design decisions. Digital literacy programs show promise as intervention fostering critical awareness of AI bias, encouraging women toward AI careers, and catalyzing growth of women-led AI projects.",journalArticle,,
303,9BDIJE9B,Artificial Intelligence and gender equality,,2024,,https://www.unwomen.org/en/articles/explainer/artificial-intelligence-and-gender-equality,"Comprehensive policy brief series analyzing how AI systems perpetuate gender inequalities while highlighting pathways for more equitable development. Documents that 44% of AI systems show gender bias, with 25% exhibiting both gender and racial bias, and provides evidence-based policy recommendations for addressing systematic underrepresentation and discriminatory outcomes.",report,,
304,V4GTLMED,Tech workers' perspectives on ethical issues in AI development: Foregrounding feminist approaches,"Browne, J.; Drage, E.; McInerney, K.",2024,10.1177/20539517231221780,https://doi.org/10.1177/20539517231221780,"Empirical study examining tech workers' understanding of ethical AI development through feminist lens, revealing critical gaps in current AI ethics approaches. Finds that the term ""bias"" creates confusion among tech workers, undermining AI ethics initiatives, and argues for moving beyond diversity narratives toward ""design justice"" that centers marginalized voices.",journalArticle,,
305,YMYHLMFS,Data feminism for AI,"Klein, L.; D'Ignazio, C.",2024,10.1145/3630106.3658543,https://doi.org/10.1145/3630106.3658543,"Extends the influential ""Data Feminism"" framework to AI research, presenting intersectional feminist principles for conducting equitable, ethical, and sustainable AI research. Rearticulates original seven data feminism principles specifically for AI contexts and introduces two new principles addressing environmental impact and consent, providing concrete methodological guidance.",conferencePaper,,
306,QM6L6XLZ,Incubating Feminist AI: Executive Summary 2021-2024,,2024,,https://aplusalliance.org/incubatingfeministai2024/,"Documents outcomes from $2 million CAD Feminist AI Research Network project across Latin America, Middle East, and Asia. Developed 12 feminist AI prototypes addressing gender-based violence, transit safety, bias detection in NLP systems, and judicial transparency, demonstrating practical applications of feminist AI principles through community-centered design and intersectional analysis capabilities.",report,,
307,ZNHUCA4B,Recommendation on the Ethics of Artificial Intelligence,,2021,,https://unesdoc.unesco.org/ark:/48223/pf0000380455,"First global standard on AI ethics adopted by 193 member states, establishing comprehensive policy frameworks addressing gender equality in AI development and deployment. Explicitly addresses gender stereotyping, discriminatory biases, and need for equitable participation across the AI lifecycle, with recent implementation including Women4Ethical AI platform and systematic bias studies.",report,,
308,SQ38TTWQ,Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation,"An, J.; Huang, D.; Lin, C.; Tai, M.",2025,10.1093/pnasnexus/pgaf089,https://doi.org/10.1093/pnasnexus/pgaf089,"Large-scale experimental study examining bias across five major LLMs using over 361,000 randomized resumes. Reveals complex intersectional patterns where LLMs favor female candidates but discriminate against Black male candidates, with bias effects translating to 1-3 percentage point differences in hiring probabilities, validating intersectionality theory through three key findings.",journalArticle,,
309,ZMW228P6,Factoring the matrix of domination: A critical review and reimagination of intersectionality in AI fairness,"Ovalle, A.; Subramonian, A.; Gautam, V.; Gee, G.; Chang, K. W.",2023,10.1145/3600211.3604705,https://dl.acm.org/doi/abs/10.1145/3600211.3604705,"Critical review examining how intersectionality is conceptualized and operationalized within AI fairness research, analyzing 30 papers from the field. Identifies significant gaps between intersectionality theory and technical applications, proposing six key tenets for properly applying intersectionality in AI research drawn from Patricia Hill Collins and Sirma Bilge's framework.",conferencePaper,,
310,XW8NHCIE,"Gender, race, and intersectional bias in AI resume screening via language model retrieval","Wilson, K.; Caliskan, A.",2024,10.1609/aies.v7i1.31748,https://doi.org/10.1609/aies.v7i1.31748,"Analyzes bias in large language models used for resume screening, examining over 550 job descriptions and 550 resumes across multiple demographic combinations. Reveals significant intersectional discrimination with Black male candidates facing most severe disadvantages, validating three key hypotheses of intersectionality theory through over 40,000 comparisons.",conferencePaper,,
311,ZLM537Z4,"Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligent Machines","Browne, J.; Cave, S.; Drage, E.; McInerney, K.",2023,,https://doi.org/10.1093/oso/9780192889898.001.0001,"First comprehensive collection bringing together leading feminist thinkers across disciplines to examine AI's societal impact. Features 21 chapters covering topics from techno-racial capitalism to AI's military applications, examining how feminist scholarship can hold the AI sector accountable for designing systems that further social justice through diverse feminist approaches.",book,,
312,JZN2I6J5,Prompting fairness: Learning prompts for debiasing large language models,"Chisca, A.-V.; Rad, A.-C.; Lemnaru, C.",2024,,https://aclanthology.org/2024.ltedi-1.6/,Introduces novel prompt-tuning method for reducing biases in encoder models like BERT and RoBERTa through training small sets of additional reusable token embeddings. Demonstrates state-of-the-art performance while maintaining minimal impact on language modeling capabilities through parameter-efficient approach applicable across different models and tasks.,conferencePaper,,
313,MS3CNU3S,Bias and fairness in large language models: A survey,"Gallegos, I. O.; Rossi, R. A.; Barrow, J.; Tanjim, M. M.; Kim, S.; Dernoncourt, F.; Yu, T.; Zhang, R.; Ahmed, N. K.",2024,10.1162/coli_a_00524,https://doi.org/10.1162/coli_a_00524,"Comprehensive survey consolidating notions of social bias and fairness in NLP, defining distinct facets of harm and introducing desiderata to operationalize fairness for LLMs. Proposes three taxonomies: metrics for bias evaluation, datasets for bias evaluation, and techniques for bias mitigation classified by intervention timing.",journalArticle,,
314,7G73H3KM,Generative AI and opportunities for feminist classroom assignments,"Small, S. F.",2023,,https://digitalcommons.calpoly.edu/feministpedagogy/vol3/iss5/10/,"Addresses integration of generative AI into feminist classroom assignments to develop reflexivity and feminist epistemology. Proposes intentional feminist approaches where students collaborate with AI to develop understanding of feminist knowledge production, transforming AI from educational threat into tool for critical learning about power and epistemology.",journalArticle,,
315,KNQYFQ6B,The power of prompts: Evaluating and mitigating gender bias in MT with LLMs,"Sant, A.; Escolano, C.; Mash, A.; De Luca Fornaciari, F.; Melero, M.",2024,10.18653/v1/2024.gebnlp-1.7,https://doi.org/10.18653/v1/2024.gebnlp-1.7,Examines gender bias in machine translation through LLMs using four widely-used test sets. Develops specific prompting engineering techniques that reduce gender bias by up to 12% on WinoMT evaluation dataset. Identifies optimal prompt structures incorporating explicit fairness instructions and context-aware guidelines.,conferencePaper,,
316,8BUHU5EP,Feminist reflections for the development of Artificial Intelligence,,2023,,https://www.derechosdigitales.org/fair-2023-en/,"Synthesizes conversations among Latin American women developing AI systems, providing methodological frameworks for feminist AI development based on four real-world projects. Emphasizes co-design methodologies, digital autonomy, data sovereignty, and intersectional approaches through community agreements and power-balancing strategies.",report,,
317,IGFYSIV8,Shaping feminist artificial intelligence,"Toupin, S.",2024,10.1177/14614448221150776,https://journals.sagepub.com/doi/full/10.1177/14614448221150776,"Comprehensive examination of feminist artificial intelligence through historical analysis and contemporary typology development. Provides detailed framework categorizing FAI as: model, design, policy, culture, discourse, and science. Traces FAI's evolution from foundational work to contemporary initiatives, analyzing tensions between commercialized approaches and community-oriented methods.",journalArticle,,
318,BDI6XU5A,Gender und KI-Anwendungen. Trägt KI zum Genderproblem oder zu seiner Lösung bei?,"Franken, Swetlana; Mauritz, Nina",,,https://www.fh-bielefeld.de/multimedia/Fachbereiche/Wirtschaft+und+Gesundheit/ Forschung/Denkfabrik+Digitalisierte+Arbeitswelt/geki_Abschlussbericht.pdf,,report,,
319,EQV4DNQR,Challenging systematic prejudices: an Investigation into Gender Bias in Large Language Models,"UNESCO, IRCAI",2024,,https://discovery.ucl.ac.uk/id/eprint/10188772/1/Unesco_results.pdf,"This study explores biases in three significant large language models (LLMs): OpenAI’s GPT-2
and ChatGPT, along with Meta’s Llama 2, highlighting their role in both advanced decision-making
systems and as user-facing conversational agents. Across multiple studies, the brief reveals how
biases emerge in the text generated by LLMs, through gendered word associations, positive or
negative regard for gendered subjects, or diversity in text generated by gender and culture.
The research uncovers persistent social biases within these state-of-the-art language models,
despite ongoing efforts to mitigate such issues. The findings underscore the critical need for
continuous research and policy intervention to address the biases that exacerbate as these
technologies are integrated across diverse societal and cultural landscapes. The emphasis on
GPT-2 and Llama 2 being open-source foundational models is particularly noteworthy, as their
widespread adoption underlines the urgent need for scalable, objective methods to assess and
correct biases, ensuring fairness in AI systems globally.",document,,
320,8NG4ZEWE,Intersektionalität,"Walgenbach, Katharina",2023,,https://wb-erwachsenenbildung.net/intersektionalitaet/,,bookSection,,
321,5T55I5Z7,ARTIFICIAL INTELLIGENCE and GENDER EQUALITY,UNESCO,2020,,unesdoc.unesco.org/in/rest/annotationSVC/DownloadWatermarkedAttachment/attach_import_ab07646d-c784-4a4e-96a1-3be7855b6f76?_=374174eng.pdf&to=54&from=1,"The present report builds on UNESCO’s previous
work on gender equality and AI and aims to continue
the conversation on this topic with a select group
of experts from key stakeholder groups. In March
2019, UNESCO published a groundbreaking report,
I’d Blush if I Could: closing gender divides in digital
skills through education, based on research funded
by the German Federal Ministry for Economic
Cooperation and Development. This report featured
recommendations on actions to overcome
global gender gaps in digital skills, with a special
examination of the impact of gender biases coded
into some of the most prevalent AI applications.",report,,
322,QUV5DQH3,When Good Algorithms Go Sexist: Why and How to Advance AI Gender Equity,"Smith, Genevieve; Rustagi, Ishita",2021,10.48558/A179-B138,https://ssir.org/articles/entry/when_good_algorithms_go_sexist_why_and_how_to_advance_ai_gender_equity,Seven actions social change leaders and machine learning developers can take to build gender-smart artificial intelligence for a more just world.,journalArticle,,
323,4KMMPA6A,Faires KIPrompting – Ein Leitfaden für Unternehmen. BSP Business and Law School – Hochschule für Management und Recht,"Gengler,, E.; Bodrožić-Brnić,, K.",2024,,https://www.businessschool-berlin.de/files/Business-School-Berlin/Publikation/Faires-KI-Prompting-Ein-Leitfaden-fuer-Unternehmen%202024.pdf,"Der vorliegende Leitfaden möchte Sie auf eine Reise durch die Welt der Generativen KI mitnehmen und Ihnen Werkzeuge an die Hand geben, um diese Technolo-
gien verantwortungsvoll und bewusst zu nutzen. Wir möchten Verständnis für die positive wie negative Wirkung von Generativer KI schaffen, zugleich aber auch den Weg für einen diversen und fairen Einsatz ebnen. Dieser Guide kann Ihr Kompass sein, um nicht nur zu navigieren, sondern die digitale Zukunft mitzugestalten",report,,
324,3ZNMTJ5B,feminist AI | ACADEMY,,,,https://www.feminist-ai.com/academy,"Get ready for transforming power! We enable organizations to create more equitable AI through education. We hold workshops, give training, and provide learning material to raise awareness, build knowledge, and ease your creation of equitable AI.",webpage,,
325,J5EF9W6M,AI & Intersectionality: A Toolkit For Fairness & Inclusion,,2024,,https://diversifair-project.eu/wp-content/uploads/2025/01/Copie-de-INDUSTRY-Educ-Kits-A4-V2.pdf,"Das DIVERSIFAIR-Toolkit ist eine praktische Ressource, die sich an politische Entscheidungsträger*innen, die Industrie und die Zivilgesellschaft richtet. Es zielt darauf ab, ein Bewusstsein für intersektionale Diskriminierung in KI-Systemen zu schaffen und konkrete Handlungsstrategien zur Risikominderung anzubieten. Das Toolkit betont die Notwendigkeit, über einzelne Diskriminierungsachsen (wie Geschlecht oder Herkunft) hinauszudenken und deren Verschränkungen zu analysieren. Es fördert eine KI-Kompetenz, die es Stakeholdern ermöglicht, KI-Systeme über ihren gesamten Lebenszyklus hinweg – von der Datensammlung über das Design bis zur Anwendung – auf intersektionale Risiken zu prüfen. Für das Prompting bedeutet dies, gezielt Szenarien zu entwerfen, die marginalisierte Identitäten an der Schnittstelle mehrerer Merkmale repräsentieren, um blinde Flecken und stereotype Assoziationen in KI-Modellen aufzudecken.",report,,
326,2EBHMYU4,Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action,"Ulnicane, Inga",2024,10.17645/si.7543,https://www.cogitatiopress.com/socialinclusion/article/view/7543,"Diese Studie analysiert die aufkommende Agenda zu Intersektionalität in AI durch Untersuchung von vier hochrangigen Berichten zu diesem Thema (2019-2021). Die Forschung zeigt, wie diese Dokumente Probleme rahmen und Empfehlungen zur Adressierung von Ungleichheiten formulieren. AI-Systeme verstärken und verschärfen oft menschliche Verzerrungen und Stereotypen, was zu Diskriminierung und Marginalisierung führt. Die Analyse deckt systematische Probleme auf: Diversitätskrisen in AI-Entwicklung, wo Gründer und Mitarbeiter hauptsächlich aus homogenen Gruppen weißer Männer stammen, sowie die Verstärkung bestehender Machtbeziehungen durch AI-Systeme. Die Studie betont die Notwendigkeit intersektionaler Ansätze, die multiple Ungleichheiten berücksichtigen, die aus der Interaktion verschiedener sozialer Identitäten entstehen. Empfehlungen umfassen partizipative AI-Design-Prinzipien und die Einbindung diverser Stakeholder in AI-Governance-Prozesse.",journalArticle,,
