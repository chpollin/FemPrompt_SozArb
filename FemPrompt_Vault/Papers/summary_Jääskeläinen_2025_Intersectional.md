---
title: summary_Jääskeläinen_2025_Intersectional
authors:
  - Unknown Author
year: 2024
type: research-paper
tags:
  - paper
  - feminist-ai
  - bias-research
date_added: 2025-10-31
date_modified: 2025-10-31
bias_types:
  - Discrimination
  - Intersectionality
  - Intersectional Visual
  - Intersectional Methods
  - Intersectional Examination
  - Intersectional Theory
mitigation_strategies:
  - Intersectional Methods
  - Intersectional Theory
  - Intersectional Visual
  - Intersectional Examination
---

# summary_Jääskeläinen_2025_Intersectional

## Key Concepts

### Bias Types
- [[Discrimination]]
- [[Intersectional Examination]]
- [[Intersectional Methods]]
- [[Intersectional Theory]]
- [[Intersectional Visual]]
- [[Intersectionality]]

### Mitigation Strategies
- [[Intersectional Examination]]
- [[Intersectional Methods]]
- [[Intersectional Theory]]
- [[Intersectional Visual]]

## Full Text

---
title: "Jääskeläinen 2025 Intersectional"
original_document: Jääskeläinen_2025_Intersectional.md
document_type: Empirical Study
research_domain: AI Ethics, AI Bias & Fairness, Generative AI
methodology: Qualitative, Case Study, Visual Analysis
keywords: Stable Diffusion, intersectionality, visual bias, algorithmic reparation, generative AI
mini_abstract: "This paper critically analyzes how Stable Diffusion perpetuates systemic inequalities through visual representation, demonstrating that generative AI is not culturally neutral but actively reproduces racism, sexism, and other forms of discrimination embedded in training data and institutional contexts."
target_audience: Researchers, Policymakers, Industry, Practitioners
key_contributions: "Intersectional visual analysis framework for generative AI bias assessment"
geographic_focus: Global
publication_year: 2025
related_fields: Feminist Science and Technology Studies, Visual Media Studies, Critical Theory
summary_date: 2025-10-31
language: English
ai_model: claude-haiku-4-5
---

# Summary: Jääskeläinen 2025 Intersectional

## Overview

This paper by Jääskeläinen, Sharma, Pallett, and Åsberg presents a critical intersectional examination of Stable Diffusion, a widely adopted open-source visual generative AI tool launched in August 2022. With 10 million monthly users generating approximately 2 million images daily (totaling 12 billion images to date), SD represents a significant technological phenomenon requiring urgent critical analysis. The research fundamentally challenges the prevailing assumption that generative AI technologies are culturally and aesthetically neutral tools, instead demonstrating that these systems actively perpetuate and amplify existing systemic inequalities. By analyzing 180 deliberately generated images across different social axes of privilege and disadvantage, the authors reveal how SD encodes hierarchies of privilege into its visual outputs, establishing a "default individual" characterized by whiteness, able-bodiedness, and masculine presentation.

## Main Findings

The analysis identifies systematic patterns of bias embedded within SD-generated imagery across multiple intersecting dimensions. Most significantly, the technology demonstrates a consistent default toward white, able-bodied, masculine-presenting individuals, suggesting these characteristics are algorithmically encoded as the "neutral" or unmarked human form. The research reveals that SD perpetuates multiple intersecting power systems including sexism, racism, heteronormativity, and ableism simultaneously rather than in isolation. Additionally, the imagery reflects pronounced Euro- and North America-centric cultural values, indicating that institutional contexts of development, training data composition, and design choices directly shape aesthetic outputs. The authors establish that power systems embedded in the technology result in continual reproduction of harmful and violent imagery against marginalized social groups. Critically, they demonstrate that these biases are traceable to specific institutional contexts rather than emerging accidentally, challenging claims of technological objectivity. The paper identifies that aesthetic choices, cultural representations, and embodied characteristics all systematically encode privilege hierarchies.

## Methodology/Approach

The study employs qualitative visual analysis as its primary methodology, systematically generating 180 images across deliberately chosen axes of privilege and disadvantage—including wealth/poverty distinctions, citizen/immigrant status, and other social categories. This interpretative approach moves beyond quantitative bias detection to engage deeply with aesthetic choices and cultural representations. The theoretical framework integrates three complementary disciplines: Feminist Science and Technology Studies (STS), visual media studies, and intersectional critical theory. Intersectionality operationalizes the analysis by examining how multiple power systems intersect within single visual representations rather than analyzing single dimensions of inequality in isolation. This interdisciplinary foundation enables the authors to trace how institutional contexts (training data, development teams, design decisions) materialize as specific aesthetic patterns in generated imagery.

## Relevant Concepts

**Intersectionality**: An analytical framework recognizing that individuals experience multiple, overlapping systems of oppression simultaneously, operationalized here to examine how racism, sexism, ableism, and heteronormativity intersect within visual representations.

**Default individual**: The unmarked, assumed-neutral subject encoded within algorithmic outputs—in SD's case, white, able-bodied, masculine-presenting—against which all other representations appear marked or deviant.

**Cultural-aesthetic neutrality myth**: The false assumption that technologies are objective tools without embedded values, politics, or cultural preferences in aesthetic representation.

**Algorithmic reparation**: A proposed framework for addressing harms perpetuated through algorithmic systems by acknowledging injustices, rendering visible embedded politics, and implementing restorative justice measures.

**Visual politics**: The ways power relations are encoded, reproduced, and naturalized through visual representation, aesthetic choices, and embodied characteristics in imagery.

**Institutional bias**: Systematic inequalities traceable to specific institutional contexts—training data composition, development team demographics, organizational values—rather than technical accidents.

## Significance

This research makes substantial contributions to critical AI studies by advancing intersectionality as a central analytical lens for understanding generative AI harms. It moves beyond technical audits to address aesthetic and representational injustices often overlooked in conventional AI ethics frameworks. The paper's proposal for reparative and social justice-oriented approaches to vGenAI development offers constructive intervention beyond critique, emphasizing the need to render visible cultural-aesthetic politics and implement restorative justice frameworks. By demonstrating how institutional contexts and training data embed social inequalities into algorithmic outputs, the work challenges techno-optimist narratives and contributes to emerging scholarship questioning AI neutrality. The methodological innovation of combining visual analysis with intersectional theory provides a model for future AI ethics research addressing representational harms. Importantly, the paper contextualizes SD within rapid market growth ($8.28 billion in 2022, projected $99.79 billion by 2030), emphasizing the urgency of addressing these harms at scale. The work establishes that vGenAI is not merely reflecting society's visual politics but actively perpetuating them through technological reproduction, requiring deliberate intervention in development practices, training data curation, and institutional decision-making.
