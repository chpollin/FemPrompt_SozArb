---
title: summary_J__skel_inen_2025_Intersectional
authors:
  - Unknown Author
year: 2024
type: research-paper
tags:
  - paper
  - feminist-ai
  - bias-research
date_added: 2025-10-31
date_modified: 2025-10-31
bias_types:
  - Intersectionality
mitigation_strategies:
  - Equitable Alternatives
  - Intersectionality
  - Equitable Ai
---

# summary_J__skel_inen_2025_Intersectional

## Key Concepts

### Bias Types
- [[Intersectionality]]

### Mitigation Strategies
- [[Equitable Ai]]
- [[Equitable Alternatives]]
- [[Intersectionality]]

## Full Text

---
title: "J skel inen 2025 Intersectional"
original_document: J__skel_inen_2025_Intersectional.md
document_type: Empirical Study
research_domain: AI Ethics, AI Bias & Fairness, Generative AI
methodology: Qualitative, Case Study, Visual Analysis
keywords: Stable Diffusion, intersectionality, visual bias, algorithmic reparation, generative AI
mini_abstract: "Critical intersectional analysis of Stable Diffusion revealing how visual generative AI perpetuates systemic inequalities through encoded biases in aesthetics, representation, and institutional contexts. Proposes reparative justice approaches to address harms."
target_audience: Researchers, Policymakers, Industry, Practitioners
key_contributions: "Intersectional framework for analyzing generative AI visual bias and harms"
geographic_focus: Global
publication_year: 2025
related_fields: Feminist Science and Technology Studies, Visual Media Studies, Decolonial Theory
summary_date: 2025-10-31
language: English
ai_model: claude-haiku-4-5
---

# Summary: J skel inen 2025 Intersectional

## Overview

This 2025 peer-reviewed paper by Jääskeläinen, Sharma, Pallett, and Åsberg (affiliated with KTH Royal Institute of Technology, University of East Anglia, and Linköping University) critically examines Stable Diffusion (SD), the dominant open-source visual generative AI tool since August 2022. The research directly challenges the pervasive assumption that generative AI operates as culturally and aesthetically neutral technology. Instead, the authors demonstrate that SD actively encodes, reflects, and perpetuates existing social hierarchies and power structures—specifically racism, colonialism, capitalism, sexism, heteronormativity, and ableism—through its visual outputs. With SD generating over 12 billion images (surpassing 150 years of human photography in 2-3 years) and attracting 10 million monthly users generating 2 million images daily, understanding embedded biases becomes critically urgent. The paper synthesizes intersectional theory, feminist technology studies, visual media analysis, and restorative justice frameworks to demonstrate how algorithmic systems reproduce systemic inequalities across multiple dimensions simultaneously.

## Main Findings

Analysis of 180 deliberately-constructed SD-generated images reveals systematic patterns of bias reproduction across interconnected power systems. The technology demonstrates a consistent "default subject" assumption: representations default to white, able-bodied, and masculine-presenting individuals, establishing normative hierarchies. SD perpetuates multiple overlapping power systems including sexism, racism, heteronormativity, and ableism simultaneously. Critically, the research documents how SD generates harmful and violent imagery targeting marginalized groups. The authors trace these biases directly to institutional contexts: development teams and training data sources exhibit pronounced geographic concentration in Euro- and North America, resulting in culturally-specific aesthetic values presented as universal. The paper fundamentally challenges industry narratives by demonstrating that vGenAI actively reproduces colonial, racist, and capitalist visual politics rather than reflecting pre-existing inequalities passively. The authors conclude that acknowledging and rendering visible these cultural-aesthetic politics is essential for developing equitable alternatives.

## Methodology/Approach

The research employs qualitative visual analysis examining 180 images deliberately generated along axes of social privilege and disadvantage (wealth/poverty, citizen/immigrant status, ability/disability). This interpretative approach prioritizes cultural meaning-making and symbolic representation over quantitative metrics. The theoretical framework integrates: (1) **intersectional critical theory** examining how multiple power systems interact compoundedly; (2) **feminist science and technology studies** questioning technological neutrality claims; (3) **visual media studies** providing analytical tools for aesthetic interpretation; and (4) **restorative justice frameworks** informing proposed solutions. This interdisciplinary methodology moves beyond technical auditing toward critical cultural analysis, recognizing that bias operates through aesthetic, symbolic, and institutional dimensions rather than isolated algorithmic parameters.

## Relevant Concepts

**Intersectionality**: Analytical framework examining how multiple social categories (race, gender, class, ability, citizenship) interact simultaneously to create compounded experiences of privilege and disadvantage, rather than operating as separate variables.

**Technological Neutrality Myth**: False assumption that tools operate objectively; the paper demonstrates technologies encode designer values, institutional contexts, and training data biases systematically.

**Algorithmic Reparation**: Proposed approach addressing harms through: acknowledging injustices enacted against social groups, rendering visible cultural-aesthetic politics, and implementing corrective mechanisms grounded in social justice.

**Restorative Justice**: Framework focused on symbolically and materially mending injustices rather than punitive responses; applied here to vGenAI development and deployment.

**Visual Politics**: Recognition that imagery carries ideological content reflecting and reinforcing power structures; aesthetics are never neutral but always encode values and hierarchies.

**Default Subject Construction**: The implicit assumption embedded in technology about what constitutes a "normal" or unmarked individual, revealing whose perspectives are centered and whose are marginalized.

## Significance

This work significantly advances critical AI scholarship by centering intersectionality and visual analysis rather than treating bias as isolated technical problems. The reparative justice framing offers constructive alternatives beyond critique, proposing accountability mechanisms grounded in social justice principles. The paper's emphasis on rendering visible cultural-aesthetic politics challenges the widespread industry claim that AI systems are neutral tools. As generative AI becomes increasingly embedded in creative industries, content moderation, hiring systems, and decision-making infrastructure, understanding its cultural-aesthetic politics becomes essential. The research provides crucial theoretical and empirical foundations for developing more equitable AI systems while challenging institutional stakeholders to acknowledge and systematically address embedded biases. The paper's contribution extends beyond Stable Diffusion to broader implications for all visual generative AI systems and algorithmic technologies more broadly.
