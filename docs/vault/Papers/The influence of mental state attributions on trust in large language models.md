---
title: The influence of mental state attributions on trust in large language models
authors:
  - C. Colombatto
  - J. Birch
  - S. M. Fleming
year: 2025
type: journalArticle
url: https://www.nature.com/articles/s44271-025-00262-1
doi: 10.1038/s44271-025-00262-1
tags:
  - paper
  - feminist-ai
  - bias-research
date_added: 2026-02-22
date_modified: 2026-02-22
---

# The influence of mental state attributions on trust in large language models

## Abstract

Empirical study examining how users' beliefs about LLM's "mind" affect trust in advice. Preregistered experiment (N=410) showing attributing intelligence to LLM strongly increased trust, whereas attributing human-like consciousness did not increase trust. Higher ascriptions of sentience correlated with less advice-taking. Participants trusted AI for perceived analytical competence, not for having feelings. Suggests prompt-engineering highlighting competence and reliability more effective than anthropomorphism for building appropriate trust.

## Key Concepts

## Full Text

---
title: "The influence of mental state attributions on trust in large language models"
authors: ["Clara Colombatto", "Jonathan Birch", "Stephen M. Fleming"]
year: 2025
type: journalArticle
language: en
processed: 2026-02-05
source_file: Colombatto_2025_The_influence_of_mental_state_attributions_on.md
confidence: 94
---

# The influence of mental state attributions on trust in large language models

## Kernbefund

Attributionen von Intelligenz-Merkmalen erhöhen signifikant die Annahme von KI-Ratschlägen, während Attributionen von Erfahrungs-Merkmalen (Emotionen, Empfindungen) diese reduzieren. Bewusstseinszuschreibungen zeigen keine positive, sondern eine schwache negative Korrelation mit Ratsannahmen.

## Forschungsfrage

Wie beeinflussen Zuschreibungen von mentalen Zuständen (insbesondere Bewusstsein und andere Mentalisierungen) das Vertrauen der Nutzer in Ratschläge von großen Sprachmodellen?

## Methodik

Empirisch: Preregistriertes Experiment (N=410) mit zwei Aufgaben (Advice-Taking-Task und Mental State Ratings), kombiniert mit Bayesian und frequentist Analysen
**Datenbasis:** N=410 US-amerikanische Erwachsene (stratifizierte Stichprobe, 204 Frauen, 199 Männer, 7 andere; Durchschnittsalter 47,19 Jahre), vollständig vorregistriert, anonymisierte Daten auf OSF verfügbar

## Hauptargumente

- Nutzer schreiben LLMs vermehrt mentale Zustände zu, die nicht mit wissenschaftlichen Erkenntnissen über KI-Bewusstsein übereinstimmen, aber diese folk-psychologischen Attributionen beeinflussen das Vertrauen und Verhalten im Umgang mit KI-Systemen.
- Mentale Zustandszuschreibungen sind keine unitäre Dimension: Die Dimensionen 'Intelligenz' (Reasoning, Planung) und 'Erfahrung' (Emotionen, Empfindungen) wirken differenziert auf Vertrauen und Ratsannahmen – Intelligenz positiv, Erfahrung negativ.
- Es existiert eine kritische Diskrepanz zwischen explizitem Vertrauen (gemessen in Selbstauskünften) und tatsächlichem Verhalten (Ratsannahmen): Bewusstseinszuschreibungen korrelieren mit selbstberichteter Vertrauensbewertung, nicht aber mit realen Entscheidungen, was auf die Bedeutung von Verhaltensmaßen hinweist.

## Kategorie-Evidenz

### Evidenz 1

Das Paper untersucht, wie Nutzer Kompetenzen und mentale Zustände von LLMs verstehen und interpretieren: 'the extent to which current AI systems possess consciousness remains contentious' und 'the majority of a representative sample of the public attributes some possibility of human-like consciousness to large language models'

### Evidenz 2

Fokus auf ChatGPT und Large Language Models: 'We recruited a stratified sample of US adults and probed their intuitions about the capacity for consciousness and a variety of other mental states in a prominent LLM, ChatGPT.'

### Evidenz 3

Thematisiert Anthropomorphismus, Mentalisierung und Trust in KI-Systemen allgemein: 'the tendency to assign mental states to AI systems is independent from whether these systems truly possess them'

### Evidenz 4

Das Paper adressiert faire und kalibrierte Vertrauensbeziehungen zwischen Menschen und KI: 'Further investigation will help the AI sector achieve well-calibrated and balanced trust, finding the middle ground between mistrust and over-reliance.'

## Assessment-Relevanz

**Domain Fit:** Das Paper ist bedingt relevant für Soziale Arbeit, da es untersucht, wie Menschen KI-Systeme wahrnehmen und vertrauen – Erkenntnisse, die für KI-gestützte Entscheidungsfindung in sozialen Kontexten relevant sind. Es adressiert jedoch weder spezifische Anwendungen in der Sozialen Arbeit noch deren Zielgruppen.

**Unique Contribution:** Erstmalige experimentelle Trennung der Effekte von Intelligenz- vs. Erfahrungs-Attributionen auf tatsächliches Vertrauen in LLM-Ratschläge mit Bayesian-Analysen und Verhaltensmaßen statt nur Selbstauskünften.

**Limitations:** Das Paper untersucht nur einen generalen Knowledge-Task mit kontrollierten Ratschlägen; die Erkenntnisse könnten in emotional oder persönlich relevanten Entscheidungskontexten (z.B. therapeutische oder beratende Situationen) anders ausfallen, wie die Autoren selbst eingestehen.

**Target Group:** KI-Forschende, Psychologen, UX/UI-Designer für KI-Systeme, Policymaker im Bereich KI-Regulierung, Entwickler von Human-AI Interaction Systemen; begrenzt relevant für Sozialarbeiter, die mit KI in Praxis arbeiten

## Schlüsselreferenzen

- [[Gray_H_M_Gray_K_Wegner_D_M_2007]] - Dimensions of mind perception
- [[Waytz_A_Cacioppo_J_Epley_N_2010]] - Who sees human? The stability and importance of individual differences in anthropomorphism
- [[Epley_N_Waytz_A_Cacioppo_J_T_2007]] - On seeing human: a three-factor theory of anthropomorphism
- [[De_Visser_E_J_et_al_2016]] - Almost human: anthropomorphism increases trust resilience in cognitive agents
- [[Waytz_A_Heafner_J_Epley_N_2014]] - The mind in the machine: anthropomorphism increases trust in an autonomous vehicle
- [[Colombatto_C_Fleming_S_M_2024]] - Folk psychological attributions of consciousness to large language models
- [[Hancock_P_A_et_al_2011]] - A meta-analysis of factors affecting trust in human-robot interaction
- [[Chalmers_D_J_2023]] - Could a large language model be conscious?
