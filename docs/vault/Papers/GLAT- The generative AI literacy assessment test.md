---
title: GLAT: The generative AI literacy assessment test
authors:
  - Yueqiao Jin
  - Roberto Martinez-Maldonado
  - Dragan Gašević
  - Lixiang Yan
year: 12/2025
type: journalArticle
url: https://linkinghub.elsevier.com/retrieve/pii/S2666920X25000761
doi: 10.1016/j.caeai.2025.100436
tags:
  - paper
  - feminist-ai
  - bias-research
date_added: 2026-02-22
date_modified: 2026-02-22
---

# GLAT: The generative AI literacy assessment test

## Key Concepts

## Full Text

---
title: "GLAT: The generative AI literacy assessment test"
authors: ["Yueqiao Jin", "Roberto Martinez-Maldonado", "Dragan Gašević", "Lixiang Yan"]
year: 2025
type: journalArticle
language: en
processed: 2026-02-05
source_file: Jin_2025_GLAT_The_generative_AI_literacy_assessment_test.md
confidence: 90
---

# GLAT: The generative AI literacy assessment test

## Kernbefund

GLAT ist ein valides und reliables 20-Item Multiple-Choice-Instrument zur Messung von GenAI-Literalität (Cronbach's alpha = 0.80; omega total = 0.81; RMSEA = 0.03; CFI = 0.97) und übertrifft selbstberichtete Maße in der Vorhersage tatsächlicher Lernleistung in GenAI-gestützten Aufgaben.

## Forschungsfrage

Wie kann GenAI-Literalität bei Studierenden in der Hochschulbildung durch ein valides und reliables Performance-basiertes Messinstrument erfasst werden?

## Methodik

Empirisch - Instrument-Entwicklung und Validierung. Classical Test Theory (CTT) und Item Response Theory (IRT); 2-Parameter Logistic (2PL) Modell; Confirmatory Factor Analysis (CFA); Externe Validität durch Korrelation mit Lernleistung in GenAI-gestützten Aufgaben.
**Datenbasis:** n=355 Studierende der Hochschulbildung; Content-Validität durch Delphi-Studium mit Fachexperten (n nicht spezifiziert)

## Hauptargumente

- Bestehende KI-Literalitätsinstrumente sind überwiegend selbstberichtet und erfassen nicht die spezifischen Kompetenzen, die für generative KI-Systeme erforderlich sind, was zu Verzerrungen und Lücken zwischen wahrgenommener und tatsächlicher Kompetenz führt.
- GenAI-Literalität ist eine spezialisierte Teilmenge der KI-Literalität, die integratives Wissen (Know What, Know How, Know Why), Prompt-Engineering, kritische Bewertung von Outputs und ethisches Verständnis erfordert - nicht abgedeckt durch allgemeine KI-Literalitätsrahmen.
- Performance-basierte Assessment-Instrumente sind notwendig und praktikabel für zuverlässige Messung von tatsächlichen GenAI-Kompetenzen in Hochschulkontexten und können als Grundlage für gezielte Interventionen und Politikgestaltung dienen.

## Kategorie-Evidenz

### Evidenz 1

GenAI literacy is defined as 'a set of competencies that enable individuals to effectively interact with AI technologies, encompassing understanding fundamental AI concepts, engaging in critical evaluation, and using AI tools ethically in diverse contexts.' Das Paper fokussiert auf Assessment von Kompetenzen wie 'recognition, application, evaluation, creation, and ethical navigation' (Ng et al., 2021b).

### Evidenz 2

Der gesamte Fokus des Papers liegt auf generativer KI: 'GenAI's capability to generate substantial content from minimal input alters the landscape, prompting a need for a revised understanding of AI literacy in this context.' Spezifische Systeme wie ChatGPT, Gemini und Claude werden erwähnt.

### Evidenz 3

Das Paper thematisiert Prompt-Engineering als Kernkompetenz: 'Developing GenAI literacy involves more than just foundational knowledge; it requires proficiency in crafting prompts, interpreting AI-generated outputs, and understanding the socio-ethical implications of using such tools.' GLAT-Items testen Prompt-Strategien (Item 11: 'strategies for creating marketing pitch', Item 12: 'customer service chatbot').

### Evidenz 4

Das Paper adressiert Bias und faire Anwendung: Item 21 thematisiert Bias in KI-gestützter Bewerbungsscreening ('The AI system could reinforce existing biases found in historical hiring data') und Item 22 behandelt Intransparenz (Black-Box-Problem) von KI-Systemen im Healthcare-Kontext.

### Evidenz 5

Das Paper behandelt Fairness-Fragen bei algorithmischen Entscheidungen, speziell in Item 21 zu Bias in Hiring-Prozessen und Item 22 zur Interpretierbarkeit und Vertrauenswürdigkeit von KI-Modellen im Healthcare-Kontext, sowie Item 23 zur Copyright-Fairness bei AI-generierten Inhalten.

## Assessment-Relevanz

**Domain Fit:** Das Paper ist für die Schnittstelle KI und Bildung hochrelevant, adressiert aber nicht direkt Soziale Arbeit oder Gender Studies. Es behandelt wichtige Fragen von Fairness und Bias im Kontext von KI-Literalität, was für sozialarbeiterische und diversitätsgerechte Kontexte relevant wäre, aber nicht explizit gemacht wird.

**Unique Contribution:** Das Paper füllt eine kritische Forschungslücke durch Entwicklung und Validierung des ersten rigorosen, performance-basierten Messinstruments für GenAI-Literalität nach etablierten Standards der psychologischen und pädagogischen Messung, mit bewiesener externer Validität durch Vorhersage tatsächlicher Lernleistung.

**Limitations:** Das Paper behandelt GenAI-Literalität primär im Hochschulkontext mit N=355 Studierenden aus einer Institution (Monash University, Australien); Generalisierbarkeit auf andere Kulturen, Bildungsstufen oder berufliche Kontexte ist begrenzt; keine explizite Analyse von Unterschieden zwischen verschiedenen Studierendengruppen (demografische, sozioökonomische oder kulturelle Faktoren).

**Target Group:** Hochschulpädagogen, KI-Bildungsforscher, Bildungspolitiker, Curriculumdesigner, Institutionen der Hochschulbildung, die GenAI-Literalität-Interventionen entwickeln möchten; sekundär relevant für Sozialarbeiter in Bildungskontexten und für die Entwicklung ethischer KI-Kompetenzen in verschiedenen Fachdomänen.

## Schlüsselreferenzen

- [[Long_Magerko_2020]] - What is AI literacy? Competencies and design considerations
- [[Ng_et_al_2021]] - Conceptualizing AI literacy: An exploratory review
- [[Bozkurt_2024]] - The 3wAI Framework (Know What, Know How, Know Why)
- [[Zhao_et_al_2024]] - GenAI literacy: pragmatic, safety, reflective, socioethical, and contextual elements
- [[Laupichler_et_al_2023]] - Scale for the Assessment of Non-Experts' AI Literacy (SNAIL)
- [[Wang_et_al_2023]] - AI Literacy Scale (AILS)
- [[Lee_Park_2024]] - ChatGPT Literacy Tool (einziges existierendes GenAI-spezifisches Instrument, selbstberichtet)
- [[Lintner_2024]] - A systematic review of AI literacy scales (13 self-reported, 3 performance-based)
- [[American_Educational_Research_Association_et_al_2014]] - Standards for educational and psychological testing
- [[Thorndike_et_al_1991]] - Measurement and evaluation in psychology and education
