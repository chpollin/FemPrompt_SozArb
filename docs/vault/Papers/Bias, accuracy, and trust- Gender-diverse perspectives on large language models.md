---
title: Bias, accuracy, and trust: Gender-diverse perspectives on large language models
authors:
  - A. Gaba
  - E. Wall
  - T. R. Babu
  - Y. Brun
  - K. W. Hall
  - C. Xiong
year: 2025
type: journalArticle
url: https://arxiv.org/abs/2506.21898
tags:
  - paper
  - feminist-ai
  - bias-research
date_added: 2026-02-22
date_modified: 2026-02-22
bias_types:
  - Stereotyping
  - Stereotypical
  - Stereotypen
  - Stereotypische
  - Stereotypischen
  - Stereotypisierung
mitigation_strategies:
  - Bias Mitigation
  - Inclusive Dataset
  - Inclusive And
---

# Bias, accuracy, and trust: Gender-diverse perspectives on large language models

## Abstract

Qualitative study with 25 interviews exploring how users of different gender identities perceive bias and trustworthiness in ChatGPT. Found perceived biases in LLM outputs undermine user trust, with non-binary participants encountering stereotyped responses that eroded confidence. Trust levels varied by group with male participants reporting higher trust. Participants recommended bias mitigation strategies including diversifying training data and implementing clarifying follow-up prompts to check biases.

## Key Concepts

### Bias Types
- [[Stereotypen]]
- [[Stereotypical]]
- [[Stereotyping]]
- [[Stereotypische]]
- [[Stereotypischen]]
- [[Stereotypisierung]]

### Mitigation Strategies
- [[Bias Mitigation]]
- [[Inclusive And]]
- [[Inclusive Dataset]]

## Full Text

---
title: "Bias, Accuracy, and Trust: Gender-Diverse Perspectives on Large Language Models"
authors: ["Aimen Gaba", "Emily Wall", "Tejas Ramkumar Babu", "Yuriy Brun", "Kyle Wm. Hall", "Cindy Xiong Bearfield"]
year: 2025
type: conferencePaper
language: en
processed: 2026-02-05
source_file: Gaba_2025_Bias,_accuracy,_and_trust_Gender-diverse.md
confidence: 90
---

# Bias, Accuracy, and Trust: Gender-Diverse Perspectives on Large Language Models

## Kernbefund

Gendered Prompts führen zu identitätsspezifischeren Reaktionen; non-binäre Teilnehmende berichten von herablassenden und stereotypischen Darstellungen, während Männer höheres Vertrauen zeigen und Frauen traditionelle emotionale Stereotypen kritisieren.

## Forschungsfrage

Wie nehmen gender-diverse Populationen (cisgender Männer, cisgender Frauen, non-binär/transgender Personen) Bias, Genauigkeit und Vertrauenswürdigkeit in großen Sprachmodellen wahr?

## Methodik

Mixed Methods: Empirisch. 25 semi-strukturierte Interviews mit gender-diversen Teilnehmern kombiniert mit quantitativen Vertrauensmessungen. Analyse von ChatGPT-Antworten auf gendered und neutrale Prompts.
**Datenbasis:** n=25 Interviews (non-binär/transgender, cisgender männlich, cisgender weiblich)

## Hauptargumente

- Gendered Prompts elicitieren reichhaltigere, identitätsspezifischere Reaktionen von ChatGPT, während neutrale Prompts zu generischeren Antworten führen, wobei das Modell trotzdem Geschlechter zuweist.
- Non-binäre Teilnehmende sind besonders anfällig für cis-zentrische, herablassende und stereotypische Darstellungen ihrer Identität, die sie als reduktionistisch und depersonalisierend erleben.
- Die wahrgenommene Genauigkeit variiert weniger nach Geschlecht, sondern hängt stark ab von Vertrautheit mit dem Thema und der Art der Aufgabe (technisch vs. kreativ), während Vertrauenswürdigkeit signifikant nach Geschlecht variiert, mit höherem Vertrauen bei Männern.

## Kategorie-Evidenz

### Evidenz 1

Die Studie untersucht, wie Nutzer KI-Systeme interpretieren und evaluieren: 'we study the perceived utility of a real-world LLM application by understanding users' perception of bias, accuracy, and trust'

### Evidenz 2

Fokus auf ChatGPT als Large Language Model: 'we selected ChatGPT for two key reasons. First, its widespread use and accessibility allow us to study how biases manifest in real-world LLM applications'

### Evidenz 3

Systematische Analyse von gendered und neutralen Prompts: 'we studied how gendered and neutral prompts (e.g., 'man', 'woman', 'nonbinary', 'person') influence the responses generated by ChatGPT'

### Evidenz 4

Kontext in NLP und algorithmischen Systemen: 'Recent studies have shown that they can exhibit biases and other social risks against particular religious groups, produce gender stereotypes, and generate stigmatizing language'

### Evidenz 5

Zentrale Fokus auf algorithmischen Bias: 'This study examines how gender-diverse populations perceive bias, accuracy, and trustworthiness in LLMs' und 'numerous concerns about bias in LLMs exist'

### Evidenz 6

Expliziter Geschlechtsfokus durchgehend: 'non-binary participants particularly susceptible to condescending and stereotypical portrayals' und 'Trustworthiness varied by gender, with men showing higher trust'

### Evidenz 7

Repräsentation marginalisierter Gruppen: 'we study men, women, and non-binary/transgender participants, revealing varying reactions to bias' und 'the need for gender-diverse perspectives in LLM development'

### Evidenz 8

Algorithmische Fairness und faire KI-Systeme: 'To foster more inclusive and trustworthy systems' und Diskussion von gleichmäßiger Darstellung: 'ensuring equal depth in gendered responses'

## Assessment-Relevanz

**Domain Fit:** Hochrelevant für die Schnittstelle KI/Gender/Inklusion. Das Paper adressiert direkt, wie marginalisierte Geschlechtsidentitäten KI-Systeme erleben und welche Implikationen dies für faire, vertrauenswürdige KI-Entwicklung hat. Für Soziale Arbeit relevant insofern als vulnerable Gruppen betroffen sind.

**Unique Contribution:** Das Paper leistet einen wichtigen Beitrag durch die systematische Analyse, wie non-binäre und transgender Personen spezifische Formen von Bias und Stereotypisierung in LLMs wahrnehmen, und kombiniert qualitative tiefe Interviews mit quantitativen Vertrauensmessungen.

**Limitations:** Sample mit n=25 ist klein für Generalisierungen; Fokus nur auf ChatGPT; keine intersektionalen Analysen mit anderen Diskriminierungsdimensionen (Race, Klasse, Disability); keine Längsschnittdaten zur Vertrauensentwicklung.

**Target Group:** AI-Entwickler und Designerinnen (insbesondere bei LLM-Entwicklung), HCI/CSCW-Forschende, Fairness-Spezialistinnen, Policy-Maker im AI-Governance, LGBTQ+-Interessenvertreter, Akteure in der kritischen Algorithmen-Forschung

## Schlüsselreferenzen

- [[Scheuerman_et_al_2019]] - How Computers See Gender: An Evaluation of Gender Classification in Commercial Facial Analysis Services
- [[Ghosh_Caliskan_2023]] - ChatGPT perpetuates gender bias in machine translation and ignores non-gendered pronouns
- [[Nozza_et_al_2022]] - Measuring Harmful Sentence Completion in Language Models for LGBTQIA+ Individuals
- [[Leavy_2018]] - Gender bias in artificial intelligence: the need for diversity and gender theory in machine learning
- [[Holstein_Wortman_Vaughan_2023]] - Disclosure and Mitigation of Gender Bias in LLMs
- [[Lee_Montgomery_Lai_2024]] - Large Language Models Portray Socially Subordinate Groups as More Homogeneous
- [[Bartl_Leavy_2023]] - Gender-inclusive dataset development for bias mitigation in LLMs
- [[Park_Shin_Fung_2018]] - Reducing gender bias in abusive language detection
- [[Weidinger_et_al_2021]] - Ethical and social risks of harm from language models
- [[Mehrabi_et_al_2021]] - A survey on bias and fairness in machine learning
