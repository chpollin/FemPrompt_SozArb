---
title: Gender in a stereo-(gender)typical EU AI law: A feminist reading of the AI Act
authors:
  - A. Karagianni
year: 2025
type: journalArticle
url: https://www.cambridge.org/core/journals/cambridge-forum-on-ai-law-and-governance/article/gender-in-astereogendertypical-eu-ai-law-a-feminist-reading-of-the-ai-act/E9DEFC1E114CBE577D737EC616610921
doi: 10.1017/cfl.2025.12
tags:
  - paper
  - feminist-ai
  - bias-research
date_added: 2026-02-22
date_modified: 2026-02-22
bias_types:
  - Intersectional Data
  - Intersectionality
mitigation_strategies:
  - Intersectional Data
  - Feminist Approach
---

# Gender in a stereo-(gender)typical EU AI law: A feminist reading of the AI Act

## Abstract

This peer-reviewed article offers a feminist critique of the European Union’s AI Act, arguing that the law’s current bias mitigation provisions only superficially address gendered risks and fail to confront deeply embedded structural biases. Using frameworks like hermeneutical injustice and feminist legal theory, Karagianni shows that AI systems can perpetuate historical power imbalances and systemic discrimination against women and marginalized groups. The AI Act’s pre-market and post-market measures are analyzed to reveal how they often reinforce, rather than challenge, algorithmic discrimination. The author concludes that effective AI governance must go beyond technical fixes, incorporating an intersectional perspective and substantive equality principles. She calls for feminist-informed revisions to the AI Act – emphasizing gender inclusivity, intersectionality, and accountability – to ensure AI regulation actively dismantles (instead of inadvertently upholding) existing power asymmetries.

## Key Concepts

### Bias Types
- [[Intersectional Data]]
- [[Intersectionality]]

### Mitigation Strategies
- [[Feminist Approach]]
- [[Intersectional Data]]

## Full Text

---
title: "Gender in a stereo-(gender)typical EU AI law: A feminist reading of the AI act"
authors: ["Anastasia Karagianni"]
year: 2025
type: journalArticle
language: en
processed: 2026-02-05
source_file: Karagianni_2025_Gender_in_a_stereo-(gender)typical_EU_AI_law_A.md
confidence: 93
---

# Gender in a stereo-(gender)typical EU AI law: A feminist reading of the AI act

## Kernbefund

Obwohl die AI Act Massnahmen zur Mitigation geschlechterspezifischer Risiken vorsieht, adressiert sie nicht die strukturellen Biase in KI-Technologien, die marginalisierte Gruppen überproportional schädigen. Eine intersektionale feministische Perspektive enthüllt formale Gleichheitskonzepte, die substantive Ungleichheiten perpetuieren.

## Forschungsfrage

Wie adressiert die EU AI Act Geschlechterdebatten, Nicht-Diskriminierung und systemische Machtungleichgewichte aus einer feministischen Perspektive?

## Methodik

Theoretisch; kritische Textanalyse mit feministischen Rechtstheorien, Hermeneutische Analyse von AI Act-Bestimmungen durch die Linse feministischer Epistemologie und dekolonialer Theorie
**Datenbasis:** nicht empirisch; kritische Analyse von EU 2024/1689 AI Act-Text, Recitals und Artikeln; literaturgestützte theoretische Argumentation

## Hauptargumente

- Die AI Act behandelt algorithmische Verzerrung als technisches statt strukturelles Problem und übersieht dadurch die patriarchalen Normen, die in Datensammlungen, Modelltraining und Regulierungsaufsicht eingebettet sind (MacKinnon's Dominanztheorie).
- Das Konzept der 'Vulnerabilität' in der AI Act ist unklar und stammt aus informatischen Kontexten, nicht aus Gender Studies oder sozialen Wissenschaften, wo es Autonomie und Abhängigkeit im Kontext kolonialer und patriarchaler Machtdynamiken bedeutet.
- Die AI Act verweist nur zweimal auf 'Geschlechtergleichstellung' in den Recitals und einmal in Artikel 95, während sie sich nicht auf inklusive Geschlechtsidentitäten (trans, nicht-binär, intersex) bezieht, was die Marginalität von Gender in der Regulierung anzeigt.
- Standardisierungsprozesse in der AI Act-Implementierung bleiben von Konzernen und Regierungen dominiert und riskieren Tokenismus, wenn marginalisierte Gruppen nicht substantiv in die Entwicklung von Fairness-Standards einbezogen werden.
- Feminist Impact Assessments sind notwendig, um zu prüfen, wie AI-Systeme intersektionale Ungleichheiten (Geschlecht, Rasse, Klasse, Behinderung) exazerbieren, besonders in Kontexten wie Einstellung, Gesundheitsversorgung und Strafverfolgung.

## Kategorie-Evidenz

### Evidenz 1

Paper adressiert 'generative AI raises legal concerns, particularly in the proliferation of non-consensual sexualised deepfakes, which constitute a form of gender-based violence'

### Evidenz 2

Analysiert high-risk AI systems unter AI Act, predictive algorithms (SyRI-Fall), recruitment tools, healthcare diagnostics, AI-driven systems für soziale Wohlfahrt

### Evidenz 3

Bezug zu social services als high-risk Kontexte: 'in high-risk domains such as law enforcement and social services, to ensure compliance with the GDPR and safeguard fundamental rights'

### Evidenz 4

Kernthema: 'AI systems have been shown to disproportionately misclassify racialised and gender-diverse individuals, reinforcing structural inequalities' und detaillierte Analyse algorithmischer Diskriminierung

### Evidenz 5

Explizites Gender-Focus durchgehend: 'gynaecologic cancer detection, operate within binary gender frameworks, often misgendering transgender and non-binary patients'; Analyse geschlechterspezifischer Auswirkungen

### Evidenz 6

Intersektionale Perspektive zentral: 'assessments must account for the ways in which gender intersects with race, class, disability and other identity factors' und Fokus auf marginalisierte Communities

### Evidenz 7

Explizit feministische Theorie verwendet: Miranda Fricker (hermeneutical injustice), Catharine MacKinnon (dominance theory), Kimberlé Crenshaw (intersectionality), Donna Haraway (agential realism), Ann Julia Cooper; 'feminist legal methods', 'decolonial feminist approach'

### Evidenz 8

Diskutiert algorithmische Fairness, Fairness in Hiring/Healthcare, 'intersectional data analysis in AI conformity assessments', Fairness-aware standards und Bias-Audits

## Assessment-Relevanz

**Domain Fit:** Hochgradig relevant für KI/Soziale Arbeit/Gender-Schnittstelle: Das Paper analysiert regulatorische Frameworks (AI Act) mit feministischer Epistemologie und deckt auf, wie Soziale Dienste als high-risk AI-Kontexte betroffen sind, während marginalisierte Gruppen (Frauen, trans, PoC) überproportional Schaden erleiden.

**Unique Contribution:** Erste systematische feministische Rechtsanalyse der EU AI Act, die Miranda Fricker's Hermeneutical Injustice, MacKinnon's Dominanztheorie und dekoloniale Ansätze integriert, um strukturelle Gender-Biase in KI-Governance zu kritisieren.

**Limitations:** Paper ist rein theoretisch-analytisch ohne empirische Daten; fokussiert auf EU AI Act und nicht auf internationale Regulierungsmodelle; begrenzte Analyse von implementierungspraktiken nach Gesetzeskraft.

**Target Group:** Policymaker und Regulatoren (EU), KI-Ethiker und -Entwickler, Sozialarbeiter und Soziale Dienste (betroffen von AI-Regulierung), Feminist und Dekolonial-Scholars, Menschenrechts- und Gleichstellungsorganisationen, digitale Rechts-Aktivisten

## Schlüsselreferenzen

- [[Fricker_M_2007]] - Epistemic Injustice: Power and the Ethics of Knowing
- [[MacKinnon_C_A_2013]] - Feminist Legal Theory on Male Dominance
- [[Crenshaw_K_1991]] - Intersectionality
- [[Quijano_A_2000]] - Coloniality of Power and Eurocentrism in Latin America
- [[Mignolo_W_2012]] - Decolonizing Western Epistemology
- [[Haraway_D_2014]] - Situated Knowledges
- [[Andrews_L_Bucher_B_2022]] - Amazon's AI Recruitment Tool and Gender Bias
- [[van_Bekkum_M_Borgesius_F_Z_2021]] - Digital Welfare Fraud Detection and the Dutch SyRI Judgment
- [[Keyes_O_2018]] - The Misgendering Machines: Trans/HCI Implications of Automatic Gender Recognition
- [[Ricaurte_P_Zasso_M_2023]] - AI, Ethics and Coloniality: A Feminist Critique
