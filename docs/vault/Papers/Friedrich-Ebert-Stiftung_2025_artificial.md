---
title: Friedrich-Ebert-Stiftung_2025_artificial
authors:
  - Unknown Author
year: 2024
type: research-paper
tags:
  - paper
  - feminist-ai
  - bias-research
date_added: 2026-02-22
date_modified: 2026-02-22
bias_types:
  - Discrimination
  - Intersectional Data
  - Intersectionality
mitigation_strategies:
  - Intersectional Data
  - Feminist Approach
---

# Friedrich-Ebert-Stiftung_2025_artificial

## Key Concepts

### Bias Types
- [[Discrimination]]
- [[Intersectional Data]]
- [[Intersectionality]]

### Mitigation Strategies
- [[Feminist Approach]]
- [[Intersectional Data]]

## Full Text

---
title: "The EU Artificial Intelligence Act through a Gender Lens"
authors: ["Anastasia Karagianni"]
year: 2025
type: report
language: en
processed: 2026-02-05
source_file: Friedrich-Ebert-Stiftung_2025_artificial.md
confidence: 95
---

# The EU Artificial Intelligence Act through a Gender Lens

## Kernbefund

Der EU AI Act enthält erhebliche Lücken bei der Adressierung von Geschlechterungerechtigkeit und verwendet überwiegend geschlechtsneutrale Sprache, die die einzigartigen Herausforderungen marginalisierter Gruppen, insbesondere Frauen of Color und LGBTQIA+ Personen, nicht angemessen berücksichtigt.

## Forschungsfrage

Wie adäquat adressiert der EU AI Act Geschlechtergerechtigkeit und strukturelle Machtungleichgewichte in KI-Systemen, insbesondere für marginalisierte Gruppen?

## Methodik

Theoretisch: Feministische Textanalyse des AI Act mit Artikel-für-Artikel-Analyse und Fallstudien; kritische Literaturanalyse; intersektionale Perspektive
**Datenbasis:** Nicht empirisch; basiert auf Dokumentenanalyse des AI Act, Fallstudien (Amazon Recruiting, Deliveroo, iBorderCtrl, VioGen) und kritischer Literaturanalyse

## Hauptargumente

- Der AI Act referenziert 'Geschlechtergleichstellung' nur minimal (Recitals 27, 48 und Artikel 95(2)(e)), während 'Nicht-Diskriminierung' häufiger erwähnt wird, was eine mangelnde explizite Verpflichtung zur Geschlechtergleichstellung widerspiegelt.
- Bestehende KI-Systeme (Amazon, Deliveroo, iBorderCtrl, VioGen) demonstrieren, wie Algorithmen ohne intersektionale Perspektive geschlechtsspezifische und rassifizierte Diskriminierung reproduzieren und marginalisierte Gruppen disproportional schädigen.
- Eine intersektionale, feministische Analyse ist notwendig, da Geschlechter-Bias nicht isoliert existiert, sondern mit Rasse, Klasse, Behinderung und anderen Identitätsfaktoren zusammenwirkt, was einen mehrdimensionalen regulatorischen Ansatz erfordert.
- Standardisierungs- und Konformitätsbewertungsprozesse müssen durch geschlechtsspezifische Auswirkungsbewertungen (GIA) und verbindliche Bias-Audits verstärkt werden, die explizit intersektionale Diskriminierung adressieren.
- Die Regulierung allein ist unzureichend; eine feministische Interpretation des AI Act mit inklusiverer Sprache, partizipativen Governance-Strukturen und Einbeziehung marginalisierter Stimmen in Designprozessen ist erforderlich.

## Kategorie-Evidenz

### Evidenz 1

Das Paper analysiert den EU AI Act als regulatorisches Framework für verschiedene KI-Systeme in Recruitment, Healthcare, Border Management und Predictive Policing.

### Evidenz 2

Analyse von KI-Systemen in der Gewaltschutzpraxis (VioGen-Fall) und deren Auswirkungen auf vulnerable Gruppen; Empfehlungen für inklusive Sozialschutzpolitik und Gewaltschutz.

### Evidenz 3

Fallstudien zeigen systematische Diskriminierung: 'Amazon's AI recruitment tool...unintentionally favoured male candidates'; 'Deliveroo's rider-ranking algorithm...violated labour laws'; iBorderCtrl könnte 'misinterpret women's facial expressions, reflecting societal biases'.

### Evidenz 4

Expliziter Gender-Fokus: 'gender equality is limited to Recitals 27 and 48 AI Act'; Analyse von Geschlechter-Bias in Objektifizierungstheorie; Schutz vor geschlechtsspezifischer KI-Gewalt (Deepfakes); Reproduktion von Geschlechterstereotypen.

### Evidenz 5

Intersektionale Analyse: 'individuals do not experience bias in isolation; rather, their experiences are shaped by the confluence of their gender, race, socioeconomic status and other identity factors'; Fokus auf 'women of colour, LGBTQIA+ individuals, and those with disabilities'.

### Evidenz 6

Explizite Verwendung feministischer Theorie: Objectification Theory (Fredrickson & Roberts 1997); Intersectionality Framework (Crenshaw 2019); Feminist Data Protection (Theilen et al. 2021); Feminist Philosophy of Law (Francs & Smith 2021); 'a feminist approach to harmonisation'; 'feminist-informed revisions'; Kritik an patriarchalischen Strukturen.

### Evidenz 7

Conformity assessment muss 'comprehensive bias audits' durchführen; Forderung nach 'fair and devoid of bias' AI systems; 'intersectional data analysis in conformity assessments' zur Gewährleistung von Fairness für marginalisierte Communities.

## Assessment-Relevanz

**Domain Fit:** Hochrelevant für die Schnittstelle AI/Soziale Arbeit/Gender. Das Paper analysiert regulatorische Rahmenbedingungen für KI-Systeme, die Sozialarbeit, Gewaltschutz und vulnerable Gruppen unmittelbar betreffen, und liefert eine feministisch-kritische Perspektive auf strukturelle Ungleichheiten.

**Unique Contribution:** Die systematische feministische und intersektionale Analyse des EU AI Act mit Fokus auf Geschlechter-Bias und strukturelle Benachteiligung marginalisierter Gruppen; konkrete Empfehlungen zur Integration geschlechterspezifischer Auswirkungsbewertungen und inklusiver Sprache.

**Limitations:** Nicht empirisch validiert; konzentriert sich auf EU-Kontext; begrenzte Analyse von Implementierungsmechanismen; keine Daten zu tatsächlichen Auswirkungen auf Sozialarbeitspraxis nach AI-Act-Implementierung

**Target Group:** Policymaker und EU-Institutionen; Sozialarbeiter und Fachkräfte in Gewaltschutz und Care-Arbeit; KI-Entwickler und Standardisierungsorgane; Feminist und Gender-Studies Scholar; Menschenrechtsorganisationen und Interessenvertretungen marginalisierter Gruppen; Forschende an der Schnittstelle KI und Soziale Arbeit

## Schlüsselreferenzen

- [[Crenshaw_K_2019]] - Intersectionality Framework
- [[Fredrickson_B_L_Roberts_T_A_1997]] - Objectification Theory: Toward Understanding Women's Lived Experiences and Mental Health Risks
- [[Theilen_J_T_Baur_A_Bieker_F_Ammicht_Quinn_R_Hansen_M_González_Fuster_G_2021]] - Feminist data protection: an introduction
- [[Dastin_J_2018]] - Amazon scraps secret AI recruiting tool that showed bias against women
- [[Zuiderveen_Borgesius_F_2018]] - Discrimination, Artificial Intelligence and Algorithmic Decision-Making
- [[Leavy_S_2018]] - Gender bias in artificial intelligence: The need for diversity and gender theory in machine learning
- [[Sovacool_B_FurszyferDel_Rio_D_D_Martiskainen_M_2021]] - Can prosuming become perilous? Exploring systems of control and domestic abuse in the smart homes of the future
- [[Karagianni_A_Doh_M_2024]] - A feminist legal analysis of non-consensual sexualized deepfakes: contextualizing its impact as AI-generated image-based violence under EU law
- [[Kloza_D_Van_Dijk_N_Casiraghi_S_Vazquez_Maymir_S_Tanas_A_2021]] - The concept of impact assessment
- [[Hendrickx_V_2024]] - Women's rights in the age of automation
