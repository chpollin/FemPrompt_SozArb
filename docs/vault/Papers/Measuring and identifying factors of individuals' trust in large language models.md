---
title: Measuring and identifying factors of individuals' trust in large language models
authors:
  - E. S. De Duro
  - G. A. Veltri
  - H. Golino
  - M. Stella
year: 2025
type: journalArticle
url: https://arxiv.org/html/2502.21028v1
doi: 10.48550/arXiv.2502.21028
tags:
  - paper
  - feminist-ai
  - bias-research
date_added: 2026-02-22
date_modified: 2026-02-22
---

# Measuring and identifying factors of individuals' trust in large language models

## Abstract

Study developing "Trust-In-LLMs Index (TILLMI)" psychometric scale to quantify trust in conversational AI. Survey of ~1,000 U.S. adults identified two trust dimensions: affective component (emotional closeness) and cognitive component (reliance on competence). Found significant individual differences with personality factors influencing trust. Prior LLM experience increased trust while those with no direct use were more skeptical. Highlights need for prompt-engineering and system design to account for user differences.

## Key Concepts

## Full Text

---
title: "Measuring and Identifying Factors of Individuals' Trust in Large Language Models"
authors: ["Edoardo Sebastiano De Duro", "Giuseppe Alessandro Veltri", "Hudson Golino", "Massimo Stella"]
year: 2025
type: journalArticle
language: en
processed: 2026-02-05
source_file: De Duro_2025_Measuring_and_identifying_factors_of_individuals'.md
confidence: 91
---

# Measuring and Identifying Factors of Individuals' Trust in Large Language Models

## Kernbefund

Vertrauen in LLMs teilt sich in zwei empirisch distinkte aber korrelierte Dimensionen: 'Closeness with LLMs' (affektiv-emotional) und 'Reliance on LLMs' (kognitiv-rational), ähnlich wie in der Organisationspsychologie. Jüngere männliche Nutzer zeigen signifikant höheres Vertrauen als ältere weibliche Nutzer.

## Forschungsfrage

Welche Faktoren prägen das Vertrauen von Einzelnen in Large Language Models und wie lässt sich dieses Vertrauen reliabel messen?

## Methodik

Empirisch, Mixed-Methods: Psychometrische Skalenentwicklung (TILLMI) mit LLM-simulierter Validität (Exploratory Factor Analysis mit GPT-4-Daten), validiert mit N=1000 US-Respondenten durch Confirmatory Factor Analysis, Convergent/Divergent Validity, Regression und Network Psychometrics.
**Datenbasis:** N=1000 US-Survey-Respondenten; N=521 mit LLM-Erfahrung, N=479 ohne; zusätzlich synthetische Daten von GPT-4; Emotional Recall Task und psychometrische Instrumente (IPIP-NEO, Cognitive Flexibility, DASentimental).

## Hauptargumente

- LLMs erfordern eine dedizierte psychometrische Messung, da sie sich durch natürlichsprachige, menschenähnliche Interaktionen von einfachen Automatisierungssystemen unterscheiden. Bestehende Vertrauensskalen können die Besonderheiten von LLM-Interaktionen nicht vollständig erfassen.
- Vertrauen in LLMs folgt dem etablierten psychologischen Modell von McAllister (1995) mit affektiver und kognitiver Dimension. Affektive Nähe ('Closeness') basiert auf emotionalen Bindungen und Unterstützungsgefühl, während kognitive Zuverlässigkeit ('Reliance') auf rationalen Bewertungen von Kompetenz und Konsistenz beruht.
- Demografische Unterschiede (Alter, Geschlecht) und Nutzungserfahrung sind signifikante Prädiktoren für Vertrauensbildung. Die Messung von Nutzervertrauen ist ethisch essenziell, um sowohl die Vorteile von LLMs zu nutzen als auch vor übermäßiger Abhängigkeit und Datenvulnerabilität zu schützen.
- Die Verwendung von LLM-generiertem synthetischen Daten ('LLM-simulated validity') zur Skalenvalidierung ist eine innovative und effiziente Methode in der psychometrischen Entwicklung, zeigt aber unterschiedliche Muster zu menschlichen Antworten bei mehrdeutigen Items.

## Kategorie-Evidenz

### Evidenz 1

Das Paper entwickelt ein Messinstrument für Nutzerkompetenz und kritisches Verständnis im Umgang mit LLMs: 'measuring individuals' trust in LLMs' als zentrale Literacy-Komponente. Nutzer mit direkter LLM-Erfahrung zeigen signifikant höheres Vertrauen als Nicht-Nutzer (t(998)=23.61, p<.001).

### Evidenz 2

Expliziter Fokus auf Large Language Models: 'LLMs present unique challenges that extend beyond traditional human-computer trust dynamics. Unlike simpler automated systems or social robots, LLMs present the unique capability of engaging in natural language exchanges that closely mirror human ones.'

### Evidenz 3

Psychometrische Methoden und Psychological Science Ansätze zur KI-Vertrauensmessung; Network Psychometrics und Machine Learning (DASentimental) zur Datenanalyse.

### Evidenz 4

Signifikante demografische Disparitäten: 'Younger males exhibited higher closeness with- and reliance on LLMs compared to older women' (Gender: β=-0.317/-0.638, Age: β=-0.457/-0.284). Digital Divide bezüglich LLM-Zugang und Vertrauensentwicklung.

### Evidenz 5

Explizite Geschlechtsunterschiede als Hauptergebnis: Männer und jüngere Personen zeigen signifikant höheres Vertrauen. Die Analyse nach Geschlecht (1=Male, 2=Female) ist zentral in der Regressionsanalyse (Table 5). Gendereffekte unterscheiden sich je nach Zielgruppe: 'literature regarding gender effects on trust towards AIs shows findings that differ according to the target of human trust.'

### Evidenz 6

Unterschiedliche Vertrauensniveaus je nach Altersgruppe, Geschlecht und Nutzungserfahrung; jedoch limitierte Analyse anderer Diversitätsdimensionen (Ethnizität, Behinderung, SES werden nicht systematisch untersucht).

### Evidenz 7

Ethische Implikationen von Vertrauen in LLMs für Fairness: 'measuring trust is essential for both leveraging the benefits of AI and guarding against its potential harms.' Konzeptionalisierung fairen Zugangs: 'Without trust, users may hesitate to follow AI-generated advice, share sensitive information, or integrate these tools into daily workflows.'

## Assessment-Relevanz

**Domain Fit:** Das Paper hat begrenzte direkte Relevanz für Soziale Arbeit, ist aber hochrelevant für AI Literacy und ethische KI-Governance. Es adressiert kritische Fragen von Vertrauen, Vulnerabilität und Datenschutz, die für vulnerable Gruppen in sozialen Diensten zentral sind. Gender- und Altersunterschiede sind relevant für Zielgruppen der Sozialen Arbeit.

**Unique Contribution:** Erstmaliges psychometrisches Messinstrument (TILLMI) speziell für menschliches Vertrauen in LLMs mit McAllister-Framework; innovative Methode der 'LLM-simulated validity' für Skalenentwicklung; empirischer Nachweis der Zwei-Faktor-Struktur (affektiv/kognitiv) in LLM-Kontext.

**Limitations:** Stichprobe begrenzt auf US-Respondenten (Generalisierbarkeit); begrenzte Erfassung anderer Diversitätsdimensionen (nur Alter/Geschlecht); Querschnittsdesign erlaubt keine Kausalitätsaussagen zur Vertrauensentwicklung; begrenzte Analyse spezifischer LLM-Typen oder Anwendungsdomänen.

**Target Group:** KI-Entwickler und UX-Designer (Skalenvalidierung für LLM-Design), Forscher im Bereich Human-AI Interaction und Psychometrik, Policymaker für AI-Governance und Transparenzanforderungen, IT-Literacy-Experten, Nutzer vulnerable Gruppen (älteren, weniger technisch affinen Personen) in Bezug auf Vertrauen und Abhängigkeit von LLMs; sekundär: Sozialarbeiter und Ethik-Fachleute.

## Schlüsselreferenzen

- [[McAllister_1995]] - Affect- and Cognition-Based Trust as Foundations for Interpersonal Cooperation in Organizations
- [[Madsen_Gregor_2000]] - Measuring Human-Computer Trust
- [[McKnight_et_al_2011]] - Trust in a Specific Technology: An Investigation of its Components and Measures
- [[Hoff_Bashir_2015]] - Trust in Automation: Integrating Empirical Evidence on Factors that Influence Trust
- [[Schaefer_et_al_2016]] - A Meta-Analysis of Factors Influencing the Development of Trust in Automation
- [[Glikson_Woolley_2020]] - Human Trust in Artificial Intelligence: Review of Empirical Research
- [[Bo_et_al_2024]] - To Rely or Not to Rely? Evaluating Interventions for Appropriate Reliance on Large Language Models
- [[Golino_Epskamp_2017]] - Exploratory Graph Analysis: A New Approach for Estimating the Number of Dimensions
- [[Fatima_et_al_2021]] - DASentimental: Detecting Depression, Anxiety, and Stress in Texts
