---
title: Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals
authors:
  - A. Choudhury
  - Z. Chaudhry
year: 2024
type: journalArticle
url: https://www.jmir.org/2024/1/e56764/
doi: 10.2196/56764
tags:
  - paper
  - feminist-ai
  - bias-research
date_added: 2026-02-22
date_modified: 2026-02-22
---

# Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals

## Abstract

Peer-reviewed viewpoint discussing integration of LLMs into healthcare requiring balance between trust and skepticism. Warns that "blind trust" in LLM recommendations can lead to automation bias and confirmation bias as clinicians may accept AI outputs uncritically. Argues that prompting for transparency in LLM reasoning enhances professionals' trust by making AI reasoning visible and verifiable. Emphasizes need for human oversight and bias mitigation to ensure LLMs complement rather than compromise expert decision-making.

## Key Concepts

## Full Text

---
title: "Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals"
authors: ["Avishek Choudhury", "Zaira Chaudhry"]
year: 2024
type: journalArticle
language: en
processed: 2026-02-05
source_file: Choudhury_2024_Large_Language_Models_and_User_Trust_Consequence.md
confidence: 90
---

# Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals

## Kernbefund

LLMs können in selbstreferenziellen Lernschleifen geraten, in denen KI-generierte Inhalte zur Trainierung führen, was zu Bias-Verstärkung, reduzierter Datenvielfalt und Deskilling von Klinikern führt. Diese Risiken erfordern proaktive Maßnahmen zur Aufrechterhaltung von Fachkompetenz und kritischer Überwachung.

## Forschungsfrage

Wie beeinflussen selbstreferenzielle Lernschleifen in LLMs und das resultierende Vertrauen von Klinikern die klinische Kompetenz sowie die Qualität von LLM-Outputs im Gesundheitswesen?

## Methodik

Theoretisch/Konzeptuell - Analysiert die Dynamiken zwischen Kliniker-Vertrauen, Datenquellen und LLM-Leistung über Zeit; basiert auf Literaturanalyse und konzeptuellem Modell (Figur 1), keine empirische Datenerhebung
**Datenbasis:** nicht angegeben (konzeptuelle Analyse ohne empirische Datenerhebung)

## Hauptargumente

- Der Aufstieg von LLM-Vertrauen bei Klinikern korreliert invers mit dem Erhalt klinischer Fähigkeiten, da zunehmende Automatisierungs-Bias und Abhängigkeit zu kognitiven Verzerrungen (Confirmation Bias, Precautionary Bias) führen.
- Selbstreferenzielle Lernschleifen entstehen, wenn KI-generierte Inhalte zur Trainierung neuer Modelle verwendet werden, was Bias-Verstärkung, Echo-Kammer-Effekte und letztlich Leistungsabbau verursacht.
- Das Risiko der Deskilling betrifft besonders zukünftige Generationen von Gesundheitsfachkräften, deren kritisches Denken, diagnostische Fähigkeiten und tiefes Verständnis durch übermäßiges Delegieren an KI-Systeme beeinträchtigt werden könnte.

## Kategorie-Evidenz

### Evidenz 1

Die Expertise von Nutzern bestimmt ihre Fähigkeit, Fehler in LLM-Outputs zu erkennen: 'Subject matter experts (doctors) may use LLMs to handle routine, time-consuming tasks... They have the advantage of being able to critically evaluate the LLM's output, verify its accuracy... The expertise of such users acts as a safeguard against potential errors'

### Evidenz 2

Fokus auf LLMs wie ChatGPT und deren Integration in Gesundheitswesen: 'generative AI like large language models (LLM) is already being deployed in the public sphere... used by health care workers, researchers, and the public for a variety of health care-related tasks'

### Evidenz 3

Behandelt algorithmische Entscheidungssysteme und KI-Accountability: 'The Algorithmic Accountability Act of 2023 and Artificial Intelligence Accountability Act represent a critical legislative step toward ensuring the responsible use of algorithms'

### Evidenz 4

Explizit thematisiert Bias-Verstärkung durch selbstreferenzielle Schleifen: 'AI systems learn from the data they are fed, and if these data include biases, the AI is likely to replicate and even amplify these biases in its outputs. In a self-referential loop, the problem becomes compounded... these biases can become more entrenched'

### Evidenz 5

Berücksichtigt unterschiedliche Nutzergruppen und deren unterschiedliche Risiken: 'individuals who turn to LLMs due to a lack of expertise in a particular area face a different set of challenges... users vulnerable to accepting erroneous information as fact'

### Evidenz 6

Adressiert faire Algorithmen-Entwicklung und Gerechtigkeit: 'ensuring transparency in LLM can further enhance trust in the system... This transparency combats algorithmic deference by encouraging health care professionals to critically assess LLM outputs'

## Assessment-Relevanz

**Domain Fit:** Das Paper ist relevant für die Schnittstelle von KI und professioneller Praxis, fokussiert aber auf Gesundheitswesen statt Soziale Arbeit. Die Themen Deskilling, Automation Bias und kritische Kompetenz lassen sich konzeptionell auf andere Fachdomänen übertragen, insbesondere auf Care-Berufe.

**Unique Contribution:** Erstmals wird die Dynamik zwischen Kliniker-Vertrauen, selbstreferenziellen LLM-Lernschleifen und Deskilling-Risiken in einem integrierten konzeptuellem Modell dargestellt, das zeitliche Trajektorien visualisiert.

**Limitations:** Die Analyse ist rein konzeptuell ohne empirische Validierung; die Selbstreferenzialitäts-These wird als zukünftiges Risiko positioniert ('has not yet materialized'), nicht als gegenwärtige Realität demonstriert.

**Target Group:** Gesundheitsfachkräfte, Kliniker, Medizinstudenten, Gesundheitsinstitutionen, KI-Entwickler im Healthtech-Bereich, Policymaker für AI-Regulierung, Bildungsverantwortliche in medizinischen Disziplinen

## Schlüsselreferenzen

- [[Harris_2021]] - AI background, selected issues, and policy considerations
- [[Goddard_Roudsari_Wyatt_2012]] - Automation bias: systematic review of frequency, effect mediators, and mitigators
- [[De_Angelis_et_al_2023]] - ChatGPT and the rise of large language models: AI-driven infodemic threat
- [[Singhal_et_al_2023]] - Large language models encode clinical knowledge
- [[Choudhury_Shamszare_2023]] - Impact of user trust on adoption and use of ChatGPT
- [[Asan_Bayrak_Choudhury_2020]] - Artificial intelligence and human trust in healthcare: focus on clinicians
- [[Meyer_et_al_2023]] - ChatGPT and large language models in academia: opportunities and challenges
- [[Lo_2023]] - Impact of ChatGPT on education
