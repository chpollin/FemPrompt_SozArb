{
  "papers": [
    {
      "id": "QM6L6XLZ",
      "stem": "A+ Alliance_2024_Incubating_Feminist_AI_Executive_Summary_2021-2024",
      "title": "Incubating Feminist AI: Executive Summary 2021-2024",
      "author_year": "A+ Alliance",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Das f<a+i>r Netzwerk hat über 2021-2024 nachgewiesen, dass eine kombinierte Strategie aus Forschung, Prototyping und Pilotierung mit echter Community-Beteiligung zu transformativen KI-Anwendungen führ",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 93
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK (KI_Sonstige: Bias-Detection in NLP, algorithmische Systeme im sozialen Kontext) und SOZIAL (Feministisch: explizit feminist AI; Gender: gender-based violence; Soziale_Arbeit: praktische Anwendungen bei Gewalt und Justiz; Bias_Ungleichheit: Bias-Detection"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "Fair Automated Decision-Making Systems",
        "Intersectional Feminism in Technology",
        "Community-Centered Design",
        "Feminist AI",
        "Algorithmic Bias Detection",
        "Design Justice",
        "Global South AI Governance",
        "Gender-Based Violence Prevention Technology"
      ],
      "knowledge_summary": "Das f<a+i>r Netzwerk hat über 2021-2024 nachgewiesen, dass eine kombinierte Strategie aus Forschung, Prototyping und Pilotierung mit echter Community-Beteiligung zu transformativen KI-Anwendungen führt, die Geschlechtergerechtigkeit, Sicherheit und Zugang zu Justiz für marginalisierte Gruppen verbessern."
    },
    {
      "id": "AFDLFCIL",
      "stem": "Ahmed_2024_Feminist_perspectives_on_AI_Ethical",
      "title": "Feminist Perspectives on AI: Ethical Considerations in Algorithmic Decision-Making",
      "author_year": "Ahmed (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Empirische Analyse zeigt signifikante geschlechtsspezifische und rassifizierte Verzerrungen in KI-Systemen: männliche Kandidaten erhalten höhere Hiring-Scores als gleich qualifizierte Frauen (p=0.001)",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt TECHNIK-Kriterium durch KI_Sonstige (algorithmische Entscheidungssysteme in Hiring, Healthcare, Law Enforcement). SOZIAL-Kriterium erfüllt durch: Bias_Ungleichheit (Diskriminierung, Bias in automatisierten Systemen), Gender (Unterrepräsentation von Frauen in AI), Diversitaet (marginali"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "disagree",
          "divergence_pattern": "Keyword-Inklusion",
          "divergence_justification": "Das LLM dehnt 'AI_Literacies' und 'Generative_KI' auf einen rein theoretisch-philosophischen Artikel über feministische Perspektiven aus, obwohl dieser keine praktischen Kompetenzrahmen oder konkrete AI-Anwendungen für Fachkräfte behandelt. Der Mensch erkennt korrekt, dass ethische Reflexion nicht automatisch in den Kompetenzkatalog gehört.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Participatory Design in AI Development",
        "Feminist Ethics in AI",
        "Gender Bias in Algorithmic Decision-Making",
        "Facial Recognition Disparities",
        "Algorithmic Bias",
        "Explainable AI (XAI)",
        "AI Team Diversity and Representation",
        "Intersectional Feminism in Technology"
      ],
      "knowledge_summary": "Empirische Analyse zeigt signifikante geschlechtsspezifische und rassifizierte Verzerrungen in KI-Systemen: männliche Kandidaten erhalten höhere Hiring-Scores als gleich qualifizierte Frauen (p=0.001), und Black Women erleben 34,5% Fehlerrate in Facial-Recognition-Systemen gegenüber 1,2% bei White Men. Allerdings zeigt sich eine Lücke zwischen Awareness für Fairness-Probleme und deren praktischer ",
      "featured": {
        "why": "Semantische Expansion: LLM liest nur 'AI Literacy', Human erkennt feministisch-ethischen Kern",
        "stance_highlight": "limits"
      }
    },
    {
      "id": "JHRVDXSD",
      "stem": "Ahn_2025_Artificial_Intelligence_(AI)_literacy_for_social",
      "title": "Artificial Intelligence (AI) Literacy for Social Work: Implications for Core Competencies",
      "author_year": "Ahn (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI-Literatur sollte in alle neun Kernkompetenzen der Sozialen Arbeit integriert werden, um Fachkräfte in die Lage zu versetzen, AI-getriebene Ungleichheiten zu adressieren, ethische Governance von AI-",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 85
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "Soziale_Arbeit"
            ],
            "reasoning": "Das Paper behandelt explizit AI Literacy im Kontext von Social Work und deren Implikationen für Kernkompetenzen. Dies erfüllt beide Bedingungen: (1) TECHNIK: AI_Literacies = Ja (KI-Kompetenzen für Sozialarbeit), (2) SOZIAL: Soziale_Arbeit = Ja (direkter Bezug zu sozialarbeiterischer Praxis und Ausbi"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Social Work Core Competencies Integration",
        "AI Literacy",
        "Algorithmic Bias",
        "Predictive Analytics in Social Services",
        "Ethical AI Governance",
        "Digital Divide",
        "Human-Centered AI Practice"
      ],
      "knowledge_summary": "AI-Literatur sollte in alle neun Kernkompetenzen der Sozialen Arbeit integriert werden, um Fachkräfte in die Lage zu versetzen, AI-getriebene Ungleichheiten zu adressieren, ethische Governance von AI-Systemen zu fördern und AI-Anwendungen verantwortungsvoll in die Praxis zu integrieren."
    },
    {
      "id": "DJEVSR8D",
      "stem": "Ahrweiler_2025_AI_FORA_–_Artificial_Intelligence_for_Assessment",
      "title": "AI FORA – Artificial Intelligence for Assessment: Fairness bei der Verteilung öffentlicher sozialer Leistungen",
      "author_year": "Ahrweiler (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Gerechtigkeitskriterien für staatliche Leistungen sind kultur- und kontextabhängig, daher reicht ein standardisiertes KI-System nicht aus. Stattdessen sind flexible, adaptive Systeme notwendig, die un",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper adressiert algorithmische Entscheidungssysteme (KI_Sonstige: Ja) in der Sozialleistungsverwaltung (Soziale_Arbeit: Ja). Zentrale Themen sind Fairness bei der Ressourcenverteilung (Fairness: Ja), kulturelle/kontextuelle Ungleichheiten (Bias_Ungleichheit: Ja) und inklusive Partizipation von marg"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness in Social Services",
        "Automated Decision-Making in Welfare Systems",
        "Algorithmic Bias in Public Administration",
        "Context-Dependent Fairness Criteria",
        "Cross-Cultural AI Governance",
        "Participatory Artificial Intelligence",
        "Vulnerable Population Inclusion"
      ],
      "knowledge_summary": "Gerechtigkeitskriterien für staatliche Leistungen sind kultur- und kontextabhängig, daher reicht ein standardisiertes KI-System nicht aus. Stattdessen sind flexible, adaptive Systeme notwendig, die unter Einbeziehung aller gesellschaftlichen Akteure, besonders vulnerabler Gruppen, entwickelt werden."
    },
    {
      "id": "PYN6HB3E",
      "stem": "Alam_2025_Social_work_in_the_age_of_artificial_intelligence",
      "title": "Social work in the age of artificial intelligence: A rights-based framework for evidence-based practice through social psychology, group dynamics, and institutional analysis",
      "author_year": "Alam (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Ein umfassendes rechtsbasiertes Framework wurde entwickelt, das vier Komponenten integriert (Ethical AI Literacy, Participatory Governance, Continuous Impact Assessment, Community-Centered Advocacy) u",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige) durch Framework für AI-Integration in Sozialarbeit; SOZIAL_OK (Soziale_Arbeit, Bias_Ungleichheit, Diversitaet, Fairness) durch expliziten Fokus auf vulnerable Populationen, Gerechtigkeit, Menschenrechte und ethische Implikation"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Discrimination in Social Services",
        "Rights-Based Framework for AI Governance",
        "Automation Bias and Algorithm Aversion",
        "Epistemic Injustice in Algorithmic Systems",
        "Participatory Governance in Digital Social Services",
        "Differential Impact Assessment",
        "Ethical AI Literacy"
      ],
      "knowledge_summary": "Ein umfassendes rechtsbasiertes Framework wurde entwickelt, das vier Komponenten integriert (Ethical AI Literacy, Participatory Governance, Continuous Impact Assessment, Community-Centered Advocacy) und AI-Integration in der sozialen Arbeit mit sozialen Psychologie-, Gruppendynamik- und institutionellen Erkenntnissen verbindet."
    },
    {
      "id": "QM6L6XLZ",
      "stem": "Alliance_2024_Incubating",
      "title": "Incubating Feminist AI: Executive Summary 2021-2024",
      "author_year": "Alliance",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Das f&a+i>r-Netzwerk hat über 4 Jahre eine praktische Bewegung für Feminist AI geschaffen, die 9 eigenständige Papers, 4 Paper-zu-Prototypen und 5 vollständig umgesetzte Prototype-zu-Pilot Projekte in",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK (KI_Sonstige: Bias-Detection in NLP, algorithmische Systeme im sozialen Kontext) und SOZIAL (Feministisch: explizit feminist AI; Gender: gender-based violence; Soziale_Arbeit: praktische Anwendungen bei Gewalt und Justiz; Bias_Ungleichheit: Bias-Detection"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "Design Justice Principles",
        "Regional Hub-Based Implementation",
        "Feminist AI",
        "Algorithmic Bias Assessment",
        "Capacity Building in AI Literacy",
        "Paper-to-Prototype-to-Pilot Methodology",
        "Intersectional Analysis in AI Systems",
        "Gender-Based Violence Intervention Technologies"
      ],
      "knowledge_summary": "Das f&a+i>r-Netzwerk hat über 4 Jahre eine praktische Bewegung für Feminist AI geschaffen, die 9 eigenständige Papers, 4 Paper-zu-Prototypen und 5 vollständig umgesetzte Prototype-zu-Pilot Projekte in 10 Ländern generiert hat, mit nachgewiesenen Auswirkungen auf Politik, technologische Tools und Geschlechtergerechtigkeit."
    },
    {
      "id": "5F7D9PEB",
      "stem": "Alvarez_2024_Policy_advice_and_best_practices_on_bias_and",
      "title": "Policy advice and best practices on bias and fairness in AI",
      "author_year": "Alvarez (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "The paper provides a comprehensive bird's-eye overview of fair-AI state-of-the-art and identifies critical gaps including the need for multi-stakeholder participatory design, intersectionality awarene",
          "stage3_completeness": 88,
          "stage3_correctness": 94,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Das Paper behandelt algorithmische Fairness und Bias-Mitigation mit Policy-Fokus (KI_Sonstige: Ja, Fairness: Ja). Es erfüllt die TECHNIK-Bedingung. Allerdings fehlt der explizite Bezug zu Sozialer Arbeit oder marginalisierten Gruppen im Kontext sozialer Dienste. Intersektionale Diskriminierung wird "
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Causal Fairness Reasoning",
        "Multi-Stakeholder Participatory Design",
        "Intersectionality in AI Systems",
        "Algorithmic Bias Mitigation",
        "Representation Bias",
        "AI Literacy for Policy and Practice",
        "Bias Monitoring and Distribution Shift"
      ],
      "knowledge_summary": "The paper provides a comprehensive bird's-eye overview of fair-AI state-of-the-art and identifies critical gaps including the need for multi-stakeholder participatory design, intersectionality awareness, causal approaches to bias, knowledge-informed models, and bias monitoring across different domains and data types."
    },
    {
      "id": "LXDG4KQK",
      "stem": "Amnesty International_2024_Coded_injustice_Surveillance_and_discrimination",
      "title": "Coded injustice: Surveillance and discrimination in Denmark's automated welfare state",
      "author_year": "Amnesty International",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 5,
          "stage1_key_finding": "Dänemarks automatisiertes Wohlfahrtssystem nutzt diskriminierende Algorithmen zur Betrugserkennung, die systematisch gegen marginalisierte Gruppen (Migranten, Menschen mit Behinderungen, Frauen, Arme)",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Report untersucht substantiell algorithmische Entscheidungssysteme (KI_Sonstige: Ja) im Wohlfahrtsbereich, direkt relevant für Soziale Arbeit (Ja). Zentral sind Bias/Diskriminierung gegen vulnerable Gruppen (Bias_Ungleichheit: Ja), Diversität marginalisierter Bevölkerungen (Diversitaet: Ja) und Fair"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Corporate Accountability in Algorithmic Systems",
        "Automated Welfare Systems",
        "Digital Exclusion in Welfare Access",
        "Intersectional Technology Harms",
        "Algorithmic Fairness Testing",
        "Mass Surveillance in Social Services",
        "Automation Bias in Casework",
        "Algorithmic Discrimination"
      ],
      "knowledge_summary": "Dänemarks automatisiertes Wohlfahrtssystem nutzt diskriminierende Algorithmen zur Betrugserkennung, die systematisch gegen marginalisierte Gruppen (Migranten, Menschen mit Behinderungen, Frauen, Arme) diskriminieren und fundamentale Menschenrechte verletzen, während gleichzeitig Transparenz und Rechenschaftspflicht völlig fehlen."
    },
    {
      "id": "SQ38TTWQ",
      "stem": "An_2025_Measuring_gender_and_racial_biases_in_large",
      "title": "Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation",
      "author_year": "An (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "LLMs bevorzugen systematisch weibliche Kandidaten und benachteiligen schwarze männliche Kandidaten bei ansonsten ähnlichen Qualifikationen. Diese Verzerrungen führen zu Unterschieden in der Einstellun",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper kombiniert Technik (Generative_KI: Evaluation von LLMs) mit mehreren sozialen Kategorien. Explizit intersektionale Analyse (Feministisch: Crenshaw'sche Intersektionalität), Gender- und Diversitätsfokus, algorithmischer Bias in Hiring-Kontexten (Bias_Ungleichheit, Fairness). Erfüllt beide Bedin"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Labor Market Discrimination via Automated Systems",
        "Racial Bias in Hiring Algorithms",
        "Gender Bias in Large Language Models",
        "Large Language Model Bias Measurement",
        "Algorithmic Fairness in High-Stakes Decision Making",
        "Synthetic Resume Testing",
        "Debiasing Mitigation Paradox",
        "Intersectional Algorithmic Bias"
      ],
      "knowledge_summary": "LLMs bevorzugen systematisch weibliche Kandidaten und benachteiligen schwarze männliche Kandidaten bei ansonsten ähnlichen Qualifikationen. Diese Verzerrungen führen zu Unterschieden in der Einstellungswahrscheinlichkeit von 1-3 Prozentpunkten und sind konsistent über verschiedene Modelle und Jobpositionen hinweg."
    },
    {
      "id": "SY8LNID7",
      "stem": "Arias López_2023_Digital_literacy_as_a_new_determinant_of_health_A",
      "title": "Digital literacy as a new determinant of health: A scoping review",
      "author_year": "Arias López (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Digital health literacy is identified as a 'super social determinant of health' that significantly impacts health outcomes including self-management, quality of life, mental health, and disease contro",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Das Paper ist ein Scoping Review zu Digital Health Literacy im Gesundheitssektor. Obwohl es marginale Überschneidungen mit AI_Literacies und Bias/Ungleichheit hat, fehlt jede substanzielle Verknüpfung zu KI-Technologien oder sozialer Arbeit. Es behandelt digitale Gesundheitskompetenz allgemein, nich"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Digital Health Literacy",
        "Digital Divide",
        "Super Social Determinant of Health",
        "Intersectional Digital Health Disparities",
        "Health Equity in Digital Access",
        "eHealth Literacy Scale (eHEALS)",
        "Digital Health Intervention"
      ],
      "knowledge_summary": "Digital health literacy is identified as a 'super social determinant of health' that significantly impacts health outcomes including self-management, quality of life, mental health, and disease control. Effective interventions include education/training and social support, though evidence remains limited."
    },
    {
      "id": "Arias_López_2023_Digital",
      "stem": "Arias_López_2023_Digital",
      "title": "Arias_López_2023_Digital",
      "author_year": "Arias",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": false
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Die WHO etabliert vier strategische Ziele zur globalen digitalen Gesundheitstransformation: (1) Förderung globaler Kollaboration und Wissenstransfer, (2) Umsetzung nationaler Strategien, (3) Stärkung ",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {}
      },
      "concepts": [
        "Stakeholder Engagement in Digital Health Implementation",
        "Health System Equity",
        "People-Centred Health Systems",
        "Digital Health Literacy",
        "Digital Health Governance",
        "Population Health Management",
        "Gender-Balanced Health Workforce Development",
        "Digital Divide in Healthcare"
      ],
      "knowledge_summary": "Die WHO etabliert vier strategische Ziele zur globalen digitalen Gesundheitstransformation: (1) Förderung globaler Kollaboration und Wissenstransfer, (2) Umsetzung nationaler Strategien, (3) Stärkung von Governance-Strukturen, (4) Förderung personenzentrierter Gesundheitssysteme. Ein intersektoraler, integrativer Ansatz mit Fokus auf Equity, Gender-Balance und Kapazitätsaufbau ist erforderlich."
    },
    {
      "id": "AIGLDZ4C",
      "stem": "Articulate_2025_How_to_Create_Inclusive_AI_Images_A_Guide_to",
      "title": "How to Create Inclusive AI Images: A Guide to Bias-Free Prompting",
      "author_year": "Articulate",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": false,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI-Bildgeneratoren reproduzieren durch unverzerrte Trainingsdaten bedingte stereotype Standardausgaben (weiß, männlich, jung, schlank). Durch spezifische, intentionale Prompt-Engineering-Techniken kön",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt TECHNIK-Kriterium (Generative_KI + Prompting + AI_Literacies) und SOZIAL-Kriterium (Bias_Ungleichheit + Gender + Diversitaet + Fairness). Substantieller Fokus auf Prompt-Engineering-Strategien zur Reduktion von Bias in generativen KI-Bildern. Adressiert stereotype Darstellungen und prä"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Inclusive Prompt Engineering",
        "Representational Fairness",
        "Algorithmic Bias in Image Generation",
        "Training Data Bias",
        "AI Literacy for Content Creators",
        "Identity-Conscious Specification",
        "Stereotype Defaults in Generative AI"
      ],
      "knowledge_summary": "AI-Bildgeneratoren reproduzieren durch unverzerrte Trainingsdaten bedingte stereotype Standardausgaben (weiß, männlich, jung, schlank). Durch spezifische, intentionale Prompt-Engineering-Techniken können Nutzer diese Defaults unterbrechen und inklusivere Darstellungen erreichen."
    },
    {
      "id": "5UAHQESQ",
      "stem": "Asseri et al._2025_Prompt_Engineering_Techniques_for_Mitigating",
      "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review",
      "author_year": "Basseri (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Fünf primäre Prompt-Engineering-Ansätze identifiziert (kulturelle Prompting, affektive Priming, Self-Debiasing, strukturierte Multi-Step-Pipelines, parameter-optimierte kontinuierliche Prompts); struk",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + Prompting substantiell fokussiert) und SOZIAL (Bias_Ungleichheit + Diversitaet + Fairness adressieren kulturelle und intersektionale Diskriminierung gegen Araber und Muslime). Systematischer Review von Prompt-Engineering zur Bias-Mitigation i"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Prompt Engineering for Bias Mitigation",
        "Cultural Bias in Large Language Models",
        "Algorithmic Fairness and Performance Trade-offs",
        "Bias Evaluation Metrics and Heterogeneity",
        "Layered Bias in Neural Representations",
        "Orientalism in AI Systems",
        "Inclusive AI Development and Community Co-Creation"
      ],
      "knowledge_summary": "Fünf primäre Prompt-Engineering-Ansätze identifiziert (kulturelle Prompting, affektive Priming, Self-Debiasing, strukturierte Multi-Step-Pipelines, parameter-optimierte kontinuierliche Prompts); strukturierte Multi-Step-Pipelines zeigen höchste Effektivität (bis zu 87,7% Bias-Reduktion), während kulturelles Prompting bessere Zugänglichkeit bietet (71-81% Verbesserung der kulturellen Ausrichtung); "
    },
    {
      "id": "5UAHQESQ",
      "stem": "Asseri_2025_Prompt_engineering_techniques_for_mitigating",
      "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review",
      "author_year": "Basseri (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Fünf primäre Prompt-Engineering-Ansätze identifiziert (kulturelles Prompting, affektive Priming, Self-Debiasing, strukturierte Multi-Step-Pipelines, parameteroptimierte kontinuierliche Prompts); struk",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + Prompting substantiell fokussiert) und SOZIAL (Bias_Ungleichheit + Diversitaet + Fairness adressieren kulturelle und intersektionale Diskriminierung gegen Araber und Muslime). Systematischer Review von Prompt-Engineering zur Bias-Mitigation i"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Relational Ethics in AI",
        "Cultural Bias in Large Language Models",
        "AI Literacy for Practitioners",
        "Prompt Engineering Techniques",
        "Orientalism in AI Systems",
        "Layered Bias in Transformer Architectures",
        "Algorithmic Fairness Metrics"
      ],
      "knowledge_summary": "Fünf primäre Prompt-Engineering-Ansätze identifiziert (kulturelles Prompting, affektive Priming, Self-Debiasing, strukturierte Multi-Step-Pipelines, parameteroptimierte kontinuierliche Prompts); strukturierte Multi-Step-Pipelines zeigen höchste Effektivität (bis 87,7% Bias-Reduktion), während kulturelles Prompting beste Zugänglichkeit bietet (71-81% Verbesserung). Kritische Forschungslücke: Nur 8 "
    },
    {
      "id": "C485NKYA",
      "stem": "Attard-Frost_2025_Countergovernance",
      "title": "AI Countergovernance: Lessons Learned from Canada and Paris",
      "author_year": "Attard-Frost (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 5,
          "stage1_key_finding": "Das kanadische AIDA weist fünf kritische Regulierungslücken auf: unzureichende Definition von 'High-Impact Systems', zu enge Regulierungsweise auf nur High-Impact-Systeme, fehlende Anwendung auf öffen",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.82,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Feministisch"
            ],
            "reasoning": "Paper behandelt kritische AI Literacies (TECHNIK) und thematisiert strukturelle Ungleichheiten bezüglich Rasse, Geschlecht und Arbeit (SOZIAL). Expliziter Fokus auf Bias/Ungleichheit und Gender. Kritische Perspektive auf Strukturen deutet auf feministische Ansätze hin. Beide Bedingungen erfüllt → In"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Multi-Stakeholder Policy Engagement",
        "Risk-Tiered Regulation",
        "Algorithmic Harm Definition",
        "Regulatory Gap in Public Sector AI",
        "Regulatory Independence",
        "Algorithmic Bias in Biometric Systems",
        "Algorithmic Accountability in Public Services"
      ],
      "knowledge_summary": "Das kanadische AIDA weist fünf kritische Regulierungslücken auf: unzureichende Definition von 'High-Impact Systems', zu enge Regulierungsweise auf nur High-Impact-Systeme, fehlende Anwendung auf öffentliche Institutionen, zu enge Definition von Schaden (nur Einzelpersonen statt Gemeinschaften), und problematische Regulatory Independence des Commissioners."
    },
    {
      "id": "38E5FZDV",
      "stem": "Bai_2025_Explicitly_unbiased_large_language_models_still",
      "title": "Explicitly unbiased large language models still form biased associations",
      "author_year": "Bai (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Alle 8 untersuchten, als unbiased geltenden LLMs zeigen weitverbreitete stereotypische Assoziationen und subtile Diskriminierungen in Entscheidungen über 4 soziale Kategorien hinweg (Rasse, Geschlecht",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper untersucht implizite Biase in LLMs durch neuartige Evaluationsmethoden (LLM-WAT, LLM-RDT). Erfüllt TECHNIK-Kriterium (Generative_KI=Ja) und SOZIAL-Kriterien (Bias_Ungleichheit=Ja, Fairness=Ja). Fokus auf stereotype Assoziationen und subtile Diskriminierung in State-of-the-Art Modellen. Kein ex"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Implicit Bias in Large Language Models",
        "Bias Benchmark Insufficiency",
        "Algorithmic Fairness in Generative AI",
        "Prompt-Based Bias Measurement",
        "Stereotype Association",
        "Discrimination in Contextual Decision-Making",
        "Value Alignment vs. Implicit Discrimination"
      ],
      "knowledge_summary": "Alle 8 untersuchten, als unbiased geltenden LLMs zeigen weitverbreitete stereotypische Assoziationen und subtile Diskriminierungen in Entscheidungen über 4 soziale Kategorien hinweg (Rasse, Geschlecht, Religion, Gesundheit), die an 21 gesellschaftliche Stereotype gebunden sind."
    },
    {
      "id": "L48P8FBG",
      "stem": "Baker_2025_Artificial_intelligence_in_social_work_An_EPIC",
      "title": "Artificial intelligence in social work: An EPIC model for practice",
      "author_year": "Goldkind (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Ein vierteiliges EPIC-Modell (Ethics and Justice, Policy Development and Advocacy, Intersectoral Collaboration, Community Engagement and Empowerment) wird als strukturierter Ansatz für die ethische In",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Inklusionskriterien: TECHNIK (AI_Literacies, KI_Sonstige) durch EPIC-Framework für KI-Integration und Bias-Mitigation; SOZIAL (Soziale_Arbeit, Bias_Ungleichheit, Fairness) durch direkten Bezug zu Sozialer Arbeit, Ethik, Transparenz und Fairness-Thematisierung. Substantielle Behan"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt die Kategorie 'KI_Sonstige' übermäßig aus, indem es jeden Bezug zu KI in Soziale Arbeit als relevante Technik-Kategorie wertet, ohne zu prüfen, ob KI das zentrale Forschungsthema oder nur kontextueller Bezug ist. Die Human-Exclusion deutet darauf hin, dass das Paper primär ein Soziale-Arbeit-Paper mit peripherem KI-Bezug ist.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Predictive Risk Assessment in Social Services",
        "Algorithmic Bias in Social Work",
        "First Nations Data Sovereignty",
        "Human-Technology Dual Model",
        "EPIC Model for AI Integration",
        "AI Literacy in Professional Standards",
        "Decolonization of AI Systems",
        "Intersectional Accuracy Disparities"
      ],
      "knowledge_summary": "Ein vierteiliges EPIC-Modell (Ethics and Justice, Policy Development and Advocacy, Intersectoral Collaboration, Community Engagement and Empowerment) wird als strukturierter Ansatz für die ethische Integration von KI in der Sozialen Arbeit vorgeschlagen."
    },
    {
      "id": "GCQ8J9XF",
      "stem": "Barman_2024_Beyond",
      "title": "Beyond transparency and explainability: On the need for adequate and contextualized user guidelines for LLM use",
      "author_year": "Barman (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": false,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Generative KI-Tools bieten erhebliche Produktivitätspotenziale für Regierungsbehörden, erfordern aber umfassende Governance-Frameworks, Datenschutzkontrollen und ethische Überlegungen, insbesondere be",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 92
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Diversitaet"
            ],
            "reasoning": "Paper behandelt substantiell AI_Literacies (user guidelines und training für LLM-Nutzung), Generative_KI (LLM-fokussiert), Prompting (diversity-sensitive prompting techniques explizit genannt) und adressiert Bias_Ungleichheit sowie Diversität (diversity-sensitive approaches). Erfüllt TECHNIK (3 Kate"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Cultural Bias in AI Design",
        "Privacy Impact Assessment for AI Systems",
        "AI Governance Frameworks",
        "LLM Hallucinations",
        "Algorithmic Bias in Government Services",
        "AI Literacy and Staff Training",
        "Stakeholder Consultation with Indigenous Populations"
      ],
      "knowledge_summary": "Generative KI-Tools bieten erhebliche Produktivitätspotenziale für Regierungsbehörden, erfordern aber umfassende Governance-Frameworks, Datenschutzkontrollen und ethische Überlegungen, insbesondere bezüglich Verzerrungen, die marginalisierte Gruppen betreffen können."
    },
    {
      "id": "GCQ8J9XF",
      "stem": "Barman_2024_Beyond_transparency_and_explainability_On_the",
      "title": "Beyond transparency and explainability: On the need for adequate and contextualized user guidelines for LLM use",
      "author_year": "Barman (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Aktuelle Fokussierung auf Transparenz und Explainability reicht nicht aus; stattdessen sollten kontextspezifische, praktische Nutzerrichtlinien zur Schulung, Risikominderung und ethischen Verwendung v",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Diversitaet"
            ],
            "reasoning": "Paper behandelt substantiell AI_Literacies (user guidelines und training für LLM-Nutzung), Generative_KI (LLM-fokussiert), Prompting (diversity-sensitive prompting techniques explizit genannt) und adressiert Bias_Ungleichheit sowie Diversität (diversity-sensitive approaches). Erfüllt TECHNIK (3 Kate"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Misinformation Risks in LLM Outputs",
        "AI Literacy",
        "User-Centered LLM Guidelines",
        "Algorithmic Bias in Large Language Models",
        "Explainable Artificial Intelligence (XAI)",
        "Contextual AI Governance",
        "Responsible AI Use",
        "Prompt Engineering"
      ],
      "knowledge_summary": "Aktuelle Fokussierung auf Transparenz und Explainability reicht nicht aus; stattdessen sollten kontextspezifische, praktische Nutzerrichtlinien zur Schulung, Risikominderung und ethischen Verwendung von LLMs im Vordergrund stehen, da Nutzer primär praktische Anleitung statt technischer Erklärungen benötigen."
    },
    {
      "id": "VP6SXQHY",
      "stem": "Benlian_2025_The_AI_literacy_development_canvas_Assessing_and",
      "title": "The AI literacy development canvas: Assessing and building AI literacy in organizations",
      "author_year": "Benlian (10/2025)",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI-Literacy muss multidimensional (konzeptuell, ethisch, praktisch) und rollenspezifisch sein. Die vorgestellte AI Literacy Assessment Matrix und AI Literacy Development Canvas bieten Organisationen e",
          "stage3_completeness": 92,
          "stage3_correctness": 96,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Titel deutet auf AI Literacy Framework hin - klare AI_Literacies-Kategorie. Kein Abstract vorhanden; basierend auf Titel keine Indikation für soziale Dimensionen (Bias, Ungleichheit, Gender, Fairness, Soziale Arbeit). Paper erfüllt nur Technik-Kriterium, nicht Sozial-Kriterium. EXCLUDE nach strikter"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Role-Specific AI Competencies",
        "Generative AI Adoption",
        "AI Literacy",
        "Algorithmic Bias",
        "Organizational Learning",
        "AI Governance Compliance"
      ],
      "knowledge_summary": "AI-Literacy muss multidimensional (konzeptuell, ethisch, praktisch) und rollenspezifisch sein. Die vorgestellte AI Literacy Assessment Matrix und AI Literacy Development Canvas bieten Organisationen ein strukturiertes Rahmenwerk zur Diagnose von Kompetenzlücken und zur Umsetzung gezielter Upskilling-Programme auf allen Organisationsebenen."
    },
    {
      "id": "J9IZDVTW",
      "stem": "Biagini_2024_Less_knowledge,_more_trust_Exploring_potentially",
      "title": "Less knowledge, more trust? Exploring potentially uncritical attitudes towards AI in higher education",
      "author_year": "Biagini (Juli 24, 2024)",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Teilnehmende mit geringem KI-Wissen zeigen paradoxerweise höheres Vertrauen in KI-Fähigkeiten über alle getesteten Aufgaben hinweg, was auf den Dunning-Kruger-Effekt hindeutet und die Notwendigkeit um",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Paper untersucht AI Literacy bei Doktoranden auf vier Dimensionen (kognitiv, operativ, kritisch, ethisch). Dies erfüllt AI_Literacies-Kriterium substantiell. Jedoch adressiert das Paper keinen direkten Bezug zu Sozialer Arbeit, behandelt Bias/Ungleichheit nicht fokussiert und hat keinen Gender-, Div"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness and Equity",
        "AI Ethics Education",
        "AI-Related Decision Automation",
        "Dunning-Kruger Effect in AI",
        "Digital Inequality and Data Privacy",
        "Overconfidence Bias in Technology Adoption",
        "Critical AI Literacy"
      ],
      "knowledge_summary": "Teilnehmende mit geringem KI-Wissen zeigen paradoxerweise höheres Vertrauen in KI-Fähigkeiten über alle getesteten Aufgaben hinweg, was auf den Dunning-Kruger-Effekt hindeutet und die Notwendigkeit umfassender KI-Literalität betont, die ethische und kritische Reflexion einschließt."
    },
    {
      "id": "9832ZJB7",
      "stem": "Biegelbauer_2023_Leitfaden_Digitale_Verwaltung_und_Ethik",
      "title": "Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden für KI in der Verwaltung, Version 1.0",
      "author_year": "Biegelbauer (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Ein menschenzentriertes KI-Governance-Modell für die öffentliche Verwaltung, das Rechtmäßigkeit als Minimalbedingung, aber ethische Prinzipien (Transparenz, Fairness, Sicherheit, Inklusion, Rechenscha",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Der Leitfaden adressiert AI Literacy und KI-Risiken in der Verwaltung substantiell. Das Konzept von 'automation bias' ist technisch orientiert (KI-Versagen), nicht sozial (Diskriminierung von Gruppen). Fehlender Bezug zu Sozialer Arbeit, Gender, Diversität oder kritischen sozialen Perspektiven führt"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "AI Governance in Public Administration",
        "AI Literacy",
        "Ethics by Design",
        "AI Impact Assessment",
        "Inclusive AI Design",
        "Digital Sovereignty"
      ],
      "knowledge_summary": "Ein menschenzentriertes KI-Governance-Modell für die öffentliche Verwaltung, das Rechtmäßigkeit als Minimalbedingung, aber ethische Prinzipien (Transparenz, Fairness, Sicherheit, Inklusion, Rechenschaftspflicht) als zentral für vertrauenswürdige KI positioniert."
    },
    {
      "id": "U9ACKGB4",
      "stem": "Bisconti_2024_A_formal_account_of_AI_trustworthiness_Connecting",
      "title": "A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization",
      "author_year": "Bisconti (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Vertrauenswürdigkeit ist ein relationales Attribut, das durch die Übereinstimmung zwischen beobachtetem Systemverhalten und mentalen Modellen von Beobachtern entsteht. Die Arbeit formalisiert dies dur",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "KI_Sonstige"
            ],
            "reasoning": "Das Paper behandelt AI trustworthiness aus theoretischer, KI-ethischer Perspektive (KI_Sonstige: Ja). Es bietet jedoch keinen Bezug zu sozialen Anwendungsdomänen, Bias, Gender, Diversität, Fairness oder Soziale Arbeit. Es ist eine rein konzeptionelle Formalisierung von Vertrauen in KI-Systemen ohne "
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "AI Trustworthiness",
        "Agency Locus",
        "Intrinsic vs. Perceived Trustworthiness",
        "AI Transparency",
        "Human Oversight in Autonomous Systems",
        "Stakeholder Heterogeneity",
        "Trust Propensity"
      ],
      "knowledge_summary": "Vertrauenswürdigkeit ist ein relationales Attribut, das durch die Übereinstimmung zwischen beobachtetem Systemverhalten und mentalen Modellen von Beobachtern entsteht. Die Arbeit formalisiert dies durch mathematische Funktionen, die Transparenz, Agency Locus und menschliche Überwachung als zentrale Faktoren der wahrgenommenen Vertrauenswürdigkeit identifizieren."
    },
    {
      "id": "L48P8FBG",
      "stem": "Boetto_2025_Artificial_Intelligence_in_Social_Work_An_EPIC",
      "title": "Artificial intelligence in social work: An EPIC model for practice",
      "author_year": "Goldkind (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Das EPIC-Modell (Ethics and Justice, Policy Development and Advocacy, Intersectoral Collaboration, Community Engagement and Empowerment) bietet einen strukturierten Rahmen für die ethische Integration",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Inklusionskriterien: TECHNIK (AI_Literacies, KI_Sonstige) durch EPIC-Framework für KI-Integration und Bias-Mitigation; SOZIAL (Soziale_Arbeit, Bias_Ungleichheit, Fairness) durch direkten Bezug zu Sozialer Arbeit, Ethik, Transparenz und Fairness-Thematisierung. Substantielle Behan"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt die Kategorie 'KI_Sonstige' übermäßig aus, indem es jeden Bezug zu KI in Soziale Arbeit als relevante Technik-Kategorie wertet, ohne zu prüfen, ob KI das zentrale Forschungsthema oder nur kontextueller Bezug ist. Die Human-Exclusion deutet darauf hin, dass das Paper primär ein Soziale-Arbeit-Paper mit peripherem KI-Bezug ist.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Predictive Risk Modeling in Child Protection",
        "Community-Centered AI Development",
        "Data Sovereignty and Decolonization",
        "Human-in-the-Loop Decision Support",
        "Algorithmic Bias in Social Work",
        "EPIC Model for AI Integration",
        "AI Literacy in Social Work Education"
      ],
      "knowledge_summary": "Das EPIC-Modell (Ethics and Justice, Policy Development and Advocacy, Intersectoral Collaboration, Community Engagement and Empowerment) bietet einen strukturierten Rahmen für die ethische Integration von KI in die Soziale Arbeit unter Berücksichtigung von Bias, Gerechtigkeit und Empowerment marginalisierter Gruppen."
    },
    {
      "id": "7QQV7R9S",
      "stem": "British Association of Social Workers_2025_Generative_AI_&_social_work_practice_guidance",
      "title": "Generative AI & social work practice guidance",
      "author_year": "British Association of Social Workers",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 4,
          "stage1_key_finding": "GenAI bietet Potenzial zur Reduzierung administrativer Lasten in der Sozialen Arbeit, erfordert aber strenge Überprüfung durch professionelle Urteile, um Risiken wie algorithmischen Bias, Halluzinatio",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Soziale_Arbeit",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + AI_Literacies durch Praxis-Guidance) und SOZIAL (Soziale_Arbeit + Bias_Ungleichheit durch Warnung vor racist/sexist assumptions). Es ist eine direkte Praxis-Orientierung für Sozialarbeiter:innen mit ethischen Reflexionspunkten und adressi"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "AI Hallucinations in Clinical Documentation",
        "GDPR Compliance in Generative AI",
        "Administrative Burden Reduction via Generative AI",
        "Transparency and Accountability in AI-Assisted Social Care",
        "Professional Judgment in AI-Augmented Practice",
        "Employer Responsibility for AI Governance",
        "Algorithmic Bias in Social Work",
        "Equalities Impact Assessment for AI Deployment"
      ],
      "knowledge_summary": "GenAI bietet Potenzial zur Reduzierung administrativer Lasten in der Sozialen Arbeit, erfordert aber strenge Überprüfung durch professionelle Urteile, um Risiken wie algorithmischen Bias, Halluzinationen und GDPR-Verstöße zu vermeiden."
    },
    {
      "id": "HJ7BHX8J",
      "stem": "Browne_2024_Engineers_on_responsibility_feminist_approaches",
      "title": "Engineers on responsibility: feminist approaches to who's responsible for ethical AI",
      "author_year": "Browne (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Verantwortung sollte nicht als statisches individuelles Merkmal verstanden werden, sondern als 'response-ability' – eine dynamische, kollektive Fähigkeit zur Rechenschaft, die durch organisationale Ku",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "KI_Sonstige",
              "Feministisch"
            ],
            "reasoning": "Paper kombiniert feministische Theorie (Browne et al.) mit KI-Verantwortungsdiskurs. KI_Sonstige=Ja (KI-Entwicklung, Verantwortungsrahmen), Feministisch=Ja (explizit feministische politische Theorie). Jedoch: Soziale_Arbeit=Nein (kein Bezug zu SW-Praxis/Zielgruppen), Bias_Ungleichheit=Nein (nicht su"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Black Box Problem and Algorithmic Explainability",
        "Algorithmic Accountability in Fluid AI Ecosystems",
        "Response-Ability as Relational Responsibility",
        "Feminist Care Ethics in AI Development",
        "Algorithmic Bias and Discriminatory AI Outcomes",
        "Organizational Culture and Ethical AI Implementation",
        "Feminist Science and Technology Studies (Feminist STS) Approach",
        "Gendered and Racialized Labor in AI Systems"
      ],
      "knowledge_summary": "Verantwortung sollte nicht als statisches individuelles Merkmal verstanden werden, sondern als 'response-ability' – eine dynamische, kollektive Fähigkeit zur Rechenschaft, die durch organisationale Kulturen ermöglicht wird, die Care- und Wartungsarbeit wertschätzen und strukturelle Barrieren abbaut."
    },
    {
      "id": "V4GTLMED",
      "stem": "Browne_2024_Tech_workers'_perspectives_on_ethical_issues_in",
      "title": "Tech workers' perspectives on ethical issues in AI development: Foregrounding feminist approaches",
      "author_year": "Browne (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Der Begriff 'Bias' verursacht Verwirrung unter Tech-Workern und kann seine beabsichtigte ethische Funktion nicht erfüllen; Tech-Worker sehen keinen direkten Zusammenhang zwischen DEI-Agenden und KI-En",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper kombiniert TECHNIK (AI_Literacies: KI-Ethik-Verständnis; KI_Sonstige: algorithmische Bias) mit SOZIAL (Bias_Ungleichheit: Analyse von Bias-Konzepten; Gender + Diversität + Feministisch: expliziter feministischer Ansatz; Fairness: Design Justice). Empirische Studie mit Tech-Worker-Perspektive u"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness in Practice",
        "Data Positionality and Situated Knowledges",
        "AI System Lifecycle Management",
        "Tech Worker Ethical Knowledge Gaps",
        "Algorithmic Bias Conceptualization",
        "Diversity, Equity and Inclusion Integration Gap",
        "Feminist Approaches to AI Ethics"
      ],
      "knowledge_summary": "Der Begriff 'Bias' verursacht Verwirrung unter Tech-Workern und kann seine beabsichtigte ethische Funktion nicht erfüllen; Tech-Worker sehen keinen direkten Zusammenhang zwischen DEI-Agenden und KI-Entwicklung; Legacy-Systeme stellen erhebliche Herausforderungen für ethische KI-Entwicklung dar."
    },
    {
      "id": "UFE85SCV",
      "stem": "Casal-Otero_2023_AI_literacy_in_K-12_a_systematic_literature_review",
      "title": "AI literacy in K-12: a systematic literature review",
      "author_year": "Casal-Otero (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI-Literacyförderung in K-12 konzentriert sich auf technische, konzeptuelle und angewandte Kompetenzen, allerdings gibt es kaum Assessments von Lernoutcomes und wenig Aufmerksamkeit für unerwünschte K",
          "stage3_completeness": 88,
          "stage3_correctness": 97,
          "stage3_overall": 92
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Das Paper ist eine systematische Literaturreviw zu AI Literacy im K-12 Bildungskontext und behandelt substantiell AI-Kompetenzen, Curricula und Lernansätze (AI_Literacies = Ja). Es erfüllt jedoch KEINE sozialen Kriterien (Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "TPACK (Technological Pedagogical Content Knowledge)",
        "AI-driven Socioeconomic Inequality",
        "AI Literacy",
        "Gender Equity in Computing Education",
        "Machine Learning Literacy",
        "Curriculum Coherence in AI Education"
      ],
      "knowledge_summary": "AI-Literacyförderung in K-12 konzentriert sich auf technische, konzeptuelle und angewandte Kompetenzen, allerdings gibt es kaum Assessments von Lernoutcomes und wenig Aufmerksamkeit für unerwünschte Konsequenzen von AI. Ein kohärenter Kompetenzrahmen ist erforderlich, um Curricula zu strukturieren und Geschlechterparität zu adressieren."
    },
    {
      "id": "4LY3SA4E",
      "stem": "Charlesworth_2024_Flexible_intersectional_stereotype_extraction",
      "title": "Flexible intersectional stereotype extraction (FISE): Analyzing intersectional biases in large language models",
      "author_year": "Charlesworth (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "59% der Traits in der englischen Sprache sind mit weißen Männern assoziiert, während nur 5% mit schwarzen Frauen assoziiert sind. Diese Imbalancen in der Sprachrepräsentation führen zu systematischen ",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper analysiert LLM-Biases mit FISE-Methode (Generative_KI: Ja). Fokus auf intersektionale Repräsentationsverzerrungen erfüllt Bias_Ungleichheit, Gender (Geschlechterstereotype), Diversitaet (Repräsentation). Intersektionale Analyse nach Crenshaw ist explizit feministisch (Feministisch: Ja). Messun"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Intersectional Stereotype Extraction",
        "Gender Bias in Language Representations",
        "Training Data Bias Propagation",
        "Word Embeddings Bias",
        "Class-Based Stereotype Dominance",
        "Representation Invisibility"
      ],
      "knowledge_summary": "59% der Traits in der englischen Sprache sind mit weißen Männern assoziiert, während nur 5% mit schwarzen Frauen assoziiert sind. Diese Imbalancen in der Sprachrepräsentation führen zu systematischen Verzerrungen in KI-Systemen, wobei Klassenzugehörigkeit ein übergeordneter Faktor ist."
    },
    {
      "id": "P82X89Q4",
      "stem": "Chee_2025_A_Competency_Framework_for_AI_Literacy_Variations",
      "title": "A Competency Framework for AI Literacy: Variations by Different Learner Groups and an Implied Learning Pathway",
      "author_year": "Chee (09/2025)",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Entwicklung eines umfassenden AI-Literacy-Kompetenzrahmens mit 8 Kompetenzen und 18 Sub-Kompetenzen, die sich je nach Bildungsstufe (K-12, Hochschule, Arbeitswelt) und Disziplin systematisch untersche",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 86
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Das Paper entwickelt ein umfassendes AI-Literacy-Kompetenzframework und erfüllt die TECHNIK-Bedingung substanziell (AI_Literacies=Ja). Allerdings fehlt jeglicher SOZIAL-Bezug: Es gibt keinen Fokus auf Soziale Arbeit, Bias/Ungleichheit, Gender, Diversität, feministische Perspektiven oder Fairness. Da"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Competency Framework",
        "Disciplinary Variation in AI Competencies",
        "AI Literacy",
        "Machine Learning Literacy",
        "Algorithmic Decision Systems",
        "Ethical AI Awareness",
        "Educational Equity in AI",
        "Lifelong Learning Pathway"
      ],
      "knowledge_summary": "Entwicklung eines umfassenden AI-Literacy-Kompetenzrahmens mit 8 Kompetenzen und 18 Sub-Kompetenzen, die sich je nach Bildungsstufe (K-12, Hochschule, Arbeitswelt) und Disziplin systematisch unterscheiden und zu einem Lernpfad über die Lebensspanne führen."
    },
    {
      "id": "Chen_2023_Ideology_Prediction_from_Scarce_and_Biased",
      "stem": "Chen_2023_Ideology_Prediction_from_Scarce_and_Biased",
      "title": "Chen_2023_Ideology_Prediction_from_Scarce_and_Biased",
      "author_year": "Chen",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": false
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Das Modell BBBG dekomponiert Dokumente in kontextuelle (neutrale) und ideologische Vektoren und erreicht damit signifikant bessere Vorhersagen bei nur 5% biased Trainingsdaten als bisherige Methoden, ",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {}
      },
      "concepts": [
        "Transfer Learning from Extreme to Moderate Positions",
        "Algorithmic Fairness in Sparse Supervision",
        "Ideology Prediction",
        "Variational Autoencoder with Gaussian Mixture Prior",
        "Selection Bias in Political Labels",
        "Vector Decomposition in Embeddings",
        "Representation Bias"
      ],
      "knowledge_summary": "Das Modell BBBG dekomponiert Dokumente in kontextuelle (neutrale) und ideologische Vektoren und erreicht damit signifikant bessere Vorhersagen bei nur 5% biased Trainingsdaten als bisherige Methoden, mit besonderer Effektivität bei der Vorhersage ideologischer Positionen von moderaten (nicht polarisierten) Autoren."
    },
    {
      "id": "VEJBIZRR",
      "stem": "Chen_2024_Exploring_complex_mental_health_symptoms_via",
      "title": "Exploring complex mental health symptoms via classifying social media data with explainable LLMs",
      "author_year": "Chen (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Das Paper demonstriert eine Pipeline zur Extraktion von Erkenntnissen aus Social-Media-Texten durch explainbare KI. Erste Ergebnisse zeigen, dass Lyme-Disease-Posts mit Mold-Erwähnungen signifikant hä",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.35,
            "categories": [
              "Generative_KI",
              "KI_Sonstige"
            ],
            "reasoning": "Der Titel deutet auf LLM-basierte Klassifikation von Mentalhygiene-Daten hin (Generative_KI + KI_Sonstige erfüllt, TECHNIK_OK). Jedoch: kein Abstract verfügbar, daher nicht erkennbar, ob ein substantieller Bezug zu sozialen/ethischen Dimensionen (SOZIAL_OK) besteht. Mental Health ist nicht automatis"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Digital Health Equity and Underserved Populations",
        "Natural Language Processing for Clinical Hypothesis Generation",
        "Embedding-based Explanation Clustering",
        "Algorithmic Fairness in Mental Health Classification",
        "Comorbidity Detection via Social Media Mining",
        "Explainable AI for Mental Health Informatics"
      ],
      "knowledge_summary": "Das Paper demonstriert eine Pipeline zur Extraktion von Erkenntnissen aus Social-Media-Texten durch explainbare KI. Erste Ergebnisse zeigen, dass Lyme-Disease-Posts mit Mold-Erwähnungen signifikant häufiger mental-health-bezogen sind als in Referenzdatensätzen (44% vs. 19,7%), was auf eine bisher unterexplorierten Drei-Wege-Interaktion hindeutet."
    },
    {
      "id": "79TL6HSB",
      "stem": "Chen_2025_Social_work_and_artificial_intelligence",
      "title": "Social work and artificial intelligence: Collaboration and challenges",
      "author_year": "Chen (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "KI kann die Effizienz in der Sozialen Arbeit erheblich verbessern (90% der Sozialarbeiter), erfordert aber robuste ethische Richtlinien, Transparenzverbesserungen und erweiterte KI-Kompetenzen der Fac",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 92
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Das Paper behandelt substantiell AI Literacy (Gaps in technical literacy, educational training), KI allgemein (Automation Bias, explainable AI), Soziale Arbeit (AI applications in social work, professional autonomy), Bias (automation bias), und Fairness (ethical governance, transparency). Beide Bedi"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "AI Literacy in Professional Practice",
        "Data Privacy Protection for Vulnerable Populations",
        "Algorithmic Bias in Social Services",
        "Explainable Artificial Intelligence (XAI)",
        "Human-AI Collaboration in Social Work",
        "Ethical Governance Frameworks",
        "Predictive Analytics in Case Management"
      ],
      "knowledge_summary": "KI kann die Effizienz in der Sozialen Arbeit erheblich verbessern (90% der Sozialarbeiter), erfordert aber robuste ethische Richtlinien, Transparenzverbesserungen und erweiterte KI-Kompetenzen der Fachkräfte zur Wahrung humanistischer Werte."
    },
    {
      "id": "XQM6WRU2",
      "stem": "Cher_2024_Exploring_machine_learning_to_support",
      "title": "Exploring machine learning to support decision-making for placement stabilization and preservation in child welfare",
      "author_year": "Cher (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Random-Forest-Modelle erreichten eine 10-fach höhere Präzision als Baseline und identifizierten zuverlässig Jugendliche mit hohem Risiko für Platzierungszerfall, während alle Modelle auf Fairness und ",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Fairness"
            ],
            "reasoning": "Paper entwickelt ML-Modelle (Random Forest, logistische Regression) für Vorhersage von Platzierungsabbruch in Jugendhilfe. Erfüllt TECHNIK-Kriterium durch KI_Sonstige (klassisches ML für algorithmische Entscheidungssysteme im Sozialbereich). Erfüllt SOZIAL-Kriterium durch: (1) direkter Bezug zu Sozi"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness Auditing",
        "Model Interpretability and Explainability",
        "Placement Stabilization Programs",
        "Racial Bias in Decision Support Systems",
        "Predictive Analytics in Child Welfare",
        "True Positive Rate (TPR) Parity",
        "Data-Driven Caseworker Decision-Making"
      ],
      "knowledge_summary": "Random-Forest-Modelle erreichten eine 10-fach höhere Präzision als Baseline und identifizierten zuverlässig Jugendliche mit hohem Risiko für Platzierungszerfall, während alle Modelle auf Fairness und Equity überprüft wurden."
    },
    {
      "id": "JZN2I6J5",
      "stem": "Chisca_2024_Prompting_fairness_Learning_prompts_for_debiasing",
      "title": "Prompting fairness: Learning prompts for debiasing large language models",
      "author_year": "Chisca (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Die vorgeschlagene Prompt-Tuning-Methode erreicht state-of-the-art Debiasing-Performance bei BERT und konkurrenzfähige Ergebnisse bei RoBERTa, während sie eine minimale Auswirkung auf die Sprachmodell",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.92,
            "categories": [
              "Prompting",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper behandelt Prompt-Tuning für Debiasing von LLMs und adressiert Fairness/Bias-Reduktion. Dies sind technische Kategorien. Allerdings fehlt ein substantieller Bezug zu sozialen Auswirkungen, sozialarbeiterischer Praxis oder marginalisierten Gruppen. Die Bias-Thematisierung ist rein algorithmisch-"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Prompt Tuning for Debiasing",
        "Algorithmic Bias in Language Models",
        "Bias Measurement Benchmarks",
        "Representational Harms",
        "Gender Bias in NLP",
        "Fairness-aware Loss Functions",
        "Parameter-Efficient Fine-Tuning"
      ],
      "knowledge_summary": "Die vorgeschlagene Prompt-Tuning-Methode erreicht state-of-the-art Debiasing-Performance bei BERT und konkurrenzfähige Ergebnisse bei RoBERTa, während sie eine minimale Auswirkung auf die Sprachmodellierungsfähigkeit hat. Die Methode trainiert nur kleine, wiederverwendbare Token-Embeddings, die zu beliebigen Eingabesequenzen hinzugefügt werden können."
    },
    {
      "id": "RARE5UFC",
      "stem": "Chisca_2024_Prompting_techniques_for_reducing_social_bias_in",
      "title": "Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes",
      "author_year": "Kamruzzaman (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Human Persona kombiniert mit System 2 Prompting reduziert stereotypische Urteile um bis zu 19%, während CoT-Prompting überraschenderweise dem System 1 ähnlicher ist als dem System 2, was die gängige A",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper behandelt systematisch Prompting-Strategien (Ja: substantieller Fokus auf 12 Prompt-Varianten) zur Bias-Reduktion in LLMs (Ja: Generative_KI). Bias-Reduktion und Fairness (Ja: beide Kategorien) sind zentrale Themen. TECHNIK-Bedingung erfüllt (Prompting + Generative_KI), SOZIAL-Bedingung erfüll"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Keyword-Inklusion",
          "divergence_justification": "Das LLM hat 'Prompting techniques' und 'social bias' als explizite Keywords erkannt und automatisch AI_Literacies zugeordnet, ohne zu erkennen, dass der Paper sich auf allgemeine LLM-Bias (nicht auf Soziale Arbeit) bezieht. Die Inklusion basiert auf oberflächlicher Keyword-Matching statt kontextueller Relevanz für das Feld.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Algorithmic Fairness in Language Models",
        "Intersectional Bias Analysis",
        "Social Bias Reduction through Prompting",
        "Stereotype Measurement in NLP",
        "Chain-of-Thought Prompting Misconception",
        "Human Persona Prompting",
        "Dual-Process Theory Application to LLMs"
      ],
      "knowledge_summary": "Human Persona kombiniert mit System 2 Prompting reduziert stereotypische Urteile um bis zu 19%, während CoT-Prompting überraschenderweise dem System 1 ähnlicher ist als dem System 2, was die gängige Annahme in der Literatur widerlegt."
    },
    {
      "id": "I78CL6R5",
      "stem": "Chiu_2024_What_are_artificial_intelligence_literacy_and",
      "title": "What are artificial intelligence literacy and competency? A comprehensive framework to support them",
      "author_year": "Chiu (06/2024)",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI-Kompetenz umfasst fünf Schlüsselkomponenten (Technologie, Impact, Ethik, Kollaboration, Selbstreflexion) und sollte Vertrauen sowie selbstreflektive Mindsets integrieren, nicht nur technisches Wiss",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.7,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Das Paper behandelt AI Literacy und Kompetenzentwicklung im KI-Bereich (AI_Literacies = Ja), erfüllt aber die zweite Bedingung nicht: Es fehlt ein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität, Feminismus oder Fairness. Das Paper ist relevant für AI Literacy generell, nich"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness and Bias",
        "Prompt Engineering",
        "AI Literacy",
        "Machine Learning Literacy",
        "Inclusive STEM Education",
        "Practitioner-Centered Co-Design",
        "AI Competency Framework",
        "AI Ethics Principles"
      ],
      "knowledge_summary": "AI-Kompetenz umfasst fünf Schlüsselkomponenten (Technologie, Impact, Ethik, Kollaboration, Selbstreflexion) und sollte Vertrauen sowie selbstreflektive Mindsets integrieren, nicht nur technisches Wissen."
    },
    {
      "id": "KPSF8EZE",
      "stem": "Chiu_2025_AI_literacy_and_competency_definitions,",
      "title": "AI literacy and competency: definitions, frameworks, development and future research directions",
      "author_year": "Chiu (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI Literacy (Wissen und kritisches Verständnis) und AI Competency (praktische Anwendungsfähigkeit) sind verwandte aber distinkte Konzepte, wobei zehn interdisziplinäre Literacies (mathematisch, data, ",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Das Paper behandelt substantiell AI Literacy und Kompetenzen (AI_Literacies = Ja). Allerdings fehlt jeglicher Bezug zu sozialen Aspekten wie Soziale Arbeit, Bias, Gender, Diversität, Fairness oder feministische Perspektiven. TECHNIK-Kriterium erfüllt, aber SOZIAL-Kriterium nicht. Daher: Exclude."
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Prompt Engineering",
        "Culturally Responsive AI Competency",
        "AI Literacy",
        "AI Competency",
        "Algorithmic Bias",
        "Interdisciplinary AI Foundations",
        "Ethical AI Literacy"
      ],
      "knowledge_summary": "AI Literacy (Wissen und kritisches Verständnis) und AI Competency (praktische Anwendungsfähigkeit) sind verwandte aber distinkte Konzepte, wobei zehn interdisziplinäre Literacies (mathematisch, data, ethisch, media, computational, linguistisch, visuell, domänenspezifisch, wissenschaftlich, design) eine integrative Grundlage bilden."
    },
    {
      "id": "WAYCKUZ8",
      "stem": "Choudhury_2024_Large_Language_Models_and_User_Trust_Consequence",
      "title": "Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals",
      "author_year": "Choudhury (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "LLMs können in selbstreferenziellen Lernschleifen geraten, in denen KI-generierte Inhalte zur Trainierung führen, was zu Bias-Verstärkung, reduzierter Datenvielfalt und Deskilling von Klinikern führt.",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 90
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.75,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Das Paper behandelt LLMs, Prompting und Bias-Mitigation substantiell (TECHNIK erfüllt). Es adressiert Fairness und Bias-Ungleichheit. Jedoch ist Healthcare-Kontext nicht identisch mit Sozialer Arbeit, und es gibt keinen expliziten Fokus auf marginalisierte Gruppen oder strukturelle Ungleichheit im S"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Automation Bias",
        "Algorithmic Transparency in Healthcare",
        "AI Accountability in Clinical Practice",
        "Professional Deskilling",
        "Self-Referential Learning Loop",
        "Algorithmic Bias Amplification",
        "User Trust in Large Language Models"
      ],
      "knowledge_summary": "LLMs können in selbstreferenziellen Lernschleifen geraten, in denen KI-generierte Inhalte zur Trainierung führen, was zu Bias-Verstärkung, reduzierter Datenvielfalt und Deskilling von Klinikern führt. Diese Risiken erfordern proaktive Maßnahmen zur Aufrechterhaltung von Fachkompetenz und kritischer Überwachung."
    },
    {
      "id": "X7ZGW6CN",
      "stem": "Ciston_2024_Intersectional_Artificial_Intelligence_Is",
      "title": "Intersectional Artificial Intelligence Is Essential: Polyvocal, Multimodal, Experimental Methods to Save AI",
      "author_year": "Ciston (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Intersektionale Strategien auf allen Ebenen der KI (von Daten bis Design bis Implementierung) können durch polyvokale, multimodale und experimentelle Ansätze strukturelle Verzerrungen adressieren und ",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 93
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies, Generative_KI, KI_Sonstige durch intersektionale AI-Strategien und Chatbot-Beispiel) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch durch explizite intersektionale und queer-feministische Theorie nach Crenshaw; Fairness durch"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Intersectional Artificial Intelligence",
        "Community-Centered AI Design",
        "Algorithmic Bias",
        "Data Provenance and Reflexivity",
        "Black Feminist Theory in Technology",
        "Queer Methodologies in Computing",
        "Feminist Data Visualization",
        "Criminal Risk Assessment AI"
      ],
      "knowledge_summary": "Intersektionale Strategien auf allen Ebenen der KI (von Daten bis Design bis Implementierung) können durch polyvokale, multimodale und experimentelle Ansätze strukturelle Verzerrungen adressieren und alternative Ethiken enthüllen. Community-fokussierte und künstlerische Praktiken sind entscheidend für die Entwicklung intersektionaler KI-Möglichkeiten."
    },
    {
      "id": "MIT8HTC6",
      "stem": "Clemmer_2024_PreciseDebias_An_automatic_prompt_engineering",
      "title": "PreciseDebias: An automatic prompt engineering approach for generative AI to mitigate image demographic biases",
      "author_year": "Clemmer (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "PreciseDebias erreicht eine 97,6%ige Erfolgsquote bei der Generierung ethnisch diverser Bilder und 99,9% bei geschlechtsspezifischen Bildern durch proportionale Lernraten-Anpassung basierend auf gemes",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper behandelt Generative KI (Bildgeneratoren) mit substantiellem Fokus auf Prompt-Engineering durch automatisierte Prompt-Rewriting. Adressiert demografische Bias-Reduktion explizit durch Gender-, Ethnizität- und Altersrepräsentation. Fairness-Metriken zur Bias-Messung zentral. TECHNIK erfüllt (Ge"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Automatic Prompt Engineering",
        "Text-to-Image Generation",
        "Fairness in Computer Vision",
        "Demographic Bias in Generative AI",
        "Large Language Model Fine-Tuning",
        "Proportional Learning Rate Adjustment",
        "Bias Measurement and Metrics",
        "Intersectional Representation"
      ],
      "knowledge_summary": "PreciseDebias erreicht eine 97,6%ige Erfolgsquote bei der Generierung ethnisch diverser Bilder und 99,9% bei geschlechtsspezifischen Bildern durch proportionale Lernraten-Anpassung basierend auf gemessenen Bias-Differenzen."
    },
    {
      "id": "7IY7AX7D",
      "stem": "Colombatto_2025_The_influence_of_mental_state_attributions_on",
      "title": "The influence of mental state attributions on trust in large language models",
      "author_year": "Colombatto (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": false,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Attributionen von Intelligenz-Merkmalen erhöhen signifikant die Annahme von KI-Ratschlägen, während Attributionen von Erfahrungs-Merkmalen (Emotionen, Empfindungen) diese reduzieren. Bewusstseinszusch",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting"
            ],
            "reasoning": "Das Paper behandelt substantiell AI_Literacies (Vertrauen, Kompetenzen im Umgang mit LLMs) und Generative_KI sowie Prompting-Strategien. Jedoch fehlt vollständig der zweite erforderliche Bereich (Sozial): keine direkte Soziale_Arbeit, keine Bias_Ungleichheit, kein Gender/Diversität/Feminismus/Fairne"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Anthropomorphism in AI Systems",
        "Behavioral Trust Measurement",
        "Mental State Attribution",
        "Human-AI Interaction Design",
        "Intelligence Attribution vs. Experience Attribution",
        "Consciousness Attribution to Large Language Models",
        "Calibrated Trust in AI"
      ],
      "knowledge_summary": "Attributionen von Intelligenz-Merkmalen erhöhen signifikant die Annahme von KI-Ratschlägen, während Attributionen von Erfahrungs-Merkmalen (Emotionen, Empfindungen) diese reduzieren. Bewusstseinszuschreibungen zeigen keine positive, sondern eine schwache negative Korrelation mit Ratsannahmen."
    },
    {
      "id": "JI24FQPV",
      "stem": "Creswell Báez_2025_Clinical_Social_Workers’_Perceptions_of_Large",
      "title": "Clinical Social Workers’ Perceptions of Large Language Models in Practice: Resistance to Automation and Prospects for Integration",
      "author_year": "Creswell Báez (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": false,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Klinische Sozialarbeiter erkennen das Potenzial von LLMs für Dokumentation, klinische Entscheidungsfindung und Klientenunterstützung, äußern aber erhebliche Bedenken bezüglich Datensicherheit, algorit",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Soziale_Arbeit"
            ],
            "reasoning": "Paper erfüllt TECHNIK- und SOZIAL-Kriterien: (1) TECHNIK: Generative_KI (LLMs in klinischer Praxis) und AI_Literacies (Training, Kompetenzentwicklung im Umgang mit LLMs) sind substantiell. (2) SOZIAL: Direkter Bezug zu Sozialarbeiter:innen (clinical social workers) und ihrer Praxis. Qualitative Stud"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Generative AI in Clinical Documentation",
        "Data Privacy and Confidentiality",
        "Professional Autonomy in Clinical Practice",
        "Two-Tiered Service Model",
        "Technology Acceptance Model (TAM)",
        "AI Literacy in Social Work",
        "Algorithmic Bias in Healthcare",
        "Healthcare Equity"
      ],
      "knowledge_summary": "Klinische Sozialarbeiter erkennen das Potenzial von LLMs für Dokumentation, klinische Entscheidungsfindung und Klientenunterstützung, äußern aber erhebliche Bedenken bezüglich Datensicherheit, algorithmischen Verzerrungen, beruflicher Normen und des Risikos einer Verschärfung von Ungleichheiten, was ihre Akzeptanz begrenzt."
    },
    {
      "id": "YMYHLMFS",
      "stem": "D'Ignazio_2024_Data_Feminism_for_AI",
      "title": "Data feminism for AI",
      "author_year": "Klein (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Feministische Prinzipien - insbesondere die Fokussierung auf Machtverhältnisse, Pluralismus, Sichtbarmachung von Arbeit und Kontext - sind essentiell zur Entwicklung gerechter, ethischer und nachhalti",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch KI_Sonstige (allgemeiner KI/ML-Bezug); SOZIAL_OK durch Bias_Ungleichheit, Gender, Diversitaet und Feministisch (explizite Verwendung des Data-Feminism-Frameworks von D'Ignazio, intersektionale feministische Perspektive). Starker konzeptioneller Beitr"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM hat die Kategorien Generative_KI und Fairness korrekt erkannt, überdehnt aber den Bedeutungsraum dieser Kategorien als ausreichend für Inclusion, während es gleichzeitig die thematische Breite von Data Feminism (AI_Literacies, Gender, Diversität, Feministisch) systematisch untererkannt und damit den interdisziplinären Kontext des Papiers nicht erfasst hat.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Labor Visibility in AI Systems",
        "Non-Consensual AI Applications",
        "Intersectional Feminism",
        "Environmental Justice in AI",
        "Data Justice",
        "Gender Bias in NLP",
        "Data Feminism"
      ],
      "knowledge_summary": "Feministische Prinzipien - insbesondere die Fokussierung auf Machtverhältnisse, Pluralismus, Sichtbarmachung von Arbeit und Kontext - sind essentiell zur Entwicklung gerechter, ethischer und nachhaltiger KI-Systeme. Die Autor:innen schlagen zudem zwei neue Prinzipien vor: Berücksichtigung von Umweltauswirkungen und intersektionales Verständnis von Konsens."
    },
    {
      "id": "D_Ignazio_2024_Data",
      "stem": "D_Ignazio_2024_Data",
      "title": "D_Ignazio_2024_Data",
      "author_year": "D",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": false
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": false,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 5,
          "stage1_key_finding": "Der Gemeinschaftsfeminismus bietet eine eigenständige epistemologische Konstruktion, die den westlichen Feminismus kritisiert und zugleich den 'originären Patriarchalismus' indigener Kulturen dekonstr",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {}
      },
      "concepts": [
        "Body-Territory Politics",
        "Feminist International Cooperation Methodology",
        "Internalized Structural Racism",
        "Decolonial Feminism",
        "Epistemic Justice",
        "Hetero-Patriarchal Cosmology Critique",
        "Community Feminism"
      ],
      "knowledge_summary": "Der Gemeinschaftsfeminismus bietet eine eigenständige epistemologische Konstruktion, die den westlichen Feminismus kritisiert und zugleich den 'originären Patriarchalismus' indigener Kulturen dekonstruiert, um eine antipatriarchale, territoriale und kosmogonische Perspektive auf Befreiung zu entwickeln."
    },
    {
      "id": "F7WNRWIC",
      "stem": "De Duro_2025_Measuring_and_identifying_factors_of_individuals'",
      "title": "Measuring and identifying factors of individuals' trust in large language models",
      "author_year": "De Duro (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 4,
          "stage1_key_finding": "Vertrauen in LLMs teilt sich in zwei empirisch distinkte aber korrelierte Dimensionen: 'Closeness with LLMs' (affektiv-emotional) und 'Reliance on LLMs' (kognitiv-rational), ähnlich wie in der Organis",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "Generative_KI"
            ],
            "reasoning": "Paper behandelt LLM-Vertrauen und individuelle Unterschiede (AI_Literacies, Generative_KI erfüllt). ABER: Kein Bezug zu sozialen Ungleichheiten, Bias, Gender, Diversität, Fairness oder Sozialer Arbeit. Prompting wird nur beiläufig erwähnt ('prompt-engineering'), nicht substantiell behandelt. Psychol"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Ethical AI Governance and Vulnerability Protection",
        "LLM-Simulated Validity",
        "Affective-Cognitive Trust Dimensions",
        "Natural Language Human-Computer Interaction Trust",
        "AI Literacy and Critical LLM Competence",
        "Age-Related Digital Divide in AI Adoption",
        "Gender Bias in AI Trust",
        "Human-AI Trust Measurement"
      ],
      "knowledge_summary": "Vertrauen in LLMs teilt sich in zwei empirisch distinkte aber korrelierte Dimensionen: 'Closeness with LLMs' (affektiv-emotional) und 'Reliance on LLMs' (kognitiv-rational), ähnlich wie in der Organisationspsychologie. Jüngere männliche Nutzer zeigen signifikant höheres Vertrauen als ältere weibliche Nutzer."
    },
    {
      "id": "2YS85B49",
      "stem": "Debnath_2024_Can_LLMs_reason_about_trust_A_pilot_study",
      "title": "Can LLMs reason about trust? A pilot study",
      "author_year": "Debnath (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": false,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "LLMs zeigen grundsätzliche Fähigkeit, Vertrauen anhand von Willingness, Competence und Safety zu analysieren und strategische Aktionspläne zu entwickeln; GPT-4o performt am besten, aber alle Modelle z",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "Generative_KI"
            ],
            "reasoning": "Paper untersucht LLM-Fähigkeiten zur Vertrauensanalyse zwischen Individuen. Während Generative_KI (LLMs) relevant ist, fehlt jeglicher substantieller Bezug zu den sozialen Kategorien: kein direkter Sozialarbeitsbezug, keine Bias/Ungleichheit-Analyse, kein Gender-, Diversitäts- oder Fairness-Fokus. D"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Structured Prompt-Based Reasoning Cycle",
        "Human-Computer-Mediated Social Interaction",
        "Model Performance Fairness Disparity",
        "Trust-Building Through Strategic Planning",
        "AI Reasoning Capability Evaluation",
        "Large Language Model Hallucination",
        "Socio-Cognitive Trust Reasoning"
      ],
      "knowledge_summary": "LLMs zeigen grundsätzliche Fähigkeit, Vertrauen anhand von Willingness, Competence und Safety zu analysieren und strategische Aktionspläne zu entwickeln; GPT-4o performt am besten, aber alle Modelle zeigen Halluzinationen und Fehlurteile bei komplexen Szenarien."
    },
    {
      "id": "Debnath_2024_LLMs",
      "stem": "Debnath_2024_LLMs",
      "title": "Debnath_2024_LLMs",
      "author_year": "Debnath",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": false
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "LLMs haben weitreichende Implikationen für soziale Dienste und erfordern kritische Auseinandersetzung mit Bias, Fairness und Gerechtigkeit",
          "stage3_completeness": 25,
          "stage3_correctness": 30,
          "stage3_overall": 22
        },
        "assessment": {}
      },
      "concepts": [
        "Algorithmic Fairness",
        "Gender Bias in Large Language Models",
        "Algorithmic Bias in Social Services",
        "Large Language Models (LLMs)",
        "Data Representation Equity",
        "AI Literacy in Social Work",
        "Intersectional AI Justice"
      ],
      "knowledge_summary": "LLMs haben weitreichende Implikationen für soziale Dienste und erfordern kritische Auseinandersetzung mit Bias, Fairness und Gerechtigkeit"
    },
    {
      "id": "S8WGUVQT",
      "stem": "Dencik_2024_Automated_government_benefits_and_welfare",
      "title": "Automated government benefits and welfare surveillance",
      "author_year": "Dencik (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Die zentralen Probleme automatisierter Wohlfahrtsüberwachung sind nicht technologisch neuartig, sondern wiederholen historisch dokumentierte Muster von Verdacht gegenüber Armen, opaker Entscheidungsfi",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Paper erfüllt beide Kriterien: TECHNIK_OK durch KI_Sonstige (algorithmische Systeme in öffentlicher Verwaltung: Fraud Detection, Chatbots). SOZIAL_OK durch Soziale_Arbeit (Wohlfahrtssysteme, marginalisierte Populationen) und Bias_Ungleichheit (Analyse von Machtsymmetrien, Surveillance von benachteil"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Street-Level Bureaucracy Automation",
        "Algorithmic Decision-Making (ADM) in Welfare Administration",
        "Algorithmic Bias in Social Services",
        "Administrative Suspicion toward the Poor",
        "Welfare Surveillance",
        "Political Technology",
        "Algorithmic Fairness and Auditability"
      ],
      "knowledge_summary": "Die zentralen Probleme automatisierter Wohlfahrtsüberwachung sind nicht technologisch neuartig, sondern wiederholen historisch dokumentierte Muster von Verdacht gegenüber Armen, opaker Entscheidungsfindung und strafender Maßnahmen gegen marginalisierte Gruppen, die bereits vor der KI-Ära bestanden."
    },
    {
      "id": "YDW5TX8M",
      "stem": "Deuze_2022_Imagination,_Algorithms_and_News_Developing_AI",
      "title": "Imagination, Algorithms and News: Developing AI Literacy for Journalism",
      "author_year": "Deuze (2022)",
      "year": 2022,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI-Literalität für Journalismus erfordert nicht nur technisches Wissen, sondern auch kritisches Verständnis der normativen Dimensionen von KI, die Fähigkeit zur Reflexion über Einsatzmöglichkeiten, un",
          "stage3_completeness": 92,
          "stage3_correctness": 96,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.75,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige"
            ],
            "reasoning": "Das Paper adressiert AI Literacy (TECHNIK erfüllt), aber der Fokus liegt auf Journalismus, nicht auf Sozialer Arbeit oder damit verbundenen sozialen Dimensionen wie Bias, Fairness oder Diversität. Keine der SOZIAL-Kategorien ist erfüllt. Ausschluss erforderlich."
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Imaginative AI Application",
        "Algorithmic Fairness",
        "Critical Algorithm Awareness",
        "Journalistic Autonomy",
        "AI Literacy",
        "Algorithmic Bias",
        "Technology Mythologization"
      ],
      "knowledge_summary": "AI-Literalität für Journalismus erfordert nicht nur technisches Wissen, sondern auch kritisches Verständnis der normativen Dimensionen von KI, die Fähigkeit zur Reflexion über Einsatzmöglichkeiten, und eine Kultur der Imagination statt bloßer instrumenteller Anwendung. Ein großes Wissensdefizit in der Nachrichtenindustrie gefährdet deren Autonomie gegenüber Tech-Konzernen."
    },
    {
      "id": "EN6GNKL3",
      "stem": "Dilek_2025_AI_literacy_in_teacher_education_Empowering",
      "title": "AI literacy in teacher education: Empowering educators through critical co-discovery",
      "author_year": "Dilek (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Durch kritische Co-Discovery-Aktivitäten konnten Pädagogen umfassende Verständigungsprozesse zu KI-Konzepten, ethischen Überlegungen und kontextspezifischen Anwendungen ko-konstruieren. Der Ansatz zei",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Paper behandelt substantiell AI Literacy durch teacher education und co-discovery-Ansätze (TECHNIK erfüllt). Expliziter Fokus auf kritische Examination von AI's power dynamics und social justice implications erfüllt Bias_Ungleichheit-Kategorie (SOZIAL erfüllt). Beide Bedingungen der Inklusionskriter"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "AI Literacy",
        "Algorithmic Bias",
        "Educator Agency",
        "Critical Co-Discovery",
        "Cultural Representation in AI",
        "Gender Bias in AI",
        "Technological Determinism"
      ],
      "knowledge_summary": "Durch kritische Co-Discovery-Aktivitäten konnten Pädagogen umfassende Verständigungsprozesse zu KI-Konzepten, ethischen Überlegungen und kontextspezifischen Anwendungen ko-konstruieren. Der Ansatz zeigt, dass prolongiertes Engagement mit KI-Literalität notwendig ist und in Lehrerausbildungsprogramme integriert werden sollte."
    },
    {
      "id": "KG8JRLRQ",
      "stem": "Dixon_2018_Measuring_and_mitigating_unintended_bias_in_text",
      "title": "Measuring and mitigating unintended bias in text data",
      "author_year": "Dixon (2018)",
      "year": 2018,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Durch strategisches Hinzufügen von nicht-toxischen Trainingsbeispielen mit bestimmten Identitätstermen lässt sich unbeabsichtigte Verzerrung in Textklassifikatoren reduzieren, ohne die Gesamtmodelllei",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.75,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper behandelt Bias-Mitigation und Fairness in ML-Systemen (KI_Sonstige: Ja, Bias_Ungleichheit: Ja, Fairness: Ja). Dies ist ein etabliertes Paper zur algorithmischen Fairness. TECHNIK-Bedingung erfüllt (KI_Sonstige), SOZIAL-Bedingung erfüllt (Bias_Ungleichheit + Fairness). Kein Sozialarbeitsbezug e"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Identity Terms Representation Disparity",
        "Model Bias versus Application Unfairness",
        "Pinned AUC Metric",
        "False Positive Bias",
        "Bias Mitigation through Data Balancing",
        "Equality of Odds in Classification",
        "Synthetic Test Set Design with Identity Terms",
        "Unintended Bias in Text Classification"
      ],
      "knowledge_summary": "Durch strategisches Hinzufügen von nicht-toxischen Trainingsbeispielen mit bestimmten Identitätstermen lässt sich unbeabsichtigte Verzerrung in Textklassifikatoren reduzieren, ohne die Gesamtmodellleistung zu beeinträchtigen. Die neu eingeführte 'Pinned AUC'-Metrik ermöglicht schwellenwertunabhängige Erkennung von Verzerrungen."
    },
    {
      "id": "8WBUGXRR",
      "stem": "Djeffal_2025_Reflexive_prompt_engineering_A_framework_for",
      "title": "Reflexive prompt engineering: A framework for responsible prompt engineering and AI interaction design",
      "author_year": "Djeffal (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Reflexive Prompt Engineering ist ein umfassendes Framework mit fünf verbundenen Komponenten (Design, Systemauswahl, Konfiguration, Evaluierung, Management), das verantwortungsvolle KI-Deployment durch",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, Generative_KI, Prompting alle Ja), SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Fairness alle Ja). Framework für verantwortungsvolles Prompt Engineering mit explizitem Fokus auf ethische Prinzipien, Menschenrechte und Diversität. Keine Sozial"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness Evaluation",
        "Chain-of-Thought Reasoning",
        "AI Literacy",
        "Model Bias Mitigation",
        "Prompt Management and Documentation",
        "Reflexive Prompt Engineering",
        "Inclusive Design Through Participatory Engagement",
        "Responsible AI Deployment"
      ],
      "knowledge_summary": "Reflexive Prompt Engineering ist ein umfassendes Framework mit fünf verbundenen Komponenten (Design, Systemauswahl, Konfiguration, Evaluierung, Management), das verantwortungsvolle KI-Deployment durch Alignment mit ethischen, rechtlichen und sozialen Prinzipien ermöglicht, ohne zugrunde liegende Modellarchitekturen zu modifizieren."
    },
    {
      "id": "K2KL8WH8",
      "stem": "Djiberou Mahamadou_2024_Revisiting_Technical_Bias_Mitigation_Strategies",
      "title": "Revisiting Technical Bias Mitigation Strategies",
      "author_year": "Djiberou Mahamadou (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 4,
          "stage1_key_finding": "Technische Bias-Mitigationsstrategien sind notwendig aber unzureichend; ein wertsensibler, partizipativer Ansatz, der Stakeholder in alle Entwicklungsphasen einbezieht, ist erforderlich, um echte Fair",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Das Paper behandelt technische Bias-Mitigation und Fairness-Metriken (TECHNIK_OK), aber ausschließlich im Gesundheitswesen ohne Bezug zu Sozialer Arbeit. Während Bias_Ungleichheit und Fairness adressiert werden, fehlt ein direkter Bezug zu den definierten SOZIAL-Kategorien (Soziale_Arbeit, Gender, D"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Translational Gap in Healthcare AI",
        "Algorithmic Fairness",
        "Value-Sensitive AI Design",
        "Health Equity and AI Systems",
        "Workforce Diversity in AI Development",
        "Participatory Design in AI Healthcare",
        "Algorithmic Bias Mitigation",
        "AI Literacy and Patient Engagement"
      ],
      "knowledge_summary": "Technische Bias-Mitigationsstrategien sind notwendig aber unzureichend; ein wertsensibler, partizipativer Ansatz, der Stakeholder in alle Entwicklungsphasen einbezieht, ist erforderlich, um echte Fairness zu erreichen."
    },
    {
      "id": "DGLWM93D",
      "stem": "Engelhardt_2025_Voll_(dia)logisch_Ein_Werkstattbericht_über_den",
      "title": "Voll (dia)logisch? Ein Werkstattbericht über den Einsatz von generativer KI in der Hochschulbildung für Soziale Arbeit – Curriculare Überlegungen und veränderte Akteurskonstellationen",
      "author_year": "Engelhardt (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": false,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 4,
          "stage1_key_finding": "Generative KI sollte nicht nur als Werkzeug, sondern als kooperativer nicht-menschlicher Akteur verstanden werden, der transformativ in Hochschulbildung einschreitet und neue curriculare sowie didakti",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "Soziale_Arbeit"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, Generative_KI, Prompting sind substantiell) + SOZIAL_OK (Soziale_Arbeit durch expliziten Hochschulbezug zur Soziale-Arbeit-Ausbildung). Prompting als metakognitive Schlüsselkompetenz ist Kernthema. Curriculare Integration in Soziale Arbeit "
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Prompting Competence",
        "Data Literacy in Social Work Education",
        "Digital Inclusivity in Higher Education",
        "Curriculum Transformation through Generative AI",
        "AI Ethics in Professional Social Work",
        "Algorithmic Bias in Social Service Systems",
        "Generative AI as Non-Human Actor"
      ],
      "knowledge_summary": "Generative KI sollte nicht nur als Werkzeug, sondern als kooperativer nicht-menschlicher Akteur verstanden werden, der transformativ in Hochschulbildung einschreitet und neue curriculare sowie didaktische Anforderungen schafft."
    },
    {
      "id": "XCS4YCQH",
      "stem": "European Commission. Joint Research Centre._2017_DigComp_2.1_the_digital_competence_framework_for",
      "title": "DigComp 2.1: the digital competence framework for citizens with eight proficiency levels and examples of use.",
      "author_year": "European Commission. Joint Research Centre.",
      "year": 2017,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "DigComp 2.1 erweitert das Framework von drei auf acht differenzierte Proficiency-Level pro Kompetenz (insgesamt 21 Kompetenzen), mit 168 einzelnen Deskriptoren basierend auf Lernzielen, Knowledge, Ski",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [],
            "reasoning": "DigComp 2.1 ist ein Rahmenwerk für digitale Kompetenzen im Allgemeinen, nicht spezifisch für KI/ML-Systeme. Es behandelt digitale Literalität umfassend, aber nicht AI Literacy im Sinne der Definition (KI-spezifische Kompetenzen, technisches KI-Verständnis). Keine Bezüge zu sozialen Arbeit, Bias, Fai"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Competency Contextualization",
        "Proficiency Level Scaling",
        "Digital Competence Framework",
        "Cultural and Generational Diversity in Digital Environments",
        "Digital Citizenship",
        "Digital Inclusion"
      ],
      "knowledge_summary": "DigComp 2.1 erweitert das Framework von drei auf acht differenzierte Proficiency-Level pro Kompetenz (insgesamt 21 Kompetenzen), mit 168 einzelnen Deskriptoren basierend auf Lernzielen, Knowledge, Skills und Attitudes sowie kontextualisierten Beispielen aus Beschäftigungs- und Lernszenarien."
    },
    {
      "id": "QFPTW4VL",
      "stem": "European Data Protection Supervisor_2023_Explainable_Artificial_Intelligence",
      "title": "Explainable Artificial Intelligence",
      "author_year": "European Data Protection Supervisor",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "XAI ist ein essentielles Mittel zur Förderung von Transparenz, Rechenschaftspflicht und Fairness bei KI-Systemen, kann aber selbst neue Risiken schaffen und muss durch menschenzentrierte Design-Ansätz",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper behandelt XAI als Kompetenz- und Transparenzthema (AI_Literacies), mit Fokus auf algorithmische Systeme (KI_Sonstige). Substantielle Thematisierung von Bias/Ungleichheit durch Marginalisierung, Diversität durch explizite Nennung marginalisierter Gruppen und Fairness durch ethische/gerechte Ent"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Human-Centric AI Design",
        "GDPR Compliance and Data Protection",
        "Algorithmic Bias",
        "Interpretability vs. Explainability",
        "Explainable Artificial Intelligence (XAI)",
        "Black Box Problem",
        "XAI Implementation Risks"
      ],
      "knowledge_summary": "XAI ist ein essentielles Mittel zur Förderung von Transparenz, Rechenschaftspflicht und Fairness bei KI-Systemen, kann aber selbst neue Risiken schaffen und muss durch menschenzentrierte Design-Ansätze und kritische Reflexion flankiert werden."
    },
    {
      "id": "MITLDF9S",
      "stem": "Feist-Ortmanns_2025_KI-basiertes_Assistenzsystem_im",
      "title": "KI-basiertes Assistenzsystem im Kinderschutzverfahren",
      "author_year": "Feist-Ortmanns (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "KI-Systeme sind nicht neutrale Werkzeuge sondern Akteure, die die sozialpädagogische Praxis grundlegend transformieren. Ein verantwortungsvoller Einsatz erfordert kritische Reflexion von Bias, Transpa",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmisches Entscheidungssystem in Kinderschutz; AI_Literacies: Schulungsbedarf erwähnt) + SOZIAL_OK (Soziale_Arbeit: Gefährdungseinschätzung im Kinderschutz; Bias_Ungleichheit: explizite Bias-Risiken; Fairness: Human-in-the-Loop und Fehl"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Bias in Child Welfare",
        "Responsible AI Implementation",
        "Digitality as Everyday Practice",
        "AI Agency in Social Work Practice",
        "Natural Language Processing in Child Welfare Assessment",
        "AI Literacy for Child Welfare Professionals",
        "Diversity Gap in AI Development"
      ],
      "knowledge_summary": "KI-Systeme sind nicht neutrale Werkzeuge sondern Akteure, die die sozialpädagogische Praxis grundlegend transformieren. Ein verantwortungsvoller Einsatz erfordert kritische Reflexion von Bias, Transparenz und das Primat des Kindeswohls."
    },
    {
      "id": "IDW7QSYG",
      "stem": "Fraile-Rojas_2025_Female_perspectives_on_algorithmic_bias",
      "title": "Female perspectives on algorithmic bias: Implications for AI researchers and practitioners",
      "author_year": "Fraile-Rojas (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Weibliche Nutzer konzentrieren sich primär auf die Zukunft von KI-Technologien und die aktive Rolle von Frauen zur Gewährleistung geschlechtergerechter Systeme; algorithmischer Bias beeinflusst weibli",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: NLP/ML für Datenanalyse; AI_Literacies: feministische digitale Kompetenzen) und SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness substantiell behandelt). Empirische Analyse von 172K Tweets zu Geschlechter-Perspektiven"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Social Opinion Mining",
        "Intersectionality in AI",
        "Women's Entrepreneurial Activism in AI Ethics",
        "Gender Bias in AI Systems",
        "Feminist Socio-Technical Approaches",
        "Responsible AI Development"
      ],
      "knowledge_summary": "Weibliche Nutzer konzentrieren sich primär auf die Zukunft von KI-Technologien und die aktive Rolle von Frauen zur Gewährleistung geschlechtergerechter Systeme; algorithmischer Bias beeinflusst weibliches Verhalten als Reaktion auf Ungerechtigkeit, wobei Frauen verstärkt unternehmerische Lösungen vorantreiben."
    },
    {
      "id": "EB7PZUZZ",
      "stem": "Freinhofer_2025_Prompten",
      "title": "Prompten nach Plan: Das PCRR-Framework als pädagogisches Werkzeug für den Einsatz von Künstlicher Intelligenz.",
      "author_year": "Freinhofer (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": false,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": false,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": false
          },
          "stage1_arguments_count": 4,
          "stage1_key_finding": "Effektive Prompts sollten vier Hauptelemente berücksichtigen (Persona, Task, Context, Format) und durchschnittlich etwa 21 Wörter mit relevantem Kontext umfassen, wobei iterative Verfeinerung zu besse",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting"
            ],
            "reasoning": "Das Paper konzentriert sich auf AI Literacy und Prompting-Strategien (TECHNIK-Anforderung erfüllt), fehlt aber vollständig die SOZIAL-Komponente. Es diskutiert weder Bias, Ungleichheit, Gender-Perspektiven, Fairness noch direkte sozialarbeiterische Anwendungen. Rein schulpädagogisches Framework ohne"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "AI Literacy",
        "Role-Based AI Application",
        "Iterative Prompt Refinement",
        "Generative AI Integration in Workplace",
        "Contextual Specificity",
        "Prompt Engineering"
      ],
      "knowledge_summary": "Effektive Prompts sollten vier Hauptelemente berücksichtigen (Persona, Task, Context, Format) und durchschnittlich etwa 21 Wörter mit relevantem Kontext umfassen, wobei iterative Verfeinerung zu besseren Ergebnissen führt."
    },
    {
      "id": "EB7PZUZZ",
      "stem": "Freinhofer_2025_Prompten_nach_Plan_Das_PCRR-Framework_als",
      "title": "Prompten nach Plan: Das PCRR-Framework als pädagogisches Werkzeug für den Einsatz von Künstlicher Intelligenz.",
      "author_year": "Freinhofer (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": false,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Das PCRR-Framework (Plan-Create-Review-Reflect) erhöht die Effizienz und Qualität der Prompterstellung signifikant: Mit Framework benötigten 64,5% der Schüler:innen nur einen Prompt für zufriedenstell",
          "stage3_completeness": 92,
          "stage3_correctness": 97,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting"
            ],
            "reasoning": "Das Paper konzentriert sich auf AI Literacy und Prompting-Strategien (TECHNIK-Anforderung erfüllt), fehlt aber vollständig die SOZIAL-Komponente. Es diskutiert weder Bias, Ungleichheit, Gender-Perspektiven, Fairness noch direkte sozialarbeiterische Anwendungen. Rein schulpädagogisches Framework ohne"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "PCRR-Framework",
        "Generative AI Models",
        "AI Literacy",
        "Prompting Techniques",
        "Digital Equity in Education",
        "Responsible AI Use",
        "Prompt Engineering"
      ],
      "knowledge_summary": "Das PCRR-Framework (Plan-Create-Review-Reflect) erhöht die Effizienz und Qualität der Prompterstellung signifikant: Mit Framework benötigten 64,5% der Schüler:innen nur einen Prompt für zufriedenstellendes Ergebnis vs. 30,8% ohne Framework. 86,9% bewerteten die Methode als hilfreich."
    },
    {
      "id": "A776TPGG",
      "stem": "Friedrich-Ebert-Stiftung_2025_artificial",
      "title": "The EU artificial intelligence act through a gender lens",
      "author_year": "Friedrich-Ebert-Stiftung",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 5,
          "stage1_key_finding": "Der EU AI Act enthält erhebliche Lücken bei der Adressierung von Geschlechterungerechtigkeit und verwendet überwiegend geschlechtsneutrale Sprache, die die einzigartigen Herausforderungen marginalisie",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.75,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Fairness"
            ],
            "reasoning": "Paper analysiert EU AI Act durch Gender-Lens: KI_Sonstige (algorithmische Systeme unter Regulierung) + Gender (expliziter Geschlechterfokus) + Bias_Ungleichheit (Geschlechtergerechtigkeit) + Fairness (Regulierungsanalyse). Beide Bedingungen erfüllt: TECHNIK_OK (KI_Sonstige), SOZIAL_OK (Gender + Bias"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Gender Bias",
        "Objectification in AI Systems",
        "Algorithmic Discrimination in Social Protection",
        "Gender Impact Assessment",
        "Feminist Regulatory Interpretation",
        "Bias Audit and Conformity Assessment",
        "Intersectional AI Fairness"
      ],
      "knowledge_summary": "Der EU AI Act enthält erhebliche Lücken bei der Adressierung von Geschlechterungerechtigkeit und verwendet überwiegend geschlechtsneutrale Sprache, die die einzigartigen Herausforderungen marginalisierter Gruppen, insbesondere Frauen of Color und LGBTQIA+ Personen, nicht angemessen berücksichtigt."
    },
    {
      "id": "QKFG72IT",
      "stem": "Fujii_2024_Bildungsteilhabe_-_Flucht_-_Digitalisierung_Eine",
      "title": "Bildungsteilhabe - Flucht - Digitalisierung: Eine multilokale Ethnografie im (digitalen) Alltag junger Geflüchteter",
      "author_year": "Fujii (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Digitale Medien ermöglichen Bildungsteilhabe junger Geflüchteter nur unter Bedingungen von gelingenden Passungsverhältnissen zwischen individuellen Fähigkeiten, artefaktgebundenen Potenzialitäten und ",
          "stage3_completeness": 88,
          "stage3_correctness": 93,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Diversitaet"
            ],
            "reasoning": "Paper behandelt digitale Medien, Bildungszugang und junge Geflüchtete. Erfüllt SOZIAL-Kriterien (Soziale_Arbeit, Bias_Ungleichheit, Diversität). ABER: Kein Fokus auf KI/ML-Systeme. Digitalisierung ≠ KI. Keine Kategorie aus TECHNIK-Bereich (AI_Literacies, Generative_KI, Prompting, KI_Sonstige) ist su"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Artifact-Mediated Educational Practices",
        "Post-Migration Everyday Coping and Social Connection",
        "Multilocal Ethnographic Methodology",
        "Digital Equity and Access Barriers",
        "Digital Literacy and Competence Development",
        "Intersectional Analysis of Refugee Youth",
        "Educational Participation under Conditions of Structural Inequality"
      ],
      "knowledge_summary": "Digitale Medien ermöglichen Bildungsteilhabe junger Geflüchteter nur unter Bedingungen von gelingenden Passungsverhältnissen zwischen individuellen Fähigkeiten, artefaktgebundenen Potenzialitäten und institutionellen Rahmungen. Ohne angemessene strukturelle und soziale Unterstützung entstehen Exklusionsrisiken trotz Mediennutzung."
    },
    {
      "id": "MUBZ8XJL",
      "stem": "Gaba_2025_Bias,_accuracy,_and_trust_Gender-diverse",
      "title": "Bias, accuracy, and trust: Gender-diverse perspectives on large language models",
      "author_year": "Gaba (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Gendered Prompts führen zu identitätsspezifischeren Reaktionen; non-binäre Teilnehmende berichten von herablassenden und stereotypischen Darstellungen, während Männer höheres Vertrauen zeigen und Frau",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 90
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt TECHNIK-Kriterien durch Generative_KI (ChatGPT-Fokus) und Prompting (Empfehlungen zu clarifying prompts). Erfüllt SOZIAL-Kriterien durch expliziten Gender-Fokus (unterschiedliche Geschlechteridentitäten), Bias_Ungleichheit (stereotype responses, verringerte Vertrauenswerte) und Fairnes"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'Bias' und 'Fairness' in algorithmischen Systemen als automatische Zugehörigkeit zu den Inklusionskriterien aus, ohne zu berücksichtigen, dass das Paper primär eine qualitative Gender-Perspektive-Studie ist, die nicht die technischen oder anwendungsorientierten Schwerpunkte des Feldes abdeckt. Die Überdehnung von KI_Sonstige + Soziale_Arbeit auf Basis des Titels ignoriert, dass keine substantielle Verbindung zu Sozialer Arbeit als Praxisfeld besteht.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Non-Binary Representation in AI",
        "Gender Bias in Large Language Models",
        "Gendered Prompt Engineering",
        "Intersectional AI Auditing",
        "Mixed-Methods User Research for AI Evaluation",
        "Perceived Trustworthiness in AI Systems"
      ],
      "knowledge_summary": "Gendered Prompts führen zu identitätsspezifischeren Reaktionen; non-binäre Teilnehmende berichten von herablassenden und stereotypischen Darstellungen, während Männer höheres Vertrauen zeigen und Frauen traditionelle emotionale Stereotypen kritisieren."
    },
    {
      "id": "MS3CNU3S",
      "stem": "Gallegos_2024_Bias_and_fairness_in_large_language_models_A",
      "title": "Bias and fairness in large language models: A survey",
      "author_year": "Gallegos (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Der Survey konsolidiert und vereinheitlicht fragmentiertes Wissen über Bias-Evaluation und Mitigation in LLMs durch drei intuitive Taxonomien, die Evaluierungsmetriken (nach Operationalisierungsebenen",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Das Paper erfüllt die TECHNIK-Bedingung (Generative_KI, Bias_Ungleichheit, Fairness alle Ja). Jedoch fehlt die SOZIAL-Bedingung im Sinne des Literature Reviews: Bias_Ungleichheit und Fairness sind zwar vorhanden, aber das Paper behandelt allgemeine KI-Ethik und -Fairness OHNE spezifischen Bezug zu S"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Intersectional Fairness",
        "Instruction Tuning for Bias Mitigation",
        "Bias Mitigation Staging Framework",
        "Algorithmic Bias in Language Models",
        "Structural Inequity Reproduction",
        "Gender Bias in NLP",
        "Fairness Metrics Taxonomy"
      ],
      "knowledge_summary": "Der Survey konsolidiert und vereinheitlicht fragmentiertes Wissen über Bias-Evaluation und Mitigation in LLMs durch drei intuitive Taxonomien, die Evaluierungsmetriken (nach Operationalisierungsebenen: Embeddings, Wahrscheinlichkeiten, generierter Text), Evaluierungsdatensätze und Mitigationstechniken (nach Interventionsstadium: Pre-, In-, Intra-, Post-Processing) strukturieren."
    },
    {
      "id": "ST9UCTJE",
      "stem": "Garg_2019_Counterfactual_fairness_in_text_classification",
      "title": "Counterfactual fairness in text classification through robustness",
      "author_year": "Garg (2019)",
      "year": 2019,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Counterfactual Logit Pairing (CLP) und Blindness adressieren erfolgreich kontrafaktische Fairness ohne Beeinträchtigung der Gesamtgenauigkeit, zeigen aber Tradeoffs mit gruppenbasierter Fairness. CLP ",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.75,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper adressiert Counterfactual Fairness in Text Classification - ein KI/ML-Thema (KI_Sonstige: Ja). Fokus auf Fairness und Robustheit deutet auf Bias-Mitigation hin (Fairness: Ja, Bias_Ungleichheit: Ja). TECHNIK und SOZIAL sind erfüllt. Ohne Abstract ist Confidence begrenzt, aber Titel und Autorsch"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Counterfactual Fairness",
        "Counterfactual Logit Pairing (CLP)",
        "Asymmetric Counterfactuals",
        "Equality of Odds vs. Counterfactual Fairness",
        "Algorithmic Bias in Toxicity Detection",
        "Counterfactual Token Fairness (CTF)",
        "Fairness-Accuracy Tradeoff"
      ],
      "knowledge_summary": "Counterfactual Logit Pairing (CLP) und Blindness adressieren erfolgreich kontrafaktische Fairness ohne Beeinträchtigung der Gesamtgenauigkeit, zeigen aber Tradeoffs mit gruppenbasierter Fairness. CLP generalisiert besser zu ungesehenen Identitätstokens als Blindness."
    },
    {
      "id": "UENFDPH9",
      "stem": "Garkisch_2024_Considering_a_unified_model_of_artificial",
      "title": "Considering a unified model of artificial intelligence enhanced social work: A systematic review",
      "author_year": "Garkisch (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Die Literatur zeigt einen signifikanten Mangel an empirischer Forschung zu AI-Implementierungen in der Sozialen Arbeit. Die Autoren entwickeln ein integriertes Modell der 'Artificial Intelligence Enha",
          "stage3_completeness": 88,
          "stage3_correctness": 97,
          "stage3_overall": 92
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit"
            ],
            "reasoning": "Systematic Review mit substanziellem Fokus auf AI Literacy und Computational Thinking in der Sozialen Arbeit (AI_Literacies: Ja). Direkter Bezug zu sozialarbeiterischer Praxis und Profession via Staub-Bernasconi's Triple Mandate (Soziale_Arbeit: Ja). Thematisiert auch algorithmische Systeme/KI-Syste"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Human Rights-Based Approach to AI",
        "Algorithmic Bias in Social Services",
        "AI Governance in Social Work Organizations",
        "Participatory Design for AI Systems",
        "Predictive Analytics in Child Welfare",
        "AI Literacy in Social Work",
        "Artificial Intelligence Enhanced Social Work"
      ],
      "knowledge_summary": "Die Literatur zeigt einen signifikanten Mangel an empirischer Forschung zu AI-Implementierungen in der Sozialen Arbeit. Die Autoren entwickeln ein integriertes Modell der 'Artificial Intelligence Enhanced Social Work' basierend auf Menschenrechtsrahmen und dem Tripelmandat der Sozialen Arbeit."
    },
    {
      "id": "YN9JAREI",
      "stem": "Gengler_2024_Faires_KI-Prompting_–_Ein_Leitfaden_für",
      "title": "Faires KI-Prompting – Ein Leitfaden für Unternehmen",
      "author_year": "Gengler (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Ein strukturiertes KI-FAIRNESS-Framework (Kontext, Input, Fokus, Ausschnitt, Iterationen, Repertoire, Nachbessern, Eignung, Sprache, Sinn) ermöglicht es Nutzer:innen, faire und diverse Ergebnisse aus ",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies, Generative_KI, Prompting alle Ja) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness alle Ja). Fokus auf feministisches KI-Prompting mit KI-FAIRNESS-Framework für faire, diskriminierungsvermeidende Prompt-Gestaltung. Su"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Fair Prompt Engineering",
        "AI Governance and Organizational Values",
        "Gender-Data-Gap",
        "Feminist AI",
        "AI Literacy for SMEs",
        "Algorithmic Fairness in Generative AI",
        "Stereotype Reproduction in Generative AI",
        "AI Hallucination and Reliability"
      ],
      "knowledge_summary": "Ein strukturiertes KI-FAIRNESS-Framework (Kontext, Input, Fokus, Ausschnitt, Iterationen, Repertoire, Nachbessern, Eignung, Sprache, Sinn) ermöglicht es Nutzer:innen, faire und diverse Ergebnisse aus Generativer KI zu gestalten. Organisationales Mindset, klare KI-Strategie und Werteorientierung sind grundlegend für fairen KI-Einsatz."
    },
    {
      "id": "YLAKP7Z2",
      "stem": "Ghosal_2024_An_empirical_study_of_structural_social_and",
      "title": "Intersectional analysis of visual generative AI: The case of Stable Diffusion",
      "author_year": "Jääskeläinen (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": false,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Stable Diffusion perpetuiert bestehende Machtsysteme wie Sexismus, Rassismus, Heteronormativität und Ableismus, indem es einen Standardindividuum als weiß, körperlich fit und maskulin präsentierend da",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch Generative_KI (Stable Diffusion-Analyse). SOZIAL_OK durch Bias_Ungleichheit (Stereotype, Diskriminierung), Gender (Sexismus-Fokus), Diversitaet (Inklusion marginalisierter Gruppen), und Feministisch (explizite feministische intersektionale Methodik n"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Institutional Bias in AI Training Data",
        "Algorithmic Transparency and Accountability",
        "Intersectional Feminist Analysis of Technology",
        "Reparative Justice in AI Systems",
        "Algorithmic Bias in Visual Generative AI",
        "Representational Harm in Generative AI",
        "Visual Culture Politics",
        "Coded Gaze in Generative AI"
      ],
      "knowledge_summary": "Stable Diffusion perpetuiert bestehende Machtsysteme wie Sexismus, Rassismus, Heteronormativität und Ableismus, indem es einen Standardindividuum als weiß, körperlich fit und maskulin präsentierend darstellt. Die hegemonialen kulturellen Werte in der Bildgeneration lassen sich auf institutionelle Kontexte zurückführen, insbesondere auf Euro- und Nordamerika-zentrische Perspektiven."
    },
    {
      "id": "EMZ33KFH",
      "stem": "Ghosal_2025_Unequal_voices_How_LLMs_construct_constrained",
      "title": "Unequal voices: How LLMs construct constrained queer narratives",
      "author_year": "Ghosal (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "LLMs generieren für queere Personas signifikant häufiger Begriffe der Diversität und Inklusion, fokussieren überproportional auf identitätsbezogene Themen und zeigen eine systematische Topic-Divergenz",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet"
            ],
            "reasoning": "Paper untersucht systematisch Bias und Diskriminierung in LLM-generierten Narrativen von queeren Personen. Erfüllt TECHNIK-Kriterium (Generative_KI: Fokus auf LLM-Outputs) und SOZIAL-Kriterium (Bias_Ungleichheit: Stereotypisierung marginalisierter Gruppen; Gender: queere Identitäten; Diversitaet: Re"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Allocation Harm",
        "LGBTQ+ Representation in Language Models",
        "LLM-as-Judge Evaluation",
        "Narrow Representation",
        "Discursive Othering",
        "AI Ethics for Healthcare Applications",
        "Representational Harm",
        "Topic Divergence Bias"
      ],
      "knowledge_summary": "LLMs generieren für queere Personas signifikant häufiger Begriffe der Diversität und Inklusion, fokussieren überproportional auf identitätsbezogene Themen und zeigen eine systematische Topic-Divergenz gegenüber nicht-queeren Personas, was als subtile Form der diskursiven Othering und Repräsentationseinengung wirkt."
    },
    {
      "id": "VFD9ENG6",
      "stem": "Gohar_2023_A_Survey_on_Intersectional_Fairness_in_Machine",
      "title": "A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges",
      "author_year": "Gohar (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Der Survey präsentiert die erste umfassende Taxonomie für intersektionale Fairness-Notionen und Fair-Learning-Methoden und identifiziert zentrale Herausforderungen wie Data Sparsity bei kleineren Subg",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 90
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Das Paper behandelt substantiell algorithmische Fairness und Bias in ML-Systemen (KI_Sonstige, Fairness). Es greift EXPLIZIT auf Crenshaw's intersektionale Theorie zurück und analysiert Diskriminierungsmechanismen aus intersektionaler Perspektive (Feministisch, Bias_Ungleichheit, Gender, Diversitaet"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Intersectional Fairness",
        "Multicalibration",
        "Algorithmic Bias in High-Stakes Decision Systems",
        "Fairness-Notion Taxonomy",
        "Subgroup Fairness",
        "Gender Bias in Natural Language Processing",
        "Data Sparsity Problem"
      ],
      "knowledge_summary": "Der Survey präsentiert die erste umfassende Taxonomie für intersektionale Fairness-Notionen und Fair-Learning-Methoden und identifiziert zentrale Herausforderungen wie Data Sparsity bei kleineren Subgruppen und die Unzulänglichkeit von Mittigationstechniken, die auf unabhängige Gruppen optimiert sind."
    },
    {
      "id": "VFD9ENG6",
      "stem": "Gohar_2023_Survey",
      "title": "A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges",
      "author_year": "Gohar (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Der Survey präsentiert die erste umfassende Taxonomie für intersektionale Fairness-Notionen (Subgroup Fairness, Multicalibration, Metric-based Fairness, Differential Fairness, Max-Min Fairness) und id",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Das Paper behandelt substantiell algorithmische Fairness und Bias in ML-Systemen (KI_Sonstige, Fairness). Es greift EXPLIZIT auf Crenshaw's intersektionale Theorie zurück und analysiert Diskriminierungsmechanismen aus intersektionaler Perspektive (Feministisch, Bias_Ungleichheit, Gender, Diversitaet"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Intersectional Fairness",
        "Algorithmic Fairness",
        "Gender Bias in Facial Recognition",
        "Multicalibration",
        "Data Sparsity in Intersectional Contexts",
        "Subgroup Fairness",
        "Fairness Gerrymandering"
      ],
      "knowledge_summary": "Der Survey präsentiert die erste umfassende Taxonomie für intersektionale Fairness-Notionen (Subgroup Fairness, Multicalibration, Metric-based Fairness, Differential Fairness, Max-Min Fairness) und identifiziert kritische Herausforderungen wie Data Sparsity, Fairness Gerrymandering und die Notwendigkeit von Fairness jenseits reiner Parität."
    },
    {
      "id": "RAV6DAFQ",
      "stem": "Goldkind_2023_The_End_of_the_World_as_We_Know_It_ChatGPT_and",
      "title": "The End of the World as We Know It? ChatGPT and Social Work",
      "author_year": "Goldkind (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "ChatGPT bietet sowohl Chancen (Effizienzsteigerung) als auch Risiken für die Soziale Arbeit, erfordert aber kritische Auseinandersetzung der Profession mit ethischen, justizialen und kompetenzbezogene",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Soziale_Arbeit",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI: ChatGPT; AI_Literacies: proaktive Auseinandersetzung mit KI-Kompetenzen für Profession) + SOZIAL (Soziale_Arbeit: direkter Bezug zur Profession; Bias_Ungleichheit: Warnung vor inequitable outcomes). Kommentar-Format mit konzeptionellem Fokus a"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "AI Ethics Curriculum Integration",
        "Generative AI Literacy for Social Workers",
        "Justice-Centered AI Policy Development",
        "Algorithmic Accountability in Welfare Systems",
        "Structural Bias in Language Models",
        "Technology Adoption Lag in Social Work",
        "Relational Competence vs. Algorithmic Automation",
        "Algorithmic Fairness in Social Work"
      ],
      "knowledge_summary": "ChatGPT bietet sowohl Chancen (Effizienzsteigerung) als auch Risiken für die Soziale Arbeit, erfordert aber kritische Auseinandersetzung der Profession mit ethischen, justizialen und kompetenzbezogenen Fragen, um vulnerable Populationen vor algorithmengestützter Marginalisierung zu schützen."
    },
    {
      "id": "RAV6DAFQ",
      "stem": "Goldkind_2024_The_end_of_the_world_as_we_know_it_ChatGPT_and",
      "title": "The End of the World as We Know It? ChatGPT and Social Work",
      "author_year": "Goldkind (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "ChatGPT bietet sowohl Chancen (Effizienzsteigerung, Unterstützung von Klient:innen) als auch erhebliche Risiken für die Soziale Arbeit, insbesondere bezüglich der Reproduktion von Bias und der Margina",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Soziale_Arbeit",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI: ChatGPT; AI_Literacies: proaktive Auseinandersetzung mit KI-Kompetenzen für Profession) + SOZIAL (Soziale_Arbeit: direkter Bezug zur Profession; Bias_Ungleichheit: Warnung vor inequitable outcomes). Kommentar-Format mit konzeptionellem Fokus a"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness in Vulnerable Populations",
        "Algorithmic Bias in Social Services",
        "Natural Language Processing (NLP)",
        "Training Data Bias in Language Models",
        "AI Literacy for Social Workers",
        "Professional Competency and Ethical Accountability",
        "Automation of Social Work Tasks"
      ],
      "knowledge_summary": "ChatGPT bietet sowohl Chancen (Effizienzsteigerung, Unterstützung von Klient:innen) als auch erhebliche Risiken für die Soziale Arbeit, insbesondere bezüglich der Reproduktion von Bias und der Marginalisierung vulnerabler Bevölkerungsgruppen. Sozialarbeitende müssen aktiv in den globalen Dialog über KI-Ethik eintreten und die Profession entsprechend ausbilden."
    },
    {
      "id": "8BUHU5EP",
      "stem": "Guerra_2023_Feminist_reflections_for_the_development_of",
      "title": "Feminist reflections for the development of Artificial Intelligence",
      "author_year": "Guerra",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Feministische KI-Entwicklung erfordert nicht nur technische Lösungen, sondern eine Transformation von Praktiken in Design, Produktion, Deployment und Governance unter Berücksichtigung lokaler Kontexte",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja, da methodologische Frameworks für AI-Entwicklung), SOZIAL_OK (Feministisch=Ja durch explizite feministische Perspektive; Gender=Ja, Latin American women focus; Diversitaet=Ja, intersektionale Ansätze; Bias_Ungleichheit=Ja, power-balancing "
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Intersectional Fairness",
        "Gender Bias in Datasets",
        "Design Justice",
        "Algorithmic Bias",
        "Feminist AI Development",
        "Decolonial Feminism in Technology",
        "Tequiología",
        "Care Infrastructure Visibility"
      ],
      "knowledge_summary": "Feministische KI-Entwicklung erfordert nicht nur technische Lösungen, sondern eine Transformation von Praktiken in Design, Produktion, Deployment und Governance unter Berücksichtigung lokaler Kontexte, Care-Arbeit und intersektionaler Machtdynamiken in Lateinamerika."
    },
    {
      "id": "8MDXCTA6",
      "stem": "Hall_2024_A_systematic_review_of_sophisticated_predictive",
      "title": "A systematic review of sophisticated predictive and prescriptive analytics in child welfare: Accuracy, equity, and bias",
      "author_year": "Hall (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Weniger als die Hälfte der Studien befasst sich mit Ethik, Equity oder Bias; nur ein Drittel der Projektteams ist interdisziplinär zusammengesetzt; es fehlt ein einheitlicher Standard zur Vermeidung v",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 93
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch KI_Sonstige (predictive/prescriptive algorithms); SOZIAL_OK durch Soziale_Arbeit (child welfare, social workers), Bias_Ungleichheit (discrimination against low-income families and communities of color), Diversitaet (marginalisierte Communities), Fair"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness Evaluation",
        "Participatory Design in Algorithmic Systems",
        "Algorithmic Bias in Child Welfare",
        "Predictive Risk Modeling in Child Welfare",
        "Disparate Impact Assessment",
        "Ethics Integration in Algorithm Design",
        "Interdisciplinary AI Governance"
      ],
      "knowledge_summary": "Weniger als die Hälfte der Studien befasst sich mit Ethik, Equity oder Bias; nur ein Drittel der Projektteams ist interdisziplinär zusammengesetzt; es fehlt ein einheitlicher Standard zur Vermeidung von Bias und Ungleichheit in algorithmierten Kinderschutzentscheidungen."
    },
    {
      "id": "8MDXCTA6",
      "stem": "Hall_2024_systematic",
      "title": "A systematic review of sophisticated predictive and prescriptive analytics in child welfare: Accuracy, equity, and bias",
      "author_year": "Hall (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Das Allegheny Family Screening Tool (AFST) zeigt verbesserte Vorhersagegenauigkeit gegenüber menschlichen Urteilen, weist aber erhebliche Kalibrierungsprobleme bei schwarzen und weißen Kindern auf (z.",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch KI_Sonstige (predictive/prescriptive algorithms); SOZIAL_OK durch Soziale_Arbeit (child welfare, social workers), Bias_Ungleichheit (discrimination against low-income families and communities of color), Diversitaet (marginalisierte Communities), Fair"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness in Child Welfare",
        "Fairness-Accuracy Trade-offs in Machine Learning",
        "Disproportionality in Child Welfare Systems",
        "Algorithm-Assisted Human Decision Making",
        "Racial Disparities in Algorithmic Decision-Making",
        "Calibration Fairness",
        "Predictive Risk Models in Social Services",
        "Selective Labels Problem"
      ],
      "knowledge_summary": "Das Allegheny Family Screening Tool (AFST) zeigt verbesserte Vorhersagegenauigkeit gegenüber menschlichen Urteilen, weist aber erhebliche Kalibrierungsprobleme bei schwarzen und weißen Kindern auf (z.B. 50% vs. 30% Platzierungsrate bei höchstem Score). In der Praxis werden die Modellvorgaben jedoch schwach umgesetzt, da Supervisoren 1 von 4 obligatorischen Screen-Ins überrufen."
    },
    {
      "id": "YTDMY5W4",
      "stem": "Hauck_2025_A_framework_for_the_learning_and_teaching_of",
      "title": "A framework for the learning and teaching of Critical AI Literacy skills (Version 0.1)",
      "author_year": "Hauck (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 4,
          "stage1_key_finding": "Ein umfassendes Framework für Critical AI Literacy, das technische KI-Konzepte mit kritischer Reflexion, ethischen Überlegungen und EDIA-Prinzipien verbindet und konkrete Handlungsempfehlungen für Leh",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper entwickelt ein Framework für Critical AI Literacy (TECHNIK: AI_Literacies, Generative_KI, Prompting). Substantieller Fokus auf Bias, Ungleichheit, Diversität und Fairness (SOZIAL) durch explizite Thematisierung von Epistemic Injustices, Power Relationships, Inequalities und Equity/Inclusion-Pr"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "AI Tool Evaluation Framework",
        "Epistemic Injustice in AI Systems",
        "Algorithmic Bias Detection",
        "EDIA-Prinzipien in KI-Kontexten",
        "Critical Digital Literacy",
        "Large Language Model (LLM) Literacy",
        "Metacognitive Competencies in AI Contexts",
        "Critical AI Literacy"
      ],
      "knowledge_summary": "Ein umfassendes Framework für Critical AI Literacy, das technische KI-Konzepte mit kritischer Reflexion, ethischen Überlegungen und EDIA-Prinzipien verbindet und konkrete Handlungsempfehlungen für Lehrende bietet."
    },
    {
      "id": "6L6WSDC8",
      "stem": "Hayati_2024_How_Far_Can_We_Extract_Diverse_Perspectives_from",
      "title": "How Far Can We Extract Diverse Perspectives from Large Language Models?",
      "author_year": "Hayati (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "LLMs können diverse Meinungen gemäß dem Grad der Aufgabensubjektivität generieren und erreichen dabei eine ähnliche Diversitätsleistung wie einzelne Menschen, aber weniger als mehrere Menschen zusamme",
          "stage3_completeness": 92,
          "stage3_correctness": 96,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper adressiert LLMs (Generative_KI) und Prompting-Strategien (Diversity Prompting) substanziell. Zentraler Fokus liegt auf Mitigation von \"dominant group bias\" und Erhöhung von Perspektivenvielfalt - klare Bias_Ungleichheit und Diversitaet sowie Fairness-Themen. TECHNIK und SOZIAL erfüllt → Includ"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Parametric Knowledge Compression",
        "Perspective Diversity Extraction",
        "Criteria-Based Diversity Prompting",
        "Semantic Diversity Measurement",
        "In-Context Prompting",
        "Diversity Saturation Point",
        "Algorithmic Fairness through Pluralism"
      ],
      "knowledge_summary": "LLMs können diverse Meinungen gemäß dem Grad der Aufgabensubjektivität generieren und erreichen dabei eine ähnliche Diversitätsleistung wie einzelne Menschen, aber weniger als mehrere Menschen zusammen. Kriterienbasiertes Prompting ermöglicht bessere Grundierung diverse Perspektiven."
    },
    {
      "id": "RM833D5N",
      "stem": "He_2024_On_the_steerability_of_large_language_models",
      "title": "On the steerability of large language models",
      "author_year": "He (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Datengesteuerte Personas, die durch kollaboratives Filtern definiert werden, ermöglichen eine 57-77% bessere Steuerung von LLMs gegenüber Baseline-Methoden und erfassen nuanciertere soziale Gruppen al",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.6,
            "categories": [
              "Generative_KI",
              "Prompting"
            ],
            "reasoning": "Paper behandelt Steerability von LLMs (Generative_KI: Ja, Prompting: Ja). Dies sind Technik-Kategorien. Jedoch fehlt jeglicher Bezug zu sozialen Aspekten (Bias, Fairness, Soziale Arbeit, etc.). Ohne Abstract kann keine substantielle Einschätzung des Inhalts erfolgen, aber auf Basis des Titels liegt "
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Demographic Limitations in Group Segmentation",
        "Collaborative Filtering for NLP",
        "Algorithmic Fairness through Diversity",
        "Opinion Representation Bias",
        "LLM Steerability",
        "Prefix-Tuning",
        "Data-Driven Personas"
      ],
      "knowledge_summary": "Datengesteuerte Personas, die durch kollaboratives Filtern definiert werden, ermöglichen eine 57-77% bessere Steuerung von LLMs gegenüber Baseline-Methoden und erfassen nuanciertere soziale Gruppen als demografische Merkmale allein."
    },
    {
      "id": "RM833D5N",
      "stem": "He_2024_steerability",
      "title": "On the steerability of large language models",
      "author_year": "He (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Aktuelle Modelle zeigen begrenzte Steerability durch Prompting, mit asymmetrischen Fähigkeiten über Persona-Dimensionen und bei steering-Richtungen. Größere Modelle sind steuerbarer, aber alle zeigen ",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.6,
            "categories": [
              "Generative_KI",
              "Prompting"
            ],
            "reasoning": "Paper behandelt Steerability von LLMs (Generative_KI: Ja, Prompting: Ja). Dies sind Technik-Kategorien. Jedoch fehlt jeglicher Bezug zu sozialen Aspekten (Bias, Fairness, Soziale Arbeit, etc.). Ohne Abstract kann keine substantielle Einschätzung des Inhalts erfolgen, aber auf Basis des Titels liegt "
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "In-Context Learning via Persona Statements",
        "Baseline Personality Bias",
        "Prompt Steerability",
        "Steerability Benchmark",
        "Algorithmic Fairness in Value Representation",
        "AI Pluralism",
        "Asymmetric Steerability"
      ],
      "knowledge_summary": "Aktuelle Modelle zeigen begrenzte Steerability durch Prompting, mit asymmetrischen Fähigkeiten über Persona-Dimensionen und bei steering-Richtungen. Größere Modelle sind steuerbarer, aber alle zeigen Schwierigkeiten, von ihrer Baseline-Persönlichkeit abzuweichen."
    },
    {
      "id": "E4G328PD",
      "stem": "Heinz_2025_Clinical_trial_of_an_LLM-based_conversational_AI",
      "title": "Clinical trial of an LLM-based conversational AI psychotherapy",
      "author_year": "Heinz (March 27, 2025)",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Therabot-Nutzer zeigten signifikant größere Symptomreduktionen für MDD (d=0.845-0.903), GAD (d=0.794-0.840) und CHR-FED (d=0.627-0.819) mit hoher Engagement-Rate (>6 Stunden durchschnittliche Nutzung)",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [
              "Generative_KI"
            ],
            "reasoning": "Das Paper untersucht LLM-basierte Psychotherapie in klinischem Kontext. Während es Generative KI behandelt (RCT mit Chatbot), fehlt der Bezug zur Sozialen Arbeit vollständig. Die Anwendung ist klinische Psychologie/Psychiatrie, nicht Soziale Arbeit. Keine Thematisierung von Bias, Fairness, Gender, D"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Generative AI-Based Psychotherapy",
        "Long-Term Treatment Durability and Follow-Up Assessment",
        "Digital Therapeutics for Mental Health",
        "Healthcare Accessibility and Scalability",
        "Selection Bias in Digital Health Recruitment",
        "Safety Guardrails in Clinical AI Systems",
        "Third-Wave Cognitive-Behavioral Therapy",
        "Therapeutic Alliance in AI-Mediated Interactions"
      ],
      "knowledge_summary": "Therabot-Nutzer zeigten signifikant größere Symptomreduktionen für MDD (d=0.845-0.903), GAD (d=0.794-0.840) und CHR-FED (d=0.627-0.819) mit hoher Engagement-Rate (>6 Stunden durchschnittliche Nutzung) und therapeutischer Allianz vergleichbar mit menschlichen Therapeuten."
    },
    {
      "id": "SSH3LVN6",
      "stem": "Hermann_2022_Artificial_intelligence_and_mass_personalization",
      "title": "Artificial Intelligence and Intersectionality",
      "author_year": "Ulnicane (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Die konzeptuelle Analyse offenbart Interdependenzen und Spannungen zwischen ethischen Prinzipien (Beneficence, Non-maleficence, Autonomy, Justice, Explicability) bei der KI-gestützten Massenpersonalis",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "TECHNIK_OK: KI_Sonstige=Ja (algorithmische Systeme und Bias-Analyse). SOZIAL_OK: Bias_Ungleichheit=Ja, Gender=Ja, Diversitaet=Ja, Feministisch=Ja (explizite intersektionale Perspektive nach Crenshaw), Fairness=Ja. Alle Bedingungen erfüllt. Paper nutzt feministische/intersektionale Theorie substantie"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM hat 'Fairness' als erfüllte Kategorie kodiert, obwohl der Human diese ablehnt – trotz Übereinstimmung in allen anderen technischen und sozialen Kategorien. Dies deutet auf eine Überausdehnung des Fairness-Konzepts hin, wo das LLM algorithmische Fairness/Bias-Analyse automatisch als 'Fairness'-Kategorie interpretiert, während der Human eine engere oder andersartige Definition dieser Kategorie anzulegen scheint.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Filter Bubbles and Echo Chambers",
        "AI Literacy",
        "Algorithmic Mass Personalization",
        "Algorithmic Transparency and Explicability",
        "Algorithmic Bias and Discrimination",
        "AI Ethics Principles"
      ],
      "knowledge_summary": "Die konzeptuelle Analyse offenbart Interdependenzen und Spannungen zwischen ethischen Prinzipien (Beneficence, Non-maleficence, Autonomy, Justice, Explicability) bei der KI-gestützten Massenpersonalisierung. AI-Literalität wird als Schlüsselmittel zur Befähigung von Individuen vorgeschlagen, um mit personalisierten Inhalten auf Weise umzugehen, die individuelles und gesellschaftliches Wohlbefinden"
    },
    {
      "id": "QXDK8Z6I",
      "stem": "Himmelreich_2022_Artificial_Intelligence_and_Structural_Injustice",
      "title": "Artificial Intelligence and Structural Injustice: Foundations for Equity, Values, and Responsibility",
      "author_year": "Himmelreich (2022)",
      "year": 2022,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 5,
          "stage1_key_finding": "Strukturelle Ungerechtigkeit bietet sowohl ein analytisches als auch ein normatives Rahmenwerk für KI-Governance und ermöglicht es, systemische Bias-Muster zu identifizieren, die Harm-and-Benefits-Ans",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch KI_Sonstige (algorithmische Systeme und AI Bias); SOZIAL_OK durch Bias_Ungleichheit (strukturelle Gerechtigkeit), Diversitaet (Diversity/Equity/Inclusion) und Fairness (Frameworks für faire KI-Governance). Theoretischer Ansatz basierend auf Young's S"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "AI Governance",
        "Algorithmic Fairness",
        "Substantive Responsibility",
        "Racial Bias in AI",
        "Diversity, Equity, and Inclusion (DEI)",
        "Ethics Washing",
        "Algorithmic Bias in Healthcare",
        "Structural Injustice"
      ],
      "knowledge_summary": "Strukturelle Ungerechtigkeit bietet sowohl ein analytisches als auch ein normatives Rahmenwerk für KI-Governance und ermöglicht es, systemische Bias-Muster zu identifizieren, die Harm-and-Benefits-Ansätze übersehen würden. Ein struktureller Gerechtigkeitsansatz basiert auf Iris Marion Youngs Theorie und liefert tiefere Grundlagen für Diversity, Equity und Inclusion als alternative governance-Ansät"
    },
    {
      "id": "MPCZVZEW",
      "stem": "Hooshyar et al._2025_Towards_responsible_AI_for_education_Hybrid",
      "title": "Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room",
      "author_year": "Goellner (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Neural-Symbolic AI und hybrid human-AI Methoden bieten einen vielversprechenden Weg zur Entwicklung verantwortungsvoller KI-Systeme im Bildungsbereich, indem sie Domain-Wissen mit datengesteuerten Ans",
          "stage3_completeness": 88,
          "stage3_correctness": 94,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper adressiert AI Literacy durch Forderung nach Stakeholder-Involvement und Transparenz in KI-Systemen für Bildung. KI_Sonstige zutreffend: Neural-Symbolic AI ist ML-Ansatz. Bias_Ungleichheit und Fairness erfüllt durch Kritik an irresponsible KI-Use, unreliablen XAI-Methoden und Forderung nach Ver"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Knowledge Tracing",
        "Stakeholder-Centered AI Design",
        "Learner Modelling",
        "Domain-Specific AI for Education",
        "Neural-Symbolic AI",
        "Responsible AI in Education",
        "Algorithmic Fairness in Education",
        "Explainable AI (XAI)"
      ],
      "knowledge_summary": "Neural-Symbolic AI und hybrid human-AI Methoden bieten einen vielversprechenden Weg zur Entwicklung verantwortungsvoller KI-Systeme im Bildungsbereich, indem sie Domain-Wissen mit datengesteuerten Ansätzen kombinieren und damit Fairness, Transparenz und Kontextangemessenheit verbessern."
    },
    {
      "id": "JZ4P8V8S",
      "stem": "Jaakkola_2024_Operationalizing_positive-constructive_pedagogy",
      "title": "Operationalizing positive-constructive pedagogy to artificial intelligence: the 3E model for scaffolding AI technology adoption",
      "author_year": "Thwaites (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": false,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Das 3E-Modell (Enter-Experience-Exit) bietet einen praktischen pädagogischen Rahmen zur Operationalisierung positive-konstruktiver Pädagogik für AI-Adoption, indem es Lernende durch persönliche Erfahr",
          "stage3_completeness": 88,
          "stage3_correctness": 94,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "Prompting",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies Ja: Pedagogisches Framework für kritische KI-Kompetenzentwicklung; Prompting Ja: Expliziter Fokus auf 'critical prompting' in Explore-Phase). SOZIAL_OK (Bias_Ungleichheit Ja: Substantielle Behandlung von Bias-Erkennung und Sichtbarmachung vo"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Positive-Constructive Pedagogy",
        "Algorithmic Bias in Generative Systems",
        "AI Literacy",
        "Prompt Engineering Literacy",
        "Structural Awareness in Technology Adoption",
        "Perception-Reality Gap in AI",
        "Media Representation Gap in AI Discourse",
        "3E Model (Enter-Experience-Exit)"
      ],
      "knowledge_summary": "Das 3E-Modell (Enter-Experience-Exit) bietet einen praktischen pädagogischen Rahmen zur Operationalisierung positive-konstruktiver Pädagogik für AI-Adoption, indem es Lernende durch persönliche Erfahrung, Experimentiertätigkeit und reflektierte Praxis führt."
    },
    {
      "id": "H59BNSX8",
      "stem": "James_2023_Algorithmic_decision-making_in_social_work",
      "title": "Algorithmic decision-making in social work practice and pedagogy: confronting the competency/critique dilemma",
      "author_year": "James (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Sozialarbeitseducation muss ein Spannungsverhältnis zwischen technischer Kompetenz und kritischer Analyse auflösen, indem sie theoriegeleitet und praxisorientiert ADM-Systeme als Werkzeuge (nicht als ",
          "stage3_completeness": 88,
          "stage3_correctness": 93,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Das Paper adressiert algorithmic decision-making in Sozialarbeit (KI_Sonstige + Soziale_Arbeit). Der Titel hebt die Competency/Critique-Dilemma hervor, was auf AI Literacies verweist. Bias, Fairness und ethische Bedenken in Algorithmen sind zentral (Bias_Ungleichheit + Fairness). TECHNIK erfüllt (3x"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Black Box Transparency in Algorithmic Systems",
        "Social Work Pedagogy and Curriculum Development",
        "Competency/Critique Dilemma",
        "Algorithmic Decision-Making (ADM) Systems",
        "Intersectional Vulnerability and ADM",
        "Algorithmic Bias in Welfare Systems",
        "Algorithmic Literacy",
        "Algorithmic Fairness in Social Work"
      ],
      "knowledge_summary": "Sozialarbeitseducation muss ein Spannungsverhältnis zwischen technischer Kompetenz und kritischer Analyse auflösen, indem sie theoriegeleitet und praxisorientiert ADM-Systeme als Werkzeuge (nicht als Rahmen) behandelt, um sowohl ihre positiven als auch ihre diskriminierenden Potenziale zu adressieren."
    },
    {
      "id": "CLKAD87H",
      "stem": "James_2025_Responsible_prompting_recommendation_Fostering",
      "title": "Responsible prompting recommendation: Fostering responsible AI practices in prompting-time",
      "author_year": "James (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": false,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Responsible Prompting Recommendations haben das Potenzial, anfängliche Prompt-Engineers zu unterstützen und Bewusstsein für Responsible AI zu schärfen, sollten aber gleichzeitig Prompt-Ähnlichkeit und",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.87,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (Kompetenzentwicklung für Prompt Engineers), Generative_KI (LLM-Fokus), Prompting (Kernthema: Prompt Recommendations). SOZIAL: Soziale_Arbeit (explizite Relevanz für social services), Bias_Ungleichheit (harmful content removal), Fairness (Respo"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Multilingual Fairness in Generative AI",
        "Responsible Prompting",
        "Algorithmic Bias in Language Models",
        "Human-in-the-Loop Validation",
        "Adversarial Prompting and Jailbreaking",
        "Prompt Engineering",
        "Recommender System for Responsible AI",
        "Role-Differentiated AI Practices"
      ],
      "knowledge_summary": "Responsible Prompting Recommendations haben das Potenzial, anfängliche Prompt-Engineers zu unterstützen und Bewusstsein für Responsible AI zu schärfen, sollten aber gleichzeitig Prompt-Ähnlichkeit und Diversität sozialer Werte maximieren."
    },
    {
      "id": "QG8IAB53",
      "stem": "Jarke_2024_Who_cares_about_data_Data_care_arrangements_in",
      "title": "Who cares about data? Data care arrangements in everyday organisational practice",
      "author_year": "Jarke (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Datenpflege ist nicht selbstverständlich, sondern ein soziomaterielles Accomplishment, das durch zugeschriebene Datenwerte und konkrete Pflegearbeiten in stabilen Datenpflegearrangements konfiguriert ",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "Soziale_Arbeit"
            ],
            "reasoning": "Das Paper behandelt Datenmanagement und -pflege in Organisationen (Sozialarbeit, Bildung), nicht aber KI/ML-Systeme. Es fehlt jede technische KI-Komponente (keine AI_Literacies, Generative_KI, Prompting oder KI_Sonstige). Das Thema ist zu weit entfernt von KI-Anwendungen. Nur eine SOZIAL-Kategorie e"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Feminist Science and Technology Studies (STS)",
        "Invisible Data Labour",
        "Data Care Arrangements",
        "Data Quality as Organizational Achievement",
        "Data Valuation Practices",
        "Competing Care Obligations",
        "Care Ethics in Data Management"
      ],
      "knowledge_summary": "Datenpflege ist nicht selbstverständlich, sondern ein soziomaterielles Accomplishment, das durch zugeschriebene Datenwerte und konkrete Pflegearbeiten in stabilen Datenpflegearrangements konfiguriert wird. Heterogene Arrangements entstehen, wenn Akteure unterschiedliche Werte (discovery, truthiness, actionability, self-evidence) Daten zuschreiben."
    },
    {
      "id": "NQEUZQF9",
      "stem": "Jarke_2025_Datafied_ageing_futures_Regimes_of_anticipation",
      "title": "Datafied ageing futures: Regimes of anticipation and participatory futuring",
      "author_year": "Jarke (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Partizipative Futuring-Methoden ermöglichen es Teilnehmenden, ihre eigenen Antizipationen zu hinterfragen und kritisch zu reflektieren, führen aber aufgrund von 'diskursiver Schließung' nicht automati",
          "stage3_completeness": 88,
          "stage3_correctness": 93,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.82,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper behandelt Datengetriebene Systeme (KI_Sonstige) und deren Auswirkungen auf vulnerable Bevölkerungsgruppen (ältere Erwachsene). Adressiert Bias durch ageist assumptions (Bias_Ungleichheit), partizipative Inklusion marginalisierter Gruppen (Diversitaet). Verwendet implizit-feministische Perspekt"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Participatory Futuring",
        "Feminist Care Ethics in Technology Design",
        "Algorithmic Surveillance and Control",
        "Intersectional Digital Justice",
        "Discursive Closure",
        "Regimes of Anticipation",
        "Ageism in Data-Driven Systems"
      ],
      "knowledge_summary": "Partizipative Futuring-Methoden ermöglichen es Teilnehmenden, ihre eigenen Antizipationen zu hinterfragen und kritisch zu reflektieren, führen aber aufgrund von 'diskursiver Schließung' nicht automatisch zu radikal anderen Zukunftsimaginären. Die Herausforderung liegt darin, über etablierte ageistische und technologiedeterministische Narrationen hinauszudenken."
    },
    {
      "id": "JMAWNUEV",
      "stem": "Jin_2025_GLAT_The_generative_AI_literacy_assessment_test",
      "title": "GLAT: The generative AI literacy assessment test",
      "author_year": "Jin (12/2025)",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": false,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "GLAT ist ein valides und reliables 20-Item Multiple-Choice-Instrument zur Messung von GenAI-Literalität (Cronbach's alpha = 0.80; omega total = 0.81; RMSEA = 0.03; CFI = 0.97) und übertrifft selbstber",
          "stage3_completeness": 92,
          "stage3_correctness": 96,
          "stage3_overall": 90
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "Generative_KI"
            ],
            "reasoning": "Das Paper behandelt ein Assessment-Tool für generative KI-Kompetenzen und liegt damit im Bereich AI Literacies. Es fehlt jedoch jeder erkennbare Bezug zu sozialen Dimensionen (Soziale Arbeit, Bias, Gender, Diversität, Fairness). Das Paper adressiert rein bildungstechnische Aspekte von KI-Kompetenzer"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Critical AI Output Evaluation",
        "Generative AI Literacy",
        "Prompt Engineering Competency",
        "AI Interpretability and Trustworthiness",
        "Psychometric Validation (IRT/CFA)",
        "Performance-Based Assessment",
        "Algorithmic Bias in Hiring Systems"
      ],
      "knowledge_summary": "GLAT ist ein valides und reliables 20-Item Multiple-Choice-Instrument zur Messung von GenAI-Literalität (Cronbach's alpha = 0.80; omega total = 0.81; RMSEA = 0.03; CFI = 0.97) und übertrifft selbstberichtete Maße in der Vorhersage tatsächlicher Lernleistung in GenAI-gestützten Aufgaben."
    },
    {
      "id": "YLAKP7Z2",
      "stem": "Jääskeläinen_2025_Intersectional_analysis_of_visual_generative_AI",
      "title": "Intersectional analysis of visual generative AI: The case of Stable Diffusion",
      "author_year": "Jääskeläinen (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Stable Diffusion perpetuiert systematisch bestehende Machtstrukturen (Sexismus, Rassismus, Heteronormativität, Ableismus) durch die Annahme eines standardisierten Individuums als weiß, nicht-behindert",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch Generative_KI (Stable Diffusion-Analyse). SOZIAL_OK durch Bias_Ungleichheit (Stereotype, Diskriminierung), Gender (Sexismus-Fokus), Diversitaet (Inklusion marginalisierter Gruppen), und Feministisch (explizite feministische intersektionale Methodik n"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Coded Gaze in AI Systems",
        "Intersectionality",
        "Reparative Justice Approach",
        "Cultural Bias in Training Data",
        "Algorithmic Bias in Visual Generative AI",
        "Representation Gaps in Generative AI",
        "Feminist Science and Technology Studies (STS)"
      ],
      "knowledge_summary": "Stable Diffusion perpetuiert systematisch bestehende Machtstrukturen (Sexismus, Rassismus, Heteronormativität, Ableismus) durch die Annahme eines standardisierten Individuums als weiß, nicht-behindert und maskulin präsentierend. Die Technologie reproduziert kontinuierlich schädliche Bilder durch ihre Euro- und Nordamerika-zentrische kulturelle Ausrichtung."
    },
    {
      "id": "RARE5UFC",
      "stem": "Kamruzzaman_2024_Prompting",
      "title": "Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes",
      "author_year": "Kamruzzaman (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Eine Kombination aus Human Persona, System 2 Prompting und explizitem Debiasing führt zu substantiellen Reduktionen von stereotypischen Urteilen (bis zu 33% Reduktion), wobei die optimale Kombination ",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper behandelt systematisch Prompting-Strategien (Ja: substantieller Fokus auf 12 Prompt-Varianten) zur Bias-Reduktion in LLMs (Ja: Generative_KI). Bias-Reduktion und Fairness (Ja: beide Kategorien) sind zentrale Themen. TECHNIK-Bedingung erfüllt (Prompting + Generative_KI), SOZIAL-Bedingung erfüll"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Keyword-Inklusion",
          "divergence_justification": "Das LLM hat 'Prompting techniques' und 'social bias' als explizite Keywords erkannt und automatisch AI_Literacies zugeordnet, ohne zu erkennen, dass der Paper sich auf allgemeine LLM-Bias (nicht auf Soziale Arbeit) bezieht. Die Inklusion basiert auf oberflächlicher Keyword-Matching statt kontextueller Relevanz für das Feld.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Algorithmic Bias Evaluation",
        "Social Bias Mitigation through Prompting",
        "AI Literacy for End-Users",
        "Human Persona Integration",
        "Dual-Process Theory in LLMs",
        "Chain-of-Thought Prompting",
        "Intersectional Bias Categories"
      ],
      "knowledge_summary": "Eine Kombination aus Human Persona, System 2 Prompting und explizitem Debiasing führt zu substantiellen Reduktionen von stereotypischen Urteilen (bis zu 33% Reduktion), wobei die optimale Kombination je nach Modell und Bias-Kategorie variiert."
    },
    {
      "id": "RARE5UFC",
      "stem": "Kamruzzaman_2024_Prompting_techniques_for_reducing_social_bias_in",
      "title": "Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes",
      "author_year": "Kamruzzaman (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Human Persona kombiniert mit System 2 Prompting reduziert stereotypische Urteile von LLMs am effektivsten (bis zu 13% Reduktion), während CoT-Prompting entgegen bisheriger Annahmen nicht dem System 2 ",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper behandelt systematisch Prompting-Strategien (Ja: substantieller Fokus auf 12 Prompt-Varianten) zur Bias-Reduktion in LLMs (Ja: Generative_KI). Bias-Reduktion und Fairness (Ja: beide Kategorien) sind zentrale Themen. TECHNIK-Bedingung erfüllt (Prompting + Generative_KI), SOZIAL-Bedingung erfüll"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Keyword-Inklusion",
          "divergence_justification": "Das LLM hat 'Prompting techniques' und 'social bias' als explizite Keywords erkannt und automatisch AI_Literacies zugeordnet, ohne zu erkennen, dass der Paper sich auf allgemeine LLM-Bias (nicht auf Soziale Arbeit) bezieht. Die Inklusion basiert auf oberflächlicher Keyword-Matching statt kontextueller Relevanz für das Feld.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Algorithmic Bias Reduction",
        "Intersectional Bias Assessment",
        "Chain-of-Thought Prompting Limitations",
        "Self-Distancing Effects",
        "Dual-Process Theory in NLP",
        "Stereotyping in Language Models",
        "Prompt Engineering for Fairness"
      ],
      "knowledge_summary": "Human Persona kombiniert mit System 2 Prompting reduziert stereotypische Urteile von LLMs am effektivsten (bis zu 13% Reduktion), während CoT-Prompting entgegen bisheriger Annahmen nicht dem System 2 Denken entspricht, sondern dem System 1 ähnlicher ist."
    },
    {
      "id": "CYZQ6XPK",
      "stem": "Kaneko_2024_Debiasing_prompts_for_gender_bias_in_large",
      "title": "Evaluating gender bias in large language models via chain-of-thought prompting",
      "author_year": "Kaneko (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "CoT-Prompting reduziert systematisch Geschlechterbias in LLMs signifikant, indem es Modelle dazu zwingt, ihre versteckten Annahmen über Geschlechterstereotypen explizit zu artikulieren, selbst bei ein",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Fairness"
            ],
            "reasoning": "Paper behandelt substantiell Generative KI (LLMs), Prompting (Chain-of-Thought als zentrale Intervention), Gender-Bias in LLM-Outputs und Fairness-Aspekte (Bias-Reduktion). Erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting) und SOZIAL_OK (Bias_Ungleichheit + Gender + Fairness). Experi"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Gender Bias in Large Language Models",
        "Word Embedding Bias",
        "Intrinsic versus Extrinsic Bias Evaluation",
        "Occupational Gender Stereotyping",
        "Chain-of-Thought Prompting",
        "Algorithmic Fairness Benchmarking",
        "Bias Mitigation through Explainability"
      ],
      "knowledge_summary": "CoT-Prompting reduziert systematisch Geschlechterbias in LLMs signifikant, indem es Modelle dazu zwingt, ihre versteckten Annahmen über Geschlechterstereotypen explizit zu artikulieren, selbst bei einfachen Zählaufgaben.",
      "featured": {
        "why": "Volle Uebereinstimmung: Pipeline und Mensch erkennen Gender-Bias-Forschung als relevant",
        "stance_highlight": "result"
      }
    },
    {
      "id": "CYZQ6XPK",
      "stem": "Kaneko_2024_Evaluating_gender_bias_in_large_language_models",
      "title": "Evaluating gender bias in large language models via chain-of-thought prompting",
      "author_year": "Kaneko (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "CoT-Prompting reduziert Geschlechtsbias in LLMs signifikant, indem es Modelle zwingt, ihre Vorhersageprozesse explizit zu artikulieren, auch wenn einfache Debiasing-Prompts allein ineffektiv sind. Der",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Fairness"
            ],
            "reasoning": "Paper behandelt substantiell Generative KI (LLMs), Prompting (Chain-of-Thought als zentrale Intervention), Gender-Bias in LLM-Outputs und Fairness-Aspekte (Bias-Reduktion). Erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting) und SOZIAL_OK (Bias_Ungleichheit + Gender + Fairness). Experi"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Fairness Evaluation",
        "Stereotype Occupational Associations",
        "Gender Bias in Large Language Models",
        "Debiasing Prompt Strategies",
        "Multi-step Gender Bias Reasoning Benchmark",
        "Chain-of-Thought Prompting",
        "Intrinsic vs. Extrinsic Bias Measurement"
      ],
      "knowledge_summary": "CoT-Prompting reduziert Geschlechtsbias in LLMs signifikant, indem es Modelle zwingt, ihre Vorhersageprozesse explizit zu artikulieren, auch wenn einfache Debiasing-Prompts allein ineffektiv sind. Der MGBR-Benchmark zeigt hohe Korrelation mit downstream-Task-Bias (BBQ, BNLI) aber niedrige Korrelation mit intrinsischen Bias-Metriken."
    },
    {
      "id": "7L93JBLR",
      "stem": "Karagianni_2025_Gender_in_a_stereo-(gender)typical_EU_AI_law_A",
      "title": "Gender in a stereo-(gender)typical EU AI law: A feminist reading of the AI Act",
      "author_year": "Karagianni (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 5,
          "stage1_key_finding": "Obwohl die AI Act Massnahmen zur Mitigation geschlechterspezifischer Risiken vorsieht, adressiert sie nicht die strukturellen Biase in KI-Technologien, die marginalisierte Gruppen überproportional sch",
          "stage3_completeness": 92,
          "stage3_correctness": 97,
          "stage3_overall": 93
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Das Paper erfüllt TECHNIK (KI_Sonstige: algorithmische Systeme und deren Regulierung) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness). Es nutzt explizit feministische Theorie (hermeneutical injustice, feminist legal theory, Intersektionalität) zur Kritik von KI-Governance"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Intersectional Algorithmic Discrimination",
        "Algorithmic Gender Bias",
        "Structural Gender Bias in AI Regulation",
        "Epistemic Injustice in AI Governance",
        "Gender-based AI Violence",
        "Feminist Impact Assessment",
        "AI-driven Misgendering",
        "Decolonial Feminist Epistemology in AI Ethics"
      ],
      "knowledge_summary": "Obwohl die AI Act Massnahmen zur Mitigation geschlechterspezifischer Risiken vorsieht, adressiert sie nicht die strukturellen Biase in KI-Technologien, die marginalisierte Gruppen überproportional schädigen. Eine intersektionale feministische Perspektive enthüllt formale Gleichheitskonzepte, die substantive Ungleichheiten perpetuieren."
    },
    {
      "id": "A776TPGG",
      "stem": "Karagianni_2025_The_EU_artificial_intelligence_act_through_a",
      "title": "The EU artificial intelligence act through a gender lens",
      "author_year": "Karagianni",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Das EU-AI-Gesetz enthält zwar Verweise auf Nicht-Diskriminierung, verfügt aber über kritische Lücken bei der expliziten Behandlung von Geschlechtergerechtigkeit und feministischen Perspektiven. Eine i",
          "stage3_completeness": 92,
          "stage3_correctness": 96,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.75,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Fairness"
            ],
            "reasoning": "Paper analysiert EU AI Act durch Gender-Lens: KI_Sonstige (algorithmische Systeme unter Regulierung) + Gender (expliziter Geschlechterfokus) + Bias_Ungleichheit (Geschlechtergerechtigkeit) + Fairness (Regulierungsanalyse). Beide Bedingungen erfüllt: TECHNIK_OK (KI_Sonstige), SOZIAL_OK (Gender + Bias"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Intersectionality in AI Governance",
        "High-Risk AI Systems Regulation",
        "Algorithmic Fairness Auditing",
        "Feminist Legal Analysis of AI Regulation",
        "Gender Bias in AI Systems",
        "Gender-Neutral Language in AI Policy",
        "Feminist Data Protection"
      ],
      "knowledge_summary": "Das EU-AI-Gesetz enthält zwar Verweise auf Nicht-Diskriminierung, verfügt aber über kritische Lücken bei der expliziten Behandlung von Geschlechtergerechtigkeit und feministischen Perspektiven. Eine intersektionale, geschlechtersensible Überarbeitung ist notwendig, um marginalisierte Gruppen effektiv zu schützen."
    },
    {
      "id": "3QUWCYVW",
      "stem": "Kattnig_2024_Assessing_trustworthy_AI_Technical_and_legal",
      "title": "Assessing trustworthy AI: Technical and legal perspectives of fairness in AI",
      "author_year": "Kattnig (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Obwohl zahlreiche technische Methoden zur Bias-Mitigation existieren, erfüllen nur wenige die bestehenden EU-Rechtsanforderungen. Eine interdisziplinäre Herangehensweise ist notwendig, um die Kluft zw",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper behandelt algorithmische Systeme (KI_Sonstige: Ja) mit substantiellem Fokus auf Bias-Mitigation und Diskriminierung (Bias_Ungleichheit: Ja) sowie Fairness-Konzepte (Fairness: Ja). Kritische Analyse von Fairness-Definitionen und Bias-Mitigation-Methoden erfüllt beide Bedingungen (TECHNIK + SOZI"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Bias Mitigation",
        "Procedural Fairness",
        "AI Literacy",
        "Algorithmic Bias",
        "Automated Decision-Making Systems",
        "Group Fairness"
      ],
      "knowledge_summary": "Obwohl zahlreiche technische Methoden zur Bias-Mitigation existieren, erfüllen nur wenige die bestehenden EU-Rechtsanforderungen. Eine interdisziplinäre Herangehensweise ist notwendig, um die Kluft zwischen technischen und rechtlichen Konzepten von Fairness zu überbrücken."
    },
    {
      "id": "JC7X3MM7",
      "stem": "Kawakami_2022_Improving_human-AI_partnerships_in_child_welfare",
      "title": "Improving human-AI partnerships in child welfare: Understanding worker practices, challenges, and desires for algorithmic decision support",
      "author_year": "Kawakami (April 30)",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Obwohl das AFST seit fünf Jahren im Einsatz ist, bleibt es eine Spannungsquelle für Arbeiter, die das System als verpasste Gelegenheit zur wirksamen Ergänzung ihrer Fähigkeiten wahrnehmen. Die Abhängi",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper adressiert Allegheny Family Screening Tool (algorithmisches Entscheidungssystem in der Jugendhilfe). TECHNIK erfüllt: KI_Sonstige (algorithmische Systeme), AI_Literacies (worker training, tool understanding). SOZIAL erfüllt: Soziale_Arbeit (child welfare practice), Bias_Ungleichheit (discrimin"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit"
            ]
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "Algorithmic Fairness in Child Welfare",
        "Mental Models of AI Systems",
        "Algorithmic Transparency and Explainability",
        "Goal Misalignment Between Algorithms and Practitioners",
        "Organizational Pressures on Algorithmic Override",
        "Human-AI Collaboration",
        "Algorithmic Decision Support Systems",
        "Contextual Information in Professional Decision-Making"
      ],
      "knowledge_summary": "Obwohl das AFST seit fünf Jahren im Einsatz ist, bleibt es eine Spannungsquelle für Arbeiter, die das System als verpasste Gelegenheit zur wirksamen Ergänzung ihrer Fähigkeiten wahrnehmen. Die Abhängigkeit der Arbeiter vom AFST wird durch vier Faktoren gesteuert: kontextuelle Informationen jenseits des Modells, Überzeugungen über die Modellleistung, organisatorische Drücke und Misalignments zwisch"
    },
    {
      "id": "Kim_2021_Why_and_What_to_Teach_AI_Curriculum_for",
      "stem": "Kim_2021_Why_and_What_to_Teach_AI_Curriculum_for",
      "title": "Kim_2021_Why_and_What_to_Teach_AI_Curriculum_for",
      "author_year": "Kim",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": false
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Ein KI-Curriculum basierend auf drei Kompetenzen (AI Knowledge, AI Skill, AI Attitude) führt zu statistisch signifikanten Verbesserungen der KI-Literalität bei Grundschülern, besonders in Wissenserwer",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {}
      },
      "concepts": [
        "Data Quality and Fair Machine Learning",
        "Algorithmic Bias in Education",
        "Computational Thinking",
        "AI Literacy",
        "Gender Bias in AI Systems",
        "K-12 Curriculum Design",
        "Critical AI Literacy"
      ],
      "knowledge_summary": "Ein KI-Curriculum basierend auf drei Kompetenzen (AI Knowledge, AI Skill, AI Attitude) führt zu statistisch signifikanten Verbesserungen der KI-Literalität bei Grundschülern, besonders in Wissenserwerb und Fähigkeitsentwicklung."
    },
    {
      "id": "YMYHLMFS",
      "stem": "Klein_2024_Data",
      "title": "Data feminism for AI",
      "author_year": "Klein (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Die sieben Prinzipien der Data Feminism (Macht untersuchen, Macht herausfordern, Binäre und Hierarchien überdenken, Emotion und Verkörperung erheben, Kontext berücksichtigen, Pluralismus umarmen, Arbe",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch KI_Sonstige (allgemeiner KI/ML-Bezug); SOZIAL_OK durch Bias_Ungleichheit, Gender, Diversitaet und Feministisch (explizite Verwendung des Data-Feminism-Frameworks von D'Ignazio, intersektionale feministische Perspektive). Starker konzeptioneller Beitr"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM hat die Kategorien Generative_KI und Fairness korrekt erkannt, überdehnt aber den Bedeutungsraum dieser Kategorien als ausreichend für Inclusion, während es gleichzeitig die thematische Breite von Data Feminism (AI_Literacies, Gender, Diversität, Feministisch) systematisch untererkannt und damit den interdisziplinären Kontext des Papiers nicht erfasst hat.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Large Language Models (LLMs) and Generative AI Ethics",
        "Algorithmic Bias and Structural Inequality",
        "Data Feminism Principles",
        "Colonial Hierarchies in AI Development",
        "Gender-Based AI Harms",
        "Invisible Labor in AI Systems",
        "Consent and Agency in AI",
        "Intersectional Feminism in AI"
      ],
      "knowledge_summary": "Die sieben Prinzipien der Data Feminism (Macht untersuchen, Macht herausfordern, Binäre und Hierarchien überdenken, Emotion und Verkörperung erheben, Kontext berücksichtigen, Pluralismus umarmen, Arbeit sichtbar machen) sind auf KI-Forschung anwendbar und können durch zwei zusätzliche Prinzipien (Umweltauswirkungen, Zustimmung) erweitert werden."
    },
    {
      "id": "QIQR449A",
      "stem": "Klinge_2024_A_sociolinguistic_approach_to_stereotype",
      "title": "A sociolinguistic approach to stereotype assessment in large language models",
      "author_year": "Klinge (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Linguistische Indikatoren aus dem SCSC-Framework ermöglichen eine objektive, interpretierbare und feingranulare Bewertung von Stereotypstärke. Größere Modelle (Llama-3.3-70B, GPT-4) zeigen bessere Per",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.75,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Gender"
            ],
            "reasoning": "Paper analysiert Stereotypen in LLMs aus soziolinguistischer Perspektive. Erfüllt TECHNIK-Bedingung durch Generative_KI (Fokus auf LLMs). Erfüllt SOZIAL-Bedingung durch Bias_Ungleichheit (Stereotype-Assessment) und Gender (implizit durch Stereotype-Analyse). Sociolinguistischer Zugang deutet auf Ana"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness in NLP",
        "Interpretable AI for Bias Mitigation",
        "Linguistic Stereotype Indicators",
        "Representational Harm Prevention",
        "Social Category Bias in Language Models",
        "In-Context Learning for Bias Detection",
        "Stereotype Strength Quantification"
      ],
      "knowledge_summary": "Linguistische Indikatoren aus dem SCSC-Framework ermöglichen eine objektive, interpretierbare und feingranulare Bewertung von Stereotypstärke. Größere Modelle (Llama-3.3-70B, GPT-4) zeigen bessere Performance, und mehr Few-Shot-Beispiele verbessern die Erkennung signifikant."
    },
    {
      "id": "XZ6Z8A9C",
      "stem": "Knowles_2023_Trustworthy_AI_and_the_Logics_of_Intersectional",
      "title": "Trustworthy AI and the Logics of Intersectional Resistance",
      "author_year": "Knowles (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Distrust von KI durch marginalisierte Gruppen ist rational und gerechtfertigt, nicht irrational. Aktuelle Trustworthy AI Ansätze ignorieren die strukturellen Gründe für dieses Misstrauen und können da",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Ja - Kritik an AI-Ethics-Frameworks als algorithmische Systeme) und SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness alle Ja). Explizit intersektional-feministischer Ansatz mit Fokus auf marginalisierte Gruppen und st"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt AI_Literacies und Prompting über ihre beabsichtigte Reichweite aus, indem es allgemeine LLM-Nutzungsrichtlinien und diversity-sensitive techniques als technische Kernthemen klassifiziert, obwohl der Paper primär intersektionale Widerstands-Logiken behandelt. Gleichzeitig erkennt es nicht, dass Gender und Feminismus (Human=Ja) zentral zum kritischen Rahmen gehören.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Trustworthy AI",
        "Intersectional Resistance",
        "Rational Distrust",
        "Algorithmic Injustice",
        "Care Ethics in AI",
        "Digital Underclass Construction",
        "Epistemic Injustice in Machine Learning"
      ],
      "knowledge_summary": "Distrust von KI durch marginalisierte Gruppen ist rational und gerechtfertigt, nicht irrational. Aktuelle Trustworthy AI Ansätze ignorieren die strukturellen Gründe für dieses Misstrauen und können dadurch öffentliches Misstrauen sogar verstärken."
    },
    {
      "id": "H3STST88",
      "stem": "Kojima_2022_Large_language_models_are_zero-shot_reasoners",
      "title": "Large language models are zero-shot reasoners",
      "author_year": "Kojima (2022)",
      "year": 2022,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": false,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": false
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Ein einfaches Single-Prompt-Template ('Let's think step by step') aktiviert Multi-Step-Reasoning in großen Sprachmodellen ohne Few-Shot-Beispiele und zeigt erhebliche Leistungssteigerungen über divers",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 92
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [
              "Generative_KI",
              "Prompting"
            ],
            "reasoning": "Paper behandelt Chain-of-Thought Prompting für LLMs (Generative_KI=Ja, Prompting=Ja). Dies ist eine rein technische Methoden-Studie ohne Bezug zu sozialen Themen. Keine Kategorien aus dem SOZIAL-Set (Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness) sind erfüllt. Damit "
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "Generative_KI",
              "Prompting"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Zero-Shot Chain-of-Thought Prompting",
        "In-Context Learning",
        "Large Language Model Reasoning Capabilities",
        "Benchmark Evaluation Across Task Domains",
        "Prompt Engineering",
        "Model Scaling Laws"
      ],
      "knowledge_summary": "Ein einfaches Single-Prompt-Template ('Let's think step by step') aktiviert Multi-Step-Reasoning in großen Sprachmodellen ohne Few-Shot-Beispiele und zeigt erhebliche Leistungssteigerungen über diverse Reasoning-Aufgaben hinweg (z.B. 17.7% → 78.7% auf MultiArith), was auf bislang wenig erforschte Zero-Shot-Fähigkeiten von LLMs hindeutet."
    },
    {
      "id": "7VFNS5R3",
      "stem": "Kong_2021_Evaluation_of_an_artificial_intelligence_literacy",
      "title": "Evaluation of an artificial intelligence literacy course for university students with diverse study backgrounds",
      "author_year": "Kong (2021)",
      "year": 2021,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "120 Studierende aus verschiedenen Studienrichtungen machten signifikante Fortschritte beim Verständnis von KI-Konzepten (von 6.31 auf 9.71 Punkte), bei selbstwahrgenommener KI-Literacy (von 2.93 auf 3",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Das Paper behandelt AI Literacy als Bildungsintervention für Studierende mit unterschiedlichen Hintergründen. Während die AI_Literacies-Kategorie klar erfüllt ist (Evaluation eines Kompetenz-Kursus), fehlt jeder Bezug zu sozialen Aspekten wie Bias, Ungleichheit, Gender, Diversität, Fairness oder Soz"
          },
          "human": {
            "decision": "Unclear",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Gender",
              "Diversitaet"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'Generative_KI' und 'Prompting' auf einen Kurs über KI-Literalität aus, der diese Technologien nur als Beispiele behandelt, nicht als Forschungsgegenstand. Die 'Bias_Ungleichheit' und 'Fairness' werden aus allgemeinen Diversity-Inhalten abgeleitet, ohne dass der Paper systematisch diese Kategorien operationalisiert.",
          "disagreement_type": "Human_Unclear",
          "severity": 1
        }
      },
      "concepts": [
        "Gender Stereotypes in STEM Self-Efficacy",
        "Conceptual Understanding of Machine Learning",
        "AI Literacy",
        "AI Empowerment",
        "AI-related Societal Concerns",
        "Flipped Classroom Learning Design",
        "Inclusive AI Education Across Disciplines"
      ],
      "knowledge_summary": "120 Studierende aus verschiedenen Studienrichtungen machten signifikante Fortschritte beim Verständnis von KI-Konzepten (von 6.31 auf 9.71 Punkte), bei selbstwahrgenommener KI-Literacy (von 2.93 auf 3.98) und beim Gefühl der Empowerment (von 3.93 auf 4.06), unabhängig von Vorkenntnissen in Programmierung."
    },
    {
      "id": "Q4LK53XW",
      "stem": "Kong_2022_Are_Intersectionally_Fair_AI_Algorithms_Really",
      "title": "Are \"Intersectionally Fair\" AI Algorithms Really Fair to Women of Color? A Philosophical Analysis",
      "author_year": "Kong (2022)",
      "year": 2022,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Self-Supervised Learning auf großen, diversen Internetdatensätzen (z.B. Instagram mit 1 Milliarde Bildern) führt zu deutlich besseren Fairness-Ergebnissen als Supervised Learning auf ImageNet, besonde",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper kombiniert Technik (KI_Sonstige: algorithmische Fairness-Systeme) mit starkem sozialen Bezug. Erfüllt mehrere SOZIAL-Kategorien: Bias_Ungleichheit (strukturelle Unterdrückungssysteme), Gender (Fokus auf Women of Color), Diversitaet (intersektionale Analyse), Feministisch (kritisch-theoretische"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Visual Feature Extractors",
        "Harmful Label Associations",
        "Intersectional Bias",
        "Fairness Indicators",
        "Algorithmic Auditing",
        "Self-Supervised Learning",
        "Geographic Disparity",
        "Representation Parity"
      ],
      "knowledge_summary": "Self-Supervised Learning auf großen, diversen Internetdatensätzen (z.B. Instagram mit 1 Milliarde Bildern) führt zu deutlich besseren Fairness-Ergebnissen als Supervised Learning auf ImageNet, besonders für unterrepräsentierte Gruppen bezüglich Geschlecht, Hautton und geografische Regionen."
    },
    {
      "id": "BBLJ4RG3",
      "stem": "Kong_2024_Developing_an_artificial_intelligence_literacy",
      "title": "Developing an artificial intelligence literacy framework: Evaluation of a literacy course for senior secondary students using a project-based learning approach",
      "author_year": "Kong (06/2024)",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": false,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Der projektbasierte KI-Literaturkurs führte zu signifikanten Verbesserungen in der Fähigkeit der Schüler:innen, KI-Konzepte zur Problemlösung anzuwenden, sowie in ihren metakognitiven Strategien und i",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 92
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Das Paper entwickelt ein KI-Literacy-Framework und evaluiert einen Kurs für Sekundarschüler – klarer Fokus auf AI Literacies als Kompetenzentwicklung. Jedoch kein Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität, Feminismus oder Fairness erkennbar. Erfüllt nur TECHNIK-Bedingung, nicht"
          },
          "human": {
            "decision": "Unclear",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Gender",
              "Diversitaet"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Keyword-Inklusion",
          "divergence_justification": "Das LLM hat die Keywords 'AI literacy' und 'framework' im Titel erkannt und diese automatisch den definierten Kategorien zugeordnet (AI_Literacies, Generative_KI), ohne zu erkennen, dass das Paper ein Schulkurs-Evaluationsstudium zu KI-Grundlagen ist und nicht den Inklusions-/Fairness-Fokus des Reviews erfüllt. Die Human-Entscheidung 'Unclear' deutet darauf hin, dass die Passung tatsächlich marginal ist.",
          "disagreement_type": "Human_Unclear",
          "severity": 1
        }
      },
      "concepts": [
        "AI Literacy for All Citizens",
        "AI Ethics Education",
        "AI Literacy Framework",
        "Project-Based Learning Approach",
        "Teacher-Guided Ethical Reflection",
        "Metacognitive Development in AI Learning",
        "Machine Learning Fundamentals"
      ],
      "knowledge_summary": "Der projektbasierte KI-Literaturkurs führte zu signifikanten Verbesserungen in der Fähigkeit der Schüler:innen, KI-Konzepte zur Problemlösung anzuwenden, sowie in ihren metakognitiven Strategien und ihrem ethischen Bewusstsein bezüglich KI-Einsatz."
    },
    {
      "id": "FJQ6XYHH",
      "stem": "Kong_2025_Artificial_Intelligence_(AI)_literacy_–_an",
      "title": "Artificial Intelligence (AI) literacy – an argument for AI literacy in education",
      "author_year": "Kong (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI-Literalität ist essentiell für alle Lernenden und die zukünftige Arbeitskraft, um AI aktiv und ethisch verantwortungsvoll zur Lösung gesellschaftlicher Herausforderungen einzusetzen. AI kann durch ",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 92
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Titel deutet auf AI Literacy in Bildung hin (TECHNIK-Kategorie erfüllt). Ohne Abstract/Volltext ist keine reliable Bewertung möglich. Kritisch: Kein erkennbarer Bezug zu Sozialer Arbeit, Bias, Gender, Diversität, Fairness oder feministischen Perspektiven. Das Paper adressiert offenbar allgemeine Bil"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Skills-Workforce Gap",
        "Educational Equity",
        "AI-Enhanced Personalized Learning",
        "AI Literacy",
        "Sustainable Development through AI",
        "Ethical AI Awareness",
        "Algorithmic Transparency"
      ],
      "knowledge_summary": "AI-Literalität ist essentiell für alle Lernenden und die zukünftige Arbeitskraft, um AI aktiv und ethisch verantwortungsvoll zur Lösung gesellschaftlicher Herausforderungen einzusetzen. AI kann durch personalisierte Lernpfade Chancengleichheit, Inklusion und lebenslanges Lernen fördern."
    },
    {
      "id": "THGC3PA2",
      "stem": "Kubes_2024_Feministische_KI_–_Künstliche_Intelligenz_für",
      "title": "Feministische KI – Künstliche Intelligenz für alle? [Feminist AI – Artificial intelligence for everyone?]",
      "author_year": "Kubes (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Eine feministische und interdisziplinäre Perspektive ist essentiell, um in KI-Systemen eingeschriebene Bias, Sexismen, Rassismen und Normierungen aufzudecken und damit gerechter, vielfältiger und inkl",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch"
            ],
            "reasoning": "Paper behandelt substantiell AI Literacy durch feministische Perspektive (Kubes, Gender-kritisch). Analysiert KI-Anwendungen im Kontext Bias (androzentrisch, eurozentrisch, kapitalistisch-patriarchal), Geschlechterperspektive und Inklusion. Praktischer Bezug durch Curriculum und Workshops. Beide Bed"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt die Kategorie 'Generative_KI' aus, um allgemeine KI-Bias-Analysen einzubeziehen, obwohl der Paper nicht spezifisch generative Modelle als Forschungsobjekt behandelt, sondern feministische Perspektiven auf KI-Systeme allgemein. Die Human-Entscheidung zu Exclude basiert wahrscheinlich auf der Abwesenheit von generativen Modellen als primärem Fokus.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Feminist AI Literacy",
        "Algorithmic Bias",
        "Intersectional Discrimination in AI",
        "Sociotechnical Perspective",
        "Power Structures in AI Systems",
        "Gender and Algorithmic Decision Systems",
        "AI Fairness and Inclusivity"
      ],
      "knowledge_summary": "Eine feministische und interdisziplinäre Perspektive ist essentiell, um in KI-Systemen eingeschriebene Bias, Sexismen, Rassismen und Normierungen aufzudecken und damit gerechter, vielfältiger und inklusiver gestaltete KI-Systeme zu ermöglichen."
    },
    {
      "id": "BR2LG8LD",
      "stem": "Kumar_2024_How_AI_hype_impacts_the_LGBTQ+_community",
      "title": "How AI hype impacts the LGBTQ+ community",
      "author_year": "Kumar (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI-Hype verschleiert reale Probleme von Bias, Schaden und Ausbeutung, die marginalisierte Communities am stärksten treffen, während er eine Cishet-Baseline und heteronormative Strukturen in AI-Systeme",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper behandelt KI-Sonstige (Computer Vision, Content-Moderation, Algorithmen) und adressiert multiple soziale Dimensionen: Bias_Ungleichheit (algorithmische Marginalisierung queerer Identitäten), Gender (Geschlechtsklassifikation, heteronormative Annahmen), Diversitaet (queere Communities), Feminis"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt die Kategorie 'Generative_KI' zu weit aus, indem es ein Paper über AI-Hype und LGBTQ+ Auswirkungen als technisches Paper klassifiziert, obwohl es primär eine sozialwissenschaftliche Kritik ist. Gleichzeitig erkennt das LLM nicht, dass die explizit feministische Perspektive des Papers (Human-Klassifizierung) es aus dem Corpus ausschließen sollte.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Queer Theoretical Critique of Technology",
        "Cisnormativity and Heteronormativity",
        "AI Hype",
        "Algorithmic Bias",
        "Algorithmic Misgendering",
        "Intersectionality in AI Systems",
        "AI Literacy Gap in Policymaking"
      ],
      "knowledge_summary": "AI-Hype verschleiert reale Probleme von Bias, Schaden und Ausbeutung, die marginalisierte Communities am stärksten treffen, während er eine Cishet-Baseline und heteronormative Strukturen in AI-Systemen verfestigt."
    },
    {
      "id": "HT9ZSI9U",
      "stem": "Kutscher_2020_Handbuch",
      "title": "Handbuch Soziale Arbeit und Digitalisierung",
      "author_year": "Kutscher (2020)",
      "year": 2020,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Digitalisierung bedeutet nicht nur digitale Umwandlung analoger Prozesse, sondern etabliert sociotechnische Arrangements mit fundamentalen Transformationen für Adressat*innen, Professionelle, Organisa",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [
              "Soziale_Arbeit"
            ],
            "reasoning": "Das Handbuch behandelt Digitalisierung in der Sozialen Arbeit substantiell (Soziale_Arbeit = Ja). Allerdings fehlt der erforderliche TECHNIK-Aspekt vollständig: Es gibt keinen Bezug zu KI-Systemen, Machine Learning, generativer KI oder KI-Kompetenzen. Das Werk konzentriert sich auf breite Digitalisi"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Sociotechnical Arrangements",
        "Professional Digital Literacy in Social Work",
        "Deprofessionalization through Automation",
        "Digital Infrastructure Justice",
        "Digital Divide in Vulnerable Populations",
        "Algorithmic Decision-Making in Social Services",
        "Datafication and Autonomy"
      ],
      "knowledge_summary": "Digitalisierung bedeutet nicht nur digitale Umwandlung analoger Prozesse, sondern etabliert sociotechnische Arrangements mit fundamentalen Transformationen für Adressat*innen, Professionelle, Organisationen und wohlfahrtsstaatliche Strukturen. Eine differenzierte Analyse muss die wechselseitige Verflechtung digitaler Logiken mit fachlichen Logiken der Sozialen Arbeit in den Blick nehmen."
    },
    {
      "id": "HT9ZSI9U",
      "stem": "Kutscher_2020_Handbuch_Soziale_Arbeit_und_Digitalisierung",
      "title": "Handbuch Soziale Arbeit und Digitalisierung",
      "author_year": "Kutscher (2020)",
      "year": 2020,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Ein sechsdimensionales Modell der Medienpädagogischen Kompetenz für Sozialarbeiter*innen wird entwickelt, das Orientierungswissen, handlungsfeldspezifisches Wissen, eigene Medienkompetenz, analytische",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [
              "Soziale_Arbeit"
            ],
            "reasoning": "Das Handbuch behandelt Digitalisierung in der Sozialen Arbeit substantiell (Soziale_Arbeit = Ja). Allerdings fehlt der erforderliche TECHNIK-Aspekt vollständig: Es gibt keinen Bezug zu KI-Systemen, Machine Learning, generativer KI oder KI-Kompetenzen. Das Werk konzentriert sich auf breite Digitalisi"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Data Protection and Informational Self-Determination",
        "Media Pedagogical Competence",
        "Assistive and Social Robotics in Care",
        "Digital Divide",
        "Digital Inclusion and Empowerment",
        "Mediatization in Social Work",
        "Algorithmic Governance in Social Services"
      ],
      "knowledge_summary": "Ein sechsdimensionales Modell der Medienpädagogischen Kompetenz für Sozialarbeiter*innen wird entwickelt, das Orientierungswissen, handlungsfeldspezifisches Wissen, eigene Medienkompetenz, analytisches Handlungswissen, Gestaltungskompetenzen und organisationales Handlungswissen umfasst."
    },
    {
      "id": "5F7D9PEB",
      "stem": "Kutscher_2023_Positionings,_challenges,_and_ambivalences_in",
      "title": "Policy advice and best practices on bias and fairness in AI",
      "author_year": "Alvarez (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 5,
          "stage1_key_finding": "Der Hauptbeitrag besteht in einer umfassenden Übersicht des Fair-AI-Forschungsstands sowie in Policy-Empfehlungen und Best Practices aus dem NoBIAS-Projekt, die zentrale unterentwickelte Themen wie Mu",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Das Paper behandelt algorithmische Fairness und Bias-Mitigation mit Policy-Fokus (KI_Sonstige: Ja, Fairness: Ja). Es erfüllt die TECHNIK-Bedingung. Allerdings fehlt der explizite Bezug zu Sozialer Arbeit oder marginalisierten Gruppen im Kontext sozialer Dienste. Intersektionale Diskriminierung wird "
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Causal Fairness",
        "Algorithmic Fairness",
        "Intersectionality in AI",
        "Human-Centered AI",
        "Multi-Stakeholder Participation",
        "Value-Laden AI Systems",
        "Distribution Shift and Monitoring",
        "Representation Bias"
      ],
      "knowledge_summary": "Der Hauptbeitrag besteht in einer umfassenden Übersicht des Fair-AI-Forschungsstands sowie in Policy-Empfehlungen und Best Practices aus dem NoBIAS-Projekt, die zentrale unterentwickelte Themen wie Multi-Stakeholder-Partizipation, Intersektionalität, Kausalität und Monitoring adressieren. Fair-AI erfordert einen multidisziplinären Ansatz jenseits reiner technischer Optimierung und muss mit rechtli"
    },
    {
      "id": "BKSU66QB",
      "stem": "Kutscher_2024_Digitalität_und_Digitalisierung_als_Gegenstand",
      "title": "Digitalität und Digitalisierung als Gegenstand der Sozialen Arbeit",
      "author_year": "Kutscher (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Digitalisierung in der Sozialen Arbeit erfordert eine kritische Reflexion von Algorithmen, ethischen Fragen und strukturellen Ungleichheiten, insbesondere bezüglich algorithmischer Entscheidungssystem",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Paper erfüllt beide Inklusionskriterien: (1) TECHNIK-Kriterium: KI_Sonstige=Ja (Algorithmen, datengetriebene Systeme, digitale Plattformen sind zentral). (2) SOZIAL-Kriterium: Soziale_Arbeit=Ja (direkter Fokus auf sozialarbeiterische Praxis und deren Transformation) UND Bias_Ungleichheit=Ja (Analyse"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "Soziale_Arbeit"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'Bias/Ungleichheit', 'Diversität' und 'Fairness' auf allgemeine sozialpolitische Diskurse aus, ohne dass diese spezifisch im Kontext algorithmischer Systeme behandelt werden. Der Human erkennt korrekt, dass die Thematisierung von Digitalisierung in der Sozialen Arbeit nicht ausreicht für die KI-spezifischen Kategorien.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Media Literacy in Professional Social Work",
        "Digital Inequality",
        "Mediatization of Social Work Practice",
        "Digitalization of Youth Services",
        "Algorithmic Transparency and Data Protection",
        "Algorithmic Fairness in Social Work",
        "Algorithmic Discrimination"
      ],
      "knowledge_summary": "Digitalisierung in der Sozialen Arbeit erfordert eine kritische Reflexion von Algorithmen, ethischen Fragen und strukturellen Ungleichheiten, insbesondere bezüglich algorithmischer Entscheidungssysteme und deren Auswirkungen auf vulnerable Gruppen."
    },
    {
      "id": "DUV4TUG3",
      "stem": "Lahoti_2023_Improving_diversity_of_demographic_representation",
      "title": "Improving diversity of demographic representation in people entities in Large Language Models",
      "author_year": "Lahoti (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "LLMs können Diversitätsmängel in ihren Antworten selbst erkennen und kritisieren. Die CCSV-Methode (Collective-Critique and Self-Voting) verbessert Menschen- und Kulturdiversität dramatisch, wobei Zer",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper adressiert Generative KI (LLMs), Prompting (CCSV-Methode), sowie Bias/Ungleichheit durch demografische Unterrepräsentation, Diversität und Fairness. TECHNIK_OK (Generative_KI + Prompting), SOZIAL_OK (Bias_Ungleichheit + Diversitaet + Fairness). Erfüllt Einschlusskriterien. Kein expliziter Gend"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "KI_Sonstige",
              "Diversitaet"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Fairness in Language Generation",
        "Collective-Critique and Self-Voting (CCSV)",
        "Zero-Shot Prompting for Diversity",
        "Diversity Metrics (Entropy and Max-Gap)",
        "Self-Improvement Capabilities in LLMs",
        "Demographic Homogenization Problem",
        "Demographic Representation Diversity in LLMs"
      ],
      "knowledge_summary": "LLMs können Diversitätsmängel in ihren Antworten selbst erkennen und kritisieren. Die CCSV-Methode (Collective-Critique and Self-Voting) verbessert Menschen- und Kulturdiversität dramatisch, wobei Zero-Shot-Prompting überraschenderweise besser und robuster ist als Few-Shot-Ansätze."
    },
    {
      "id": "2SLISKSW",
      "stem": "Laine_2025_Avoiding_Catastrophe_Through_Intersectionality_in",
      "title": "Avoiding catastrophe through intersectionality in global AI governance",
      "author_year": "McCrory (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Keine der sieben analysierten globalen AI-Sicherheitsinitiativen erfüllt die Ziele eines intersektionalen feministischen Ansatzes; sie vernachlässigen gegenwärtige Schäden marginalisierter Gruppen und",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 93
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmische Governance und AI-Systeme sind substantiell adressiert) und SOZIAL_OK (Bias_Ungleichheit, Gender, Diversität und explizit Feministisch durch Intersektionalitätsanalyse nach Crenshaw-Perspektive, Fairness durch Kritik unglei"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Feminist Policy Analysis Framework",
        "Community Participation in AI Governance",
        "Epistemic Justice in AI Development",
        "Intersectional Feminism in AI Governance",
        "Power Dynamics in AI Governance",
        "Present Harms vs. Existential Risks Dichotomy",
        "Technological Redlining",
        "Algorithmic Discrimination"
      ],
      "knowledge_summary": "Keine der sieben analysierten globalen AI-Sicherheitsinitiativen erfüllt die Ziele eines intersektionalen feministischen Ansatzes; sie vernachlässigen gegenwärtige Schäden marginalisierter Gruppen und mangelt es an aussagekräftiger Partizipation betroffener Gemeinschaften."
    },
    {
      "id": "AXEIVEW3",
      "stem": "Lanzetta_2024_Artificial_Intelligence_Competence_Needs_for",
      "title": "Artificial Intelligence Competence Needs for Youth Workers",
      "author_year": "Lanzetta (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Jugendarbeiter:innen benötigen ein Spektrum technischer, analytischer und sozialer Kompetenzen (AI-Literalität, kritisches Denken, Datenkompetenz, ethische Verantwortung), wobei erhebliche Kompetenzlü",
          "stage3_completeness": 88,
          "stage3_correctness": 98,
          "stage3_overall": 92
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Soziale_Arbeit"
            ],
            "reasoning": "Paper adressiert zentral AI-Kompetenzen von Fachkräften (AI_Literacies: Ja - Kompetenzrahmen für Jugendhilfe), behandelt Generative AI (Ja - explizit Gen Z und generative Tools), hat direkten Bezug zu Sozialarbeit/Jugendhilfe (Ja - Youth Workers, Jugendhilfesektor). Empirische Studie mit Fokusgruppe"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "KI_Sonstige",
              "Soziale_Arbeit"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Data Literacy",
        "Algorithmic Fairness",
        "Ethical AI Integration",
        "AI Literacy",
        "Critical AI Thinking",
        "Algorithmic Bias",
        "Prompt Engineering",
        "Digital Inclusion"
      ],
      "knowledge_summary": "Jugendarbeiter:innen benötigen ein Spektrum technischer, analytischer und sozialer Kompetenzen (AI-Literalität, kritisches Denken, Datenkompetenz, ethische Verantwortung), wobei erhebliche Kompetenzlücken und Herausforderungen bei der Integration von KI-Tools bestehen."
    },
    {
      "id": "CHJQ52DC",
      "stem": "Latif_2023_AI_Gender_Bias,_Disparities,_and_Fairness_Does",
      "title": "AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?",
      "author_year": "Latif (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Mixed-trained Modelle zeigen keinen signifikanten geschlechtsspezifischen Bias und erzeugen geringere geschlechtsspezifische Disparitäten im Vergleich zu humans, während gender-spezifisch trainierte M",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt TECHNIK-Bedingung (Generative_KI, KI_Sonstige) und SOZIAL-Bedingung (Bias_Ungleichheit, Gender, Fairness). ABER: Kein Bezug zu Sozialer Arbeit oder deren Zielgruppen/Praxis. Rein technische Studie zu LLM-Bias ohne Anwendungskontext in Sozialbereich. Für Literature Review Soziale Arbeit"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Gender"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'AI Literacies', 'Fairness' und 'KI_Sonstige' zu breit aus und erfasst damit HCI-Forschung ohne sozialarb. Kontext, während es gleichzeitig die substantiellen Dimensionen 'Gender' und 'Bias_Ungleichheit' ignoriert, die der Mensch korrekt als zentral erkannt hat.",
          "disagreement_type": "Human_Include_Agent_Exclude",
          "severity": 3
        }
      },
      "concepts": [
        "Pseudo-AI Bias",
        "Mean Score Gap (MSG)",
        "Equalized Odds (EO) Metric",
        "Gender Bias in AI Systems",
        "Algorithmic Fairness in Automated Assessment",
        "Diversity in Machine Learning",
        "Natural Language Processing for Educational Assessment",
        "Training Data Composition"
      ],
      "knowledge_summary": "Mixed-trained Modelle zeigen keinen signifikanten geschlechtsspezifischen Bias und erzeugen geringere geschlechtsspezifische Disparitäten im Vergleich zu humans, während gender-spezifisch trainierte Modelle größere MSG aufweisen und Disparitäten vergrößern können."
    },
    {
      "id": "LBLF9BCW",
      "stem": "Lau_2023_Dipper_Diversity_in_Prompts_for_Producing_Large",
      "title": "Dipper: Diversity in Prompts for Producing Large Language Model Outputs",
      "author_year": "Lau (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": false,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": false
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Ein Ensemble aus drei kleinen Modellen (Qwen2-MATH-1.5B) mit optimierten, diversen Prompts kann die Leistung eines größeren Modells (Qwen2-MATH-7B) erreichen oder übertreffen, mit etwa 10%-Punkt Accur",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Das Paper behandelt Prompting (Dipper-Framework mit diversen Prompts) und Generative KI (LLMs) substantiell. Es adressiert Diversity (marginalisierte Gruppen, Perspektivenvielfalt), Bias_Ungleichheit (Verbesserung von Darstellung minorities) und Fairness (faire Output-Generierung). TECHNIK und SOZIA"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Diversitaet"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Chain-of-Thought Reasoning",
        "LLM-based Response Aggregation",
        "Inference-Time Optimization",
        "Large Language Model Ensembles",
        "Submodular Function Maximization",
        "Prompt Diversity Optimization",
        "Mathematical Reasoning Benchmarks"
      ],
      "knowledge_summary": "Ein Ensemble aus drei kleinen Modellen (Qwen2-MATH-1.5B) mit optimierten, diversen Prompts kann die Leistung eines größeren Modells (Qwen2-MATH-7B) erreichen oder übertreffen, mit etwa 10%-Punkt Accuracy-Verbesserung gegenüber dem einzelnen Modell."
    },
    {
      "id": "4BRSDIPP",
      "stem": "Laupichler_2023_Development_of_the_“Scale_for_the_assessment_of",
      "title": "Development of the “Scale for the assessment of non-experts’ AI literacy” – An exploratory factor analysis",
      "author_year": "Laupichler (12/2023)",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": false
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Ein Drei-Faktoren-Modell (TUCAPA: Technical Understanding, Critical Appraisal, Practical Application) erklärt die AI-Literacy-Struktur bei Laien am besten. Die finale SNAIL-Skala mit 31 Items zeigt ex",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Der Titel deutet auf ein Instrument zur Messung von AI Literacy hin – ein TECHNIK-Thema. Jedoch fehlt jede Evidenz für soziale Dimensionen wie Anwendung in der Sozialen Arbeit oder Fokus auf Bias/Fairness. Das Paper erfüllt nur eine Bedingung der strikten Entscheidungslogik (TECHNIK_OK, aber nicht S"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "AI Literacy",
        "Machine Learning Literacy",
        "Explainable AI",
        "Psychometric Scale Development",
        "Algorithmic Bias",
        "Gender-Stratified Sampling",
        "Exploratory Factor Analysis"
      ],
      "knowledge_summary": "Ein Drei-Faktoren-Modell (TUCAPA: Technical Understanding, Critical Appraisal, Practical Application) erklärt die AI-Literacy-Struktur bei Laien am besten. Die finale SNAIL-Skala mit 31 Items zeigt exzellente interne Konsistenz (α=0.93-0.94)."
    },
    {
      "id": "Y8J3HI9J",
      "stem": "Lin_2022_Artificial_Intelligence_in_a_Structurally_Unjust",
      "title": "Artificial Intelligence in a Structurally Unjust Society",
      "author_year": "Lin (2022)",
      "year": 2022,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "KI-Bias sollte als Form struktureller Ungerechtigkeit verstanden werden, die auftritt, wenn KI-Systeme mit anderen sozialen Faktoren interagieren und bestehende Ungleichheiten verschärfen. Dies erford",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja, algorithmische Systeme im Gesundheitskontext), SOZIAL_OK (Bias_Ungleichheit=Ja zu struktureller Ungerechtigkeit, Fairness=Ja zu AI Fairness). Kein direkter Soziale_Arbeit-Bezug, daher keine entsprechende Kategorie."
          },
          "human": {
            "decision": "Unclear",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'Prompting' und 'Generative_KI' auf allgemeine KI-Literacy-Diskussionen aus, obwohl der Paper primär strukturelle Ungerechtigkeit thematisiert. Die Human-Kodierung erkannte korrekt, dass die KI-Dimensionen nur peripher sind, während Bias/Fairness-Aspekte zentral sind.",
          "disagreement_type": "Human_Unclear",
          "severity": 1
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Healthcare AI Equity",
        "Social Connection Model",
        "Collective Responsibility in AI Systems",
        "Algorithmic Bias",
        "Gender Bias in AI",
        "Structural Injustice",
        "Racial Bias in Algorithmic Decision Systems"
      ],
      "knowledge_summary": "KI-Bias sollte als Form struktureller Ungerechtigkeit verstanden werden, die auftritt, wenn KI-Systeme mit anderen sozialen Faktoren interagieren und bestehende Ungleichheiten verschärfen. Dies erfordert kollektive Verantwortung aller Beteiligten, nicht nur von Technik-Entwickler:innen."
    },
    {
      "id": "JNLPSHD5",
      "stem": "Linnemann_2023_Bedeutung_von_Künstlicher_Intelligenz_in_der",
      "title": "Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit: Eine exemplarische arbeitsfeldübergreifende Betrachtung des Natural Language Processing (NLP)",
      "author_year": "Linnemann (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "NLP bietet sowohl Chancen (niederschwelliger Zugang, Teilhabe, erweiterte Wissensbasis) als auch erhebliche Risiken (Modularisierung der Profession, Reproduktion von Stereotypen und Diskriminierung, e",
          "stage3_completeness": 92,
          "stage3_correctness": 96,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit"
            ],
            "reasoning": "Paper behandelt NLP als KI-Technologie (KI_Sonstige: Ja) und adressiert direkt sozialarbeiterische Praxis, Theorie und professionsethische Fragen (Soziale_Arbeit: Ja). Kritische Auseinandersetzung mit KI-Chancen/-Risiken impliziert auch AI Literacy-Anforderungen (AI_Literacies: Ja). TECHNIK-Kriteriu"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "Generative_KI",
              "KI_Sonstige",
              "Soziale_Arbeit"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'Prompting' und 'Fairness' als relevante Kategorien aus, obwohl diese im Paper nur als technische Randthemen behandelt werden, und übersieht, dass der fehlende explizite Sozialarbeitsbezug ein zentrales Ausschlusskriterium darstellt.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "AI Ethics in Human Rights-Based Practice",
        "Algorithmic Transparency and Interpretability",
        "Algorithmic Bias in Social Services",
        "Interdisciplinary AI Development",
        "Digital Divide in Social Services",
        "Natural Language Processing in Social Work",
        "Professional Deskilling and Modularization"
      ],
      "knowledge_summary": "NLP bietet sowohl Chancen (niederschwelliger Zugang, Teilhabe, erweiterte Wissensbasis) als auch erhebliche Risiken (Modularisierung der Profession, Reproduktion von Stereotypen und Diskriminierung, ethische Probleme) für die Soziale Arbeit; ein kritisch begleiteter Einsatz unter Wahrung von Menschenrechten und Transparenz ist erforderlich."
    },
    {
      "id": "L6PH7GDL",
      "stem": "Linnemann_2025_Künstliche_Intelligenz_in_der_Sozialen_Arbeit",
      "title": "Künstliche Intelligenz in der Sozialen Arbeit: Grundlagen für Theorie und Praxis",
      "author_year": "Linnemann (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 5,
          "stage1_key_finding": "KI birgt sowohl erhebliche Chancen (Inklusion, Assistenz, Dokumentation) als auch Risiken (Bias, Automatisierungsfehler, Marginalisierung) für die Soziale Arbeit; ein reflexierter, an Fachlichkeit und",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (KI-Kompetenzen in Bildung/Organisationen), KI_Sonstige (technische Grundlagen, Automatisierungsbias). SOZIAL: Soziale_Arbeit (Direktbezug zu Praxis, Theorie und Anwendungsfeldern), Bias_Ungleichheit (Diskriminierung in Trainingsdaten), Fairnes"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Automation Bias",
        "Algorithmic Bias in Social Services",
        "Inclusive AI Design",
        "Professional Ethics in AI Deployment",
        "Gender Bias in AI Development",
        "Predictive Analytics in Child Welfare",
        "AI Literacy in Social Work"
      ],
      "knowledge_summary": "KI birgt sowohl erhebliche Chancen (Inklusion, Assistenz, Dokumentation) als auch Risiken (Bias, Automatisierungsfehler, Marginalisierung) für die Soziale Arbeit; ein reflexierter, an Fachlichkeit und Ethik orientierter Umgang erfordert systematische KI-Kompetenzentwicklung in Studium, Praxis und Organisationen sowie kritische Inklusionsperspektiven."
    },
    {
      "id": "UHEM78MX",
      "stem": "Long_2020_What_is_AI_Literacy_Competencies_and_Design",
      "title": "What is AI Literacy? Competencies and Design Considerations",
      "author_year": "Long (2020)",
      "year": 2020,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Definition von AI Literacy als Satz von Kompetenzen, die Individuen befähigen, KI-Technologien kritisch zu bewerten, effektiv mit KI zu kommunizieren und zu kollaborieren, sowie KI als Werkzeug online",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.7,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Das Paper behandelt AI Literacy und Kompetenzenrahmen (erfüllt TECHNIK-Kriterium). Allerdings erfüllt es KEINE der SOZIAL-Kategorien: Es gibt keinen Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder Fairness. Es ist ein rein bildungstechnologisches Konzept-Paper ohne sozialarbeite"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Computational Literacy",
        "AI Literacy",
        "Explainable AI",
        "Algorithmic Bias",
        "Digital Divide",
        "Workforce Diversity in AI",
        "Inclusive AI Education Design"
      ],
      "knowledge_summary": "Definition von AI Literacy als Satz von Kompetenzen, die Individuen befähigen, KI-Technologien kritisch zu bewerten, effektiv mit KI zu kommunizieren und zu kollaborieren, sowie KI als Werkzeug online, zu Hause und am Arbeitsplatz zu nutzen. Ein konzeptuelles Framework mit 16 Kern-Kompetenzen und 12 Design-Überlegungen wird präsentiert."
    },
    {
      "id": "QZJL6KBZ",
      "stem": "Lund_2025_Algorithms,_artificial_intelligence_and",
      "title": "Algorithms, artificial intelligence and discrimination",
      "author_year": "Lund (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Das EAD Act ist zwar auf algorithmische Diskriminierung anwendbar, weist aber erhebliche Lücken auf. Eine spezifische Gesetzesbestimmung zu algorithmischer Differenzialbehandlung ist erforderlich, um ",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 93
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Der Bericht adressiert algorithmische Diskriminierung (KI_Sonstige: Ja) und behandelt substantiell Bias/Ungleichheit sowie Fairness im Kontext von Antidiskriminierungsrecht. Er erfüllt beide Bedingungen (TECHNIK + SOZIAL) und ist damit included, obwohl er keinen direkten Soziale-Arbeit-Bezug hat."
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Fairness"
            ]
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "AI Governance and Legal Regulation",
        "Algorithmic Transparency and Explainability",
        "Proxy Discrimination",
        "Intersectional Discrimination",
        "Burden of Proof in Algorithmic Contexts",
        "Algorithmic Discrimination"
      ],
      "knowledge_summary": "Das EAD Act ist zwar auf algorithmische Diskriminierung anwendbar, weist aber erhebliche Lücken auf. Eine spezifische Gesetzesbestimmung zu algorithmischer Differenzialbehandlung ist erforderlich, um präzisere Schutzstandards etablieren zu können und rechtliche Sicherheit für alle beteiligten Akteure zu schaffen."
    },
    {
      "id": "UBYTNGNV",
      "stem": "Lütz_2024_The_AI_Act,_gender_equality_and",
      "title": "The AI Act, gender equality and non-discrimination: what role for the AI Office?",
      "author_year": "Giannoni Adielsson (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 4,
          "stage1_key_finding": "Der AI Act bietet einige Mechanismen zur Adressierung von Geschlechtergleichstellung und Diskriminierung durch Fundamental Rights Impact Assessments und Bias Audits, bleibt aber durch Ausnahmen und be",
          "stage3_completeness": 92,
          "stage3_correctness": 97,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Fairness"
            ],
            "reasoning": "Paper analysiert EU AI Act durch die Linse von Geschlechtergerechtigkeit und Nichtdiskriminierung. TECHNIK erfüllt: KI_Sonstige (algorithmische Systeme reguliert durch AI Act). SOZIAL erfüllt: Bias_Ungleichheit (Geschlechterverzerrungen, Diskriminierungsrisiken), Gender (expliziter Gender-Fokus), Fa"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "High-Risk AI Classification",
        "Algorithmic Gender Bias",
        "AI Governance Institutional Framework",
        "Fundamental Rights Impact Assessment",
        "Developer Diversity and Inclusive Design",
        "Algorithmic Fairness Metrics",
        "Bias Auditing and Compliance"
      ],
      "knowledge_summary": "Der AI Act bietet einige Mechanismen zur Adressierung von Geschlechtergleichstellung und Diskriminierung durch Fundamental Rights Impact Assessments und Bias Audits, bleibt aber durch Ausnahmen und begrenzte Reichweite insuffizient. Eine umfassendere Zusammenarbeit zwischen AI Office, nationalen Gleichstellungsbehörden und internationalen Institutionen ist notwendig."
    },
    {
      "id": "HMDFMBV3",
      "stem": "Ma_2023_Intersectional_Stereotypes_in_Large_Language",
      "title": "Intersectional Stereotypes in Large Language Models: Dataset and Analysis",
      "author_year": "Ma (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Trotz Debiasing-Maßnahmen zeigen moderne LLMs (GPT-3, ChatGPT) komplexe intersektionale Stereotypen gegenüber 106 verschiedenen Gruppen, mit unterschiedlichen Stereotypisierungsmustern pro Modell, was",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper behandelt substantiell Generative KI (LLMs) und Prompting (Prompt Engineering zur Bias-Reduktion). SOZIAL-Kriterien erfüllt: (1) Bias_Ungleichheit (intersektionale Stereotypes analyse), (2) Gender (Geschlechter-Stereotypen in LLMs), (3) Diversitaet (multiple Attribute/Gruppen), (4) Feministisc"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Fairness Evaluation",
        "Demographic Category Intersectionality",
        "Annotation Consensus Validation",
        "Model-Specific Debiasing Strategies",
        "Stereotype Degree Measurement",
        "Prompt Engineering for Data Generation",
        "Intersectional Bias in Large Language Models"
      ],
      "knowledge_summary": "Trotz Debiasing-Maßnahmen zeigen moderne LLMs (GPT-3, ChatGPT) komplexe intersektionale Stereotypen gegenüber 106 verschiedenen Gruppen, mit unterschiedlichen Stereotypisierungsmustern pro Modell, was spezifische Mitigationsmaßnahmen erfordert."
    },
    {
      "id": "Maeda_2025_Toward_Agency‐Centered_span",
      "stem": "Maeda_2025_Toward_Agency‐Centered_span",
      "title": "Maeda_2025_Toward_Agency‐Centered_span",
      "author_year": "Maeda",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": false
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI-Literacy-Definitionen sind inkonsistent und fokussieren zu eng auf Wissenserwerb und Kompetenzerwerb, während soziale Faktoren und Fragen von Agency und kritischem Engagement weitgehend übersehen w",
          "stage3_completeness": 92,
          "stage3_correctness": 96,
          "stage3_overall": 94
        },
        "assessment": {}
      },
      "concepts": [
        "Social Inequality and Marginalization in Technology",
        "Intersectional Approach to AI Literacy",
        "Algorithmic Knowledge Gaps",
        "Digital Equity in AI Systems",
        "Critical Technology Engagement",
        "Algorithmic Fairness Across Demographic Groups",
        "Agency-Centered AI Literacy",
        "Algorithmic Opacity and Transparency"
      ],
      "knowledge_summary": "AI-Literacy-Definitionen sind inkonsistent und fokussieren zu eng auf Wissenserwerb und Kompetenzerwerb, während soziale Faktoren und Fragen von Agency und kritischem Engagement weitgehend übersehen werden. Ein agency-zentrierter Ansatz ist notwendig, der informierte Entscheidungsfindung, kritisches Engagement und das Recht auf Ablehnung von KI-Systemen einbezieht."
    },
    {
      "id": "2SLISKSW",
      "stem": "McCrory_2024_Avoiding_catastrophe_through_intersectionality_in",
      "title": "Avoiding catastrophe through intersectionality in global AI governance",
      "author_year": "McCrory (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Keine der sieben analysierten KI-Sicherheits-Initiativen erfüllt die intersektionalen und feministischen Standards des Frameworks; die KI-Sicherheitsbewegung versäumt es, aktuelle Schäden für marginal",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmische Governance und AI-Systeme sind substantiell adressiert) und SOZIAL_OK (Bias_Ungleichheit, Gender, Diversität und explizit Feministisch durch Intersektionalitätsanalyse nach Crenshaw-Perspektive, Fairness durch Kritik unglei"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Substantive Equality vs. Formal Equality",
        "Feminist Policy Analysis Framework",
        "Epistemic Justice in AI Policy Development",
        "Intersectional AI Governance",
        "AI Safety Governance Evaluation",
        "Algorithmic Harm to Marginalized Communities",
        "Algorithmic Bias in Gender Classification"
      ],
      "knowledge_summary": "Keine der sieben analysierten KI-Sicherheits-Initiativen erfüllt die intersektionalen und feministischen Standards des Frameworks; die KI-Sicherheitsbewegung versäumt es, aktuelle Schäden für marginalisierte Gruppen zu adressieren und diese mit zukünftigen existenziellen Risiken zu verbinden."
    },
    {
      "id": "7IVS7X63",
      "stem": "Mei_2023_Assessing_GPT's_bias_towards_stigmatized_social",
      "title": "Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia",
      "author_year": "Jiang (2022)",
      "year": 2022,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "GPT-Modelle zeigen systematisch stärkere negative Vorurteile gegenüber Nordkoreaner:innen im Vergleich zu Amerikaner:innen, insbesondere bei Kombination mit psychischen Erkrankungen. Die Modelle zeige",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Das Paper untersucht systematisch Bias in GPT gegenüber stigmatisierten Gruppen (Generative_KI: Ja). Der Fokus auf Nationalität und psychische Erkrankungen adressiert Bias und Ungleichheit (Bias_Ungleichheit: Ja). Der explizit erwähnte intersektionale Ansatz deutet auf feministische Methodik hin (Fe"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Prompt Engineering for Bias Assessment",
        "Stigmatization in Large Language Models",
        "Nationality Prejudice in Language Models",
        "Psychophobia in AI Systems",
        "Cultural Essentialism in AI Outputs",
        "Algorithmic Fairness Measurement Reliability",
        "Intersectional Algorithmic Bias"
      ],
      "knowledge_summary": "GPT-Modelle zeigen systematisch stärkere negative Vorurteile gegenüber Nordkoreaner:innen im Vergleich zu Amerikaner:innen, insbesondere bei Kombination mit psychischen Erkrankungen. Die Modelle zeigen zudem Inkonsistenzen bei invertierten Skalen und treffen implizite kulturelle Annahmen."
    },
    {
      "id": "BTLTEA6Y",
      "stem": "Meilvang_2024_Decision_support_and_algorithmic_support_The",
      "title": "Decision support and algorithmic support: The construction of algorithms and professional discretion in social work",
      "author_year": "Meilvang (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Algorithms in Danish social work create an ambivalent and undecided relationship with professional discretion: they are designed to constrain subjective judgment through standardization and objectific",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK-Ja (KI_Sonstige: algorithmische Entscheidungssysteme in der Jugendhilfe), SOZIAL-Ja (Soziale_Arbeit: direkter Fokus auf Auswirkungen von Algorithmen auf sozialarbeiterische Praxis und Professionalität; Bias_Ungleichheit und Fairness: kritische Analyse von Mac"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit"
            ]
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Technological Determinism in Welfare Governance",
        "Algorithmic Bias in Social Work",
        "Algorithmic Decision Support",
        "Algorithmic Transparency in Public Administration",
        "Professional Discretion",
        "Datification of Social Work"
      ],
      "knowledge_summary": "Algorithms in Danish social work create an ambivalent and undecided relationship with professional discretion: they are designed to constrain subjective judgment through standardization and objectification, yet fundamentally depend on professional discretion for contextual information, data provision, and bias assessment."
    },
    {
      "id": "PI5H2LZ2",
      "stem": "Moreau_2024_Failing_our_youngest_On_the_biases,_pitfalls,_and",
      "title": "Failing our youngest: On the biases, pitfalls, and risks in a decision support algorithm used for child protection",
      "author_year": "Moreau (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Der DSS-Algorithmus weist erhebliche Mängel auf, darunter Informationslecks zwischen Test- und Trainings-Sets, unangemessene Proxies für Kindesmisshandlung, inkonsistente Risikoscores und signifikante",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Einschlusskriterien: (1) TECHNIK: KI_Sonstige=Ja (algorithmisches Entscheidungssystem in der Jugendhilfe). (2) SOZIAL: Soziale_Arbeit=Ja (direkter Bezug zu Child Protection/Jugendhilfe), Bias_Ungleichheit=Ja (Analyse von Diskriminierung gegenüber Immigrantenfamilien), Diversitaet"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Gender",
              "Fairness"
            ]
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "Algorithmic Accountability in Social Work Practice",
        "Algorithmic Fairness in Child Protection",
        "Information Leakage in Machine Learning",
        "Protected Attributes and Discrimination",
        "Proxy Variables for Social Harm",
        "Algorithmic Audit and Reverse Engineering",
        "Self-Validating Prediction Systems",
        "AI Literacy in Public Sector Implementation"
      ],
      "knowledge_summary": "Der DSS-Algorithmus weist erhebliche Mängel auf, darunter Informationslecks zwischen Test- und Trainings-Sets, unangemessene Proxies für Kindesmisshandlung, inkonsistente Risikoscores und signifikante altersbasierte Diskriminierung, die Kinder systematisch unterschiedlich bewertet."
    },
    {
      "id": "NUVZI357",
      "stem": "Mosene_2023_Feministische",
      "title": "Feministische Netzpolitik und Künstliche Intelligenz in der politischen Bildung [Feminist network politics and artificial intelligence in political education]",
      "author_year": "Mosene (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": false
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Die Cyborg als imaginäre und materielle Figur ermöglicht einen Weg aus binären Dualismen (Natur/Kultur, Mensch/Maschine, Mann/Frau) und erlaubt eine neue Form feministischer Politik, die Partialität, ",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige) + SOZIAL_OK (Bias_Ungleichheit, Gender, Feministisch, Fairness). Behandelt substantiell feministische Perspektive auf KI (intersektional), Gender-Bias in KI-Systemen, AI-Literacy in politischer Bildung. Kein Sozialarb.-Bezug, d"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt AI_Literacies und Soziale_Arbeit über ihre Kerndefnitionen hinaus aus, indem es feministische Netzpolitik als KI-Literacies-Rahmenwerk und politische Bildung als sozialarbeiterische Praxis interpretiert. Der Mensch erkennt korrekt, dass der Schwerpunkt auf Gender/Feminismus liegt, nicht auf KI-Integration in der Praxis.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Socialist Feminism",
        "Situated Knowledge",
        "Informatics of Domination",
        "Nature-Culture Boundary Deconstruction",
        "Cyborg Politics",
        "Feminist Epistemology",
        "Women of Color Intersectionality"
      ],
      "knowledge_summary": "Die Cyborg als imaginäre und materielle Figur ermöglicht einen Weg aus binären Dualismen (Natur/Kultur, Mensch/Maschine, Mann/Frau) und erlaubt eine neue Form feministischer Politik, die Partialität, Ironie und strategische Verbindungen ohne essentiale Identität nutzt."
    },
    {
      "id": "NUVZI357",
      "stem": "Mosene_2023_Feministische_Netzpolitik_und_Künstliche",
      "title": "Feministische Netzpolitik und Künstliche Intelligenz in der politischen Bildung [Feminist network politics and artificial intelligence in political education]",
      "author_year": "Mosene (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Diskriminierende Stereotype und koloniale Strukturen sind bereits in der technischen Infrastruktur und den Trainingsdaten von KI-Systemen eingeschrieben; feministische Netzpolitik und kritische politi",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige) + SOZIAL_OK (Bias_Ungleichheit, Gender, Feministisch, Fairness). Behandelt substantiell feministische Perspektive auf KI (intersektional), Gender-Bias in KI-Systemen, AI-Literacy in politischer Bildung. Kein Sozialarb.-Bezug, d"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt AI_Literacies und Soziale_Arbeit über ihre Kerndefnitionen hinaus aus, indem es feministische Netzpolitik als KI-Literacies-Rahmenwerk und politische Bildung als sozialarbeiterische Praxis interpretiert. Der Mensch erkennt korrekt, dass der Schwerpunkt auf Gender/Feminismus liegt, nicht auf KI-Integration in der Praxis.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Feminist Data Set",
        "Intersectionality",
        "Algorithmic Bias",
        "Training Data Bias",
        "Digital Colonialism",
        "Gender Bias in AI Design",
        "Feminist Net Politics",
        "Critical AI Literacy"
      ],
      "knowledge_summary": "Diskriminierende Stereotype und koloniale Strukturen sind bereits in der technischen Infrastruktur und den Trainingsdaten von KI-Systemen eingeschrieben; feministische Netzpolitik und kritische politische Bildung sind notwendig, um diese Mechanismen aufzudecken und emanzipatorische Alternativen zu schaffen."
    },
    {
      "id": "7L78MV2V",
      "stem": "Navigli_2023_Biases_in_large_language_models_Origins,",
      "title": "Biases in large language models: Origins, inventory and discussion",
      "author_year": "Navigli (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Die meisten Verzerrungen in Sprachmodellen entstehen durch Data Selection Bias bei der Wahl der Trainingstexte; das Paper inventarisiert elf Typen sozialer Bias (Gender, Alter, sexuelle Orientierung, ",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt TECHNIK-Kriterium (Generative_KI: LLMs) und SOZIAL-Kriterium (Bias_Ungleichheit, Gender, Diversitaet, Fairness). Systematische Analyse von sozialen Biases in LLMs mit Fokus auf Geschlecht, Ethnie, sexuelle Orientierung, Alter und Religion. Behandelt Messungs- und Mitigationsstrategien."
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Bias in High-Stakes Decisions",
        "Social Bias Inventory",
        "Gender Bias in Language Models",
        "Intersectional Bias",
        "Fairness Metrics",
        "Data Selection Bias",
        "Corpus-Driven Bias Amplification"
      ],
      "knowledge_summary": "Die meisten Verzerrungen in Sprachmodellen entstehen durch Data Selection Bias bei der Wahl der Trainingstexte; das Paper inventarisiert elf Typen sozialer Bias (Gender, Alter, sexuelle Orientierung, Ethnizität, Religion, Kultur, Nationalität, Behinderung, Sprache und intersektionale Bias) und argumentiert für präventive Ansätze bei der Datenkurierung statt nachträglicher Debiasingmaßnahmen."
    },
    {
      "id": "SE579V7B",
      "stem": "Ng_2021_Conceptualizing_AI_literacy_An_exploratory_review",
      "title": "Conceptualizing AI literacy: An exploratory review",
      "author_year": "Ng (2021)",
      "year": 2021,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Das Paper schlägt vier zentrale Aspekte zur Konzeptualisierung von AI Literacy vor: (1) Know and Understand, (2) Use and Apply, (3) Evaluate and Create, und (4) Ethical Issues. Dabei wird AI Literacy ",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Das Paper ist explizit ein exploratory review zu AI Literacy und behandelt somit substantiell KI-Kompetenzen und Wissenstransfer. Es erfüllt die TECHNIK-Bedingung. Jedoch gibt es keinen erkennbaren Bezug zu sozialen Dimensionen wie Soziale Arbeit, Bias/Ungleichheit, Gender, Diversität, Feminismus od"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Fairness and Transparency",
        "K-12 AI Curriculum Design",
        "Social Inclusion and Diversity in AI Adoption",
        "AI Literacy",
        "Ethical AI in Education",
        "Teacher Professional Development in AI",
        "TPACK Model (Technological Pedagogical Content Knowledge)",
        "Gender Bias in AI Automation"
      ],
      "knowledge_summary": "Das Paper schlägt vier zentrale Aspekte zur Konzeptualisierung von AI Literacy vor: (1) Know and Understand, (2) Use and Apply, (3) Evaluate and Create, und (4) Ethical Issues. Dabei wird AI Literacy als essenzielle Kompetenz für alle Menschen in der digitalen Ära definiert, mit spezifischem Fokus auf K-12 Bildung und ethische Verantwortung."
    },
    {
      "id": "Ng_2022_Using_digital_story_writing_as_a_pedagogy_to",
      "stem": "Ng_2022_Using_digital_story_writing_as_a_pedagogy_to",
      "title": "Ng_2022_Using_digital_story_writing_as_a_pedagogy_to",
      "author_year": "Ng",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": false
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "DSW als Inquiery-basierter pädagogischer Ansatz fördert wirksam AI-Literalität, indem Schüler authentische Szenarien entwickeln, AI-Wissen anwenden und bedeutungsvolle AI-gestützte Lösungen für reale ",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {}
      },
      "concepts": [
        "Data Bias",
        "AI Literacy",
        "Algorithmic Fairness and AI Ethics",
        "Gender Differences in AI Education",
        "Machine Learning Fundamentals for K-12",
        "Inquiry-Based Learning",
        "Digital Story Writing as Pedagogy"
      ],
      "knowledge_summary": "DSW als Inquiery-basierter pädagogischer Ansatz fördert wirksam AI-Literalität, indem Schüler authentische Szenarien entwickeln, AI-Wissen anwenden und bedeutungsvolle AI-gestützte Lösungen für reale Probleme gestalten können – weit über bloße konzeptionelle Kenntnisse hinaus."
    },
    {
      "id": "Ng_2025_Opportunities,_challenges_and_school_strategies",
      "stem": "Ng_2025_Opportunities,_challenges_and_school_strategies",
      "title": "Ng_2025_Opportunities,_challenges_and_school_strategies",
      "author_year": "Ng",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": false
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Erfahrene Lehrkräfte zeigen optimistische Einstellungen zur GenAI, während zentrale Herausforderungen bei der Schulbereitschaft, KI-Kompetenzen von Lehrkräften und KI-Literalität von Schüler:innen lie",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {}
      },
      "concepts": [
        "School Readiness for AI",
        "Generative AI Integration in Education",
        "Teacher AI Competencies",
        "AI Literacy",
        "Algorithmic Bias in Educational Content",
        "Socio-Ecological Framework for AI Implementation",
        "Academic Integrity in AI-Supported Learning"
      ],
      "knowledge_summary": "Erfahrene Lehrkräfte zeigen optimistische Einstellungen zur GenAI, während zentrale Herausforderungen bei der Schulbereitschaft, KI-Kompetenzen von Lehrkräften und KI-Literalität von Schüler:innen liegen. Professionelle Entwicklung, klare Richtlinien und technische Unterstützung sind Schlüsselstrategien."
    },
    {
      "id": "PAZJHB8J",
      "stem": "Nuwasiima_2024_The_role_of_artificial_intelligence_(AI)_and",
      "title": "The role of artificial intelligence (AI) and machine learning in social work practice",
      "author_year": "Nuwasiima (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI und ML bieten erhebliches Potenzial zur Verbesserung der sozialen Arbeitspraxis durch Automatisierung, Predictive Analytics und bessere Ressourcenallokation, erfordern aber strikte ethische Richtli",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Predictive Analytics, algorithmische Systeme; AI_Literacies: Involvement von Social Workers in AI-Entwicklung) und SOZIAL_OK (Soziale_Arbeit: direkter Bezug zu SW-Praxis und Kinderschutz; Bias_Ungleichheit: algorithmischer Bias gegen Fami"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Fairness and Transparency",
        "Predictive Analytics in Social Work",
        "Inclusive Dataset Design and Representation",
        "Algorithmic Bias in Social Services",
        "Ethical Governance of AI in Social Services",
        "Automation of Routine Tasks in Case Management",
        "AI Literacy in Social Work Education"
      ],
      "knowledge_summary": "AI und ML bieten erhebliches Potenzial zur Verbesserung der sozialen Arbeitspraxis durch Automatisierung, Predictive Analytics und bessere Ressourcenallokation, erfordern aber strikte ethische Richtlinien, Transparenz und ein proaktives Vorgehen gegen algorithmische Vorurteile, um Gerechtigkeit und Gleichheit zu gewährleisten."
    },
    {
      "id": "TNLGELEQ",
      "stem": "Näscher_2025_ReflectAI_Design_and_evaluation_of_an_AI_coach_to",
      "title": "ReflectAI: Design and evaluation of an AI coach to support public servants' self-reflection",
      "author_year": "Näscher (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": false,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": false,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": false
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Herausforderungen bei intra-gouvernementaler Zusammenarbeit lassen sich in drei Kategorien einteilen: interpersonelle und individuelle Dynamiken, strukturelle und systemische Herausforderungen sowie v",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "Soziale_Arbeit"
            ],
            "reasoning": "Paper erfüllt TECHNIK- und SOZIAL-Kriterien: (1) Generative_KI (LLM-basierter AI Coach), Prompting (strukturiertes Prompting für reflektive Praxis), AI_Literacies (Entwicklung von Self-Reflection-Kompetenzen). (2) Soziale_Arbeit (öffentliche Verwaltung, human services context, Kompetenzentwicklung)."
          },
          "human": {
            "decision": "Unclear",
            "categories": [
              "Generative_KI"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt Kategorien wie 'Bias_Ungleichheit', 'Gender' und 'Fairness' zu weit aus, indem es stereotype responses und verringerte Vertrauenswerte als Bias-Analyse interpretiert, obwohl dies eher Evaluationsergebnisse eines KI-Systems sind. Die Kategorien werden auf Phänomene angewendet, die nicht primär auf Ungleichheit/Gerechtigkeit abzielen, sondern auf technische Systemleistung.",
          "disagreement_type": "Human_Unclear",
          "severity": 1
        }
      },
      "concepts": [
        "Organizational Culture Differences",
        "Agency Theory",
        "Information Asymmetry",
        "Governance in IT Implementation",
        "Multi-Principal Problem",
        "Intra-Governmental Collaboration",
        "Bounded Rationality"
      ],
      "knowledge_summary": "Herausforderungen bei intra-gouvernementaler Zusammenarbeit lassen sich in drei Kategorien einteilen: interpersonelle und individuelle Dynamiken, strukturelle und systemische Herausforderungen sowie vertragliche und risikobezogene Herausforderungen. Die Sättigung wurde nach zehn Interviews erreicht."
    },
    {
      "id": "IW32JGWV",
      "stem": "OECD_2023_Advancing_Accountability_in_AI",
      "title": "Advancing Accountability in AI",
      "author_year": "OECD",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Das Papier präsentiert ein strukturiertes DEFINE-ASSESS-TREAT-GOVERN Rahmenwerk, das zeigt, wie Risikomanagement-Praktiken über den gesamten KI-Lebenszyklus (Planung, Datenerfassung, Modellentwicklung",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 92
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt TECHNIK-Kriterium (KI_Sonstige=Ja) und SOZIAL-Kriterium (Bias_Ungleichheit=Ja, Fairness=Ja). ABER: Kein direkter Bezug zu Sozialer Arbeit, keine spezifische soziale Ungleichheit oder marginalisierte Gruppen adressiert. Allgemeine AI-Governance und Fairness-Diskussion ohne Sozialarbeits"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM hat 'Generative_KI' und 'Prompting' als substantiell kodiert, obwohl diese nur implizit/oberflächlich erwähnt sind, während es gleichzeitig 'Bias_Ungleichheit' und 'Diversität' als nicht substantiell ausschloss, obwohl der OECD-Accountability-Fokus genau diese Dimensionen adressiert. Die Kategoriegrenzen wurden asymmetrisch gedehnt.",
          "disagreement_type": "Human_Include_Agent_Exclude",
          "severity": 3
        }
      },
      "concepts": [
        "Trustworthy AI",
        "Algorithmic Fairness",
        "Stakeholder Inclusivity in AI Governance",
        "Organizational Risk Culture",
        "Algorithmic Transparency and Explainability",
        "Algorithmic Accountability",
        "AI Robustness and Safety",
        "AI Lifecycle Risk Management"
      ],
      "knowledge_summary": "Das Papier präsentiert ein strukturiertes DEFINE-ASSESS-TREAT-GOVERN Rahmenwerk, das zeigt, wie Risikomanagement-Praktiken über den gesamten KI-Lebenszyklus (Planung, Datenerfassung, Modellentwicklung, Validierung, Deployment, Betrieb) konkrete technische und prozessuale Maßnahmen zur Umsetzung der OECD-KI-Prinzipien ermöglichen kann."
    },
    {
      "id": "ZMW228P6",
      "stem": "Ovalle_2023_Factoring",
      "title": "Factoring the matrix of domination: A critical review and reimagination of intersectionality in AI fairness",
      "author_year": "Ovalle (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI-Fairness-Forscher reduzieren Intersektionalität überwiegend auf die Optimierung von Fairness-Metriken über demografische Subgruppen, während sie kritische Aspekte wie Machtanalyse, sozialen Kontext",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper adressiert AI Fairness (KI_Sonstige) mit explizit feministisch-theoretischem Rahmen (Collins, Bilge). Substantielle Behandlung von Intersektionalität (Feministisch, Gender, Diversität, Bias_Ungleichheit). Kritische Analyse algorithmischer Systeme unter intersektionaler Perspektive. TECHNIK (KI"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'AI Literacies', 'Generative KI' und 'Prompting' zu weit aus und übersieht dabei, dass das Paper primär intersektionale Fairness und Machtstrukturen (Matrix of Domination) behandelt – Kategorien, die das LLM fälschlicherweise als nicht-relevant einstuft. Die technischen Aspekte sind Mittel, nicht Zweck der Analyse.",
          "disagreement_type": "Human_Unclear",
          "severity": 1
        }
      },
      "concepts": [
        "Algorithmic Power Analysis",
        "Black Feminist Theory in AI",
        "Subgroup Fairness Metrics",
        "Design Justice Framework",
        "Intersectional AI Fairness",
        "Colonial Epistemology in AI",
        "Epistemic Justice in Technology"
      ],
      "knowledge_summary": "AI-Fairness-Forscher reduzieren Intersektionalität überwiegend auf die Optimierung von Fairness-Metriken über demografische Subgruppen, während sie kritische Aspekte wie Machtanalyse, sozialen Kontext und intersektionale Praxis vernachlässigen. Dies reproduziert koloniale Epistemologien und untergrä&bt die liberatorische Kraft der intersektionalen Theorie."
    },
    {
      "id": "ZMW228P6",
      "stem": "Ovalle_2023_Factoring_the_Matrix_of_Domination_A_Critical",
      "title": "Factoring the matrix of domination: A critical review and reimagination of intersectionality in AI fairness",
      "author_year": "Ovalle (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI-Fairness-Forscher reduzieren Intersektionalität primär auf Fairness-Metriken für demographische Subgruppen und ignorieren dabei zentrale intersektionale Prinzipien wie Machtrelationen, soziale Kont",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper adressiert AI Fairness (KI_Sonstige) mit explizit feministisch-theoretischem Rahmen (Collins, Bilge). Substantielle Behandlung von Intersektionalität (Feministisch, Gender, Diversität, Bias_Ungleichheit). Kritische Analyse algorithmischer Systeme unter intersektionaler Perspektive. TECHNIK (KI"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'AI Literacies', 'Generative KI' und 'Prompting' zu weit aus und übersieht dabei, dass das Paper primär intersektionale Fairness und Machtstrukturen (Matrix of Domination) behandelt – Kategorien, die das LLM fälschlicherweise als nicht-relevant einstuft. Die technischen Aspekte sind Mittel, nicht Zweck der Analyse.",
          "disagreement_type": "Human_Unclear",
          "severity": 1
        }
      },
      "concepts": [
        "Black Feminist Epistemology",
        "Epistemological Colonialism in AI Research",
        "Design Justice",
        "Subgroup Fairness Metrics",
        "Algorithmic Bias",
        "Matrix of Domination",
        "Intersectionality in AI Fairness",
        "Citational Praxis"
      ],
      "knowledge_summary": "AI-Fairness-Forscher reduzieren Intersektionalität primär auf Fairness-Metriken für demographische Subgruppen und ignorieren dabei zentrale intersektionale Prinzipien wie Machtrelationen, soziale Kontexte und kritisches Handeln für Gerechtigkeit."
    },
    {
      "id": "JPUCNHNU",
      "stem": "Pan_2025_AI_literacy_and_trust_A_multi-method_study_of",
      "title": "LIBRA: Measuring bias of large language model from a local context",
      "author_year": "Pan (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": false,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": false,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Higher AI literacy shows a paradox: while perceived value increases trust in GAI, greater knowledge can lead to distrust. The study identified three trust categories (trust 52%, distrust 26%, ambivale",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt TECHNIK-Kriterium (Generative_KI, KI_Sonstige) und SOZIAL-Kriterium (Bias_Ungleichheit, Diversitaet, Fairness). Allerdings: Paper behandelt allgemeine LLM-Bias-Evaluation ohne direkten Bezug zu Sozialer Arbeit, sozialarbeiterischen Zielgruppen oder sozialarbeiterischer Praxis. Bias-Ana"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'AI_Literacies' und 'KI_Sonstige' auf allgemeine LLM-Bias-Messungen aus, ohne zu erkennen, dass der Mensch die kritische Dimension von Bias/Ungleichheit in sozialarbeiterischen und diversitätsgerechten Kontexten als zentral bewertet. Das LLM interpretiert 'Bias' technisch statt sozial-kritisch.",
          "disagreement_type": "Human_Include_Agent_Exclude",
          "severity": 3
        }
      },
      "concepts": [
        "AI Accuracy Perception",
        "Human-AI Team Collaboration",
        "AI Trustworthiness",
        "AI Literacy",
        "Knowledge-Trust Paradox",
        "Generative AI (GAI)",
        "Trust in Automation"
      ],
      "knowledge_summary": "Higher AI literacy shows a paradox: while perceived value increases trust in GAI, greater knowledge can lead to distrust. The study identified three trust categories (trust 52%, distrust 26%, ambivalence 22%) and found that AI accuracy perceptions are critical determinants of trust formation."
    },
    {
      "id": "JPUCNHNU",
      "stem": "Pan_2025_LIBRA_Measuring_bias_of_large_language_model_from",
      "title": "LIBRA: Measuring bias of large language model from a local context",
      "author_year": "Pan (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Lokale Wörter (besonders Te Reo Māori) liegen außerhalb der Wissensgrenzen der meisten LLMs; Llama-3 zeigt besseres Verständnis für kulturelle Kontexte, aber alle getesteten Modelle zeigen erhebliche ",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt TECHNIK-Kriterium (Generative_KI, KI_Sonstige) und SOZIAL-Kriterium (Bias_Ungleichheit, Diversitaet, Fairness). Allerdings: Paper behandelt allgemeine LLM-Bias-Evaluation ohne direkten Bezug zu Sozialer Arbeit, sozialarbeiterischen Zielgruppen oder sozialarbeiterischer Praxis. Bias-Ana"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'AI_Literacies' und 'KI_Sonstige' auf allgemeine LLM-Bias-Messungen aus, ohne zu erkennen, dass der Mensch die kritische Dimension von Bias/Ungleichheit in sozialarbeiterischen und diversitätsgerechten Kontexten als zentral bewertet. Das LLM interpretiert 'Bias' technisch statt sozial-kritisch.",
          "disagreement_type": "Human_Include_Agent_Exclude",
          "severity": 3
        }
      },
      "concepts": [
        "Automated Bias Dataset Construction",
        "Knowledge Boundary Problem",
        "Algorithmic Bias in Large Language Models",
        "Low-Resource Language Bias",
        "Marginalized Community Representation",
        "Fairness Metrics for LLM Evaluation",
        "Cultural Context Bias Measurement"
      ],
      "knowledge_summary": "Lokale Wörter (besonders Te Reo Māori) liegen außerhalb der Wissensgrenzen der meisten LLMs; Llama-3 zeigt besseres Verständnis für kulturelle Kontexte, aber alle getesteten Modelle zeigen erhebliche Bias - insbesondere gilt: linguistisch kompetentere Modelle sind stärker verzerrt."
    },
    {
      "id": "ZHQMHHPQ",
      "stem": "Park_2025_AI_algorithm_transparency,_pipelines_for_trust",
      "title": "AI algorithm transparency, pipelines for trust not prisms: mitigating general negative attitudes and enhancing trust toward AI",
      "author_year": "Park (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Algorithmen-Transparenz vermindert signifikant die negative Beziehung zwischen allgemeiner negativer Einstellung gegenüber KI und Vertrauen in das Mutterunternehmen, besonders bei hohem Issue-Involvem",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "Prompting",
              "KI_Sonstige",
              "Fairness"
            ],
            "reasoning": "Paper adressiert AI-Vertrauen, Transparenz und Fairness-Aspekte (TECHNIK-Seite erfüllt). Jedoch fehlt der Sozialarbeitsbezug und kein expliziter Fokus auf Ungleichheit oder Marginalisierung. Allgemeine organisationale Trust-Studie ohne Sozialbereich-Spezifika."
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Bias_Ungleichheit"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "General Negative Attitudes toward AI",
        "AI Literacy",
        "Algorithmic Bias",
        "Issue Involvement",
        "Trust Pipeline Model",
        "Organizational Accountability Signaling",
        "Algorithm Transparency"
      ],
      "knowledge_summary": "Algorithmen-Transparenz vermindert signifikant die negative Beziehung zwischen allgemeiner negativer Einstellung gegenüber KI und Vertrauen in das Mutterunternehmen, besonders bei hohem Issue-Involvement. Transparenz funktioniert als Signalisierungsmechanismus für Organisationsverantwortung und nicht nur als Prism-Modell der Reputation."
    },
    {
      "id": "QY6P4RGQ",
      "stem": "Parrish_2022_BBQ_A_hand-built_bias_benchmark_for_question",
      "title": "BBQ: A hand-built bias benchmark for question answering",
      "author_year": "Parrish (2022)",
      "year": 2022,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "NLP-Modelle zeigen starke Abhängigkeit von Stereotypen in unterinformativen Kontexten und wählen in 77% der Fälle stereotype Antworten; auch mit informativem Kontext reduziert sich die Genauigkeit um ",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "BBQ ist ein etablierter Benchmark für Bias-Evaluationen in QA-Systemen (KI-Sonstige: Ja; Generative_KI: Ja, da oft für LLM-Evaluation genutzt). Das Paper adressiert substantiell Bias, Fairness und Diversität mit explizitem Gender-Fokus. Keine Soziale_Arbeit. Technik+Sozial erfüllt → Include."
          },
          "human": {
            "decision": "Include",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Context-Dependent Bias Manifestation",
        "Bias Benchmark Construction",
        "Intersectional Bias Measurement",
        "Representational Harm",
        "Gender Bias in NLP Models",
        "Algorithmic Bias in Question Answering",
        "Fairness Auditing in NLP Systems",
        "Protected Class Discrimination in AI"
      ],
      "knowledge_summary": "NLP-Modelle zeigen starke Abhängigkeit von Stereotypen in unterinformativen Kontexten und wählen in 77% der Fälle stereotype Antworten; auch mit informativem Kontext reduziert sich die Genauigkeit um bis zu 3,4 Prozentpunkte, wenn die korrekte Antwort mit sozialen Vorurteilen kollidiert."
    },
    {
      "id": "YUVR5YNQ",
      "stem": "Parrish_2025_Self-debiasing_large_language_models_Zero-shot",
      "title": "Self-debiasing large language models: Zero-shot recognition and reduction of stereotypes",
      "author_year": "Parrish (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": false,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Both self-debiasing techniques significantly reduce stereotyping across nine diverse social groups (age, disability, gender identity, nationality, physical appearance, race/ethnicity, religion, sexual",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Das Paper behandelt deutlich Generative KI (LLMs) und Prompting-Strategien zur Bias-Reduktion (Self-Debiasing). Es adressiert auch Fairness und Bias. Jedoch: Keine direkten sozialen Aspekte im Sinne der Review-Kriterien (Soziale_Arbeit, strukturelle Ungleichheit, Gender-Fokus, Diversität, oder femin"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet"
            ]
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "Social Bias in Language Models",
        "Explainability-Based Bias Mitigation",
        "Intersectional Social Groups",
        "Zero-Shot Prompt Engineering",
        "Algorithmic Fairness in Black-Box Systems",
        "Stereotype Benchmark Evaluation",
        "Self-Debiasing via Prompting"
      ],
      "knowledge_summary": "Both self-debiasing techniques significantly reduce stereotyping across nine diverse social groups (age, disability, gender identity, nationality, physical appearance, race/ethnicity, religion, sexual orientation, socioeconomic status), with reprompting showing the greatest bias reduction while relying only on simple prompts and the LLM itself."
    },
    {
      "id": "LGYFN6JK",
      "stem": "Patton_2023_ChatGPT_for_Social_Work_Science_Ethical",
      "title": "ChatGPT for Social Work Science: Ethical Challenges and Opportunities",
      "author_year": "Patton (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "ChatGPT bietet innovative Möglichkeiten für die Sozialarbeitswissenschaft (Forschungsdesign, Qualitative Datenanalyse, Textbearbeitung), erfordert jedoch strenge ethische Richtlinien bezüglich Bias, D",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI: LLM-Fokus; AI_Literacies: kritischer Umgang mit KI-Systemen) und SOZIAL_OK (Soziale_Arbeit: Ethik für sozialarbeiterische Forschung; Bias_Ungleichheit & Fairness: explizit Bias-Bekämpfung und Inklusion/Social Justice). Substantieller Bezug "
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Antiracist Research Framework",
        "AI Literacy in Professional Practice",
        "Data Privacy and Informed Consent in AI Systems",
        "Equitable Access to AI Tools for Researchers",
        "Algorithmic Bias in Large Language Models",
        "Scientific Integrity and Source Verification",
        "Prompt Engineering for Inclusive Research",
        "Algorithmic Fairness and Social Justice"
      ],
      "knowledge_summary": "ChatGPT bietet innovative Möglichkeiten für die Sozialarbeitswissenschaft (Forschungsdesign, Qualitative Datenanalyse, Textbearbeitung), erfordert jedoch strenge ethische Richtlinien bezüglich Bias, Datenschutz, Transparenz und sozialer Gerechtigkeit. Die Profession muss ein umfassendes ethisches Framework entwickeln."
    },
    {
      "id": "HTXCWNQ9",
      "stem": "Peng_2022_A_Literature_Review_of_Digital_Literacy_over_Two",
      "title": "A Literature Review of Digital Literacy over Two Decades",
      "author_year": "Peng (2022)",
      "year": 2022,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Digitale Literalität wird in zwei Hauptrichtungen definiert (technische Fähigkeiten vs. konzeptuelle/kritische Kompetenzen); Familie und elterliche Bildung sind konsistent wichtige Einflussfaktoren, w",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [
              "Gender"
            ],
            "reasoning": "Das Paper behandelt digitale Literalität von Studierenden im Allgemeinen, nicht KI-spezifische Kompetenzen (AI Literacy). Gender wird erwähnt als Einflussfaktor, aber substantiell nicht fokussiert. Keine KI/ML-Technologie-Komponente. Kein Bezug zu Sozialer Arbeit. TECHNIK-Bedingung nicht erfüllt → E"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Gender Differences in Digital Literacy",
        "Socioeconomic Status and Digital Competence",
        "Digital Divide",
        "Technostress",
        "Digital Literacy",
        "Digital Equity in Education",
        "Critical Digital Thinking"
      ],
      "knowledge_summary": "Digitale Literalität wird in zwei Hauptrichtungen definiert (technische Fähigkeiten vs. konzeptuelle/kritische Kompetenzen); Familie und elterliche Bildung sind konsistent wichtige Einflussfaktoren, während Alter und Geschlecht inkonsistente Ergebnisse zeigen; verbesserte digitale Literalität reduziert Technostress, erhöht Selbstkontrolle und Engagement."
    },
    {
      "id": "22XEFRWP",
      "stem": "Petzel_2025_Prejudiced_interactions_with_large_language",
      "title": "Prejudiced interactions with large language models (LLMs) reduce trustworthiness and behavioral intentions among members of stigmatized groups",
      "author_year": "Petzel (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Mitglieder stigmatisierter Gruppen (Black Americans, Frauen) berichten höheres Vertrauen in LLMs nach unvoreingenommenen Interaktionen; Vertrauen erklärt erhöhte Nutzungsabsichten bei stigmatisierten ",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI: LLMs sind zentral), SOZIAL_OK (Bias_Ungleichheit: Analyse von Prejudice in LLM-Outputs; Diversitaet: Fokus auf marginalized groups; Fairness: Implikationen für vertrauenswürdige KI-Nutzung). Drei preregistrierte experimentelle Studien mit k"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Trustworthiness in AI Systems",
        "AI Literacy and Behavioral Intentions",
        "Algorithmic Bias",
        "Algorithmic Justice in Employment and Education",
        "Digital Divide and Technology Adoption",
        "Differential Impact of AI Fairness Interventions",
        "Implicit Bias and Intersectionality"
      ],
      "knowledge_summary": "Mitglieder stigmatisierter Gruppen (Black Americans, Frauen) berichten höheres Vertrauen in LLMs nach unvoreingenommenen Interaktionen; Vertrauen erklärt erhöhte Nutzungsabsichten bei stigmatisierten Gruppen. Umgekehrt nutzen White Americans und Männer LLMs vermehrt, wenn KI-generierte Vorurteile ihre impliziten Biases bestätigen."
    },
    {
      "id": "3K9RDWLY",
      "stem": "Pinski_2023_AI_Literacy_-_Towards_Measuring_Human_Competency",
      "title": "AI Literacy - Towards Measuring Human Competency in Artificial Intelligence",
      "author_year": "Pinski (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": false,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Entwicklung und Validierung eines 13-Item-Messinstruments für allgemeine AI Literacy mit 5 Dimensionen, das sowohl explizites Wissen als auch tacit knowledge und Erfahrung (Nutzung und Design) erfasst",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Das Paper adressiert AI Literacy als Messung von KI-Kompetenzen (TECHNIK_OK). Es fehlt jedoch jede SOZIAL-Komponente: kein Bezug zu Sozialer Arbeit, keinen spezifischen Fokus auf Bias, Gender, Diversität oder Fairness. Ohne Sozialarbeitsbezug oder sozialwissenschaftliche Kritik wird es ausgeschlosse"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Socio-Technical Competencies",
        "Tacit Knowledge in AI Contexts",
        "AI Literacy",
        "AI Ethics and Fairness Evaluation",
        "Human-AI Collaboration",
        "Measurement Instrument Development",
        "AI Properties Framework"
      ],
      "knowledge_summary": "Entwicklung und Validierung eines 13-Item-Messinstruments für allgemeine AI Literacy mit 5 Dimensionen, das sowohl explizites Wissen als auch tacit knowledge und Erfahrung (Nutzung und Design) erfasst"
    },
    {
      "id": "Pinski_2024_AI_Literacy_for_the_top_management_An_upper",
      "stem": "Pinski_2024_AI_Literacy_for_the_top_management_An_upper",
      "title": "Pinski_2024_AI_Literacy_for_the_top_management_An_upper",
      "author_year": "Pinski",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": false
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": false
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "TMT AI-Literalität hat positive Effekte auf AI-Orientierung und HR-bezogene AI-Implementierungsfähigkeit. AI-Orientierung mediiert den Effekt auf Implementierungsfähigkeit, und dieser Effekt ist in St",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 86
        },
        "assessment": {}
      },
      "concepts": [
        "Upper Echelons Theory",
        "AI Value Realization Gap",
        "Organizational Context Moderation",
        "Top Management Team Composition",
        "AI Literacy",
        "Corporate AI Orientation",
        "Skill-Oriented Approach",
        "AI Implementation Capability"
      ],
      "knowledge_summary": "TMT AI-Literalität hat positive Effekte auf AI-Orientierung und HR-bezogene AI-Implementierungsfähigkeit. AI-Orientierung mediiert den Effekt auf Implementierungsfähigkeit, und dieser Effekt ist in Startups stärker als in etablierten Firmen."
    },
    {
      "id": "7G78SADI",
      "stem": "Pinski_2024_AI_literacy_for_users_–_A_comprehensive_review",
      "title": "AI literacy for users – A comprehensive review and future research directions of learning methods, components, and effects",
      "author_year": "Pinski (01/2024)",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI Literacy ist ein fragmentiertes Forschungsfeld, das durch ein overarching conceptual framework strukturiert werden kann, das fünf Kernbereiche (AI Models, Data for AI, AI Interfaces, AI Tools, Huma",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 86
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Das Paper fokussiert auf AI Literacy als Kompetenzbreich – ein eindeutiges TECHNIK-Thema (AI_Literacies: Ja). Der Titel und Fokus deuten auf eine Übersichtsarbeit zu Lernmethoden und Effekten hin, ohne soziale/kritische Dimensionen. Ohne Abstract und Hinweise auf Soziale Arbeit oder kritische Bias-/"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender"
            ]
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "AI Literacy",
        "Algorithmic Bias",
        "Explainable AI (XAI)",
        "Human-AI Collaboration",
        "Differentiated AI Literacy Requirements",
        "AI Tool Inscrutability",
        "Responsible AI"
      ],
      "knowledge_summary": "AI Literacy ist ein fragmentiertes Forschungsfeld, das durch ein overarching conceptual framework strukturiert werden kann, das fünf Kernbereiche (AI Models, Data for AI, AI Interfaces, AI Tools, Humans/Organizations/Society) und differentielle Anforderungen für Expert- vs. Non-Expert-Nutzer aufzeigt."
    },
    {
      "id": "J5EF9W6M",
      "stem": "Project_2024_Intersectionality",
      "title": "AI & Intersectionality: A Toolkit For Fairness & Inclusion",
      "author_year": "Project",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Intersektionale Vorurteile in KI entstehen durch Überlagerung mehrerer Diskriminierungsformen und erfordern ganzheitliche, disziplinübergreifende Strategien zur Mitigation, die über isolierte Bias-Ans",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "Prompting",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Das Toolkit adressiert substantiell KI-Kompetenzen (Bewusstsein und Handlungsstrategien für Stakeholder), Prompting-Strategien (intersektionale Szenarien zur Bias-Aufdeckung) und KI-Systeme generell. Die intersektionale Perspektive erfüllt implizit feministische Ansätze (Crenshaw'sche Intersektional"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Holistic Fairness Design",
        "Gender Bias in Algorithmic Systems",
        "AI Literacy",
        "Culturally Localized Solutions",
        "Organizational Accountability Frameworks",
        "Participatory Machine Learning",
        "Intersectional Algorithmic Bias",
        "Data Feminism"
      ],
      "knowledge_summary": "Intersektionale Vorurteile in KI entstehen durch Überlagerung mehrerer Diskriminierungsformen und erfordern ganzheitliche, disziplinübergreifende Strategien zur Mitigation, die über isolierte Bias-Ansätze hinausgehen."
    },
    {
      "id": "MTMU9UPJ",
      "stem": "Qiu_2025_DR.GAP_Mitigating_bias_in_large_language_models",
      "title": "DR.GAP: Mitigating bias in large language models using gender-aware prompting with demonstration and reasoning",
      "author_year": "Qiu (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "DR.GAP reduziert Gender-Bias um durchschnittlich 44,98% (GPT-3.5), 36,32% (Llama3) und 39,32% (Llama2-Alpaca) in Coreference-Resolution-Tasks, während die Modellnützlichkeit erhalten bleibt und das Ve",
          "stage3_completeness": 92,
          "stage3_correctness": 96,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt TECHNIK-Bedingung (Generative_KI, Prompting). Jedoch fehlt SOZIAL-Bedingung für Anwendungsgebiet mit sozialem Fokus: Bias_Ungleichheit, Gender und Fairness behandeln allgemeine LLM-Bias, nicht spezifisch Soziale Arbeit, marginalisierte Communities oder sozialarbeiterische Kontexte. Rei"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'Fairness' zu einer eigenständigen Kategorie aus und gewichtet technische Fairness-Aspekte höher als die implizite Anforderung, dass Gender/Diversität nur im Kontext von Sozialer Arbeit relevant sind. Die menschliche Entscheidung erkennt Gender und Diversität als feldspezifisch relevant an, während das LLM diese Kategorien narrower interpretiert.",
          "disagreement_type": "Human_Include_Agent_Exclude",
          "severity": 3
        }
      },
      "concepts": [
        "Model-Agnostic Debiasing",
        "Algorithmic Fairness in Vision-Language Models",
        "Fairness-Utility Trade-off",
        "Gender Bias in Large Language Models",
        "Gender-Aware Prompting",
        "Iterative Reasoning Refinement",
        "Coreference Resolution Bias Evaluation",
        "In-Context Learning with Demonstrations"
      ],
      "knowledge_summary": "DR.GAP reduziert Gender-Bias um durchschnittlich 44,98% (GPT-3.5), 36,32% (Llama3) und 39,32% (Llama2-Alpaca) in Coreference-Resolution-Tasks, während die Modellnützlichkeit erhalten bleibt und das Verfahren auf Vision-Language Models generalisiert."
    },
    {
      "id": "MTMU9UPJ",
      "stem": "Qiu_2025_Mitigating",
      "title": "DR.GAP: Mitigating bias in large language models using gender-aware prompting with demonstration and reasoning",
      "author_year": "Qiu (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "EDITBIAS zeigt überlegene Debiasing-Leistung im Vergleich zu klassischen Methoden und ist robust gegenüber Geschlechts-Umkehrung und semantischer Verallgemeinerung, jedoch offenbart die Arbeit einen f",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt TECHNIK-Bedingung (Generative_KI, Prompting). Jedoch fehlt SOZIAL-Bedingung für Anwendungsgebiet mit sozialem Fokus: Bias_Ungleichheit, Gender und Fairness behandeln allgemeine LLM-Bias, nicht spezifisch Soziale Arbeit, marginalisierte Communities oder sozialarbeiterische Kontexte. Rei"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'Fairness' zu einer eigenständigen Kategorie aus und gewichtet technische Fairness-Aspekte höher als die implizite Anforderung, dass Gender/Diversität nur im Kontext von Sozialer Arbeit relevant sind. Die menschliche Entscheidung erkennt Gender und Diversität als feldspezifisch relevant an, während das LLM diese Kategorien narrower interpretiert.",
          "disagreement_type": "Human_Include_Agent_Exclude",
          "severity": 3
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Model Editing",
        "Bias Localization",
        "Robustness Testing",
        "Gender Bias in Language Models",
        "Stereotype Bias",
        "Debiasing-Effectiveness Trade-off"
      ],
      "knowledge_summary": "EDITBIAS zeigt überlegene Debiasing-Leistung im Vergleich zu klassischen Methoden und ist robust gegenüber Geschlechts-Umkehrung und semantischer Verallgemeinerung, jedoch offenbart die Arbeit einen fundamentalen Trade-off zwischen Debiasing-Effektivität und Bewahrung von Sprachmodellierungsfähigkeiten, insbesondere bei größeren und kausalen Sprachmodellen."
    },
    {
      "id": "HN7KKNYV",
      "stem": "Reamer_2023_Artificial_Intelligence_in_Social_Work_Emerging",
      "title": "Artificial intelligence in social work: Emerging ethical issues",
      "author_year": "Reamer (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Sozialarbeiter müssen sich mit neun zentralen ethischen Herausforderungen auseinandersetzen (informierte Zustimmung, Datenschutz, Transparenz, Fehldiagnose, Patientenabbruch, Überwachung, Plagiarismus",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies + KI_Sonstige: Bezug zu ethischen Rahmenbedingungen und Implementierung von KI-Systemen in sozialer Arbeit) und SOZIAL_OK (Soziale_Arbeit + Bias_Ungleichheit + Fairness: Direkter Bezug zu Sozialarbeitspraxis, explizite Behandlung von algorit"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "Generative_KI",
              "Prompting",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Implizite Feldzugehoerigkeit",
          "divergence_justification": "Das LLM erkannte die expliziten Keywords (Bias, Fairness, Diversität, Intersektionalität) und aktivierte eine Include-Entscheidung, übersah aber das implizite Feldwissen, dass ein Paper über ethische Fragen in der Sozialen Arbeit (nicht technisch-fokussiert) und Gender-Aspekte als zentrale Kategorien hätte klassifizieren müssen – der Human erkannte diese domänen-spezifischen Kontexte, das LLM nicht.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Informed Consent in AI-Mediated Social Services",
        "Professional Ethics Standards for AI Integration",
        "Algorithmic Fairness and Justice",
        "AI Transparency and Explainability",
        "Algorithmic Bias in Social Work",
        "AI Literacy for Social Work Practitioners",
        "Data Privacy and Confidentiality in AI Systems",
        "Predictive Analytics in Risk Assessment"
      ],
      "knowledge_summary": "Sozialarbeiter müssen sich mit neun zentralen ethischen Herausforderungen auseinandersetzen (informierte Zustimmung, Datenschutz, Transparenz, Fehldiagnose, Patientenabbruch, Überwachung, Plagiarismus, algorithmischer Bias), und es existiert derzeit keine umfassende berufsethische Reflexion dieser Themen in der Sozialarbeits-Literatur."
    },
    {
      "id": "XIYX5HJS",
      "stem": "Ricaurte Quijano_2024_Towards_Substantive_Equality_in_Artificial",
      "title": "Towards substantive equality in artificial intelligence: Transformative AI policy for gender equality and diversity",
      "author_year": "Ricaurte Quijano (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "KI-Systeme sind nicht neutral und reproduzieren strukturelle Ungleichheiten. Eine transformative Gleichstellungspolitik muss systembezogene Nachteile adressieren, demokratische Defizite überbrücken un",
          "stage3_completeness": 92,
          "stage3_correctness": 96,
          "stage3_overall": 93
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (Policy-Rahmen für AI-Governance), KI_Sonstige (algorithmische Diskriminierung). SOZIAL: Bias_Ungleichheit (Fokus auf algorithmische Diskriminierung), Gender (explizite intersektionale Genderanalyse), Diversitaet (inklusive Repräsentation),"
          },
          "human": {
            "decision": "Unclear",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Implizite Feldzugehoerigkeit",
          "divergence_justification": "Das LLM erkannte nicht, dass ein Paper mit Titel zu 'substantive equality', 'gender equality' und 'diversity' im Kontext von KI-Policy primär ein sozialwissenschaftliches Werk ist, das KI nur als Anwendungsfeld behandelt. Das implizite Feldwissen, dass solche Policy-Papers zur Sozialen Arbeit/Fairness gehören, fehlte; stattdessen fokussierte das LLM auf technische Keywords wie 'Generative KI' und 'Prompting'.",
          "disagreement_type": "Human_Unclear",
          "severity": 1
        }
      },
      "concepts": [
        "Algorithmic Oppression",
        "Intersectional Feminism in AI",
        "AI Literacy and Capacity Building",
        "Inclusive AI Design",
        "Substantive Equality",
        "Gender Bias in AI",
        "AI Governance and Accountability",
        "Meaningful Participation of Marginalized Groups"
      ],
      "knowledge_summary": "KI-Systeme sind nicht neutral und reproduzieren strukturelle Ungleichheiten. Eine transformative Gleichstellungspolitik muss systembezogene Nachteile adressieren, demokratische Defizite überbrücken und Fehlrepräsentation umkehren durch inklusives Design, sinnvolle Partizipation und Accountability über den gesamten KI-Lebenszyklus."
    },
    {
      "id": "TRAN2GJU",
      "stem": "Ricaurte_2024_How_can_feminism_inform_AI_governance_in_practice",
      "title": "How can feminism inform AI governance in practice?",
      "author_year": "Ricaurte (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 4,
          "stage1_key_finding": "Feminist AI Governance adressiert Machtungleichgewichte im KI-Ökosystem durch systemic-strukturelle Ansätze und erkennt algorithmische Systeme als soziotechnische Assemblierungen an, die von Machtverh",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper adressiert KI-Governance als algorithmisches System (KI_Sonstige) mit explizitem Fokus auf feministische Theorie und Methodik (Feministisch). Substantielle Behandlung von Machtungleichgewichten, Bias, strukturellen Ungleichheiten (Bias_Ungleichheit), Gender-Perspektive und Diversität/Inklusion"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'Generative_KI' und 'KI_Sonstige' auf einen konzeptionellen/governanceorientierten Text aus, der sich nicht mit generativen Modellen oder technischen KI-Systemen befasst, sondern mit Feminist Theory als analytischer Rahmen für Governance. Der menschliche Bewerter erkannte korrekt, dass dies außerhalb des Scope liegt.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Feminist AI Governance",
        "Algorithmic Bias in Global South Contexts",
        "Algorithmic Assemblages",
        "Gender Representation in AI Ecosystems",
        "Participatory Design Justice",
        "Intersectional AI Literacy",
        "Care Ethics in Technology Design"
      ],
      "knowledge_summary": "Feminist AI Governance adressiert Machtungleichgewichte im KI-Ökosystem durch systemic-strukturelle Ansätze und erkennt algorithmische Systeme als soziotechnische Assemblierungen an, die von Machtverhältnissen geprägt sind. Drei implementierte Initiativen demonstrieren die Machbarkeit und Notwendigkeit, KI-Systeme auf Basis von sozialer Gerechtigkeit, Menschenrechten und Care-Prinzipien zu entwick"
    },
    {
      "id": "YRBP6IEJ",
      "stem": "Rodriguez_2024_Introducing_Generative_Artificial_Intelligence",
      "title": "Introducing Generative Artificial Intelligence into the MSW Curriculum: A Proposal for the 2029 Educational Policy and Accreditation Standards",
      "author_year": "Rodriguez (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Die Autor:innen schlagen eine neue EPAS-Kompetenzindikator vor: Sozialarbeiter demonstrieren Wissen, Fähigkeiten und Verständnis für verantwortungsvolle Nutzung generativer KI-Tools. Vier Kernfähigkei",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper behandelt substantiell KI-Kompetenzen (AI_Literacies) und generative KI im MSW-Curriculum. Direkter Bezug zu Sozialer Arbeit (Ausbildung, Akkreditierungsstandards). Adressiert Bias, Transparenz und Fairness als zentrale Themen für verantwortungsvolle AI-Nutzung im Kontext von Klientenschutz un"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Narrow AI in Social Work Practice",
        "Large Language Model (LLM) Applications in Practice",
        "Social Work Education Policy Integration",
        "Generative AI Literacy in Social Work",
        "Algorithmic Bias in Generative AI",
        "Prompt Engineering",
        "AI Hallucination Mitigation"
      ],
      "knowledge_summary": "Die Autor:innen schlagen eine neue EPAS-Kompetenzindikator vor: Sozialarbeiter demonstrieren Wissen, Fähigkeiten und Verständnis für verantwortungsvolle Nutzung generativer KI-Tools. Vier Kernfähigkeiten werden identifiziert: konzeptionelles Verständnis von KI-Modellen, effektives Prompting, Output-Bewertung und kontinuierliches Lernen."
    },
    {
      "id": "9WIGR47Y",
      "stem": "Rodríguez-Martínez_2024_Ethical_issues_related_to_the_use_of_technology",
      "title": "Ethical issues related to the use of technology in social work practice: A systematic review",
      "author_year": "Rodríguez-Martínez (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Die Analyse von 15 Studien zeigt drei Hauptkategorien ethischer Herausforderungen: Auswirkungen der Digitalisierung auf die berufliche Praxis, Implikationen für Bildung und Forschung, sowie spezifisch",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 87
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Paper behandelt ethische Herausforderungen von Technologie in der Sozialen Arbeit substantiell. KI_Sonstige erfüllt (automatisierte Systeme, digitale Technologien), Soziale_Arbeit erfüllt (direkte Praxisanalyse), Bias_Ungleichheit erfüllt (digital divide, soziale Gerechtigkeit, Selbstbestimmung). Sy"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "Soziale_Arbeit"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'Bias_Ungleichheit' und 'Fairness' auf allgemeine algorithmische Bias-Reduktion aus, obwohl der Paper spezifisch auf Soziale Arbeit fokussiert und nicht primär um LLM-Fairness geht. Das LLM überkompensiert die fehlende Soziale_Arbeit-Kategorie durch Überinterpretation von Bias-Aspekten.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Digital Professional Boundaries in Social Work",
        "Cultural Competence and Technological Accessibility",
        "Data Protection and Client Privacy in Digital Practice",
        "Algorithmic Decision-Making in Social Services",
        "Digital Divide and Inclusive Online Services",
        "Digital Ethics in Social Work",
        "Digital Literacy in Social Work Education"
      ],
      "knowledge_summary": "Die Analyse von 15 Studien zeigt drei Hauptkategorien ethischer Herausforderungen: Auswirkungen der Digitalisierung auf die berufliche Praxis, Implikationen für Bildung und Forschung, sowie spezifische ethische Probleme der digitalen Praxis. Es besteht Bedarf für verbindliche ethische Grundsätze und regulatorische Standards zum Schutz persönlicher digitaler Daten."
    },
    {
      "id": "K3YCLBXK",
      "stem": "Ruiz_2024_AI_Literacy_A_Framework_to_Understand,_Evaluate,",
      "title": "AI Literacy: A Framework to Understand, Evaluate, and Use Emerging Technology",
      "author_year": "Ruiz (06/2024)",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 5,
          "stage1_key_finding": "Ein integriertes AI Literacy Framework mit drei Modes of Engagement (Understand, Evaluate, Use), sechs AI Literacy Practices und zwei Core Values (Human Judgment, Centering Justice), das Schulleitern ",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 92
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Das Paper behandelt AI Literacy im Bildungskontext (PK-12 Education) und erfüllt die TECHNIK-Bedingung durch AI_Literacies = Ja. Es fehlt jedoch jegliche SOZIAL-Komponente: Kein Bezug zu Sozialer Arbeit, keine Behandlung von Bias/Ungleichheit, Gender, Diversität, feministischen Perspektiven oder Fai"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM hat die Kategorie SOZIAL_OK zu restriktiv interpretiert und verlangt explizite Inhaltsverifikation, während der Mensch erkannt hat, dass ein Framework zu AI Literacy inhärent soziale/ethische Dimensionen (Bias, Gender, Diversität) impliziert. Das LLM dehnte die technische Anforderung aus, während es soziale Kontexte unterdehnte.",
          "disagreement_type": "Human_Include_Agent_Exclude",
          "severity": 3
        }
      },
      "concepts": [
        "Algorithmic Fairness Evaluation",
        "Tech Justice",
        "Computational Thinking",
        "AI Literacy",
        "Generative AI Literacy",
        "Algorithmic Bias",
        "Data Privacy Literacy"
      ],
      "knowledge_summary": "Ein integriertes AI Literacy Framework mit drei Modes of Engagement (Understand, Evaluate, Use), sechs AI Literacy Practices und zwei Core Values (Human Judgment, Centering Justice), das Schulleitern bei der Gestaltung von KI-Kompetenzvermittlung hilft und Risiken wie Bias, Datenschutz und Desinformation adressiert."
    },
    {
      "id": "M7AGB7LI",
      "stem": "Salecha_2025_Model_explanations_for_gender_and_ethnicity_bias",
      "title": "Model explanations for gender and ethnicity bias mitigation in AI-generated narratives",
      "author_year": "Salecha (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Die Integration von modellgenerierten Erklärungen in Prompts reduziert Bias um 2%-20% und verbessert die demografische Repräsentation signifikant. Alle drei Modelle zeigen konsistente Muster bei den S",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.75,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Das Paper adressiert explizit Bias-Mitigation in KI-generierten Narrativen mit Fokus auf Gender und Ethnizität. Generative_KI (narratives generation), Bias_Ungleichheit (gender and ethnicity bias), Gender (expliziter Gender-Fokus), Diversitaet (ethnicity representation) und Fairness (bias mitigation"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Model-Generated Explanations as Feedback Mechanism",
        "Explainable AI (XAI) for Fairness",
        "Training Data-Induced Bias in Narratives",
        "Gender and Ethnicity Bias Mitigation",
        "Prompt Engineering for Bias Reduction",
        "Intersectional Bias Analysis",
        "Fairness Metrics (TVD and DPR)"
      ],
      "knowledge_summary": "Die Integration von modellgenerierten Erklärungen in Prompts reduziert Bias um 2%-20% und verbessert die demografische Repräsentation signifikant. Alle drei Modelle zeigen konsistente Muster bei den Stereotypen-Erklärungen, die auf zugrundeliegende Bias-Strukturen hinweisen."
    },
    {
      "id": "YMABYPKF",
      "stem": "Salinas_2025_What’s_in_a_name_Auditing_large_language_models",
      "title": "What’s in a name? Auditing large language models for race and gender bias",
      "author_year": "Salinas (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "LLMs zeigen systematische Biase, die Namen benachteiligen, die mit rassischen Minderheiten und Frauen assoziiert sind, wobei Black Women die am meisten benachteiligten Ergebnisse erhalten. Diese Biase",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Das Paper adressiert Generative KI (GPT-4, LLMs) mit Fokus auf Prompting (42 Prompt-Templates als Audit-Instrument). Es thematisiert explizit intersektionalen Bias (Rasse × Geschlecht nach Crenshaw-Logik), Gender-Bias und Fairness. Die intersektionale Analyse erfüllt den feministischen Kriterium. TE"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "AI Auditing",
        "Bias Mitigation Strategies",
        "Name-Based Discrimination",
        "Disparate Impact Assessment",
        "Algorithmic Bias in Large Language Models",
        "Intersectional Bias",
        "Stereotype Embedding in Language Models"
      ],
      "knowledge_summary": "LLMs zeigen systematische Biase, die Namen benachteiligen, die mit rassischen Minderheiten und Frauen assoziiert sind, wobei Black Women die am meisten benachteiligten Ergebnisse erhalten. Diese Biase sind konsistent über Modelle und Szenarien hinweg und können durch numerische Anker, nicht aber durch qualitative Details reduziert werden."
    },
    {
      "id": "KNQYFQ6B",
      "stem": "Sant_2024_The_power_of_prompts_Evaluating_and_mitigating",
      "title": "The power of prompts: Evaluating and mitigating gender bias in MT with LLMs",
      "author_year": "Sant (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Base LLMs zeigen signifikant höheren Geschlechterbias als NMT-Modelle; durch gezielte Prompt-Strukturen mit Chain-of-Thought kann Gender-Bias bei instruktionsgestimmten LLMs um bis zu 12% reduziert we",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Fairness"
            ],
            "reasoning": "Das Paper behandelt LLMs (Generative_KI=Ja), Prompt-Engineering (Prompting=Ja), Gender-Bias (Gender=Ja) und Fairness-Metriken (Fairness=Ja). Dies erfüllt die TECHNIK-Bedingung klar. SOZIAL-Bedingung: Bias_Ungleichheit=Ja ist erfüllt. Allerdings liegt der Fokus ausschließlich auf technischer Sprachüb"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender"
            ]
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "Gender Coreference Resolution in NLP",
        "Prompt Engineering for Bias Mitigation",
        "Algorithmic Fairness Evaluation",
        "Chain-of-Thought Reasoning",
        "Representational and Allocational Harm",
        "Neural Machine Translation vs. Large Language Models",
        "Gender Bias in Machine Translation"
      ],
      "knowledge_summary": "Base LLMs zeigen signifikant höheren Geschlechterbias als NMT-Modelle; durch gezielte Prompt-Strukturen mit Chain-of-Thought kann Gender-Bias bei instruktionsgestimmten LLMs um bis zu 12% reduziert werden, wodurch die Lücke zu NMT-Systemen deutlich verkleinert wird."
    },
    {
      "id": "YV53DKI2",
      "stem": "Santos_2024_Explainability_through_systematicity_The_hard",
      "title": "Explainability through systematicity: The hard systematicity challenge",
      "author_year": "Santos (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 5,
          "stage1_key_finding": "Explainability ist nur eine Facette eines breiteren Ideals der Systematizität. Die angemessene Forderung an KI-Systeme ist nicht nur Mikro-Systematizität (Kompositionialität), sondern Makro-Systematiz",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige"
            ],
            "reasoning": "Paper behandelt philosophische Grundlagen von AI-Explainability und Systematizität (KI_Sonstige: Ja). Dies ist jedoch rein technisch-philosophischer Natur ohne Bezug zu sozialen Auswirkungen, Bias, Ungleichheit, Gender, Diversität, Feminismus oder Fairness. Kein Bezug zu Sozialer Arbeit. TECHNIK erf"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Fairness"
            ]
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Epistemological Quality Control",
        "Macro-Systematicity",
        "Large Language Model Consistency",
        "Interpretability in Neural Networks",
        "AI Explainability",
        "AI Accountability"
      ],
      "knowledge_summary": "Explainability ist nur eine Facette eines breiteren Ideals der Systematizität. Die angemessene Forderung an KI-Systeme ist nicht nur Mikro-Systematizität (Kompositionialität), sondern Makro-Systematizität: ein integriertes, konsistentes, kohärentes und parsimonisch prinzipiertes Gedankensystem. Diese Forderung wird durch fünf identifizierbare Rationales (kognitiv, hermeneutisch, epistemologisch, k"
    },
    {
      "id": "99QJDBSV",
      "stem": "Santos_2025_How_large_language_models_judge_cooperation",
      "title": "How large language models judge cooperation",
      "author_year": "Santos (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "LLMs zeigen Übereinstimmung bei der Beurteilung von Kooperation mit wohlreputablen Personen, aber erhebliche Varianz bei der Beurteilung von Kooperation mit Personen mit schlechtem Ruf; diese Untersch",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.82,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper untersucht LLMs (Generative_KI: Ja) systematisch mit 43.200 Prompts (Prompting: Ja, substantiell). Zentrale Ergebnisse adressieren Bias und Malleabilität sozialer Normen (Bias_Ungleichheit: Ja) sowie Fairness-Aspekte bei moralischen Urteilen (Fairness: Ja). TECHNIK und SOZIAL erfüllt → Include"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "LLM Model Variability in Norm Judgment",
        "Algorithmic Bias in Social Judgment",
        "Evolutionary Game Theory in Cooperation Dynamics",
        "AI-Mediated Social Decision-Making",
        "Indirect Reciprocity",
        "Algorithmic Fairness in Cooperative Systems",
        "LLM Social Norm Internalization",
        "Prompt Engineering for Norm Steering"
      ],
      "knowledge_summary": "LLMs zeigen Übereinstimmung bei der Beurteilung von Kooperation mit wohlreputablen Personen, aber erhebliche Varianz bei der Beurteilung von Kooperation mit Personen mit schlechtem Ruf; diese Unterschiede können Kooperationsniveaus signifikant beeinflussen, und gezielte Prompt-Interventionen können LLM-Normen lenken."
    },
    {
      "id": "QLXLEUCG",
      "stem": "Schneider_2018_Der_Einfluss_der_Algorithmen_Neue_Qualitäten",
      "title": "Der Einfluss der Algorithmen: Neue Qualitäten durch Big Data Analytics und Künstliche Intelligenz",
      "author_year": "Schneider (2018)",
      "year": 2018,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 4,
          "stage1_key_finding": "Big Data und KI verkleinern die von Rolf beschriebene 'vorläufige Formalisierungslücke' erheblich, führen aber zu neuen Herausforderungen hinsichtlich vermeintlicher Objektivität, Bias-Reproduktion un",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Paper behandelt substantiell AI_Literacies (kritische Reflexion über KI-Kompetenzen von Fachkräften, Automation Bias), KI_Sonstige (Algorithmen, Big Data Analytics), Soziale_Arbeit (direkter Bezug zur sozialen Arbeit und professionellen Entscheidungsfindung) und Bias_Ungleichheit (Automation Bias). "
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Keyword-Inklusion",
          "divergence_justification": "Das LLM interpretiert 'Big Data Analytics und KI' als Generative_KI und dehnt 'Bias-Mitigation' zu einem eigenständigen Inklusionskriterium, ohne zu erkennen, dass das Paper primär ein technisches KI-Paper ohne Sozialer-Arbeit-Bezug ist. Die fehlende explizite Verbindung zum Anwendungsfeld Soziale Arbeit wird nicht als Ausschlusskriterium gewichtet.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "De-Professionalization Through Standardization",
        "Transparency and Objectivity in AI Systems",
        "Algorithmic Fairness and Decision Support Systems",
        "Data Literacy in Professional Practice",
        "Algorithmic Bias in Social Work",
        "Formalization Gap Theory",
        "Machine Learning on Unstructured Data",
        "Privacy Protection in Algorithmic Social Services"
      ],
      "knowledge_summary": "Big Data und KI verkleinern die von Rolf beschriebene 'vorläufige Formalisierungslücke' erheblich, führen aber zu neuen Herausforderungen hinsichtlich vermeintlicher Objektivität, Bias-Reproduktion und Ermessensspielraum-Einschränkung. Neue Anforderungen an Fachlichkeit entstehen durch erforderliche 'data literacy' und konsistenten Persönlichkeitsschutz."
    },
    {
      "id": "M2FYV58I",
      "stem": "Schneider_2022_Exploring_opportunities_and_risks_in_decision",
      "title": "Exploring opportunities and risks in decision support technologies for social workers: An empirical study in the field of disabled people's services",
      "author_year": "Schneider (2022)",
      "year": 2022,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "DSSs mit Visualisierungen der Klient*innen-Entwicklung werden als unterstützend wahrgenommen; es besteht Bedarf für partizipative Entscheidungsfindung; technische und professionelle Zuverlässigkeit dü",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Entscheidungsunterstützungssysteme/algorithmische Systeme) und SOZIAL_OK (Soziale_Arbeit: expliziter Fokus auf Sozialarbeiter und Behindertenservices; Bias_Ungleichheit: Reduktion subjektiver Vorurteile; Fairness: Transparenz in Ressourcenall"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Keyword-Inklusion",
          "divergence_justification": "Das LLM dehnt 'decision support technologies' zu Generative KI und Prompting aus, obwohl der Paper primär traditionelle Decision-Support-Systeme für Sozialarbeit behandelt. Der Human erkannte korrekt, dass die KI-Technologie sekundär ist und das Paper hauptsächlich im Feld der Sozialen Arbeit angesiedelt ist.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Decision Support Systems (DSS) in Social Work",
        "Algorithmic Fairness in Vulnerable Populations",
        "Participatory Decision-Making with Service Users",
        "Professional Judgment and AI Transparency",
        "Data Literacy in Professional Practice",
        "Technology Assessment in Social Care",
        "Algorithmic Bias in Professional Documentation"
      ],
      "knowledge_summary": "DSSs mit Visualisierungen der Klient*innen-Entwicklung werden als unterstützend wahrgenommen; es besteht Bedarf für partizipative Entscheidungsfindung; technische und professionelle Zuverlässigkeit dürfen nicht verwechselt werden."
    },
    {
      "id": "2WHGF83D",
      "stem": "Schneider_2024_AI_for_decision_support_What_are_possible",
      "title": "AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?",
      "author_year": "Schneider (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Die Erklärbarkeit von KI-Systemen ist zentral für deren legitime Verwendung, muss aber situativ ausgestaltet werden und darf professionelle Rollen nicht infrage stellen. Vertrauen wird durch funktiona",
          "stage3_completeness": 78,
          "stage3_correctness": 92,
          "stage3_overall": 86
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper behandelt algorithmische Entscheidungssysteme (KI_Sonstige: Ja) in sensiblen Bereichen (Healthcare, Justiz, Grenzschutz). Substantielle Behandlung von Diskriminierungsrisiken in Trainingsdaten (Bias_Ungleichheit: Ja) und Fairness-Aspekten durch Analyse von Opazität und Accountability (Fairness"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt die Kategorien 'Generative_KI', 'Prompting', 'Bias_Ungleichheit', 'Diversitaet' und 'Fairness' zu weit aus und interpretiert eine allgemeine Diskussionspapier über KI-Futures als spezialisierte technische Studie zu Bias-Mitigation durch Prompting-Strategien. Die Kategorien werden nicht nach dem feldspezifischen Standard des Reviews angewendet.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Explainability in AI Systems",
        "Algorithmic Fairness",
        "Technology Assessment",
        "Discrimination in Border Control Systems",
        "Algorithmic Bias in Decision Support",
        "Trust in AI Systems",
        "Automated Decision-Making Systems",
        "Stakeholder Perspectives on AI Governance"
      ],
      "knowledge_summary": "Die Erklärbarkeit von KI-Systemen ist zentral für deren legitime Verwendung, muss aber situativ ausgestaltet werden und darf professionelle Rollen nicht infrage stellen. Vertrauen wird durch funktionale Legitimation als Zweitmeinung aufgebaut, nicht primär durch technische Transparenz."
    },
    {
      "id": "LMW8DZ78",
      "stem": "Schneider_2025_Indecision_on_the_use_of_artificial_intelligence",
      "title": "Indecision on the use of artificial intelligence in healthcare: A qualitative study of patient perspectives on trust, responsibility and self-determination using AI-CDSS",
      "author_year": "Schneider (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Patienten zeigen erhebliche Unsicherheit gegenüber AI-CDSS-Implementierung. Die Wahrnehmung oszilliert zwischen supportivem Werkzeug und Zweitmeinung, wobei enge Verflechtungen zwischen Vertrauen, Ver",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 92
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper behandelt AI-CDSS im Healthcare-Kontext (KI_Sonstige: Ja), adressiert algorithmische Fairness und Bias-Aspekte (Bias_Ungleichheit, Fairness: Ja). JEDOCH: Kein direkter Bezug zu Sozialer Arbeit. Healthcare-Setting mit Patient:innen-Perspektiven ist nicht Sozialarbeit (keine Zielgruppen-Überschn"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Dehumanization Risk in Algorithmic Care",
        "Shared Decision-Making with AI Integration",
        "Trust in Human-AI Collaborative Care",
        "AI Literacy in Healthcare",
        "Responsibility Distribution in AI-CDSS Systems",
        "Stakeholder-Inclusive Algorithm Design",
        "Algorithmic Fairness in Clinical Decision Support",
        "Informed Patient Autonomy"
      ],
      "knowledge_summary": "Patienten zeigen erhebliche Unsicherheit gegenüber AI-CDSS-Implementierung. Die Wahrnehmung oszilliert zwischen supportivem Werkzeug und Zweitmeinung, wobei enge Verflechtungen zwischen Vertrauen, Verantwortung und Selbstbestimmung bestehen, die durch unzureichendes Verständnis der KI-Funktionalität gefährdet sind."
    },
    {
      "id": "R5QQTD95",
      "stem": "Schönauer_2025_Akzeptanz_von_KI_und_organisationale",
      "title": "Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen Arbeit – Eine empirische Untersuchung aus der Perspektive von Berufseinsteiger:innen",
      "author_year": "Schönauer (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Während digitale Technologien im administrativen Bereich positiv bewertet werden, zeigt sich bei KI eine kritischere Haltung: Nicht einmal ein Drittel der Befragten bewerten KI-Einsatz positiv. Die Ak",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 92
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch AI_Literacies (Fokus auf digitale Kompetenz und kritische KI-Literacy in Aus- und Weiterbildung) und KI_Sonstige (Organisationale KI-Rahmenbedingungen). SOZIAL_OK durch Soziale_Arbeit (direkter Bezug zu Berufseinsteiger:innen und Klientenarbeit in de"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "KI_Sonstige",
              "Soziale_Arbeit"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Participatory Technology Design",
        "Algorithmic Fairness in Social Services",
        "Technology Acceptance in Professional Practice",
        "Large Language Models in Human Services",
        "Organizational Change Management for Digital Implementation",
        "AI Literacy in Social Work",
        "Data Protection and Privacy in Social Work",
        "Relational Care and Human Empathy"
      ],
      "knowledge_summary": "Während digitale Technologien im administrativen Bereich positiv bewertet werden, zeigt sich bei KI eine kritischere Haltung: Nicht einmal ein Drittel der Befragten bewerten KI-Einsatz positiv. Die Akzeptanz hängt stark von digitalen Kompetenzen und praktischen Erfahrungen ab, wobei hauptsächlich Bedenken bezüglich Datenschutz und der Unersetzbarkeit menschlicher Empathie bestehen."
    },
    {
      "id": "64DQYVVB",
      "stem": "Shafie_2025_More_or_less_wrong_A_benchmark_for_directional",
      "title": "More or less wrong: A benchmark for directional bias in LLM comparative reasoning",
      "author_year": "Liu (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "LLMs zeigen konsistente und richtungsabhängige Reasoning-Verzerrungen, wobei die Wahl und Position von Vergleichsbegriffen zu systematischen Vorhersagen in Richtung des Framing-Terms führt, unabhängig",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper behandelt Bias und Fairness in LLMs durch ein Benchmark für direktionalen Bias. Dies erfüllt TECHNIK-Bedingung (Generative_KI, Bias_Ungleichheit, Fairness). JEDOCH: Keine Verbindung zu Soziale_Arbeit erkennbar. Das Paper fokussiert auf technische Bias-Metriken in LLMs, nicht auf sozialarbeiter"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Keyword-Inklusion",
          "divergence_justification": "Das LLM hat die Keywords 'Bias', 'Fairness' und 'LLM' erkannt und automatisch die Kategorien positiv markiert, ohne zu erkennen, dass der klinische Anwendungskontext außerhalb des Soziale-Arbeit-Fokus liegt. Der Mensch erkannte korrekt, dass die fachliche Domäne (Gesundheitswesen, nicht Soziale Arbeit) die Inklusion rechtfertigt, während das LLM nur oberflächliche Keyword-Matches identifizierte.",
          "disagreement_type": "Human_Include_Agent_Exclude",
          "severity": 3
        }
      },
      "concepts": [
        "Directional Reasoning Bias",
        "Chain-of-Thought Prompting Limitations",
        "Framing Effects in LLMs",
        "Demographic Fairness Disparities",
        "Grounded Reasoning Tasks",
        "LLM Bias Benchmarking",
        "Protected Attributes Interaction"
      ],
      "knowledge_summary": "LLMs zeigen konsistente und richtungsabhängige Reasoning-Verzerrungen, wobei die Wahl und Position von Vergleichsbegriffen zu systematischen Vorhersagen in Richtung des Framing-Terms führt, unabhängig von der korrekten Antwort. Diese Effekte werden durch demografische Identitätsmarker (Geschlecht, Rasse) verstärkt.",
      "featured": {
        "why": "Keyword-Inklusion: LLM findet Bias-Keywords, aber nicht den klinischen Kontext ausserhalb Sozialer Arbeit",
        "stance_highlight": "process"
      }
    },
    {
      "id": "9Y7ZFGI5",
      "stem": "Shah_2025_Gender_Bias_in_Artificial_Intelligence_Empowering",
      "title": "Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy",
      "author_year": "Shah (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Digitale Literaturprogramme sind ein vielversprechendes Instrument zur Bekämpfung von Geschlechtsbias in KI, indem sie kritisches Denken fördern, Frauen zum Verfolgen von KI-Karrieren ermutigen und fr",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: (1) TECHNIK: AI_Literacies (Digital Literacy Programme), KI_Sonstige (algorithmische Systeme in Recruitment/Healthcare/Finance); (2) SOZIAL: Bias_Ungleichheit (systematische Gender-Biases), Gender (expliziter Gender-Fokus), Diversitaet (Unterrepräsentation von Frauen"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "disagree",
          "divergence_pattern": "Keyword-Inklusion",
          "divergence_justification": "Das LLM dehnt Kategorien wie 'AI_Literacies', 'Bias_Ungleichheit' und 'Gender' über ihre Kernbedeutung hinaus aus, indem es allgemeine Konzepte von Geschlechtergerechtigkeit und digitaler Befähigung als spezifische Fachinhalte klassifiziert. Die Inklusion basiert auf thematischer Relevanz statt auf expliziter methodischer/technischer Zugehörigkeit zum definierten Forschungsfeld.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "AI Literacy",
        "Digital Literacy Programs",
        "Gender-Responsive Education Policy",
        "Gender Bias in AI Systems",
        "Women Underrepresentation in AI",
        "Inclusive AI Design"
      ],
      "knowledge_summary": "Digitale Literaturprogramme sind ein vielversprechendes Instrument zur Bekämpfung von Geschlechtsbias in KI, indem sie kritisches Denken fördern, Frauen zum Verfolgen von KI-Karrieren ermutigen und frauengeführte KI-Projekte katalysieren. Systemische Veränderungen in KI-Entwicklung, Design und Bildungspolitik sind jedoch notwendig."
    },
    {
      "id": "HLBXNWAZ",
      "stem": "Sharma_2024_Intersectional",
      "title": "Intersectional analysis of visual generative AI: the case of stable diffusion",
      "author_year": "Sharma (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Das Paper entwickelt einen theoretischen Rahmen, der zeigt, wie Nutzer:innen durch Cue-Routes (Wahrnehmung) und Action-Routes (Verhalten) unterschiedliche Workaround-Strategien entwickeln, um mit algo",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: (1) TECHNIK: Generative_KI=Ja (Stable Diffusion als visuelles generatives KI-System). (2) SOZIAL: Bias_Ungleichheit=Ja (Analyse von Sexismus, Rassismus, Heteronormativität, Ableismus), Gender=Ja (expliziter Fokus auf Geschlechterstereotype/masculine-presenting standa"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Bias Workarounds",
        "Intersectional Algorithmic Discrimination",
        "HAII-TIME Theory (Human-AI Interaction Theory of Interactive Media Effects)",
        "Technological Naivety",
        "Algorithmic Literacy",
        "Proxy Bias in Machine Learning",
        "Algorithmic Fairness Metrics"
      ],
      "knowledge_summary": "Das Paper entwickelt einen theoretischen Rahmen, der zeigt, wie Nutzer:innen durch Cue-Routes (Wahrnehmung) und Action-Routes (Verhalten) unterschiedliche Workaround-Strategien entwickeln, um mit algorithmischem Bias umzugehen. Die vier Kategorien von Bias führen zu verschiedenen Ergebnissen: von Empowerment bei erfolgreichen Workarounds bis zu technologischer Naivität bei unwahrgenommenen Bias."
    },
    {
      "id": "MJDTRLAI",
      "stem": "Shin_2024_Can_prompt_modifiers_control_bias_A_comparative",
      "title": "Can prompt modifiers control bias? A comparative analysis of text-to-image generative models",
      "author_year": "Shin (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": false,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Prompt-Modifizierer können Verzerrungen teilweise reduzieren, zeigen aber inkonsistente Ergebnisse. Prompt-Sequenzierung hat signifikante Auswirkungen, und mehrere Verzerrungen sind resistent gegen ei",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Das Paper erfüllt beide Bedingungen: (1) TECHNIK: Generative_KI (Text-to-Image Modelle), Prompting (Analyse von Prompt-Modifizierern zur Bias-Reduktion) → Ja. (2) SOZIAL: Bias_Ungleichheit (explizit: Analyse gesellschaftlicher Biases), Diversitaet (diversity-reflective prompting), Fairness (ethische"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Prompt Engineering for Bias Mitigation",
        "Prompt Sequencing Effects",
        "Gender Bias in Generative Models",
        "AI Fairness Metrics",
        "AI Literacy and Critical Reflection",
        "Algorithmic Bias Taxonomy",
        "Representation Bias in Text-to-Image Models"
      ],
      "knowledge_summary": "Prompt-Modifizierer können Verzerrungen teilweise reduzieren, zeigen aber inkonsistente Ergebnisse. Prompt-Sequenzierung hat signifikante Auswirkungen, und mehrere Verzerrungen sind resistent gegen einfache Modifier-basierte Interventionen."
    },
    {
      "id": "GJF776AY",
      "stem": "Shukla_2025_Investigating_AI_systems_examining_data_and",
      "title": "Investigating AI systems: examining data and algorithmic bias through hermeneutic reverse engineering",
      "author_year": "Shukla (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI-Systeme als Boundary Objects zu betrachten und durch hermeneutisches Reverse Engineering kritisch zu untersuchen ermöglicht es, versteckte Bias in Daten und Algorithmen aufzudecken und alternative,",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper behandelt Bias in AI-Systemen (KI_Sonstige: Ja) mittels hermeneutischer Reverse Engineering. Fokus auf algorithmischen Bias (Bias_Ungleichheit: Ja), Auswirkungen auf verschiedene soziale Gruppen (Diversitaet: Ja) und Fairness-Aspekte durch participatory design (Fairness: Ja). Beide Bedingungen"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Algorithmic Bias",
        "Hermeneutic Reverse Engineering",
        "Data Politics",
        "Technology as Political Object",
        "Gender Recognition Systems Bias",
        "Intersectional Feminism in Technology"
      ],
      "knowledge_summary": "AI-Systeme als Boundary Objects zu betrachten und durch hermeneutisches Reverse Engineering kritisch zu untersuchen ermöglicht es, versteckte Bias in Daten und Algorithmen aufzudecken und alternative, inklusivere Futures zu imaginieren. Bias ist kein Fehler sondern systemische Kodierung der dominanten sozialen Struktur."
    },
    {
      "id": "LT3D3ZQ2",
      "stem": "Siapka_2023_Towards_a_Feminist_Metaethics_of_AI",
      "title": "Towards a Feminist Metaethics of AI",
      "author_year": "Siapka (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Eine feministische Metaethik des AI sollte sich systematisch mit vier Dimensionen befassen: (i) der Kontinuität zwischen Theorie und Handlung in der AI-Ethik, (ii) den realen Effekten von AI-Ethik, (i",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Feministisch"
            ],
            "reasoning": "Das Paper entwickelt einen explizit feministischen theoretischen Rahmen (feminist metaethics) für KI-Ethik und untersucht Machtstrukturen, Kontexte und die Rolle von Akteur:innen. KI_Sonstige=Ja (KI-Ethik als Teilbereich), Feministisch=Ja (explizite feministische Metaethik-Perspektive), Bias_Ungleic"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Contextual and Narrative AI Ethics Methods",
        "Epistemic Injustice in AI Systems",
        "Data Colonialism",
        "Feminist Metaethics of AI",
        "Ethics Washing",
        "Intersectional AI Governance",
        "Algorithmic Fairness and Power Relations"
      ],
      "knowledge_summary": "Eine feministische Metaethik des AI sollte sich systematisch mit vier Dimensionen befassen: (i) der Kontinuität zwischen Theorie und Handlung in der AI-Ethik, (ii) den realen Effekten von AI-Ethik, (iii) der Rolle und dem Profil der in AI-Ethik involvierten Akteure, und (iv) den Auswirkungen von AI auf Machtbeziehungen durch kontextuelle, emotionale und narrative Methoden."
    },
    {
      "id": "3B87U5LN",
      "stem": "Siddals_2024_It_happened_to_be_the_perfect_thing_Experiences",
      "title": "\"It happened to be the perfect thing\": Experiences of generative AI chatbots for mental health",
      "author_year": "Siddals (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Generative KI-Chatbots werden von Nutzern als emotionaler Schutzraum erlebt und bieten bedeutungsvolle psychische Unterstützung mit Berichten über positive Auswirkungen einschließlich verbesserter Bez",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "Generative_KI",
              "Soziale_Arbeit"
            ],
            "reasoning": "Paper untersucht Nutzung von generativen KI-Chatbots (Pi, ChatGPT) für mentale Gesundheitsunterstützung. Generative_KI=Ja (LLM-basierte Chatbots im Fokus). Soziale_Arbeit=Ja (mentale Gesundheit, therapeutische Kontexte, Unterstützung von vulnerable Personen - Kernbereich Sozialer Arbeit). Beide TECH"
          },
          "human": {
            "decision": "Unclear",
            "categories": [
              "Generative_KI",
              "Soziale_Arbeit",
              "Bias_Ungleichheit"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt die Kategorien Gender, Diversität und Fairness über ihre operationale Definition aus, indem es generische Erwähnungen von psychischer Gesundheit und Nutzererfahrungen als Auslöser interpretiert, obwohl der Paper nicht explizit diese Dimensionen als analytischen Fokus behandelt. Das Human-Assessment erkennt korrekt, dass die Soziale Arbeit-Dimension fehlt, während das LLM diese übersieht.",
          "disagreement_type": "Human_Unclear",
          "severity": 1
        }
      },
      "concepts": [
        "Therapeutic Safe Space",
        "AI Safety Guardrails in Mental Health",
        "Contextual Design of AI Safety",
        "Algorithmic Hallucination and Transparency",
        "Conversational Memory in Therapeutic AI",
        "Generative AI Chatbots for Mental Health",
        "Digital Mental Health Accessibility Gap"
      ],
      "knowledge_summary": "Generative KI-Chatbots werden von Nutzern als emotionaler Schutzraum erlebt und bieten bedeutungsvolle psychische Unterstützung mit Berichten über positive Auswirkungen einschließlich verbesserter Beziehungen und Heilung von Trauma, erfordern aber bessere Sicherheitsvorkehrungen und menschenähnliches Gedächtnis."
    },
    {
      "id": "DA6T4Z5B",
      "stem": "Sinders_2017_Feminist_Data_Set",
      "title": "Feminist Data Set",
      "author_year": "Sinders (2017)",
      "year": 2017,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 5,
          "stage1_key_finding": "Ein kritisches Kunstprojekt demonstriert, dass feministische und intersektionale Perspektiven auf jeden Schritt der maschinellen Lernentwicklung angewendet werden können – von der gemeinschaftlichen D",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 90
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK-Seite mit AI_Literacies (kritische Kompetenzentwicklung), Generative_KI (Chatbot-Design), Prompting (explizit critical prompting practices), und KI_Sonstige (Datensammlung, Training, Algorithmenauswahl). SOZIAL-Seite mit starkem Feministisch-Fokus (expliz"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt die Kategorie 'KI_Sonstige' und 'Bias_Ungleichheit' zu weit aus, indem es einen Paper über feministische Datenethik als technisches KI-Paper klassifiziert, obwohl die Human-Entscheidung (Exclude) signalisiert, dass der Paper die spezifischen technischen/sozialen Arbeit-Anforderungen des Feldes nicht erfüllt. Das LLM überkategorisiert basierend auf thematischer Relevanz statt echter Feldintegration.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Fair Labor Practices in Data Annotation",
        "Critical Design and Technological Democratization",
        "Gender Bias in AI Systems",
        "Algorithmic Bias",
        "Community-Based Data Governance",
        "Data Feminism",
        "Intersectional Feminism in AI Development"
      ],
      "knowledge_summary": "Ein kritisches Kunstprojekt demonstriert, dass feministische und intersektionale Perspektiven auf jeden Schritt der maschinellen Lernentwicklung angewendet werden können – von der gemeinschaftlichen Datenerfassung über ethische Arbeitsmodelle bis zur Gestaltung von Benutzeroberflächen – um Bias abzubauen und gerechtere KI-Systeme zu schaffen."
    },
    {
      "id": "VICS443I",
      "stem": "Singer_2023_AI_Creates_the_Message_Integrating_AI_Language",
      "title": "AI Creates the Message: Integrating AI Language Learning Models into Social Work Education and Practice",
      "author_year": "Singer (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Sozialarbeitende haben eine ethische Verantwortung, sich mit KI-Technologien auseinanderzusetzen und diese in Lehre und Praxis zu integrieren, während gleichzeitig Bias, Datenschutz und technologische",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Soziale_Arbeit",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: (1) TECHNIK: Generative_KI (LLMs in Social Work) + AI_Literacies (Pädagogische Integration, Kompetenzentwicklung). (2) SOZIAL: Soziale_Arbeit (direkter Bezug zu Lehre und Praxis Sozialer Arbeit) + Bias_Ungleichheit (Warnung vor Bias, faktischen Fehlern). Substantiell"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "Generative_KI",
              "KI_Sonstige",
              "Soziale_Arbeit"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt die Kategorie 'Prompting' über ihre operative Technik-Dimension hinaus aus und interpretiert theoretische Diskussionen von KI-Biases als 'Interventionsstrategie', ohne dass Prompting als primäres methodisches Instrument im Paper belegt ist. Dies führt zu einer Überinklusivität durch Bedeutungserweiterung statt zu echter Keyword-Erkennung.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Technology-Mediated Social Work Education",
        "Large Language Models (LLMs) for Social Work Practice",
        "Technological Justice in Social Work",
        "Algorithmic Bias in Social Work",
        "Natural Language Processing (NLP)",
        "AI Ethics Curriculum Integration",
        "Prompt Engineering for Social Work"
      ],
      "knowledge_summary": "Sozialarbeitende haben eine ethische Verantwortung, sich mit KI-Technologien auseinanderzusetzen und diese in Lehre und Praxis zu integrieren, während gleichzeitig Bias, Datenschutz und technologische Gerechtigkeit kritisch reflektiert werden müssen."
    },
    {
      "id": "BDBYDLVK",
      "stem": "Singh_2025_A_reparative_turn_in_AI",
      "title": "A reparative turn in AI",
      "author_year": "Singh (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Reparationsbemühungen konzentrieren sich stark auf symbolische, frühe Phasen (Ankündigung, Attribution), während Maßnahmen zur Rechenschaftspflicht und Systemreform deutlich seltener sind. Nur 10,75% ",
          "stage3_completeness": 88,
          "stage3_correctness": 93,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper behandelt KI-Governance und -Harms substantiell (KI_Sonstige: Ja). Adressiert algorithmische Diskriminierung und Schadensanalyse (Bias_Ungleichheit: Ja) sowie Accountability und Fairness in KI-Systemen (Fairness: Ja). Erfüllt beide Bedingungen: TECHNIK (KI_Sonstige) UND SOZIAL (Bias_Ungleichhe"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "AI Accountability Gap",
        "Reparative Justice in AI",
        "Corporate Perception Management",
        "Harm Documentation and Taxonomy",
        "Transformative Justice Framework",
        "Community-Centered Reparation",
        "Algorithmic Discrimination"
      ],
      "knowledge_summary": "Reparationsbemühungen konzentrieren sich stark auf symbolische, frühe Phasen (Ankündigung, Attribution), während Maßnahmen zur Rechenschaftspflicht und Systemreform deutlich seltener sind. Nur 10,75% der Fälle zeigen Strafen und 6,13% Kompensationen, was auf ein erhebliches Verantwortungsdefizit hindeutet."
    },
    {
      "id": "BDBYDLVK",
      "stem": "Singh_2025_reparative",
      "title": "A reparative turn in AI",
      "author_year": "Singh (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Reparative Maßnahmen konzentrieren sich auf frühe, symbolische Phasen wie öffentliche Stellungnahmen und Audits, während substantielle Maßnahmen wie Kompensation, Systemreformen und strukturelle Verän",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper behandelt KI-Governance und -Harms substantiell (KI_Sonstige: Ja). Adressiert algorithmische Diskriminierung und Schadensanalyse (Bias_Ungleichheit: Ja) sowie Accountability und Fairness in KI-Systemen (Fairness: Ja). Erfüllt beide Bedingungen: TECHNIK (KI_Sonstige) UND SOZIAL (Bias_Ungleichhe"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Harm Documentation",
        "Reparative Justice in AI",
        "Affected Community Participation",
        "Disparate Impact in AI Systems",
        "AI Governance Frameworks",
        "Algorithmic Accountability",
        "Transformative Justice Framework"
      ],
      "knowledge_summary": "Reparative Maßnahmen konzentrieren sich auf frühe, symbolische Phasen wie öffentliche Stellungnahmen und Audits, während substantielle Maßnahmen wie Kompensation, Systemreformen und strukturelle Veränderungen deutlich unterrepräsentiert sind. Die Analyse offenbart ein grundlegendes Accountability-Defizit."
    },
    {
      "id": "6L85PRUW",
      "stem": "Skilton_2024_Inclusive_prompt_engineering_A_methodology_for",
      "title": "Inclusive prompt engineering: A methodology for hacking biased AI image generation",
      "author_year": "Skilton (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": false,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "DALL-E betreibt sogenannte 'Toxic Positivity', die durch Content-Filter verursacht wird und dazu führt, dass der KI-Generator bei der Darstellung menschlicher Subjekte standardmäßig zu jungen, dünnen,",
          "stage3_completeness": 92,
          "stage3_correctness": 96,
          "stage3_overall": 93
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper behandelt substantiell Prompting-Strategien zur Bias-Mitigation in generativen KI-Systemen (Bildgeneratoren). TECHNIK erfüllt: AI_Literacies (Kompetenzentwicklung für inklusives Prompting), Generative_KI (Bildgeneratoren), Prompting (Prompt-Engineering als Kernmethodologie). SOZIAL erfüllt: Bi"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Inclusive Prompt Engineering",
        "Iterative Prompt Refinement",
        "AI Literacy for Content Designers",
        "Generative AI Content Moderation",
        "Visual Representation and Social Justice",
        "Toxic Positivity in AI Systems",
        "Algorithmic Representation Bias"
      ],
      "knowledge_summary": "DALL-E betreibt sogenannte 'Toxic Positivity', die durch Content-Filter verursacht wird und dazu führt, dass der KI-Generator bei der Darstellung menschlicher Subjekte standardmäßig zu jungen, dünnen, schönen und weißen Menschen greift. Durch iteratives Prompt Engineering mit positiver Umformulierung von sensiblen Begriffen können Nutzer diese Verzerrungen umgehen und vielfältigere Darstellungen e"
    },
    {
      "id": "VVEEL68I",
      "stem": "Slesinger_2024_Training_in_Co-Creation_as_a_Methodological",
      "title": "Training in Co-Creation as a Methodological Approach to Improve AI Fairness",
      "author_year": "Slesinger (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Training in Co-Creation Prozessen kann technische Barrieren reduzieren und nicht-technische Stakeholder befähigen, informiert über AI-Bias zu diskutieren, erfordert aber sorgfältige Kalibrierung des I",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 90
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies: Training zu KI-Konzepten; KI_Sonstige: AI Bias Detection/Mitigation Tools; Fairness: Algorithmen-Fairness) + SOZIAL (Bias_Ungleichheit: Fokus auf Bias Detection; Diversitaet: vulnerable/marginalized stakeholder groups, inklusive AI-Entwicklung"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Participatory Design for AI Fairness",
        "Ethics-Washing in AI Governance",
        "Intersectional Bias in AI",
        "Algorithmic Bias in Socio-Technical Systems",
        "Multi-Modal Bias Detection in AI",
        "Vulnerability-Centered AI Design",
        "AI Literacy Through Co-Creation Training",
        "Gender Bias in Generative AI Systems"
      ],
      "knowledge_summary": "Training in Co-Creation Prozessen kann technische Barrieren reduzieren und nicht-technische Stakeholder befähigen, informiert über AI-Bias zu diskutieren, erfordert aber sorgfältige Kalibrierung des Inhalts und kritische Reflexion zur Vermeidung von instrumentalisierendem 'Ethics-Washing'."
    },
    {
      "id": "7G73H3KM",
      "stem": "Small_2023_Generative_AI_and_opportunities_for_feminist",
      "title": "Generative AI and opportunities for feminist classroom assignments",
      "author_year": "Small (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": false,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Generative AI kann für transformative feministische Pädagogik genutzt werden, indem Lehrende Studierende zur kritischen Auseinandersetzung mit Wissensentstehung, Epistemologie und Community-Engagement",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 93
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Gender",
              "Feministisch"
            ],
            "reasoning": "Paper behandelt Integration generativer KI in Bildung (AI_Literacies: Ja, reflexive Kompetenzentwicklung). Fokus auf generative AI-Tools (Ja). Explizit feministische Perspektive und Epistemologie (Feministisch: Ja). Gender-Fokus durch feministische Theorieentwicklung (Gender: Ja). TECHNIK erfüllt (A"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "AI Literacy",
        "Epistemology and Standpoint Theory",
        "Algorithmic Bias",
        "Feminist Pedagogy",
        "Community Engagement in Education",
        "Critical Citation Practices",
        "Surveillance in Educational Technology",
        "Gender Bias in AI"
      ],
      "knowledge_summary": "Generative AI kann für transformative feministische Pädagogik genutzt werden, indem Lehrende Studierende zur kritischen Auseinandersetzung mit Wissensentstehung, Epistemologie und Community-Engagement ermutigen, statt Überwachung zu verstärken."
    },
    {
      "id": "7U29SIC8",
      "stem": "Sperling_2024_In_search_of_artificial_intelligence_(AI)",
      "title": "In search of artificial intelligence (AI) literacy in teacher education: A scoping review",
      "author_year": "Sperling (06/2024)",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI literacy ist ein global emergierendes Forschungsthema, aber kaum in Teacher Education verankert. Die Literatur ist computer-science-zentriert und konzeptualisiert AI literacy implizit und überwiege",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Das Paper behandelt AI Literacy explizit, aber ausschließlich im Kontext von Lehrerausbildung (Teacher Education). Es fehlt jeder Bezug zu Sozialer Arbeit, Bias, Fairness, Gender oder Diversität. Die SOZIAL-Kategorie ist nicht erfüllt: Weder Soziale_Arbeit noch eine der Ungleichheits-/Fairness-Kateg"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Algorithmic Bias in Educational Assessment",
        "Professional Knowledge Framework (Episteme-Techne-Phronesis)",
        "AI Literacy",
        "Teacher Education Gap in AI Integration",
        "Ethical AI in Education",
        "Computer Science-Centric Epistemology in AI Education",
        "Contextual Diversity in AI Literacy Design"
      ],
      "knowledge_summary": "AI literacy ist ein global emergierendes Forschungsthema, aber kaum in Teacher Education verankert. Die Literatur ist computer-science-zentriert und konzeptualisiert AI literacy implizit und überwiegend durch technische und praktische Dimensionen, während ethische und kontextgebundene professionelle Urteile unterrepräsentiert sind."
    },
    {
      "id": "7AS5MAU9",
      "stem": "Srinivasan_2025_Mitigating_trust-induced_inappropriate_reliance",
      "title": "Mitigating trust-induced inappropriate reliance on AI assistance",
      "author_year": "Srinivasan (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Vertrauensadaptive Interventionen – unterstützende Erklärungen bei niedrigem Vertrauen und Gegenargumente bei hohem Vertrauen – reduzieren unangemessene Abhängigkeit um bis zu 38% und verbessern die E",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Fairness"
            ],
            "reasoning": "Paper behandelt Trust-Management in AI-Systemen und Fairness durch adaptive Interventionen (TECHNIK: AI_Literacies, KI_Sonstige, Fairness erfüllt). Es befasst sich jedoch nicht mit sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder feministischen Perspektiven (SOZIAL nicht erfüllt). Fokus l"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "AI Literacy",
        "Trust-Adaptive Interventions",
        "Appropriate Reliance",
        "Algorithmic Decision Support",
        "Human-AI Collaboration",
        "Explainable AI (XAI)",
        "Trust Calibration"
      ],
      "knowledge_summary": "Vertrauensadaptive Interventionen – unterstützende Erklärungen bei niedrigem Vertrauen und Gegenargumente bei hohem Vertrauen – reduzieren unangemessene Abhängigkeit um bis zu 38% und verbessern die Entscheidungsgenauigkeit um 20%."
    },
    {
      "id": "N7J2ZRFP",
      "stem": "Srivastava_2024_Algorithmic_Governance_and_the_International",
      "title": "Algorithmic Governance and the International Politics of Big Tech",
      "author_year": "Srivastava (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Algorithmic governance by Big Tech creates unprecedented forms of private authority that challenge existing IR frameworks of legitimacy, and requires new conceptualizations of corporate international ",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Das Paper behandelt algorithmische Systeme (KI_Sonstige: Ja) und Machtungleichgewichte durch Big Tech (Bias_Ungleichheit: Ja). Allerdings fehlt ein direkter Bezug zu Sozialer Arbeit oder deren Praxis/Zielgruppen. Die Analyse fokussiert auf politische Wirtschaft und staatliche Souveränität, nicht auf"
          },
          "human": {
            "decision": "Unclear",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt die Kategorien 'Gender' und 'Feministisch' auf eine allgemeine kritische Analyse von Machtverhältnissen aus, ohne dass diese Perspektiven explizit im Titel oder als Kern des Papers erkennbar sind. Die Kategorien werden als intersektionale Kritik interpretiert, statt sie auf Papers mit explizitem Gender- oder feministischem Fokus zu beschränken.",
          "disagreement_type": "Human_Unclear",
          "severity": 1
        }
      },
      "concepts": [
        "State-Corporate Relations",
        "Algorithmic Opacity",
        "Private Authority",
        "Algorithmic Legitimacy Crisis",
        "Algorithmic Governance",
        "Corporate Accountability Mechanisms",
        "Algorithmic Discrimination"
      ],
      "knowledge_summary": "Algorithmic governance by Big Tech creates unprecedented forms of private authority that challenge existing IR frameworks of legitimacy, and requires new conceptualizations of corporate international responsibility that account for opacity, scale, and the complex oscillation between state-corporate collaboration and contestation."
    },
    {
      "id": "XG7RFFC7",
      "stem": "Statistics_2023_Occupational",
      "title": "Occupational employment statistics",
      "author_year": "Statistics",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": false,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": false
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Produktionsberufe beschäftigen 8,7 Millionen Menschen (5,7% der Gesamtbeschäftigung) mit Durchschnittslöhnen von 50.090 USD, deutlich unter dem nationalen Durchschnitt von 67.920 USD. Gebäudereinigung",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [],
            "reasoning": "Dies ist ein Statistikbericht des U.S. Bureau of Labor Statistics über Beschäftigungsstatistiken nach Beruf. Es behandelt weder KI-Themen (TECHNIK-Kriterien nicht erfüllt) noch soziale Gerechtigkeit/Bias-Probleme im KI-Kontext (SOZIAL-Kriterien nicht erfüllt). Kein Bezug zu KI-Systemen oder deren Au"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Care Work and Service Sector Precarity",
        "Geographic Employment Concentration",
        "Occupational Wage Inequality",
        "Structural Labor Market Inequality",
        "Occupational Employment Statistics Survey (OEWS)",
        "Labor Market Segmentation"
      ],
      "knowledge_summary": "Produktionsberufe beschäftigen 8,7 Millionen Menschen (5,7% der Gesamtbeschäftigung) mit Durchschnittslöhnen von 50.090 USD, deutlich unter dem nationalen Durchschnitt von 67.920 USD. Gebäudereinigungs- und Instandhaltungsberufe mit 4,5 Millionen Beschäftigten haben noch niedrigere Durchschnittslöhne von 39.540 USD."
    },
    {
      "id": "4JN7NIS4",
      "stem": "Steiner_2022_Künstliche_Intelligenz_in_der_Sozialen_Arbeit",
      "title": "Künstliche Intelligenz in der Sozialen Arbeit: Grundlagen, Entwicklungen, Herausforderungen",
      "author_year": "Steiner (2022)",
      "year": 2022,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 4,
          "stage1_key_finding": "Während KI-Technologien wie Predictive Risk Modeling und Chatbots vielversprechende Potenziale für die Soziale Arbeit bieten, bestehen erhebliche Risiken durch Algorithmen-Bias, Datenschutzproblematik",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "Generative_KI",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Das Paper behandelt KI-Anwendungen in der Sozialen Arbeit (Chatbots, Predictive Risk Modeling) und analysiert algorithmischen Bias sowie ethische Fragen. Es erfüllt beide Anforderungen: TECHNIK (Generative_KI für Chatbots, KI_Sonstige für PRM und neuronale Netze) und SOZIAL (Soziale_Arbeit als Fokus"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit"
            ]
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Neoliberal Instrumentalization of AI",
        "Black-Box Problem in AI Systems",
        "Algorithmic Bias in Social Services",
        "Relationship-Centered Social Work Practice",
        "Predictive Risk Modeling",
        "Automated Social Exclusion",
        "AI Literacy in Social Work"
      ],
      "knowledge_summary": "Während KI-Technologien wie Predictive Risk Modeling und Chatbots vielversprechende Potenziale für die Soziale Arbeit bieten, bestehen erhebliche Risiken durch Algorithmen-Bias, Datenschutzproblematiken und die Gefahr der Automatisierung sozialer Exklusion, insbesondere wenn KI zu Kosteneinsparungen statt professioneller Unterstützung eingesetzt wird."
    },
    {
      "id": "KRUQB7L2",
      "stem": "Steyvers_2025_What_large_language_models_know_and_what_people",
      "title": "What large language models know and what people think they know",
      "author_year": "Steyvers (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": false,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Nutzer überschätzen systematisch die Genauigkeit von LLM-Ausgaben mit Standarderklärungen; längere Erklärungen erhöhen das Vertrauen ohne die Antwortgenauigkeit zu verbessern. Durch Anpassung der Erkl",
          "stage3_completeness": 92,
          "stage3_correctness": 96,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "Fairness"
            ],
            "reasoning": "Das Paper behandelt AI Literacy (Benutzervertrauen, Kalibrierung), Generative KI (LLMs) und Prompting-Strategien (Unsicherheitskonveyance). Fairness-relevant bezueglich Transparenz. Jedoch kein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversitaet oder feministischer Perspektive."
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "AI Literacy in Uncertainty Interpretation",
        "Prompt Engineering for Confidence Alignment",
        "Discrimination Gap",
        "Length Bias in AI Trust",
        "Human-AI Alignment in Decision-Making",
        "Calibration Gap",
        "Uncertainty Communication in LLMs",
        "Internal Model Confidence"
      ],
      "knowledge_summary": "Nutzer überschätzen systematisch die Genauigkeit von LLM-Ausgaben mit Standarderklärungen; längere Erklärungen erhöhen das Vertrauen ohne die Antwortgenauigkeit zu verbessern. Durch Anpassung der Erklärungen an die interne Modellkonfidenz können Kalibrierungs- und Diskriminierungslücken erheblich reduziert werden."
    },
    {
      "id": "QVNJIQJG",
      "stem": "Strauß_2024_CAIL_–_Critical_AI_Literacy_Kritische",
      "title": "CAIL – Critical AI Literacy: Kritische Technikkompetenz für konstruktiven Umgang mit KI-basierter Technologie in Betrieben",
      "author_year": "Strauß (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "KI-basierte Automatisierung unterscheidet sich durch höhere Dynamik, Volatilität und Intransparenz von klassischen Automatisierungsformen. Das Risiko von Deep Automation Bias stellt ein Meta-Risiko da",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Das Paper behandelt Critical AI Literacy und Automation Bias (TECHNIK: AI_Literacies + KI_Sonstige erfüllt). Der Kontext ist jedoch betrieblich/wirtschaftlich orientiert, nicht sozialarbeiterisch. Bias_Ungleichheit ist nur marginal via 'automation bias' berührt, nicht substantiell. SOZIAL-Kriterien "
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "AI_Literacies"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Deep Automation Bias",
        "Digital Divide in AI Adoption",
        "Adaptive Automation Systems",
        "Algorithmic Fairness in Workplace Contexts",
        "Technology Literacy Assessment",
        "Socio-technical System Design",
        "Critical AI Literacy"
      ],
      "knowledge_summary": "KI-basierte Automatisierung unterscheidet sich durch höhere Dynamik, Volatilität und Intransparenz von klassischen Automatisierungsformen. Das Risiko von Deep Automation Bias stellt ein Meta-Risiko dar, das durch kritische KI-Kompetenzen und bewusstes, zweckorientiertes System-Design reduziert werden kann."
    },
    {
      "id": "LXKXPD5H",
      "stem": "Studeny_2025_Digitale_Werkzeuge_und_Machtasymmetrien",
      "title": "Digitale Werkzeuge und Machtasymmetrien?",
      "author_year": "Studeny (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 5,
          "stage1_key_finding": "Digitale Systeme sind nicht neutral und verstärken bestehende Machvasymmetrien durch unsichtbare Kontrolle, algorithmische Diskriminierung und Ausgrenzung, während sie gleichzeitig professionelle Auto",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Einschlusskriterien: TECHNIK_OK (KI_Sonstige: Algorithmen in digitaler Sozialer Arbeit; AI_Literacies: kritische Reflexion digitaler Technologien) und SOZIAL_OK (Soziale_Arbeit: direkter Bezug zu sozialarbeiterischer Praxis; Bias_Ungleichheit: Diskriminierung durch biased data; F"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Digital Power Asymmetries",
        "Technology-Mediated Power in Social Work",
        "Digital Nudging",
        "Digital Divide",
        "Digital Sovereignty",
        "Algorithmic Transparency",
        "Algorithmic Discrimination"
      ],
      "knowledge_summary": "Digitale Systeme sind nicht neutral und verstärken bestehende Machvasymmetrien durch unsichtbare Kontrolle, algorithmische Diskriminierung und Ausgrenzung, während sie gleichzeitig professionelle Autonomie und Klient:innen-Selbstbestimmung untergraben. Soziale Arbeit muss diese Dynamiken kritisch reflektieren und sich politisch für digitale Gerechtigkeit einsetzen."
    },
    {
      "id": "H7E3N6VR",
      "stem": "Sūna_2024_Diskriminierung_durch_Algorithmen_–_Überlegungen",
      "title": "Diskriminierung durch Algorithmen – Überlegungen zur Stärkung KI-bezogener Kompetenzen",
      "author_year": "Sūna (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "KI-bezogene Kompetenzen müssen drei Dimensionen umfassen (kognitiv, affektiv, handlungsbezogen), um algorithmen-basierte Diskriminierung zu erkennen und zu adressieren. Es besteht ein erhebliches Komp",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Das Paper behandelt algorithmische Diskriminierung (KI_Sonstige: Ja) und kritische KI-Kompetenzen (AI_Literacies: Ja). Es adressiert substantiell Bias, Ungleichheit (Bias_Ungleichheit: Ja), digitale Teilhabe/Inklusion (Diversitaet: Ja) und implizit Fairness durch Fokus auf Benachteiligungen. TECHNIK"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Sample Bias",
        "Digital Resignation",
        "Automation Bias",
        "AI Literacy",
        "Intersectional Algorithmic Fairness",
        "Proxy Variables",
        "Algorithmic Discrimination",
        "Media-Pedagogical Intervention"
      ],
      "knowledge_summary": "KI-bezogene Kompetenzen müssen drei Dimensionen umfassen (kognitiv, affektiv, handlungsbezogen), um algorithmen-basierte Diskriminierung zu erkennen und zu adressieren. Es besteht ein erhebliches Kompetenzdefizit und eine 'digitale Resignation' bei betroffenen Gruppen."
    },
    {
      "id": "T9KEZN3G",
      "stem": "Taeihagh_2025_Governance_of_generative_AI_A_comprehensive",
      "title": "Governance of generative AI: A comprehensive framework for navigating challenges and opportunities",
      "author_year": "Taeihagh (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Traditionelle IT-Governance-Modelle und Regulierungsrahmen sind unzureichend für generative KI; es bedarf innovativer, inklusiver und komplexitätsbasierter Governance-Ansätze, die Stakeholder-Partizip",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Das Paper behandelt substantiell Generative KI (Kategorie erfüllt) und adressiert Bias-Amplifikation, Machtungleichgewichte und Fairness-Fragen (Bias_Ungleichheit und Fairness erfüllt). Es erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja) und SOZIAL_OK (Bias_Ungleichheit=Ja, Fairness=Ja). Kei"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Intellectual Property Rights Reform for Generative AI",
        "Tech Corporate Power Concentration",
        "AI Literacy for Policymakers",
        "Generative AI Governance",
        "Participatory AI Governance",
        "Algorithmic Bias Amplification",
        "Generative AI Risk Taxonomy"
      ],
      "knowledge_summary": "Traditionelle IT-Governance-Modelle und Regulierungsrahmen sind unzureichend für generative KI; es bedarf innovativer, inklusiver und komplexitätsbasierter Governance-Ansätze, die Stakeholder-Partizipation, Datenschutz, IPR, Bias-Reduktion und internationale Kooperation adressieren."
    },
    {
      "id": "GFXYER4F",
      "stem": "Takaoka_2022_AI_implementation_science_for_social_issues",
      "title": "AI implementation science for social issues: Pitfalls and tips",
      "author_year": "Takaoka (2022)",
      "year": 2022,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 5,
          "stage1_key_finding": "Soziale Implementierung von KI erfordert nicht nur technische Lösungen, sondern umfassende Systemveränderungen in Dateninfrastruktur, Organisationskultur und stakeholder-engagement. Der entwickelte SS",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK: KI_Sonstige (Machine Learning, Gradient Boosting für Prediction) + AI_Literacies (Training von Fachkräften, eXplainable AI für Transparenz). SOZIAL: Soziale_Arbeit (Jugendhilfe/Child Guidance Centers, direkte sozialarbeiterische Praxis und Zielgruppen). Impl"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt die Kategorien AI_Literacies, Bias_Ungleichheit, Gender, Diversität und Feministisch über ihren intendierten Scope aus, indem es ein generisches Policy-Paper zu AI-Governance als spezifische soziale Interventionen klassifiziert. Der Mensch erkennt dagegen, dass das Paper primär Soziale Arbeit ist und nicht die spezifischen technologie- und genderorientierten Kategorien erfüllt.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Data Quality Design for Social Systems",
        "Algorithmic Fairness in Social Services",
        "AI Implementation Science",
        "Human-in-the-Loop Decision Support",
        "Explainable AI (XAI)",
        "Sustainable Service Team as R&D (SSTRD)",
        "Vulnerable Population Protection in AI",
        "Organizational Change Management for AI"
      ],
      "knowledge_summary": "Soziale Implementierung von KI erfordert nicht nur technische Lösungen, sondern umfassende Systemveränderungen in Dateninfrastruktur, Organisationskultur und stakeholder-engagement. Der entwickelte SSTRD-Modell (Sustainable Service Team as R&D) zeigt, dass KI-Implementierung im Sozialbereich nur durch langfristige Zusammenarbeit von Wissenschaft, Industrie und Behörden erfolgreich ist."
    },
    {
      "id": "Tang_2024_GenderCARE_A_Comprehensive_Framework_for",
      "stem": "Tang_2024_GenderCARE_A_Comprehensive_Framework_for",
      "title": "Tang_2024_GenderCARE_A_Comprehensive_Framework_for",
      "author_year": "Tang",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": false
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Das GenderCARE Framework reduziert Gender-Bias in LLMs um bis zu 90% (durchschnittlich über 35% bei 17 LLMs), während die Leistung in Standard-NLP-Tasks unter 2% variabel bleibt. Das Framework etablie",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {}
      },
      "concepts": [
        "Algorithmic Fairness",
        "Low-Rank Adaptation Fine-Tuning",
        "Gender Bias in Large Language Models",
        "Benchmark Design for Inclusive AI Assessment",
        "Toxicity and Regard Metrics",
        "Counterfactual Data Augmentation",
        "Digital Discrimination Against Marginalized Communities"
      ],
      "knowledge_summary": "Das GenderCARE Framework reduziert Gender-Bias in LLMs um bis zu 90% (durchschnittlich über 35% bei 17 LLMs), während die Leistung in Standard-NLP-Tasks unter 2% variabel bleibt. Das Framework etabliert erstmals explizite Kriterien für Gender-Gleichheit (Inclusivity, Diversity, Explainability, Objectivity, Robustness, Realisticity) und integriert bisher übersehene Gruppen wie Transgender und Non-B"
    },
    {
      "id": "43CRXRRT",
      "stem": "Tinmaz_2022_A_systematic_review_on_digital_literacy",
      "title": "A systematic review on digital literacy",
      "author_year": "Tinmaz (12/2022)",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Die Analyse identifizierte vier Hauptthemen: Digital Literacy, Digital Competencies, Digital Skills und Digital Thinking. Der Forschungstrend zeigt steigende Publikationszahlen ab 2013, mit dominantem",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 87
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [],
            "reasoning": "Das Paper behandelt digitale Literalität allgemein, NICHT AI Literacy im Sinne von Kompetenzen im Umgang mit KI-Systemen. Der Fokus liegt auf digital competencies, digital skills und digital thinking ohne spezifischen Bezug zu KI, generativen Modellen oder algorithmischen Systemen. Weder Technologie"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Digital Inequalities",
        "Digital Competencies",
        "Industry 4.0 Digital Skills",
        "Digital Divide",
        "Digital Literacy",
        "Vulnerable Population Digital Equity",
        "Critical Digital Thinking"
      ],
      "knowledge_summary": "Die Analyse identifizierte vier Hauptthemen: Digital Literacy, Digital Competencies, Digital Skills und Digital Thinking. Der Forschungstrend zeigt steigende Publikationszahlen ab 2013, mit dominantem qualitativem Forschungsansatz."
    },
    {
      "id": "WLV8L8PM",
      "stem": "Tint_2025_Guardrails,_not_guidance_Understanding_responses",
      "title": "Guardrails, not guidance: Understanding responses to LGBTQ+ language in large language models",
      "author_year": "Tint (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": false,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Safety-Mechanismen neutralisieren offene heteronormative Bias durch neutrale/korrektive Antworten, versagen aber bei systemischen Verzerrungen gegenüber LGBTQ+-Slang, das überproportional negative emo",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper untersucht substantiell LLM-Responses auf LGBTQ+-Prompts (Generative_KI: Ja; Prompting: Ja - systematische Prompt-Variation). Zeigt algorithmischen Bias gegen queer/marginalisierte Communities (Bias_Ungleichheit: Ja), thematisiert Gender/sexuelle Orientierung (Gender: Ja; Diversitaet: Ja - LGB"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Emotional Content Classification in NLP",
        "Embedding-Based Cluster Analysis with Mahalanobis Distance",
        "Inclusive NLP Systems",
        "Algorithmic Bias in Language Models",
        "Queerphobic Bias Detection",
        "LGBTQ+ Linguistic Representation",
        "Safety Mechanisms and Fairness Trade-offs"
      ],
      "knowledge_summary": "Safety-Mechanismen neutralisieren offene heteronormative Bias durch neutrale/korrektive Antworten, versagen aber bei systemischen Verzerrungen gegenüber LGBTQ+-Slang, das überproportional negative emotionale Labels auslöst."
    },
    {
      "id": "IGFYSIV8",
      "stem": "Toupin_2024_Shaping_feminist_artificial_intelligence",
      "title": "Shaping feminist artificial intelligence",
      "author_year": "Toupin (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "FAI ist ein plurales Konzept mit multiplen Bedeutungen und Manifestationen, das sich in sechs Typologien manifestiert (als Modell, Design, Policy, Kultur, Diskurs und Wissenschaft), jedoch bleibt es b",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch"
            ],
            "reasoning": "Das Paper analysiert Feminist AI (FAI) mit explizit feministischer Theorie und Perspektive (erfüllt Feministisch=Ja). Es behandelt algorithmische Systeme und deren gesellschaftliche Auswirkungen (KI_Sonstige=Ja, Bias_Ungleichheit=Ja). Gender und Diversität sind zentral für die Analyse von FAI-Framew"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt Kategorien wie 'AI_Literacies' und 'Generative_KI' auf theoretisch-konzeptuelle Auseinandersetzung mit KI aus, während der Human wahrscheinlich praktische/operative Anwendungen oder Schulungskontexte erwartet. Die hohe Confidence (0.95) bei vollständiger Divergenz aller Kategorien deutet auf systematische Überinterpretation hin.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Decolonial AI",
        "Situated Knowledge in AI Development",
        "Participatory AI Design",
        "Algorithmic Fairness in Legal AI",
        "Intersectional Algorithmic Bias",
        "Feminist Artificial Intelligence",
        "Feminist Data Practices"
      ],
      "knowledge_summary": "FAI ist ein plurales Konzept mit multiplen Bedeutungen und Manifestationen, das sich in sechs Typologien manifestiert (als Modell, Design, Policy, Kultur, Diskurs und Wissenschaft), jedoch bleibt es bislang in der technoliberalen Sphäre der Inklusion stecken und stellt keine transformative Politik dar, die kapitalistische Ungleichheitsstrukturen fundamental herausfordert."
    },
    {
      "id": "Tun_2025_Trust",
      "stem": "Tun_2025_Trust",
      "title": "Tun_2025_Trust",
      "author_year": "Tun",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": false
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": false,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": false,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": false
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "nicht angegeben",
          "stage3_completeness": 15,
          "stage3_correctness": 100,
          "stage3_overall": 22
        },
        "assessment": {}
      },
      "concepts": [
        "Systematic Review Methodology",
        "PRISMA 2020 Standards"
      ],
      "knowledge_summary": "nicht angegeben"
    },
    {
      "id": "GFHALQS2",
      "stem": "Tun_2025_Trust_in_artificial_intelligence–based_clinical",
      "title": "Trust in artificial intelligence–based clinical decision support systems among health care workers: Systematic review",
      "author_year": "Tun (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Acht zentrale Themen prägen das Vertrauen von Gesundheitsfachkräften in AI-CDSSs: Systemtransparenz, Training und Vertrautheit, Usability, klinische Zuverlässigkeit, Glaubwürdigkeit und Validierung, e",
          "stage3_completeness": 92,
          "stage3_correctness": 96,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper behandelt AI-Kompetenzen (Vertrauen, Transparenzverstaendnis) und KI-Sonstige (klinische Entscheidungssysteme) sowie Bias/Fairness. ABER: Fokus liegt auf Healthcare/klinischem Kontext, nicht auf Soziale Arbeit. Keine direkter Bezug zu sozialarbeiterischer Praxis, Theorie oder Zielgruppen. SOZI"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Clinical AI Literacy",
        "Ethical Fairness in Clinical AI",
        "Explainable AI (XAI) in Medicine",
        "Trust in Human-AI Interaction",
        "Human-Centered AI Design",
        "Algorithmic Bias in Healthcare",
        "Clinical Decision Support Systems (CDSS)",
        "Algorithmic Transparency"
      ],
      "knowledge_summary": "Acht zentrale Themen prägen das Vertrauen von Gesundheitsfachkräften in AI-CDSSs: Systemtransparenz, Training und Vertrautheit, Usability, klinische Zuverlässigkeit, Glaubwürdigkeit und Validierung, ethische Überlegungen, Human-Centric Design und Customization mit Kontrolle. Algorithmische Intransparenz und unzureichendes Training gelten als Hauptbarrieren."
    },
    {
      "id": "SSH3LVN6",
      "stem": "Ulnicane_2024_Artificial_Intelligence_and_Intersectionality",
      "title": "Artificial Intelligence and Intersectionality",
      "author_year": "Ulnicane (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI amplifiziert und verstärkt menschliche Vorurteile und reflektiert tiefgreifende historische und systemische Ungleichheiten; ein breiter intersektionaler Ansatz ist notwendig, der über reine Diversi",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 92
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "TECHNIK_OK: KI_Sonstige=Ja (algorithmische Systeme und Bias-Analyse). SOZIAL_OK: Bias_Ungleichheit=Ja, Gender=Ja, Diversitaet=Ja, Feministisch=Ja (explizite intersektionale Perspektive nach Crenshaw), Fairness=Ja. Alle Bedingungen erfüllt. Paper nutzt feministische/intersektionale Theorie substantie"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM hat 'Fairness' als erfüllte Kategorie kodiert, obwohl der Human diese ablehnt – trotz Übereinstimmung in allen anderen technischen und sozialen Kategorien. Dies deutet auf eine Überausdehnung des Fairness-Konzepts hin, wo das LLM algorithmische Fairness/Bias-Analyse automatisch als 'Fairness'-Kategorie interpretiert, während der Human eine engere oder andersartige Definition dieser Kategorie anzulegen scheint.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "AI Governance and Policy Analysis",
        "Systemic Inequality and Power Asymmetries",
        "Intersectionality in AI",
        "Gender Bias in AI Systems",
        "Algorithmic Bias",
        "Sociotechnical Framing of Bias",
        "Diversity Crisis in AI Development"
      ],
      "knowledge_summary": "AI amplifiziert und verstärkt menschliche Vorurteile und reflektiert tiefgreifende historische und systemische Ungleichheiten; ein breiter intersektionaler Ansatz ist notwendig, der über reine Diversitätszahlen hinausgeht und strukturelle sowie kulturelle Probleme in der Tech-Branche adressiert."
    },
    {
      "id": "5T55I5Z7",
      "stem": "UN Women_2024_Artificial_Intelligence_and_gender_equality",
      "title": "ARTIFICIAL INTELLIGENCE and GENDER EQUALITY",
      "author_year": "UNESCO (2020)",
      "year": 2020,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Etwa 44% der analysierten AI-Systeme zeigen Geschlechterbias, und nur 30% der AI-Fachkräfte sind Frauen. Diese Unterrepräsentation von Frauen in der AI-Entwicklung perpetuiert vorhandene gesellschaftl",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet"
            ],
            "reasoning": "Das UNESCO-Paper behandelt substantiell die Themen Gender Equality und AI (Kategorie Gender: Ja). Es adressiert digitale Kompetenzen und Geschlechterkluft (AI_Literacies: Ja), Gender Bias in KI-Systemen (KI_Sonstige: Ja, Bias_Ungleichheit: Ja) sowie Diversität. Beide Bedingungen (Technik + Sozial) s"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Gender-Sensitive AI Governance",
        "Gender Digital Divide",
        "Women Underrepresentation in AI Development",
        "Women STEM Education Access",
        "Gender Bias in AI Systems",
        "Generative AI Bias Reproduction",
        "Intersectional AI Bias"
      ],
      "knowledge_summary": "Etwa 44% der analysierten AI-Systeme zeigen Geschlechterbias, und nur 30% der AI-Fachkräfte sind Frauen. Diese Unterrepräsentation von Frauen in der AI-Entwicklung perpetuiert vorhandene gesellschaftliche Stereotypen in Trainingsdaten und führt zu diskriminierenden Konsequenzen in kritischen Bereichen wie Gesundheit, Kredite und Beschäftigung."
    },
    {
      "id": "ZNHUCA4B",
      "stem": "UNESCO_2021_Recommendation_on_the_Ethics_of_Artificial",
      "title": "Recommendation on the Ethics of Artificial Intelligence",
      "author_year": "UNESCO",
      "year": 2021,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "UNESCO etabliert ein umfassendes, multikulturelles Ethical Framework für KI, das Menschenwürde, Menschenrechte, Fairness, Diversität und Umweltschutz als zentrale Leitwerte definiert und konkrete Poli",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "UNESCO-Empfehlung behandelt AI Ethics als globale Governance-Frage (KI_Sonstige: Ja). Substantieller Fokus auf Gender Equality, Geschlechterstereotypen und diskriminatorische Biases (Gender: Ja, Bias_Ungleichheit: Ja), sowie equitable participation und Fairness (Diversitaet: Ja, Fairness: Ja). Erfül"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Fairness and Equity",
        "AI Literacy and Democratic Engagement",
        "Gender Equality in AI Systems",
        "Multi-Stakeholder Inclusion and Participation",
        "Protection of Vulnerable Populations in AI Contexts",
        "AI Ethics Governance Framework",
        "AI System Lifecycle Governance",
        "Cultural Diversity and Ethical Pluralism in AI"
      ],
      "knowledge_summary": "UNESCO etabliert ein umfassendes, multikulturelles Ethical Framework für KI, das Menschenwürde, Menschenrechte, Fairness, Diversität und Umweltschutz als zentrale Leitwerte definiert und konkrete Policy-Maßnahmen für alle Lebensphasen von KI-Systemen vorsieht."
    },
    {
      "id": "EQV4DNQR",
      "stem": "UNESCO_2024_Bias_against_women_and_girls_in_large_language",
      "title": "Challenging systematic prejudices: an Investigation into Gender Bias in Large Language Models",
      "author_year": "UNESCO, IRCAI (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "State-of-the-art LLMs (GPT-2, Llama 2, ChatGPT) perpetuieren persistente Geschlechterstereotypen, wobei nicht-optimierte Modelle in ~20% der Fälle sexistische Inhalte generieren und LGBTQ+-Themen in 6",
          "stage3_completeness": 92,
          "stage3_correctness": 94,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper untersucht Gender-Bias in LLMs (ChatGPT, GPT-2, Llama 2) – erfüllt TECHNIK-Kriterium (Generative_KI). Expliziter Fokus auf Geschlechter-Stereotypen, soziale Vorurteile und Fairness in generierten Texten erfüllt SOZIAL-Kriterien (Bias_Ungleichheit, Gender, Diversitaet, Fairness). Kein feministi"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Reinforcement Learning from Human Feedback (RLHF)",
        "Gender Bias in Large Language Models",
        "Multi-Stage Bias Accumulation in AI Development",
        "Technology-Facilitated Gender-Based Violence (TF-GBV)",
        "Stereotype Amplification Through Training Data",
        "Word Embedding Association Testing",
        "Sentiment and Regard Analysis for Bias Measurement",
        "Intersectional AI Ethics and Representation"
      ],
      "knowledge_summary": "State-of-the-art LLMs (GPT-2, Llama 2, ChatGPT) perpetuieren persistente Geschlechterstereotypen, wobei nicht-optimierte Modelle in ~20% der Fälle sexistische Inhalte generieren und LGBTQ+-Themen in 60-70% negativ darstellen, während RLHF-gesteuerte Modelle wie ChatGPT diese Biases teilweise reduzieren."
    },
    {
      "id": "TSYJ3Y57",
      "stem": "UNESCO_2024_Women4Ethical_AI_Global_cooperation_for",
      "title": "Women4Ethical AI: Global cooperation for gender-inclusive AI",
      "author_year": "UNESCO",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Eine geschlechtergerechte KI-Governance erfordert ex-ante und ex-post Assessments, starke Regulierung, Datenschutz, Haftungsmechanismen und gezielte Maßnahmen zur Erhöhung der Frauenbeteiligung als En",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt TECHNIK-Kriterien (AI_Literacies für KI-Kompetenzen/Beteiligung; KI_Sonstige für allgemeine KI-Entwicklung; Fairness für genderinklusive/faire KI-Systeme) und SOZIAL-Kriterien (Gender explizit im Titel; Bias_Ungleichheit durch Menschenrechtsfokus; Diversitaet durch globale Kooperation "
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt die Kategorie 'KI_Sonstige' auf theoretische Policy-Papiere über KI-Ethik aus, während die menschliche Entscheidung eine engere Definition (praktische/technische KI-Arbeiten) anwendet. Die hohe Konfidenz (0.95) bei vollständiger Kategorie-Divergenz deutet auf überdehntes Kategorieverständnis hin.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Algorithmic Fairness and Human Rights Protection",
        "Women's Representation in AI Leadership",
        "Gender-Inclusive AI Governance",
        "Multi-Stakeholder Accountability Mechanisms",
        "AI-Driven Discrimination and Marginalization",
        "Gender Bias in AI Systems",
        "AI Regulatory Framework"
      ],
      "knowledge_summary": "Eine geschlechtergerechte KI-Governance erfordert ex-ante und ex-post Assessments, starke Regulierung, Datenschutz, Haftungsmechanismen und gezielte Maßnahmen zur Erhöhung der Frauenbeteiligung als Entscheidungsträgerinnen und Gründerinnen im KI-Sektor."
    },
    {
      "id": "SSF5Q33W",
      "stem": "Unknown_2024_Research",
      "title": "Research on the application risks and countermeasures of ChatGPT generative artificial intelligence in social work",
      "author_year": "Unknown",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 4,
          "stage1_key_finding": "ChatGPT und generative KI-Systeme präsentieren erhebliche Risiken für die Soziale Arbeit in den Bereichen Datensicherheit, algorithmischer Bias und ethische Herausforderungen. Diese Risiken erfordern ",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Soziale_Arbeit"
            ],
            "reasoning": "Der Titel deutet auf eine Analyse von ChatGPT (generative KI) in der Sozialen Arbeit hin. Dies erfüllt beide notwendigen Bedingungen: TECHNIK (Generative_KI = Ja, da ChatGPT explizit genannt) und SOZIAL (Soziale_Arbeit = Ja, da direkt im Titel). AI_Literacies ist ebenfalls Ja, da es um Kompetenzen/R"
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Algorithmic Fairness and Transparency",
        "Algorithmic Bias in Social Services",
        "Data Security in Social Work Institutions",
        "Regulatory Framework for Generative AI",
        "Representation and Diversity in Training Data",
        "AI Literacy for Social Workers",
        "Human-Centered Social Work Practice",
        "Generative AI Ethics in Human Services"
      ],
      "knowledge_summary": "ChatGPT und generative KI-Systeme präsentieren erhebliche Risiken für die Soziale Arbeit in den Bereichen Datensicherheit, algorithmischer Bias und ethische Herausforderungen. Diese Risiken erfordern robuste regulatorische Rahmenwerke, technische Schutzmaßnahmen und eine Neuorientierung der Fachkräfte zur Bewahrung humanistischer Werte."
    },
    {
      "id": "KQ5A8D6E",
      "stem": "Unknown_AI_competency_framework_for_students",
      "title": "AI competency framework for students",
      "author_year": "Unknown",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "UNESCO präsentiert den ersten globalen Kompetenzrahmen für Schüler:innen mit 12 Kompetenzen in vier Dimensionen (Human-centred mindset, Ethics of AI, AI techniques and applications, AI system design) ",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.3,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Der Titel deutet auf AI Literacies (Kompetenzen für Studierende) hin, was die TECHNIK-Bedingung erfüllen könnte. Jedoch fehlt das Abstract und der vollständige Text, was eine fundierte Bewertung unmöglich macht. Zudem ist unklar, ob ein Sozialbezug vorliegt. Ohne Volltext kann nicht überprüft werden"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet"
            ]
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "AI Ethics Competency",
        "Inclusive AI Education",
        "AI Literacy Framework",
        "Critical AI User Competency",
        "Machine Learning Techniques Literacy",
        "Algorithmic Fairness in Education",
        "Human-Centred AI Design"
      ],
      "knowledge_summary": "UNESCO präsentiert den ersten globalen Kompetenzrahmen für Schüler:innen mit 12 Kompetenzen in vier Dimensionen (Human-centred mindset, Ethics of AI, AI techniques and applications, AI system design) über drei Progressionsstufen (Understand, Apply, Create), um Schüler:innen sowohl als kritische Nutzer:innen als auch als Ko-Gestalter:innen von KI zu befähigen."
    },
    {
      "id": "SS5HTYY6",
      "stem": "Unknown_Artificial_Intelligence_in_Social_Sciences_and",
      "title": "Artificial Intelligence in Social Sciences and Social Work: Bridging Technology and Humanity to Revolutionize Research, Policy, and Human Services",
      "author_year": "Unknown",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 5,
          "stage1_key_finding": "KI bietet erhebliche Chancen zur Verbesserung der Forschungseffizienz, Ressourcenallokation und Entscheidungsfindung in sozialen Diensten, wirft aber kritische ethische Bedenken hinsichtlich algorithm",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 92
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Das Paper ist ein Literaturreview, das KI-Technologien (Machine Learning, NLP) substantiell im Kontext Sozialer Arbeit und Human Services behandelt. Es adressiert Bias, Fairness und Ethical Concerns—zentrale soziale Aspekte. TECHNIK erfüllt (KI_Sonstige + AI_Literacies: Diskurs über KI-Integration)."
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Natural Language Processing for Sentiment Analysis",
        "Inclusive AI Design for Vulnerable Populations",
        "Predictive Analytics in Social Work",
        "Human-in-the-Loop Decision Making",
        "Algorithmic Bias in Social Services",
        "AI Ethics Governance Frameworks",
        "Data Privacy and Surveillance Risk in Human Services"
      ],
      "knowledge_summary": "KI bietet erhebliche Chancen zur Verbesserung der Forschungseffizienz, Ressourcenallokation und Entscheidungsfindung in sozialen Diensten, wirft aber kritische ethische Bedenken hinsichtlich algorithmischer Verzerrung, Datenschutz, Überwachung und Erosion menschenzentrierter Betreuung auf."
    },
    {
      "id": "L48P8FBG",
      "stem": "Unknown_Artificial_Intelligence_in_Social_Work_An_EPIC",
      "title": "Artificial intelligence in social work: An EPIC model for practice",
      "author_year": "Goldkind (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Ein vierpeiliges EPIC-Modell (Ethics and Justice, Policy Development and Advocacy, Intersectoral Collaboration, Community Engagement and Empowerment) wird als strukturierter Ansatz zur ethischen Integ",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Inklusionskriterien: TECHNIK (AI_Literacies, KI_Sonstige) durch EPIC-Framework für KI-Integration und Bias-Mitigation; SOZIAL (Soziale_Arbeit, Bias_Ungleichheit, Fairness) durch direkten Bezug zu Sozialer Arbeit, Ethik, Transparenz und Fairness-Thematisierung. Substantielle Behan"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt die Kategorie 'KI_Sonstige' übermäßig aus, indem es jeden Bezug zu KI in Soziale Arbeit als relevante Technik-Kategorie wertet, ohne zu prüfen, ob KI das zentrale Forschungsthema oder nur kontextueller Bezug ist. Die Human-Exclusion deutet darauf hin, dass das Paper primär ein Soziale-Arbeit-Paper mit peripherem KI-Bezug ist.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Algorithmic Fairness in Social Services",
        "Data Sovereignty and Colonial Knowledge Reproduction",
        "Human-Technology Complementarity in Social Work",
        "Generative AI Risks in Child Protection",
        "Algorithmic Bias in Social Work",
        "AI Literacy and Community Empowerment",
        "EPIC Model for AI Integration",
        "Intersectional AI Governance"
      ],
      "knowledge_summary": "Ein vierpeiliges EPIC-Modell (Ethics and Justice, Policy Development and Advocacy, Intersectoral Collaboration, Community Engagement and Empowerment) wird als strukturierter Ansatz zur ethischen Integration von KI in der Sozialen Arbeit präsentiert, um Chancen zu nutzen und Risiken wie algorithmische Verzerrungen und Desinformation zu mitigieren."
    },
    {
      "id": "2FUXNFZS",
      "stem": "Unknown_RETHINKING_SOCIAL_SERVICES_WITH_ARTIFICIAL",
      "title": "RETHINKING SOCIAL SERVICES WITH ARTIFICIAL INTELLIGENCE: OPPORTUNITIES, RISKS, AND FUTURE PERSPECTIVES",
      "author_year": "Unknown",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "AI bietet bedeutende Potenziale für die Soziale Arbeit (Datenanalyse, Ressourcenallokation, administrative Automatisierung), birgt aber erhebliche ethische Risiken (Bias, Datenschutz, Verlust von Mens",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch AI_Literacies (Vorbereitung von Social Workers auf KI-Nutzung), KI_Sonstige (Predictive Modeling, Algorithmen in Social Services) und Fairness. SOZIAL_OK durch Soziale_Arbeit (direkter Bezug zu Social Work Practice), Bias_Ungleichheit (algorithmic bi"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'Generative_KI' und 'Prompting' auf einen Paper aus, der primär KI-Policy-Bias analysiert, nicht aber generative KI-Systeme oder Prompting-Techniken direkt untersucht. Gleichzeitig überdehnt es 'Feministisch' und 'Fairness' auf konzeptionelle Bias-Kritik, während der Human die fehlende direkte Soziale-Arbeit-Verankerung als Ausschlusskriterium wertet.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Human-Centered Social Work Ethics",
        "Algorithmic Bias in Social Services",
        "AI-Enhanced Administrative Automation",
        "Predictive Modeling for Risk Assessment",
        "AI Literacy for Social Work Professionals",
        "Structural Inequality Reinforcement through AI",
        "Informed Consent and Transparency in AI Systems"
      ],
      "knowledge_summary": "AI bietet bedeutende Potenziale für die Soziale Arbeit (Datenanalyse, Ressourcenallokation, administrative Automatisierung), birgt aber erhebliche ethische Risiken (Bias, Datenschutz, Verlust von Menschlichkeit), die durch multidisziplinäre Ansätze und ethische Standards adressiert werden müssen."
    },
    {
      "id": "7D3ICY7Z",
      "stem": "van Toorn_2024_Introduction_to_the_digital_welfare_state",
      "title": "Introduction to the digital welfare state: Contestations, considerations and entanglements",
      "author_year": "van Toorn (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Digitalisierung des Wohlfahrtsstaates verstärkt bestehende Ungleichheitsmuster und reproduziert historische Machtdynamiken, anstatt transformativ zu wirken; sie ist nicht technologisch neutral sondern",
          "stage3_completeness": 88,
          "stage3_correctness": 93,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper behandelt algorithmische Systeme im Sozialbereich (digital welfare, automated decision-making) substantiell → KI_Sonstige: Ja. Direkter Bezug zu Sozialarbeit und Welfare-Praxis → Soziale_Arbeit: Ja. Kritische Analyse von algorithmischer Diskriminierung marginalisierter Populationen → Bias_Ungl"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'AI_Literacies' auf allgemeine digitale Literacies aus und interpretiert 'digital welfare state' als KI-bezogen, obwohl der Paper primär wohlfahrtsstaatliche Strukturen behandelt. Die Überidentifikation mit Bias/Gender-Keywords führt zu einer Überinterpretation des KI-Bezugs.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Digital Welfare Surveillance",
        "Governmentality and Digital Codification",
        "Algorithmic Discrimination in Welfare Systems",
        "Automated Decision-Making Systems (ADM)",
        "Change Within Continuity Framework",
        "Ableist and Racialized Classification Systems",
        "Intersectional Algorithmic Bias"
      ],
      "knowledge_summary": "Digitalisierung des Wohlfahrtsstaates verstärkt bestehende Ungleichheitsmuster und reproduziert historische Machtdynamiken, anstatt transformativ zu wirken; sie ist nicht technologisch neutral sondern durch neoliberale Gouvernanzlogiken und rassifizierte, geschlechtliche und ableistische Klassifikationssysteme geprägt."
    },
    {
      "id": "Y6SAPNT2",
      "stem": "Vethman_2025_Fairness_Beyond_the_Algorithmic_Frame_Actionable",
      "title": "Fairness Beyond the Algorithmic Frame: Actionable Recommendations for an Intersectional Approach",
      "author_year": "Vethman (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 5,
          "stage1_key_finding": "Die aktuelle AI-Fairness-Forschung reduziert Intersektionalität auf einen engen technischen Rahmen (Fairness-Metriken für Subgruppen), während ein echter intersektionaler Ansatz strukturelle Ungleichh",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige, Fairness behandeln KI-Systeme und Kompetenzentwicklung). SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Feministisch durch intersektionale Perspektive, expliziter Fokus auf Machtstrukturen und marginalisierte Gruppen). Intersektion"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Interdisciplinary Knowledge Integration in AI",
        "Algorithmic Bias in Welfare Systems",
        "AI Expert Positionality Reflexivity",
        "Socio-Technical Frame",
        "Fairness Metrics Limitation",
        "Intersectional AI Fairness",
        "Community Co-Ownership in AI Design"
      ],
      "knowledge_summary": "Die aktuelle AI-Fairness-Forschung reduziert Intersektionalität auf einen engen technischen Rahmen (Fairness-Metriken für Subgruppen), während ein echter intersektionaler Ansatz strukturelle Ungleichheit, Machtdynamiken und marginalisierte Stimmen zentralstellen muss. Das Paper identifiziert fünf konkrete Handlungsempfehlungen für AI-Expert:innen, die über den algorithmischen Rahmen hinausgehen."
    },
    {
      "id": "U3AJIXAJ",
      "stem": "Victor_2023_Recommendations_for_social_work_researchers_and",
      "title": "Recommendations for social work researchers and journal editors on the use of generative AI and large language models",
      "author_year": "Victor (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Generative KI und LLMs bieten sowohl transformatives Potenzial als auch erhebliche Risiken für die Sozialarbeitsforschung; es werden spezifische Empfehlungen für Forscher und Herausgeber entwickelt, d",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting",
              "Soziale_Arbeit"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, Prompting=Ja, AI_Literacies=Ja durch Fokus auf Wissen/Kompetenzen für Forschende) und SOZIAL_OK (Soziale_Arbeit=Ja durch expliziten Bezug zu Social-Work-Forschung und -Praxis). Entwickelt Framework für KI-Einsatz in Sozialarbeit mit Empf"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Gender"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Bias in Academic Research",
        "AI Literacy in Academic Practice",
        "Peer Review Quality Control for AI-Generated Content",
        "Disruptive-Disrupting Framework",
        "Research Ethics in Vulnerable Populations",
        "Generative AI in Research",
        "Research Transparency and Accountability",
        "Large Language Model Hallucinations"
      ],
      "knowledge_summary": "Generative KI und LLMs bieten sowohl transformatives Potenzial als auch erhebliche Risiken für die Sozialarbeitsforschung; es werden spezifische Empfehlungen für Forscher und Herausgeber entwickelt, die Chancen maximieren und Risiken (Bias, Datenschutz, Qualität) minimieren."
    },
    {
      "id": "3XMBE43Z",
      "stem": "Voutyrakou_2025_Algorithmic_Governance_Gender_Bias_in",
      "title": "Algorithmic Governance: Gender Bias in AI-Generated Policymaking?",
      "author_year": "Voutyrakou (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "KI-Tools integrieren genderspezifische Bedürfnisse (Menstruationshygiene, Sicherheit, Kinderbetreuung, thermischer Komfort) nur dann in Politikempfehlungen, wenn Gender explizit im Prompt erwähnt wird",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting), SOZIAL_OK (Bias_Ungleichheit + Gender + Feministisch + Fairness). Empirische Analyse von Gender-Bias in GPT-4/Copilot-generierten Policies mit intersektionalem Rahmen. Fokus auf Androzentrizität und strukturelle Biases entspric"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Gender Bias",
        "Feminist Ethics in AI Design",
        "Androcentric Design Bias",
        "Prompt-Dependent Bias Activation",
        "Algorithmic Governance",
        "Large Language Model Bias Propagation",
        "Algorithmic Literacy and User Participation",
        "Intersectional AI Fairness"
      ],
      "knowledge_summary": "KI-Tools integrieren genderspezifische Bedürfnisse (Menstruationshygiene, Sicherheit, Kinderbetreuung, thermischer Komfort) nur dann in Politikempfehlungen, wenn Gender explizit im Prompt erwähnt wird; ansonsten wird eine androzentristische Standardperspektive angewandt."
    },
    {
      "id": "GZ9B3GPD",
      "stem": "Waag_2023_Rationalisierung_durch_Digitalisierung",
      "title": "Rationalisierung durch Digitalisierung?",
      "author_year": "Waag (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Die Nutzung digitaler Technologien führt nicht zu einer Rationalisierung der Sozialen Arbeit, sondern begünstigt die Entstehung soziotechnischer Interaktionssysteme, die neue und andersartige Abstimmu",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 92
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [
              "Soziale_Arbeit"
            ],
            "reasoning": "Das Paper behandelt Digitalisierung in der Sozialen Arbeit (erfüllt Soziale_Arbeit = Ja), thematisiert aber keine KI-Systeme, ML-Algorithmen oder andere technische KI-Aspekte (alle Technik-Kategorien = Nein). Der Fokus liegt auf Rationalisierungslogik und Interaktionssoziologie. Ohne TECHNIK-Kompone"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "Generative_KI",
              "KI_Sonstige",
              "Soziale_Arbeit"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Sociotechnical Interaction Systems",
        "Asymmetry Intensification through Digitalization",
        "Coordination Problem Management",
        "Professional Practice Transformation",
        "Non-Trivial Machine Systems",
        "Interactive Labour in Social Work",
        "Digitalization and Rationalization Critique"
      ],
      "knowledge_summary": "Die Nutzung digitaler Technologien führt nicht zu einer Rationalisierung der Sozialen Arbeit, sondern begünstigt die Entstehung soziotechnischer Interaktionssysteme, die neue und andersartige Abstimmungsprobleme hervorbringen statt bestehende zu reduzieren."
    },
    {
      "id": "PQC9G5EU",
      "stem": "Wajcman_2023_Feminism_Confronts_AI_The_Gender_Relations_of",
      "title": "Feminism Confronts AI: The Gender Relations of Digitalisation",
      "author_year": "Wajcman (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Die Dominanz von Männern in AI- und Datenwissenschaftsteams führt zu einem Feedback-Loop, in dem geschlechtliche und rassistische Bias in algorithmische Systeme eingebettet und verstärkt werden. Dies ",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch"
            ],
            "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja: kritische Analyse von KI-Systemen und deren Entwicklung) UND SOZIAL_OK (Bias_Ungleichheit=Ja: Fokus auf Gender Bias in AI; Gender=Ja: expliziter Gender-Fokus; Diversitaet=Ja: Unterrepräsentation von Frauen; Feministisch=Ja: verwendet f"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Fairness in Algorithmic Systems",
        "Feminist STS (Science and Technology Studies)",
        "Gender Representation in AI",
        "Masculine Defaults in Technoculture",
        "Algorithmic Bias",
        "Intersectional Bias",
        "Technofeminism"
      ],
      "knowledge_summary": "Die Dominanz von Männern in AI- und Datenwissenschaftsteams führt zu einem Feedback-Loop, in dem geschlechtliche und rassistische Bias in algorithmische Systeme eingebettet und verstärkt werden. Dies ist nicht das Resultat unbewusster Diskriminierung, sondern das Ergebnis einer Technokultur, die Frauen und marginalisierte Gruppen systematisch aus Führungs- und Gestaltungspositionen ausgeschlossen "
    },
    {
      "id": "23Y3627L",
      "stem": "Wang_2023_Measuring_user_competence_in_using_artificial",
      "title": "Measuring user competence in using artificial intelligence: validity and reliability of artificial intelligence literacy scale",
      "author_year": "Wang (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": false,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": false
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Entwicklung und Validierung einer 12-Item-Skala (AILS - Artificial Intelligence Literacy Scale) mit vier stabilen Konstrukten (Awareness, Usage, Evaluation, Ethics), die Benutzer:innenkompetenz in KI-",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 93
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Das Paper behandelt AI Literacy durch die Entwicklung und Validierung einer Messskala. Obwohl AI_Literacies erfüllt ist (Kompetenzen im Umgang mit KI), fehlt jeglicher Bezug zu sozialen Dimensionen (Soziale_Arbeit, Bias, Gender, Diversität, Fairness). Die Entscheidungslogik erfordert BEIDE Technik U"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Artificial Intelligence Literacy Scale (AILS)",
        "User Competence Assessment",
        "Algorithmic Ethics",
        "AI Literacy",
        "Human-AI Interaction (HAII)",
        "Digital Literacy",
        "Psychometric Validation"
      ],
      "knowledge_summary": "Entwicklung und Validierung einer 12-Item-Skala (AILS - Artificial Intelligence Literacy Scale) mit vier stabilen Konstrukten (Awareness, Usage, Evaluation, Ethics), die Benutzer:innenkompetenz in KI-Nutzung reliabel und valide misst."
    },
    {
      "id": "NVZA58ML",
      "stem": "Wang_2024_A_survey_on_fairness_in_large_language_models",
      "title": "A survey on fairness in large language models",
      "author_year": "Wang (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Der Survey zeigt, dass Fairness-Forschung in LLMs je nach Modellgröße und Trainingsparadigma unterschiedliche Ansätze erfordert: Medium-sized LLMs benötigen intrinsische und extrinsische Debiasing-Met",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper behandelt Fairness in Large Language Models (Generative_KI: Ja, Fairness: Ja, Bias_Ungleichheit: Ja). Jedoch fehlt der erforderliche SOZIAL-Kontext: Es gibt keinen direkten Bezug zu Sozialer Arbeit, Gender-Perspektive, Diversität oder feministischen Ansätzen. Allgemeine Fairness-Diskussionen i"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "Intrinsic vs. Extrinsic Bias",
        "Social Bias in Language Models",
        "Algorithmic Fairness in Large Language Models",
        "Fairness Metrics in Machine Learning",
        "Debiasing Methods for LLMs",
        "Prompt-Based Fairness Evaluation",
        "Marginalized Demographic Representation",
        "Gender Bias in NLP Systems"
      ],
      "knowledge_summary": "Der Survey zeigt, dass Fairness-Forschung in LLMs je nach Modellgröße und Trainingsparadigma unterschiedliche Ansätze erfordert: Medium-sized LLMs benötigen intrinsische und extrinsische Debiasing-Methoden, während Large-sized LLMs im Prompting-Paradigma neue Evaluations- und Debiasing-Strategien brauchen."
    },
    {
      "id": "XK6G84V7",
      "stem": "Wang_2024_Algorithmic_discrimination_examining_its_types",
      "title": "Algorithmic discrimination: examining its types and regulatory measures with emphasis on US legal practices",
      "author_year": "Wang (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Das Paper identifiziert fünf primäre Typen algorithmischer Voreingenommenheit (Bias durch algorithmische Agenten, diskriminierende Feature-Selektion, Proxy-Diskriminierung, Disparate Impact, zielgeric",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 92
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmische Diskriminierungssysteme), SOZIAL_OK (Bias_Ungleichheit + Fairness: systematische Analyse von Diskriminierungstypen, Regelungsansätze, strukturelle Ungleichheiten). Kein direkter Sozialarbeitsbezug, daher nicht als SA-Paper klas"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'Algorithmic Discrimination' als juristische/regulatorische Analyse zu weit als AI Fairness-Literatur aus und interpretiert eine erwähnende Referenz zu Intersectionality als substantielle feministische Perspektive, obwohl das Paper primär rechtswissenschaftlich ist.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "US Legal and Regulatory Framework for Algorithmic Systems",
        "Algorithmic Transparency and Accountability",
        "Proxy Discrimination",
        "Algorithmic Auditing and Impact Assessment",
        "Disparate Impact",
        "Algorithmic Decision-Making in Criminal Justice",
        "Algorithmic Discrimination"
      ],
      "knowledge_summary": "Das Paper identifiziert fünf primäre Typen algorithmischer Voreingenommenheit (Bias durch algorithmische Agenten, diskriminierende Feature-Selektion, Proxy-Diskriminierung, Disparate Impact, zielgerichtete Werbung) und kategorisiert US-Regulierungsrahmen in fünf Ansätze (principled regulation, preventive controls, consequential liability, self-regulation, heteronomy regulation)."
    },
    {
      "id": "6BZ5353S",
      "stem": "Wang_2025_Multilingual_Prompting_for_Improving_LLM",
      "title": "Multilingual Prompting for Improving LLM Generation Diversity",
      "author_year": "Wang (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": false,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Multilingual Prompting übertrifft bestehende Diversitäts-Techniken konsistent und erhöht demografische, kulturelle und perspektivische Vielfalt in LLM-Outputs, während die Genauigkeit bei faktischen A",
          "stage3_completeness": 92,
          "stage3_correctness": 96,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Das Paper adressiert TECHNIK-Aspekte durch Generative_KI (LLMs) und substantiell Prompting (multilingual prompting als zentrale Strategie). SOZIAL-Aspekte sind erfüllt durch Bias_Ungleichheit (kulturelle Halluzinationen, demographische Bias), Diversitaet (narrative und demographische Diversität) und"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "disagree",
          "divergence_pattern": "Keyword-Inklusion",
          "divergence_justification": "Das LLM hat die Keywords 'Diversity', 'Bias', 'Gender' im Titel erkannt und automatisch relevante Kategorien aktiviert, ohne zu prüfen, ob das Paper tatsächlich eine substantielle sozialwissenschaftliche oder feministische Analyse bietet oder nur oberflächlich diese Begriffe behandelt. Der Titel verspricht eine rein technische Studie zu Prompting-Techniken, nicht eine kritische Auseinandersetzung mit sozialen Implikationen.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Generative Monoculturalism",
        "Multilingual Prompting",
        "Algorithmic Fairness in Content Generation",
        "Cultural Cue Integration",
        "Language-Specific Knowledge Encoding",
        "Demographic Representation Bias",
        "Perspective Diversity in LLM Generation"
      ],
      "knowledge_summary": "Multilingual Prompting übertrifft bestehende Diversitäts-Techniken konsistent und erhöht demografische, kulturelle und perspektivische Vielfalt in LLM-Outputs, während die Genauigkeit bei faktischen Aufgaben erhalten bleibt."
    },
    {
      "id": "UKKQKL7I",
      "stem": "Washington_2025_Fragile",
      "title": "Fragile Foundations: Hidden Risks of Generative AI",
      "author_year": "Washington (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Foundation models are trained on uncurated, biased internet data and sustained by business models that prioritize profit over safety, resulting in systems that reinforce historical inequalities and fa",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Das Paper behandelt substantiell Generative KI (Foundation Models, ChatGPT, Gemini) mit kritischem Fokus. Es adressiert Systemische Risiken und Gefahren für vulnerable Gruppen (Bias_Ungleichheit) sowie kritisches Verständnis der KI-Technologie (AI_Literacies). Die Kombination TECHNIK (Generative_KI "
          },
          "agreement": "llm_only"
        }
      },
      "concepts": [
        "Participatory AI Development",
        "Linguistic Representation and Low-Resource Languages",
        "Algorithmic Bias",
        "Foundation Models",
        "WEIRD Value Embedding",
        "Gender Bias in AI",
        "AI Governance and Accountability",
        "AI Fairness and Responsible AI Design"
      ],
      "knowledge_summary": "Foundation models are trained on uncurated, biased internet data and sustained by business models that prioritize profit over safety, resulting in systems that reinforce historical inequalities and fail vulnerable populations. Alternative approaches—computational, participatory, data-sourced, and collaborative—demonstrate that responsible foundation models aligned with public interest are possible"
    },
    {
      "id": "LPMP8QAY",
      "stem": "Weber_2023_Messung_von_AI_Literacy_–_Empirische_Evidenz_und",
      "title": "Messung von AI Literacy – Empirische Evidenz und Implikationen",
      "author_year": "Weber (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": false
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Es besteht keine eindeutige Korrelation zwischen subjektiver und objektiver AI Literacy, was die Notwendigkeit objektiver Messinstrumente unterstreicht. Das entwickelte 25-Item-Messinstrument diskrimi",
          "stage3_completeness": 92,
          "stage3_correctness": 88,
          "stage3_overall": 86
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.65,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Titel deutet auf AI Literacy hin (Technik-Komponente erfüllt). Jedoch fehlt Abstract und Volltextinformation. Ohne Textmaterial kann nicht überprüft werden, ob die SOZIAL-Komponente erfüllt ist (Bezug zu Sozialer Arbeit, Bias, Gender, Diversität, Fairness oder Feministik). Konservative Bewertung: Ex"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Socio-Technical Competency Framework",
        "AI Literacy",
        "Digital Divide",
        "Mixed Methods Scale Validation",
        "Human-AI Interaction",
        "Subjective-Objective Competency Gap",
        "Measurement Instrument Development"
      ],
      "knowledge_summary": "Es besteht keine eindeutige Korrelation zwischen subjektiver und objektiver AI Literacy, was die Notwendigkeit objektiver Messinstrumente unterstreicht. Das entwickelte 25-Item-Messinstrument diskriminiert erfolgreich zwischen verschiedenen Populationen mit unterschiedlicher AI Literacy."
    },
    {
      "id": "RZ4QFXQI",
      "stem": "West_2023_Discriminating_Systems_Gender,_Race,_and_Power_in",
      "title": "Discriminating Systems: Gender, Race, and Power in AI",
      "author_year": "West (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 5,
          "stage1_key_finding": "Die Diversity-Krise in der AI-Industrie und der Bias in AI-Systemen sind zwei Manifestationen desselben Problems struktureller Ungleichheit. Die herrschende Pipeline-Fokus-Strategie hat nach Jahrzehnt",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Bias in KI-Systemen), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch als intersektionale Kritik an Machtverhältnissen, Fairness). Kritische Analyse von algorithmischen Systemen mit explizitem Gender- und Rassismus-Fokus sowie"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'Generative_KI' und 'Prompting' auf allgemeine KI-Bias-Studien aus, obwohl der Paper primär theoretisch-soziologisch ist. Die Human-Einordnung unter 'KI_Sonstige' signalisiert, dass KI hier Kontextstoff ist, nicht methodologischer Fokus.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Pipeline Problem in Diversity",
        "Algorithmic Fairness",
        "Structural Inequality in Tech Workforce",
        "Algorithmic Bias",
        "Gender Classification Systems",
        "Intersectional Feminism in AI",
        "Surveillance Capitalism and Marginalized Communities"
      ],
      "knowledge_summary": "Die Diversity-Krise in der AI-Industrie und der Bias in AI-Systemen sind zwei Manifestationen desselben Problems struktureller Ungleichheit. Die herrschende Pipeline-Fokus-Strategie hat nach Jahrzehnten kein substantielles Fortschritt gebracht, weil sie tiefere Probleme wie Workplace-Kultur, Machtasymmetrien und Ausschlusslogiken ignoriert."
    },
    {
      "id": "XW8NHCIE",
      "stem": "Wilson_2024_AI_tools_show_biases_in_ranking_job_applicants'",
      "title": "Gender, race, and intersectional bias in AI resume screening via language model retrieval",
      "author_year": "Wilson (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Massive Text Embedding Models zeigen signifikante Bias: Weiße Namen werden in 85,1% der Fälle bevorzugt, während schwarze männliche Namen in bis zu 100% der Fälle benachteiligt werden. Dokument-Länge ",
          "stage3_completeness": 92,
          "stage3_correctness": 96,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + KI_Sonstige: LLM-basierte Systeme für Resume Screening), SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch via explizite intersektionale Analyse nach Crenshaw-Theorie, Fairness). Substanzielle empirische Analyse von Diskriminierung"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Fairness Evaluation",
        "Gender Bias",
        "Massive Text Embedding Models",
        "Textual Feature Impact on Bias",
        "Racial Bias in NLP Systems",
        "Algorithmic Bias in Hiring",
        "Intersectional Bias Analysis",
        "Resume Screening Automation"
      ],
      "knowledge_summary": "Massive Text Embedding Models zeigen signifikante Bias: Weiße Namen werden in 85,1% der Fälle bevorzugt, während schwarze männliche Namen in bis zu 100% der Fälle benachteiligt werden. Dokument-Länge und Namenhäufigkeit beeinflussen die Bias-Messung erheblich."
    },
    {
      "id": "XW8NHCIE",
      "stem": "Wilson_2024_Gender,_race,_and_intersectional_bias_in_AI",
      "title": "Gender, race, and intersectional bias in AI resume screening via language model retrieval",
      "author_year": "Wilson (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "MTE-Modelle zeigen signifikante Verzerrungen, wobei weiße Namen in 85,1% der Tests bevorzugt werden, männliche Namen in 51,9% bevorzugt werden, und Black-Männer in bis zu 100% der Fälle benachteiligt ",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + KI_Sonstige: LLM-basierte Systeme für Resume Screening), SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch via explizite intersektionale Analyse nach Crenshaw-Theorie, Fairness). Substanzielle empirische Analyse von Diskriminierung"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Fairness in Automated Decision-Making",
        "Massive Text Embedding Models (MTE)",
        "Language Model Bias Propagation",
        "Resume Audit Study Methodology",
        "Algorithmic Bias in Hiring Systems",
        "Name-Based Demographic Inference",
        "Intersectional Bias Analysis"
      ],
      "knowledge_summary": "MTE-Modelle zeigen signifikante Verzerrungen, wobei weiße Namen in 85,1% der Tests bevorzugt werden, männliche Namen in 51,9% bevorzugt werden, und Black-Männer in bis zu 100% der Fälle benachteiligt sind – das reale Diskriminierungsmuster am Arbeitsmarkt widerspiegelnd."
    },
    {
      "id": "Women_2024_Artificial",
      "stem": "Women_2024_Artificial",
      "title": "Women_2024_Artificial",
      "author_year": "Women",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": false
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Unter Annahme aktueller LLM-Entwicklungstrends werden Modelle zwischen 2026 und 2032 (oder früher bei Übertraining) auf der Größe des verfügbaren Bestands an öffentlichen Textdaten trainiert sein. Der",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {}
      },
      "concepts": [
        "Transfer Learning for Data Efficiency",
        "LLM Scaling Laws",
        "Human-Generated Text Corpus Estimation",
        "Synthetic Data Generation",
        "Data Scarcity Bottleneck",
        "Web Data Scraping and Privacy",
        "Data Creator Compensation"
      ],
      "knowledge_summary": "Unter Annahme aktueller LLM-Entwicklungstrends werden Modelle zwischen 2026 und 2032 (oder früher bei Übertraining) auf der Größe des verfügbaren Bestands an öffentlichen Textdaten trainiert sein. Der Engpass entsteht durch eine Diskrepanz zwischen exponentieller Nachfrage nach Trainingsdaten und limitiertem Bestand."
    },
    {
      "id": "QZJL6KBZ",
      "stem": "Wong_2020_Broadening_artificial_intelligence_education_in",
      "title": "Algorithms, artificial intelligence and discrimination",
      "author_year": "Lund (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Ein dreidimensionales AI-Literacy-Rahmenwerk (AI concepts, AI applications, AI ethics/safety) und ein altersdifferenziertes Programm (K-Grade 2, Grades 3-6, Grades 7-9) ermöglichen scaffolded AI-Bildu",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Der Bericht adressiert algorithmische Diskriminierung (KI_Sonstige: Ja) und behandelt substantiell Bias/Ungleichheit sowie Fairness im Kontext von Antidiskriminierungsrecht. Er erfüllt beide Bedingungen (TECHNIK + SOZIAL) und ist damit included, obwohl er keinen direkten Soziale-Arbeit-Bezug hat."
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Fairness"
            ]
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "AI Literacy",
        "Machine Learning Fundamentals for K-12",
        "Inclusive AI Education Design",
        "Scaffolded Curriculum Design",
        "AI Ethics and Safety Integration",
        "Digital Equity in Education",
        "Computational Thinking in K-12"
      ],
      "knowledge_summary": "Ein dreidimensionales AI-Literacy-Rahmenwerk (AI concepts, AI applications, AI ethics/safety) und ein altersdifferenziertes Programm (K-Grade 2, Grades 3-6, Grades 7-9) ermöglichen scaffolded AI-Bildung in K-12, ausgehend von Spielaktivitäten bis zu syntaxbasiertem Programmieren."
    },
    {
      "id": "C325Y32P",
      "stem": "World Economic Forum_2024_AI_for_impact_The_PRISM_framework_for_responsible",
      "title": "AI for impact: The PRISM framework for responsible AI in social innovation",
      "author_year": "World Economic Forum",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Das PRISM Framework bietet einen iterativen, schichtenweisen Ansatz zur verantwortungsvollen KI-Einführung, der Organisationsbereitschaft über technologische Fähigkeit priorisiert und fünf kritische C",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit"
            ],
            "reasoning": "Paper behandelt KI-Kompetenzen und -Integration (AI_Literacies) sowie allgemeine KI-Governance (KI_Sonstige) im Kontext sozialer Innovationen und Dienstleistungen (Soziale_Arbeit). PRISM-Framework adressiert direkt Organisationen in sozialen Sektoren mit Readiness-Assessments. TECHNIK_OK (AI_Literac"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Algorithmic Bias in Low-Income Contexts",
        "AI for Social Impact",
        "Inclusive Data Representation",
        "PRISM Framework",
        "Algorithmic Transparency and Explainability",
        "Gender Gap in AI Adoption",
        "Organizational Readiness"
      ],
      "knowledge_summary": "Das PRISM Framework bietet einen iterativen, schichtenweisen Ansatz zur verantwortungsvollen KI-Einführung, der Organisationsbereitschaft über technologische Fähigkeit priorisiert und fünf kritische Capabilites and Risks (Ethics, Data, Business/Organization, Technology, Costs/Metrics) identifiziert."
    },
    {
      "id": "JGZDWMN3",
      "stem": "Wu_2025_Bias_in_decision-making_for_AI's_ethical_dilemmas",
      "title": "Bias in decision-making for AI's ethical dilemmas: A comparative study of ChatGPT and Claude",
      "author_year": "Wu (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Beide LLMs zeigen signifikante Biases bei geschützten Attributen in ethischen Entscheidungen: GPT-3.5 Turbo bevorzugt stereotypisch dominante Gruppen (nicht-behinderte, männliche, hellhäutige, mittela",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 91
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.3,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Titel deutet auf Generative KI und Bias hin, aber ohne Abstract oder Volltext ist die Einordnung spekulativ. Sicher sind: Generative_KI=Ja (ChatGPT/Claude), Bias_Ungleichheit=Ja (Titel erwähnt 'Bias'). ABER: Kein Bezug zu Sozialer Arbeit erkennbar. Die Themen 'ethical dilemmas' sind philosophisch, n"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Ethical Decision-Making in AI Systems",
        "Algorithmic Bias in Large Language Models",
        "Protected Attribute Analysis",
        "Intersectional Discrimination",
        "Model Transparency and Accountability",
        "Linguistic Framing Effects"
      ],
      "knowledge_summary": "Beide LLMs zeigen signifikante Biases bei geschützten Attributen in ethischen Entscheidungen: GPT-3.5 Turbo bevorzugt stereotypisch dominante Gruppen (nicht-behinderte, männliche, hellhäutige, mittelaltrige Personen), während Claude 3.5 Sonnet ausgewogenere Präferenzen zeigt. Beide Modelle bevorzugen stark 'Good-looking' Personen, und die ethische Sensibilität sinkt drastisch in komplexeren inters"
    },
    {
      "id": "GPSB87RN",
      "stem": "Wudel_2025_What",
      "title": "What is Feminist AI?",
      "author_year": "Wudel (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Feminist AI (FAI) bietet einen kritischen Rahmen zur Bekämpfung struktureller Ungleichheiten in KI-Systemen durch intersektionale feministische Methodologie, Theory-Practice-Feedback-Loops und Multi-S",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 94
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK-OK durch KI_Sonstige (Bias- und Fairness-Fokus auf AI-Systeme). SOZIAL-OK durch Bias_Ungleichheit, Gender, Diversitaet, Feministisch (explizit intersektionales Feminist AI Framework) und Fairness. Verwendet feministische Theorie substantiell als Kernrahmen zu"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'Prompting' und 'AI_Literacies' auf einen konzeptionellen/theoretischen Paper aus, der primär Feminist Theory anwendet. Der Human erkennt korrekt, dass der feministisch-theoretische Kern dominant ist, während das LLM technische Begriffe überinterpretiert und gleichzeitig den Gender/Feminist-Fokus unterschätzt.",
          "disagreement_type": "Human_Unclear",
          "severity": 1
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Feminist Design Tools",
        "Structural Discrimination",
        "Feminist AI",
        "Multi-Stakeholder Engagement",
        "Intersectional Feminism",
        "Gender Bias in AI"
      ],
      "knowledge_summary": "Feminist AI (FAI) bietet einen kritischen Rahmen zur Bekämpfung struktureller Ungleichheiten in KI-Systemen durch intersektionale feministische Methodologie, Theory-Practice-Feedback-Loops und Multi-Stakeholder-Engagement. FAI unterscheidet sich von 'Responsible AI' durch ihre Analyse von Machstrukturen als Wurzel von Ungerechtigkeit statt einzelner 'bad actors'."
    },
    {
      "id": "GPSB87RN",
      "stem": "Wudel_2025_What_is_Feminist_AI",
      "title": "What is Feminist AI?",
      "author_year": "Wudel (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Feminist AI (FAI) stellt einen paradigmatischen Paradigmenwechsel dar, der systematische Machtstrukturen als Wurzel von Ungleichheiten adressiert, im Gegensatz zu Responsible AI, das einzelne 'bad act",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK-OK durch KI_Sonstige (Bias- und Fairness-Fokus auf AI-Systeme). SOZIAL-OK durch Bias_Ungleichheit, Gender, Diversitaet, Feministisch (explizit intersektionales Feminist AI Framework) und Fairness. Verwendet feministische Theorie substantiell als Kernrahmen zu"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch"
            ]
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt 'Prompting' und 'AI_Literacies' auf einen konzeptionellen/theoretischen Paper aus, der primär Feminist Theory anwendet. Der Human erkennt korrekt, dass der feministisch-theoretische Kern dominant ist, während das LLM technische Begriffe überinterpretiert und gleichzeitig den Gender/Feminist-Fokus unterschätzt.",
          "disagreement_type": "Human_Unclear",
          "severity": 1
        }
      },
      "concepts": [
        "Feminist AI",
        "Equitable Healthcare AI",
        "Multi-Stakeholder Participation in AI Governance",
        "Feminist Methodology in AI Design",
        "Gender Bias in AI Systems",
        "Algorithmic Bias in Marginalized Populations",
        "Intersectional Feminism",
        "Theory-Practice Feedback Loops"
      ],
      "knowledge_summary": "Feminist AI (FAI) stellt einen paradigmatischen Paradigmenwechsel dar, der systematische Machtstrukturen als Wurzel von Ungleichheiten adressiert, im Gegensatz zu Responsible AI, das einzelne 'bad actors' fokussiert. FAI operationalisiert feministische Prinzipien durch iterative Theorie-Praxis-Schleifen, interdisziplinäre Teams und Multi-Stakeholder-Beteiligung."
    },
    {
      "id": "NUYCHW2T",
      "stem": "Xu_2023_Transparency_enhances_positive_perceptions_of",
      "title": "Transparency enhances positive perceptions of social artificial intelligence",
      "author_year": "Xu (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Transparenz führt zu positiven Wahrnehmungsveränderungen: Nutzer finden den Chatbot weniger gruselig, fühlen größere Nähe und nehmen ihn als sozial intelligenter wahr, wobei die Effektgrößen jedoch kl",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Prompting"
            ],
            "reasoning": "Paper erfüllt TECHNIK-Kriterium (AI_Literacies: Ja - Nutzer-Kompetenzen/Wissen über KI; Generative_KI: Ja - Chatbot-fokussiert; Prompting: Ja - Transparenz-Strategien für Prompt-Engineering erwähnt). Aber SOZIAL-Kriterium nicht erfüllt: Kein direkter Bezug zu Sozialer Arbeit, keine substantielle Beh"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Fairness in User Perception",
        "Algorithmic Opacity",
        "AI Literacy",
        "Trust in Social AI Systems",
        "AI Transparency",
        "Social Chatbot Design",
        "Anthropomorphism in Human-AI Interaction"
      ],
      "knowledge_summary": "Transparenz führt zu positiven Wahrnehmungsveränderungen: Nutzer finden den Chatbot weniger gruselig, fühlen größere Nähe und nehmen ihn als sozial intelligenter wahr, wobei die Effektgrößen jedoch klein sind."
    },
    {
      "id": "2SNYUZG4",
      "stem": "Yan_2024_Promises_and_challenges_of_generative_artificial",
      "title": "Promises and challenges of generative artificial intelligence for human learning",
      "author_year": "Yan (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "GenAI bietet erhebliches Potenzial zur Transformation von Lernprozessen durch personalisierte Unterstützung, diversifizierte Ressourcen und innovative Bewertungsmethoden, erfordert aber eine kritische",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.3,
            "categories": [
              "AI_Literacies",
              "Generative_KI"
            ],
            "reasoning": "Basierend auf dem Titel behandelt das Paper generative KI und menschliches Lernen, was auf AI_Literacies und Generative_KI hindeutet. Allerdings fehlt das Abstract und der Volltext für eine verlässliche Bewertung. Ohne substantielle Informationen kann nicht überprüft werden, ob ein echter Sozialarbe"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Bias_Ungleichheit"
            ]
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "Gender Stereotype Reduction in STEAM Education",
        "Algorithmic Fairness in Educational Assessment",
        "AI Literacy",
        "Generative AI for Personalized Learning",
        "Evidence-Based Evaluation of GenAI Learning Impact",
        "Digital Divide in AI-Enhanced Education",
        "Multimodal Learning Resource Diversification"
      ],
      "knowledge_summary": "GenAI bietet erhebliches Potenzial zur Transformation von Lernprozessen durch personalisierte Unterstützung, diversifizierte Ressourcen und innovative Bewertungsmethoden, erfordert aber eine kritische Auseinandersetzung mit ethischen Fragen, Modellmängeln und dem Digital Divide sowie die Kultivierung von AI Literacy."
    },
    {
      "id": "Y6M97SWQ",
      "stem": "Yu_2025_Algorithmic-Assisted_Decision-Making_Tools_in",
      "title": "Algorithmic-Assisted Decision-Making Tools in Child Welfare Practice: A Systematic Review",
      "author_year": "Yu (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Algorithmisch unterstützte Entscheidungshilfen zeigen Potenzial für den Kinderschutz, erfordern aber systematische Aufmerksamkeit für ethische Implementierung und Fairness, insbesondere bezüglich rass",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper adressiert algorithmic decision-making tools (KI_Sonstige) in child welfare (Soziale_Arbeit). Zentrale Themen sind Bias und Fairness. Nennt Practitioner Training (AI_Literacies). Systematischer Review mit PRISMA-Methode. Erfüllt beide Bedingungen: TECHNIK (KI_Sonstige + AI_Literacies) UND SOZI"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Predictive Analytics in Child Maltreatment Risk Assessment",
        "Algorithmic Fairness in Child Welfare",
        "Algorithmic Transparency and Interpretability",
        "Racial Disparities in Algorithmic Decision-Making",
        "Community Engagement in Algorithm Governance",
        "Automation Bias in Social Work Practice",
        "Implementation Science Framework for Algorithm Deployment"
      ],
      "knowledge_summary": "Algorithmisch unterstützte Entscheidungshilfen zeigen Potenzial für den Kinderschutz, erfordern aber systematische Aufmerksamkeit für ethische Implementierung und Fairness, insbesondere bezüglich rassischer Disparitäten und Datenqualität. Bedeutende methodische und empirische Lücken bleiben bestehen."
    },
    {
      "id": "22KJL3PC",
      "stem": "Yuan_2025_The_cultural_stereotype_and_cultural_bias_of",
      "title": "The cultural stereotype and cultural bias of ChatGPT",
      "author_year": "Yuan (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "ChatGPT 3.5 zeigt signifikante kulturelle Stereotype in 8 von 13 Aufgaben und Vorurteile in Fairness-Aufgaben, während ChatGPT 4.0 Stereotype auf mehr Aufgaben ausweitet, aber in weniger Aufgaben Voru",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt TECHNIK-Kriterium: Fokus auf Generative KI (ChatGPT-3.5/GPT-4), substantielle Analyse von Prompting-Strategien zur Bias-Reduktion. Erfüllt SOZIAL-Kriterium: Explizite Behandlung von Bias (kulturelle Stereotype), Diversität (diversity-sensitive prompts) und Fairness (Bias-Mitigation-Str"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Cultural Stereotyping in Large Language Models",
        "Representational Similarity Analysis in AI Auditing",
        "Prompt-Based Bias Mitigation",
        "Cultural Value Dimensions in AI Systems",
        "AI Transparency and Cultural Accountability",
        "Algorithmic Fairness in Cross-Cultural Contexts",
        "Model Version Bias Divergence"
      ],
      "knowledge_summary": "ChatGPT 3.5 zeigt signifikante kulturelle Stereotype in 8 von 13 Aufgaben und Vorurteile in Fairness-Aufgaben, während ChatGPT 4.0 Stereotype auf mehr Aufgaben ausweitet, aber in weniger Aufgaben Vorurteile zeigt. Vier Prompt-Strategien reduzieren erfolgreich Stereotype und Bias."
    },
    {
      "id": "UDZLIJWX",
      "stem": "Yunusov_2024_MirrorStories",
      "title": "MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models",
      "author_year": "Yunusov (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": false,
            "Gender": false,
            "Diversitaet": false,
            "Feministisch": false,
            "Fairness": false
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Der verbesserte Skip-Gram mit Negative Sampling und Subsampling ermöglicht effiziente Trainingsprozesse bei gleichzeitiger Verbesserung der Vektorqualität; Phrase-Vektoren können präzise analoges Denk",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 84
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + Prompting substantiell behandelt) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Fairness). Fokus auf LLM-generierte Stories mit explizitem Prompting für Diversität. Empirische Analyse von Bias und Engagement-Disparitäten nach Identitäts"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Phrase Representation",
        "Skip-Gram Model",
        "Subsampling of Frequent Words",
        "Analogical Reasoning",
        "Negative Sampling",
        "Compositional Semantics",
        "Word Embeddings"
      ],
      "knowledge_summary": "Der verbesserte Skip-Gram mit Negative Sampling und Subsampling ermöglicht effiziente Trainingsprozesse bei gleichzeitiger Verbesserung der Vektorqualität; Phrase-Vektoren können präzise analoges Denken ermöglichen und Vektoren zeigen additive Zusammensetzungsstrukturen."
    },
    {
      "id": "UDZLIJWX",
      "stem": "Yunusov_2024_MirrorStories_Reflecting_Diversity_through",
      "title": "MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models",
      "author_year": "Yunusov (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": false,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Personalisierte LLM-generierte Geschichten erzielen signifikant höhere Bewertungen bei Engagement und Zufriedenheit (4,22 vs. 3,37 auf 5-Punkte-Skala) als generische Narrative, während sie gleichzeiti",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + Prompting substantiell behandelt) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Fairness). Fokus auf LLM-generierte Stories mit explizitem Prompting für Diversität. Empirische Analyse von Bias und Engagement-Disparitäten nach Identitäts"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Bias in LLM Evaluation",
        "Identity Element Integration",
        "Reader Engagement Evaluation",
        "Textual Diversity Metrics",
        "Personalized Narrative Generation",
        "Mirror Stories",
        "Cultural Representation in Generative AI",
        "Inclusive Content Generation"
      ],
      "knowledge_summary": "Personalisierte LLM-generierte Geschichten erzielen signifikant höhere Bewertungen bei Engagement und Zufriedenheit (4,22 vs. 3,37 auf 5-Punkte-Skala) als generische Narrative, während sie gleichzeitig textuelle Diversität erhöhen und die vermittelte Moral bewahren."
    },
    {
      "id": "49PPZJ7Z",
      "stem": "Zakharova_2024_Tensions_in_digital_welfare_states_Three",
      "title": "Tensions in digital welfare states: Three perspectives on care and control",
      "author_year": "Zakharova (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": true,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Digitale Wohlfahrtstechnologien verschieben das Gleichgewicht zwischen Care und Kontrolle zugunsten der Kontrolle und transformieren Care-Praktiken potenziell in schädliche Kontrollmaßnahmen. Ein anal",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.85,
            "categories": [
              "KI_Sonstige",
              "Soziale_Arbeit",
              "Bias_Ungleichheit"
            ],
            "reasoning": "Paper adressiert algorithmische Governance und digitale Systeme in Wohlfahrtsstaat (KI_Sonstige: Ja). Direkter Bezug zu Sozialarbeit durch Analyse von Welfare Services und deren Digitalisierung (Soziale_Arbeit: Ja). Thematisiert Spannungen zwischen Care und Control, was Fragen zu Überwachung und alg"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "disagree",
          "divergence_pattern": "Semantische Expansion",
          "divergence_justification": "Das LLM dehnt AI_Literacies und KI_Sonstige auf einen Policy-/Governance-Text aus, der algorithmische Systeme nur als Kontext für sozialpolitische Analyse nutzt, nicht als primären Gegenstand. Die sozialen Kategorien (Gender, Feministisch, Fairness) werden aktiviert, obwohl sie hier ohne technische KI-Dimension vorliegen.",
          "disagreement_type": "Human_Exclude_Agent_Include",
          "severity": 2
        }
      },
      "concepts": [
        "Intersectional Digital Discrimination",
        "Digital Welfare Technology Infrastructure",
        "Feminist Care Ethics in Digital Systems",
        "Digital Justice in Public Services",
        "Algorithmic Welfare Administration",
        "Invisible Care Labor",
        "Care-Control Tension"
      ],
      "knowledge_summary": "Digitale Wohlfahrtstechnologien verschieben das Gleichgewicht zwischen Care und Kontrolle zugunsten der Kontrolle und transformieren Care-Praktiken potenziell in schädliche Kontrollmaßnahmen. Ein analytisches Framework auf drei Ebenen (Werte, Infrastrukturen, Datenarbeit) ermöglicht die Analyse dieser Spannungen."
    },
    {
      "id": "K6JQ7SVA",
      "stem": "Zannone_2023_Intersectional_Fairness_A_Fractal_Approach",
      "title": "Intersectional Fairness: A Fractal Approach",
      "author_year": "Zannone (2023)",
      "year": 2023,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": true,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Fairness propagiert 'aufwärts' durch Intersektionalitätsebenen (von spezifischsten zu allgemeineren Gruppen), aber nicht 'abwärts'. Die Varianz der empirischen Erfolgsquoten folgt unter perfekter Fair",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ],
            "reasoning": "Paper behandelt KI/ML-Systeme (KI_Sonstige: mathematische Fairness-Metriken für Klassifikation). Substantieller Fokus auf intersektionale Fairness und algorithmischen Bias (Bias_Ungleichheit). Intersektionale Analyse nach Crenshaw ist explizit feministisches Konzept (Feministisch). Fairness-Metriken"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Feministisch",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Fairness Gerrymandering",
        "Intersectional Fairness",
        "Hypercube Geometric Modeling",
        "Algorithmic Bias in Protected Subgroups",
        "Intersectional Statistical Parity",
        "VarRatio Fairness Metrics",
        "Fairness Propagation"
      ],
      "knowledge_summary": "Fairness propagiert 'aufwärts' durch Intersektionalitätsebenen (von spezifischsten zu allgemeineren Gruppen), aber nicht 'abwärts'. Die Varianz der empirischen Erfolgsquoten folgt unter perfekter Fairness einem exponentiellen Skalierungsgesetz, das als Benchmark für Intersektionale Statistische Parität verwendet werden kann."
    },
    {
      "id": "UB9NK8KI",
      "stem": "Zayed_2024_Scaling_implicit_bias_analysis_across",
      "title": "Scaling implicit bias analysis across transformer-based language models through embedding association test and prompt engineering",
      "author_year": "Zayed (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Prompt Engineering reduziert Genre-Bias durchschnittlich um über 37% in Basismodellen, während Skalierung nur etwa 29% Reduktion erreicht. Größere Modelle zeigen tendenziell geringere Biase als kleine",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.85,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper adressiert impliziten Bias in Transformer-Sprachmodellen mittels Embedding Association Test und Prompt Engineering. Dies deckt Generative_KI, Prompting und Bias ab. Jedoch fehlt der Sozial-Kontext: Keine Bezüge zu Sozialer Arbeit, struktureller Benachteiligung oder konkreten sozialen Auswirkun"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "disagree"
        }
      },
      "concepts": [
        "Word Embedding Association Test (WEAT)",
        "Prompt Engineering for Bias Mitigation",
        "Algorithmic Fairness in NLP",
        "Multi-Scale Bias Measurement",
        "Implicit Bias in Language Models",
        "Transformer Model Scaling Effects",
        "Gender Bias in Text Representation"
      ],
      "knowledge_summary": "Prompt Engineering reduziert Genre-Bias durchschnittlich um über 37% in Basismodellen, während Skalierung nur etwa 29% Reduktion erreicht. Größere Modelle zeigen tendenziell geringere Biase als kleinere, aber gezieltes Prompt-Engineering ist die wirksamere Mitigation."
    },
    {
      "id": "XY2WVKBY",
      "stem": "Zeng_2025_Governing_discriminatory_content_in",
      "title": "Governing discriminatory content in conversational AI: A cross-system, cross-lingual, and cross-topic audit",
      "author_year": "Zeng (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Erhebliche Unterschiede in Refusal-Raten zwischen Systemen (0,54%-21,70%) und Sprachen; identifiziert vier typische Antwortstrategien bei diskriminatorischen Fragen: Moral arbiter, Know-it-all expert,",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 88
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "Generative_KI",
              "Bias_Ungleichheit",
              "Diversitaet",
              "Fairness"
            ],
            "reasoning": "Das Paper untersucht Diskriminierungsinhalte in conversational AI-Systemen (Generative_KI: Ja). Die Audit-Analyse fokussiert auf Bias und Diskriminierung (Bias_Ungleichheit: Ja), behandelt Unterschiede across Sprachen und Themen (Diversitaet: Ja) und diskutiert Fairness-Aspekte durch RLHF und Guardr"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Diversitaet",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Conversational AI Refusal Mechanisms",
        "Cross-Platform and Cross-Cultural Auditing",
        "Fairness in Generative AI Systems",
        "Regulatory Framework Divergence",
        "Algorithmic Bias in Conversational AI",
        "Training Data Bias Perpetuation",
        "Vulnerable Group Representation in AI",
        "Content Moderation Governance"
      ],
      "knowledge_summary": "Erhebliche Unterschiede in Refusal-Raten zwischen Systemen (0,54%-21,70%) und Sprachen; identifiziert vier typische Antwortstrategien bei diskriminatorischen Fragen: Moral arbiter, Know-it-all expert, Non-confrontational fence-sitter, Local values ideologue."
    },
    {
      "id": "55ZR64VU",
      "stem": "Zhang_2024_GenderAlign_An_Alignment_Dataset_for_Mitigating",
      "title": "Debiasing prompts for gender bias in large language models",
      "author_year": "Kaneko (2024)",
      "year": 2024,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": false,
            "Generative_KI": true,
            "Prompting": true,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": true,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Das GenderAlign-Dataset mit 8k Dialogpaaren reduziert Gender-Bias in LLMs signifikant effektiver als das bestehende HH-RLHF-Dataset und zeigt konsistent bessere Performance über verschiedene Modelle (",
          "stage3_completeness": 92,
          "stage3_correctness": 98,
          "stage3_overall": 95
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.92,
            "categories": [
              "Generative_KI",
              "Prompting",
              "Bias_Ungleichheit",
              "Gender",
              "Fairness"
            ],
            "reasoning": "Paper behandelt Prompting-Strategien zur Reduktion von Gender-Bias in LLMs—klare technische Komponente mit Fairness-Fokus. Allerdings fehlt jeder Bezug zu Sozialer Arbeit, Zielgruppen sozialer Dienste oder strukturellen sozialen Implikationen. Es ist eine Standard-Debiasing-Studie ohne sozialarbeitl"
          },
          "human": {
            "decision": "Exclude",
            "categories": []
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Reinforcement Learning from Human Feedback (RLHF)",
        "Algorithmic Fairness",
        "Gender Bias in Large Language Models",
        "Alignment Dataset",
        "Bias Categorization",
        "LLM-Based Data Annotation",
        "Responsible AI Development"
      ],
      "knowledge_summary": "Das GenderAlign-Dataset mit 8k Dialogpaaren reduziert Gender-Bias in LLMs signifikant effektiver als das bestehende HH-RLHF-Dataset und zeigt konsistent bessere Performance über verschiedene Modelle (Llama2-7B und Llama2-13B)."
    },
    {
      "id": "2MVSL3UV",
      "stem": "Zhang_2025_Learning_About_AI_A_Systematic_Review_of_Reviews",
      "title": "Learning About AI: A Systematic Review of Reviews on AI Literacy",
      "author_year": "Zhang (09/2025)",
      "year": null,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": false,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "The field shows consensus on AI literacy definition and availability of effective teaching tools and pedagogical approaches, but significant gaps remain in standardized assessment, integration of ethi",
          "stage3_completeness": 88,
          "stage3_correctness": 92,
          "stage3_overall": 89
        },
        "assessment": {
          "llm": {
            "decision": "Exclude",
            "confidence": 0.95,
            "categories": [
              "AI_Literacies"
            ],
            "reasoning": "Das Paper behandelt AI Literacy substantiell (Kategorie AI_Literacies = Ja) und ist ein hochwertiger Umbrella Review. Allerdings erfüllt es nicht die SOZIAL-Bedingung: Es gibt keinen direkten Bezug zu Sozialer Arbeit, keine Analyse von Bias/Ungleichheit, kein Gender/Diversitäts-Fokus und keine Fairn"
          },
          "human": {
            "decision": "Exclude",
            "categories": [
              "AI_Literacies",
              "KI_Sonstige"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "AI Pedagogy",
        "Digital Divide in AI Education",
        "Ethical AI Integration",
        "AI Assessment Standardization",
        "AI Literacy",
        "AI Competency Framework"
      ],
      "knowledge_summary": "The field shows consensus on AI literacy definition and availability of effective teaching tools and pedagogical approaches, but significant gaps remain in standardized assessment, integration of ethical considerations, interdisciplinary approaches, and policy guidance."
    },
    {
      "id": "MT54YHER",
      "stem": "Zhao_2025_Thinking_like_a_scientist_Can_interactive",
      "title": "Thinking like a scientist: Can interactive simulations foster critical AI literacy?",
      "author_year": "Zhao (2025)",
      "year": 2025,
      "stages": {
        "identification": {
          "in_zotero": true
        },
        "conversion": {
          "pdf_acquired": true,
          "markdown_converted": true
        },
        "ske": {
          "stage1_categories": {
            "AI_Literacies": true,
            "Generative_KI": true,
            "Prompting": false,
            "KI_Sonstige": true,
            "Soziale_Arbeit": false,
            "Bias_Ungleichheit": true,
            "Gender": false,
            "Diversitaet": true,
            "Feministisch": false,
            "Fairness": true
          },
          "stage1_arguments_count": 3,
          "stage1_key_finding": "Interaktive Simulationen verbessern KI-Literalität signifikant und unterstützen stärkere Wissensgeneralisierung über Themen hinweg. Die Menge der Interaktion korreliert jedoch schwach mit Lerngewinnen",
          "stage3_completeness": 0,
          "stage3_correctness": 0,
          "stage3_overall": 0
        },
        "assessment": {
          "llm": {
            "decision": "Include",
            "confidence": 0.92,
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "Bias_Ungleichheit",
              "Fairness"
            ],
            "reasoning": "Paper erfüllt TECHNIK-Kriterium durch AI_Literacies (Fokus auf kritische KI-Kompetenzentwicklung), Generative_KI (Erwähnung von Language Models) und SOZIAL-Kriterium durch Bias_Ungleichheit (expliziter Fokus auf Bias-Mechanismen) und Fairness (Verständnis von Fairness als zentrales Element). Empiris"
          },
          "human": {
            "decision": "Include",
            "categories": [
              "AI_Literacies",
              "Generative_KI",
              "KI_Sonstige",
              "Bias_Ungleichheit",
              "Fairness"
            ]
          },
          "agreement": "agree"
        }
      },
      "concepts": [
        "Algorithmic Fairness",
        "Explorable Explanations",
        "Scientific Thinking in AI Education",
        "Dataset Representativeness",
        "Knowledge Generalization in AI Learning",
        "Engagement Quality vs. Engagement Quantity",
        "Critical AI Literacy"
      ],
      "knowledge_summary": "Interaktive Simulationen verbessern KI-Literalität signifikant und unterstützen stärkere Wissensgeneralisierung über Themen hinweg. Die Menge der Interaktion korreliert jedoch schwach mit Lerngewinnen – Qualität der Engagement ist entscheidender als Quantität."
    }
  ],
  "concepts": {
    "nodes": [
      {
        "id": "AI Literacy",
        "label": "AI Literacy",
        "frequency": 43,
        "papers_count": 43,
        "definition": "Die kognitiven und behavioralen Fähigkeiten, die es Individuen auf allen Organisationsebenen ermöglichen, AI-Systeme kritisch zu verstehen, zu evaluieren und damit zu interagieren. Umfasst eine tripartite Struktur (Kognition-Verhalten-Einstellung) über generische AI-Konzepte, domänenspezifische Anwe",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Fairness",
        "label": "Algorithmic Fairness",
        "frequency": 42,
        "papers_count": 42,
        "definition": "Formal definierte mathematische und statistische Kriterien (z.B. Demographic Parity, Equalized Odds, Calibration) zur Messung und Sicherung gerechter Behandlung durch KI-Systeme. Das Paper problematisiert die Inkompatibilität und Nicht-Neutralität dieser Definitionen.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Bias",
        "label": "Algorithmic Bias",
        "frequency": 29,
        "papers_count": 29,
        "definition": "Systematische Verzerrungen in Machine-Learning-Systemen, die durch männlich-dominierte Entwicklungsteams, geschlechtsspezifische Trainingsdaten und unbewusste Entscheidungen in allen Phasen der Modellentwicklung entstehen und bestehende gesellschaftliche Ungleichheiten reproduzieren.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Bias in Social Services",
        "label": "Algorithmic Bias in Social Services",
        "frequency": 12,
        "papers_count": 12,
        "definition": "Systematische Verzerrungen in KI-Modellen, die marginalisierte Bevölkerungsgruppen in Bereichen wie Kinderschutz, Kriminalprognose und Ressourcenallokation diskriminierend benachteiligen. Diese entstehen durch verzerrte Trainingsdaten und unzureichende Repräsentation unterrepräsentierter Gruppen.",
        "cluster": "bridge"
      },
      {
        "id": "Gender Bias in AI Systems",
        "label": "Gender Bias in AI Systems",
        "frequency": 10,
        "papers_count": 10,
        "definition": "Systematische Verzerrungen in künstlichen Intelligenzsystemen, die bestehende gesellschaftliche Geschlechterverzerrungen perpetuieren und zu diskriminierenden Auswirkungen gegen Frauen und marginalisierte Geschlechtsgruppen führen. Manifestiert sich in Rekrutierung, Gesundheitswesen, Grenzmanagement",
        "cluster": "sozial"
      },
      {
        "id": "Algorithmic Discrimination",
        "label": "Algorithmic Discrimination",
        "frequency": 9,
        "papers_count": 9,
        "definition": "Die systematische Benachteiligung von Menschen durch Algorithmen und KI-Systeme aufgrund von verzerrten Trainingsdaten, die historische Diskriminierungsmuster reproduzieren und marginalisierte Gruppen (Migranten, People of Color, Frauen, Menschen mit Behinderungen) benachteiligen.",
        "cluster": "sozial"
      },
      {
        "id": "Gender Bias in Large Language Models",
        "label": "Gender Bias in Large Language Models",
        "frequency": 9,
        "papers_count": 9,
        "definition": "Systematische Verzerrungen in LLM-Ausgaben, die geschlechtsspezifische Stereotypen reproduzieren und bestimmte Geschlechtsidentitäten (insbesondere non-binäre Personen) durch herablassende oder reduktionistische Darstellungen benachteiligen.",
        "cluster": "bridge"
      },
      {
        "id": "Prompt Engineering",
        "label": "Prompt Engineering",
        "frequency": 9,
        "papers_count": 9,
        "definition": "Die Fähigkeit, strukturierte und kontextreiche Eingaben (Prompts) für KI-Systeme zu formulieren, um optimale Outputs zu erzielen. Umfasst Techniken wie iterative Verfeinerung und Strukturierung nach Persona, Task, Context und Format.",
        "cluster": "bridge"
      },
      {
        "id": "Digital Divide",
        "label": "Digital Divide",
        "frequency": 8,
        "papers_count": 8,
        "definition": "Die systemische Ungleichheit im Zugang zu digitalen Technologien und digitaler Kompetenz, die vulnerable Bevölkerungsgruppen (ältere Menschen, Minderheiten, ländliche Gemeinden, Menschen mit Behinderungen) überproportional ausschließt.",
        "cluster": "sozial"
      },
      {
        "id": "Algorithmic Bias in Social Work",
        "label": "Algorithmic Bias in Social Work",
        "frequency": 8,
        "papers_count": 8,
        "definition": "Systematische Verzerrungen in KI-Systemen, die durch historische, nicht-repräsentative Datensätze entstehen und marginalisierte Gruppen, insbesondere First Nations Peoples und People of Color, diskriminieren (z.B. Fehlerquoten bis 34,7% bei Geschlechtserkennung für dunklere Hauttypen).",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Fairness Evaluation",
        "label": "Algorithmic Fairness Evaluation",
        "frequency": 7,
        "papers_count": 7,
        "definition": "Systematische Bewertung von KI-Systemen auf Diskriminierung und Bias über demografische Gruppen hinweg durch Benchmarking-Frameworks, die Fairness-Implikationen von Prompt-Designs und Modelltendenz zu verzerrten Outputs berücksichtigen.",
        "cluster": "bridge"
      },
      {
        "id": "Critical AI Literacy",
        "label": "Critical AI Literacy",
        "frequency": 6,
        "papers_count": 6,
        "definition": "Ein umfassendes Framework zur Befähigung von Individuen, KI-Technologien kritisch zu evaluieren und produktiv mit ihnen zusammenzuarbeiten, bestehend aus vier Dimensionen: kognitiv, operativ, kritisch und ethisch. Es geht über technische Fähigkeiten hinaus und umfasst ethische, soziale und wirtschaf",
        "cluster": "bridge"
      },
      {
        "id": "AI Literacy in Social Work",
        "label": "AI Literacy in Social Work",
        "frequency": 6,
        "papers_count": 6,
        "definition": "Die notwendigen Kompetenzen und das kritische Verständnis von KI-Systemen, insbesondere Large Language Models, die Sozialarbeiter:innen und Praktiker:innen in sozialen Diensten benötigen, um informed und verantwortungsvoll mit diesen Technologien umzugehen.",
        "cluster": "bridge"
      },
      {
        "id": "Gender Bias in AI",
        "label": "Gender Bias in AI",
        "frequency": 6,
        "papers_count": 6,
        "definition": "Die Verstärkung geschlechtsspezifischer Diskriminierung durch KI-Systeme, einschließlich der Problematisierung binärer Geschlechtskategorien und der rational-maskulinen Intelligenz-Definition in der KI-Entwicklung.",
        "cluster": "sozial"
      },
      {
        "id": "Feminist AI",
        "label": "Feminist AI",
        "frequency": 5,
        "papers_count": 5,
        "definition": "Ein kritischer Rahmen zur Bekämpfung struktureller Ungleichheiten in KI-Systemen durch intersektionale feministische Methodologie, Theory-Practice-Feedback-Loops und Multi-Stakeholder-Engagement, der sich von Responsible AI durch die Analyse von Machstrukturen als Wurzel von Ungerechtigkeit untersch",
        "cluster": "sozial"
      },
      {
        "id": "Explainable AI (XAI)",
        "label": "Explainable AI (XAI)",
        "frequency": 5,
        "papers_count": 5,
        "definition": "Ansatz zur Gestaltung von Machine-Learning-Modellen mit nachvollziehbarer Logik für Entscheidungsfindung, insbesondere relevant in sozialen und administrativen Kontexten wo Transparenz und Rechenschaftspflicht essentiell sind.",
        "cluster": "bridge"
      },
      {
        "id": "Intersectional Algorithmic Bias",
        "label": "Intersectional Algorithmic Bias",
        "frequency": 5,
        "papers_count": 5,
        "definition": "AI-bedingte Diskriminierung, die durch Überlagerung mehrerer marginalisierter Identitätsmerkmale (Rasse, Geschlecht, Klasse, Migrationsstatus) entsteht und Menschen mit multiplen vulnerablen Positionen in spezifischen Weisen schadet (z.B. Black women in Gesichtserkennung, migrant families in Wohlfah",
        "cluster": "sozial"
      },
      {
        "id": "Algorithmic Bias in Large Language Models",
        "label": "Algorithmic Bias in Large Language Models",
        "frequency": 5,
        "papers_count": 5,
        "definition": "Systematische Verzerrungen in LLMs, die durch Training auf nicht-verifizierten Internet-Daten entstehen und eurozentristische sowie weiße Perspektiven überrepräsentieren, während Arbeiten von BIPOC-Autoren und marginalisierten Gruppen marginalisiert werden.",
        "cluster": "bridge"
      },
      {
        "id": "Intersectional Fairness",
        "label": "Intersectional Fairness",
        "frequency": 5,
        "papers_count": 5,
        "definition": "Ein Fairness-Konzept in Machine Learning, das Diskriminierungsmuster an den Schnittstellen mehrerer Identitätsdimensionen (z.B. Rasse und Geschlecht) analysiert und adressiert, basierend auf Crenshaws intersektionaler Theorie. Es geht über marginalfreie Fairness einzelner Gruppen hinaus.",
        "cluster": "sozial"
      },
      {
        "id": "Algorithmic Fairness in Social Services",
        "label": "Algorithmic Fairness in Social Services",
        "frequency": 4,
        "papers_count": 4,
        "definition": "Die Gewährleistung fairer und nicht-diskriminierender automatisierter Entscheidungsfindung bei der Verteilung öffentlicher sozialer Leistungen (Renten, Arbeitslosengeld, Asylbewilligung) unter Berücksichtigung kultureller und kontextabhängiger Gerechtigkeitskriterien.",
        "cluster": "bridge"
      },
      {
        "id": "Prompt Engineering for Bias Mitigation",
        "label": "Prompt Engineering for Bias Mitigation",
        "frequency": 4,
        "papers_count": 4,
        "definition": "Die systematische Verwendung von Prompt-Modifizierern und spezifischen Formulierungen, um Verzerrungen in generativen KI-Modellen zu reduzieren oder zu kontrollieren. Das Paper zeigt, dass diese Technik fragmentarisch wirkt und stark von der Sequenzierung abhängt.",
        "cluster": "bridge"
      },
      {
        "id": "Inclusive AI Design",
        "label": "Inclusive AI Design",
        "frequency": 4,
        "papers_count": 4,
        "definition": "Kritische Perspektive auf KI-Technologien, die hinterfragt, wie diese zur Unterstützung marginalisierter Menschen (Assistenztechnologien, Zugänglichkeit) eingesetzt werden können, ohne gleichzeitig als Kontrollwerkzeuge missbraucht zu werden.",
        "cluster": "sozial"
      },
      {
        "id": "Machine Learning Literacy",
        "label": "Machine Learning Literacy",
        "frequency": 4,
        "papers_count": 4,
        "definition": "Understanding of machine learning components including data-driven design, knowledge representation, and model training. A foundational technical component within broader AI literacy for K-12 students.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Bias in Language Models",
        "label": "Algorithmic Bias in Language Models",
        "frequency": 4,
        "papers_count": 4,
        "definition": "Systematische Verzerrungen in großen Sprachmodellen, die Stereotypen, Missrepräsentationen und ausgrenzende Sprache aus Trainingsdaten erben und reproduzieren, mit überproportionalen Schäden für marginalisierte Gemeinschaften.",
        "cluster": "bridge"
      },
      {
        "id": "Intersectional Bias Analysis",
        "label": "Intersectional Bias Analysis",
        "frequency": 4,
        "papers_count": 4,
        "definition": "Analytischer Ansatz, der die Überlagerung von Gender- und Ethnizitätsbias untersucht (z.B. gender-within-ethnicity-Analysen) statt diese separat zu betrachten, um komplexe Diskriminierungsmuster in KI-Systemen abzubilden.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Gender Bias",
        "label": "Algorithmic Gender Bias",
        "frequency": 4,
        "papers_count": 4,
        "definition": "Systematische Benachteiligung oder Nichtberücksichtigung geschlechtsspezifischer Bedürfnisse in KI-generierten Politikempfehlungen, die genderspezifische Perspektiven nur als Sonderfälle behandelt, wenn explizit erwähnt.",
        "cluster": "sozial"
      },
      {
        "id": "Intersectional AI Fairness",
        "label": "Intersectional AI Fairness",
        "frequency": 4,
        "papers_count": 4,
        "definition": "Ein regulatorischer und analytischer Ansatz, der Diskriminierung nicht isoliert nach einzelnen Merkmalen (Geschlecht, Rasse, Klasse) analysiert, sondern deren Zusammenwirkung bei der Benachteiligung von Personen mit multiplen marginalizierten Identitäten berücksichtigt.",
        "cluster": "sozial"
      },
      {
        "id": "Human-AI Collaboration",
        "label": "Human-AI Collaboration",
        "frequency": 4,
        "papers_count": 4,
        "definition": "The interaction between human workers and algorithmic systems in shared decision-making contexts, including how professionals integrate, override, or trust algorithmic recommendations based on contextual expertise and organizational pressures.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Transparency and Explainability",
        "label": "Algorithmic Transparency and Explainability",
        "frequency": 4,
        "papers_count": 4,
        "definition": "The degree to which users understand how an algorithmic system operates, which features influence its outputs, and how different inputs affect predictions. Workers demand explicit explanations rather than opaque 'black-box' decision-making.",
        "cluster": "sozial"
      },
      {
        "id": "Intersectional Bias",
        "label": "Intersectional Bias",
        "frequency": 4,
        "papers_count": 4,
        "definition": "Overlapping Diskriminierungsmechanismen, die Frauen insbesondere aus marginalisierten Gruppen (Frauen of Color, Black women) in Tech-Feldern treffen und deren Perspektiven systematisch aus Systemgestaltung ausschließen.",
        "cluster": "bridge"
      },
      {
        "id": "Design Justice",
        "label": "Design Justice",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Ein Ansatz, der betroffene Gemeinschaften aktiv in Designprozesse einbezieht und lokale Bedürfnisse, Autonomie und kollektive Selbstbestimmung statt Marktlogiken und Effizienzoptimierung in den Mittelpunkt stellt.",
        "cluster": "sozial"
      },
      {
        "id": "Intersectional Feminism in Technology",
        "label": "Intersectional Feminism in Technology",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Ein theoretischer Rahmen, der verschachtelte Systeme von Unterdrückung (race, gender, class, ability, religion) als ineinandergreifende Strukturen analysiert und auf die kritische Untersuchung von technologischen Systemen anwendet.",
        "cluster": "sozial"
      },
      {
        "id": "Representation Bias",
        "label": "Representation Bias",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Systematic underrepresentation of social minorities and marginalized groups in training datasets and development teams, leading to reduced model performance and fairness for underrepresented populations.",
        "cluster": "sozial"
      },
      {
        "id": "Algorithmic Fairness Metrics",
        "label": "Algorithmic Fairness Metrics",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Quantitative Messverfahren zur Evaluierung von Diskriminierungsfreiheit in KI-Systemen, etwa Equalized Odds oder Demographic Parity, zur Überprüfung geschlechterbezogener Fairness.",
        "cluster": "bridge"
      },
      {
        "id": "EPIC Model for AI Integration",
        "label": "EPIC Model for AI Integration",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Ein vierpeiliger Rahmen (Ethics and Justice, Policy Development and Advocacy, Intersectoral Collaboration, Community Engagement and Empowerment) zur ethischen und gerechten Integration von KI in der Sozialen Arbeit unter Wahrung beruflicher Werte.",
        "cluster": "bridge"
      },
      {
        "id": "Explainable Artificial Intelligence (XAI)",
        "label": "Explainable Artificial Intelligence (XAI)",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Techniken und Methoden zur Transparentmachung von KI-Entscheidungsprozessen, um die 'Black-Box'-Problematik zu überwinden und Vertrauen bei Fachkräften zu schaffen. Im Kontext der Sozialen Arbeit essentiell für Nachvollziehbarkeit von Risikobewertungen und Fallentscheidungen.",
        "cluster": "bridge"
      },
      {
        "id": "Predictive Analytics in Child Welfare",
        "label": "Predictive Analytics in Child Welfare",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Einsatz von algorithmischen Vorhersagesystemen (z.B. AFST) zur Einschätzung von Kindeswohlgefährdung, das ethische und Fairness-Fragen bezüglich Transparenz, Datenschutz und professioneller Kontrolle aufwirft.",
        "cluster": "bridge"
      },
      {
        "id": "Gender Bias in NLP",
        "label": "Gender Bias in NLP",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Phänomen, bei dem Natural Language Processing-Modelle sexistische Stereotypen in Trainigsdaten reproduzieren und verstärken, z.B. durch problematische Word Embeddings oder Autocomplete-Systeme.",
        "cluster": "bridge"
      },
      {
        "id": "Automation Bias",
        "label": "Automation Bias",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Risiko, dass Fachkräfte der Sozialen Arbeit KI-Systeme unkritisch überbewerten und ihre professionelle Urteilskraft schwächen, was insbesondere bei der Arbeit mit vulnerablen Personen zu Schädigungen führen kann.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Bias in Healthcare",
        "label": "Algorithmic Bias in Healthcare",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Systematische Verzerrungen in medizinischen Algorithmen, die durch strukturelle Faktoren wie unterschiedliche Gesundheitsausgaben für marginalisierte Gruppen entstehen und zu diskriminierender Behandlung führen.",
        "cluster": "sozial"
      },
      {
        "id": "Intersectional Feminism",
        "label": "Intersectional Feminism",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Ein Forschungs- und analytisches Framework, das Unterdrückung von Menschen als durch multiple soziale Kategorien (Geschlecht, Ethnie, Klasse, Sexualität, Fähigkeit, Alter, Religion, Geographie) geprägt versteht und zur Analyse von KI-Systemen angewendet wird.",
        "cluster": "sozial"
      },
      {
        "id": "Data Feminism",
        "label": "Data Feminism",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Ein methodischer Ansatz, der sieben Prinzipien (Examine Power, Challenge Power, Consider Context, Make Labor Visible, usw.) anwendet, um Machtverhältnisse in Datensammlung, -analyse und KI-Systemen zu kritisieren und gerechtere Praktiken zu ermöglichen.",
        "cluster": "sozial"
      },
      {
        "id": "Chain-of-Thought Reasoning",
        "label": "Chain-of-Thought Reasoning",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Eine Prompt-Engineering-Technik, die LLMs anleitet, Probleme schrittweise zu lösen (z.B. Entitätsidentifikation → Pronomenbestimmung → Coreference-Auflösung → Geschlechtsbestimmung), um transparentere und accuratere Ergebnisse zu erzielen.",
        "cluster": "bridge"
      },
      {
        "id": "Intersectionality in AI",
        "label": "Intersectionality in AI",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Analytischer Ansatz, der untersucht, wie sich mehrere geschützte Merkmale (z.B. Geschlecht, Rasse, Klasse) wechselseitig verstärken und zu versteckten Diskriminierungsmustern führen, die Single-Attribute-Debiasing-Methoden übersehen.",
        "cluster": "sozial"
      },
      {
        "id": "Digital Equity in Education",
        "label": "Digital Equity in Education",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Politische und pädagogische Massnahmen zur Sicherung gleicher Chancen beim Zugang zu digitalen Ressourcen und Kompetenzentwicklung für alle Schülergruppen, insbesondere benachteiligte und ländliche Gemeinden.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Fairness in Social Work",
        "label": "Algorithmic Fairness in Social Work",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Kritische Analyse von Algorithmen und automatisierten Entscheidungssystemen in sozialen Diensten hinsichtlich ihrer gerechten Anwendung gegenüber verschiedenen Klientelgruppen, insbesondere zur Vermeidung diskriminierender Kategorisierung vulnerabler Bevölkerungsgruppen.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Fairness in Child Welfare",
        "label": "Algorithmic Fairness in Child Welfare",
        "frequency": 3,
        "papers_count": 3,
        "definition": "The systematic assessment and correction of algorithmic decision-making tools to ensure consistent and equitable performance across demographic subgroups (particularly by race and gender) in child protective services. Includes fairness metrics, cross-subgroup validation, and bias correction techniqu",
        "cluster": "sozial"
      },
      {
        "id": "Chain-of-Thought Prompting",
        "label": "Chain-of-Thought Prompting",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Prompting-Technik, die LLMs zwingt, ihre Vorhersageprozesse explizit Schritt-für-Schritt zu artikulieren, wodurch versteckte Vorurteile aufgedeckt und als intrinsischer Debiasing-Mechanismus fungiert.",
        "cluster": "bridge"
      },
      {
        "id": "Intersectional Feminism in AI",
        "label": "Intersectional Feminism in AI",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Ein analytischer Ansatz, der feministische Theorien (insbesondere intersektionale Perspektiven von Frauen of Color, Black Feminists und queeren Theoretiker:innen) auf KI-Systeme anwendet, um verschachtelte Machtstrukturen und Diskriminierungsmechanismen aufzudecken.",
        "cluster": "sozial"
      },
      {
        "id": "Algorithmic Transparency",
        "label": "Algorithmic Transparency",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Die Forderung nach Offenlegung von Entscheidungslogiken, Trainingsdaten und Funktionsweise von Algorithmen und KI-Systemen in der Sozialen Arbeit, um Nachvollziehbarkeit und Verantwortlichkeit zu ermöglichen.",
        "cluster": "bridge"
      },
      {
        "id": "Intersectional AI Governance",
        "label": "Intersectional AI Governance",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Integration von intersektionalen und feministischen Perspektiven in globale KI-Sicherheitspolitiken, um die heterogenen Auswirkungen von KI-Systemen auf marginalisierte Gruppen (Frauen, racialisierte Menschen, wirtschaftlich Benachteiligte) systematisch zu adressieren.",
        "cluster": "sozial"
      },
      {
        "id": "Algorithmic Fairness and Transparency",
        "label": "Algorithmic Fairness and Transparency",
        "frequency": 3,
        "papers_count": 3,
        "definition": "The requirement that AI systems used in social services be interpretable, auditable, and demonstrably fair across different demographic groups, enabling social workers to scrutinize and evaluate the equity of algorithmic decisions.",
        "cluster": "bridge"
      },
      {
        "id": "Digital Literacy",
        "label": "Digital Literacy",
        "frequency": 3,
        "papers_count": 3,
        "definition": "Ein mehrdimensionales Konzept, das sowohl technische Grundfähigkeiten als auch kritisches Denken, intelligente Inhaltsnutzung und bewusste Handhabung digitaler Ressourcen umfasst, um Menschen vor Desinformation zu schützen.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Bias Detection",
        "label": "Algorithmic Bias Detection",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Technische und analytische Verfahren zur Identifikation von Verzerrungen und Stereotypen in Algorithmen und Natural Language Processing Systemen, besonders im Hinblick auf Gender und intersektionale Dimensionen.",
        "cluster": "bridge"
      },
      {
        "id": "Ethical AI Literacy",
        "label": "Ethical AI Literacy",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Die Fähigkeit, AI-Technologien zu verstehen, zu bewerten und ethisch zu nutzen, einschließlich technischen Verständnisses, kritischen Bewusstseins für soziale Implikationen und der 'algorithmischen Übersetzung' für Klienten in der sozialen Arbeit.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Bias Mitigation",
        "label": "Algorithmic Bias Mitigation",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Technische Strategien zur Erkennung und Reduktion von Verzerrungen in Machine-Learning-Systemen durch Pre-, In- oder Post-Processing-Interventionen. Das Paper zeigt, dass diese Ansätze notwendig aber unzureichend für echte Fairness sind.",
        "cluster": "bridge"
      },
      {
        "id": "Intersectionality in AI Systems",
        "label": "Intersectionality in AI Systems",
        "frequency": 2,
        "papers_count": 2,
        "definition": "The critical challenge that debiasing efforts targeting one protected group may simultaneously exacerbate discrimination against intersectional subgroups defined by multiple overlapping characteristics.",
        "cluster": "bridge"
      },
      {
        "id": "Digital Health Literacy",
        "label": "Digital Health Literacy",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Die Fähigkeit von Individuen, digitale Gesundheitstechnologien kritisch zu verstehen, zu nutzen und ihre Risiken einzuschätzen, um effektiv auf Online-Gesundheitsinformationen und -dienste zuzugreifen. Wird mit dem eHealth Literacy Scale (eHEALS) gemessen.",
        "cluster": "sozial"
      },
      {
        "id": "Inclusive Prompt Engineering",
        "label": "Inclusive Prompt Engineering",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Eine systematische Methodologie zur strategischen Formulierung und iterativen Anpassung von Eingabeaufforderungen an KI-Bildgeneratoren, um strukturelle Verzerrungen zu umgehen und vielfältigere, authentischere visuelle Darstellungen von menschlicher Diversität zu erzeugen.",
        "cluster": "bridge"
      },
      {
        "id": "Training Data Bias",
        "label": "Training Data Bias",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Verzerrungen in KI-Systemen, die aus unrepräsentativen, homogenen oder vorselektierten Trainingsdatensätzen entstehen, die statistische Muster widerspiegeln, die diskriminatorische Outcomes perpetuieren.",
        "cluster": "bridge"
      },
      {
        "id": "Cultural Bias in Large Language Models",
        "label": "Cultural Bias in Large Language Models",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Systematische Verzerrungen in LLMs, die arabische und muslimische Gruppen durch Stereotypisierung, Assoziation mit Gewalt/Terrorismus und orientalistische Darstellungen marginalisieren und perpetuieren westlich-zentrische Perspektiven aus Trainingsdaten.",
        "cluster": "bridge"
      },
      {
        "id": "Orientalism in AI Systems",
        "label": "Orientalism in AI Systems",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Die Reproduktion von Edward Said's Konzept des Orientalismus durch statistische Muster in LLM-Trainingsdaten, die historische Machtverhältnisse und koloniale Strukturen in der Darstellung von Arabern und Muslimen widerspiegeln.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Fairness in Generative AI",
        "label": "Algorithmic Fairness in Generative AI",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Systematische Ansätze zur Gewährleistung, dass LLMs in Entscheidungskontexten keine diskriminierenden Ergebnisse basierend auf geschützten Merkmalen (Rasse, Geschlecht, Religion) produzieren. Umfasst Design, Evaluation und Mitigation von Bias in KI-Systemen.",
        "cluster": "bridge"
      },
      {
        "id": "AI Governance Frameworks",
        "label": "AI Governance Frameworks",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Umfassende organisatorische und rechtliche Strukturen zur Kontrolle, Überwachung und Regulierung des Einsatzes von generativen KI- und LLM-Tools in Behörden.",
        "cluster": "bridge"
      },
      {
        "id": "Responsible AI Use",
        "label": "Responsible AI Use",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Ethische Prinzipien und praktische Richtlinien zur Sicherstellung, dass LLMs zum Gemeinwohl eingesetzt werden und Schäden durch Fehlerinformation, Diskriminierung oder Missbrauch minimiert werden.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Fairness and Equity",
        "label": "Algorithmic Fairness and Equity",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Die Analyse und Gewährleistung von gerechter und diskriminierungsfreier Behandlung durch KI-Systeme, einschließlich der Identifikation von potenziellen Verzerrungen und Benachteiligungen von Randgruppen.",
        "cluster": "sozial"
      },
      {
        "id": "AI Ethics Education",
        "label": "AI Ethics Education",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Systematische Vermittlung von ethischer Reflexion in KI-Kontexten, um Schüler:innen zur kritischen Bewertung von Auswirkungen KI-gestützter Lösungen zu befähigen und verantwortungsvolle Entscheidungsfindung zu fördern.",
        "cluster": "bridge"
      },
      {
        "id": "Digital Sovereignty",
        "label": "Digital Sovereignty",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Fähigkeit der Verwaltung zur unabhängigen Kontrolle, Bewertung und Sicherung sensibler Bürgerdaten sowie zur Verhandlung auf Augenhöhe mit KI-Dienstleistern durch adequate Kompetenzen und Ressourcen.",
        "cluster": "sozial"
      },
      {
        "id": "AI Trustworthiness",
        "label": "AI Trustworthiness",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Ein relationales Attribut, das durch die Übereinstimmung zwischen beobachtetem Systemverhalten und mentalen Modellen von Stakeholdern entsteht und sowohl intrinsische als auch wahrgenommene Dimensionen umfasst.",
        "cluster": "bridge"
      },
      {
        "id": "AI Transparency",
        "label": "AI Transparency",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Die Offenlegung der Funktionsweise und Mechanismen von KI-Systemen gegenüber Nutzern, um deren Verständnis von Fähigkeiten, Grenzen und Entscheidungsprozessen zu verbessern und Unsicherheit zu reduzieren.",
        "cluster": "bridge"
      },
      {
        "id": "Human-in-the-Loop Decision Support",
        "label": "Human-in-the-Loop Decision Support",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Ein Dual-Ansatz, bei dem KI als Entscheidungsunterstützungssystem fungiert, nicht als autonomer Ersatz für professionelle menschliche Urteilskraft, um therapeutische Beziehungen und professionelle Verantwortung zu bewahren.",
        "cluster": "bridge"
      },
      {
        "id": "AI Literacy in Social Work Education",
        "label": "AI Literacy in Social Work Education",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Die notwendige Integration von KI-Inhalten in Akkreditierungsanforderungen und Lehrpläne der Sozialen Arbeit sowie die Entwicklung von Schulungsprogrammen für Fachkräfte und Communities zum kritischen Verständnis von KI-Risiken und -Chancen.",
        "cluster": "bridge"
      },
      {
        "id": "Ethical AI Awareness",
        "label": "Ethical AI Awareness",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Die kritische Reflexionsfähigkeit von Lernenden, Bias, Diskriminierung und gesellschaftliche Auswirkungen von AI-Lösungen zu erkennen und verantwortungsvolle Entscheidungen zu treffen.",
        "cluster": "bridge"
      },
      {
        "id": "AI Literacy in Professional Practice",
        "label": "AI Literacy in Professional Practice",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Erweiterte Kompetenzen von Sozialarbeitern im Umgang mit KI-Systemen, einschließlich kritischen Verständnisses von Funktionsweise, Limitationen und ethischen Implikationen. 92% der Studienteilnehmer fordern formalisierte Schulungsprogramme.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Fairness Auditing",
        "label": "Algorithmic Fairness Auditing",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Systematische Überprüfungsprozesse, die über die Identifikation offener Diskriminierung hinausgehen und umfassende Bias-Audits durchführen, insbesondere in hochrisikanten KI-Anwendungen wie Rekrutierung und Gesundheitswesen.",
        "cluster": "sozial"
      },
      {
        "id": "AI Competency Framework",
        "label": "AI Competency Framework",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Structured models defining AI literacy constructs across cognitive, metacognitive, emotional, and social dimensions. Examples include recognition, knowledge, application, evaluation, creation, and ethical navigation components tailored to different educational levels.",
        "cluster": "bridge"
      },
      {
        "id": "AI Ethics Principles",
        "label": "AI Ethics Principles",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Systematische Analyse von fünf Kernprinzipien (Beneficence, Non-maleficence, Autonomy, Justice, Explicability) und deren Zielkonflikten bei der Gestaltung ethisch verantwortungsvoller KI-Systeme.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Bias Amplification",
        "label": "Algorithmic Bias Amplification",
        "frequency": 2,
        "papers_count": 2,
        "definition": "The systematic reinforcement and scaling of demographic representation biases in generative AI models during training on non-representative data, leading to disproportionate underperformance for marginalized populations.",
        "cluster": "bridge"
      },
      {
        "id": "Digital Inclusion",
        "label": "Digital Inclusion",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Strategie zur Gewährleistung von chancengleichem Zugang zu digitalen Technologien und Kompetenzen für alle Bürger, insbesondere zur Unterstützung von benachteiligten und unterrepräsentierten Gruppen in der digitalen Partizipation.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Bias in Child Welfare",
        "label": "Algorithmic Bias in Child Welfare",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Systematische Verzerrungen in Machine-Learning-Modellen für Kinderschutzentscheidungen, die zu diskriminierenden Outcomes für bestimmte Bevölkerungsgruppen (z.B. Familien mit häufigem Kontakt zu öffentlichen Systemen, ethnische Minderheiten) führen und strukturelle Benachteiligung amplifizieren.",
        "cluster": "bridge"
      },
      {
        "id": "Responsible AI Development",
        "label": "Responsible AI Development",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Normative Forderung nach transparenten, inklusiven und ethisch fundierten Prozessen bei der Entwicklung von KI-Systemen, die marginalisierte Gruppen schützen und Geschlechtergerechtigkeit gewährleisten.",
        "cluster": "sozial"
      },
      {
        "id": "Iterative Prompt Refinement",
        "label": "Iterative Prompt Refinement",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Ein zyklischer Prozess der wiederholten Anpassung und Umformulierung von Eingabeaufforderungen basierend auf KI-Outputs, um gezielt unerwünschte Bias-Muster zu korrigieren und gewünschte Ergebnisse zu erreichen.",
        "cluster": "technik"
      },
      {
        "id": "Algorithmic Bias in Visual Generative AI",
        "label": "Algorithmic Bias in Visual Generative AI",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Systematische Verzerrungen in generativen KI-Systemen, die bestehende gesellschaftliche Machtverhältnisse (Sexismus, Rassismus, Ableismus) durch trainierte Daten und westlich geprägte Designentscheidungen perpetuieren und in generierten Bildern manifestieren.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Transparency and Accountability",
        "label": "Algorithmic Transparency and Accountability",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Regulatorische und technische Anforderungen für Nachvollziehbarkeit, Erklärbarkeit und Verantwortlichkeit von algorithmischen Entscheidungssystemen gegenüber betroffenen Personen, Behörden und der Öffentlichkeit.",
        "cluster": "sozial"
      },
      {
        "id": "Representational Harm",
        "label": "Representational Harm",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Schädigung durch systematisch eingeengte, stereotype oder othering Darstellungen marginalisierter Gruppen in KI-generierten Texten, die auch bei nicht-explizit negativem Inhalt soziale Marginalisierung perpetuieren.",
        "cluster": "sozial"
      },
      {
        "id": "Subgroup Fairness",
        "label": "Subgroup Fairness",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Eine intersektionale Fairness-Notion, die fordert, dass ML-Systeme für alle möglichen Kombinationen geschützter Attribute faire Ergebnisse erzielen, um Diskriminierung von marginalen Subgruppen zu vermeiden.",
        "cluster": "sozial"
      },
      {
        "id": "Multicalibration",
        "label": "Multicalibration",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Eine spezialisierte Fairness-Mitigationstechnik, die gewährleistet, dass Modellvorhersagen für verschiedene intersektionale Subgruppen kalibriert sind, unabhängig davon, ob diese Gruppen explizit in den Trainingsdaten vertreten sind.",
        "cluster": "sozial"
      },
      {
        "id": "Fairness Gerrymandering",
        "label": "Fairness Gerrymandering",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Ein Phänomen, bei dem Diskriminierung gegen intersektional marginalisierte Gruppen (z.B. schwarze Frauen) größer ist als die Summe der Diskriminierung gegen einzelne Attribute (z.B. schwarze Menschen + Frauen separat).",
        "cluster": "sozial"
      },
      {
        "id": "AI Ethics Curriculum Integration",
        "label": "AI Ethics Curriculum Integration",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Systematische Vermittlung von KI-Chancen, Limitationen (Halluzinationen, Datenschutz, mangelnde Empathie) und kritischen Reflexion in Sozialarbeit-Ausbildungsprogrammen zur Vorbereitung auf technologisch vermittelte Praxis.",
        "cluster": "bridge"
      },
      {
        "id": "AI Literacy for Social Workers",
        "label": "AI Literacy for Social Workers",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Erforderliche Schulungen und Kompetenzen für Sozialarbeitende zur kritischen Bewertung, ethischen Anwendung und Vermittlung von KI-Tools sowie zur Vermeidung unkritischer Automatisierung professioneller Urteile.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Fairness in Vulnerable Populations",
        "label": "Algorithmic Fairness in Vulnerable Populations",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Die Forderung nach ethisch vertretbaren und gerechten KI-Systemen, die marginalisierte und vulnerable Gruppen nicht diskriminieren und deren Perspektiven in Trainigsdatensätzen adäquat repräsentiert sind.",
        "cluster": "bridge"
      },
      {
        "id": "Natural Language Processing (NLP)",
        "label": "Natural Language Processing (NLP)",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Ein Teilgebiet der KI seit den 1960er Jahren, das die Fähigkeit von Computern trainiert, geschriebene und gesprochene Texte zu verstehen und intelligent zu verarbeiten, wie es in ChatGPT und modernen Sprachmodellen implementiert ist.",
        "cluster": "bridge"
      },
      {
        "id": "Disparate Impact Assessment",
        "label": "Disparate Impact Assessment",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Quantitative Analyse der unterschiedlichen Auswirkungen von Algorithmen auf verschiedene Rassen-, ethnische und soziale Gruppen, insbesondere im Kontext von disproportionalen Kindesentfernungsraten.",
        "cluster": "sozial"
      },
      {
        "id": "Racial Disparities in Algorithmic Decision-Making",
        "label": "Racial Disparities in Algorithmic Decision-Making",
        "frequency": 2,
        "papers_count": 2,
        "definition": "The phenomenon where machine learning models used in child welfare disproportionately flag or recommend investigation for children from specific racial groups (particularly Black children), thereby amplifying existing systemic inequalities in the child protection system.",
        "cluster": "sozial"
      },
      {
        "id": "Epistemic Injustice in AI Systems",
        "label": "Epistemic Injustice in AI Systems",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Die systematische Benachteiligung bestimmter Gruppen durch KI-Systeme bei der Wissensproduktion und -vermittlung, insbesondere wie Large Language Models zu epistemischen Ungerechtigheden beitragen.",
        "cluster": "bridge"
      },
      {
        "id": "Structural Injustice",
        "label": "Structural Injustice",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Ein analytisches und normatives Rahmenwerk, das erklärt, wie gesellschaftliche Strukturen individuelle Handlungen und aggregierte Ergebnisse beeinflussen und systemische Ungerechtigkeit reproduzieren, die über individuelle Schäden hinausgeht.",
        "cluster": "sozial"
      },
      {
        "id": "Ethics Washing",
        "label": "Ethics Washing",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Die Praxis, ethische Richtlinien und Frameworks zu implementieren, ohne substantielle ethische Reformen oder Veränderungen in der Unternehmensorientierung zu bewirken, wie dokumentiert bei Technologiekonzernen wie Google und Facebook.",
        "cluster": "sozial"
      },
      {
        "id": "Algorithmic Fairness in Education",
        "label": "Algorithmic Fairness in Education",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Sicherstellung, dass KI-Systeme im Bildungsbereich keine systematischen Vorurteile verstärken, insbesondere durch Adressierung von Klassenungleichgewicht in Trainungsdaten und Diskriminierung bestimmter Schülergruppen.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Literacy",
        "label": "Algorithmic Literacy",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Kombinierte Fähigkeit, technische Funktionsweise algorithmischer Systeme zu verstehen und deren soziale Auswirkungen sowie kritische Limitations kritisch zu reflektieren.",
        "cluster": "sozial"
      },
      {
        "id": "Algorithmic Bias in Welfare Systems",
        "label": "Algorithmic Bias in Welfare Systems",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Die systematische Diskriminierung vulnerabler Gruppen durch automatisierte Entscheidungssysteme in Sozialleistungsprogrammen, die strukturelle Ungleichheit verstärkt und marginalisierte Communities überproportional schadet.",
        "cluster": "sozial"
      },
      {
        "id": "Feminist Science and Technology Studies (STS)",
        "label": "Feminist Science and Technology Studies (STS)",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Scholarly approach that critiques technological systems through feminist perspectives, examining how care work, power relations, and social inequalities are embedded in and sustained by technological infrastructures and data practices.",
        "cluster": "sozial"
      },
      {
        "id": "Generative AI Literacy",
        "label": "Generative AI Literacy",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Ein Kompetenzensemble, das Individuen befähigt, effektiv mit generativen KI-Technologien zu interagieren, einschließlich Verständnis fundamentaler Konzepte, kritischer Evaluation von Outputs, Prompt-Engineering und ethischer Nutzung in diversen Kontexten.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Bias in Hiring Systems",
        "label": "Algorithmic Bias in Hiring Systems",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Systematische Verzerrungen in automatisierten Resume-Screening-Systemen, die bestimmte demografische Gruppen (basierend auf Race und Gender) benachteiligen und reale Arbeitsmarktdiskriminierung durch KI-Modelle reproduzieren.",
        "cluster": "bridge"
      },
      {
        "id": "Intersectionality",
        "label": "Intersectionality",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Analytischer Ansatz, der die Verschränkung mehrerer Achsen von Ungleichheit (Rasse, Geschlecht, Klasse, Fähigkeit) untersucht und zeigt, wie komplexe Intersektionen (z.B. Frauen of Color) in Bildgenerierung systematisch ausgelassen werden.",
        "cluster": "sozial"
      },
      {
        "id": "Chain-of-Thought Prompting Limitations",
        "label": "Chain-of-Thought Prompting Limitations",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Empirischer Nachweis, dass CoT-Prompting entgegen bisheriger Annahmen nicht System 2 Reasoning modelliert, sondern stärker mit System 1 korreliert und keine konsistente Bias-Reduktion bei sozialen Stereotypen erreicht.",
        "cluster": "bridge"
      },
      {
        "id": "Intersectional Algorithmic Discrimination",
        "label": "Intersectional Algorithmic Discrimination",
        "frequency": 2,
        "papers_count": 2,
        "definition": "The compounded negative effects of algorithmic bias on individuals with overlapping marginalized identities (race, gender, socioeconomic status), resulting in systematic disadvantage across critical domains like welfare, criminal justice, and healthcare.",
        "cluster": "sozial"
      },
      {
        "id": "Automated Decision-Making Systems",
        "label": "Automated Decision-Making Systems",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Technologische Systeme, die auf Basis von Machine Learning und Algorithmen autonome oder halbautonome Entscheidungen treffen, insbesondere in kritischen Domänen wie Medizin, Justiz und Grenzpolizei.",
        "cluster": "bridge"
      },
      {
        "id": "Computational Thinking",
        "label": "Computational Thinking",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Problem-solving approach involving algorithmic thinking and logical reasoning to understand how AI systems process information and make decisions, foundational to AI literacy.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Fairness in NLP",
        "label": "Algorithmic Fairness in NLP",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Systematischer Ansatz zur Sicherstellung gerechter Behandlung und ausgewogener Repräsentation aller Gruppen in Sprachmodell-Outputs, um diskriminierende oder marginalisierte Narrativen zu verhindern.",
        "cluster": "bridge"
      },
      {
        "id": "Trustworthy AI",
        "label": "Trustworthy AI",
        "frequency": 2,
        "papers_count": 2,
        "definition": "KI-Systeme, die mehrere Dimensionen adressieren: Human-centered values, Fairness, Transparenz, Robustheit, Safety und gesellschaftlicher Nutzen, mit expliziten Strategien zur Behandlung von Zielkonflikten zwischen diesen Prinzipien.",
        "cluster": "sozial"
      },
      {
        "id": "AI Literacy Framework",
        "label": "AI Literacy Framework",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Ein strukturierter Kompetenzrahmen, der Schüler:innen befähigt, KI-Systeme zu verstehen, anzuwenden und zu gestalten, über drei Progressionsstufen (Understand, Apply, Create) verteilt auf vier Dimensionen (Human-centred mindset, Ethics of AI, AI techniques and applications, AI system design).",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Decision-Making in Social Services",
        "label": "Algorithmic Decision-Making in Social Services",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Die Automatisierung und Algorithmisierung von Entscheidungsprozessen in der Sozialen Arbeit (z.B. Risikobewertungen bei Kindeswohlgefährdung), die professionelle Urteilsbildung transformiert und neue Formen der Handlungsdisziplinierung etabliert.",
        "cluster": "sozial"
      },
      {
        "id": "Feminist Policy Analysis Framework",
        "label": "Feminist Policy Analysis Framework",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Operationalisiertes analytisches Framework basierend auf feministischen und sozialarbeitlichen Theorien (Crenshaw, McPhail) mit fünf Dimensionen (Intersektionalität, Kontextbewusstsein, Neutralitätskritik, Kontrolle, Machtverteilung) zur Bewertung von KI-Governance-Initiativen.",
        "cluster": "sozial"
      },
      {
        "id": "Ethical AI Integration",
        "label": "Ethical AI Integration",
        "frequency": 2,
        "papers_count": 2,
        "definition": "The systematic incorporation of ethical considerations, algorithmic fairness, and social impact assessment into AI curricula and educational practices. Identified as a critical gap requiring further development across educational levels.",
        "cluster": "bridge"
      },
      {
        "id": "Explainable AI",
        "label": "Explainable AI",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Ansätze zur Offenlegung und Verständlichmachung der inneren Funktionsweise von KI-Systemen, um Black-Box-Algorithmen transparenter zu machen und Missverständnisse über KI-Funktionalität zu reduzieren.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Transparency and Interpretability",
        "label": "Algorithmic Transparency and Interpretability",
        "frequency": 2,
        "papers_count": 2,
        "definition": "The requirement for child welfare agencies and practitioners to understand the limitations, decision boundaries, and operational logic of algorithmic tools, supported by clear documentation, external ethical review, and feedback mechanisms for continuous improvement.",
        "cluster": "bridge"
      },
      {
        "id": "Inclusive AI Education Design",
        "label": "Inclusive AI Education Design",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Gestaltungsansätze für KI-Lerninterventionen, die kulturelle Relevanz, Identität, Werte und Hintergründe von Lernenden berücksichtigen, besonders für unterrepräsentierte Gruppen und nicht-technische Lernende.",
        "cluster": "bridge"
      },
      {
        "id": "Proxy Discrimination",
        "label": "Proxy Discrimination",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Diskriminierung durch die Verwendung von Ersatzfaktoren (Proxy-Variablen), die eng mit geschützten Charakteristiken korrelieren, aber nicht explizit diese Charakteristiken verwenden, wodurch klassische rechtliche Unterscheidungen zwischen direkter und indirekter Diskriminierung erschwert werden.",
        "cluster": "sozial"
      },
      {
        "id": "Intersectional Discrimination",
        "label": "Intersectional Discrimination",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Diskriminierung, die mehrere geschützte Charakteristiken simultan betrifft und marginalisierte Gruppen innerhalb größerer Schutzgruppen betrifft, was durch algorithmische Systeme verstärkt werden kann.",
        "cluster": "sozial"
      },
      {
        "id": "Algorithmic Decision Support",
        "label": "Algorithmic Decision Support",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Technological systems designed to assist professional judgment in organizational decision-making, particularly in social work casework, by standardizing information processing and constraining subjective discretion through automated recommendations.",
        "cluster": "bridge"
      },
      {
        "id": "Gender Bias in Language Models",
        "label": "Gender Bias in Language Models",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Systematische Verzerrung in Sprachmodellen, die Geschlechter ungleich behandelt und stereotype Zuordnungen verstärkt, messbar durch differentielle Modellverhalten bei männlichen vs. weiblichen Attributen und Kontexten.",
        "cluster": "sozial"
      },
      {
        "id": "Ethical AI in Education",
        "label": "Ethical AI in Education",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Kritische Auseinandersetzung mit Fragen der Fairness, Transparenz und Verantwortlichkeit von AI-basierten Bildungstechnologien. Geht über technische Lösungen hinaus und erfordert professionelle ethische Urteilskraft von Lehrkräften.",
        "cluster": "bridge"
      },
      {
        "id": "Machine Learning Fundamentals for K-12",
        "label": "Machine Learning Fundamentals for K-12",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Die Vermittlung grundlegender Machine-Learning-Konzepte (Neural Networks, Trainingsmechanismen, Datenbias) an Grundschüler durch praktische Tools wie Teachable Machine und Code.org.",
        "cluster": "bridge"
      },
      {
        "id": "Predictive Analytics in Social Work",
        "label": "Predictive Analytics in Social Work",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Data-driven techniques that analyze historical case data, demographics, and social determinants to identify at-risk individuals or groups for early intervention in areas such as child welfare, mental health, and substance abuse prevention.",
        "cluster": "sozial"
      },
      {
        "id": "Algorithmic Accountability",
        "label": "Algorithmic Accountability",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Rahmenwerk zur Gewährleistung von Verantwortlichkeit und Transparenz bei der Entwicklung und dem Einsatz von KI-Systemen durch klar definierte Verantwortungen für verschiedene Akteure und dokumentierte Governance-Strukturen.",
        "cluster": "sozial"
      },
      {
        "id": "Subgroup Fairness Metrics",
        "label": "Subgroup Fairness Metrics",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Technische Operationalisierung von Fairness durch Berechnung von Leistungsmetriken über demografische Subgruppen, die in der Literatur dominiert, aber kritische Aspekte wie Machtstrukturen und sozialen Kontext vernachlässigt.",
        "cluster": "sozial"
      },
      {
        "id": "Social Bias in Language Models",
        "label": "Social Bias in Language Models",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Die Annahme durch Sprachmodelle, dass Personen bestimmte Charakteristiken aufgrund ihrer Zugehörigkeit zu sozialen Gruppen (Geschlecht, Rasse, Religion, Alter) haben. Basiert auf unkurierten Trainingsdaten, die gesellschaftliche Vorurteile kodifizieren.",
        "cluster": "bridge"
      },
      {
        "id": "Critical Digital Thinking",
        "label": "Critical Digital Thinking",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Cognitive capacity to evaluate, analyze, and critically assess digital information sources, detect misinformation and deception, and form evidence-based judgments in digital contexts.",
        "cluster": "sozial"
      },
      {
        "id": "Measurement Instrument Development",
        "label": "Measurement Instrument Development",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Systematischer Prozess zur Konstruktion und Validierung eines objektiven 25-Item-Messinstruments mit vier Dimensionen (Socio User, Socio Creator/Evaluator, Technical User, Technical Creator/Evaluator) nach etablierten psychometrischen Standards.",
        "cluster": "bridge"
      },
      {
        "id": "AI Governance and Accountability",
        "label": "AI Governance and Accountability",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Regulatorische und institutionelle Mechanismen zur Sicherung von Verantwortlichkeit in KI-Systemen über den gesamten Lebenszyklus, einschließlich Überwachung, Audit und Rechenschaftspflicht gegenüber betroffenen Gemeinschaften.",
        "cluster": "sozial"
      },
      {
        "id": "Data Literacy in Professional Practice",
        "label": "Data Literacy in Professional Practice",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Erforderliche Kompetenz von Sozialarbeiter*innen, um die technischen Prozesse, Grenzen und Unterschiede zwischen technischer und professioneller Zuverlässigkeit von Algorithmen kritisch zu verstehen.",
        "cluster": "sozial"
      },
      {
        "id": "Reparative Justice in AI",
        "label": "Reparative Justice in AI",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Ein Rahmenwerk zur Wiedergutmachung von Schäden durch AI-Systeme, das über symbolische Maßnahmen hinausgeht und substantielle Kompensation, Systemreformen und die Beteiligung betroffener Gemeinschaften einfordert.",
        "cluster": "sozial"
      },
      {
        "id": "Transformative Justice Framework",
        "label": "Transformative Justice Framework",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Ein Gerechtigkeitsansatz, der nicht nur Täter-Opfer-Beziehungen adressiert, sondern die strukturellen Bedingungen reformiert, die Schäden ermöglicht haben, mit Fokus auf Prävention zukünftiger Harms.",
        "cluster": "sozial"
      },
      {
        "id": "Algorithmic Governance",
        "label": "Algorithmic Governance",
        "frequency": 2,
        "papers_count": 2,
        "definition": "The exercise of private authority by Big Tech companies through automated decision-making systems that regulate user behavior, content visibility, and access to information at scale, functioning as a form of non-state governance affecting billions of users.",
        "cluster": "bridge"
      },
      {
        "id": "Algorithmic Opacity",
        "label": "Algorithmic Opacity",
        "frequency": 2,
        "papers_count": 2,
        "definition": "The fundamental lack of transparency in machine-learning systems that makes algorithms appear as 'black boxes' to both users and designers, preventing meaningful understanding of how automated decisions are made and undermining legitimacy through inability to internalize governance norms.",
        "cluster": "sozial"
      },
      {
        "id": "Reinforcement Learning from Human Feedback (RLHF)",
        "label": "Reinforcement Learning from Human Feedback (RLHF)",
        "frequency": 2,
        "papers_count": 2,
        "definition": "Trainingsmethode zur Anpassung von LLM-Ausgaben basierend auf menschlichem Feedback, die nachweislich Gender Bias und sexistische Inhalte substanziell reduzieren kann.",
        "cluster": "bridge"
      }
    ],
    "edges": [
      {
        "source": "AI Literacy",
        "target": "Algorithmic Bias",
        "weight": 13
      },
      {
        "source": "AI Literacy",
        "target": "Algorithmic Fairness",
        "weight": 13
      },
      {
        "source": "Algorithmic Bias",
        "target": "Algorithmic Fairness",
        "weight": 11
      },
      {
        "source": "AI Literacy",
        "target": "Prompt Engineering",
        "weight": 6
      },
      {
        "source": "AI Literacy",
        "target": "Machine Learning Literacy",
        "weight": 4
      },
      {
        "source": "AI Literacy in Social Work",
        "target": "Algorithmic Bias in Social Services",
        "weight": 4
      },
      {
        "source": "Algorithmic Fairness",
        "target": "Gender Bias in Large Language Models",
        "weight": 4
      },
      {
        "source": "Algorithmic Bias",
        "target": "Gender Bias in AI",
        "weight": 4
      },
      {
        "source": "AI Literacy",
        "target": "Digital Divide",
        "weight": 3
      },
      {
        "source": "Algorithmic Bias in Social Work",
        "target": "EPIC Model for AI Integration",
        "weight": 3
      },
      {
        "source": "Algorithmic Fairness",
        "target": "Gender Bias in AI",
        "weight": 3
      },
      {
        "source": "Algorithmic Fairness",
        "target": "Algorithmic Transparency and Explainability",
        "weight": 3
      },
      {
        "source": "AI Literacy",
        "target": "Human-AI Collaboration",
        "weight": 3
      },
      {
        "source": "Algorithmic Bias",
        "target": "Explainable AI (XAI)",
        "weight": 2
      },
      {
        "source": "Algorithmic Bias",
        "target": "Intersectional Feminism in Technology",
        "weight": 2
      },
      {
        "source": "Algorithmic Bias",
        "target": "Digital Divide",
        "weight": 2
      },
      {
        "source": "Algorithmic Fairness",
        "target": "Digital Divide",
        "weight": 2
      },
      {
        "source": "Algorithmic Bias Mitigation",
        "target": "Algorithmic Fairness",
        "weight": 2
      },
      {
        "source": "Algorithmic Fairness",
        "target": "Representation Bias",
        "weight": 2
      },
      {
        "source": "Cultural Bias in Large Language Models",
        "target": "Orientalism in AI Systems",
        "weight": 2
      },
      {
        "source": "AI Literacy",
        "target": "Responsible AI Use",
        "weight": 2
      },
      {
        "source": "Prompt Engineering",
        "target": "Responsible AI Use",
        "weight": 2
      },
      {
        "source": "AI Literacy",
        "target": "Inclusive AI Design",
        "weight": 2
      },
      {
        "source": "Algorithmic Fairness",
        "target": "Inclusive AI Design",
        "weight": 2
      },
      {
        "source": "AI Literacy",
        "target": "Ethical AI Awareness",
        "weight": 2
      },
      {
        "source": "Algorithmic Bias in Language Models",
        "target": "Gender Bias in NLP",
        "weight": 2
      },
      {
        "source": "AI Competency Framework",
        "target": "AI Literacy",
        "weight": 2
      },
      {
        "source": "AI Ethics Principles",
        "target": "AI Literacy",
        "weight": 2
      },
      {
        "source": "Algorithmic Bias",
        "target": "Prompt Engineering",
        "weight": 2
      },
      {
        "source": "Algorithmic Fairness",
        "target": "Intersectional Feminism",
        "weight": 2
      },
      {
        "source": "AI Literacy in Social Work",
        "target": "Algorithmic Fairness",
        "weight": 2
      },
      {
        "source": "Algorithmic Bias in Social Services",
        "target": "Algorithmic Fairness",
        "weight": 2
      },
      {
        "source": "AI Literacy",
        "target": "Gender Bias in AI",
        "weight": 2
      },
      {
        "source": "AI Literacy",
        "target": "Algorithmic Fairness Evaluation",
        "weight": 2
      },
      {
        "source": "Algorithmic Fairness Evaluation",
        "target": "Chain-of-Thought Reasoning",
        "weight": 2
      },
      {
        "source": "Algorithmic Fairness",
        "target": "Gender Bias in AI Systems",
        "weight": 2
      },
      {
        "source": "Algorithmic Fairness",
        "target": "Intersectionality in AI",
        "weight": 2
      },
      {
        "source": "Algorithmic Fairness",
        "target": "Responsible AI Development",
        "weight": 2
      },
      {
        "source": "Gender Bias in AI Systems",
        "target": "Intersectionality in AI",
        "weight": 2
      },
      {
        "source": "AI Literacy",
        "target": "Digital Equity in Education",
        "weight": 2
      },
      {
        "source": "Algorithmic Gender Bias",
        "target": "Intersectional AI Fairness",
        "weight": 2
      },
      {
        "source": "AI Literacy in Social Work",
        "target": "Predictive Analytics in Child Welfare",
        "weight": 2
      },
      {
        "source": "Algorithmic Bias in Social Services",
        "target": "Predictive Analytics in Child Welfare",
        "weight": 2
      },
      {
        "source": "Intersectional Fairness",
        "target": "Multicalibration",
        "weight": 2
      },
      {
        "source": "Intersectional Fairness",
        "target": "Subgroup Fairness",
        "weight": 2
      },
      {
        "source": "Multicalibration",
        "target": "Subgroup Fairness",
        "weight": 2
      },
      {
        "source": "Fairness Gerrymandering",
        "target": "Intersectional Fairness",
        "weight": 2
      },
      {
        "source": "AI Literacy for Social Workers",
        "target": "Algorithmic Bias in Social Services",
        "weight": 2
      },
      {
        "source": "Algorithmic Bias",
        "target": "Design Justice",
        "weight": 2
      },
      {
        "source": "Algorithmic Fairness in Child Welfare",
        "target": "Racial Disparities in Algorithmic Decision-Making",
        "weight": 2
      },
      {
        "source": "Algorithmic Fairness",
        "target": "Structural Injustice",
        "weight": 2
      },
      {
        "source": "Chain-of-Thought Prompting",
        "target": "Gender Bias in Large Language Models",
        "weight": 2
      },
      {
        "source": "Algorithmic Fairness",
        "target": "Automated Decision-Making Systems",
        "weight": 2
      },
      {
        "source": "AI Literacy",
        "target": "Computational Thinking",
        "weight": 2
      },
      {
        "source": "AI Literacy",
        "target": "Gender Bias in AI Systems",
        "weight": 2
      },
      {
        "source": "AI Literacy",
        "target": "Ethical AI Integration",
        "weight": 2
      },
      {
        "source": "Algorithmic Fairness",
        "target": "Ethical AI Integration",
        "weight": 2
      },
      {
        "source": "AI Literacy",
        "target": "Explainable AI",
        "weight": 2
      },
      {
        "source": "Algorithmic Bias",
        "target": "Explainable AI",
        "weight": 2
      },
      {
        "source": "AI Literacy",
        "target": "Inclusive AI Education Design",
        "weight": 2
      },
      {
        "source": "Algorithmic Discrimination",
        "target": "Algorithmic Fairness",
        "weight": 2
      },
      {
        "source": "Algorithmic Discrimination",
        "target": "Proxy Discrimination",
        "weight": 2
      },
      {
        "source": "Algorithmic Fairness",
        "target": "Intersectional Discrimination",
        "weight": 2
      },
      {
        "source": "Algorithmic Fairness",
        "target": "Proxy Discrimination",
        "weight": 2
      },
      {
        "source": "AI Literacy",
        "target": "Ethical AI in Education",
        "weight": 2
      },
      {
        "source": "AI Literacy",
        "target": "Machine Learning Fundamentals for K-12",
        "weight": 2
      },
      {
        "source": "Algorithmic Bias in Social Services",
        "target": "Algorithmic Fairness and Transparency",
        "weight": 2
      },
      {
        "source": "Algorithmic Bias in Social Services",
        "target": "Predictive Analytics in Social Work",
        "weight": 2
      },
      {
        "source": "Critical Digital Thinking",
        "target": "Digital Divide",
        "weight": 2
      },
      {
        "source": "Critical Digital Thinking",
        "target": "Digital Literacy",
        "weight": 2
      },
      {
        "source": "Digital Divide",
        "target": "Digital Literacy",
        "weight": 2
      },
      {
        "source": "AI Literacy",
        "target": "Measurement Instrument Development",
        "weight": 2
      },
      {
        "source": "AI Literacy",
        "target": "Explainable AI (XAI)",
        "weight": 2
      },
      {
        "source": "Explainable AI (XAI)",
        "target": "Human-AI Collaboration",
        "weight": 2
      },
      {
        "source": "AI Governance and Accountability",
        "target": "Gender Bias in AI",
        "weight": 2
      },
      {
        "source": "Algorithmic Bias",
        "target": "Gender Bias in AI Systems",
        "weight": 2
      },
      {
        "source": "Reparative Justice in AI",
        "target": "Transformative Justice Framework",
        "weight": 2
      },
      {
        "source": "Gender Bias in Large Language Models",
        "target": "Reinforcement Learning from Human Feedback (RLHF)",
        "weight": 2
      },
      {
        "source": "Feminist AI",
        "target": "Intersectional Feminism",
        "weight": 2
      }
    ]
  },
  "divergences": [
    {
      "paper_id": "3",
      "title": "Feminist perspectives on AI: Ethical considerations in algorithmic decision-making",
      "author_year": "Ahmed (2024)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "AI_Literacies, Generative_KI, KI_Sonstige, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness",
      "n_affected": 8,
      "llm_confidence": 0.85,
      "llm_reasoning": "Titel deutet auf AI Literacy Framework hin - klare AI_Literacies-Kategorie. Kein Abstract vorhanden; basierend auf Titel keine Indikation für soziale Dimensionen (Bias, Ungleichheit, Gender, Fairness, Soziale Arbeit). Paper erfüllt nur Technik-Kriterium, nicht Sozial-Kriterium. EXCLUDE nach strikter Logik.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'AI Literacies' zu stark aus und kategorisiert ein Paper mit feministisch-ethischem Fokus auf Algorithmen als reines Technik-Literacy-Paper, während es die implizite soziale und normative Dimension (Gender, Fairness, Bias) systematisch unterschätzt.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "156",
      "title": "Intersectional Stereotypes in Large Language Models: Dataset and Analysis",
      "author_year": "Ma (2023)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "AI_Literacies, Generative_KI, KI_Sonstige, Bias_Ungleichheit, Gender, Diversitaet, Fairness",
      "n_affected": 7,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper behandelt AI-Vertrauen, Transparenz und Accountability (AI_Literacies, KI_Sonstige, Fairness) sowie Prompting im Design-Kontext. Erfüllt TECHNIK_OK. Fehlt aber SOZIAL_OK: Kein direkter Bezug zu Sozialer Arbeit/Zielgruppen, keine Analyse von Bias/Ungleichheit oder struktureller Benachteiligung. Fairness bezieht sich auf Transparenz-Metriken, nicht auf systematische Diskriminierung.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Fairness' über technische Transparenz-Metriken aus und übersieht, dass 'intersectional stereotypes' im Kontext dieses Papers auf gesellschaftliche Diskriminierung (Gender, Diversität, Bias/Ungleichheit) abzielt – Kategorien, die das LLM systematisch nicht erkannt hat. Die Divergenzen folgen einem Muster von zu breiter technischer vs. zu enger sozialer Interpretation.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "177",
      "title": "LIBRA: Measuring bias of large language model from a local context",
      "author_year": "Pan (2025)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "AI_Literacies, Generative_KI, Prompting, KI_Sonstige, Bias_Ungleichheit, Gender, Diversitaet",
      "n_affected": 7,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper behandelt Trust-Management in AI-Systemen und Fairness durch adaptive Interventionen (TECHNIK: AI_Literacies, KI_Sonstige, Fairness erfüllt). Es befasst sich jedoch nicht mit sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder feministischen Perspektiven (SOZIAL nicht erfüllt). Fokus liegt auf allgemeiner Human-AI Collaboration ohne direkten Bezug zu sozialarbeiterischen Kontexten.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'AI_Literacies' und 'KI_Sonstige' auf allgemeine LLM-Bias-Messungen aus, ohne zu erkennen, dass der Mensch die kritische Dimension von Bias/Ungleichheit in sozialarbeiterischen und diversitätsgerechten Kontexten als zentral bewertet. Das LLM interpretiert 'Bias' technisch statt sozial-kritisch.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "200",
      "title": "Introducing Generative Artificial Intelligence into the MSW Curriculum: A Proposal for the 2029 Educational Policy and Accreditation Standards",
      "author_year": "Rodriguez (2024)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "AI_Literacies, KI_Sonstige, Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Fairness",
      "n_affected": 7,
      "llm_confidence": 0.6,
      "llm_reasoning": "Paper behandelt Steerability von LLMs (Generative_KI: Ja, Prompting: Ja). Dies sind Technik-Kategorien. Jedoch fehlt jeglicher Bezug zu sozialen Aspekten (Bias, Fairness, Soziale Arbeit, etc.). Ohne Abstract kann keine substantielle Einschätzung des Inhalts erfolgen, aber auf Basis des Titels liegt ein rein technisches Paper zu LLM-Kontrollierbarkeit vor. TECHNIK_OK, aber SOZIAL nicht erfüllt → Exclude.",
      "pattern": "Implizite Feldzugehoerigkeit",
      "justification": "Das LLM erkannte nicht, dass eine Publikation über die Integration von Generativer KI in die MSW-Curricula (Master of Social Work) inhärent sozialarbeitsfachliche und normative Dimensionen (Bias, Fairness, Diversität) behandelt, die über reine Technik hinausgehen. Der Titel signalisiert ein Curriculum- und Akkreditierungspapier im Kontext Sozialer Arbeit, was dem LLM als implizite Feldverwandtschaft entging.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "145",
      "title": "AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?",
      "author_year": "Latif (2024)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "AI_Literacies, Generative_KI, KI_Sonstige, Bias_Ungleichheit, Gender, Fairness",
      "n_affected": 6,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper adressiert AI Literacy (kritisches Verständnis von KI-Limitations und vertrauensbasierter Reliance), KI_Sonstige (Decision-Support-Systeme), und Fairness (angemessenes Vertrauen, bias-freie Entscheidungen). ABER: Kein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder feministischer Theorie. Rein HCI-/Human-AI-Interaction-Forschung ohne sozialarbeiterischen oder kritischen Ungleichheits-Fokus. Sozialarbeitsbezug nicht substantiell gegeben.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'AI Literacies', 'Fairness' und 'KI_Sonstige' zu breit aus und erfasst damit HCI-Forschung ohne sozialarb. Kontext, während es gleichzeitig die substantiellen Dimensionen 'Gender' und 'Bias_Ungleichheit' ignoriert, die der Mensch korrekt als zentral erkannt hat.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "152",
      "title": "Künstliche Intelligenz in der Sozialen Arbeit: Grundlagen für Theorie und Praxis",
      "author_year": "Linnemann (2025)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "KI_Sonstige, Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Fairness",
      "n_affected": 6,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper erfüllt TECHNIK-Kriterium mit 3 Ja-Kategorien (AI_Literacies: User-Trust-Kompetenz; Generative_KI: LLMs; Prompting: Prompt-Engineering-Vorschläge). Jedoch keine SOZIAL-Kategorien erfüllt: Kein direkter Sozialarbeitsbezug, keine Ungleichheits-, Gender-, Diversitäts-, feministischen oder Fairness-Analysen. Rein kognitive Vertrauensstudie ohne sozialgerechte Dimensionen.",
      "pattern": "Implizite Feldzugehoerigkeit",
      "justification": "Das LLM erkannte nicht, dass ein Paper über KI-Vertrauen in der Sozialen Arbeit (Titel!) implizit zum Feld der Sozialen Arbeit gehört und soziale Dimensionen wie Fairness und Bias in diesem Kontext automatisch relevant werden. Es wendete starre Kategorie-Checklisten an, statt den interdisziplinären Zusammenhang zu erfassen.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "171",
      "title": "Advancing Accountability in AI",
      "author_year": "OECD (2023)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "Generative_KI, Prompting, KI_Sonstige, Bias_Ungleichheit, Diversitaet, Fairness",
      "n_affected": 6,
      "llm_confidence": 0.85,
      "llm_reasoning": "Das Paper behandelt substantiell AI_Literacies (Vertrauen, Kompetenzen im Umgang mit LLMs) und Generative_KI sowie Prompting-Strategien. Jedoch fehlt vollständig der zweite erforderliche Bereich (Sozial): keine direkte Soziale_Arbeit, keine Bias_Ungleichheit, kein Gender/Diversität/Feminismus/Fairness-Fokus. Es ist eine reine Nutzerpsychologie-Studie zu LLM-Vertrauen ohne sozialen Impact-Fokus. Die Prompt-Engineering-Erwähnung ist eher ein Implikation, nicht substantiell. TECHNIK_OK=Ja, SOZIAL_O",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM hat 'Generative_KI' und 'Prompting' als substantiell kodiert, obwohl diese nur implizit/oberflächlich erwähnt sind, während es gleichzeitig 'Bias_Ungleichheit' und 'Diversität' als nicht substantiell ausschloss, obwohl der OECD-Accountability-Fokus genau diese Dimensionen adressiert. Die Kategoriegrenzen wurden asymmetrisch gedehnt.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "175",
      "title": "Towards Substantive Equality in Artificial Intelligence: Transformative AI Policy for Gender Equality and Diversity",
      "author_year": "Ovalle (2024)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "Prompting, Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch",
      "n_affected": 6,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper adressiert AI-Vertrauen, Transparenz und Fairness-Aspekte (TECHNIK-Seite erfüllt). Jedoch fehlt der Sozialarbeitsbezug und kein expliziter Fokus auf Ungleichheit oder Marginalisierung. Allgemeine organisationale Trust-Studie ohne Sozialbereich-Spezifika.",
      "pattern": "Implizite Feldzugehoerigkeit",
      "justification": "Das LLM erkannte nicht, dass ein Paper über Geschlechterparität und Diversität in der KI-Governance intrinsisch zur Sozialen Arbeit gehört und Marginalisierung adressiert. Es fokussierte auf oberflächliche technische Aspekte (Trust, Fairness) statt die transformative Sozialpolitik-Dimension zu verstehen.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "180",
      "title": "BBQ: A hand-built bias benchmark for question answering",
      "author_year": "Parrish (2022)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "Generative_KI, KI_Sonstige, Bias_Ungleichheit, Gender, Diversitaet, Fairness",
      "n_affected": 6,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper untersucht LLM-Fähigkeiten zur Vertrauensanalyse zwischen Individuen. Während Generative_KI (LLMs) relevant ist, fehlt jeglicher substantieller Bezug zu den sozialen Kategorien: kein direkter Sozialarbeitsbezug, keine Bias/Ungleichheit-Analyse, kein Gender-, Diversitäts- oder Fairness-Fokus. Die Vertrauensforschung ist allgemein sozialwissenschaftlich, nicht sozialarbeiterisch spezifisch. TECHNIK_OK, aber SOZIAL_NICHT_OK → Exclude.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Generative_KI' zu eng aus und kodiert 'Bias/Fairness in QA-Systemen' nicht als genuine Sozialarbeits-Inhalte, während es die implizite Feldrelevanz von BBQ für Fairness/Bias-Arbeit (zentral für Soziale Arbeit) übersieht.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "153",
      "title": "More or less wrong: A benchmark for directional bias in LLM comparative reasoning",
      "author_year": "Liu (2025)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "AI_Literacies, Generative_KI, Prompting, Gender, Diversitaet",
      "n_affected": 5,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper erfüllt TECHNIK-Kriterium (AI_Literacies, KI_Sonstige) und SOZIAL-Kriterium (Bias_Ungleichheit, Fairness). JEDOCH: Thema ist klinische Entscheidungsunterstützung im Gesundheitswesen, NICHT Soziale Arbeit. Keine Verknüpfung zu sozialarbeiterischer Praxis, Theorie oder Zielgruppen. Bias/Fairness-Diskussion bezieht sich auf medizinische Kontexte, nicht auf Soziale Arbeit. Daher: außerhalb des Scope des Literaturreviews zu KI in Sozialer Arbeit.",
      "pattern": "Keyword-Inklusion",
      "justification": "Das LLM hat die Keywords 'Bias', 'Fairness' und 'LLM' erkannt und automatisch die Kategorien positiv markiert, ohne zu erkennen, dass der klinische Anwendungskontext außerhalb des Soziale-Arbeit-Fokus liegt. Der Mensch erkannte korrekt, dass die fachliche Domäne (Gesundheitswesen, nicht Soziale Arbeit) die Inklusion rechtfertigt, während das LLM nur oberflächliche Keyword-Matches identifizierte.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "163",
      "title": "Biases in large language models: Origins, inventory and discussion",
      "author_year": "Navigli (2023)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "Prompting, Bias_Ungleichheit, Gender, Diversitaet, Fairness",
      "n_affected": 5,
      "llm_confidence": 0.85,
      "llm_reasoning": "Das Paper behandelt LLMs (Generative_KI = Ja) und untersucht deren Fähigkeiten zur Analyse von Vertrauen. Es erfüllt aber KEINE der erforderlichen SOZIAL-Kategorien: Kein Bezug zu Sozialer Arbeit, keine Behandlung von Bias/Ungleichheit, Gender, Diversität, feministischen Perspektiven oder Fairness. Die Studie zu Trust-Reasoning ist rein technisch-kognitiv ausgerichtet, ohne sozialkritischen oder anwendungsbezogenen Fokus auf vulnerable Gruppen oder strukturelle Ungleichheiten.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorie 'Bias_Ungleichheit' zu eng aus und erkennt nicht, dass die Untersuchung von Biases in LLMs selbst ein relevantes Thema für die Schnittstelle von KI und sozialen Implikationen darstellt. Der Mensch versteht implizit, dass technische Bias-Analyse einen sozial-kritischen Gehalt hat, den das LLM als 'rein technisch' klassifiziert.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "202",
      "title": "AI Literacy: A Framework to Understand, Evaluate, and Use Emerging Technology",
      "author_year": "Ruiz (2024)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "AI_Literacies, Prompting, Bias_Ungleichheit, Gender, Diversitaet",
      "n_affected": 5,
      "llm_confidence": 0.35,
      "llm_reasoning": "Der Titel deutet auf LLM-basierte Klassifikation von Mentalhygiene-Daten hin (Generative_KI + KI_Sonstige erfüllt, TECHNIK_OK). Jedoch: kein Abstract verfügbar, daher nicht erkennbar, ob ein substantieller Bezug zu sozialen/ethischen Dimensionen (SOZIAL_OK) besteht. Mental Health ist nicht automatisch Soziale Arbeit. Ohne Volltext: konservative Einstufung, da SOZIAL-Dimensionen nicht verifiable sind.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM hat die Kategorie SOZIAL_OK zu restriktiv interpretiert und verlangt explizite Inhaltsverifikation, während der Mensch erkannt hat, dass ein Framework zu AI Literacy inhärent soziale/ethische Dimensionen (Bias, Gender, Diversität) impliziert. Das LLM dehnte die technische Anforderung aus, während es soziale Kontexte unterdehnte.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "121",
      "title": "Evaluating gender bias in large language models via chain-of-thought prompting",
      "author_year": "Kaneko (2024)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "Prompting, Bias_Ungleichheit, Gender, Fairness",
      "n_affected": 4,
      "llm_confidence": 0.95,
      "llm_reasoning": "Das Paper behandelt rein technische Aspekte von LLM-Inferenz (Nondeterminismus, Sampling, Reproducibility). Während es Generative KI erwähnt, fehlt jeder soziale Bezug: keine AI Literacies, keine Bias/Fairness-Analyse, keine Bezüge zu Soziale Arbeit, Diversität oder Ungleichheit. Es ist ein rein technisches Konzeptpaper ohne gesellschaftliche, ethische oder pädagogische Dimension.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorie 'Prompting' zu eng aus (nur technische Chain-of-Thought-Inferenz) und erkennt nicht, dass empirische Bias-Evaluationen via Prompting dem Forschungsfeld inhärent angehören. Es unterschätzt systematisch, wie Bias/Gender/Fairness durch Prompting-Methoden untersucht werden.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "159",
      "title": "Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia",
      "author_year": "Mei (2023)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "AI_Literacies, Bias_Ungleichheit, Gender, Diversitaet",
      "n_affected": 4,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper behandelt Chatbot-Transparenz und Nutzerwahrnehmung mit Fokus auf AI Literacy (Effekt bei niedriger Vorkenntnis) und Prompting (Transparenz durch Prompt-Engineering). Technische Kriterien erfüllt. Es fehlt jedoch jeglicher Bezug zu sozialen Kategorien: Keine Soziale Arbeit, keine Analyse von Bias/Ungleichheit, kein Gender-, Diversitäts- oder Fairness-Fokus. Reine HCI-Studie ohne Bezug zu marginalizierten Gruppen, struktureller Benachteiligung oder Sozialarbeitspraxis.",
      "pattern": "Implizite Feldzugehoerigkeit",
      "justification": "Der Titel erwähnt explizit 'stigmatized social groups', 'nationality prejudice' und 'psychophobia' – das LLM erkannte diese Keywords nicht als Indikatoren für Bias/Ungleichheit, Diversität und Gender-relevante Inhalte, die zum sozialen/kritischen KI-Diskurs gehören. Das LLM klassifizierte die Arbeit als reine HCI-Studie, obwohl sie sich dezidiert mit Diskriminierung marginalisierter Gruppen auseinandersetzt.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "189",
      "title": "DR.GAP: Mitigating bias in large language models using gender-aware prompting with demonstration and reasoning",
      "author_year": "Qiu (2025)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "Prompting, Gender, Diversitaet, Fairness",
      "n_affected": 4,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper behandelt Fairness in Large Language Models (Generative_KI: Ja, Fairness: Ja, Bias_Ungleichheit: Ja). Jedoch fehlt der erforderliche SOZIAL-Kontext: Es gibt keinen direkten Bezug zu Sozialer Arbeit, Gender-Perspektive, Diversität oder feministischen Ansätzen. Allgemeine Fairness-Diskussionen in KI ohne Sozialarbeitsbezug sind nicht relevant für das Literature Review zu KI und Sozialer Arbeit. TECHNIK-Bedingung erfüllt, aber SOZIAL-Bedingung nicht erfüllt → Exclude.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Fairness' zu einer eigenständigen Kategorie aus und gewichtet technische Fairness-Aspekte höher als die implizite Anforderung, dass Gender/Diversität nur im Kontext von Sozialer Arbeit relevant sind. Die menschliche Entscheidung erkennt Gender und Diversität als feldspezifisch relevant an, während das LLM diese Kategorien narrower interpretiert.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "254",
      "title": "Intersectionality in artificial intelligence: Framing concerns and recommendations for action",
      "author_year": "Ulnicane (2024)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "Gender, Diversitaet, Feministisch, Fairness",
      "n_affected": 4,
      "llm_confidence": 0.85,
      "llm_reasoning": "Das Paper behandelt algorithmische Systeme (KI_Sonstige: Ja) und Machtungleichgewichte durch Big Tech (Bias_Ungleichheit: Ja). Allerdings fehlt ein direkter Bezug zu Sozialer Arbeit oder deren Praxis/Zielgruppen. Die Analyse fokussiert auf politische Wirtschaft und staatliche Souveränität, nicht auf Sozialbereich. SOZIAL-Bedingung ist nicht erfüllt.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM hat die Kategorien Gender, Diversität und Feministisch zu eng definiert und erkannt nicht, dass Intersectionality als konzeptioneller Rahmen diese Dimensionen inhärent umfasst, auch wenn sie nicht explizit im Titel genannt sind. Der Human erkannte die semantische Breite dieser Begriffe im Kontext von KI-Kritik korrekt.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "262",
      "title": "Recommendation on the Ethics of Artificial Intelligence",
      "author_year": "UNESCO (2021)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "AI_Literacies, Soziale_Arbeit, Gender, Feministisch",
      "n_affected": 4,
      "llm_confidence": 0.85,
      "llm_reasoning": "Das Paper erfüllt die SOZIAL-Bedingung (Bias_Ungleichheit, Diversitaet, Feministisch, Fairness alle Ja) und auch die TECHNIK-Bedingung (KI_Sonstige Ja). Jedoch ist dies eine Keynote-Zusammenfassung von fundamentaler Kritischer Techniktheorie/Philosophie ohne direkten Bezug zu Sozialer Arbeit als Praxis oder Disziplin. Das Literature Review fokussiert auf KI-Anwendungen in der Sozialen Arbeit, nicht auf allgemeine KI-Ethik und Gesellschaftskritik. Benjamin's Werk ist relevant für theoretische Gru",
      "pattern": "Implizite Feldzugehoerigkeit",
      "justification": "Das LLM erkannte nicht, dass eine UNESCO-Empfehlung zu KI-Ethik für das Feld Soziale Arbeit relevant ist, obwohl der Human diese Verbindung als selbstverständlich einstuft. Das LLM interpretierte das Dokument als zu allgemein und philosophisch, ohne das implizite Wissen zu haben, dass normative KI-Governance-Dokumente grundlegend für Sozialarbeitspraxis sind.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "188",
      "title": "Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review",
      "author_year": "Prakash (2023)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "Prompting, Gender, Diversitaet",
      "n_affected": 3,
      "llm_confidence": 0.3,
      "llm_reasoning": "Titel deutet auf Generative KI und Bias hin, aber ohne Abstract oder Volltext ist die Einordnung spekulativ. Sicher sind: Generative_KI=Ja (ChatGPT/Claude), Bias_Ungleichheit=Ja (Titel erwähnt 'Bias'). ABER: Kein Bezug zu Sozialer Arbeit erkennbar. Die Themen 'ethical dilemmas' sind philosophisch, nicht sozialarbeitsbezogen. Daher TECHNIK_OK (Generative_KI), aber SOZIAL nicht erfüllt (Bias allein ohne Sozialarbeitsbezug reicht nicht, und ohne Text kann nicht überprüft werden, ob tatsächlicher Fo",
      "pattern": "Implizite Feldzugehoerigkeit",
      "justification": "Das LLM erkannte nicht, dass Prompt Engineering zur Bias-Mitigation im Kontext von sozialen Gruppen (Arabs/Muslims) eine relevante Anwendung für Soziale Arbeit darstellt. Es fehlte die implizite Verbindung zwischen technischer Intervention und Feldverantwortung für vulnerable Populationen.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "168",
      "title": "Opportunities, challenges and school strategies for integrating generative AI in education",
      "author_year": "Ng (2025)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "Prompting, Bias_Ungleichheit",
      "n_affected": 2,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper behandelt LLM-Vertrauen und individuelle Unterschiede (AI_Literacies, Generative_KI erfüllt). ABER: Kein Bezug zu sozialen Ungleichheiten, Bias, Gender, Diversität, Fairness oder Sozialer Arbeit. Prompting wird nur beiläufig erwähnt ('prompt-engineering'), nicht substantiell behandelt. Psychologische Vertrauensforschung ohne sozialen/kritischen Bezug. SOZIAL-Kriterien nicht erfüllt → Exclude.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnte die Kategorie 'Prompting' zu eng aus und forderte substantielle Behandlung, während der Mensch beiläufige Erwähnungen als ausreichend bewertete. Gleichzeitig erkannte das LLM nicht, dass 'Vertrauen in KI' und 'individuelle Unterschiede' implizit Bias- und Ungleichheitsfragen in Bildungskontexten adressieren, was der Mensch korrekt unter 'Bias_Ungleichheit' subsumierte.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "169",
      "title": "The role of artificial intelligence (AI) and machine learning in social work practice",
      "author_year": "Nuwasiima (2024)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "Soziale_Arbeit, Diversitaet",
      "n_affected": 2,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper behandelt AI-Kompetenzen (Vertrauen, Transparenzverstaendnis) und KI-Sonstige (klinische Entscheidungssysteme) sowie Bias/Fairness. ABER: Fokus liegt auf Healthcare/klinischem Kontext, nicht auf Soziale Arbeit. Keine direkter Bezug zu sozialarbeiterischer Praxis, Theorie oder Zielgruppen. SOZIAL-Kriterium nicht erfuellt: Bias_Ungleichheit und Fairness im Healthcare-Kontext sind nicht spezifisch fuer Soziale Arbeit. Paper adressiert Technik + allgemeine Ethik, aber nicht die sozialen Dimens",
      "pattern": "Implizite Feldzugehoerigkeit",
      "justification": "Das LLM erkannte nicht, dass ein Paper über AI in Social Work Practice trotz Healthcare-Kontexts zur Sozialen Arbeit gehört, da es implizites Feldwissen über die Disziplin und ihre Anwendungsbereiche fehlte. Der Titel signalisiert explizit den Sozialarbeit-Bezug, den das LLM durch zu strikte Kontextgrenzen (Healthcare = nicht Soziale Arbeit) übersah.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "176",
      "title": "AI literacy and trust: A multi-method study of Human-GAI team collaboration",
      "author_year": "Pan (2025)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "Bias_Ungleichheit, Fairness",
      "n_affected": 2,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper erfüllt TECHNIK-Kriterium (AI_Literacies: Ja - Nutzer-Kompetenzen/Wissen über KI; Generative_KI: Ja - Chatbot-fokussiert; Prompting: Ja - Transparenz-Strategien für Prompt-Engineering erwähnt). Aber SOZIAL-Kriterium nicht erfüllt: Kein direkter Bezug zu Sozialer Arbeit, keine substantielle Behandlung von Bias, Gender, Diversität, Feminismus oder Fairness. Allgemeine Trust/UX-Studie ohne Fokus auf soziale Ungleichheit oder marginalisierte Gruppen.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die SOZIAL-Kriterien zu eng aus und erkennt nicht, dass 'Trust' und 'Human-AI Collaboration' in diesem Kontext implizit Fairness- und Bias-Aspekte adressieren. Der Mensch erfasst die implizite normative Dimension (Bias_Ungleichheit, Fairness), die das LLM als nicht-substantiell einstuft.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "219",
      "title": "Can prompt modifiers control bias? A comparative analysis of text-to-image generative models",
      "author_year": "Shin (2024)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "Diversitaet, Fairness",
      "n_affected": 2,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper erfüllt TECHNIK-Bedingung (Generative_KI, Prompting). Jedoch fehlt SOZIAL-Bedingung für Anwendungsgebiet mit sozialem Fokus: Bias_Ungleichheit, Gender und Fairness behandeln allgemeine LLM-Bias, nicht spezifisch Soziale Arbeit, marginalisierte Communities oder sozialarbeiterische Kontexte. Reine KI-Ethik-Studie ohne Sozialarbeitsbezug → Exclude für dieses Review.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die SOZIAL-Bedingung restriktiv aus, indem es fordert, dass Bias-Themen explizit auf Sozialarbeit/marginalisierte Communities bezogen sein müssen, während der Human die allgemeine Relevanz von Bias in generativen Modellen als ausreichend für Inklusion bewertet. Dies ist eine Überinterpretation der Kategoriengrenzen.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "204",
      "title": "What’s in a name? Auditing large language models for race and gender bias",
      "author_year": "Salinas (2025)",
      "human_decision": "Include",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Include_Agent_Exclude",
      "severity": 3,
      "affected_categories": "Diversitaet",
      "n_affected": 1,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper behandelt Prompting-Strategien zur Reduktion von Gender-Bias in LLMs—klare technische Komponente mit Fairness-Fokus. Allerdings fehlt jeder Bezug zu Sozialer Arbeit, Zielgruppen sozialer Dienste oder strukturellen sozialen Implikationen. Es ist eine Standard-Debiasing-Studie ohne sozialarbeitlichen Kontext.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM hat die Kategorie 'Diversität' zu eng definiert und nur technische Diversity-Metriken berücksichtigt, während der Mensch 'Diversität' semantisch breiter als intersektionales Konzept verstand, das race und gender bias umfasst. Dies führte zu unterschiedlichen Schwellwerten für die Inklusion derselben inhaltlichen Komponente.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "165",
      "title": "Conceptualizing AI literacy: An exploratory review",
      "author_year": "Ng (2021)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Generative_KI, Prompting, KI_Sonstige, Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Fairness",
      "n_affected": 8,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, Prompting=Ja, AI_Literacies=Ja durch Fokus auf Bildung und kritisches Verständnis von LLMs) und SOZIAL_OK (Soziale_Arbeit=Ja, direkter Bezug zu Forschung und Praxis in Social Work). Framework für verantwortungsvollen KI-Einsatz mit Dokumentations- und Transparenzanforderungen ist substantiell.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'AI_Literacies' und 'Generative_KI' zu weit aus und interpretiert einen konzeptuellen Review zu AI Literacy als direkt relevant für Social Work Practice, obwohl der Paper keine praktische Anwendung in der Sozialen Arbeit demonstriert. Gleichzeitig übersieht es, dass der Human die Kategorien Bias, Gender, Diversität und Fairness erkannt hat, die das LLM nicht erfasst.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Generative_KI": {
          "human": "?",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "225",
      "title": "AI Creates the Message: Integrating AI Language Learning Models into Social Work Education and Practice",
      "author_year": "Singer (2023)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Prompting, KI_Sonstige, Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness",
      "n_affected": 8,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper behandelt generative KI (LLMs) mit substantiellem Fokus auf intersektionale Gender-Rassen-Biases. Explizite Bezüge zu Intersektionalitätstheorie (feministische Perspektive nach Crenshaw). Prompting wird als Interventionsstrategie adressiert. Erfüllt beide Bedingungen: TECHNIK (Generative_KI + Prompting) und SOZIAL (Bias_Ungleichheit + Gender + Diversitaet + Feministisch + Fairness).",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorie 'Prompting' über ihre operative Technik-Dimension hinaus aus und interpretiert theoretische Diskussionen von KI-Biases als 'Interventionsstrategie', ohne dass Prompting als primäres methodisches Instrument im Paper belegt ist. Dies führt zu einer Überinklusivität durch Bedeutungserweiterung statt zu echter Keyword-Erkennung.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "130",
      "title": "Trustworthy AI and the Logics of Intersectional Resistance",
      "author_year": "Knowles (2023)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, Generative_KI, Prompting, KI_Sonstige, Gender, Feministisch, Fairness",
      "n_affected": 7,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper behandelt substantiell AI_Literacies (user guidelines und training für LLM-Nutzung), Generative_KI (LLM-fokussiert), Prompting (diversity-sensitive prompting techniques explizit genannt) und adressiert Bias_Ungleichheit sowie Diversität (diversity-sensitive approaches). Erfüllt TECHNIK (3 Kategorien) und SOZIAL (2 Kategorien). Kein direkter Sozialarbeitsbezug identifizierbar.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt AI_Literacies und Prompting über ihre beabsichtigte Reichweite aus, indem es allgemeine LLM-Nutzungsrichtlinien und diversity-sensitive techniques als technische Kernthemen klassifiziert, obwohl der Paper primär intersektionale Widerstands-Logiken behandelt. Gleichzeitig erkennt es nicht, dass Gender und Feminismus (Human=Ja) zentral zum kritischen Rahmen gehören.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "147",
      "title": "Development of the “Scale for the assessment of non-experts’ AI literacy” – An exploratory factor analysis",
      "author_year": "Laupichler (2023)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, Generative_KI, Prompting, KI_Sonstige, Bias_Ungleichheit, Diversitaet, Fairness",
      "n_affected": 7,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI, Prompting substantiell behandelt), SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Fairness adressiert). Systematische Review zu Prompt-Engineering-Techniken für Bias-Mitigation gegen arabische/muslimische Minderheiten in LLMs. Direkter Fokus auf kulturelle Diskriminierung und Fairness-Optimierung.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Bias_Ungleichheit', 'Diversität' und 'Fairness' auf ein rein technisches Paper über AI-Literacy-Skalen aus, obwohl diese Kategorien im Kontext des Reviews auf soziale Gerechtigkeit und Diskriminierungsfolgen abzielen. Die halluzinierte 'Systematische Review zu Prompt-Engineering für Bias-Mitigation gegen Minderheiten' existiert nicht im Paper.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "210",
      "title": "Exploring opportunities and risks in decision support technologies for social workers: An empirical study in the field of disabled people's services",
      "author_year": "Schneider (2022)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Generative_KI, Prompting, KI_Sonstige, Soziale_Arbeit, Bias_Ungleichheit, Gender, Fairness",
      "n_affected": 7,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper behandelt substantiell Generative KI (LLMs), Prompting (Chain-of-Thought als zentrale Intervention), Gender-Bias in LLM-Outputs und Fairness-Aspekte (Bias-Reduktion). Erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting) und SOZIAL_OK (Bias_Ungleichheit + Gender + Fairness). Experimentelle Evaluationsstudie mit klarem Fokus auf Prompt-Strategien zur Bias-Mitigation.",
      "pattern": "Keyword-Inklusion",
      "justification": "Das LLM dehnt 'decision support technologies' zu Generative KI und Prompting aus, obwohl der Paper primär traditionelle Decision-Support-Systeme für Sozialarbeit behandelt. Der Human erkannte korrekt, dass die KI-Technologie sekundär ist und das Paper hauptsächlich im Feld der Sozialen Arbeit angesiedelt ist.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "243",
      "title": "AI implementation science for social issues: Pitfalls and tips",
      "author_year": "Takaoka (2022)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness",
      "n_affected": 7,
      "llm_confidence": 0.95,
      "llm_reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (Policy-Rahmen für AI-Governance), KI_Sonstige (algorithmische Diskriminierung). SOZIAL: Bias_Ungleichheit (Fokus auf algorithmische Diskriminierung), Gender (explizite intersektionale Genderanalyse), Diversitaet (inklusive Repräsentation), Feministisch (intersektionale Analyse nach Crenshaw-Logik), Fairness (substantive equality vs. formale Fairness). Transformatives Policy-Framework mit starkem Fokus auf strukturelle Gerechtigkeit.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorien AI_Literacies, Bias_Ungleichheit, Gender, Diversität und Feministisch über ihren intendierten Scope aus, indem es ein generisches Policy-Paper zu AI-Governance als spezifische soziale Interventionen klassifiziert. Der Mensch erkennt dagegen, dass das Paper primär Soziale Arbeit ist und nicht die spezifischen technologie- und genderorientierten Kategorien erfüllt.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "258",
      "title": "Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action",
      "author_year": "Ulnicane (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, Generative_KI, KI_Sonstige, Gender, Diversitaet, Feministisch, Fairness",
      "n_affected": 7,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper erfüllt TECHNIK-Kriterium durch AI_Literacies (Fokus auf kritische KI-Kompetenzentwicklung), Generative_KI (Erwähnung von Language Models) und SOZIAL-Kriterium durch Bias_Ungleichheit (expliziter Fokus auf Bias-Mechanismen) und Fairness (Verständnis von Fairness als zentrales Element). Empirische Studie mit großer Stichprobe (n=605). Kein Soziale_Arbeit-Bezug erkennbar.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'AI_Literacies' und 'Generative_KI' zu weit aus, indem es eine konzeptionelle Arbeit zu Intersektionalität und KI-Bias als technisches KI-Papier klassifiziert, während es gleichzeitig die genuinen sozialwissenschaftlichen Kategorien (Gender, Feministisch, Diversität) unterschätzt, die den eigentlichen Fokus des Papers bilden.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "267",
      "title": "RETHINKING SOCIAL SERVICES WITH ARTIFICIAL INTELLIGENCE: OPPORTUNITIES, RISKS, AND FUTURE PERSPECTIVES",
      "author_year": "Unknown (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Generative_KI, Prompting, KI_Sonstige, Soziale_Arbeit, Diversitaet, Feministisch, Fairness",
      "n_affected": 7,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting), SOZIAL_OK (Bias_Ungleichheit + Gender + Feministisch + Fairness). Empirische Analyse von Gender-Bias in GPT-4/Copilot-generierten Policies mit intersektionalem Rahmen. Fokus auf Androzentrizität und strukturelle Biases entspricht feministischer Technikforschung. Keine direkte Soziale-Arbeit-Verbindung, aber Policymaking ist relevant für soziale Systeme.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Generative_KI' und 'Prompting' auf einen Paper aus, der primär KI-Policy-Bias analysiert, nicht aber generative KI-Systeme oder Prompting-Techniken direkt untersucht. Gleichzeitig überdehnt es 'Feministisch' und 'Fairness' auf konzeptionelle Bias-Kritik, während der Human die fehlende direkte Soziale-Arbeit-Verankerung als Ausschlusskriterium wertet.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "276",
      "title": "Rationalisierung durch Digitalisierung?",
      "author_year": "Waag (2023)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, Generative_KI, Soziale_Arbeit, Bias_Ungleichheit, Diversitaet, Feministisch, Fairness",
      "n_affected": 7,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies: Training zu KI-Konzepten; KI_Sonstige: AI Bias Detection/Mitigation Tools; Fairness: Algorithmen-Fairness) + SOZIAL (Bias_Ungleichheit: Fokus auf Bias Detection; Diversitaet: vulnerable/marginalized stakeholder groups, inklusive AI-Entwicklung; Feministisch: Co-creation mit marginalisierten Gruppen, Machtstrukturen in AI-Design). Substantieller Fokus auf Training zur Befähigung von Non-Technical Stakeholders und kritische Reflexion von Ink",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt Kategorien wie 'AI_Literacies', 'Bias_Ungleichheit', 'Diversität' und 'Feministisch' zu weit aus, indem es allgemeine Diskussionen über marginalisierte Gruppen und Co-creation als substantiellen technologischen oder intersektionalen Fokus interpretiert, obwohl der Paper primär sozialarbeiterisch ausgerichtet ist. Die menschliche Entscheidung erkennt, dass der Fokus auf Soziale Arbeit zentral ist und die anderen Kategorien nicht hinreichend erfüllt sind.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "287",
      "title": "Broadening artificial intelligence education in K-12: where to start?",
      "author_year": "Wong (2020)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, Generative_KI, Prompting, KI_Sonstige, Bias_Ungleichheit, Diversitaet, Fairness",
      "n_affected": 7,
      "llm_confidence": 0.92,
      "llm_reasoning": "Das Paper erfüllt beide Bedingungen: (1) TECHNIK: Generative_KI (Text-to-Image Modelle), Prompting (Analyse von Prompt-Modifizierern zur Bias-Reduktion) → Ja. (2) SOZIAL: Bias_Ungleichheit (explizit: Analyse gesellschaftlicher Biases), Diversitaet (diversity-reflective prompting), Fairness (ethische KI-Entwicklung) → Ja. Empirische Vergleichsstudie mit substantiellem Fokus auf Prompt-Engineering und algorithmischen Bias.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorien 'Generative_KI', 'Prompting' und 'Bias_Ungleichheit' auf einen K-12 Bildungskontext aus, der primär um AI Literacy und Curriculum Design geht, nicht um technische Implementierung oder empirische Bias-Analyse. Der Human erkannte korrekt, dass das Paper konzeptionell-pädagogisch ist, nicht technisch-empirisch.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "116",
      "title": "Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes",
      "author_year": "Kamruzzaman (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, Prompting, Soziale_Arbeit, Bias_Ungleichheit, Gender, Fairness",
      "n_affected": 6,
      "llm_confidence": 0.85,
      "llm_reasoning": "Der Titel deutet auf eine Analyse von ChatGPT (generative KI) in der Sozialen Arbeit hin. Dies erfüllt beide notwendigen Bedingungen: TECHNIK (Generative_KI = Ja, da ChatGPT explizit genannt) und SOZIAL (Soziale_Arbeit = Ja, da direkt im Titel). AI_Literacies ist ebenfalls Ja, da es um Kompetenzen/Risiken im Umgang mit ChatGPT geht. Ohne Abstract kann Studientyp nicht genau bestimmt werden.",
      "pattern": "Keyword-Inklusion",
      "justification": "Das LLM hat 'Prompting techniques' und 'social bias' als explizite Keywords erkannt und automatisch AI_Literacies zugeordnet, ohne zu erkennen, dass der Paper sich auf allgemeine LLM-Bias (nicht auf Soziale Arbeit) bezieht. Die Inklusion basiert auf oberflächlicher Keyword-Matching statt kontextueller Relevanz für das Feld.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "128",
      "title": "Data feminism for AI",
      "author_year": "Klein (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, Prompting, KI_Sonstige, Gender, Diversitaet, Feministisch",
      "n_affected": 6,
      "llm_confidence": 0.82,
      "llm_reasoning": "Paper untersucht LLMs (Generative_KI: Ja) systematisch mit 43.200 Prompts (Prompting: Ja, substantiell). Zentrale Ergebnisse adressieren Bias und Malleabilität sozialer Normen (Bias_Ungleichheit: Ja) sowie Fairness-Aspekte bei moralischen Urteilen (Fairness: Ja). TECHNIK und SOZIAL erfüllt → Include. Kein direkter Soziale-Arbeit-Bezug, aber relevante Erkenntnisse zu KI-Systemen und Ungleichheiten.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Prompting' und 'Generative KI' zu weit aus und übersieht die konzeptionellen Besonderheiten von Data Feminism (Gender, Feministisch) als eigenständige theoretische Perspektive, die über technische KI-Bias-Analysen hinausgeht.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "140",
      "title": "Digitalität und Digitalisierung als Gegenstand der Sozialen Arbeit",
      "author_year": "Kutscher (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, KI_Sonstige, Soziale_Arbeit, Bias_Ungleichheit, Diversitaet, Fairness",
      "n_affected": 6,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper behandelt XAI als Kompetenz- und Transparenzthema (AI_Literacies), mit Fokus auf algorithmische Systeme (KI_Sonstige). Substantielle Thematisierung von Bias/Ungleichheit durch Marginalisierung, Diversität durch explizite Nennung marginalisierter Gruppen und Fairness durch ethische/gerechte Entscheidungsfindung. TECHNIK und SOZIAL erfüllt → Include.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Bias/Ungleichheit', 'Diversität' und 'Fairness' auf allgemeine sozialpolitische Diskurse aus, ohne dass diese spezifisch im Kontext algorithmischer Systeme behandelt werden. Der Human erkennt korrekt, dass die Thematisierung von Digitalisierung in der Sozialen Arbeit nicht ausreicht für die KI-spezifischen Kategorien.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "151",
      "title": "Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit: Eine exemplarische arbeitsfeldübergreifende Betrachtung des Natural Language Processing (NLP)",
      "author_year": "Linnemann (2023)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Prompting, KI_Sonstige, Soziale_Arbeit, Bias_Ungleichheit, Gender, Fairness",
      "n_affected": 6,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper erfüllt beide Inklusionskriterien: TECHNIK_OK (Generative_KI, Prompting), SOZIAL_OK (Bias_Ungleichheit, Gender, Fairness). Qualitative Studie zu LLM-Bias mit explizitem Gender-Fokus. Prompting wird als Bias-Mitigationsstrategie thematisiert. Kein Sozialarbeitsbezug, keine explizit feministische Theorie.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Prompting' und 'Fairness' als relevante Kategorien aus, obwohl diese im Paper nur als technische Randthemen behandelt werden, und übersieht, dass der fehlende explizite Sozialarbeitsbezug ein zentrales Ausschlusskriterium darstellt.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "209",
      "title": "Der Einfluss der Algorithmen: Neue Qualitäten durch Big Data Analytics und Künstliche Intelligenz",
      "author_year": "Schneider (2018)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Generative_KI, KI_Sonstige, Soziale_Arbeit, Bias_Ungleichheit, Diversitaet, Fairness",
      "n_affected": 6,
      "llm_confidence": 0.75,
      "llm_reasoning": "Das Paper behandelt Large Language Models (Generative_KI=Ja) und fokussiert auf Bias-Mitigation bezüglich Altersmerkmalen (Bias_Ungleichheit=Ja, Diversitaet=Ja, Fairness=Ja). Titel und Fokus auf 'responsible AI development' und 'mitigating bias' sind substantiell. TECHNIK_OK (Generative_KI) + SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Fairness) = Include. Kein direkter Bezug zu Sozialer Arbeit identifizierbar, aber Bias/Fairness-Fokus ausreichend für Inklusion.",
      "pattern": "Keyword-Inklusion",
      "justification": "Das LLM interpretiert 'Big Data Analytics und KI' als Generative_KI und dehnt 'Bias-Mitigation' zu einem eigenständigen Inklusionskriterium, ohne zu erkennen, dass das Paper primär ein technisches KI-Paper ohne Sozialer-Arbeit-Bezug ist. Die fehlende explizite Verbindung zum Anwendungsfeld Soziale Arbeit wird nicht als Ausschlusskriterium gewichtet.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "213",
      "title": "Indecision on the use of artificial intelligence in healthcare: A qualitative study of patient perspectives on trust, responsibility and self-determination using AI-CDSS",
      "author_year": "Schneider (2025)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Generative_KI, Prompting, KI_Sonstige, Soziale_Arbeit, Bias_Ungleichheit, Fairness",
      "n_affected": 6,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper behandelt systematisch Prompting-Strategien (Ja: substantieller Fokus auf 12 Prompt-Varianten) zur Bias-Reduktion in LLMs (Ja: Generative_KI). Bias-Reduktion und Fairness (Ja: beide Kategorien) sind zentrale Themen. TECHNIK-Bedingung erfüllt (Prompting + Generative_KI), SOZIAL-Bedingung erfüllt (Bias_Ungleichheit + Fairness). Kein Bezug zu Sozialer Arbeit oder Gender-spezifisch, kein feministischer Rahmen.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Bias-Reduktion in Healthcare-KI' als generisch auf Generative_KI und Prompting aus, obwohl die Studie qualitativ Patientenperspektiven auf KI-CDSS (Clinical Decision Support Systems) untersucht – nicht auf Prompt-Engineering oder generative Modelle. Der Human erkannte korrekt, dass dies eher zu KI_Sonstige und Soziale_Arbeit (Patient Trust/Autonomie) gehört.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "230",
      "title": "In search of artificial intelligence (AI) literacy in teacher education: A scoping review",
      "author_year": "Sperling (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, Generative_KI, Gender, Diversitaet, Feministisch, Fairness",
      "n_affected": 6,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper behandelt KI-Governance (KI_Sonstige: Ja) mit explizit feministischer Theorie (intersektionale Analyse nach Crenshaw-Tradition, Feministisch: Ja). Adressiert strukturelle Ungleichheiten, Bias und Gerechtigkeit (Bias_Ungleichheit, Gender, Diversitaet, Fairness alle Ja). Erfüllt beide Bedingungen: TECHNIK (KI_Sonstige) UND SOZIAL (Feministisch + multiple Dimensionen). Theoretischer Policy-Rahmen mit intersektionalem Fokus.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM hat 'AI literacy in teacher education' über-interpretiert als intersektionale feministische Governance-Analyse und dehnte damit die Kategorien Gender/Diversität/Feministisch auf einen Paper aus, der primär pädagogische Kompetenzentwicklung behandelt, nicht strukturelle Ungleichheitsanalyse. Die hohe Confidence (0.95) trotz fehlender expliziter Hinweise auf feministische Theorie im Titel deutet auf unkritische Bedeutungsexpansion hin.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "274",
      "title": "Recommendations for social work researchers and journal editors on the use of generative AI and large language models",
      "author_year": "Victor (2023)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Generative_KI, Prompting, Soziale_Arbeit, Diversitaet, Feministisch, Fairness",
      "n_affected": 6,
      "llm_confidence": 0.92,
      "llm_reasoning": "Das Paper behandelt substantiell feministische Theorie und Perspektiven im KI-Kontext (KI_Sonstige: Ja). Es adressiert explizit feministische Ansätze, Machtstrukturen und soziale Gerechtigkeit (Feministisch: Ja). Gender und Diversität sind zentral (Gender: Ja, Diversitaet: Ja). Bias/Ungleichheit wird durch die Fokussierung auf Machtrelationen thematisiert (Bias_Ungleichheit: Ja). TECHNIK erfüllt (KI_Sonstige), SOZIAL erfüllt (multiple Kategorien). → Include",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorien 'Feministisch', 'Fairness' und 'Diversität' auf ein Papier aus, das primär methodologische Empfehlungen für Social-Work-Forschung gibt, ohne dass feministische Theorie oder explizite Fairness-Konzepte den Kern des Beitrags bilden. Die Kategorien werden durch die Präsenz von Machtrelationen und sozialer Gerechtigkeit (als implizite Kontexte) überdehnbar interpretiert.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "293",
      "title": "Transparency enhances positive perceptions of social artificial intelligence",
      "author_year": "Xu (2023)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Generative_KI, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness",
      "n_affected": 6,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies, Generative_KI, KI_Sonstige durch intersektionale AI-Strategien und Chatbot-Beispiel) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch durch explizite intersektionale und queer-feministische Theorie nach Crenshaw; Fairness durch Data Nutrition Label für Bias-Assessment). Substantiell behandelt.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorien 'Generative_KI', 'Bias_Ungleichheit', 'Gender', 'Diversitaet' und 'Feministisch' über ihre intendierten Grenzen aus, indem es generische AI-Systeme (Chatbots) und theoretische Bezüge zu intersektionaler Theorie als substantielle Erfüllung dieser spezifischen Kategorien interpretiert, ohne dass die Paper diese Themen als Kernfokus behandelt.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "127",
      "title": "Data Feminism for AI",
      "author_year": "Klein (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, KI_Sonstige, Gender, Diversitaet, Feministisch",
      "n_affected": 5,
      "llm_confidence": 0.85,
      "llm_reasoning": "Das Paper behandelt substantiell Generative KI (Kategorie erfüllt) und adressiert Bias-Amplifikation, Machtungleichgewichte und Fairness-Fragen (Bias_Ungleichheit und Fairness erfüllt). Es erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja) und SOZIAL_OK (Bias_Ungleichheit=Ja, Fairness=Ja). Keine Sozialen-Arbeit-Perspektive erkennbar, daher diese Kategorie Nein.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM hat die Kategorien Generative_KI und Fairness korrekt erkannt, überdehnt aber den Bedeutungsraum dieser Kategorien als ausreichend für Inclusion, während es gleichzeitig die thematische Breite von Data Feminism (AI_Literacies, Gender, Diversität, Feministisch) systematisch untererkannt und damit den interdisziplinären Kontext des Papiers nicht erfasst hat.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "135",
      "title": "Artificial Intelligence (AI) literacy – an argument for AI literacy in education",
      "author_year": "Kong (2025)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, Generative_KI, KI_Sonstige, Bias_Ungleichheit, Diversitaet",
      "n_affected": 5,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI: LLMs sind zentral), SOZIAL_OK (Bias_Ungleichheit: Analyse von Prejudice in LLM-Outputs; Diversitaet: Fokus auf marginalized groups; Fairness: Implikationen für vertrauenswürdige KI-Nutzung). Drei preregistrierte experimentelle Studien mit klarem Fokus auf Diskriminierung und deren Auswirkungen auf Nutzervertrauen.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Generative_KI' und 'Bias_Ungleichheit' zu weit aus: Der Fokus liegt auf AI literacy und Bildung, nicht auf generative KI-Systeme selbst. Die Analyse von LLM-Bias wird als Forschungsgegenstand (nicht als Kern-Methodologie) überinterpretiert.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "139",
      "title": "Positionings, challenges, and ambivalences in children's and parents' perspectives in digitalized familial contexts",
      "author_year": "Kutscher (2023)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Generative_KI, Prompting, Bias_Ungleichheit, Diversitaet, Fairness",
      "n_affected": 5,
      "llm_confidence": 0.85,
      "llm_reasoning": "Das Paper behandelt Prompting (Dipper-Framework mit diversen Prompts) und Generative KI (LLMs) substantiell. Es adressiert Diversity (marginalisierte Gruppen, Perspektivenvielfalt), Bias_Ungleichheit (Verbesserung von Darstellung minorities) und Fairness (faire Output-Generierung). TECHNIK und SOZIAL sind erfüllt. Kein Soziale_Arbeit-Bezug, kein Gender- oder Feministisch-Fokus.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Diversität', 'Bias_Ungleichheit' und 'Fairness' auf ein Paper aus, das primär familiale Digitalisierungskontexte aus Kinderperspektive behandelt, ohne dass diese Kategorien substantiell im KI-Kontext adressiert werden. Die Begriffe werden zu breit interpretiert, um auch soziologische Perspektivenvielfalt zu erfassen, statt sich auf KI-spezifische Bias- und Fairness-Probleme zu fokussieren.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "162",
      "title": "Feministische Netzpolitik und Künstliche Intelligenz in der politischen Bildung [Feminist network politics and artificial intelligence in political education]",
      "author_year": "Mosene (2023)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, Soziale_Arbeit, Gender, Diversitaet, Feministisch",
      "n_affected": 5,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: (1) TECHNIK: AI_Literacies (Rahmenmodell für KI-Integration in Praxis), KI_Sonstige (algorithmische Systeme im Sozialbereich), Fairness (Bias-Mitigation, Transparenz). (2) SOZIAL: Soziale_Arbeit (direkter Fokus auf Integrationsstrategie in sozialarbeiterischer Praxis), Bias_Ungleichheit (explizite Behandlung von Bias-Mitigation), Fairness. EPIC-Modell ist substantielle konzeptuelle Beitrag mit Ethik, Policy und Community-Perspektive für KI in Sozialer Arbeit.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt AI_Literacies und Soziale_Arbeit über ihre Kerndefnitionen hinaus aus, indem es feministische Netzpolitik als KI-Literacies-Rahmenwerk und politische Bildung als sozialarbeiterische Praxis interpretiert. Der Mensch erkennt korrekt, dass der Schwerpunkt auf Gender/Feminismus liegt, nicht auf KI-Integration in der Praxis.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "174",
      "title": "Factoring the matrix of domination: A critical review and reimagination of intersectionality in AI fairness",
      "author_year": "Ovalle (2023)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Generative_KI, Prompting, KI_Sonstige, Gender, Diversitaet",
      "n_affected": 5,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, Prompting=Ja) und SOZIAL_OK (Bias_Ungleichheit=Ja, Fairness=Ja). Substantieller Fokus auf Prompt-Engineering-Strategien zur Bias-Reduktion in LLMs. Direkte Behandlung sozialer Bias und Fairness-Ansätze. Nicht auf Soziale Arbeit fokussiert, aber klar KI-Bias-Thematik mit fairness-Implikation.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM interpretierte 'intersectionality' und 'matrix of domination' als direkte Generative_KI- und Prompting-Inhalte, obwohl die Arbeit primär intersektionale Theorie auf KI-Fairness anwendet. Gender und Diversität wurden vom LLM übersehen, weil es die theoretischen Kernkategorien des Papiers nicht korrekt als sozialwissenschaftliche (nicht technische) Prompt-Engineering-Strategien klassifizierte.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "195",
      "title": "Artificial intelligence in social work: Emerging ethical issues",
      "author_year": "Reamer (2023)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Prompting, KI_Sonstige, Soziale_Arbeit, Gender, Feministisch",
      "n_affected": 5,
      "llm_confidence": 0.85,
      "llm_reasoning": "Das Paper untersucht Bias in GPT (Generative_KI: Ja) durch eine intersektionale Fallstudie (Feministisch: Ja - Intersektionalitätsansatz nach Crenshaw). Es thematisiert Diskriminierung gegen stigmatisierte Gruppen (Bias_Ungleichheit: Ja) und Diversität von Gruppen (Diversitaet: Ja). Fairness ist impliziert durch Bias-Analyse. TECHNIK und SOZIAL sind erfüllt → Include.",
      "pattern": "Implizite Feldzugehoerigkeit",
      "justification": "Das LLM erkannte die expliziten Keywords (Bias, Fairness, Diversität, Intersektionalität) und aktivierte eine Include-Entscheidung, übersah aber das implizite Feldwissen, dass ein Paper über ethische Fragen in der Sozialen Arbeit (nicht technisch-fokussiert) und Gender-Aspekte als zentrale Kategorien hätte klassifizieren müssen – der Human erkannte diese domänen-spezifischen Kontexte, das LLM nicht.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "201",
      "title": "Ethical issues related to the use of technology in social work practice: A systematic review",
      "author_year": "Rodríguez-Martínez (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Generative_KI, Prompting, Soziale_Arbeit, Bias_Ungleichheit, Fairness",
      "n_affected": 5,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper fokussiert auf Prompting-Techniken (substantiell im Titel) und Generative KI (LLMs). Adressiert social bias reduction - direkt relevant für Bias_Ungleichheit und Fairness. TECHNIK erfüllt (Generative_KI + Prompting), SOZIAL erfüllt (Bias_Ungleichheit + Fairness). Kein Soziale_Arbeit-Bezug, aber algorithmische Bias-Reduktion ist relevant für LLM-Fairness insgesamt.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Bias_Ungleichheit' und 'Fairness' auf allgemeine algorithmische Bias-Reduktion aus, obwohl der Paper spezifisch auf Soziale Arbeit fokussiert und nicht primär um LLM-Fairness geht. Das LLM überkompensiert die fehlende Soziale_Arbeit-Kategorie durch Überinterpretation von Bias-Aspekten.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "246",
      "title": "A systematic review on digital literacy",
      "author_year": "Tinmaz (2022)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness",
      "n_affected": 5,
      "llm_confidence": 0.95,
      "llm_reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmische Governance und AI-Systeme sind substantiell adressiert) und SOZIAL_OK (Bias_Ungleichheit, Gender, Diversität und explizit Feministisch durch Intersektionalitätsanalyse nach Crenshaw-Perspektive, Fairness durch Kritik ungleicher AI-Harm-Verteilung). Kritische Analyse von AI Governance aus intersektionaler, feministischer Perspektive mit Fokus auf marginalisierte Gruppen.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorien Bias_Ungleichheit, Gender, Diversität und Feministisch auf ein Paper über digitale Literatur aus, das zwar KI-Governance behandelt, aber nicht primär um diese sozialen Dimensionen strukturiert ist. Die starke Konfidenz (0.95) trotz massiver Kategorie-Divergenz deutet auf Überinterpretation von möglicherweise vorhandenen aber marginalen Keywords hin.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "273",
      "title": "Recommendations for social work researchers and journal editors on the use of generative AI and large language models",
      "author_year": "Victor (2023)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Generative_KI, Prompting, Soziale_Arbeit, Gender, Fairness",
      "n_affected": 5,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja, da es sich um Studie zu AI-Development befasst) und SOZIAL_OK (Bias_Ungleichheit=Ja: 'injustices and inequalities'; Fairness=Ja: Responsibility und strukturelle Herausforderungen). Kein Bezug zu Sozialer Arbeit oder feministischer Theorie erkennbar. Empirische Studie mit N=32 Professionals.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Fairness' und 'Responsibility' aus einemMethodologie-Paper zu KI-Governance als erfüllende soziale Gerechtigkeit-Kriterien aus, ohne zu erkennen, dass das Paper nicht auf Soziale Arbeit anwendbar ist. Gleichzeitig übersieht es die fehlende praktische Relevanz für das Feld (Human erkannte korrekt: kein Bezug zu Sozialer Arbeit oder feministischer Theorie).",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "283",
      "title": "Messung von AI Literacy – Empirische Evidenz und Implikationen",
      "author_year": "Weber (2023)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness",
      "n_affected": 5,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies: Ja – Framework für KI-Kompetenzen/Werte-Integration; KI_Sonstige: Ja – Systemische Analyse von AI-Biases). SOZIAL_OK (Bias_Ungleichheit: Ja – adressiert Biases und Injustices; Gender: Ja – expliziter Gender-Fokus; Diversitaet: Ja – Inklusion; Feministisch: Ja – intersektionale Feminismus, feministische Theorie; Fairness: Ja – Gerechtigkeit und Equity als zentrale Werte). Praktische Anwendungen (MIRA, EU AI Act) zeigen Substanz.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt Kategorien wie 'Bias_Ungleichheit', 'Gender' und 'Feministisch' auf eine technische AI-Literacy-Arbeit aus, die diese Dimensionen möglicherweise nur peripheral behandelt. Der Mensch erkennt, dass die primäre Substanz im technischen Bereich liegt und soziale/feministische Themen nicht zentral genug sind für Inklusion.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "303",
      "title": "Learning About AI: A Systematic Review of Reviews on AI Literacy",
      "author_year": "Zhang (2025)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, Bias_Ungleichheit, Gender, Diversitaet, Fairness",
      "n_affected": 5,
      "llm_confidence": 0.92,
      "llm_reasoning": "TECHNIK_OK: KI_Sonstige=Ja (algorithmische Systeme und deren Auswirkungen). SOZIAL_OK: Bias_Ungleichheit=Ja (Gender-Bias in 44% der KI-Systeme), Gender=Ja (expliziter Gender-Fokus), Fairness=Ja (Policy-Empfehlungen). Kein direkter Soziale_Arbeit-Bezug erkennbar. Nicht feministisch theoretisch fundiert, aber empirisch-analytisch zu Gender-Bias.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorien Bias_Ungleichheit, Gender, Diversität und Fairness über ihre kontextuelle Relevanz hinaus aus. Der Paper behandelt AI Literacy allgemein mit empirischen Gender-Daten, nicht aber Gender-Bias als zentrales Forschungsthema der Sozialen Arbeit oder feministische Perspektiven auf KI.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "131",
      "title": "Large language models are zero-shot reasoners",
      "author_year": "Kojima (2022)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Prompting, Bias_Ungleichheit, Gender, Diversitaet",
      "n_affected": 4,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper untersucht systematisch Bias und Diskriminierung in LLM-generierten Narrativen von queeren Personen. Erfüllt TECHNIK-Kriterium (Generative_KI: Fokus auf LLM-Outputs) und SOZIAL-Kriterium (Bias_Ungleichheit: Stereotypisierung marginalisierter Gruppen; Gender: queere Identitäten; Diversitaet: Repräsentation von LGBTQ+-Personen in KI-Systemen). Keine explizite feministische Theorie erkannt, daher Feministisch=Nein.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorien Bias_Ungleichheit, Gender und Diversitaet über ihre Kernkriterien hinaus aus, indem es allgemeine Erwähnungen von queeren Identitäten und LGBTQ+-Repräsentation als thematische Zugehörigkeit interpretiert, ohne dass der Paper explizit sozialpolitische oder diversitätsorientierte Forschung betreibt. Der Human erkennt korrekt, dass der Fokus auf eine technische Methode (Chain-of-Thought-Prompting) liegt, nicht auf die sozialen Implikationen.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "183",
      "title": "A Literature Review of Digital Literacy over Two Decades",
      "author_year": "Peng (2022)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, Generative_KI, Prompting, Soziale_Arbeit",
      "n_affected": 4,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, Prompting=Ja, AI_Literacies=Ja durch Fokus auf Wissen/Kompetenzen für Forschende) und SOZIAL_OK (Soziale_Arbeit=Ja durch expliziten Bezug zu Social-Work-Forschung und -Praxis). Entwickelt Framework für KI-Einsatz in Sozialarbeit mit Empfehlungen für Transparenz, Dokumentation und kontinuierliche Bildung.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'AI_Literacies' und 'Generative_KI' auf einen allgemeinen Review über digitale Literatur aus, obwohl der Paper sich mit Digital Literacy (nicht AI Literacy) befasst und keine generative KI behandelt. Die Kategorien werden über ihre intendierte Bedeutung hinaus expandiert.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Gender": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "278",
      "title": "Measuring user competence in using artificial intelligence: validity and reliability of artificial intelligence literacy scale",
      "author_year": "Wang (2023)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Bias_Ungleichheit, Diversitaet, Feministisch, Fairness",
      "n_affected": 4,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige, Fairness behandeln KI-Systeme und Kompetenzentwicklung). SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Feministisch durch intersektionale Perspektive, expliziter Fokus auf Machtstrukturen und marginalisierte Gruppen). Intersektionalität ist feministische Theorie (Crenshaw). Framework für verantwortungsvolle AI-Entwicklung mit gesellschaftlichem Kontext.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'AI Literacy' und 'Fairness' zu breiten sozialwissenschaftlichen Kategorien (Bias, Diversität, Feministisch) aus, ohne dass der Paper explizit diese Perspektiven als Kernfokus behandelt. Die intersektionale Interpretation wird vom LLM hinzugefügt, nicht aus dem Paper abgelesen.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "284",
      "title": "Discriminating Systems: Gender, Race, and Power in AI",
      "author_year": "West (2023)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Generative_KI, Prompting, KI_Sonstige, Fairness",
      "n_affected": 4,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper behandelt Generative KI (Bildgeneratoren) mit substantiellem Fokus auf Prompt-Engineering durch automatisierte Prompt-Rewriting. Adressiert demografische Bias-Reduktion explizit durch Gender-, Ethnizität- und Altersrepräsentation. Fairness-Metriken zur Bias-Messung zentral. TECHNIK erfüllt (Generative_KI + Prompting), SOZIAL erfüllt (Bias_Ungleichheit + Gender + Diversitaet + Fairness). Keine feministische Theorie-Nutzung, kein SA-Bezug.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Generative_KI' und 'Prompting' auf allgemeine KI-Bias-Studien aus, obwohl der Paper primär theoretisch-soziologisch ist. Die Human-Einordnung unter 'KI_Sonstige' signalisiert, dass KI hier Kontextstoff ist, nicht methodologischer Fokus.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "137",
      "title": "How AI hype impacts the LGBTQ+ community",
      "author_year": "Kumar (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Generative_KI, KI_Sonstige, Feministisch",
      "n_affected": 3,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper erfüllt TECHNIK-Kriterium (Generative_KI: LLMs) und SOZIAL-Kriterium (Bias_Ungleichheit, Gender, Diversitaet, Fairness). Systematische Analyse von sozialen Biases in LLMs mit Fokus auf Geschlecht, Ethnie, sexuelle Orientierung, Alter und Religion. Behandelt Messungs- und Mitigationsstrategien. Nicht feministisch (keine explizite feministische Theorie), nicht zu Sozialer Arbeit.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorie 'Generative_KI' zu weit aus, indem es ein Paper über AI-Hype und LGBTQ+ Auswirkungen als technisches Paper klassifiziert, obwohl es primär eine sozialwissenschaftliche Kritik ist. Gleichzeitig erkennt das LLM nicht, dass die explizit feministische Perspektive des Papers (Human-Klassifizierung) es aus dem Corpus ausschließen sollte.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "292",
      "title": "What is Feminist AI?",
      "author_year": "Wudel (2025)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, Feministisch, Fairness",
      "n_affected": 3,
      "llm_confidence": 0.88,
      "llm_reasoning": "Paper erfüllt BEIDE Bedingungen: TECHNIK (AI_Literacies: digitale Literacy als Intervention + KI_Sonstige: Bias in Hiring/Healthcare/Finance-Systemen) UND SOZIAL (Gender: expliziter Gender-Bias-Fokus; Bias_Ungleichheit: Underrepresentation von Frauen; Diversitaet: Women-led AI projects; Fairness: algorithmic bias combating). Substantielle Behandlung von Gender und Bias in KI-Systemen. Keine explizit feministische Theorie erkennbar, daher Feministisch=Nein.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'AI_Literacies' auf jede KI-bezogene Bildungsdimension aus (hier: Gender-Bias-Awareness), obwohl die Kategorie spezifisch auf digitale Kompetenzvermittlung abzielt. Gleichzeitig klassifiziert das LLM das Paper als nicht-feministisch, während der Mensch die explizit feministische Perspektive ('Feminist AI' im Titel) als zentral erkennt—ein Verständnis der impliziten theoretischen Rahmung, die das LLM unterschätzt.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "291",
      "title": "What is Feminist AI?",
      "author_year": "Wudel (2025)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, Fairness",
      "n_affected": 2,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: NLP/ML für Datenanalyse; AI_Literacies: feministische digitale Kompetenzen) und SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness substantiell behandelt). Empirische Analyse von 172K Tweets zu Geschlechter-Perspektiven auf algorithmischen Bias mit explizit feministischem Rahmen.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'AI_Literacies' (digitale Kompetenzen im Bildungskontext) auf theoretische Analyse von feministischen Perspektiven auf KI aus und interpretiert 'Fairness' als behandelt, obwohl das Paper primär Geschlechter-Bias in Tweets analysiert, nicht algorithmische Fairness direkt adressiert.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "299",
      "title": "Tensions in digital welfare states: Three perspectives on care and control",
      "author_year": "Zakharova (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies, KI_Sonstige",
      "n_affected": 2,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch AI_Literacies (Framework für KI-Kompetenzen/ethische Prinzipien) und KI_Sonstige (ML-Design, algorithmische Systeme). SOZIAL_OK durch Feministisch (explizite Data Feminism Theorie nach D'Ignazio), Bias_Ungleichheit (Fokus auf diskriminatorische Systeme), Gender (Geschlechterperspektive), Diversitaet (Pluralismus, marginalisierte Gruppen), Fairness (gerechte KI-Entwicklung). Substantielle Behandlung aller Dimensionen.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt AI_Literacies und KI_Sonstige auf einen Policy-/Governance-Text aus, der algorithmische Systeme nur als Kontext für sozialpolitische Analyse nutzt, nicht als primären Gegenstand. Die sozialen Kategorien (Gender, Feministisch, Fairness) werden aktiviert, obwohl sie hier ohne technische KI-Dimension vorliegen.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "136",
      "title": "Feministische KI – Künstliche Intelligenz für alle? [Feminist AI – Artificial intelligence for everyone?]",
      "author_year": "Kubes (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Generative_KI",
      "n_affected": 1,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + KI_Sonstige: LLM-basierte Entscheidungssysteme), SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch: intersektionale Analyse von Geschlechts- und Rassenbias). Explizit intersektionales Design nach Crenshaw-Ansatz (Gender × Rasse). Substanzielle empirische Analyse mit sozialen Implikationen für algorithmic fairness.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorie 'Generative_KI' aus, um allgemeine KI-Bias-Analysen einzubeziehen, obwohl der Paper nicht spezifisch generative Modelle als Forschungsobjekt behandelt, sondern feministische Perspektiven auf KI-Systeme allgemein. Die Human-Entscheidung zu Exclude basiert wahrscheinlich auf der Abwesenheit von generativen Modellen als primärem Fokus.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "256",
      "title": "Artificial Intelligence and Intersectionality",
      "author_year": "Ulnicane (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Fairness",
      "n_affected": 1,
      "llm_confidence": 0.95,
      "llm_reasoning": "Das Paper behandelt substantiell algorithmische Fairness und Bias in ML-Systemen (KI_Sonstige, Fairness). Es greift EXPLIZIT auf Crenshaw's intersektionale Theorie zurück und analysiert Diskriminierungsmechanismen aus intersektionaler Perspektive (Feministisch, Bias_Ungleichheit, Gender, Diversitaet). TECHNIK-Bedingung erfüllt (KI_Sonstige=Ja), SOZIAL-Bedingung erfüllt (Bias_Ungleichheit, Gender, Diversitaet, Feministisch = alle Ja). Include-Schwelle erreicht.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM hat 'Fairness' als erfüllte Kategorie kodiert, obwohl der Human diese ablehnt – trotz Übereinstimmung in allen anderen technischen und sozialen Kategorien. Dies deutet auf eine Überausdehnung des Fairness-Konzepts hin, wo das LLM algorithmische Fairness/Bias-Analyse automatisch als 'Fairness'-Kategorie interpretiert, während der Human eine engere oder andersartige Definition dieser Kategorie anzulegen scheint.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "257",
      "title": "Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action",
      "author_year": "Ulnicane (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies",
      "n_affected": 1,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper behandelt substantiell AI Literacy durch feministische Perspektive (Kubes, Gender-kritisch). Analysiert KI-Anwendungen im Kontext Bias (androzentrisch, eurozentrisch, kapitalistisch-patriarchal), Geschlechterperspektive und Inklusion. Praktischer Bezug durch Curriculum und Workshops. Beide Bedingungen erfüllt: TECHNIK (AI_Literacies, KI_Sonstige), SOZIAL (Feministisch, Gender, Bias_Ungleichheit, Diversitaet).",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'AI_Literacies' auf feministische Perspektiven und intersektionale Analysen von KI-Bias aus, obwohl der Paper primär eine kritische Gesellschaftsanalyse ist, nicht eine Vermittlung von KI-Verständnis/Kompetenz. Die Kategorie wird über ihre technische Literacy-Dimension hinaus erweitert.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "259",
      "title": "Intersectionality in artificial intelligence: Framing concerns and recommendations for action",
      "author_year": "Ulnicane (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "Fairness",
      "n_affected": 1,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (KI_Sonstige: algorithmische Systeme für Recruitment/Resume-Screening) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet: explizite Analyse von Rassismus und Geschlechterdiskriminierung; Feministisch: intersektionale Analyse nach Crenshaw-Logik; Fairness: algorithmic fairness in hiring). Substanzielle empirische Studie mit klarem intersektionalem Fokus.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorie 'Fairness' über algorithmische Fairness hinaus auf intersektionale Gerechtigkeitsanalyse aus, während der Human diese als eigenständige technische Kategorie enger fasst. Die divergente Kodierung bei Fairness ist keine Keyword-Übereinstimmung, sondern eine unterschiedliche Interpretation des Kategorienumfangs.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "270",
      "title": "Introduction to the digital welfare state: Contestations, considerations and entanglements",
      "author_year": "van Toorn (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "AI_Literacies",
      "n_affected": 1,
      "llm_confidence": 0.82,
      "llm_reasoning": "Paper behandelt kritische AI Literacies (TECHNIK) und thematisiert strukturelle Ungleichheiten bezüglich Rasse, Geschlecht und Arbeit (SOZIAL). Expliziter Fokus auf Bias/Ungleichheit und Gender. Kritische Perspektive auf Strukturen deutet auf feministische Ansätze hin. Beide Bedingungen erfüllt → Include.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'AI_Literacies' auf allgemeine digitale Literacies aus und interpretiert 'digital welfare state' als KI-bezogen, obwohl der Paper primär wohlfahrtsstaatliche Strukturen behandelt. Die Überidentifikation mit Bias/Gender-Keywords führt zu einer Überinterpretation des KI-Bezugs.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "4",
      "title": "Feminist Perspectives on AI: Ethical Considerations in Algorithmic Decision-Making",
      "author_year": "Ahmed (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper adressiert zentral AI-Kompetenzen von Fachkräften (AI_Literacies: Ja - Kompetenzrahmen für Jugendhilfe), behandelt Generative AI (Ja - explizit Gen Z und generative Tools), hat direkten Bezug zu Sozialarbeit/Jugendhilfe (Ja - Youth Workers, Jugendhilfesektor). Empirische Studie mit Fokusgruppen und Interviews. Beide Bedingungen erfüllt: Technik (AI_Literacies + Generative_KI) UND Sozial (Soziale_Arbeit).",
      "pattern": "Keyword-Inklusion",
      "justification": "Das LLM dehnt 'AI_Literacies' und 'Generative_KI' auf einen rein theoretisch-philosophischen Artikel über feministische Perspektiven aus, obwohl dieser keine praktischen Kompetenzrahmen oder konkrete AI-Anwendungen für Fachkräfte behandelt. Der Mensch erkennt korrekt, dass ethische Reflexion nicht automatisch in den Kompetenzkatalog gehört.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "34",
      "title": "A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization",
      "author_year": "Bisconti (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper erfüllt beide Kriterien: (1) TECHNIK: KI_Sonstige=Ja (NLP für Risikobewertung, algorithmische Systeme im Sozialbereich); (2) SOZIAL: Soziale_Arbeit=Ja (Child Protective Services), Bias_Ungleichheit=Ja (racial bias, automated discrimination), Diversitaet=Ja (Black/Latinx families), Fairness=Ja (bias in risk scoring). Empirische Studie mit substantiellem Fokus auf diskriminatorische Auswirkungen von ML-Systemen in der Jugendhilfe.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorie 'Soziale_Arbeit' und 'Bias_Ungleichheit' auf ein philosophisches Paper über KI-Trustworthiness aus, das zwar Child Protective Services erwähnt, aber keine empirische Studie mit Feldforschung in der Sozialen Arbeit darstellt. Die Kategorien werden basierend auf thematischer Relevanz statt tatsächlicher disziplinärer Zugehörigkeit angewendet.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "51",
      "title": "Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals",
      "author_year": "Choudhury (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper behandelt KI-Kompetenzen und -Integration (AI_Literacies) sowie allgemeine KI-Governance (KI_Sonstige) im Kontext sozialer Innovationen und Dienstleistungen (Soziale_Arbeit). PRISM-Framework adressiert direkt Organisationen in sozialen Sektoren mit Readiness-Assessments. TECHNIK_OK (AI_Literacies + KI_Sonstige) und SOZIAL_OK (Soziale_Arbeit) erfüllt. Framework-Dokument ohne explizite Bias-, Gender-, oder Fairness-Fokussierung.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'AI_Literacies' und 'KI_Sonstige' zu weit aus, indem es allgemeine Vertrauens- und Deskilling-Diskurse als direkt framework-relevant interpretiert, obwohl der Paper primär ein Kritik-Essay ohne operationalisierbare KI-Kompetenz-Assessments darstellt. Die Kategorisierung unter 'Soziale_Arbeit' ist ebenfalls überdehnbar, da der Text eher health care professionals als klassische Sozialarbeiter adressiert.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "55",
      "title": "The influence of mental state attributions on trust in large language models",
      "author_year": "Colombatto (2025)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper erfüllt TECHNIK- und SOZIAL-Kriterien: (1) Generative_KI (LLM-basierter AI Coach), Prompting (strukturiertes Prompting für reflektive Praxis), AI_Literacies (Entwicklung von Self-Reflection-Kompetenzen). (2) Soziale_Arbeit (öffentliche Verwaltung, human services context, Kompetenzentwicklung). Empirische User Study mit klarem Fokus auf KI-gestützte Professionalisierung im sozialen Sektor.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'AI_Literacies' und 'Generative_KI' auf einen Paper über Vertrauensattribution und mentale Zustandszuschreibungen aus, obwohl dieser primär psychologisch-epistemisch ist und nicht um KI-Kompetenzentwicklung oder praktisches LLM-Prompting in sozialen Kontexten geht. Die Kategorien werden zu breit interpretiert.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "60",
      "title": "Measuring and identifying factors of individuals' trust in large language models",
      "author_year": "De Duro (2025)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.95,
      "llm_reasoning": "Systematic Review mit substanziellem Fokus auf AI Literacy und Computational Thinking in der Sozialen Arbeit (AI_Literacies: Ja). Direkter Bezug zu sozialarbeiterischer Praxis und Profession via Staub-Bernasconi's Triple Mandate (Soziale_Arbeit: Ja). Thematisiert auch algorithmische Systeme/KI-Systeme generell (KI_Sonstige: Ja). TECHNIK und SOZIAL beide erfüllt → Include.",
      "pattern": "Keyword-Inklusion",
      "justification": "LLM dehnt 'Trust in LLMs' und 'AI Literacy' zu breit aus und verbindet sie gezwungen mit Sozialer Arbeit via Staub-Bernasconi, obwohl die Paper primär auf Vertrauensmessung in KI-Systemen fokussiert, nicht auf sozialarbeiterische Praxis oder Literacy-Entwicklung im Berufsfeld.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "80",
      "title": "Bias, accuracy, and trust: Gender-diverse perspectives on large language models",
      "author_year": "Gaba (2025)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.85,
      "llm_reasoning": "Das Paper adressiert algorithmic decision-making in Sozialarbeit (KI_Sonstige + Soziale_Arbeit). Der Titel hebt die Competency/Critique-Dilemma hervor, was auf AI Literacies verweist. Bias, Fairness und ethische Bedenken in Algorithmen sind zentral (Bias_Ungleichheit + Fairness). TECHNIK erfüllt (3x Ja), SOZIAL erfüllt (4x Ja). Inklusion gerechtfertigt.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Bias' und 'Fairness' in algorithmischen Systemen als automatische Zugehörigkeit zu den Inklusionskriterien aus, ohne zu berücksichtigen, dass das Paper primär eine qualitative Gender-Perspektive-Studie ist, die nicht die technischen oder anwendungsorientierten Schwerpunkte des Feldes abdeckt. Die Überdehnung von KI_Sonstige + Soziale_Arbeit auf Basis des Titels ignoriert, dass keine substantielle Verbindung zu Sozialer Arbeit als Praxisfeld besteht.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "82",
      "title": "Bias and fairness in large language models: A survey",
      "author_year": "Gallegos (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper behandelt substantiell KI-Kompetenzen (AI_Literacies) und generative KI im MSW-Curriculum. Direkter Bezug zu Sozialer Arbeit (Ausbildung, Akkreditierungsstandards). Adressiert Bias, Transparenz und Fairness als zentrale Themen für verantwortungsvolle AI-Nutzung im Kontext von Klientenschutz und Equity. Erfüllt beide Bedingungen: TECHNIK (AI_Literacies + Generative_KI) und SOZIAL (Soziale_Arbeit + Bias_Ungleichheit + Fairness).",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'AI_Literacies' und 'Soziale_Arbeit' über ihre Kernintention aus: Ein Survey zu Bias in LLMs ist eine technische Publikation ohne direkten Bezug zu MSW-Curriculum oder Sozialarbeits-Praxis. Die angenommene 'Relevanz für Klientenschutz' ist eine implizite Anwendungskonstruktion, nicht die Paper-Substanz.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "94",
      "title": "Artificial intelligence in social work: An EPIC model for practice",
      "author_year": "Goldkind (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.85,
      "llm_reasoning": "Das Paper behandelt KI in der Sozialen Arbeit und adressiert dabei sowohl Kompetenzentwicklung (AI Literacies: reflektierte Implementationsstrategien, professioneller Diskurs) als auch allgemeine KI/algorithmische Systeme (KI_Sonstige) in sozialarbeiterischen Praxisfeldern (Soziale_Arbeit: direkter Bezug zur Praxis). Beide Bedingungen (TECHNIK + SOZIAL) sind erfüllt → Include. Bias/Fairness nicht substantiell behandelt basierend auf Abstract.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorie 'AI_Literacies' zu weit aus, indem es allgemeine KI-Anwendungen in der Sozialen Arbeit als 'reflektierte Kompetenzentwicklung' klassifiziert, während der Human wahrscheinlich eine spezifischere Definition (z.B. explizite Schulungs- oder Curriculum-fokussierte Inhalte) anwendet. Die Begründung vermischt unterschiedliche Abstraktion-Ebenen.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "95",
      "title": "Artificial intelligence in social work: An EPIC model for practice",
      "author_year": "Goldkind (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, Generative_KI, Prompting sind substantiell) + SOZIAL_OK (Soziale_Arbeit durch expliziten Hochschulbezug zur Soziale-Arbeit-Ausbildung). Prompting als metakognitive Schlüsselkompetenz ist Kernthema. Curriculare Integration in Soziale Arbeit direkt adressiert. Konzept/Werkstattbericht, kein empirisch-experimenteller Nachweis.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorie 'AI_Literacies' aus, indem es 'Prompting' als substanzielle technische Komponente interpretiert, während der Human diese als zu oberflächlich oder methodisch-anwendungsorientiert wertet. Gleichzeitig wird 'Soziale_Arbeit' beim LLM durch einen curricularen Bezug zur Ausbildung als erfüllend bewertet, während der Human einen direkteren Forschungsbezug zu Sozialer Arbeit vermisst.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "98",
      "title": "Künstliche Intelligenz in der Sozialen Arbeit – Zwischen Bedenken und Optionen",
      "author_year": "Gravelmann (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper erfüllt TECHNIK-Bedingung: AI_Literacies (Ja - untersucht digitale Kompetenzen und KI-Akzeptanz von Fachkräften) und KI_Sonstige (Ja - behandelt KI-Einsatz in Organisationen). Erfüllt SOZIAL-Bedingung: Soziale_Arbeit (Ja - direkter Fokus auf KI-Akzeptanz und Praxis in der Sozialen Arbeit, Bedenken zu Datenschutz und ethischen Herausforderungen). Empirische Studie mit Berufseinsteiger:innen. Inklusion berechtigt.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorie 'AI_Literacies' aus, um generische digitale Kompetenzen und KI-Akzeptanz zu erfassen, obwohl dies über die typische Definition hinausgeht. Der Mensch erkennt, dass bloße Erwähnung von KI-Bedenken in der Sozialen Arbeit nicht automatisch die spezifischen Inklusionskriterien erfüllt.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "120",
      "title": "Debiasing prompts for gender bias in large language models",
      "author_year": "Kaneko (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.92,
      "llm_reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (digitale Kompetenzen zur Empowerment), KI_Sonstige (algorithmische Systeme in Recruitment, Healthcare, Finance). SOZIAL: Gender (expliziter Gender-Bias-Fokus), Bias_Ungleichheit (Diskriminierung durch biased datasets und unter-representation), Diversitaet (Inklusion in AI-Entwicklung), Fairness (Bias-Mitigation). Kein explizit feministischer Rahmen erkennbar, aber substantielle Behandlung von Gender und Ungleichheit.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt Kategorien wie 'AI_Literacies' und 'Bias_Ungleichheit' zu weit aus und behandelt technische KI-Debiasing-Forschung als sozialwissenschaftlich relevant für ein Sozialen-Arbeit-Review, während der Mensch die fehlende praktische Anwendungsorientierung für Soziale Arbeit erkannt hat.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "191",
      "title": "Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy",
      "author_year": "Quaid-i-Azam University (2025)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.72,
      "llm_reasoning": "Das Paper behandelt Bias-Mitigation in LLMs mit Fokus auf altersbedingte Diskriminierung. Generative_KI (LLMs), Bias_Ungleichheit (Age Bias), Diversitaet (Age-related groups), und Fairness (Debiasing-Strategien) sind substantiell adressiert. TECHNIK_OK (Generative_KI=Ja) und SOZIAL_OK (Bias_Ungleichheit=Ja, Diversitaet=Ja, Fairness=Ja) erfüllt. Ohne Abstract ist Zuordnung unsicher, aber Titel deutet auf relevantes Thema hin.",
      "pattern": "Keyword-Inklusion",
      "justification": "Das LLM hat sich primär auf Keywords wie 'Gender', 'Bias', 'Digital Literacy' und 'Women Empowerment' im Titel konzentriert, ohne zu erkennen, dass das Paper sich mit Age Bias (nicht Gender Bias) befasst und damit außerhalb des Fokus auf geschlechtsspezifische Diskriminierung liegt. Die fehlende Abstractprüfung führte zu einer Fehlklassifikation durch oberflächliche Keyword-Matching.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "192",
      "title": "The Algorithmic Auditing Landscape: A Social Justice Approach",
      "author_year": "Raji (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.75,
      "llm_reasoning": "Das Paper adressiert explizit Bias-Mitigation in KI-generierten Narrativen mit Fokus auf Gender und Ethnizität. Generative_KI (narratives generation), Bias_Ungleichheit (gender and ethnicity bias), Gender (expliziter Gender-Fokus), Diversitaet (ethnicity representation) und Fairness (bias mitigation) sind substantiell behandelt. TECHNIK (Generative_KI) und SOZIAL (Bias_Ungleichheit, Gender, Fairness) sind erfüllt → Include. Confidence begrenzt durch fehlenden Abstract und unklaren Studientyp.",
      "pattern": "Keyword-Inklusion",
      "justification": "Das LLM hat Keywords wie 'Bias', 'Gender', 'Ethnizität' und 'Fairness' im Titel erkannt und mechanisch den Include-Schwellenwert überschritten, ohne zu erfassen, dass das Paper sich auf algorithmic auditing in einem allgemeinen Policy-Kontext bezieht und nicht auf die spezifische Intersection von KI + Sozialer Arbeit zielt, die das Inclusion-Kriterium erfordert.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "198",
      "title": "How can feminism inform AI governance in practice?",
      "author_year": "Ricaurte (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.85,
      "llm_reasoning": "BBQ ist ein etablierter Benchmark für Bias-Evaluationen in QA-Systemen (KI-Sonstige: Ja; Generative_KI: Ja, da oft für LLM-Evaluation genutzt). Das Paper adressiert substantiell Bias, Fairness und Diversität mit explizitem Gender-Fokus. Keine Soziale_Arbeit. Technik+Sozial erfüllt → Include.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Generative_KI' und 'KI_Sonstige' auf einen konzeptionellen/governanceorientierten Text aus, der sich nicht mit generativen Modellen oder technischen KI-Systemen befasst, sondern mit Feminist Theory als analytischer Rahmen für Governance. Der menschliche Bewerter erkannte korrekt, dass dies außerhalb des Scope liegt.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "211",
      "title": "Das verflixte Problem mit Klassifikationen: Zum Einfluss der Digitalisierung auf die Soziale Diagnostik in der Sozialen Arbeit",
      "author_year": "Schneider (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting substantiell behandelt), SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Fairness adressiert). Systematische Review zu Prompt-Engineering-Techniken für Bias-Mitigation gegen Arabs/Muslims in LLMs mit quantifizierten Ergebnissen. Fokus auf kulturelle Repräsentation und algorithmische Fairness. Kein direkter Sozialarbeitsbezug, aber relevantes Anwendungsfeld für diskriminierungssensible KI-Nutzung.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorien Bias_Ungleichheit, Diversität und Fairness auf ein allgemeines KI-Ethik-Paper aus, obwohl der Titel keinen expliziten Bezug zu Sozialer Arbeit hat und das Paper nicht in den definierten Scope (Generative KI + Soziale Arbeit) fällt. Die Überinterpretation von Anwendungsrelevanz führt zu einer unkritischen Inclusion.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "212",
      "title": "AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?",
      "author_year": "Schneider (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper erfüllt TECHNIK-Kriterium: Fokus auf Generative KI (ChatGPT-3.5/GPT-4), substantielle Analyse von Prompting-Strategien zur Bias-Reduktion. Erfüllt SOZIAL-Kriterium: Explizite Behandlung von Bias (kulturelle Stereotype), Diversität (diversity-sensitive prompts) und Fairness (Bias-Mitigation-Strategien). Empirische Studie mit drei experimentellen Phasen.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorien 'Generative_KI', 'Prompting', 'Bias_Ungleichheit', 'Diversitaet' und 'Fairness' zu weit aus und interpretiert eine allgemeine Diskussionspapier über KI-Futures als spezialisierte technische Studie zu Bias-Mitigation durch Prompting-Strategien. Die Kategorien werden nicht nach dem feldspezifischen Standard des Reviews angewendet.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "215",
      "title": "Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen Arbeit",
      "author_year": "Schönauer (2025)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper behandelt algorithmische Systeme und deren Schäden (KI_Sonstige=Ja). Community-geleiteter Ansatz zur fairen KI adressiert Fairness, Bias/Ungleichheit und Diversität. Queer-Fokus involviert Gender-Perspektive und intersektionale Analysen (feministisch). Erfüllt beide Bedingungen: Technik (KI_Sonstige) + Sozial (Bias, Gender, Diversität, Fairness, Feministisch).",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorie 'KI_Sonstige' zu weit aus, indem es ein Paper zur Sozialen Arbeit als KI-Paper klassifiziert, nur weil es algorithmische Systeme erwähnt. Der menschliche Assessor erkannte korrekt, dass das Paper primär ein Soziale-Arbeit-Paper ist, bei dem KI nur peripher behandelt wird, nicht umgekehrt.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "217",
      "title": "Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy",
      "author_year": "Shah (2025)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper erfüllt TECHNIK-Kriterien (AI_Literacies für KI-Kompetenzen/Beteiligung; KI_Sonstige für allgemeine KI-Entwicklung; Fairness für genderinklusive/faire KI-Systeme) und SOZIAL-Kriterien (Gender explizit im Titel; Bias_Ungleichheit durch Menschenrechtsfokus; Diversitaet durch globale Kooperation und Expertinnenbeteiligung marginalisierter Gruppen). Feministisch=Nein, da keine explizite feministische Theorie erkennbar ist.",
      "pattern": "Keyword-Inklusion",
      "justification": "Das LLM dehnt Kategorien wie 'AI_Literacies', 'Bias_Ungleichheit' und 'Gender' über ihre Kernbedeutung hinaus aus, indem es allgemeine Konzepte von Geschlechtergerechtigkeit und digitaler Befähigung als spezifische Fachinhalte klassifiziert. Die Inklusion basiert auf thematischer Relevanz statt auf expliziter methodischer/technischer Zugehörigkeit zum definierten Forschungsfeld.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "224",
      "title": "Feminist Data Set",
      "author_year": "Sinders (2017)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (KI_Sonstige: algorithmische Systeme in Robotik, HR-Tools) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch: Intersektionalität ist etablierter feministischer Forschungsrahmen nach Crenshaw; Fairness). Substantielle Behandlung von Vorurteilen in KI-Systemen und struktureller Benachteiligung.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorie 'KI_Sonstige' und 'Bias_Ungleichheit' zu weit aus, indem es einen Paper über feministische Datenethik als technisches KI-Paper klassifiziert, obwohl die Human-Entscheidung (Exclude) signalisiert, dass der Paper die spezifischen technischen/sozialen Arbeit-Anforderungen des Feldes nicht erfüllt. Das LLM überkategorisiert basierend auf thematischer Relevanz statt echter Feldintegration.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "232",
      "title": "Mitigating trust-induced inappropriate reliance on AI assistance",
      "author_year": "Srinivasan (2025)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.85,
      "llm_reasoning": "Der Bericht adressiert algorithmische Diskriminierung (KI_Sonstige: Ja) und behandelt substantiell Bias/Ungleichheit sowie Fairness im Kontext von Antidiskriminierungsrecht. Er erfüllt beide Bedingungen (TECHNIK + SOZIAL) und ist damit included, obwohl er keinen direkten Soziale-Arbeit-Bezug hat.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM hat 'trust-induced inappropriate reliance' als algorithmische Diskriminierung klassifiziert und damit die Kategorien Bias/Ungleichheit und Fairness überdehnt, obwohl der Fokus auf menschliches Vertrauen in KI-Assistenten liegt, nicht auf Diskriminierung. Die Einbeziehung basiert auf einer zu breiten Interpretation dieser Kategorien jenseits ihres beabsichtigten Scope.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "233",
      "title": "Mitigating trust-induced inappropriate reliance on AI assistance",
      "author_year": "Srinivasan (2025)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper analysiert EU AI Act durch die Linse von Geschlechtergerechtigkeit und Nichtdiskriminierung. TECHNIK erfüllt: KI_Sonstige (algorithmische Systeme reguliert durch AI Act). SOZIAL erfüllt: Bias_Ungleichheit (Geschlechterverzerrungen, Diskriminierungsrisiken), Gender (expliziter Gender-Fokus), Fairness (Bias-Audits, grundrechtliche Folgenabschätzungen). Keine Soziale_Arbeit, kein expliziter feministischer Theorierahmen erkennbar.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'KI_Sonstige' zu weit aus, indem es regulatorische Analysen des EU AI Act als technisches KI-Forschungsthema klassifiziert. Die Paper analysiert Governance und Geschlechtergerechtigkeit, nicht KI-Systeme oder deren technische Eigenschaften selbst.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "237",
      "title": "What large language models know and what people think they know",
      "author_year": "Steyvers (2025)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper behandelt KI/ML-Systeme (KI_Sonstige: mathematische Fairness-Metriken für Klassifikation). Substantieller Fokus auf intersektionale Fairness und algorithmischen Bias (Bias_Ungleichheit). Intersektionale Analyse nach Crenshaw ist explizit feministisches Konzept (Feministisch). Fairness-Metriken sind Kernthema (Fairness). Diversität durch Fokus auf Untergruppen und marginalisierte Intersektionen adressiert (Diversitaet). Erfüllt TECHNIK (KI_Sonstige) + SOZIAL (Bias_Ungleichheit, Feministisch",
      "pattern": "Keyword-Inklusion",
      "justification": "Das LLM dehnt 'Fairness' und 'Bias' zu weit aus: Ein Paper über LLM-Wissensgrenzen wird als automatisch relevant für intersektionale Fairness klassifiziert, obwohl der tatsächliche Fokus eher epistemologisch ist. Die Kategorien werden auf jedes KI-Paper mit kritischem Unterton angewendet.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "238",
      "title": "CAIL – Critical AI Literacy: Kritische Technikkompetenz für konstruktiven Umgang mit KI-basierter Technologie in Betrieben",
      "author_year": "Strauß (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.95,
      "llm_reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige) durch Framework-Entwicklung für KI-Kompetenzentwicklung und Machtanalyse in KI-Systemen. SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness) durch explizite feministische Methodologie nach intersektionalem Ansatz, Bekämpfung struktureller Ungleichheiten und Fairness-Fokus. Das Paper entwickelt substantiell einen feministischen Rahmen für KI-Governance.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorien Bias_Ungleichheit, Gender, Diversität und Feministisch über ihre beabsichtigte Reichweite aus, indem es eine allgemeine feministische Methodologie als hinreichend für diese spezifischen technologie- und KI-bezogenen Kategorien interpretiert. Der Human erkannte korrekt, dass das Paper trotz dieser Aspekte nicht die substanziellen technischen oder sozialen Kriterien für Inklusion erfüllt.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "239",
      "title": "Homoglyph unlearning: A novel approach to bias mitigation",
      "author_year": "Struppek (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmische Diskriminierungssysteme), SOZIAL_OK (Bias_Ungleichheit + Fairness: systematische Analyse von Diskriminierungstypen, Regelungsansätze, strukturelle Ungleichheiten). Kein direkter Sozialarbeitsbezug, daher nicht als SA-Paper klassifiziert. Substantieller Fokus auf algorithmische Fairness und Bias-Analyse.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorien 'Bias_Ungleichheit' und 'Fairness' zu weit aus, indem es ein rein technisches Paper über algorithmische Debiasing-Methoden als sozialwissenschaftlich relevant klassifiziert, ohne dass ein substantieller Bezug zu sozialen Ungleichheitsstrukturen oder Sozialarbeitspraxis besteht. Der Human erkennt, dass 'Homoglyph unlearning' eine spezifische technische Methode ist, die nicht die inhaltlichen Kriterien der sozialwissenschaftlichen Kategorien erfüllt.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "249",
      "title": "Shaping feminist artificial intelligence",
      "author_year": "Toupin (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige) + SOZIAL_OK (Bias_Ungleichheit, Gender, Feministisch, Fairness). Behandelt substantiell feministische Perspektive auf KI (intersektional), Gender-Bias in KI-Systemen, AI-Literacy in politischer Bildung. Kein Sozialarb.-Bezug, daher nicht in dieser Kategorie.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorie 'AI_Literacies' und 'KI_Sonstige' aus, indem es feministische Theoriearbeit zu KI als technische Literacy interpretiert, obwohl der Paper primär konzeptionelle/politische Kritik leistet. Die Kategorien werden auf normative Gestaltungsargumente ausgedehnt, die über technisches Verständnis hinausgehen.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "250",
      "title": "Shaping feminist artificial intelligence",
      "author_year": "Toupin (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper behandelt substantiell feministische Perspektiven auf KI-Entwicklung (Feministisch: Ja), thematisiert Gender-Perspektiven und Geschlechtergerechtigkeit (Gender: Ja), adressiert Diversität und Inklusion durch intersektionale Teams und Community-Partizipation (Diversitaet: Ja), diskutiert Bias und Ungleichheit (Bias_Ungleichheit: Ja), und entwickelt KI-Kompetenzen und Methodologien (AI_Literacies: Ja). Erfüllt beide TECHNIK- und SOZIAL-Kriterien.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM hat die Kriterien für AI_Literacies, Bias_Ungleichheit, Gender und Diversität zu breit interpretiert und abstrakte theoretische Debatten über feministische KI-Perspektiven als konkrete Kompetenzvermittlung oder praktische Interventionen klassifiziert. Der Human erkannte, dass der Text konzeptionell-akademisch ist und nicht die praktische/sozialarbeiterische Dimension erfüllt, die für einen Include erforderlich ist.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "251",
      "title": "Shaping feminist artificial intelligence",
      "author_year": "Toupin (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.95,
      "llm_reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK-Seite mit AI_Literacies (kritische Kompetenzentwicklung), Generative_KI (Chatbot-Design), Prompting (explizit critical prompting practices), und KI_Sonstige (Datensammlung, Training, Algorithmenauswahl). SOZIAL-Seite mit starkem Feministisch-Fokus (explizit feministische und intersektionale Perspektive), Bias_Ungleichheit (Protest gegen biased AI), Gender und Diversitaet (community-based approach). Substanzielle Behandlung aller Kategorien.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt Kategorien wie 'AI_Literacies' und 'Generative_KI' auf theoretisch-konzeptuelle Auseinandersetzung mit KI aus, während der Human wahrscheinlich praktische/operative Anwendungen oder Schulungskontexte erwartet. Die hohe Confidence (0.95) bei vollständiger Divergenz aller Kategorien deutet auf systematische Überinterpretation hin.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "253",
      "title": "Trust in artificial intelligence–based clinical decision support systems among health care workers: Systematic review",
      "author_year": "Tun (2025)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies, Prompting) + SOZIAL (Feministisch, Gender, Bias_Ungleichheit, Diversitaet). Explizit feministische Theorie (Rahmenwerk mit 6 Kategorien), Fokus auf Design und Prompting-Praktiken sowie Geschlechterperspektive auf KI-Systeme. Typen-orientierter Literaturreview mit substanziellem feministischem Ansatz.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorie 'Prompting' auf einen Review über Clinical Decision Support Systems aus, wo es um Trust und Healthcare Worker Perspektiven geht – nicht um Prompting-Praktiken. Die 'feministische Theorie' wird in den Abstrakt-Keywords nicht erwähnt und scheint eine Überinterpretation des LLM zu sein.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "260",
      "title": "Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action",
      "author_year": "Ulnicane (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: analysiert AI-Systeme wie Voice Assistants, Robots, Hiring Tools). SOZIAL_OK (Bias_Ungleichheit: Diskriminierung durch homogene Dev-Teams; Gender + Diversitaet: mehrere Diskriminierungsformen; Feministisch: explizite Anwendung von Crenshaw's Intersektionalitätstheorie). Substantielle intersektionale Analyse von strukturellen KI-Ungleichheiten.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorie 'Bias_Ungleichheit' auf theoretische intersektionale Analysen aus, während der Human eine engere Definition anwendet, die empirische Nachweise von KI-bedingten Diskriminierungsschäden erfordert. Die Unterschiede in allen anderen Kategorien sind identisch, was zeigt, dass die Divergenz in der Interpretation des Umfangs dieser einen Kategorie liegt.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "261",
      "title": "Artificial Intelligence and gender equality",
      "author_year": "UN Women (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper behandelt substantiell AI Literacy durch teacher education und co-discovery-Ansätze (TECHNIK erfüllt). Expliziter Fokus auf kritische Examination von AI's power dynamics und social justice implications erfüllt Bias_Ungleichheit-Kategorie (SOZIAL erfüllt). Beide Bedingungen der Inklusionskriterien sind erfüllt.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'AI Literacy' und 'Bias/Ungleichheit' zu breit aus, indem es allgemeine gender equality und social justice Diskurse als substantielle technische AI-Literacyvermittlung interpretiert, ohne dass konkrete AI-Konzepte oder Prompting-Techniken vorliegen müssen.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "263",
      "title": "Women4Ethical AI: Global cooperation for gender-inclusive AI",
      "author_year": "UNESCO (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.95,
      "llm_reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja: kritische Analyse von KI-Systemen und deren Entwicklung) UND SOZIAL_OK (Bias_Ungleichheit=Ja: Fokus auf Gender Bias in AI; Gender=Ja: expliziter Gender-Fokus; Diversitaet=Ja: Unterrepräsentation von Frauen; Feministisch=Ja: verwendet feministische Theorie und Kritik von Machtstrukturen). Theoretisches Paper mit substantiellem feministischem Anspruch.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorie 'KI_Sonstige' auf theoretische Policy-Papiere über KI-Ethik aus, während die menschliche Entscheidung eine engere Definition (praktische/technische KI-Arbeiten) anwendet. Die hohe Konfidenz (0.95) bei vollständiger Kategorie-Divergenz deutet auf überdehntes Kategorieverständnis hin.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "265",
      "title": "",
      "author_year": "Unknown ()",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.85,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies Ja: Pedagogisches Framework für kritische KI-Kompetenzentwicklung; Prompting Ja: Expliziter Fokus auf 'critical prompting' in Explore-Phase). SOZIAL_OK (Bias_Ungleichheit Ja: Substantielle Behandlung von Bias-Erkennung und Sichtbarmachung von Diskriminierung in KI-Outputs). Kein Bezug zu Sozialer Arbeit oder explizit feministischer Theorie; daher nur diese Kategorien Ja.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'critical prompting' und 'Bias-Erkennung' als ausreichend für Inklusion, obwohl diese isolierten technischen Elemente nicht das geforderte Integrationskriterium (TECHNIK_OK + SOZIAL_OK mit substantiellem Bezug zu Sozialer Arbeit/Feminismus) erfüllen. Die Human-Entscheidung basiert auf der fehlenden Kontextualisierung im Anwendungsfeld Soziale Arbeit.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "268",
      "title": "Artificial Intelligence in Social Work: An EPIC Model for Practice",
      "author_year": "Unknown (2025)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.95,
      "llm_reasoning": "Das Paper behandelt substantiell feministische Perspektiven auf KI (KI_Sonstige: Ja), thematisiert Gender-Bias und Machtdynamiken in AI-Systemen (Bias_Ungleichheit: Ja, Gender: Ja) und nutzt explizit feministische Theorie als Analyserahmen (Feministisch: Ja). Erfüllt beide Bedingungen: TECHNIK (KI_Sonstige) und SOZIAL (Bias_Ungleichheit, Gender, Feministisch).",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorie 'KI_Sonstige' übermäßig aus, indem es jeden Bezug zu KI in Soziale Arbeit als relevante Technik-Kategorie wertet, ohne zu prüfen, ob KI das zentrale Forschungsthema oder nur kontextueller Bezug ist. Die Human-Exclusion deutet darauf hin, dass das Paper primär ein Soziale-Arbeit-Paper mit peripherem KI-Bezug ist.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "279",
      "title": "Algorithmic discrimination: examining its types and regulatory measures with emphasis on US legal practices",
      "author_year": "Wang (2024)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.95,
      "llm_reasoning": "Das Paper behandelt substantiell AI Fairness und Bias/Ungleichheit im KI-Kontext (KI_Sonstige erfüllt Technik-Bedingung). Bias_Ungleichheit, Diversitaet und Fairness sind zentrale Themen. Explizite feministische Perspektive durch Bezug zu Crenshaw's Intersectionality-Framework und kritische Analyse von Machtstrukturen (Feministisch = Ja). Beide Bedingungen (Technik + Sozial) erfüllt → Include.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Algorithmic Discrimination' als juristische/regulatorische Analyse zu weit als AI Fairness-Literatur aus und interpretiert eine erwähnende Referenz zu Intersectionality als substantielle feministische Perspektive, obwohl das Paper primär rechtswissenschaftlich ist.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "282",
      "title": "Multilingual Prompting for Improving LLM Generation Diversity",
      "author_year": "Wang (2025)",
      "human_decision": "Exclude",
      "llm_decision": "Include",
      "disagreement_type": "Human_Exclude_Agent_Include",
      "severity": 2,
      "affected_categories": "",
      "n_affected": 0,
      "llm_confidence": 0.95,
      "llm_reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Ja - behandelt AI-Systeme im Allgemeinen), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness alle Ja). Explizit feministische Arbeit (D'Ignazio ist Autorin von Data Feminism), intersektionale Perspektive auf KI-Systeme, Fokus auf ungleiche Machtbeziehungen und Gerechtigkeit. Substantielle Behandlung aller relevanten Kategorien.",
      "pattern": "Keyword-Inklusion",
      "justification": "Das LLM hat die Keywords 'Diversity', 'Bias', 'Gender' im Titel erkannt und automatisch relevante Kategorien aktiviert, ohne zu prüfen, ob das Paper tatsächlich eine substantielle sozialwissenschaftliche oder feministische Analyse bietet oder nur oberflächlich diese Begriffe behandelt. Der Titel verspricht eine rein technische Studie zu Prompting-Techniken, nicht eine kritische Auseinandersetzung mit sozialen Implikationen.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Prompting": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "148",
      "title": "Artificial Intelligence in a Structurally Unjust Society",
      "author_year": "Lin (2022)",
      "human_decision": "Unclear",
      "llm_decision": "Include",
      "disagreement_type": "Human_Unclear",
      "severity": 1,
      "affected_categories": "AI_Literacies, Generative_KI, Prompting, KI_Sonstige, Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness",
      "n_affected": 10,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper adressiert substantiell generative KI und LLMs (Generative_KI: Ja), entwickelt ein Framework für kritische Analyse (AI_Literacies: Ja) mit explizitem Fokus auf Prompt-Dokumentation und -Transparenz (Prompting: Ja). Direkter Bezug zu sozialarbeiterischer Forschungspraxis und Journal-Editoren in der Sozialen Arbeit (Soziale_Arbeit: Ja). Alle Bedingungen erfüllt: TECHNIK (3x Ja) UND SOZIAL (1x Ja).",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Prompting' und 'Generative_KI' auf allgemeine KI-Literacy-Diskussionen aus, obwohl der Paper primär strukturelle Ungerechtigkeit thematisiert. Die Human-Kodierung erkannte korrekt, dass die KI-Dimensionen nur peripher sind, während Bias/Fairness-Aspekte zentral sind.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "199",
      "title": "Towards substantive equality in artificial intelligence: Transformative AI policy for gender equality and diversity",
      "author_year": "Ricaurte Quijano (2024)",
      "human_decision": "Unclear",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Unclear",
      "severity": 1,
      "affected_categories": "AI_Literacies, Generative_KI, Prompting, KI_Sonstige, Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness",
      "n_affected": 10,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper behandelt Chain-of-Thought Prompting für LLMs (Generative_KI=Ja, Prompting=Ja). Dies ist eine rein technische Methoden-Studie ohne Bezug zu sozialen Themen. Keine Kategorien aus dem SOZIAL-Set (Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness) sind erfüllt. Damit fehlt die SOZIAL_OK-Bedingung für Inklusion.",
      "pattern": "Implizite Feldzugehoerigkeit",
      "justification": "Das LLM erkannte nicht, dass ein Paper mit Titel zu 'substantive equality', 'gender equality' und 'diversity' im Kontext von KI-Policy primär ein sozialwissenschaftliches Werk ist, das KI nur als Anwendungsfeld behandelt. Das implizite Feldwissen, dass solche Policy-Papers zur Sozialen Arbeit/Fairness gehören, fehlte; stattdessen fokussierte das LLM auf technische Keywords wie 'Generative KI' und 'Prompting'.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "172",
      "title": "Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness",
      "author_year": "Ovalle (2023)",
      "human_decision": "Unclear",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Unclear",
      "severity": 1,
      "affected_categories": "AI_Literacies, Generative_KI, Prompting, KI_Sonstige, Bias_Ungleichheit, Gender, Diversitaet",
      "n_affected": 7,
      "llm_confidence": 0.95,
      "llm_reasoning": "Das Paper behandelt AI Literacy (Benutzervertrauen, Kalibrierung), Generative KI (LLMs) und Prompting-Strategien (Unsicherheitskonveyance). Fairness-relevant bezueglich Transparenz. Jedoch kein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversitaet oder feministischer Perspektive. Allgemeine KI-HCI-Studie ohne soziale Gerechtigkeitsdimension.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'AI Literacies', 'Generative KI' und 'Prompting' zu weit aus und übersieht dabei, dass das Paper primär intersektionale Fairness und Machtstrukturen (Matrix of Domination) behandelt – Kategorien, die das LLM fälschlicherweise als nicht-relevant einstuft. Die technischen Aspekte sind Mittel, nicht Zweck der Analyse.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "290",
      "title": "What is Feminist AI?",
      "author_year": "Wudel (2025)",
      "human_decision": "Unclear",
      "llm_decision": "Include",
      "disagreement_type": "Human_Unclear",
      "severity": 1,
      "affected_categories": "AI_Literacies, Generative_KI, Prompting, KI_Sonstige, Gender, Feministisch, Fairness",
      "n_affected": 7,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, Generative_KI, Prompting alle Ja), SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Fairness alle Ja). Framework für verantwortungsvolles Prompt Engineering mit explizitem Fokus auf ethische Prinzipien, Menschenrechte und Diversität. Keine Soziale_Arbeit oder feministischen Theoriebezug identifiziert.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Prompting' und 'AI_Literacies' auf einen konzeptionellen/theoretischen Paper aus, der primär Feminist Theory anwendet. Der Human erkennt korrekt, dass der feministisch-theoretische Kern dominant ist, während das LLM technische Begriffe überinterpretiert und gleichzeitig den Gender/Feminist-Fokus unterschätzt.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Feministisch": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "132",
      "title": "Evaluation of an artificial intelligence literacy course for university students with diverse study backgrounds",
      "author_year": "Kong (2021)",
      "human_decision": "Unclear",
      "llm_decision": "Include",
      "disagreement_type": "Human_Unclear",
      "severity": 1,
      "affected_categories": "AI_Literacies, Generative_KI, Prompting, KI_Sonstige, Bias_Ungleichheit, Fairness",
      "n_affected": 6,
      "llm_confidence": 0.92,
      "llm_reasoning": "Paper untersucht substantiell LLM-Responses auf LGBTQ+-Prompts (Generative_KI: Ja; Prompting: Ja - systematische Prompt-Variation). Zeigt algorithmischen Bias gegen queer/marginalisierte Communities (Bias_Ungleichheit: Ja), thematisiert Gender/sexuelle Orientierung (Gender: Ja; Diversitaet: Ja - LGBTQ+ als marginalisierte Gruppe; Fairness: Ja - Asymmetrien in Safety-Guardrails). TECHNIK + SOZIAL erfüllt.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt 'Generative_KI' und 'Prompting' auf einen Kurs über KI-Literalität aus, der diese Technologien nur als Beispiele behandelt, nicht als Forschungsgegenstand. Die 'Bias_Ungleichheit' und 'Fairness' werden aus allgemeinen Diversity-Inhalten abgeleitet, ohne dass der Paper systematisch diese Kategorien operationalisiert.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "134",
      "title": "Developing an artificial intelligence literacy framework: Evaluation of a literacy course for senior secondary students using a project-based learning approach",
      "author_year": "Kong (2024)",
      "human_decision": "Unclear",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Unclear",
      "severity": 1,
      "affected_categories": "AI_Literacies, Generative_KI, KI_Sonstige, Bias_Ungleichheit, Gender, Fairness",
      "n_affected": 6,
      "llm_confidence": 0.95,
      "llm_reasoning": "Das Paper ist ein Überblicks-Survey zu Bias und Fairness in LLMs. Es erfüllt die TECHNIK-Bedingung (Generative_KI=Ja) und behandelt substantiell Bias_Ungleichheit und Fairness. Allerdings fehlt die SOZIAL-Bedingung im Sinne der definierten Kategorien: Es gibt keinen direkten Bezug zu Sozialer Arbeit, und die Bias/Fairness-Behandlung ist allgemein auf NLP fokussiert, nicht spezifisch auf sozialarbeiterische Kontexte oder marginalisierte Gruppen im Sozialbereich. Daher: Exclude.",
      "pattern": "Keyword-Inklusion",
      "justification": "Das LLM hat die Keywords 'AI literacy' und 'framework' im Titel erkannt und diese automatisch den definierten Kategorien zugeordnet (AI_Literacies, Generative_KI), ohne zu erkennen, dass das Paper ein Schulkurs-Evaluationsstudium zu KI-Grundlagen ist und nicht den Inklusions-/Fairness-Fokus des Reviews erfüllt. Die Human-Entscheidung 'Unclear' deutet darauf hin, dass die Passung tatsächlich marginal ist.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "170",
      "title": "ReflectAI: Design and evaluation of an AI coach to support public servants' self-reflection",
      "author_year": "Näscher (2025)",
      "human_decision": "Unclear",
      "llm_decision": "Include",
      "disagreement_type": "Human_Unclear",
      "severity": 1,
      "affected_categories": "Prompting, Soziale_Arbeit, Bias_Ungleichheit, Gender, Fairness",
      "n_affected": 5,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper erfüllt TECHNIK-Kriterien durch Generative_KI (ChatGPT-Fokus) und Prompting (Empfehlungen zu clarifying prompts). Erfüllt SOZIAL-Kriterien durch expliziten Gender-Fokus (unterschiedliche Geschlechteridentitäten), Bias_Ungleichheit (stereotype responses, verringerte Vertrauenswerte) und Fairness (Bias-Mitigation-Strategien). Qualitative empirische Studie mit substantiellem Bias- und Gender-Fokus.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt Kategorien wie 'Bias_Ungleichheit', 'Gender' und 'Fairness' zu weit aus, indem es stereotype responses und verringerte Vertrauenswerte als Bias-Analyse interpretiert, obwohl dies eher Evaluationsergebnisse eines KI-Systems sind. Die Kategorien werden auf Phänomene angewendet, die nicht primär auf Ungleichheit/Gerechtigkeit abzielen, sondern auf technische Systemleistung.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "?",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Gender": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "223",
      "title": "\"It happened to be the perfect thing\": Experiences of generative AI chatbots for mental health",
      "author_year": "Siddals (2024)",
      "human_decision": "Unclear",
      "llm_decision": "Include",
      "disagreement_type": "Human_Unclear",
      "severity": 1,
      "affected_categories": "Soziale_Arbeit, Gender, Diversitaet, Fairness",
      "n_affected": 4,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, LLM-Analyse), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversität, Fairness alle Ja). Dokumentiert Gender-Stereotypen und algorithmischen Bias in LLMs systematisch. Kein explizit feministischer Theorie-Einsatz, daher Feministisch=Nein. Keine Soziale_Arbeit-Dimension erkennbar.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorien Gender, Diversität und Fairness über ihre operationale Definition aus, indem es generische Erwähnungen von psychischer Gesundheit und Nutzererfahrungen als Auslöser interpretiert, obwohl der Paper nicht explizit diese Dimensionen als analytischen Fokus behandelt. Das Human-Assessment erkennt korrekt, dass die Soziale Arbeit-Dimension fehlt, während das LLM diese übersieht.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        }
      }
    },
    {
      "paper_id": "125",
      "title": "Why and What to Teach: AI Curriculum for Elementary School",
      "author_year": "Kim (2021)",
      "human_decision": "Unclear",
      "llm_decision": "Exclude",
      "disagreement_type": "Human_Unclear",
      "severity": 1,
      "affected_categories": "AI_Literacies, Bias_Ungleichheit, Gender",
      "n_affected": 3,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper behandelt philosophische Grundlagen von AI-Explainability und Systematizität (KI_Sonstige: Ja). Dies ist jedoch rein technisch-philosophischer Natur ohne Bezug zu sozialen Auswirkungen, Bias, Ungleichheit, Gender, Diversität, Feminismus oder Fairness. Kein Bezug zu Sozialer Arbeit. TECHNIK erfüllt (KI_Sonstige), SOZIAL nicht erfüllt → Exclude.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM hat die Kategorien AI_Literacies, Bias_Ungleichheit und Gender zu eng definiert und deren Bedeutungsraum unterschätzt. Ein Curriculum-Paper über KI-Bildung für Grundschüler adressiert notwendig Literacies, Bias-Bewusstsein und potenziell Geschlechterdimensionen – der Human erkannte diese impliziten Verbindungen korrekt, das LLM dehnte diese Kategorien zu sehr auf explizit-technische Aspekte ein.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Gender": {
          "human": "Ja",
          "llm": "Nein",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Fairness": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        }
      }
    },
    {
      "paper_id": "234",
      "title": "Algorithmic Governance and the International Politics of Big Tech",
      "author_year": "Srivastava (2024)",
      "human_decision": "Unclear",
      "llm_decision": "Include",
      "disagreement_type": "Human_Unclear",
      "severity": 1,
      "affected_categories": "Gender, Feministisch",
      "n_affected": 2,
      "llm_confidence": 0.95,
      "llm_reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Bias in KI-Systemen), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch als intersektionale Kritik an Machtverhältnissen, Fairness). Kritische Analyse von algorithmischen Systemen mit explizitem Gender- und Rassismus-Fokus sowie intersektionaler Perspektive auf Diskriminierung.",
      "pattern": "Semantische Expansion",
      "justification": "Das LLM dehnt die Kategorien 'Gender' und 'Feministisch' auf eine allgemeine kritische Analyse von Machtverhältnissen aus, ohne dass diese Perspektiven explizit im Titel oder als Kern des Papers erkennbar sind. Die Kategorien werden als intersektionale Kritik interpretiert, statt sie auf Papers mit explizitem Gender- oder feministischem Fokus zu beschränken.",
      "category_comparison": {
        "AI_Literacies": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Generative_KI": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Prompting": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "KI_Sonstige": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Soziale_Arbeit": {
          "human": "Nein",
          "llm": "Nein",
          "divergent": false
        },
        "Bias_Ungleichheit": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Gender": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Diversitaet": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        },
        "Feministisch": {
          "human": "Nein",
          "llm": "Ja",
          "divergent": true
        },
        "Fairness": {
          "human": "Ja",
          "llm": "Ja",
          "divergent": false
        }
      }
    }
  ],
  "pipeline": {
    "stages": [
      {
        "id": "identification",
        "name": "Identifikation",
        "description": "Deep Research mit 4 Anbietern + Manuelle Ergaenzung",
        "type": "mixed",
        "input": 326,
        "output": 326,
        "prompt": null,
        "limitations": [
          "Keine vollstaendige Reproduzierbarkeit der Deep-Research-Ergebnisse",
          "290/326 Papers ohne verifizierte Source_Tool-Zuordnung"
        ]
      },
      {
        "id": "conversion",
        "name": "Konversion",
        "description": "PDF -> Markdown (Docling, deterministisch)",
        "type": "deterministic",
        "input": 257,
        "output": 252,
        "loss": 5,
        "prompt": null,
        "limitations": [
          "69 Papers ohne PDF (Open-Access-Verfuegbarkeit)",
          "Tabellen und Abbildungen gehen verloren",
          "Layout-komplexe Papers produzieren Artefakte"
        ]
      },
      {
        "id": "ske",
        "name": "Structured Knowledge Extraction",
        "description": "3-Stage LLM-Pipeline (Extract, Format, Verify)",
        "type": "probabilistic",
        "input": 252,
        "output": 249,
        "loss": 3,
        "prompts": {
          "stage1": "Du bist ein Experte für wissenschaftliche Literaturanalyse im Bereich KI, Soziale Arbeit und Gender Studies.\n\n# PAPER (Markdown-Format)\n[PAPER-INHALT]\n\n# AUFGABE\nExtrahiere alle relevanten Informationen und klassifiziere das Paper nach 10 Kategorien.\nAntworte im JSON-Format (kein Markdown-Codeblock, nur reines JSON).\n\n{\n  \"metadata\": {\n    \"title\": \"Vollständiger Titel des Papers\",\n    \"authors\": [\"Autor1\", \"Autor2\"],\n    \"year\": 2024,\n    \"type\": \"journalArticle|conferencePaper|report|book|thesis|workingPaper\",\n    \"language\": \"en|de|other\"\n  },\n  \"core\": {\n    \"research_question\": \"Die zentrale Forschungsfrage (1 Satz)\",\n    \"methodology\": \"Ansatz und Methoden (kurz: Empirisch/Theoretisch/Mixed/Review + spezifische Methoden)\",\n    \"key_finding\": \"Wichtigster Befund oder Beitrag (1-2 Sätze)\",\n    \"data_basis\": \"Datenbasis falls empirisch (z.B. n=125 Surveys, 50 Interviews)\"\n  },\n  \"arguments\": [\n    \"Hauptargument 1 (1-2 Sätze)\",\n    \"Hauptargument 2 (1-2 Sätze)\",\n    \"Hauptargument 3 (1-2 Sätze)\"\n  ],\n  \"categories\": {\n    \"AI_Literacies\": true,\n    \"Generative_KI\": false,\n    \"Prompting\": false,\n    \"KI_Sonstige\": true,\n    \"Soziale_Arbeit\": true,\n    \"Bias_Ungleichheit\": true,\n    \"Gender\": false,\n    \"Diversitaet\": true,\n    \"Feministisch\": false,\n    \"Fairness\": true\n  },\n  \"category_evidence\": {\n    \"AI_Literacies\": \"Direktes Zitat oder Paraphrase als Evidenz\",\n    \"KI_Sonstige\": \"Evidenz...\",\n    \"Soziale_Arbeit\": \"Evidenz...\",\n    \"Bias_Ungleichheit\": \"Evidenz...\",\n    \"Diversitaet\": \"Evidenz...\",\n    \"Fairness\": \"Evidenz...\"\n  },\n  \"references\": [\n    {\"author\": \"Buolamwini\", \"year\": 2018, \"short_title\": \"Gender Shades\"},\n    {\"author\": \"D'Ignazio & Klein\", \"year\": 2020, \"short_title\": \"Data Feminism\"},\n    {\"author\": \"Eubanks\", \"year\": 2019, \"short_title\": \"Automating Inequality\"}\n  ],\n  \"assessment\": {\n    \"domain_fit\": \"Wie relevant ist das Paper für die Schnittstelle AI/Soziale Arbeit/Gender? (1-2 Sätze)\",\n    \"unique_contribution\": \"Was ist der besond",
          "stage2_note": "Stufe 2 ist DETERMINISTISCH: Python-Template, kein LLM.",
          "stage3": "Du bist ein wissenschaftlicher Qualitätsprüfer. Vergleiche das generierte Wissensdokument mit dem Originaltext.\n\n# ORIGINAL-MARKDOWN (Ausschnitt)\n[ORIGINALTEXT-AUSSCHNITT]\n\n# GENERIERTES WISSENSDOKUMENT\n[GENERIERTES WISSENSDOKUMENT]\n\n# AUFGABE\nPrüfe auf drei Dimensionen und gib einen Confidence-Score.\n\nAntworte im JSON-Format:\n\n{\n  \"verification\": {\n    \"completeness\": {\n      \"score\": 85,\n      \"missing_critical\": [],\n      \"missing_minor\": [\"Detail X nicht erwähnt\"]\n    },\n    \"correctness\": {\n      \"score\": 95,\n      \"errors\": [],\n      \"distortions\": []\n    },\n    \"category_validation\": {\n      \"score\": 90,\n      \"incorrect_categories\": [],\n      \"missing_categories\": []\n    }\n  },\n  \"overall_confidence\": 90,\n  \"needs_correction\": false,\n  \"corrections\": {\n    \"frontmatter\": null,\n    \"content_fixes\": []\n  }\n}\n\n# PRÜFKRITERIEN\n\n**Completeness (0-100)**:\n- Sind Forschungsfrage, Methodik und Hauptbefunde korrekt erfasst?\n- Fehlen kritische Informationen?\n\n**Correctness (0-100)**:\n- Gibt es faktische Fehler?\n- Wurden Aussagen verzerrt?\n\n**Category Validation (0-100)**:\n- Sind die Kategorien durch den Originaltext belegt?\n- Fehlen offensichtliche Kategorien?\n\n**Overall Confidence**: Gewichteter Durchschnitt (Completeness 40%, Correctness 40%, Categories 20%)\n\n**needs_correction**: true wenn overall_confidence < 75\n\nFalls needs_correction = true, füge spezifische Korrekturen hinzu.\n\nAntworte NUR mit dem JSON-Objekt."
        },
        "limitations": [
          "LLM-Extraktion abhaengig von Markdown-Qualitaet",
          "Kategorie-Extraktion probabilistisch",
          "Verifikation prueft nur gegen Originaltext"
        ]
      },
      {
        "id": "assessment",
        "name": "Duales Assessment",
        "description": "Human (210/326) + LLM (326/326) mit identischem 10K-Schema",
        "type": "probabilistic",
        "input": 249,
        "output": 210,
        "prompt": "Du bist ein wissenschaftlicher Reviewer...\n\n## Kategorien (binaer: Ja/Nein)\n\n- **AI_Literacies**: Das Paper behandelt Kompetenzen, Fähigkeiten oder Wissen im Umgang mit KI-Systemen. Umfasst kritische Reflexion, technisches Verständnis oder praktische Anwendungskompetenz.\n  Beispiele JA: Framework für KI-Kompetenzentwicklung, Curriculum für AI Literacy in Schulen\n  Beispiele NEIN: Rein technische KI-Implementierung ohne Bildungsbezug\n- **Generative_KI**: Fokus auf generative KI-Modelle wie Large Language Models, Bildgeneratoren oder andere generative Systeme.\n  Beispiele JA: ChatGPT in der Beratung, Midjourney für kreative Prozesse\n  Beispiele NEIN: Klassische ML-Klassifikation ohne generativen Aspekt\n- **Prompting**: Behandelt Prompt-Engineering, Prompt-Strategien oder die Gestaltung von Eingaben für KI-Systeme.\n  Beispiele JA: Chain-of-Thought Prompting für Bias-Reduktion, Prompt-Templates für Dokumentation\n  Beispiele NEIN: KI-Nutzung ohne Fokus auf Eingabegestaltung\n- **KI_Sonstige**: Andere KI/ML-Themen: klassisches Machine Learning, algorithmische Entscheidungssysteme, Predictive Analytics, Robotik, Computer Vision. WICHTIG: Algorithmische Systeme im Sozialbereich (z.B. Risikobewertung in der Jugendhilfe) zählen hierzu und sind relevant!\n  Beispiele JA: Algorithmische Risikobewertung in der Jugendhilfe, Predictive Policing und soziale Auswirkungen\n  Beispiele NEIN: Reine Robotik ohne sozialen Bezug\n- **Soziale_Arbeit**: Direkter Bezug zu sozialarbeiterischer Praxis, Theorie, Ausbildung oder den Zielgruppen Sozialer Arbeit.\n  Beispiele JA: KI in der Jugendhilfe, Algorithmische Entscheidungssysteme im Sozialamt\n  Beispiele NEIN: Allgemeine KI-Ethik ohne Sozialarbeitsbezug\n- **Bias_Ungleichheit**: Thematisiert Diskriminierung, algorithmischen Bias, soziale Ungleichheit oder strukturelle Benachteiligung im KI-Kontext.\n  Beispiele JA: Analyse von Racial Bias in LLM-Outputs, Algorithmische Diskriminierung bei Kreditvergabe\n  Beispiele NEIN: Allgemeine KI-Performance-Studie ohne Bias-Fokus\n- **Gender**: Expliziter Gender-Fokus, Geschlechterperspektive oder Analyse von Gender-Bias.\n  Beispiele JA: Gender-Bias in Sprachmodellen, Geschlechterstereotype in KI-generierten Bildern\n  Beispiele NEIN: Demografische Daten enthalten Geschlecht, aber kein Gender-Fokus\n- **Diversitaet**: Thematisiert Diversität, Inklusion oder Repräsentation verschiedener Gruppen.\n  Beispiele JA: Inklusive KI-Entwicklung mit marginalisierten Communities, Repraesentation verschiedener Ethnien in Trainingsdaten\n  Beispiele NEIN: Diverse Methoden verwendet (methodische Diversitaet, nicht sozial)\n- **Feministisch**: Verwendet feministische Theorie, Methodik oder Perspektive. Auch implizit feministische Ansätze zählen: intersektionale Analysen, kritische Betrachtung von Machtstrukturen, Fokus auf marginalisierte Gruppen aus Geschlechterperspektive.\n  Beispiele JA: Intersektionale Analyse nach Crenshaw, Kritik aus feministischer Technikforschung\n  Beispiele NEIN: Gender nur als Varia",
        "limitations": [
          "Human Assessment nur 210/326 (64%)",
          "LLM hat keinen Zugriff auf Volltexte",
          "Kappa = 0.035 ist Prevalence-Bias-Artefakt"
        ]
      },
      {
        "id": "synthesis",
        "name": "Synthese",
        "description": "Vault-Generierung (deterministisch + LLM-Konzeptextraktion)",
        "type": "mixed",
        "input": 249,
        "output": 249,
        "prompt": null,
        "limitations": [
          "Konzept-Extraktion probabilistisch",
          "Co-Occurrence dokumentbasiert, nicht satzbasiert"
        ]
      }
    ],
    "flow": {
      "nodes": [
        {
          "id": "zotero",
          "label": "Zotero (326)",
          "value": 326,
          "stage": 0
        },
        {
          "id": "pdfs",
          "label": "PDFs (257)",
          "value": 257,
          "stage": 1
        },
        {
          "id": "pdfs_missing",
          "label": "Fehlende PDFs (69)",
          "value": 69,
          "stage": 1
        },
        {
          "id": "markdown",
          "label": "Markdown (252)",
          "value": 252,
          "stage": 2
        },
        {
          "id": "conv_fail",
          "label": "Konversions-Fehler (5)",
          "value": 5,
          "stage": 2
        },
        {
          "id": "knowledge",
          "label": "Knowledge Docs (249)",
          "value": 249,
          "stage": 3
        },
        {
          "id": "ske_fail",
          "label": "SKE-Verlust (3)",
          "value": 3,
          "stage": 3
        },
        {
          "id": "assessed_both",
          "label": "Dual bewertet (210)",
          "value": 210,
          "stage": 4
        },
        {
          "id": "assessed_llm",
          "label": "Nur LLM (39)",
          "value": 39,
          "stage": 4
        },
        {
          "id": "agree",
          "label": "Agreement (99)",
          "value": 99,
          "stage": 5
        },
        {
          "id": "disagree",
          "label": "Disagreement (111)",
          "value": 111,
          "stage": 5
        }
      ],
      "links": [
        {
          "source": "zotero",
          "target": "pdfs",
          "value": 257
        },
        {
          "source": "zotero",
          "target": "pdfs_missing",
          "value": 69
        },
        {
          "source": "pdfs",
          "target": "markdown",
          "value": 252
        },
        {
          "source": "pdfs",
          "target": "conv_fail",
          "value": 5
        },
        {
          "source": "markdown",
          "target": "knowledge",
          "value": 249
        },
        {
          "source": "markdown",
          "target": "ske_fail",
          "value": 3
        },
        {
          "source": "knowledge",
          "target": "assessed_both",
          "value": 210
        },
        {
          "source": "knowledge",
          "target": "assessed_llm",
          "value": 39
        },
        {
          "source": "assessed_both",
          "target": "agree",
          "value": 99
        },
        {
          "source": "assessed_both",
          "target": "disagree",
          "value": 111
        }
      ]
    }
  },
  "categories": [
    {
      "name": "AI_Literacies",
      "group": "technik",
      "definition": "Das Paper behandelt Kompetenzen, Fähigkeiten oder Wissen im Umgang mit KI-Systemen. Umfasst kritische Reflexion, technisches Verständnis oder praktische Anwendungskompetenz.",
      "examples_positive": [
        "Framework für KI-Kompetenzentwicklung",
        "Curriculum für AI Literacy in Schulen"
      ],
      "examples_negative": [
        "Rein technische KI-Implementierung ohne Bildungsbezug"
      ]
    },
    {
      "name": "Generative_KI",
      "group": "technik",
      "definition": "Fokus auf generative KI-Modelle wie Large Language Models, Bildgeneratoren oder andere generative Systeme.",
      "examples_positive": [
        "ChatGPT in der Beratung",
        "Midjourney für kreative Prozesse"
      ],
      "examples_negative": [
        "Klassische ML-Klassifikation ohne generativen Aspekt"
      ]
    },
    {
      "name": "Prompting",
      "group": "technik",
      "definition": "Behandelt Prompt-Engineering, Prompt-Strategien oder die Gestaltung von Eingaben für KI-Systeme.",
      "examples_positive": [
        "Chain-of-Thought Prompting für Bias-Reduktion",
        "Prompt-Templates für Dokumentation"
      ],
      "examples_negative": [
        "KI-Nutzung ohne Fokus auf Eingabegestaltung"
      ]
    },
    {
      "name": "KI_Sonstige",
      "group": "technik",
      "definition": "Andere KI/ML-Themen: klassisches Machine Learning, algorithmische Entscheidungssysteme, Predictive Analytics, Robotik, Computer Vision. WICHTIG: Algorithmische Systeme im Sozialbereich (z.B. Risikobewertung in der Jugendhilfe) zählen hierzu und sind relevant!",
      "examples_positive": [
        "Algorithmische Risikobewertung in der Jugendhilfe",
        "Predictive Policing und soziale Auswirkungen",
        "Machine Learning für Fallprognosen"
      ],
      "examples_negative": [
        "Reine Robotik ohne sozialen Bezug"
      ]
    },
    {
      "name": "Soziale_Arbeit",
      "group": "sozial",
      "definition": "Direkter Bezug zu sozialarbeiterischer Praxis, Theorie, Ausbildung oder den Zielgruppen Sozialer Arbeit.",
      "examples_positive": [
        "KI in der Jugendhilfe",
        "Algorithmische Entscheidungssysteme im Sozialamt"
      ],
      "examples_negative": [
        "Allgemeine KI-Ethik ohne Sozialarbeitsbezug"
      ]
    },
    {
      "name": "Bias_Ungleichheit",
      "group": "sozial",
      "definition": "Thematisiert Diskriminierung, algorithmischen Bias, soziale Ungleichheit oder strukturelle Benachteiligung im KI-Kontext.",
      "examples_positive": [
        "Analyse von Racial Bias in LLM-Outputs",
        "Algorithmische Diskriminierung bei Kreditvergabe"
      ],
      "examples_negative": [
        "Allgemeine KI-Performance-Studie ohne Bias-Fokus"
      ]
    },
    {
      "name": "Gender",
      "group": "sozial",
      "definition": "Expliziter Gender-Fokus, Geschlechterperspektive oder Analyse von Gender-Bias.",
      "examples_positive": [
        "Gender-Bias in Sprachmodellen",
        "Geschlechterstereotype in KI-generierten Bildern"
      ],
      "examples_negative": [
        "Demografische Daten enthalten Geschlecht, aber kein Gender-Fokus"
      ]
    },
    {
      "name": "Diversitaet",
      "group": "sozial",
      "definition": "Thematisiert Diversität, Inklusion oder Repräsentation verschiedener Gruppen.",
      "examples_positive": [
        "Inklusive KI-Entwicklung mit marginalisierten Communities",
        "Repraesentation verschiedener Ethnien in Trainingsdaten"
      ],
      "examples_negative": [
        "Diverse Methoden verwendet (methodische Diversitaet, nicht sozial)"
      ]
    },
    {
      "name": "Feministisch",
      "group": "sozial",
      "definition": "Verwendet feministische Theorie, Methodik oder Perspektive. Auch implizit feministische Ansätze zählen: intersektionale Analysen, kritische Betrachtung von Machtstrukturen, Fokus auf marginalisierte Gruppen aus Geschlechterperspektive.",
      "examples_positive": [
        "Intersektionale Analyse nach Crenshaw",
        "Kritik aus feministischer Technikforschung",
        "Analyse von Gender-Bias mit Fokus auf strukturelle Ungleichheit",
        "Bezug auf feminist HCI, feminist STS"
      ],
      "examples_negative": [
        "Gender nur als Variable erwähnt ohne kritische Perspektive"
      ]
    },
    {
      "name": "Fairness",
      "group": "sozial",
      "definition": "Thematisiert algorithmische Fairness, faire ML-Systeme oder Fairness-Metriken.",
      "examples_positive": [
        "Fairness-Metriken fuer Klassifikationsmodelle",
        "Fair ML Frameworks und Debiasing-Strategien"
      ],
      "examples_negative": [
        "Allgemeine Ethik-Diskussion ohne spezifischen Fairness-Bezug"
      ]
    }
  ],
  "meta": {
    "total_papers": 326,
    "knowledge_docs": 249,
    "human_assessed": 210,
    "llm_assessed": 326,
    "disagreements": 111,
    "concepts_count": 136,
    "confusion_matrix": {
      "Include_Include": 65,
      "Include_Exclude": 23,
      "Include_Unclear": 0,
      "Exclude_Include": 78,
      "Exclude_Exclude": 34,
      "Exclude_Unclear": 0,
      "Unclear_Include": 6,
      "Unclear_Exclude": 4,
      "Unclear_Unclear": 0
    },
    "kappa": 0.035,
    "overall_agreement": 0.4714,
    "llm_include_rate": 0.68,
    "human_include_rate": 0.42,
    "pattern_distribution": {
      "Semantische Expansion": 90,
      "Implizite Feldzugehoerigkeit": 9,
      "Keyword-Inklusion": 12
    },
    "asymmetry": {
      "llm_overincludes": 78,
      "human_overincludes": 23
    }
  }
}