{
  "papers": [
    {
      "id": "A+_Alliance_2024_Incubating_Feminist_AI__Executive_Summary_2021-202",
      "filename": "A+_Alliance_2024_Incubating_Feminist_AI__Executive_Summary_2021-202.md",
      "title": "Incubating Feminist AI: Executive Summary 2021-2024",
      "author_year": "A+ Alliance (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://aplusalliance.org/incubatingfeministai2024/",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 2,
      "rel_praxis": 3,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Practical Implementation"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-medium",
        "dim-praxis-high"
      ],
      "has_summary": true,
      "summary_file": "summary_Alliance_2024_Incubating.md",
      "abstract": "Documents outcomes from $2 million CAD Feminist AI Research Network project across Latin America, Middle East, and Asia. Developed 12 feminist AI prototypes addressing gender-based violence, transit safety, bias detection in NLP systems, and judicial transparency, demonstrating practical applications of feminist AI principles through community-centered design and intersectional analysis capabilities.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "KALSNSNC"
    },
    {
      "id": "Ahmed_2024_Feminist_perspectives_on_AI__Ethical_consideration",
      "filename": "Ahmed_2024_Feminist_perspectives_on_AI__Ethical_consideration.md",
      "title": "Feminist Perspectives on AI: Ethical Considerations in Algorithmic Decision-Making",
      "author_year": "Ahmed (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.researchcorridor.org/index.php/jgsi/article/download/330/314",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Explores ethical implications of algorithmic decision-making from feminist perspective, examining data bias, discrimination in automated systems, and underrepresentation of women in AI development. Algorithmic biases disproportionately affect marginalized groups and reinforce societal inequalities in areas like hiring, healthcare, and law enforcement. Feminist ethical framework emphasizes transparency, fairness, and inclusivity while questioning patriarchal and corporate-driven narratives in AI ",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "HIX79B5L"
    },
    {
      "id": "Ahn_2025_Artificial_Intelligence_(AI)_literacy_for_social_w",
      "filename": "Ahn_2025_Artificial_Intelligence_(AI)_literacy_for_social_w.md",
      "title": "Artificial Intelligence (AI) Literacy for Social Work: Implications for Core Competencies",
      "author_year": "Ahn (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1086/735187",
      "url": "https://www.journals.uchicago.edu/doi/10.1086/735187",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "T9NBQNPV"
    },
    {
      "id": "Ahrweiler_2025_AI_FORA_–_Artificial_Intelligence_for_Assessment__",
      "filename": "Ahrweiler_2025_AI_FORA_–_Artificial_Intelligence_for_Assessment__.md",
      "title": "AI FORA – Artificial Intelligence for Assessment: Fairness bei der Verteilung öffentlicher sozialer Leistungen",
      "author_year": "Ahrweiler (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://nachrichten.idw-online.de/2025/03/18/wie-koennte-kuenstliche-intelligenz-weltweit-fairness-bei-der-verteilung-oeffentlicher-sozialer-leistungen-erhoehen",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 11,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-medium",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This international research project examined AI-supported social assessments across nine countries on four continents. The study demonstrates that justice criteria for receiving state benefits are culture- and context-dependent. A central finding is that deploying a standardized AI system globally is insufficient; instead, flexible, dynamic, and adaptive systems are required. Development of such systems depends on contributions from all societal actors, including vulnerable groups, for designing",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "FTR33WLG"
    },
    {
      "id": "Alam_2025_Social_work_in_the_age_of_artificial_intelligence_",
      "filename": "Alam_2025_Social_work_in_the_age_of_artificial_intelligence_.md",
      "title": "Social work in the age of artificial intelligence: A rights-based framework for evidence-based practice through social psychology, group dynamics, and institutional analysis",
      "author_year": "Alam (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1080/26408066.2025.2547219",
      "url": "https://doi.org/10.1080/26408066.2025.2547219",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 11,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-medium",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This study develops a comprehensive rights-based framework for navigating AI integration in social work practice while addressing ethical implications across micro, meso, and macro practice levels. The framework bridges social work theory with interdisciplinary insights, demonstrating that AI systems profoundly impact vulnerable populations by mediating interpersonal relationships and constructing meaning in AI-mediated environments. It provides evidence-based guidance for practitioners to harne",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "63XWTA3J"
    },
    {
      "id": "Alvarez_2024_Policy_advice_and_best_practices_on_bias_and_fairn",
      "filename": "Alvarez_2024_Policy_advice_and_best_practices_on_bias_and_fairn.md",
      "title": "Policy advice and best practices on bias and fairness in AI",
      "author_year": "Alvarez (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1007/s10676-024-09746-w",
      "url": "https://link.springer.com/article/10.1007/s10676-024-09746-w",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Alvarez_2024_Policy.md",
      "abstract": "This open-access paper provides a comprehensive overview of fairness in AI, bridging technical bias mitigation methods with legal and policy considerations. Alvarez et al. survey the state-of-the-art in fair AI techniques and review major policy initiatives and standards on algorithmic bias. A key contribution is the NoBIAS architecture introduced in the paper, which comprises a “Legal Layer” (focusing on EU non-discrimination law and human rights requirements) and a “Bias Management Layer” (cov",
      "summary_section": "## Overview\n\nThis 2024 paper, authored by 13 international researchers and published in April 2024, addresses a critical gap in AI governance by providing comprehensive guidance on bias and fairness in artificial intelligence systems. Grounded in the NoBIAS research project—a European initiative exa",
      "source_tool": "Manual",
      "zotero_key": "V3TWQUZC"
    },
    {
      "id": "Amnesty_International_2024_Coded_injustice__Surveillance_and_discrimination_i",
      "filename": "Amnesty_International_2024_Coded_injustice__Surveillance_and_discrimination_i.md",
      "title": "Coded injustice: Surveillance and discrimination in Denmark's automated welfare state",
      "author_year": "Amnesty International (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.amnesty.org/en/documents/eur18/8709/2024/en/",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 2,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-prof-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Amnesty_International_2024_Coded.md",
      "abstract": "Critical human rights investigation documenting how Denmark's AI-powered welfare fraud detection systems risk discriminating against people with disabilities, low-income individuals, migrants, refugees, and marginalized racial groups. Report examines up to 60 algorithmic models used to flag individuals for fraud investigations, revealing mass surveillance practices violating privacy rights and creating atmospheres of fear among welfare recipients. Key findings demonstrate how automated systems p",
      "summary_section": "## Overview\n\nAmnesty International's 2024 report \"Coded Injustice\" examines Denmark's automated welfare administration system, specifically investigating how Udbetaling Danmark (UDK) deploys fraud-control algorithms that create discriminatory surveillance mechanisms targeting vulnerable populations.",
      "source_tool": "Manual",
      "zotero_key": "C2MFBS5K"
    },
    {
      "id": "An_2025_Measuring_gender_and_racial_biases_in_large_langua",
      "filename": "An_2025_Measuring_gender_and_racial_biases_in_large_langua.md",
      "title": "Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation",
      "author_year": "An (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1093/pnasnexus/pgaf089",
      "url": "https://doi.org/10.1093/pnasnexus/pgaf089",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Vethman_2025_Fairness.md",
      "abstract": "Large-scale experimental study examining bias across five major LLMs using over 361,000 randomized resumes. Reveals complex intersectional patterns where LLMs favor female candidates but discriminate against Black male candidates, with bias effects translating to 1-3 percentage point differences in hiring probabilities, validating intersectionality theory through three key findings.",
      "summary_section": "![[summary_Vethman_2025_Fairness.md]]",
      "source_tool": "Manual",
      "zotero_key": "5WVV8IRY"
    },
    {
      "id": "Arias_López_2023_Digital_literacy_as_a_new_determinant_of_health__A",
      "filename": "Arias_López_2023_Digital_literacy_as_a_new_determinant_of_health__A.md",
      "title": "Digital literacy as a new determinant of health: A scoping review",
      "author_year": "Arias López (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1371/journal.pdig.0000279",
      "url": "https://dx.plos.org/10.1371/journal.pdig.0000279",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Introduction Harnessing new digital technologies can improve access to health care but can also widen the health divide for those with poor digital literacy. This scoping review aims to assess the current situation of low digital health literacy in terms of its definition, reach, impact on health and interventions for its mitigation. Methods A comprehensive literature search strategy was composed by a qualified medical librarian. Literature databases [Medline (Ovid), Embase (Ovid), Scopus, and G",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "5WQ2JR8V"
    },
    {
      "id": "Articulate_2025_How_to_Create_Inclusive_AI_Images__A_Guide_to_Bias",
      "filename": "Articulate_2025_How_to_Create_Inclusive_AI_Images__A_Guide_to_Bias.md",
      "title": "How to Create Inclusive AI Images: A Guide to Bias-Free Prompting",
      "author_year": "Articulate (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "webpage",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.articulate.com/blog/how-to-create-inclusive-ai-images-a-guide-to-bias-free-prompting/",
      "decision": "Exclude",
      "exclusion_reason": "Wrong publication type",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Practical guide examining prompt engineering strategies for creating inclusive AI-generated images and avoiding biased default outputs. Analysis shows AI image generators often produce stereotypical representations (white, male, slim, young, physically able) due to training on unbalanced internet datasets. Presents concrete techniques for inclusive prompt engineering: specifying visible identity characteristics (age, race, gender, body size, visible disabilities), using diversity-promoting terms",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "3VBJ2BPX"
    },
    {
      "id": "Asseri_2024_Prompt_engineering_techniques_for_mitigating_cultu",
      "filename": "Asseri_2024_Prompt_engineering_techniques_for_mitigating_cultu.md",
      "title": "Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review",
      "author_year": "Asseri (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.48550/arXiv.2506.18199",
      "url": "https://doi.org/10.48550/arXiv.2506.18199",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Systematic review following PRISMA guidelines analyzing prompt-engineering techniques to reduce cultural bias against Arabs and Muslims in LLMs, evaluating eight empirical studies (2021-2024). Identifies five primary approaches: Cultural Prompting, Affective Priming, Self-Debiasing techniques, structured multi-stage pipelines, and parameter-optimized continuous prompts. Structured multi-stage pipelines most effective with up to 87.7% bias reduction. Emphasizes accessibility of prompt engineering",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "FEC83VIS"
    },
    {
      "id": "Asseri_2025_Prompt_engineering_techniques_for_mitigating_cultu",
      "filename": "Asseri_2025_Prompt_engineering_techniques_for_mitigating_cultu.md",
      "title": "Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review",
      "author_year": "Asseri (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2506.18199",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This systematic review of 8 studies (2021–2024) identifies five prompt engineering approaches to mitigate bias against Arabs and Muslims: self-debiasing, cultural context prompting, affective priming, structured multi-step pipelines, and continuous prompt tuning. Multi-step pipelines were most effective, reducing biased content by up to ~88%, while simpler methods like cultural prompts showed ~71–81% improvement. The review concludes that while prompt engineering can mitigate biases without retr",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "VP7WHIQM"
    },
    {
      "id": "Attard-Frost_2025_AI_Countergovernance__Lessons_Learned_from_Canada_",
      "filename": "Attard-Frost_2025_AI_Countergovernance__Lessons_Learned_from_Canada_.md",
      "title": "AI Countergovernance: Lessons Learned from Canada and Paris",
      "author_year": "Attard-Frost (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://techpolicy.press/ai-countergovernance-lessons-learned-from-canada-and-paris/",
      "decision": "Unclear",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 3,
      "rel_bias": 2,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "unclear",
        "medium-relevance",
        "dim-ai-komp-medium",
        "dim-vulnerable-high",
        "dim-bias-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Argues against superficial \"AI literacy\" programs, promoting instead grassroots critical AI literacies that engage directly with structural inequalities related to race, gender, and labor. Stresses collectiv",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "MIMGY67J"
    },
    {
      "id": "Bai_2025_Explicitly_unbiased_large_language_models_still_fo",
      "filename": "Bai_2025_Explicitly_unbiased_large_language_models_still_fo.md",
      "title": "Explicitly unbiased large language models still form biased associations",
      "author_year": "Bai (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1073/pnas.2416228122",
      "url": "https://doi.org/10.1073/pnas.2416228122",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 7,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Demonstrates that even when LLMs are aligned to avoid overt bias, they can still harbor implicit biases. Introduces novel evaluation methods inspired by psychology: LLM Word Association Test (LLM-WAT) and LLM Relative Decision Test (LLM-RDT) to probe automatic associations and subtle discrimination. Finds pervasive stereotype-consistent biases across multiple domains in eight state-of-the-art, value-aligned models.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "IWMJYP2I"
    },
    {
      "id": "Baker_2025_Artificial_intelligence_in_social_work__An_EPIC_mo",
      "filename": "Baker_2025_Artificial_intelligence_in_social_work__An_EPIC_mo.md",
      "title": "Artificial intelligence in social work: An EPIC model for practice",
      "author_year": "Baker (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1080/0312407X.2025.2488345",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 3,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 12,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-medium",
        "dim-vulnerable-high",
        "dim-bias-medium",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Presents EPIC model for integrating AI into social work consisting of four components: Ethics and justice, Policy development and advocacy, Intersectoral collaboration, and Community engagement and empowerment. Following comprehensive literature review, examines AI's influence on social work including opportunities to advance socially just outcomes and challenges risking ethical practice. Emphasizes community-based initiatives promoting AI digital literacy and partnerships with local organizatio",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "KQPFZ6DZ"
    },
    {
      "id": "Barman_2024_Beyond_transparency_and_explainability__On_the_nee",
      "filename": "Barman_2024_Beyond_transparency_and_explainability__On_the_nee.md",
      "title": "Beyond transparency and explainability: On the need for adequate and contextualized user guidelines for LLM use",
      "author_year": "Barman (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1007/s10676-024-09778-2",
      "url": "https://doi.org/10.1007/s10676-024-09778-2",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 1,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "AI Literacy",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-ai-komp-medium",
        "dim-bias-medium",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Barman_2024_Beyond.md",
      "abstract": "Argues for user-centered approach to governing AI systems, contending that transparency alone is insufficient. Proposes contextualized guidelines and training for users including clear instructions on LLM reliability, diversity-sensitive prompting techniques, and iterative query refinement. Emphasizes shifting focus from AI's internal workings to human-AI interaction context.",
      "summary_section": "## Overview\n\nThe South Australian Government's Generative AI Guideline establishes a comprehensive governance framework for integrating generative AI and large language model (LLM) tools—including OpenAI's ChatGPT and Google Bard—across all SA Government agencies, personnel, and non-government suppl",
      "source_tool": "Manual",
      "zotero_key": "DMTJNKKH"
    },
    {
      "id": "Basseri_2025_Prompt_Engineering_Techniques_for_Mitigating_Cultu",
      "filename": "Basseri_2025_Prompt_Engineering_Techniques_for_Mitigating_Cultu.md",
      "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review",
      "author_year": "Basseri (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2506.18199v1",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This systematic review identifies five major prompt engineering strategies to reduce cultural and intersectional bias in LLMs. Structured multi-step pipelines were most effective but complex, while cultural prompting offered a practical balance. Results show varying mitigation success depending on stereotype type, and emphasize trade-offs between bias reduction and performance.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "G53VSXJU"
    },
    {
      "id": "Benjamin_2023_Keynote_Summary__The_New_Jim_Code__Reimagining_the",
      "filename": "Benjamin_2023_Keynote_Summary__The_New_Jim_Code__Reimagining_the.md",
      "title": "Keynote Summary: The New Jim Code: Reimagining the Default Settings of Technology & Society",
      "author_year": "Benjamin (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1145/3589139",
      "url": "https://dl.acm.org/doi/10.1145/3589139",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "In this summary of her foundational work, Ruha Benjamin introduces the concept of the \"New Jim Code,\" which describes how new technologies, including AI, can reproduce and even deepen existing racial hierarchies and discrimination under a veneer of neutrality and progress. She argues that discrimination becomes embedded in the very architecture of these systems. This framework is crucial for understanding how various forms of discrimination are co-constituted in AI, not as accidental bugs, but a",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "NXQ4CGF6"
    },
    {
      "id": "Benlian_2025_The_AI_literacy_development_canvas__Assessing_and_",
      "filename": "Benlian_2025_The_AI_literacy_development_canvas__Assessing_and_.md",
      "title": "The AI literacy development canvas: Assessing and building AI literacy in organizations",
      "author_year": "Benlian (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1016/j.bushor.2025.10.001",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S0007681325001673",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "AWM5HCGV"
    },
    {
      "id": "Biagini_2024_Less_knowledge,_more_trust__Exploring_potentially_",
      "filename": "Biagini_2024_Less_knowledge,_more_trust__Exploring_potentially_.md",
      "title": "Less knowledge, more trust? Exploring potentially uncritical attitudes towards AI in higher education",
      "author_year": "Biagini (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.17471/2499-4324/1337",
      "url": "https://doi.org/10.17471/2499-4324/1337",
      "decision": "Unclear",
      "exclusion_reason": "nan",
      "rel_ai_komp": 3,
      "rel_vulnerable": 1,
      "rel_bias": 1,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 7,
      "relevance_category": "medium",
      "top_dimensions": [
        "AI Literacy",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "unclear",
        "medium-relevance",
        "dim-ai-komp-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "L'intelligenza artificiale (IA) ha il potenziale per trasformare vari aspetti delle nostre vite, ma il suo sviluppo è stato accompagnato da numerose preoccupazioni sociali ed etiche. Per comprendere le implicazioni e i meccanismi sottostanti, è essenziale acquisire una comprensione ampia dei suoi benefici e svantaggi. A questo scopo, l'alfabetizzazione all'IA è un fattore fondamentale per promuovere atteggiamenti più consapevoli verso lo sviluppo dell'IA e delle sue implicazioni. Tuttavia, la ri",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "MQEHCQMW"
    },
    {
      "id": "Biegelbauer_2023_Leitfaden_Digitale_Verwaltung_und_Ethik__Praxislei",
      "filename": "Biegelbauer_2023_Leitfaden_Digitale_Verwaltung_und_Ethik__Praxislei.md",
      "title": "Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden für KI in der Verwaltung, Version 1.0",
      "author_year": "Biegelbauer (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://oeffentlicherdienst.gv.at/wp-content/uploads/2023/11/Leitfaden-Digitale-Verwaltung-Ethik.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 3,
      "rel_vulnerable": 1,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "AI Literacy",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-ai-komp-high",
        "dim-bias-medium",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Biegelbauer_2023_Leitfaden.md",
      "abstract": "This guideline defines AI literacy as the ability to understand and use AI, emphasizing that safe, self-determined, and responsible use requires sufficient understanding of the technology's functioning, possibilities, and challenges. It identifies automation bias as a central risk and emphasizes competency building and training as the foundation for all further measures, recommending the creation of educational standards for AI procurement and application.",
      "summary_section": "## Overview\n\nThe Austrian government's \"Guide for Digital Administration and Ethics\" (Version 1.0, 2023) represents an institutional policy document establishing frameworks for integrating artificial intelligence and digital technologies into public administration while maintaining ethical standards",
      "source_tool": "Manual",
      "zotero_key": "DI8XTFWR"
    },
    {
      "id": "Birru_2024_Mitigating_age-related_bias_in_large_language_mode",
      "filename": "Birru_2024_Mitigating_age-related_bias_in_large_language_mode.md",
      "title": "Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development",
      "author_year": "Birru (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://pubsonline.informs.org/doi/10.1287/ijoc.2024.0645",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "6SMMDL8E"
    },
    {
      "id": "Bisconti_2024_A_formal_account_of_AI_trustworthiness__Connecting",
      "filename": "Bisconti_2024_A_formal_account_of_AI_trustworthiness__Connecting.md",
      "title": "A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization",
      "author_year": "Bisconti (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://philarchive.org/archive/BISAFA",
      "decision": "Unclear",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 4,
      "relevance_category": "low",
      "top_dimensions": [
        "Practical Implementation",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "unclear",
        "low-relevance",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Develops formal conceptualization of AI trustworthiness connecting intrinsic and perceived trustworthiness in operationalizable schema. Argues that trustworthiness extends beyond inherent AI system capabilities to include significant influences from observer perceptions such as perceived transparency, agency locus, and human oversight. Identifies three central perceived characteristics: transparency (access to information about functionality), agency locus (perception of action autonomy source),",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "464WKRRQ"
    },
    {
      "id": "Boetto_2025_Artificial_Intelligence_in_Social_Work__An_EPIC_Mo",
      "filename": "Boetto_2025_Artificial_Intelligence_in_Social_Work__An_EPIC_Mo.md",
      "title": "Artificial Intelligence in Social Work: An EPIC Model for Practice",
      "author_year": "Boetto (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1080/0312407X.2025.2488345",
      "url": "https://doi.org/10.1080/0312407X.2025.2488345",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Professional Context",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Narrative review proposing the EPIC model—Ethics & justice, Policy, Intersectoral collaboration, Community engagement—to guide ethical AI integration. Balances efficiency opportunities with risks of bias and value erosion, advocating structured, human-centered adoption aligned with social justice.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "95CGSWND"
    },
    {
      "id": "British_Association_of_Social_Workers_2025_Generative_AI_&_social_work_practice_guidance",
      "filename": "British_Association_of_Social_Workers_2025_Generative_AI_&_social_work_practice_guidance.md",
      "title": "Generative AI & social work practice guidance",
      "author_year": "British Association of Social Workers (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://basw.co.uk/policy-and-practice/resources/generative-ai-social-work-practice-guidance",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Professional Context",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "BASW's initial practice guidance specifically addressing generative AI use in social work, offering reflection points for practitioners relating to ethical considerations under BASW Code of Ethics. Warns that AI tools are prone to replicating racist/sexist assumptions from training datasets, generating misleading information (hallucinations), and risking data privacy breaches. Emphasizes generative AI should create capacity for relationship-based practice rather than justify increased caseloads ",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "8IXX8VAB"
    },
    {
      "id": "Browne_2023_Feminist_AI__Critical_Perspectives_on_Algorithms,_",
      "filename": "Browne_2023_Feminist_AI__Critical_Perspectives_on_Algorithms,_.md",
      "title": "Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligent Machines",
      "author_year": "Browne (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "book",
      "language": "nan",
      "doi": "nan",
      "url": "https://doi.org/10.1093/oso/9780192889898.001.0001",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "First comprehensive collection bringing together leading feminist thinkers across disciplines to examine AI's societal impact. Features 21 chapters covering topics from techno-racial capitalism to AI's military applications, examining how feminist scholarship can hold the AI sector accountable for designing systems that further social justice through diverse feminist approaches.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "8LG58R7T"
    },
    {
      "id": "Browne_2024_Engineers_on_responsibility__feminist_approaches_t",
      "filename": "Browne_2024_Engineers_on_responsibility__feminist_approaches_t.md",
      "title": "Engineers on responsibility: feminist approaches to who's responsible for ethical AI",
      "author_year": "Browne (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1007/s10676-023-09739-1",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 1,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 7,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Practical Implementation"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Through interviews with AI practitioners interpreted via feminist political thought, reimagines responsibility in AI development beyond individualized approaches. Critiques current AI responsibility frameworks focused on individual competency and technical solutions, proposing instead \"responsibility as the product of work cultures that enable tech workers to be responsive and answerable for their products.\" Moves beyond \"individual competency approaches\" toward understanding responsibility as e",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "5TV6QMRD"
    },
    {
      "id": "Browne_2024_Tech_workers'_perspectives_on_ethical_issues_in_AI",
      "filename": "Browne_2024_Tech_workers'_perspectives_on_ethical_issues_in_AI.md",
      "title": "Tech workers' perspectives on ethical issues in AI development: Foregrounding feminist approaches",
      "author_year": "Browne (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1177/20539517231221780",
      "url": "https://doi.org/10.1177/20539517231221780",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Empirical study examining tech workers' understanding of ethical AI development through feminist lens, revealing critical gaps in current AI ethics approaches. Finds that the term \"bias\" creates confusion among tech workers, undermining AI ethics initiatives, and argues for moving beyond diversity narratives toward \"design justice\" that centers marginalized voices.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "GFL24IE5"
    },
    {
      "id": "Casal-Otero_2023_AI_literacy_in_K-12__a_systematic_literature_revie",
      "filename": "Casal-Otero_2023_AI_literacy_in_K-12__a_systematic_literature_revie.md",
      "title": "AI literacy in K-12: a systematic literature review",
      "author_year": "Casal-Otero (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1186/s40594-023-00418-7",
      "url": "https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00418-7",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Abstract The successful irruption of AI-based technology in our daily lives has led to a growing educational, social, and political interest in training citizens in AI. Education systems now need to train students at the K-12 level to live in a society where they must interact with AI. Thus, AI literacy is a pedagogical and cognitive challenge at the K-12 level. This study aimed to understand how AI is being integrated into K-12 education worldwide. We conducted a search process following the sy",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "EDU8F937"
    },
    {
      "id": "Charlesworth_2024_Flexible_intersectional_stereotype_extraction_(FIS",
      "filename": "Charlesworth_2024_Flexible_intersectional_stereotype_extraction_(FIS.md",
      "title": "Flexible intersectional stereotype extraction (FISE): Analyzing intersectional biases in large language models",
      "author_year": "Charlesworth (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://news.northwestern.edu/stories/2024/03/kellogg-study-suggests-that-some-intersectional-groups-are-more-represented-than-others-in-internet-text/",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Studie entwickelt FISE-Methode zur Messung intersektionaler Repräsentationsverzerrungen. Zeigt massive Dominanz weißer Männer in Internettexten und Ableitung entsprechender LLM-Biases.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "48NCCNDH"
    },
    {
      "id": "Chatterji_2025_How_People_Use_ChatGPT",
      "filename": "Chatterji_2025_How_People_Use_ChatGPT.md",
      "title": "How People Use ChatGPT",
      "author_year": "Chatterji (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "en",
      "doi": "nan",
      "url": "https://www.nber.org/papers/w34255",
      "decision": "Exclude",
      "exclusion_reason": "Wrong publication type",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Founded in 1920, the NBER is a private, non-profit, non-partisan organization dedicated to conducting economic research and to disseminating research findings among academics, public policy makers, and business professionals.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "49VKYVJV"
    },
    {
      "id": "Chee_2025_A_Competency_Framework_for_AI_Literacy__Variations",
      "filename": "Chee_2025_A_Competency_Framework_for_AI_Literacy__Variations.md",
      "title": "A Competency Framework for AI Literacy: Variations by Different Learner Groups and an Implied Learning Pathway",
      "author_year": "Chee (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1111/bjet.13556",
      "url": "https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13556",
      "decision": "Unclear",
      "exclusion_reason": "nan",
      "rel_ai_komp": 3,
      "rel_vulnerable": 1,
      "rel_bias": 1,
      "rel_praxis": 2,
      "rel_prof": 0,
      "total_relevance": 7,
      "relevance_category": "medium",
      "top_dimensions": [
        "AI Literacy",
        "Practical Implementation"
      ],
      "tags": [
        "paper",
        "unclear",
        "medium-relevance",
        "dim-ai-komp-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This study aims to develop a comprehensive competency framework for artificial intelligence (AI) literacy, delineating essential competencies and sub‐competencies. This framework and its potential variations, tailored to different learner groups (by educational level and discipline), can serve as a crucial reference for designing and implementing AI curricula. However, the research on AI literacy by target learners is still in its infancy, and the findings of several existing studies provide inc",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "9YRIUFDC"
    },
    {
      "id": "Cheng_2022_How_child_welfare_workers_reduce_racial_disparitie",
      "filename": "Cheng_2022_How_child_welfare_workers_reduce_racial_disparitie.md",
      "title": "How child welfare workers reduce racial disparities in algorithmic decisions",
      "author_year": "Cheng (2022)",
      "authors": [],
      "publication_year": 2022.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.1145/3491102.3501831",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 3,
      "rel_prof": 3,
      "total_relevance": 13,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-high",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Mixed-methods study analyzing four years of child welfare call screening data alongside worker interviews to investigate how human-algorithm collaboration affects racial bias in decision-making. Demonstrates Allegheny Family Screening Tool algorithm alone would have created 20% disparity in screen-in rates between Black and white children, but workers reduced this to 9% through holistic risk assessments and adjustments for algorithmic limitations. Reveals critical discrimination risks: algorithm",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "C3UQDJJA"
    },
    {
      "id": "Chen_2024_Exploring_complex_mental_health_symptoms_via_class",
      "filename": "Chen_2024_Exploring_complex_mental_health_symptoms_via_class.md",
      "title": "Exploring complex mental health symptoms via classifying social media data with explainable LLMs",
      "author_year": "Chen (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.arxivdaily.com/thread/62478",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "YRTQN7KB"
    },
    {
      "id": "Chen_2025_Social_work_and_artificial_intelligence__Collabora",
      "filename": "Chen_2025_Social_work_and_artificial_intelligence__Collabora.md",
      "title": "Social work and artificial intelligence: Collaboration and challenges",
      "author_year": "Chen (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://dmjr-journals.com/assets/article/1755892935-SOCIAL_WORK_&_ARTIFICIAL_INTELLIGENCE-_COLLABORATION_AND_CHALLENGES.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 1,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Professional Context",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-medium",
        "dim-bias-medium",
        "dim-praxis-medium",
        "dim-prof-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Chen_2025_Social.md",
      "abstract": "This qualitative study explores current AI applications in social work through interviews with professionals, AI developers, and policymakers, identifying challenges including insufficient decision-making transparency, gaps in ethical frameworks, and inadequate technical literacy among professionals. The research reveals that while ninety percent of social work professionals acknowledge AI's auxiliary function in daily operations, concerns persist about automation bias and the potential undermin",
      "summary_section": "## Overview\n\nThis peer-reviewed academic study, published in the Journals of Business & Management Studies (Vol. 1, Issue 2, July 2025), examines the integration of artificial intelligence technology within social work practice through qualitative research involving social work professionals, AI dev",
      "source_tool": "Manual",
      "zotero_key": "9I8UUAMQ"
    },
    {
      "id": "Cher_2024_Exploring_machine_learning_to_support_decision-mak",
      "filename": "Cher_2024_Exploring_machine_learning_to_support_decision-mak.md",
      "title": "Exploring machine learning to support decision-making for placement stabilization and preservation in child welfare",
      "author_year": "Cher (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1007/s10826-024-02993-x",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 3,
      "rel_prof": 3,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Practical Implementation",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-high",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Statewide study analyzed 12,621 child welfare cases in large Midwestern state (2017-2020) to develop machine learning models predicting placement disruption risk. Goal was to identify youth who could benefit from placement stabilization services to prevent unnecessary residential care under Family First Prevention Services Act. Random forest models were compared with conventional logistic regression for predicting placement disruption and referral to stabilization programs. ML models demonstrate",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "IAW426ZK"
    },
    {
      "id": "Chisca_2024_Prompting_fairness__Learning_prompts_for_debiasing",
      "filename": "Chisca_2024_Prompting_fairness__Learning_prompts_for_debiasing.md",
      "title": "Prompting fairness: Learning prompts for debiasing large language models",
      "author_year": "Chisca (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "https://aclanthology.org/2024.ltedi-1.6/",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Introduces novel prompt-tuning method for reducing biases in encoder models like BERT and RoBERTa through training small sets of additional reusable token embeddings. Demonstrates state-of-the-art performance while maintaining minimal impact on language modeling capabilities through parameter-efficient approach applicable across different models and tasks.",
      "summary_section": "## Overview\n\nThis research addresses a critical challenge in contemporary NLP: mitigating social biases embedded in Large Language Models (LLMs) such as BERT and RoBERTa. The paper proposes an innovative prompt-tuning approach that efficiently reduces gender bias without requiring computationally ex",
      "source_tool": "Manual",
      "zotero_key": "MHZTET9D"
    },
    {
      "id": "Chisca_2024_Prompting_techniques_for_reducing_social_bias_in_L",
      "filename": "Chisca_2024_Prompting_techniques_for_reducing_social_bias_in_L.md",
      "title": "Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes",
      "author_year": "Chisca (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2404.17218v3",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": true,
      "summary_file": "summary_Chisca_2024_Prompting.md",
      "abstract": "nan",
      "summary_section": "## Overview\n\nThis research addresses a critical challenge in contemporary NLP: mitigating social biases embedded in Large Language Models (LLMs) such as BERT and RoBERTa. The paper proposes an innovative prompt-tuning approach that efficiently reduces gender bias without requiring computationally ex",
      "source_tool": "Manual",
      "zotero_key": "UDWBF53B"
    },
    {
      "id": "Chiu_2024_What_are_artificial_intelligence_literacy_and_comp",
      "filename": "Chiu_2024_What_are_artificial_intelligence_literacy_and_comp.md",
      "title": "What are artificial intelligence literacy and competency? A comprehensive framework to support them",
      "author_year": "Chiu (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1016/j.caeo.2024.100171",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2666557324000120",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "I49J8KMK"
    },
    {
      "id": "Chiu_2025_AI_literacy_and_competency__definitions,_framework",
      "filename": "Chiu_2025_AI_literacy_and_competency__definitions,_framework.md",
      "title": "AI literacy and competency: definitions, frameworks, development and future research directions",
      "author_year": "Chiu (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1080/10494820.2025.2514372",
      "url": "https://www.tandfonline.com/doi/full/10.1080/10494820.2025.2514372",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "3FXYBAWC"
    },
    {
      "id": "Choudhury_2024_Large_Language_Models_and_User_Trust__Consequence_",
      "filename": "Choudhury_2024_Large_Language_Models_and_User_Trust__Consequence_.md",
      "title": "Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals",
      "author_year": "Choudhury (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.2196/56764",
      "url": "https://www.jmir.org/2024/1/e56764/",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 1,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 2,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Practical Implementation"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-bias-high",
        "dim-praxis-medium",
        "dim-prof-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Peer-reviewed viewpoint discussing integration of LLMs into healthcare requiring balance between trust and skepticism. Warns that \"blind trust\" in LLM recommendations can lead to automation bias and confirmation bias as clinicians may accept AI outputs uncritically. Argues that prompting for transparency in LLM reasoning enhances professionals' trust by making AI reasoning visible and verifiable. Emphasizes need for human oversight and bias mitigation to ensure LLMs complement rather than compro",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "HR4KR4YL"
    },
    {
      "id": "Ciston_2024_Intersectional_Artificial_Intelligence_Is_Essentia",
      "filename": "Ciston_2024_Intersectional_Artificial_Intelligence_Is_Essentia.md",
      "title": "Intersectional Artificial Intelligence Is Essential: Polyvocal, Multimodal, Experimental Methods to Save AI",
      "author_year": "Ciston (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.7559/CITARJ.V11I2.665",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Arguments for applying intersectional strategies to AI at all levels - from data through design to implementation. Intersectionality, integrating institutional power analysis and queer-feminist plus critical race theories, can contribute to AI reconceptualization. Intersectional framework enables analysis of existing AI biases and uncovering alternative ethics from counter-narratives. Research presents intersectional strategies as polyvocal, multimodal, and experimental, with community-focused a",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "8GG7KEEY"
    },
    {
      "id": "Clemmer_2024_PreciseDebias__An_automatic_prompt_engineering_app",
      "filename": "Clemmer_2024_PreciseDebias__An_automatic_prompt_engineering_app.md",
      "title": "PreciseDebias: An automatic prompt engineering approach for generative AI to mitigate image demographic biases",
      "author_year": "Clemmer (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "https://openaccess.thecvf.com/content/WACV2024/html/Clemmer_PreciseDebias_An_Automatic_Prompt_Engineering_Approach_for_Generative_AI_WACV_2024_paper.html",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This paper presents a technical solution for reducing demographic bias in AI image generators through \"PreciseDebias,\" an automated approach that rewrites simple prompts into more complex, diversity-reflective prompts. The system analyzes model bias and strategically adds attributes like ethnicity, gender, or age to enforce fairer representation distributions, demonstrating that diversity-reflective prompting can be automated at the system level.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "I32VV8SK"
    },
    {
      "id": "Colombatto_2025_The_influence_of_mental_state_attributions_on_trus",
      "filename": "Colombatto_2025_The_influence_of_mental_state_attributions_on_trus.md",
      "title": "The influence of mental state attributions on trust in large language models",
      "author_year": "Colombatto (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1038/s44271-025-00262-1",
      "url": "https://www.nature.com/articles/s44271-025-00262-1",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 1,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "AI Literacy",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-ai-komp-medium",
        "dim-bias-medium",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Colombatto_2025_influence.md",
      "abstract": "Empirical study examining how users' beliefs about LLM's \"mind\" affect trust in advice. Preregistered experiment (N=410) showing attributing intelligence to LLM strongly increased trust, whereas attributing human-like consciousness did not increase trust. Higher ascriptions of sentience correlated with less advice-taking. Participants trusted AI for perceived analytical competence, not for having feelings. Suggests prompt-engineering highlighting competence and reliability more effective than an",
      "summary_section": "![[summary_Colombatto_2025_influence.md]]",
      "source_tool": "Manual",
      "zotero_key": "7XBEESMD"
    },
    {
      "id": "Creswell_Báez_2025_Clinical_Social_Workers’_Perceptions_of_Large_Lang",
      "filename": "Creswell_Báez_2025_Clinical_Social_Workers’_Perceptions_of_Large_Lang.md",
      "title": "Clinical Social Workers’ Perceptions of Large Language Models in Practice: Resistance to Automation and Prospects for Integration",
      "author_year": "Creswell Báez (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1080/26408066.2025.2542450",
      "url": "https://doi.org/10.1080/26408066.2025.2542450",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 1,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Professional Context",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-medium",
        "dim-bias-medium",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Qualitative study (interviews, reflexive thematic analysis) of clinicians exposed to LLM-supported consultation. Identifies efficiency and documentation support alongside concerns over confidentiality, loss of nuance, and reduced empathy. Concludes AI should augment—not replace—clinical judgment, requiring training and ethics.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "TKHRE6ZY"
    },
    {
      "id": "Cvoelcker_2023_Queer_in_AI__A_case_study_in_community-led_partici",
      "filename": "Cvoelcker_2023_Queer_in_AI__A_case_study_in_community-led_partici.md",
      "title": "Queer in AI: A case study in community-led participatory AI",
      "author_year": "Cvoelcker (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "https://cvoelcker.de/assets/pdf/paper_queer_in_ai.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Cvoelcker_2023_Queer.md",
      "abstract": "Fallstudie zu Queer in AI, dokumentiert Schäden durch KI-Systeme an queeren Menschen und beschreibt community-geleitete Strategien für partizipative, faire KI.",
      "summary_section": "## Overview\n\nThe Queer in AI case study documents a globally distributed, community-led participatory initiative addressing systemic underrepresentation of LGBTQ+ professionals within artificial intelligence. Comprising 50+ contributors across 15+ countries embedded in leading research institutions ",
      "source_tool": "Manual",
      "zotero_key": "ZQAKXVE6"
    },
    {
      "id": "D'Ignazio_2024_Data_Feminism_for_AI",
      "filename": "D'Ignazio_2024_Data_Feminism_for_AI.md",
      "title": "Data Feminism for AI",
      "author_year": "D'Ignazio (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2405.01286v1",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high"
      ],
      "has_summary": true,
      "summary_file": "summary_D_Ignazio_2024_Data.md",
      "abstract": "Comprehensive feminist framework directly critiques individualized approaches to AI ethics, challenging the \"liberal framework of making algorithms unbiased and inclusive\" in favor of structural \"remediation\" addressing \"systemic and structural dimensions of discrimination.\" Examines how AI research is captured by \"racial, gendered capitalism\" and proposes nine principles focusing on structural power analysis including examining power, challenging power, and making labor visible. Explicitly posi",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "G82RHBDD"
    },
    {
      "id": "Debnath_2024_Can_LLMs_reason_about_trust__A_pilot_study",
      "filename": "Debnath_2024_Can_LLMs_reason_about_trust__A_pilot_study.md",
      "title": "Can LLMs reason about trust? A pilot study",
      "author_year": "Debnath (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2507.21075v1",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 0,
      "rel_bias": 1,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 6,
      "relevance_category": "medium",
      "top_dimensions": [
        "AI Literacy",
        "Practical Implementation"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-ai-komp-medium",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Debnath_2024_LLMs.md",
      "abstract": "Pilot study investigating LLMs' ability to reason about trust between individuals by capturing complex mental states that exceed traditional symbolic approaches. Evaluates whether LLMs can analyze conversations and assess trust levels based on willingness, competence, and security factors. Demonstrates that LLMs can identify individual goals and develop improvement suggestions for trust building, as well as generate strategic action plans for trust promotion. Suggests LLMs have potential to brid",
      "summary_section": "![[summary_Debnath_2024_LLMs.md]]",
      "source_tool": "Manual",
      "zotero_key": "VLC4TVQT"
    },
    {
      "id": "Dencik_2024_Automated_government_benefits_and_welfare_surveill",
      "filename": "Dencik_2024_Automated_government_benefits_and_welfare_surveill.md",
      "title": "Automated government benefits and welfare surveillance",
      "author_year": "Dencik (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://ojs.library.queensu.ca/index.php/surveillance-and-society/article/view/16107",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 2,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-prof-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Critical surveillance studies analysis examining digital welfare state historically, presently, and prospectively, focusing on AI-driven welfare surveillance systems. Authors argue problems posed by AI in public administration are often misattributed to technological novelty when they actually represent historically familiar patterns of surveillance and control. Drawing on bureaucracy, welfare state, and automation scholarship, demonstrates how algorithmic fraud detection and chatbot assistance ",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "TIX2ZW6A"
    },
    {
      "id": "Derechos_Digitales_2023_Feminist_reflections_for_the_development_of_Artifi",
      "filename": "Derechos_Digitales_2023_Feminist_reflections_for_the_development_of_Artifi.md",
      "title": "Feminist reflections for the development of Artificial Intelligence",
      "author_year": "Derechos Digitales (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.derechosdigitales.org/fair-2023-en/",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-medium",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Synthesizes conversations among Latin American women developing AI systems, providing methodological frameworks for feminist AI development based on four real-world projects. Emphasizes co-design methodologies, digital autonomy, data sovereignty, and intersectional approaches through community agreements and power-balancing strategies.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "Q5VIXYTS"
    },
    {
      "id": "Deuze_2022_Imagination,_Algorithms_and_News__Developing_AI_Li",
      "filename": "Deuze_2022_Imagination,_Algorithms_and_News__Developing_AI_Li.md",
      "title": "Imagination, Algorithms and News: Developing AI Literacy for Journalism",
      "author_year": "Deuze (2022)",
      "authors": [],
      "publication_year": 2022.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1080/21670811.2022.2119152",
      "url": "https://www.tandfonline.com/doi/full/10.1080/21670811.2022.2119152",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "4CK7LDP3"
    },
    {
      "id": "De_Duro_2025_Measuring_and_identifying_factors_of_individuals'_",
      "filename": "De_Duro_2025_Measuring_and_identifying_factors_of_individuals'_.md",
      "title": "Measuring and identifying factors of individuals' trust in large language models",
      "author_year": "De Duro (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.48550/arXiv.2502.21028",
      "url": "https://arxiv.org/html/2502.21028v1",
      "decision": "Unclear",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 4,
      "relevance_category": "low",
      "top_dimensions": [
        "Practical Implementation",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "unclear",
        "low-relevance",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Study developing \"Trust-In-LLMs Index (TILLMI)\" psychometric scale to quantify trust in conversational AI. Survey of ~1,000 U.S. adults identified two trust dimensions: affective component (emotional closeness) and cognitive component (reliance on competence). Found significant individual differences with personality factors influencing trust. Prior LLM experience increased trust while those with no direct use were more skeptical. Highlights need for prompt-engineering and system design to accou",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "4EI9H33G"
    },
    {
      "id": "Dilek_2025_AI_literacy_in_teacher_education__Empowering_educa",
      "filename": "Dilek_2025_AI_literacy_in_teacher_education__Empowering_educa.md",
      "title": "AI literacy in teacher education: Empowering educators through critical co-discovery",
      "author_year": "Dilek (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1177/00224871251325083",
      "url": "nan",
      "decision": "Exclude",
      "exclusion_reason": "Wrong publication type",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Implements critical co-discovery approaches within AI teacher education to move beyond technical automation toward critical pedagogical engagement. Through co-discovery activities, educators developed understanding of AI concepts, ethical considerations, and context-specific applications while co-constructing knowledge. Emphasizes that prolonged engagement with AI literacy integrated into teacher education programs enables educators to critically navigate AI systems and examine broader pedagogic",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "TGIXGKR6"
    },
    {
      "id": "DIVERSIFAIR_Project_2024_AI_&_Intersectionality__A_Toolkit_For_Fairness_&_I",
      "filename": "DIVERSIFAIR_Project_2024_AI_&_Intersectionality__A_Toolkit_For_Fairness_&_I.md",
      "title": "AI & Intersectionality: A Toolkit For Fairness & Inclusion",
      "author_year": "DIVERSIFAIR Project (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://diversifair-project.eu/wp-content/uploads/2025/01/Copie-de-INDUSTRY-Educ-Kits-A4-V2.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 11,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-medium",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_DIVERSIFAIR_Project_2024_Intersectionality.md",
      "abstract": "Das DIVERSIFAIR-Toolkit ist eine praktische Ressource, die sich an politische Entscheidungsträger*innen, die Industrie und die Zivilgesellschaft richtet. Es zielt darauf ab, ein Bewusstsein für intersektionale Diskriminierung in KI-Systemen zu schaffen und konkrete Handlungsstrategien zur Risikominderung anzubieten. Das Toolkit betont die Notwendigkeit, über einzelne Diskriminierungsachsen (wie Geschlecht oder Herkunft) hinauszudenken und deren Verschränkungen zu analysieren. Es fördert eine KI-",
      "summary_section": "![[summary_DIVERSIFAIR_Project_2024_Intersectionality.md]]",
      "source_tool": "Manual",
      "zotero_key": "9XWIAWFN"
    },
    {
      "id": "Dixon_2018_Measuring_and_mitigating_unintended_bias_in_text_d",
      "filename": "Dixon_2018_Measuring_and_mitigating_unintended_bias_in_text_d.md",
      "title": "Measuring and mitigating unintended bias in text data",
      "author_year": "Dixon (2018)",
      "authors": [],
      "publication_year": 2018.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "nan",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "95UW59KT"
    },
    {
      "id": "Djeffal_2025_Reflexive_prompt_engineering__A_framework_for_resp",
      "filename": "Djeffal_2025_Reflexive_prompt_engineering__A_framework_for_resp.md",
      "title": "Reflexive prompt engineering: A framework for responsible prompt engineering and AI interaction design",
      "author_year": "Djeffal (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.1145/3715275.3732118",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 3,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "AI Literacy",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-high",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This paper proposes \"Reflexive Prompt Engineering\" as a comprehensive framework to embed ethical and inclusive principles into prompt crafting for generative AI. Framework consists of five components: prompt design, system selection, system configuration, performance evaluation, and prompt management, each considered from social responsibility perspective. Positions responsible prompt engineering as essential component of AI literacy, bridging gap between AI development and deployment by alignin",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "YH8KK797"
    },
    {
      "id": "Djiberou_Mahamadou_2024_Revisiting_Technical_Bias_Mitigation_Strategies",
      "filename": "Djiberou_Mahamadou_2024_Revisiting_Technical_Bias_Mitigation_Strategies.md",
      "title": "Revisiting Technical Bias Mitigation Strategies",
      "author_year": "Djiberou Mahamadou (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/abs/2410.17433",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Djiberou_Mahamadou_2024_Revisiting.md",
      "abstract": "Diese systematische Überprüfung identifiziert praktische Limitationen technischer Bias-Mitigation-Strategien im Gesundheitswesen entlang fünf Schlüsseldimensionen: wer Bias und Fairness definiert, welche Mitigation-Strategie zu verwenden und zu priorisieren ist, wann in den KI-Entwicklungsstadien die Lösungen am effektivsten sind, für welche Populationen und in welchem Kontext die Lösungen entworfen sind. Die Studie zeigt mathematische Inkonsistenzen und Inkompatibilitäten zwischen verschiedenen",
      "summary_section": "## Overview\n\nThis Stanford-based review systematically examines why technical approaches to bias mitigation in healthcare AI frequently fail in real-world implementation despite their theoretical soundness. The authors conduct a critical analysis of practical limitations inherent in current bias mit",
      "source_tool": "Manual",
      "zotero_key": "KQU9D6DL"
    },
    {
      "id": "Engelhardt_2025_Voll_(dia)logisch__Ein_Werkstattbericht_über_den_E",
      "filename": "Engelhardt_2025_Voll_(dia)logisch__Ein_Werkstattbericht_über_den_E.md",
      "title": "Voll (dia)logisch? Ein Werkstattbericht über den Einsatz von generativer KI in der Hochschulbildung für Soziale Arbeit – Curriculare Überlegungen und veränderte Akteurskonstellationen",
      "author_year": "Engelhardt (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "bookSection",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.pedocs.de/volltexte/2025/33133/pdf/Engelhardt_Ley_2025_Voll_dia_logisch.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 3,
      "rel_vulnerable": 1,
      "rel_bias": 1,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "AI Literacy",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-high",
        "dim-praxis-medium",
        "dim-prof-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Engelhardt_2025_Voll.md",
      "abstract": "Werkstattbericht zur curricularen Integration generativer KI. Positioniert Prompting als metakognitive Schlüsselkompetenz und diskutiert Rollenwandel von Lehr-/Lernakteuren; fordert reflektierte, ethisch eingebettete Nutzung mit Fokus auf kritische Validierung von KI-Ergebnissen.",
      "summary_section": "## Overview\n\nEngelhardt and Ley's chapter addresses the critical challenge of integrating generative AI into social work higher education while maintaining academic integrity and critical thinking. Published in a 2025 edited volume on digitalization in social work education, this work is explicitly ",
      "source_tool": "Manual",
      "zotero_key": "MCLFUR45"
    },
    {
      "id": "European_Commission._Joint_Research_Centre._2017_DigComp_2.1__the_digital_competence_framework_for_",
      "filename": "European_Commission._Joint_Research_Centre._2017_DigComp_2.1__the_digital_competence_framework_for_.md",
      "title": "DigComp 2.1: the digital competence framework for citizens with eight proficiency levels and examples of use.",
      "author_year": "European Commission. Joint Research Centre. (2017)",
      "authors": [],
      "publication_year": 2017.0,
      "item_type": "book",
      "language": "en",
      "doi": "nan",
      "url": "https://data.europa.eu/doi/10.2760/38842",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "UI47PY9E"
    },
    {
      "id": "European_Data_Protection_Supervisor_2023_Explainable_Artificial_Intelligence",
      "filename": "European_Data_Protection_Supervisor_2023_Explainable_Artificial_Intelligence.md",
      "title": "Explainable Artificial Intelligence",
      "author_year": "European Data Protection Supervisor (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.edps.europa.eu/system/files/2023-11/23-11-16_techdispatch_xai_en.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_European_Data_Protection_Supervisor_2023_Explainab.md",
      "abstract": "Emphasizes the necessity of Explainable AI (XAI) to ensure human-centered, transparent, and ethical AI systems. By increasing transparency and comprehensibility of algorithmic decisions, XAI enables individuals—including those from marginalized groups—to participate meaningfully in digital decision-making and challenge unjust outcomes.",
      "summary_section": "## Overview\n\nThe EDPS TechDispatch addresses a critical governance challenge in contemporary AI deployment: the \"black box\" problem. This regulatory document, issued by the European Data Protection Supervisor, examines why artificial intelligence systems' opacity poses unacceptable risks in automate",
      "source_tool": "Manual",
      "zotero_key": "CE3C2JNF"
    },
    {
      "id": "Feist-Ortmanns_2025_KI-basiertes_Assistenzsystem_im_Kinderschutzverfah",
      "filename": "Feist-Ortmanns_2025_KI-basiertes_Assistenzsystem_im_Kinderschutzverfah.md",
      "title": "KI-basiertes Assistenzsystem im Kinderschutzverfahren",
      "author_year": "Feist-Ortmanns (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "bookSection",
      "language": "nan",
      "doi": "nan",
      "url": "https://afet-ev.de/assets/themenplattform/KI-in-der-Kinder--und-Jugendhilfe-Naher-Grasshoff-Forum-2.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 3,
      "rel_prof": 3,
      "total_relevance": 11,
      "relevance_category": "high",
      "top_dimensions": [
        "Practical Implementation",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-high",
        "dim-prof-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Feist-Ortmanns_2025_basiertes.md",
      "abstract": "Praxisnaher Bericht zu einem KI-Assistenzsystem für Gefährdungseinschätzung. Betont Human-in-the-Loop, Datenschutz, Organisationsentwicklung und Schulungsbedarf; zeigt Potenziale (präventive Hinweise) und Risiken (Bias, Fehlklassifikationen) im sensiblen Kinderschutzkontext.",
      "summary_section": "![[summary_Feist-Ortmanns_2025_basiertes.md]]",
      "source_tool": "Manual",
      "zotero_key": "6W9NNFY2"
    },
    {
      "id": "Field_2023_Examining_risks_of_racial_biases_in_NLP_tools_for_",
      "filename": "Field_2023_Examining_risks_of_racial_biases_in_NLP_tools_for_.md",
      "title": "Examining risks of racial biases in NLP tools for child protective services",
      "author_year": "Field (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.1145/3593013.3594094",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 2,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-prof-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Empirical study examining racial bias in natural language processing tools used to analyze child protective services case notes and make risk assessments. Demonstrates language models trained on case narratives exhibit systematic biases disadvantaging families of color. Testing multiple NLP architectures on real child welfare text data, finds models consistently predict higher risk scores for cases mentioning dialects or cultural contexts associated with Black and Latinx families, even when case",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "5UGREN98"
    },
    {
      "id": "Fraile-Rojas_2025_Female_perspectives_on_algorithmic_bias__Implicati",
      "filename": "Fraile-Rojas_2025_Female_perspectives_on_algorithmic_bias__Implicati.md",
      "title": "Female perspectives on algorithmic bias: Implications for AI researchers and practitioners",
      "author_year": "Fraile-Rojas (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1108/MD-04-2024-0884",
      "url": "https://colab.ws/articles/10.1108%2Fmd-04-2024-0884",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This study uses NLP and machine learning to analyze 172,041 tweets from female users discussing gender inequality in AI. It identifies prominent themes including the future of AI technologies and women's active role in ensuring gender-balanced systems. Findings show that algorithmic bias directly affects women's experiences, prompting engagement in online discourse about injustices. Women lead constructive conversations and create entrepreneurial solutions when faced with bias, demonstrating how",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "HVLX8ERT"
    },
    {
      "id": "Franken__Gender_und_KI-Anwendungen._Trägt_KI_zum_Genderprob",
      "filename": "Franken__Gender_und_KI-Anwendungen._Trägt_KI_zum_Genderprob.md",
      "title": "Gender und KI-Anwendungen. Trägt KI zum Genderproblem oder zu seiner Lösung bei?",
      "author_year": "Franken ()",
      "authors": [],
      "publication_year": "nan",
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.fh-bielefeld.de/multimedia/Fachbereiche/Wirtschaft+und+Gesundheit/ Forschung/Denkfabrik+Digitalisierte+Arbeitswelt/geki_Abschlussbericht.pdf",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "VBCWG8MC"
    },
    {
      "id": "Freinhofer_2025_Prompten_nach_Plan__Das_PCRR-Framework_als_pädagog",
      "filename": "Freinhofer_2025_Prompten_nach_Plan__Das_PCRR-Framework_als_pädagog.md",
      "title": "Prompten nach Plan: Das PCRR-Framework als pädagogisches Werkzeug für den Einsatz von Künstlicher Intelligenz.",
      "author_year": "Freinhofer (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "de",
      "doi": "10.21243/mi-01-25-26",
      "url": "https://journals.univie.ac.at/index.php/mp/article/view/9274",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Die rasante Entwicklung generativer Künstlicher Intelligenz (KI) macht Prompt Engineering zu einer Schlüsselkompetenz für den kompetenten Umgang mit KI-Modellen. Während zahlreiche Prompting-Techniken und -Frameworks existieren, fehlt bislang eine systematische Integration in den schulischen Kontext. Diese Publikation stellt das PCRR-Framework (Plan – Create – Review – Reflect) vor, das als ganzheitlicher Ansatz für den Einsatz von Prompt Engineering im Unterricht dient. Basierend auf Erfahrunge",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "ENXUUHW8"
    },
    {
      "id": "Friedrich-Ebert-Stiftung_2025_The_EU_artificial_intelligence_act_through_a_gende",
      "filename": "Friedrich-Ebert-Stiftung_2025_The_EU_artificial_intelligence_act_through_a_gende.md",
      "title": "The EU artificial intelligence act through a gender lens",
      "author_year": "Friedrich-Ebert-Stiftung (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://library.fes.de/pdf-files/bueros/bruessel/21887-20250304.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Friedrich-Ebert-Stiftung_2025_artificial.md",
      "abstract": "Politikanalyse des EU AI Acts mit Fokus auf Geschlechtergerechtigkeit. Identifiziert Potenziale und Lücken im Gesetzestext und gibt konkrete Empfehlungen zur Umsetzung.",
      "summary_section": "![[summary_Friedrich-Ebert-Stiftung_2025_artificial.md]]",
      "source_tool": "Manual",
      "zotero_key": "LGNJHQQL"
    },
    {
      "id": "Fujii_2024_Bildungsteilhabe_-_Flucht_-_Digitalisierung__Eine_",
      "filename": "Fujii_2024_Bildungsteilhabe_-_Flucht_-_Digitalisierung__Eine_.md",
      "title": "Bildungsteilhabe - Flucht - Digitalisierung: Eine multilokale Ethnografie im (digitalen) Alltag junger Geflüchteter",
      "author_year": "Fujii (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "book",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.beltz.de/fachmedien/sozialpaedagogik_soziale_arbeit/produkte/details/53146",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Presents ethnographic research on how digital media shapes educational participation for young refugees, examining ambivalent role of digital technologies: enabling connection to educational resources and transnational networks while simultaneously creating new forms of exclusion through surveillance, documentation requirements, and digital skill barriers. Reveals digital access alone does not guarantee educational participation—digital literacy, linguistic competence, and social support remain ",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "BBPB9TPN"
    },
    {
      "id": "Furniturewala_2024_Reasoning_towards_fairness__Mitigating_bias_in_lan",
      "filename": "Furniturewala_2024_Reasoning_towards_fairness__Mitigating_bias_in_lan.md",
      "title": "Reasoning towards fairness: Mitigating bias in language models through reasoning-guided fine-tuning",
      "author_year": "Furniturewala (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://powerdrill.ai/discover/summary-reasoning-towards-fairness-mitigating-bias-in-cm9af0g1h7sb507pn7f5qh32q",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "FDIUT6YU"
    },
    {
      "id": "Gaba_2025_Bias,_accuracy,_and_trust__Gender-diverse_perspect",
      "filename": "Gaba_2025_Bias,_accuracy,_and_trust__Gender-diverse_perspect.md",
      "title": "Bias, accuracy, and trust: Gender-diverse perspectives on large language models",
      "author_year": "Gaba (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/abs/2506.21898",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Gaba_2025_Bias.md",
      "abstract": "Qualitative study with 25 interviews exploring how users of different gender identities perceive bias and trustworthiness in ChatGPT. Found perceived biases in LLM outputs undermine user trust, with non-binary participants encountering stereotyped responses that eroded confidence. Trust levels varied by group with male participants reporting higher trust. Participants recommended bias mitigation strategies including diversifying training data and implementing clarifying follow-up prompts to chec",
      "summary_section": "## Overview\n\nThis empirical study examines how gender-diverse populations perceive bias, accuracy, and trustworthiness in Large Language Models, specifically ChatGPT. Conducted by researchers across multiple institutions and submitted to ACM for publication in the CSCW/HCI field, the work addresses ",
      "source_tool": "Manual",
      "zotero_key": "AGNLCTFB"
    },
    {
      "id": "Gallegos_2024_Bias_and_fairness_in_large_language_models__A_surv",
      "filename": "Gallegos_2024_Bias_and_fairness_in_large_language_models__A_surv.md",
      "title": "Bias and fairness in large language models: A survey",
      "author_year": "Gallegos (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1162/coli_a_00524",
      "url": "https://doi.org/10.1162/coli_a_00524",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Comprehensive survey consolidating notions of social bias and fairness in NLP, defining distinct facets of harm and introducing desiderata to operationalize fairness for LLMs. Proposes three taxonomies: metrics for bias evaluation, datasets for bias evaluation, and techniques for bias mitigation classified by intervention timing.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "FIZ9DG6J"
    },
    {
      "id": "Garg_2019_Counterfactual_fairness_in_text_classification_thr",
      "filename": "Garg_2019_Counterfactual_fairness_in_text_classification_thr.md",
      "title": "Counterfactual fairness in text classification through robustness",
      "author_year": "Garg (2019)",
      "authors": [],
      "publication_year": 2019.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "nan",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "9BUXCM4X"
    },
    {
      "id": "Garkisch_2024_Considering_a_unified_model_of_artificial_intellig",
      "filename": "Garkisch_2024_Considering_a_unified_model_of_artificial_intellig.md",
      "title": "Considering a unified model of artificial intelligence enhanced social work: A systematic review",
      "author_year": "Garkisch (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1007/s41134-024-00326-y",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 1,
      "rel_bias": 1,
      "rel_praxis": 1,
      "rel_prof": 3,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Professional Context",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-ai-komp-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Systematic review mapping research landscape of social work AI scholarship, analyzing 67 articles using qualitative analytic approaches to explore how social work researchers investigate AI. Identified themes consistent with Staub-Bernasconi's triple mandate covering profession level, social agencies/organizations, and clients. Emphasizes importance of enhancing computational thinking, AI literacy, and data literacy, and developing skills for evaluating automated systems. Stresses that professio",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "2C2GPB6P"
    },
    {
      "id": "Gengler_2024_Faires_KI-Prompting_–_Ein_Leitfaden_für_Unternehme",
      "filename": "Gengler_2024_Faires_KI-Prompting_–_Ein_Leitfaden_für_Unternehme.md",
      "title": "Faires KI-Prompting – Ein Leitfaden für Unternehmen",
      "author_year": "Gengler (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.digitalzentrum-zukunftskultur.de/wp-content/uploads/2024/05/Faires-KI-Prompting-Ein-Leitfaden-fuer-Unternehmen.pdf",
      "decision": "Exclude",
      "exclusion_reason": "Wrong publication type",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This practical guide argues that feminist AI competency results in conscious prompt design to actively reduce stereotypes and discrimination in AI-generated content. It presents the \"KI-FAIRNESS\" framework for diversity-reflective prompting and demonstrates how specific, context-rich prompts lead to fairer and more representative results by compensating for AI's \"blind spots\" through targeted instructions.",
      "summary_section": "## Overview\n\n\"Faires KI-Prompting\" is a practitioner-oriented guidance document explicitly designed for business leaders and employees in small and medium-sized enterprises (SMEs) implementing generative artificial intelligence responsibly. Published by the Mittelstand-Digital Zentrum Zukunftskultur",
      "source_tool": "Manual",
      "zotero_key": "UWGSYB27"
    },
    {
      "id": "Gengler_2024_Faires_KIPrompting_–_Ein_Leitfaden_für_Unternehmen",
      "filename": "Gengler_2024_Faires_KIPrompting_–_Ein_Leitfaden_für_Unternehmen.md",
      "title": "Faires KIPrompting – Ein Leitfaden für Unternehmen. BSP Business and Law School – Hochschule für Management und Recht",
      "author_year": "Gengler (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.businessschool-berlin.de/files/Business-School-Berlin/Publikation/Faires-KI-Prompting-Ein-Leitfaden-fuer-Unternehmen%202024.pdf",
      "decision": "Exclude",
      "exclusion_reason": "Wrong publication type",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Der vorliegende Leitfaden möchte Sie auf eine Reise durch die Welt der Generativen KI mitnehmen und Ihnen Werkzeuge an die Hand geben, um diese Technolo- gien verantwortungsvoll und bewusst zu nutzen. Wir möchten Verständnis für die positive wie negative Wirkung von Generativer KI schaffen, zugleich aber auch den Weg für einen diversen und fairen Einsatz ebnen. Dieser Guide kann Ihr Kompass sein, um nicht nur zu navigieren, sondern die digitale Zukunft mitzugestalten",
      "summary_section": "## Overview\n\n\"Faires KI-Prompting\" is a practitioner-oriented guidance document explicitly designed for business leaders and employees in small and medium-sized enterprises (SMEs) implementing generative artificial intelligence responsibly. Published by the Mittelstand-Digital Zentrum Zukunftskultur",
      "source_tool": "Manual",
      "zotero_key": "Q7SK8TJB"
    },
    {
      "id": "Ghosal_2024_An_empirical_study_of_structural_social_and_ethica",
      "filename": "Ghosal_2024_An_empirical_study_of_structural_social_and_ethica.md",
      "title": "An empirical study of structural social and ethical challenges in AI",
      "author_year": "Ghosal (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://link.springer.com/article/10.1007/s00146-025-02207-y",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 2,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-medium",
        "dim-prof-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Ghosal_2024_empirical.md",
      "abstract": "This empirical study examines how professionals (N=32) in AI development perceive structural ethical challenges such as injustices and inequalities. The research identifies three main themes: (1) barriers to responsibility in a changing ecosystem, (2) the need for holistic consideration of AI products and their harms, and (3) structural obstacles that prevent engineers from taking personal responsibility.",
      "summary_section": "![[summary_Ghosal_2024_empirical.md]]",
      "source_tool": "Manual",
      "zotero_key": "CV9PXYWB"
    },
    {
      "id": "Ghosal_2025_Unequal_voices__How_LLMs_construct_constrained_que",
      "filename": "Ghosal_2025_Unequal_voices__How_LLMs_construct_constrained_que.md",
      "title": "Unequal voices: How LLMs construct constrained queer narratives",
      "author_year": "Ghosal (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/abs/2507.15585",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Ghosal_2025_Unequal.md",
      "abstract": "Investigates how large language models represent queer individuals in generated narratives, uncovering tendencies toward stereotyped and narrow portrayals. Identifies phenomena including narrow topic range, discursive othering, and identity foregrounding. Shows LLMs unconsciously reinforce divide where marginalized groups are not afforded same breadth of narrative roles as others.",
      "summary_section": "## Overview\n\nThis paper investigates how Large Language Models systematically construct narrower, more identity-constrained narratives about LGBTQ+ individuals compared to non-queer personas in neutral contexts. The research addresses a critical gap in AI ethics by moving beyond traditional toxicity",
      "source_tool": "Manual",
      "zotero_key": "RSMUFA95"
    },
    {
      "id": "Giannoni_Adielsson_2024_The_AI_Act,_gender_equality_and_non-discrimination",
      "filename": "Giannoni_Adielsson_2024_The_AI_Act,_gender_equality_and_non-discrimination.md",
      "title": "The AI Act, gender equality and non-discrimination: what role for the AI Office?",
      "author_year": "Giannoni Adielsson (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1007/s12027-024-00785-w",
      "url": "https://doi.org/10.1007/s12027-024-00785-w",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Diese Analyse bewertet, ob der EU AI Act Fragen der Geschlechtergerechtigkeit und Nichtdiskriminierung ausreichend adressiert. Die substantiellen Bestimmungen des AI Acts werden durch die Linse von Gleichstellungs- und Antidiskriminierungsrecht analysiert, wobei vorgeschlagene Tools wie grundrechtliche Folgenabschätzungen und Bias-Audits zur Reduzierung von Geschlechterverzerrungen und Diskriminierungsrisiken hervorgehoben werden. Die Rolle des AI Office und seine Kooperation mit nationalen, eur",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "ZSBFXEKJ"
    },
    {
      "id": "Goellner_2025_Towards_responsible_AI_for_education__Hybrid_human",
      "filename": "Goellner_2025_Towards_responsible_AI_for_education__Hybrid_human.md",
      "title": "Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room",
      "author_year": "Goellner (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/pdf/2504.16148",
      "decision": "Unclear",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 1,
      "rel_bias": 1,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 7,
      "relevance_category": "medium",
      "top_dimensions": [
        "AI Literacy",
        "Practical Implementation"
      ],
      "tags": [
        "paper",
        "unclear",
        "medium-relevance",
        "dim-ai-komp-medium",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Identifies nine persistent challenges undermining responsible use of AI in education, including neglect of key learning processes, lack of stakeholder involvement, and use of unreliable XAI methods. Proposes hybrid human-AI methods, specifically neural-symbolic AI (NSAI), which integrates expert domain knowledge with data-driven approaches. This hybrid architecture allows for built-in transparency, stakeholder engagement, and modeling of complex pedagogically-grounded principles.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "7L8SGYKI"
    },
    {
      "id": "Gohar_2023_A_Survey_on_Intersectional_Fairness_in_Machine_Lea",
      "filename": "Gohar_2023_A_Survey_on_Intersectional_Fairness_in_Machine_Lea.md",
      "title": "A Survey on Intersectional Fairness in Machine Learning: Opportunities and Challenges",
      "author_year": "Gohar (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.24963/ijcai.2023/742",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Gohar_2023_Survey.md",
      "abstract": "Comprehensive survey examining intersectional fairness in machine learning systems beyond traditional binary fairness approaches. Presents taxonomy of intersectional fairness concepts showing how multiple sensitive attributes interact to create unique discrimination forms. Identifies challenges including data sparsity for intersectional groups and bias mitigation complexity. Demonstrates applications in NLP systems, ranking algorithms, and image recognition. Shows traditional fairness metrics fa",
      "summary_section": "![[summary_Gohar_2023_Survey.md]]",
      "source_tool": "Manual",
      "zotero_key": "8IR8NQTC"
    },
    {
      "id": "Goldkind_2023_The_End_of_the_World_as_We_Know_It__ChatGPT_and_So",
      "filename": "Goldkind_2023_The_End_of_the_World_as_We_Know_It__ChatGPT_and_So.md",
      "title": "The End of the World as We Know It? ChatGPT and Social Work",
      "author_year": "Goldkind (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1093/sw/swad044",
      "url": "https://doi.org/10.1093/sw/swad044",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 1,
      "rel_bias": 1,
      "rel_praxis": 1,
      "rel_prof": 3,
      "total_relevance": 7,
      "relevance_category": "medium",
      "top_dimensions": [
        "Professional Context",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Brief commentary marking ChatGPT as a pivotal moment for social work. Encourages proactive engagement to steer AI toward just, human-centered outcomes and warns that non-engagement risks value misalignment and inequity.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "GBF7IZM4"
    },
    {
      "id": "Goldkind_2024_Artificial_intelligence_in_social_work__An_EPIC_mo",
      "filename": "Goldkind_2024_Artificial_intelligence_in_social_work__An_EPIC_mo.md",
      "title": "Artificial intelligence in social work: An EPIC model for practice",
      "author_year": "Goldkind (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1080/0312407X.2025.2488345",
      "url": "https://doi.org/10.1080/0312407X.2025.2488345",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Professional Context",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Develops EPIC model (Ethics and justice, Policy development and advocacy, Intersectoral collaboration, Community engagement and empowerment) as structured approach for AI integration in social work. Emphasizes that AI integration must align with social work mission and values through four overlapping components. Ethics and justice aspect addresses bias mitigation and transparent use, while policy development prioritizes client protection. Warns against excessive calibration of AI systems toward ",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "322LCA6H"
    },
    {
      "id": "Goldkind_2024_The_end_of_the_world_as_we_know_it__ChatGPT_and_so",
      "filename": "Goldkind_2024_The_end_of_the_world_as_we_know_it__ChatGPT_and_so.md",
      "title": "The end of the world as we know it? ChatGPT and social work",
      "author_year": "Goldkind (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1093/sw/swad044",
      "url": "nan",
      "decision": "Exclude",
      "exclusion_reason": "Wrong publication type",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Editorial in flagship journal Social Work providing critical reflection on ChatGPT's introduction and implications for social work practice. Addresses how ChatGPT, built on natural language processing, responds to prompts and generates text responses. Notes social work's historical reluctance to embrace new technologies and positions ChatGPT as opportunity to reflect on strategies promoting just technology use. Urges social workers to join cross-disciplinary conversations about AI evolution and ",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "7QB9DC7Z"
    },
    {
      "id": "Gravelmann_2024_Künstliche_Intelligenz_in_der_Sozialen_Arbeit_–_Zw",
      "filename": "Gravelmann_2024_Künstliche_Intelligenz_in_der_Sozialen_Arbeit_–_Zw.md",
      "title": "Künstliche Intelligenz in der Sozialen Arbeit – Zwischen Bedenken und Optionen",
      "author_year": "Gravelmann (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://content-select.com/de/portal/media/view/66472512-3a7c-4fef-b8e6-a5d3ac1b0002",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 3,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Bias Analysis",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Gravelmann analyzes the impact of AI on the social work profession, identifying both opportunities and risks. Potential benefits include AI as communication aid, documentation tool, and data analysis instrument. Critical concerns include the danger of decision delegation to AI systems, potentially reducing professionals to executors. The author warns against automated AI-based procedures that massively intervene in people's lives, especially in child protection. Ethical problems are identified i",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "J8S8EDJ6"
    },
    {
      "id": "Guerra_2023_Feminist_reflections_for_the_development_of_artifi",
      "filename": "Guerra_2023_Feminist_reflections_for_the_development_of_artifi.md",
      "title": "Feminist reflections for the development of artificial intelligence",
      "author_year": "Guerra (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.derechosdigitales.org/fair-2023-en/",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-medium",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Comprehensive synthesis of Latin American women's conversations developing AI under feminist frameworks establishes methodological commitments for co-design with communities, gender perspective integration in data science projects, and strategies for women crowd workers. Key feminist AI principles include building diverse intersectional teams, establishing community collaboration agreements, choosing technology based on context rather than consumption, and protecting autonomy through strong anon",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "U7QYR2AA"
    },
    {
      "id": "Hall_2024_A_systematic_review_of_sophisticated_predictive_an",
      "filename": "Hall_2024_A_systematic_review_of_sophisticated_predictive_an.md",
      "title": "A systematic review of sophisticated predictive and prescriptive analytics in child welfare: Accuracy, equity, and bias",
      "author_year": "Hall (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1007/s10560-023-00931-2",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 12,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium",
        "dim-prof-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Hall_2024_systematic.md",
      "abstract": "Systematic review examining how researchers address ethics, equity, bias, and model performance in predictive and prescriptive algorithms used in child welfare settings. Analyzing 67 articles published 2010-2020, reveals inconsistent approaches to measuring and mitigating algorithmic fairness in child welfare applications. Identifies that most predictive models use administrative data reflecting surveillance biases rather than true child maltreatment incidence, leading to discrimination against ",
      "summary_section": "![[summary_Hall_2024_systematic.md]]",
      "source_tool": "Manual",
      "zotero_key": "36MC5GZ8"
    },
    {
      "id": "Hartshorne_2025_Generative_AI_and_the_Future_of_Digital_Literacy__",
      "filename": "Hartshorne_2025_Generative_AI_and_the_Future_of_Digital_Literacy__.md",
      "title": "Generative AI and the Future of Digital Literacy: Opportunities for Gender Inclusion",
      "author_year": "Hartshorne (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 3,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-medium",
        "dim-vulnerable-high",
        "dim-bias-medium",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Examines generative AI potential for promoting female inclusion in primary and secondary education while addressing inherent risks of reinforcing gender-specific biases. Qualitative research analyzes systemic effects of generative AI tools on teaching and learning through interviews and focus groups with students and teachers. Results show complex interactions between generative AI technologies and gender dynamics. Key factors for effective AI-supported learning environments include personalized",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "GFNQY7GF"
    },
    {
      "id": "Hauck_2025_A_framework_for_the_learning_and_teaching_of_Criti",
      "filename": "Hauck_2025_A_framework_for_the_learning_and_teaching_of_Criti.md",
      "title": "A framework for the learning and teaching of Critical AI Literacy skills (Version 0.1)",
      "author_year": "Hauck (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.open.ac.uk/blogs/learning-design/wp-content/uploads/2025/01/OU-Critical-AI-Literacy-framework-2025-external-sharing.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 3,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "AI Literacy",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-high",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This framework defines Critical AI Literacy as expanding beyond traditional AI literacy to examine how Large Language Models contribute to ongoing epistemic injustices that can lead to significant social and personal harm. It applies equality, diversity, inclusion, and accessibility principles to AI use, emphasizing the importance of critically evaluating AI-generated outputs and engaging in equitable and inclusive prompting practices. Critical AI Literacy is conceptualized as context-specific a",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "B4S75S5S"
    },
    {
      "id": "Hayati_2024_How_Far_Can_We_Extract_Diverse_Perspectives_from_L",
      "filename": "Hayati_2024_How_Far_Can_We_Extract_Diverse_Perspectives_from_L.md",
      "title": "How Far Can We Extract Diverse Perspectives from Large Language Models?",
      "author_year": "Hayati (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "https://aclanthology.org/2024.emnlp-main.306.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Hayati_2024_Extract.md",
      "abstract": "Systematically evaluates prompting strategies to extract diverse perspectives from LLMs and mitigate dominant group bias in outputs. Measuring subjective tasks such as argumentation and hate speech labeling, the study finds that diversity prompting increases perspective variety and reduces monocultural output tendencies.",
      "summary_section": "![[summary_Hayati_2024_Extract.md]]",
      "source_tool": "Manual",
      "zotero_key": "ZHLPQEII"
    },
    {
      "id": "Heinz_2025_Clinical_trial_of_an_LLM-based_conversational_AI_p",
      "filename": "Heinz_2025_Clinical_trial_of_an_LLM-based_conversational_AI_p.md",
      "title": "Clinical trial of an LLM-based conversational AI psychotherapy",
      "author_year": "Heinz (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400802",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 1,
      "rel_praxis": 3,
      "rel_prof": 2,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Practical Implementation",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-praxis-high",
        "dim-prof-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Groundbreaking study representing first randomized controlled trial of generative AI-powered therapy chatbot, Therabot. Trial included 106 participants across United States diagnosed with major depressive disorder, generalized anxiety disorder, or eating disorders. Participants interacted with Therabot via smartphone app over 4-8 weeks. Results showed clinically significant symptom improvements: 51% reduction in depression symptoms, 31% reduction in anxiety symptoms, and 19% reduction in eating ",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "7RQUVQ5I"
    },
    {
      "id": "He_2024_On_the_steerability_of_large_language_models",
      "filename": "He_2024_On_the_steerability_of_large_language_models.md",
      "title": "On the steerability of large language models",
      "author_year": "He (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "https://aclanthology.org/2025.naacl-long.400.pdf",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "WUC94FIH"
    },
    {
      "id": "Himmelreich_2022_Artificial_Intelligence_and_Structural_Injustice__",
      "filename": "Himmelreich_2022_Artificial_Intelligence_and_Structural_Injustice__.md",
      "title": "Artificial Intelligence and Structural Injustice: Foundations for Equity, Values, and Responsibility",
      "author_year": "Himmelreich (2022)",
      "authors": [],
      "publication_year": 2022.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/abs/2205.02389",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Himmelreich_2022_Artificial.md",
      "abstract": "This work develops a structural injustice approach to AI governance based on Iris Marion Young's theory of structural injustice. The authors argue that structural injustice is a powerful conceptual tool that enables researchers and practitioners to identify, articulate, and potentially even anticipate AI bias. The approach includes both an analytical component (structural explanations) and an evaluative component (justice theory) and provides methodological and normative foundations for diversit",
      "summary_section": "## Overview\n\nThis academic work by Himmelreich and Lim presents a philosophical framework for understanding and addressing AI bias through structural injustice theory, derived from philosopher Iris Marion Young's work. The authors argue that AI systems perpetuate pre-existing unjust social structure",
      "source_tool": "Manual",
      "zotero_key": "RCD7NJM2"
    },
    {
      "id": "Hodgson_2022_Problematising_artificial_intelligence_in_social_w",
      "filename": "Hodgson_2022_Problematising_artificial_intelligence_in_social_w.md",
      "title": "Problematising artificial intelligence in social work education: Challenges, issues and possibilities",
      "author_year": "Hodgson (2022)",
      "authors": [],
      "publication_year": 2022.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1093/bjsw/bcab168",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 1,
      "rel_prof": 3,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Professional Context",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-medium",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Critical examination of AI's fourth industrial revolution implications for social work education, questioning what skills and knowledge should be taught to prepare students for digital working lives. Adopts problematizing approach challenging both celebratory and catastrophic narratives about AI. Argues social work education must address fundamental tensions between training for technologically-driven environments and engaging students in critical theories acknowledging unequal power distributio",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "M7UIILSI"
    },
    {
      "id": "James_2023_Algorithmic_decision-making_in_social_work_practic",
      "filename": "James_2023_Algorithmic_decision-making_in_social_work_practic.md",
      "title": "Algorithmic decision-making in social work practice and pedagogy: confronting the competency/critique dilemma",
      "author_year": "James (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1080/02615479.2023.2195425",
      "url": "https://www.tandfonline.com/doi/full/10.1080/02615479.2023.2195425",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 11,
      "relevance_category": "high",
      "top_dimensions": [
        "Professional Context",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-medium",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "The world is experiencing an accelerating digital transformation. One aspect of this is the implementation of...[source](https://www.google.com/search?q=https://www.aminer.cn/pub/645d0410d68f896efa94d024/algorithmic-decision-making-in-social-work-practice-and-pedagogy-confronting-the-competency)",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "XPY6WUH4"
    },
    {
      "id": "James_2025_Responsible_prompting_recommendation__Fostering_re",
      "filename": "James_2025_Responsible_prompting_recommendation__Fostering_re.md",
      "title": "Responsible prompting recommendation: Fostering responsible AI practices in prompting-time",
      "author_year": "James (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.1145/3706598.3713365",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 1,
      "rel_bias": 2,
      "rel_praxis": 3,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Practical Implementation",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-ai-komp-medium",
        "dim-bias-medium",
        "dim-praxis-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Presents insights from interviews and user studies with IT professionals exploring prompting practices and develops open-source responsible prompting recommender system. Research reveals responsible prompt recommendations can support novice prompt engineers and raise awareness about Responsible AI in prompting-time, helping people reflect on responsible practices before LLM content generation. Demonstrates that finding right balance between adding social values to prompts and removing potentiall",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "5ED8INP4"
    },
    {
      "id": "Jarke_2024_Who_cares_about_data__Data_care_arrangements_in_ev",
      "filename": "Jarke_2024_Who_cares_about_data__Data_care_arrangements_in_ev.md",
      "title": "Who cares about data? Data care arrangements in everyday organisational practice",
      "author_year": "Jarke (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1080/1369118X.2024.2320917",
      "url": "nan",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Introduces data care arrangements concept to understand mundane data work in organizations. Demonstrates through empirical research in educational and social service organizations how data care work is distributed across organizational members with different, often conflicting care obligations. Reveals data quality maintenance involves complex sociomaterial configurations of people, infrastructures, routines, and practices. Shows data care work is frequently backgrounded and assumed effortless d",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "PQ78ECFH"
    },
    {
      "id": "Jarke_2025_Datafied_ageing_futures__Regimes_of_anticipation_a",
      "filename": "Jarke_2025_Datafied_ageing_futures__Regimes_of_anticipation_a.md",
      "title": "Datafied ageing futures: Regimes of anticipation and participatory futuring",
      "author_year": "Jarke (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1177/20539517241306363",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 1,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Practical Implementation"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Challenges regimes of anticipation suggesting datafied futures are inevitable, arguing futures are actively made through sociotechnical imaginaries promoted by powerful actors. Explores how to democratize futures-making regarding aging populations, critiquing how current anticipations around data-driven systems and ageist assumptions dominate discussions without adequate participation from affected populations. Develops participatory futuring methodology enabling diverse stakeholders, particular",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "ZKHP6UJ5"
    },
    {
      "id": "Jarrahi_2021_Algorithmic_management_in_a_work_context",
      "filename": "Jarrahi_2021_Algorithmic_management_in_a_work_context.md",
      "title": "Algorithmic management in a work context",
      "author_year": "Jarrahi (2021)",
      "authors": [],
      "publication_year": 2021.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1177/20539517211020332",
      "url": "nan",
      "decision": "Unclear",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 6,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "unclear",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Interdisciplinary analysis examining algorithmic management as sociotechnical phenomenon shaped by organizational choices and power structures rather than technological determinism. Authors critically analyze how algorithmic systems in standard work settings redefine pre-existing power dynamics between workers and managers, demanding new competencies while fostering oppositional attitudes. Key critical insights include risk of treating workers as programmable cogs through automation, commodifica",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "9XLDABF4"
    },
    {
      "id": "Jiang_2022_Assessing_GPT's_bias_towards_stigmatized_social_gr",
      "filename": "Jiang_2022_Assessing_GPT's_bias_towards_stigmatized_social_gr.md",
      "title": "Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia",
      "author_year": "Jiang (2022)",
      "authors": [],
      "publication_year": 2022.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/pdf/2505.17045",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "TNTENI9Y"
    },
    {
      "id": "Jin_2025_GLAT__The_generative_AI_literacy_assessment_test",
      "filename": "Jin_2025_GLAT__The_generative_AI_literacy_assessment_test.md",
      "title": "GLAT: The generative AI literacy assessment test",
      "author_year": "Jin (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1016/j.caeai.2025.100436",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2666920X25000761",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "B4KQS7M5"
    },
    {
      "id": "Jääskeläinen_2025_Intersectional_analysis_of_visual_generative_AI__T",
      "filename": "Jääskeläinen_2025_Intersectional_analysis_of_visual_generative_AI__T.md",
      "title": "Intersectional analysis of visual generative AI: The case of Stable Diffusion",
      "author_year": "Jääskeläinen (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1007/s00146-025-02207-y",
      "url": "https://link.springer.com/article/10.1007/s00146-025-02207-y",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high"
      ],
      "has_summary": true,
      "summary_file": "summary_Jääskeläinen_2025_Intersectional.md",
      "abstract": "This open-access paper provides a feminist intersectional critique of Stable Diffusion through qualitative visual analysis of 180 AI-generated images. Authors examined how power systems like racism, sexism, heteronormativity, ableism, colonialism, and capitalism are reflected in AI outputs. Found that default outputs frequently perpetuate harmful stereotypes and assume a \"white, able-bodied, masculine-presenting\" default subject position. Advocates for social justice-oriented approach to AI by a",
      "summary_section": "## Overview\n\nThis paper by Jääskeläinen, Sharma, Pallett, and Åsberg presents a critical examination of Stable Diffusion (SD), a widely-adopted open-source visual generative AI tool launched in August 2022 that has generated over 12 billion images with 10 million daily active users. The research fun",
      "source_tool": "Manual",
      "zotero_key": "9G6AQ3AA"
    },
    {
      "id": "Jørgensen_2023_Data_and_rights_in_the_digital_welfare_state__the_",
      "filename": "Jørgensen_2023_Data_and_rights_in_the_digital_welfare_state__the_.md",
      "title": "Data and rights in the digital welfare state: the case of Denmark",
      "author_year": "Jørgensen (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1080/1369118X.2021.1934069",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 2,
      "rel_praxis": 1,
      "rel_prof": 2,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-medium",
        "dim-prof-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Critical analysis examining how surveillance capitalism logic manifests in public sector through Denmark's automated welfare decision-making systems. Argues that unless more human-centric approaches are adopted, digital welfare states advance digital technocracy treating citizens as data points for calculation and prediction rather than individuals with agency and rights. Employs theories of surveillance capitalism, digital-era governance, and data politics to analyze automated decision support ",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "7V9GKPNZ"
    },
    {
      "id": "Kamruzzaman_2024_Prompting_techniques_for_reducing_social_bias_in_L",
      "filename": "Kamruzzaman_2024_Prompting_techniques_for_reducing_social_bias_in_L.md",
      "title": "Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes",
      "author_year": "Kamruzzaman (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2404.17218v1",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 3,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Bias Analysis",
        "Practical Implementation"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Kamruzzaman_2024_Prompting.md",
      "abstract": "This study evaluates 12 prompt strategies across five LLMs, finding that instructing a model to adopt a System 2 (deliberative) reasoning style and a \"human persona\" most effectively reduces stereotypes. Combining these two strategies yielded up to a 13% reduction in stereotypical responses. Contrary to prior assumptions, Chain-of-Thought (CoT) prompting alone was not as effective, showing bias levels similar to a default prompt. The results suggest that prompts encouraging careful, human-like r",
      "summary_section": "## Overview\n\nThis research paper addresses social bias mitigation in Large Language Models through psychologically-informed prompting techniques grounded in dual process theory. Authored by Kamruzzaman and Kim (University of South Florida), and accepted at RANLP-2025, the work applies Kahneman's cog",
      "source_tool": "Manual",
      "zotero_key": "FMXXR4DH"
    },
    {
      "id": "Kaneko_2024_Debiasing_prompts_for_gender_bias_in_large_languag",
      "filename": "Kaneko_2024_Debiasing_prompts_for_gender_bias_in_large_languag.md",
      "title": "Debiasing prompts for gender bias in large language models",
      "author_year": "Kaneko (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2404.17218v3",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "YJAHIZMR"
    },
    {
      "id": "Kaneko_2024_Evaluating_gender_bias_in_large_language_models_vi",
      "filename": "Kaneko_2024_Evaluating_gender_bias_in_large_language_models_vi.md",
      "title": "Evaluating gender bias in large language models via chain-of-thought prompting",
      "author_year": "Kaneko (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/abs/2401.15585",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This study investigates if Chain-of-Thought (CoT) prompting reduces implicit gender bias in LLMs. Using a synthetic task of counting gendered words, the authors found that without step-by-step prompting, models made biased errors. CoT prompting, which forced the model to explicitly label each word's gender before counting, significantly reduced these mistakes. This suggests that guiding the model through an explicit reasoning process makes it rely more on logic than on stereotypes, thereby mitig",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "CCCM5RLR"
    },
    {
      "id": "Karagianni_2025_Gender_in_a_stereo-(gender)typical_EU_AI_law__A_fe",
      "filename": "Karagianni_2025_Gender_in_a_stereo-(gender)typical_EU_AI_law__A_fe.md",
      "title": "Gender in a stereo-(gender)typical EU AI law: A feminist reading of the AI Act",
      "author_year": "Karagianni (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1017/cfl.2025.12",
      "url": "https://www.cambridge.org/core/journals/cambridge-forum-on-ai-law-and-governance/article/gender-in-astereogendertypical-eu-ai-law-a-feminist-reading-of-the-ai-act/E9DEFC1E114CBE577D737EC616610921",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Karagianni_2025_Gender.md",
      "abstract": "This peer-reviewed article offers a feminist critique of the European Union’s AI Act, arguing that the law’s current bias mitigation provisions only superficially address gendered risks and fail to confront deeply embedded structural biases. Using frameworks like hermeneutical injustice and feminist legal theory, Karagianni shows that AI systems can perpetuate historical power imbalances and systemic discrimination against women and marginalized groups. The AI Act’s pre-market and post-market me",
      "summary_section": "## Overview\n\nAnastasia Karagianni's research article, published in 2025, provides a critical feminist examination of the European Union's AI Act (Regulation 2024/1689), questioning whether this landmark regulatory framework adequately protects marginalized communities from gender-based discriminatio",
      "source_tool": "Manual",
      "zotero_key": "F8GURS3C"
    },
    {
      "id": "Kattnig_2024_Assessing_trustworthy_AI__Technical_and_legal_pers",
      "filename": "Kattnig_2024_Assessing_trustworthy_AI__Technical_and_legal_pers.md",
      "title": "Assessing trustworthy AI: Technical and legal perspectives of fairness in AI",
      "author_year": "Kattnig (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1016/j.clsr.2024.106053",
      "url": "https://graz.elsevierpure.com/ws/portalfiles/portal/89954869/1-s2.0-S0267364924001195-main.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Kattnig_2024_Assessing.md",
      "abstract": "Kattnig et al. examine the disconnect between existing AI fairness techniques and the requirements of non-discrimination law, highlighting that many algorithmic bias mitigation methods do not meet legal standards for equality. Focusing on the EU context (with particular attention to the forthcoming AI Act and established non-discrimination directives), the authors review state-of-the-art bias mitigation approaches – from pre-processing data fixes to in-processing algorithms – and evaluate them a",
      "summary_section": "## Overview\n\nThis academic paper addresses a critical contemporary challenge in artificial intelligence: ensuring fairness and non-discrimination in AI systems through integrated technical and legal analysis. Published in *Computer Law & Security Review*, the work by Kattnig et al. from Graz Univers",
      "source_tool": "Manual",
      "zotero_key": "9A73EE79"
    },
    {
      "id": "Kawakami_2022_Improving_human-AI_partnerships_in_child_welfare__",
      "filename": "Kawakami_2022_Improving_human-AI_partnerships_in_child_welfare__.md",
      "title": "Improving human-AI partnerships in child welfare: Understanding worker practices, challenges, and desires for algorithmic decision support",
      "author_year": "Kawakami (2022)",
      "authors": [],
      "publication_year": 2022.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.1145/3491102.3517439",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 12,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Empirical study examining how child maltreatment hotline workers interact with Allegheny Family Screening Tool, an AI-based decision support system. Through interviews and contextual inquiries, found workers' reliance on algorithmic predictions guided by four key factors: knowledge of contextual information beyond AI model capabilities, beliefs about system limitations, organizational pressures around tool use, and awareness of misalignments between algorithmic predictions and their own decision",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "958TWYXZ"
    },
    {
      "id": "Keddell_2019_Algorithmic_justice_in_child_protection__Statistic",
      "filename": "Keddell_2019_Algorithmic_justice_in_child_protection__Statistic.md",
      "title": "Algorithmic justice in child protection: Statistical fairness, social justice and the implications for practice",
      "author_year": "Keddell (2019)",
      "authors": [],
      "publication_year": 2019.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.3390/socsci8100281",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 2,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-prof-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Critical analysis examining fairness and justice implications of predictive algorithms in child protection from both statistical and social justice perspectives. Identifies fundamental problems with using child protection system data: biased sample frames reflecting reporting patterns rather than actual abuse incidence, feedback loops amplifying discrimination, and spurious correlations conflating system surveillance with genuine risk. Demonstrates racial and socioeconomic disparities in child w",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "G5TXUQLW"
    },
    {
      "id": "Klein_2024_Data_Feminism_for_AI",
      "filename": "Klein_2024_Data_Feminism_for_AI.md",
      "title": "Data feminism for AI",
      "author_year": "Klein (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.1145/3630106.3658543",
      "url": "https://doi.org/10.1145/3630106.3658543",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-medium",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Klein_2024_Data.md",
      "abstract": "Extends the influential \"Data Feminism\" framework to AI research, presenting intersectional feminist principles for conducting equitable, ethical, and sustainable AI research. Rearticulates original seven data feminism principles specifically for AI contexts and introduces two new principles addressing environmental impact and consent, providing concrete methodological guidance.",
      "summary_section": "## Overview\n\nKlein and D'Ignazio's \"Data Feminism for AI\" extends their influential 2020 framework to address contemporary challenges in artificial intelligence development and deployment. Published at the 2024 ACM Conference on Fairness, Accountability, and Transparency, this paper argues that femi",
      "source_tool": "Manual",
      "zotero_key": "IXCQLI27"
    },
    {
      "id": "Klinge_2024_A_sociolinguistic_approach_to_stereotype_assessmen",
      "filename": "Klinge_2024_A_sociolinguistic_approach_to_stereotype_assessmen.md",
      "title": "A sociolinguistic approach to stereotype assessment in large language models",
      "author_year": "Klinge (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "https://facctconference.org/static/docs/facct2025-206archivalpdfs/facct2025-final1618-acmpaginated.pdf",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "JTXKA8KJ"
    },
    {
      "id": "Knowles_2023_Trustworthy_AI_and_the_Logics_of_Intersectional_Re",
      "filename": "Knowles_2023_Trustworthy_AI_and_the_Logics_of_Intersectional_Re.md",
      "title": "Trustworthy AI and the Logics of Intersectional Resistance",
      "author_year": "Knowles (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.1145/3593013.3593986",
      "url": "https://doi.org/10.1145/3593013.3593986",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 2,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Critically examines mainstream \"Trustworthy AI\" frameworks from an intersectional feminist perspective, arguing that traditional AI ethics often privilege dominant groups and fail marginalized communities. Suggests reframing trustworthy AI principles to incorporate stewardship, care, humility, and empowerment, addressing intersectional injustices through power-sharing and structural change.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "U7DW2TX8"
    },
    {
      "id": "Kojima_2022_Large_language_models_are_zero-shot_reasoners",
      "filename": "Kojima_2022_Large_language_models_are_zero-shot_reasoners.md",
      "title": "Large language models are zero-shot reasoners",
      "author_year": "Kojima (2022)",
      "authors": [],
      "publication_year": 2022.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "nan",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "VL782GFG"
    },
    {
      "id": "Kong_2021_Evaluation_of_an_artificial_intelligence_literacy_",
      "filename": "Kong_2021_Evaluation_of_an_artificial_intelligence_literacy_.md",
      "title": "Evaluation of an artificial intelligence literacy course for university students with diverse study backgrounds",
      "author_year": "Kong (2021)",
      "authors": [],
      "publication_year": 2021.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1016/j.caeai.2021.100026",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000205",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "8H4QJL4B"
    },
    {
      "id": "Kong_2024_Developing_an_artificial_intelligence_literacy_fra",
      "filename": "Kong_2024_Developing_an_artificial_intelligence_literacy_fra.md",
      "title": "Developing an artificial intelligence literacy framework: Evaluation of a literacy course for senior secondary students using a project-based learning approach",
      "author_year": "Kong (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1016/j.caeai.2024.100214",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2666920X24000158",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "H3QCKDMQ"
    },
    {
      "id": "Kong_2025_Artificial_Intelligence_(AI)_literacy_–_an_argumen",
      "filename": "Kong_2025_Artificial_Intelligence_(AI)_literacy_–_an_argumen.md",
      "title": "Artificial Intelligence (AI) literacy – an argument for AI literacy in education",
      "author_year": "Kong (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1080/14703297.2024.2332744",
      "url": "https://www.tandfonline.com/doi/full/10.1080/14703297.2024.2332744",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "W7UBW2XJ"
    },
    {
      "id": "Kubes_2024_Feministische_KI_–_Künstliche_Intelligenz_für_alle",
      "filename": "Kubes_2024_Feministische_KI_–_Künstliche_Intelligenz_für_alle.md",
      "title": "Feministische KI – Künstliche Intelligenz für alle? [Feminist AI – Artificial intelligence for everyone?]",
      "author_year": "Kubes (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "webpage",
      "language": "nan",
      "doi": "nan",
      "url": "https://blogs.fu-berlin.de/toolbox/2024/04/08/feministische-ki-kuenstliche-intelligenz-fuer-alle/",
      "decision": "Unclear",
      "exclusion_reason": "nan",
      "rel_ai_komp": 3,
      "rel_vulnerable": 3,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 11,
      "relevance_category": "high",
      "top_dimensions": [
        "AI Literacy",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "unclear",
        "high-relevance",
        "dim-ai-komp-high",
        "dim-vulnerable-high",
        "dim-bias-medium",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Innovative interdisciplinary seminar teaches students to critically analyze everyday AI applications from sociotechnical feminist perspectives across four domains: love, robots, work, and creativity. Students analyze AI within androcentric, Eurocentric, anthropocentric, and capitalist-patriarchal structures. Curriculum combines theoretical foundations with practical application through \"queerbot\" design workshops that reimagine AI beyond normative dichotomies, demonstrating concrete pedagogical ",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "5ZQRB5PS"
    },
    {
      "id": "Kumar_2024_How_AI_hype_impacts_the_LGBTQ+_community",
      "filename": "Kumar_2024_How_AI_hype_impacts_the_LGBTQ+_community.md",
      "title": "How AI hype impacts the LGBTQ+ community",
      "author_year": "Kumar (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1007/s43681-024-00423-8",
      "url": "https://doi.org/10.1007/s43681-024-00423-8",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Die Studie analysiert, wie der Hype um KI heteronormative Annahmen verstärkt. Sie führt Fallstudien zur Gesichtserkennung, Content-Moderation und Geschlechtsklassifikation durch und zeigt auf, wie queere Identitäten algorithmisch marginalisiert werden.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "56VU98F2"
    },
    {
      "id": "Kutscher_2020_Handbuch_Soziale_Arbeit_und_Digitalisierung",
      "filename": "Kutscher_2020_Handbuch_Soziale_Arbeit_und_Digitalisierung.md",
      "title": "Handbuch Soziale Arbeit und Digitalisierung",
      "author_year": "Kutscher (2020)",
      "authors": [],
      "publication_year": 2020.0,
      "item_type": "book",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.beltz.de/fileadmin/beltz/leseproben/978-3-7799-5258-9.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 1,
      "rel_praxis": 1,
      "rel_prof": 3,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Professional Context",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-prof-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Kutscher_2020_Handbuch.md",
      "abstract": "Dieses umfassende Handbuch mit über 50 Beiträgen behandelt erstmals systematisch Digitalisierung in Bezug auf Disziplin und Praxis der Sozialen Arbeit. Das 658-seitige Werk beleuchtet aus verschiedenen disziplinären Perspektiven gesellschaftliche Entwicklungen, Diskurse, digitalisierte Formen der Dienstleistungserbringung, Profession, Organisation und Handlungsfelder sowie neue Herausforderungen für Forschung. Zentrale Themen umfassen Mediatisierung, Akteur-Netzwerk-Theorie, ethische Fragen, inf",
      "summary_section": "## Overview\n\nThis German handbook introduction addresses digitalization in social work as a critical scholarly concern requiring systematic theoretical and empirical analysis. The authors argue that while digitalization discussions have existed for decades, recent developments represent qualitative ",
      "source_tool": "Manual",
      "zotero_key": "C8VLMDBK"
    },
    {
      "id": "Kutscher_2023_Positionings,_challenges,_and_ambivalences_in_chil",
      "filename": "Kutscher_2023_Positionings,_challenges,_and_ambivalences_in_chil.md",
      "title": "Positionings, challenges, and ambivalences in children's and parents' perspectives in digitalized familial contexts",
      "author_year": "Kutscher (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "bookSection",
      "language": "nan",
      "doi": "nan",
      "url": "nan",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Examines family dynamics in digitalized contexts, analyzing tensions between children's digital participation rights and parental protection responsibilities. Presents research revealing ambivalences in both parental and children's perspectives: parents struggle between enabling children's digital competence and protecting them from risks; children experience tension between desire for autonomy and need for guidance. Addresses sharenting (parents sharing children's images/information online), ex",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "THB3SZPD"
    },
    {
      "id": "Kutscher_2024_Digitalität_und_Digitalisierung_als_Gegenstand_der",
      "filename": "Kutscher_2024_Digitalität_und_Digitalisierung_als_Gegenstand_der.md",
      "title": "Digitalität und Digitalisierung als Gegenstand der Sozialen Arbeit",
      "author_year": "Kutscher (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "bookSection",
      "language": "nan",
      "doi": "nan",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 1,
      "rel_prof": 3,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Professional Context",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Differentiates between digitalization (technical processes of making things digital) and digitality (sociotechnical transformations of social practices and relations), arguing social work must engage with both technological changes and their social implications. Demonstrates how algorithms, data-driven systems, and digital platforms reshape professional practice, client relationships, and social inequalities. Argues digitalization fundamentally transforms social contexts where social work operat",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "D66Q6JUR"
    },
    {
      "id": "Lahoti_2023_Improving_diversity_of_demographic_representation_",
      "filename": "Lahoti_2023_Improving_diversity_of_demographic_representation_.md",
      "title": "Improving diversity of demographic representation in people entities in Large Language Models",
      "author_year": "Lahoti (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "https://aclanthology.org/2023.emnlp-main.643/",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Introduces the Collective-Critique and Self-Voting (CCSV) prompting method to systematically enhance demographic diversity in LLM outputs. The approach leverages LLMs' internal capacity for diversity reasoning and combines critique and self-voting mechanisms to iteratively improve output balance while maintaining model performance.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "9VB7N2YA"
    },
    {
      "id": "Laine_2025_Avoiding_Catastrophe_Through_Intersectionality_in_",
      "filename": "Laine_2025_Avoiding_Catastrophe_Through_Intersectionality_in_.md",
      "title": "Avoiding Catastrophe Through Intersectionality in Global AI Governance",
      "author_year": "Laine (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.cigionline.org/publications/avoiding-catastrophe-through-intersectionality-in-global-ai-governance/",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 2,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Dieses Working Paper nutzt einen feministischen Policy-Analyse-Rahmen, der auf fünf thematischen Bereichen basiert: Intersektionalität, Kontext, Neutralität, Macht und Gerechtigkeit. Die Forschung schlägt einen feministischen KI-Policy-Rahmen vor, der Entscheidungsträger und Stakeholder ermutigt, potenzielle KI-Sicherheitsprojekte in Übereinstimmung mit vier Zielen zu bewerten: Förderung der Intersektionalität, Bereitstellung diverser Kontexte, Bekämpfung der Neutralität und transformative Gerec",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "G2U5EDD3"
    },
    {
      "id": "Lanzetta_2024_Artificial_Intelligence_Competence_Needs_for_Youth",
      "filename": "Lanzetta_2024_Artificial_Intelligence_Competence_Needs_for_Youth.md",
      "title": "Artificial Intelligence Competence Needs for Youth Workers",
      "author_year": "Lanzetta (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.5281/ZENODO.11525357",
      "url": "https://zenodo.org/doi/10.5281/zenodo.11525357",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 3,
      "rel_vulnerable": 2,
      "rel_bias": 1,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 11,
      "relevance_category": "high",
      "top_dimensions": [
        "AI Literacy",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-high",
        "dim-vulnerable-medium",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "The rapid developments in AI technology and the rise of accessible AI-powered tools are transforming the way we live, work and learn. While young people have already warmly embraced these solutions, with Gen Z being the most active users and experimenters of Generative AI (Microsoft, 2024), there is a sense of confusion and fear among youth workers about the future of how AI tools are going to be used in the youth sector, mixed with diverse emotions and viewpoints ranging from apprehension, scep",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "9Y3CDP9V"
    },
    {
      "id": "Latif_2023_AI_gender_bias,_disparities,_and_fairness__Does_tr",
      "filename": "Latif_2023_AI_gender_bias,_disparities,_and_fairness__Does_tr.md",
      "title": "AI gender bias, disparities, and fairness: Does training data matter?",
      "author_year": "Latif (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2312.10833v2",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 7,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Empirische Analyse von Geschlechterbias in Bewertungssystemen mit BERT und GPT-3.5. Mixed-gender Trainingsdaten reduzierten Bias, aber verstärkten Unterschiede. Drei Bias-Metriken angewendet.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "5KXKHIKC"
    },
    {
      "id": "Latif_2024_AI_Gender_Bias,_Disparities,_and_Fairness__Does_Tr",
      "filename": "Latif_2024_AI_Gender_Bias,_Disparities,_and_Fairness__Does_Tr.md",
      "title": "AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?",
      "author_year": "Latif (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2312.10833v4",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 7,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Empirical study examining gender bias in large language models through analysis of over 6000 evaluated student responses from 70 male and 70 female participants. Research uses fine-tuned BERT models and GPT-3.5 to evaluate various training data configurations: gender-specific versus mixed datasets. Three evaluation metrics applied: Scoring Accuracy Difference for bias assessment, Mean Score Gaps (MSG) for gender disparities, and Equalized Odds (EO) for fairness measurement. Results show mixed-ge",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "LAR4DQJF"
    },
    {
      "id": "Laupichler_2023_Development_of_the_“Scale_for_the_assessment_of_no",
      "filename": "Laupichler_2023_Development_of_the_“Scale_for_the_assessment_of_no.md",
      "title": "Development of the “Scale for the assessment of non-experts’ AI literacy” – An exploratory factor analysis",
      "author_year": "Laupichler (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1016/j.chbr.2023.100338",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2451958823000714",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "ZHG8756W"
    },
    {
      "id": "Lau_2023_Dipper__Diversity_in_Prompts_for_Producing_Large_L",
      "filename": "Lau_2023_Dipper__Diversity_in_Prompts_for_Producing_Large_L.md",
      "title": "Dipper: Diversity in Prompts for Producing Large Language Model Outputs",
      "author_year": "Lau (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.comp.nus.edu.sg/~greglau/assets/pdf/dipper_neurips_mint.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 2,
      "rel_praxis": 3,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Practical Implementation"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-medium",
        "dim-praxis-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Lau_2023_Dipper.md",
      "abstract": "Presents 'Dipper', an LLM prompting ensemble framework that systematically deploys a diverse set of prompts in parallel to improve the breadth of generated perspectives, including those of minority or marginalized groups. This training-free technique enhances demographic and perspective diversity without performance degradation.",
      "summary_section": "## Overview\n\n\"Dipper\" addresses the challenge of improving reasoning performance in resource-constrained environments by proposing a training-free LLM ensemble framework that operates at inference time. Rather than requiring multiple distinct models or relying on limited stochastic variation from re",
      "source_tool": "Manual",
      "zotero_key": "I4EY5MX4"
    },
    {
      "id": "Linnemann_2023_Bedeutung_von_Künstlicher_Intelligenz_in_der_Sozia",
      "filename": "Linnemann_2023_Bedeutung_von_Künstlicher_Intelligenz_in_der_Sozia.md",
      "title": "Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit",
      "author_year": "Linnemann (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "de",
      "doi": "10.1007/s12592-023-00455-7",
      "url": "https://doi.org/10.1007/s12592-023-00455-7",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 2,
      "rel_bias": 1,
      "rel_praxis": 1,
      "rel_prof": 3,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Professional Context",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-ai-komp-medium",
        "dim-vulnerable-medium",
        "dim-prof-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Linnemann_2023_Bedeutung.md",
      "abstract": "Die Bedeutung des Einsatzes von Verfahren, die unter dem Begriff der Künstlichen Intelligenz (KI) zusammenzufassen sind, wird sowohl für gesellschaftliche Prozesse als auch den Auftrag an die Soziale Arbeit zunehmend erkannt und diskutiert. Mit diesem Artikel wird ein Beitrag zum Diskurs geleistet, indem vertieft der Bereich der Sprachverarbeitung durch KI, das Natural Language Processing (NLP), in den Blick genommen wird. Verarbeitung natürlicher Sprache ist aufgrund der hohen Bedeutung kommuni",
      "summary_section": "## Overview\n\nThis German-language academic forum article by Linnemann, Löhe, and Rottkemper (published June 2023, accepted May 2023) addresses the growing importance of Artificial Intelligence in social work, with particular emphasis on Natural Language Processing (NLP) technologies. Published acros",
      "source_tool": "Manual",
      "zotero_key": "678XS28X"
    },
    {
      "id": "Linnemann_2025_Künstliche_Intelligenz_in_der_Sozialen_Arbeit__Gru",
      "filename": "Linnemann_2025_Künstliche_Intelligenz_in_der_Sozialen_Arbeit__Gru.md",
      "title": "Künstliche Intelligenz in der Sozialen Arbeit: Grundlagen für Theorie und Praxis",
      "author_year": "Linnemann (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "book",
      "language": "nan",
      "doi": "nan",
      "url": "https://doi.org/10.3262/978-3-7799-8562-4",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 3,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 12,
      "relevance_category": "high",
      "top_dimensions": [
        "AI Literacy",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-high",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "First major German-language systematic treatment of AI in social work from multiple perspectives. Bridges technological progress and ethics, treating AI theoretically and in practice-oriented applications. Addresses technical AI basics for social work, ethical and legal frameworks, bias and discrimination in training data, automation bias risks, development of AI competencies in education and organizations, and specific application fields. Emphasizes responsible, reflective engagement with AI en",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "7YWKF2UF"
    },
    {
      "id": "Lin_2022_Artificial_Intelligence_in_a_Structurally_Unjust_S",
      "filename": "Lin_2022_Artificial_Intelligence_in_a_Structurally_Unjust_S.md",
      "title": "Artificial Intelligence in a Structurally Unjust Society",
      "author_year": "Lin (2022)",
      "authors": [],
      "publication_year": 2022.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://ojs.lib.uwo.ca/index.php/fpq/article/download/14191/12139/38443",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Lin_2022_Artificial.md",
      "abstract": "This study argues that AI bias represents a form of structural injustice that arises when AI systems interact with other social factors to exacerbate existing social inequalities. Using the example of AI in healthcare, the authors show that the goal of AI fairness is to strive for a more just social structure through appropriate development and use of AI systems. They argue that all actors involved in the unjust social structure bear shared responsibility to join collective actions for structura",
      "summary_section": "## Overview\n\nThis paper by Lin and Chen presents a philosophical critique of mainstream AI fairness approaches by reframing AI bias as **structural injustice** rather than a technical problem. The authors argue that contemporary efforts—which pursue statistical parity and algorithmic debiasing—funda",
      "source_tool": "Manual",
      "zotero_key": "WC6JF3VD"
    },
    {
      "id": "Lin_2024_SWIFTSAGE__A_new_dual-module_framework_for_better_",
      "filename": "Lin_2024_SWIFTSAGE__A_new_dual-module_framework_for_better_.md",
      "title": "SWIFTSAGE: A new dual-module framework for better action planning in complex interactive reasoning tasks",
      "author_year": "Lin (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2404.17218v3",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "E5BB97AY"
    },
    {
      "id": "Liu_2025_More_or_less_wrong__A_benchmark_for_directional_bi",
      "filename": "Liu_2025_More_or_less_wrong__A_benchmark_for_directional_bi.md",
      "title": "More or less wrong: A benchmark for directional bias in LLM comparative reasoning",
      "author_year": "Liu (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2506.03923v1",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "MP5QFDGI"
    },
    {
      "id": "Li_2023_Ethics_&_AI__A_systematic_review_on_ethical_concer",
      "filename": "Li_2023_Ethics_&_AI__A_systematic_review_on_ethical_concer.md",
      "title": "Ethics & AI: A systematic review on ethical concerns and related strategies for designing with AI in healthcare",
      "author_year": "Li (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.3390/ai4010003",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 2,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium",
        "dim-prof-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Systematic literature review (2010-2020) identifying 12 main ethical issues in AI healthcare applications with direct relevance to social services. Critical value tensions identified include justice and fairness (algorithmic bias causing discrimination), freedom and autonomy (control, respecting human autonomy, and informed consent challenges), privacy violations through data-driven systems, transparency conflicts with black-box algorithms, dignity concerns when AI reduces persons to data, and c",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "A592677H"
    },
    {
      "id": "Long_2020_What_is_AI_Literacy__Competencies_and_Design_Consi",
      "filename": "Long_2020_What_is_AI_Literacy__Competencies_and_Design_Consi.md",
      "title": "What is AI Literacy? Competencies and Design Considerations",
      "author_year": "Long (2020)",
      "authors": [],
      "publication_year": 2020.0,
      "item_type": "conferencePaper",
      "language": "en",
      "doi": "10.1145/3313831.3376727",
      "url": "https://dl.acm.org/doi/10.1145/3313831.3376727",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "BFK6D38S"
    },
    {
      "id": "Lund_2025_Algorithms,_artificial_intelligence_and_discrimina",
      "filename": "Lund_2025_Algorithms,_artificial_intelligence_and_discrimina.md",
      "title": "Algorithms, artificial intelligence and discrimination",
      "author_year": "Lund (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://ldo.no/content/uploads/2025/05/Algorithms-artificial-intelligence-and-discrimination-report.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Lund_2025_Algorithms.md",
      "abstract": "Dieser norwegische Regierungsbericht überprüft Schlüsselelemente des norwegischen Gleichstellungs- und Antidiskriminierungsgesetzes mit primärem Fokus auf algorithmische Diskriminierung. Der Bericht diskutiert die mögliche Einführung spezifischer Definitionen direkter und indirekter algorithmischer Diskriminierung und schlägt die Schaffung einer spezifischen Bestimmung zu rechtmäßiger algorithmischer Differenzialbehandlung vor. Die Komplexität algorithmischer Systeme erschwert die Unterscheidung",
      "summary_section": "## Overview\n\nThis Norwegian legal report, authored by Professor Vibeke Blaker Strand (University of Oslo) and published by the Equality and Anti-Discrimination Ombud (LDO) in 2024, examines whether existing equality and anti-discrimination legislation adequately protects individuals from discriminat",
      "source_tool": "Manual",
      "zotero_key": "GN5D9A7D"
    },
    {
      "id": "Marjanovic_2022_Theorising_algorithmic_justice",
      "filename": "Marjanovic_2022_Theorising_algorithmic_justice.md",
      "title": "Theorising algorithmic justice",
      "author_year": "Marjanovic (2022)",
      "authors": [],
      "publication_year": 2022.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1080/0960085X.2021.1934130",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 2,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-prof-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Theoretical article developing framework for understanding algorithmic justice in automated decision-making systems with specific application to social welfare services. Authors examine how AI-driven automated algorithmic decision-making threatens core social justice principles in transformative services like welfare. Framework addresses WHAT matters in algorithmic justice (fairness, equity, human rights), WHO counts as subjects (vulnerable populations disproportionately affected), and HOW algor",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "MJTUVCJ2"
    },
    {
      "id": "Ma_2023_Intersectional_Stereotypes_in_Large_Language_Model",
      "filename": "Ma_2023_Intersectional_Stereotypes_in_Large_Language_Model.md",
      "title": "Intersectional Stereotypes in Large Language Models: Dataset and Analysis",
      "author_year": "Ma (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.18653/v1/2023.findings-emnlp.575",
      "url": "https://aclanthology.org/2023.findings-emnlp.575.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Ma_2023_Intersectional.md",
      "abstract": "This EMNLP paper introduces a dataset for studying intersectional stereotypes and applies it to three LLMs. Results reveal emergent stereotypes not predictable from single-attribute analysis. Prompt engineering reduces but does not eliminate such patterns, highlighting persistent biases in generated narratives.",
      "summary_section": "## Overview\n\nThis research addresses a critical gap in AI bias scholarship by systematically investigating how Large Language Models propagate stereotypes targeting intersectional demographic groups—individuals defined by multiple overlapping identity categories simultaneously. While extensive prior",
      "source_tool": "Manual",
      "zotero_key": "9YEC6SQ7"
    },
    {
      "id": "McCrory_2024_Avoiding_catastrophe_through_intersectionality_in_",
      "filename": "McCrory_2024_Avoiding_catastrophe_through_intersectionality_in_.md",
      "title": "Avoiding catastrophe through intersectionality in global AI governance",
      "author_year": "McCrory (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.cigionline.org/documents/3375/DPH-paper-Laine_McCrory.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 2,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_McCrory_2024_Avoiding.md",
      "abstract": "In this working paper, McCrory argues that prevailing global AI governance efforts (especially those focused on “AI safety” and existential risks) lack an adequate intersectional, feminist perspective, which is needed to avert not just hypothetical future catastrophes but ongoing injustices. The paper analyzes seven prominent international AI policy initiatives (e.g. global AI accords and principles) against criteria derived from feminist theory, such as intersectionality, context, and power dyn",
      "summary_section": "## Overview\n\nThis working paper from CIGI's Digital Policy Hub, supported by Mitacs partnership, addresses a critical gap in artificial intelligence governance by integrating feminist intersectional analysis into AI safety discourse. The research, authored by Laine McCrory, challenges the predominan",
      "source_tool": "Manual",
      "zotero_key": "NMD5P5LN"
    },
    {
      "id": "McDonald_2023_Algorithmic_decision-making_in_social_work_practic",
      "filename": "McDonald_2023_Algorithmic_decision-making_in_social_work_practic.md",
      "title": "Algorithmic decision-making in social work practice and pedagogy: Confronting the competency/critique dilemma",
      "author_year": "McDonald (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1080/02615479.2023.2195425",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 3,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 12,
      "relevance_category": "high",
      "top_dimensions": [
        "AI Literacy",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-high",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Explores tensions between training social workers for technologically-driven environments and engaging students in critical theories acknowledging unequal power distributions that technological change reinforces. Introduces algorithmic literacy centered on understanding critical limitations of algorithmic decision-making systems. Argues that adding social work subjects on ADM alone proves insufficient; students need opportunities to develop algorithmic literacy enabling them to ask appropriate q",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "ANU3K3D4"
    },
    {
      "id": "Meilvang_2024_Decision_support_and_algorithmic_support__The_cons",
      "filename": "Meilvang_2024_Decision_support_and_algorithmic_support__The_cons.md",
      "title": "Decision support and algorithmic support: The construction of algorithms and professional discretion in social work",
      "author_year": "Meilvang (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "bookSection",
      "language": "nan",
      "doi": "nan",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 1,
      "rel_prof": 3,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Professional Context",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Critical analysis examining three decision-support algorithms developed for Danish municipalities in child and family social work, analyzing how they affect professional discretion despite claims to merely support professionals. Demonstrates how algorithmic systems designed to minimize subjective judgment and promote efficiency actually embody positivist assumptions that professional discretion can and should be eliminated. Key findings reveal how political actors favor standardized, automated a",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "KJF5FFWW"
    },
    {
      "id": "Mei_2023_Assessing_GPT's_bias_towards_stigmatized_social_gr",
      "filename": "Mei_2023_Assessing_GPT's_bias_towards_stigmatized_social_gr.md",
      "title": "Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia",
      "author_year": "Mei (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/pdf/2505.17045",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "AKZCFJN2"
    },
    {
      "id": "Moreau_2024_Failing_our_youngest__On_the_biases,_pitfalls,_and",
      "filename": "Moreau_2024_Failing_our_youngest__On_the_biases,_pitfalls,_and.md",
      "title": "Failing our youngest: On the biases, pitfalls, and risks in a decision support algorithm used for child protection",
      "author_year": "Moreau (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.1145/3630106.3658906",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 2,
      "total_relevance": 11,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium",
        "dim-prof-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Critical empirical study examining child protection decision support algorithm deployed in Danish municipalities, analyzing its biases and implementation challenges. Using real administrative data from Denmark's child welfare system, evaluated algorithm's predictions against actual case outcomes and found significant biases including disproportionate impacts on immigrant families and systematic errors in risk assessment. Results revealed concerning patterns of false positives for marginalized co",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "EFFZR3YS"
    },
    {
      "id": "Mosene_2023_Feministische_Netzpolitik_und_Künstliche_Intellige",
      "filename": "Mosene_2023_Feministische_Netzpolitik_und_Künstliche_Intellige.md",
      "title": "Feministische Netzpolitik und Künstliche Intelligenz in der politischen Bildung [Feminist network politics and artificial intelligence in political education]",
      "author_year": "Mosene (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "webpage",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.politische-medienkompetenz.de/debatte/ki-und-intersektionalitaet/",
      "decision": "Unclear",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 7,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "unclear",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Examines intersections of feminist network politics and AI within political education, emphasizing intersectional feminist perspectives on digital technologies. Argues feminist network politics involves supporting AI researchers and activists working to eliminate bias in development and outcomes. Discusses how traditional gender roles are reinforced through AI systems and advocates for political education helping users understand how technologies function, emerged, which societal ideas they refl",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "SAT98SKE"
    },
    {
      "id": "National_Association_of_Social_Workers_2017_NASW,_ASWB,_CSWE,_&_CSWA_standards_for_technology_",
      "filename": "National_Association_of_Social_Workers_2017_NASW,_ASWB,_CSWE,_&_CSWA_standards_for_technology_.md",
      "title": "NASW, ASWB, CSWE, & CSWA standards for technology in social work practice",
      "author_year": "National Association of Social Workers (2017)",
      "authors": [],
      "publication_year": 2017.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.socialworkers.org/Practice/NASW-Practice-Standards-Guidelines/Standards-for-Technology-in-Social-Work-Practice",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 1,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Professional Context",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Landmark 64-page collaborative document representing unprecedented coordination among four major U.S. social work organizations to establish comprehensive technology standards for the profession. Standards address four main areas: providing information to public, designing and delivering services, gathering/managing/storing/accessing client information, and educating and supervising social workers. Covers practitioner competence, informed consent, privacy and confidentiality, boundaries and dual",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "5FRR2AAG"
    },
    {
      "id": "Navigli_2023_Biases_in_large_language_models__Origins,_inventor",
      "filename": "Navigli_2023_Biases_in_large_language_models__Origins,_inventor.md",
      "title": "Biases in large language models: Origins, inventory and discussion",
      "author_year": "Navigli (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1145/3597307",
      "url": "https://doi.org/10.1145/3597307",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 7,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Provides an overview of various social biases manifested by large language models and discusses their root causes. Examines how training data selection leads to bias and surveys different types of biases including gender, racial/ethnic, sexual orientation, age, religious and cultural biases. Compiles an inventory of biased behaviors and discusses emerging approaches to measure and mitigate such biases.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "IBI7IQV2"
    },
    {
      "id": "Ng_2021_Conceptualizing_AI_literacy__An_exploratory_review",
      "filename": "Ng_2021_Conceptualizing_AI_literacy__An_exploratory_review.md",
      "title": "Conceptualizing AI literacy: An exploratory review",
      "author_year": "Ng (2021)",
      "authors": [],
      "publication_year": 2021.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1016/j.caeai.2021.100041",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000357",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "8WU98XN8"
    },
    {
      "id": "Nuwasiima_2024_The_role_of_artificial_intelligence_(AI)_and_machi",
      "filename": "Nuwasiima_2024_The_role_of_artificial_intelligence_(AI)_and_machi.md",
      "title": "The role of artificial intelligence (AI) and machine learning in social work practice",
      "author_year": "Nuwasiima (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.30574/wjarr.2024.24.1.2998",
      "url": "https://doi.org/10.30574/wjarr.2024.24.1.2998",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 12,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium",
        "dim-prof-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Nuwasiima_2024_role.md",
      "abstract": "This comprehensive review identifies algorithmic bias as a critical challenge in social work AI implementation, noting that algorithms trained on historical data may perpetuate existing inequalities by replicating racial, gender, and socio-economic disparities. The authors document specific cases where predictive analytics tools disproportionately flagged families of color for child welfare interventions despite lacking substantial evidence of higher abuse rates. The study emphasizes that addres",
      "summary_section": "![[summary_Nuwasiima_2024_role.md]]",
      "source_tool": "Manual",
      "zotero_key": "63ULI2RK"
    },
    {
      "id": "Näscher_2025_ReflectAI__Design_and_evaluation_of_an_AI_coach_to",
      "filename": "Näscher_2025_ReflectAI__Design_and_evaluation_of_an_AI_coach_to.md",
      "title": "ReflectAI: Design and evaluation of an AI coach to support public servants' self-reflection",
      "author_year": "Näscher (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.1007/978-3-032-02515-9_7",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 3,
      "rel_prof": 2,
      "total_relevance": 7,
      "relevance_category": "medium",
      "top_dimensions": [
        "Practical Implementation",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-ai-komp-medium",
        "dim-praxis-high",
        "dim-prof-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Design science research presenting ReflectAI, an LLM-based AI coach designed to support public servants in developing self-reflection competencies—critical skill for digital transformation in public administration. Two-week user study with seven public servants revealed three key benefits: increased awareness of self-reflection opportunities, improved thought structure, and valuable conversation documentation. Demonstrates how conversational AI can facilitate reflective practice through structur",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "L3MJF7R8"
    },
    {
      "id": "OECD_2023_Advancing_Accountability_in_AI",
      "filename": "OECD_2023_Advancing_Accountability_in_AI.md",
      "title": "Advancing Accountability in AI",
      "author_year": "OECD (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.oecd.org/content/dam/oecd/en/publications/reports/2023/02/advancing-accountability-in-ai_753bf8c8/2448f04b-en.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_OECD_2023_Advancing.md",
      "abstract": "Delivers a multi-level review of AI accountability, focusing on transparency, fairness, and privacy. Discusses trade-offs in adopting explainability and transparency measures while mitigating algorithmic bias and upholding fairness, framed within legal, social, and ethical requirements for inclusive, trustworthy AI.",
      "summary_section": "## Overview\n\nThe OECD Digital Economy Paper No. 349 (February 2023, reference: DSTI/CDEP/AIGO(2022)5/FINAL) presents a comprehensive framework for advancing accountability and trustworthiness in artificial intelligence systems through integrated risk management approaches spanning the entire AI life",
      "source_tool": "Manual",
      "zotero_key": "4RTCRZXA"
    },
    {
      "id": "Ovalle_2023_Factoring_the_Matrix_of_Domination__A_Critical_Rev",
      "filename": "Ovalle_2023_Factoring_the_Matrix_of_Domination__A_Critical_Rev.md",
      "title": "Factoring the matrix of domination: A critical review and reimagination of intersectionality in AI fairness",
      "author_year": "Ovalle (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.1145/3600211.3604705",
      "url": "https://dl.acm.org/doi/abs/10.1145/3600211.3604705",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Ovalle_2023_Factoring.md",
      "abstract": "Critical review examining how intersectionality is conceptualized and operationalized within AI fairness research, analyzing 30 papers from the field. Identifies significant gaps between intersectionality theory and technical applications, proposing six key tenets for properly applying intersectionality in AI research drawn from Patricia Hill Collins and Sirma Bilge's framework.",
      "summary_section": "## Overview\n\nThis paper by Ovalle, Subramonian, Gee, Gautam, and Chang presents a critical examination of how intersectionality—a foundational framework from critical scholarship—is adopted and operationalized within AI fairness research. The authors argue that contemporary AI fairness literature fu",
      "source_tool": "Manual",
      "zotero_key": "PXQE9GBV"
    },
    {
      "id": "Ovalle_2024_Towards_Substantive_Equality_in_Artificial_Intelli",
      "filename": "Ovalle_2024_Towards_Substantive_Equality_in_Artificial_Intelli.md",
      "title": "Towards Substantive Equality in Artificial Intelligence: Transformative AI Policy for Gender Equality and Diversity",
      "author_year": "Ovalle (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://datapopalliance.org/publications/towards-real-diversity-and-gender-equality-in-ai/",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Dieser GPAI-Bericht, basierend auf Konsultationen mit über 200 Teilnehmern aus mehr als 50 Ländern, entwickelt einen menschenrechtsbasierten Rahmen für substantielle Gleichberechtigung in der KI. Der Bericht betont, dass KI ohne Intervention das Risiko birgt, gesellschaftliche Verzerrungen zu perpetuieren und zu verstärken, insbesondere gegen historisch marginalisierte Gruppen. Die Empfehlungen zielen darauf ab, die strukturellen Wurzeln der Ungleichheit zu bekämpfen und transformative Veränderu",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "FQVQFLTQ"
    },
    {
      "id": "Pan_2025_LIBRA__Measuring_bias_of_large_language_model_from",
      "filename": "Pan_2025_LIBRA__Measuring_bias_of_large_language_model_from.md",
      "title": "LIBRA: Measuring bias of large language model from a local context",
      "author_year": "Pan (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.researchgate.net/publication/388686547_LIBRA_Measuring_Bias_of_Large_Language_Model_from_a_Local_Context",
      "decision": "Unclear",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 0,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "unclear",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Critiques the U.S.-centricity of existing LLM bias evaluation methods. Proposes the Local Integrated Bias Recognition and Assessment (LIBRA) framework and develops dataset of over 360,000 test cases specific to New Zealand context. Results show models like BERT and GPT-2 struggle with local context, while Llama-3 responds better to different cultural contexts despite exhibiting larger bias overall.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "J8E5TXGQ"
    },
    {
      "id": "Park_2025_AI_algorithm_transparency,_pipelines_for_trust_not",
      "filename": "Park_2025_AI_algorithm_transparency,_pipelines_for_trust_not.md",
      "title": "AI algorithm transparency, pipelines for trust not prisms: mitigating general negative attitudes and enhancing trust toward AI",
      "author_year": "Park (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1057/s41599-025-05116-z",
      "url": "https://www.nature.com/articles/s41599-025-05116-z",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 1,
      "rel_bias": 0,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 5,
      "relevance_category": "medium",
      "top_dimensions": [
        "Practical Implementation",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Park_2025_algorithm.md",
      "abstract": "Peer-reviewed study examining how algorithmic transparency overcomes users' predisposed distrust in AI. 2×2 experiment showing transparency significantly increases trust, especially for those with initially negative AI attitudes. Making AI workings visible mitigated negative relationship between user AI-skepticism and organizational trust. Transparency acts as \"signal of trustworthiness,\" reducing skepticism and uncertainty. Emphasizing transparency in prompts and system design can reassure wary",
      "summary_section": "![[summary_Park_2025_algorithm.md]]",
      "source_tool": "Manual",
      "zotero_key": "IV7XJZVF"
    },
    {
      "id": "Parrish_2022_BBQ__A_hand-built_bias_benchmark_for_question_answ",
      "filename": "Parrish_2022_BBQ__A_hand-built_bias_benchmark_for_question_answ.md",
      "title": "BBQ: A hand-built bias benchmark for question answering",
      "author_year": "Parrish (2022)",
      "authors": [],
      "publication_year": 2022.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "nan",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "QRJRGPDE"
    },
    {
      "id": "Parrish_2025_Self-debiasing_large_language_models__Zero-shot_re",
      "filename": "Parrish_2025_Self-debiasing_large_language_models__Zero-shot_re.md",
      "title": "Self-debiasing large language models: Zero-shot recognition and reduction of stereotypes",
      "author_year": "Parrish (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "https://aclanthology.org/2025.naacl-short.74.pdf",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "485SYMAZ"
    },
    {
      "id": "Patton_2023_ChatGPT_for_Social_Work_Science__Ethical_Challenge",
      "filename": "Patton_2023_ChatGPT_for_Social_Work_Science__Ethical_Challenge.md",
      "title": "ChatGPT for Social Work Science: Ethical Challenges and Opportunities",
      "author_year": "Patton (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1086/726042",
      "url": "https://doi.org/10.1086/726042",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Professional Context",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Ethical framework for using LLMs in social work research. Recommends transparency, verification, authorship integrity, anti-plagiarism, and inclusion/social justice to counter bias. Positions LLMs as assistive tools requiring critical human oversight.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "F7WBGDRS"
    },
    {
      "id": "Peng_2022_A_Literature_Review_of_Digital_Literacy_over_Two_D",
      "filename": "Peng_2022_A_Literature_Review_of_Digital_Literacy_over_Two_D.md",
      "title": "A Literature Review of Digital Literacy over Two Decades",
      "author_year": "Peng (2022)",
      "authors": [],
      "publication_year": 2022.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1155/2022/2533413",
      "url": "https://www.hindawi.com/journals/edri/2022/2533413/",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "The COVID-19 pandemic has forced online learning to be a “new normal” during the past three years, which highly emphasizes students’ improved digital literacy. This study aims to present a literature review of students’ digital literacy. Grounded on about twenty journal articles and other related publications from the Web of Science Core Collection, this paper focused on the definition of digital literacy; the factors affecting students’ digital literacy (age, gender, family socioeconomic status",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "PBGVQHC5"
    },
    {
      "id": "Perron_2023_Recommendations_for_social_work_researchers_and_jo",
      "filename": "Perron_2023_Recommendations_for_social_work_researchers_and_jo.md",
      "title": "Recommendations for social work researchers and journal editors on the use of generative AI and large language models",
      "author_year": "Perron (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1086/726021",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 1,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Professional Context",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-medium",
        "dim-bias-medium",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Evidence-based recommendations for social work researchers using generative AI and LLMs, addressing prompt engineering and critical engagement with AI outputs. Examines how LLMs can improve research efficiency through facilitating literature reviews, data analysis, and writing assistance while emphasizing need for critical evaluation of AI-generated content. Discusses concerns about over-reliance on AI potentially diminishing research quality when researchers don't engage in critical thinking or",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "9QTT2ZFP"
    },
    {
      "id": "Petzel_2025_Prejudiced_interactions_with_large_language_models",
      "filename": "Petzel_2025_Prejudiced_interactions_with_large_language_models.md",
      "title": "Prejudiced interactions with large language models (LLMs) reduce trustworthiness and behavioral intentions among members of stigmatized groups",
      "author_year": "Petzel (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1016/j.chb.2025.108563",
      "url": "https://doi.org/10.1016/j.chb.2025.108563",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Investigates how biased or prejudiced content in LLM responses affects user trust and willingness to use the system, particularly for users from marginalized communities. Through three preregistered experiments, finds that when AI responses exhibited prejudice, participants from marginalized groups reported significantly lower trust and decreased intentions to continue using the system.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "XTQSJGIB"
    },
    {
      "id": "Pinski_2023_AI_Literacy_-_Towards_Measuring_Human_Competency_i",
      "filename": "Pinski_2023_AI_Literacy_-_Towards_Measuring_Human_Competency_i.md",
      "title": "AI Literacy - Towards Measuring Human Competency in Artificial Intelligence",
      "author_year": "Pinski (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.24251/HICSS.2023.021",
      "url": "http://hdl.handle.net/10125/102649",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "IKFHPHCV"
    },
    {
      "id": "Pinski_2024_AI_literacy_for_users_–_A_comprehensive_review_and",
      "filename": "Pinski_2024_AI_literacy_for_users_–_A_comprehensive_review_and.md",
      "title": "AI literacy for users – A comprehensive review and future research directions of learning methods, components, and effects",
      "author_year": "Pinski (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1016/j.chbah.2024.100062",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2949882124000227",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "KLJFJQBH"
    },
    {
      "id": "Prakash_2023_Prompt_engineering_techniques_for_mitigating_cultu",
      "filename": "Prakash_2023_Prompt_engineering_techniques_for_mitigating_cultu.md",
      "title": "Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review",
      "author_year": "Prakash (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.researchgate.net/publication/392942981_Prompt_Engineering_Techniques_for_Mitigating_Cultural_Bias_Against_Arabs_and_Muslims_in_Large_Language_Models_A_Systematic_Review",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "8F9ZD4CD"
    },
    {
      "id": "Qiu_2025_DR.GAP__Mitigating_bias_in_large_language_models_u",
      "filename": "Qiu_2025_DR.GAP__Mitigating_bias_in_large_language_models_u.md",
      "title": "DR.GAP: Mitigating bias in large language models using gender-aware prompting with demonstration and reasoning",
      "author_year": "Qiu (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2502.11603v1",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 3,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Bias Analysis",
        "Practical Implementation"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Qiu_2025_Mitigating.md",
      "abstract": "DR.GAP ist eine prompting-basierte Methode zur Bias-Reduktion in LLMs. Sie nutzt Beispielfälle und strukturierte Reasoning-Schritte, um gendergerechtere Antworten zu erzielen.",
      "summary_section": "![[summary_Qiu_2025_Mitigating.md]]",
      "source_tool": "Manual",
      "zotero_key": "H28VMEA3"
    },
    {
      "id": "Quaid-i-Azam_University_2025_Gender_Bias_in_Artificial_Intelligence__Empowering",
      "filename": "Quaid-i-Azam_University_2025_Gender_Bias_in_Artificial_Intelligence__Empowering.md",
      "title": "Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy",
      "author_year": "Quaid-i-Azam University (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.70389/PJAI.1000088",
      "url": "https://premierscience.com/pjai-24-524/",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-medium",
        "dim-vulnerable-high",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Quaid-i-Azam_University_2025_Gender.md",
      "abstract": "Purpose This narrative review investigates the interplay between gender bias in artificial intelligence (AI) systems and the potential of digital literacy to empower women in technology. By synthesising research from 2010 to 2024, the study examines how gender bias manifests in AI, its impact on women’s participation in technology, and the effectiveness of digital literacy initiatives in addressing these disparities. Methods A systematic literature search was conducted across major academic data",
      "summary_section": "## Overview\n\nThis narrative review, published in January 2025, examines the intersection of gender bias in artificial intelligence systems and digital literacy as a mechanism for women's empowerment in technology sectors. Authored by Syed Sibghatullah Shah from Quaid-i-Azam University, the study syn",
      "source_tool": "Manual",
      "zotero_key": "H8TEPVZW"
    },
    {
      "id": "Raji_2024_The_Algorithmic_Auditing_Landscape__A_Social_Justi",
      "filename": "Raji_2024_The_Algorithmic_Auditing_Landscape__A_Social_Justi.md",
      "title": "The Algorithmic Auditing Landscape: A Social Justice Approach",
      "author_year": "Raji (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.1145/3630659.3630671",
      "url": "https://dl.acm.org/doi/10.1145/3630659.3630671",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Raji and Buolamwini, pioneers of algorithmic auditing, argue for an approach rooted in social justice. They critique audits that focus solely on technical metrics, advocating instead for methods that center the lived experiences of marginalized communities. This involves a multi-stakeholder process, transparency, and a focus on real-world harms. Such an audit practice inherently makes the co-constitution of discrimination visible by investigating not just the algorithm's output, but the entire s",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "8DTUS7Y2"
    },
    {
      "id": "Reamer_2023_Artificial_intelligence_in_social_work__Emerging_e",
      "filename": "Reamer_2023_Artificial_intelligence_in_social_work__Emerging_e.md",
      "title": "Artificial intelligence in social work: Emerging ethical issues",
      "author_year": "Reamer (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.55521/10-020-205",
      "url": "https://doi.org/10.55521/10-020-205",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 11,
      "relevance_category": "high",
      "top_dimensions": [
        "Bias Analysis",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium",
        "dim-prof-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Reamer_2023_Artificial.md",
      "abstract": "Systematic analysis of ethical challenges in AI use within social work, developing comprehensive framework for ethical AI implementation. Identifies eight central ethical areas including informed consent, data privacy, transparency, misdiagnosis, client neglect, surveillance, scientific misconduct, and algorithmic bias. Emphasizes transparency as fundamental principle requiring social workers to inform clients about AI use. Develops eight-step protocol for ethical AI implementation including eth",
      "summary_section": "## Overview\n\nFrederic G. Reamer's article addresses a critical professional gap: while artificial intelligence applications proliferate across social work practice, systematic ethical examination remains underdeveloped. Published in the International Journal of Social Work Values and Ethics (2023), ",
      "source_tool": "Manual",
      "zotero_key": "JIYZUBLR"
    },
    {
      "id": "Ricaurte_2024_How_can_feminism_inform_AI_governance_in_practice_",
      "filename": "Ricaurte_2024_How_can_feminism_inform_AI_governance_in_practice_.md",
      "title": "How can feminism inform AI governance in practice?",
      "author_year": "Ricaurte (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.unesco.org/en/articles/how-can-feminism-inform-ai-governance-practice",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-medium",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Diese UNESCO-Publikation definiert feministische KI-Governance als einen aufkommenden Bereich von Politik, Forschung und Entwicklung, der darauf abzielt, KI-Systeme gerecht, gleichberechtigt und inklusiv zu gestalten. Feministische KI-Governance zielt darauf ab, Machtungleichgewichte im KI-Ökosystem zu adressieren und strukturelle Ungleichheiten, koloniale Vermächtnisse und multidimensionale Schäden zu berücksichtigen, die überproportional Gemeinschaften der globalen Mehrheit betreffen. Der Ansa",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "N3JH3CUQ"
    },
    {
      "id": "Ricaurte_Quijano_2024_Towards_substantive_equality_in_artificial_intelli",
      "filename": "Ricaurte_Quijano_2024_Towards_substantive_equality_in_artificial_intelli.md",
      "title": "Towards substantive equality in artificial intelligence: Transformative AI policy for gender equality and diversity",
      "author_year": "Ricaurte Quijano (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://wp.oecd.ai/app/uploads/2025/05/towards-substantive-equality-in-artificial-intelligence_Transformative-AI-policy-for-gender-equality-and-diversity.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Ricaurte_Quijano_2024_Towards.md",
      "abstract": "This extensive report – developed through the Global Partnership on AI (GPAI) with contributors from academia and policy – sets out a vision for “substantive equality” in AI as opposed to mere formal equality. It recognizes that AI systems can replicate and even amplify societal power imbalances (“algorithmic discrimination”), thus requiring proactive governance to ensure historically marginalized groups are not left behind in the AI era. The report argues that purely technical bias mitigation i",
      "summary_section": "## Overview\n\nThis November 2024 GPAI-endorsed report addresses a critical gap in AI governance by examining how to achieve genuine gender equality and diversity in artificial intelligence systems. Developed through a formal project titled \"Towards Real Diversity and Gender Equality in AI: Evidence-B",
      "source_tool": "Manual",
      "zotero_key": "SXXJ9Y7C"
    },
    {
      "id": "Rodriguez_2024_Introducing_Generative_Artificial_Intelligence_int",
      "filename": "Rodriguez_2024_Introducing_Generative_Artificial_Intelligence_int.md",
      "title": "Introducing Generative Artificial Intelligence into the MSW Curriculum: A Proposal for the 2029 Educational Policy and Accreditation Standards",
      "author_year": "Rodriguez (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1080/10437797.2024.2340931",
      "url": "https://doi.org/10.1080/10437797.2024.2340931",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 3,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 12,
      "relevance_category": "high",
      "top_dimensions": [
        "AI Literacy",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-high",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Proposal to add an explicit AI competency to MSW accreditation. Outlines benefits and risks of generative AI, recommends curricular content on ethics, bias, transparency, and responsible use, and frames AI literacy as essential for safeguarding client dignity and equity while leveraging innovation.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "C3QBQA6X"
    },
    {
      "id": "Rodríguez-Martínez_2024_Ethical_issues_related_to_the_use_of_technology_in",
      "filename": "Rodríguez-Martínez_2024_Ethical_issues_related_to_the_use_of_technology_in.md",
      "title": "Ethical issues related to the use of technology in social work practice: A systematic review",
      "author_year": "Rodríguez-Martínez (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1177/21582440241274842",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 1,
      "rel_prof": 3,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Professional Context",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Systematic literature review examining ethical tensions arising from technology integration in social work practice. Review identifies three main categories of ethical challenges: effects of digitization on professional practice (tensions between efficiency and human connection), education, research and engagement challenges (digital literacy requirements conflicting with traditional social work training), and ethical challenges in digital professional practice (boundary issues, dual relationshi",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "HCKBP3EJ"
    },
    {
      "id": "Ruiz_2024_AI_Literacy__A_Framework_to_Understand,_Evaluate,_",
      "filename": "Ruiz_2024_AI_Literacy__A_Framework_to_Understand,_Evaluate,_.md",
      "title": "AI Literacy: A Framework to Understand, Evaluate, and Use Emerging Technology",
      "author_year": "Ruiz (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://hdl.handle.net/20.500.12265/218",
      "decision": "Exclude",
      "exclusion_reason": "Wrong publication type",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "To enable all who participate in educational settings to leverage AI tools for powerful learning, this paper describes a framework and strategies for educational leaders to design and implement a clear approach to AI Literacy for their specific audiences (e.g. learners, teachers, or others) that are safe and effective. The first part of the paper describes a framework that identifies essential components of AI Literacy and connects them to existing initiatives. The second part of the paper ident",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "BGS9X3FC"
    },
    {
      "id": "Sabour_2023_A_chatbot_for_mental_health_support__Exploring_the",
      "filename": "Sabour_2023_A_chatbot_for_mental_health_support__Exploring_the.md",
      "title": "A chatbot for mental health support: Exploring the impact of Emohaa on reducing mental distress in China",
      "author_year": "Sabour (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.3389/fdgth.2023.1133987",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 2,
      "rel_bias": 1,
      "rel_praxis": 3,
      "rel_prof": 2,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Practical Implementation",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-praxis-high",
        "dim-prof-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Three-arm randomized controlled trial examining Emohaa mental health chatbot system in China with 141 university students experiencing mental distress. Participants assigned to: CBT-Bot only (n=69), Full Emohaa with CBT-Bot and emotional support bot (n=31), or control group (n=41). 8-week intervention used AI-powered conversational agents providing cognitive-behavioral therapy techniques and emotional support. Results showed significant improvements in depression (p<.001, effect size η²p=.05) an",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "B59U6C8V"
    },
    {
      "id": "Salecha_2025_Model_explanations_for_gender_and_ethnicity_bias_m",
      "filename": "Salecha_2025_Model_explanations_for_gender_and_ethnicity_bias_m.md",
      "title": "Model explanations for gender and ethnicity bias mitigation in AI-generated narratives",
      "author_year": "Salecha (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "thesis",
      "language": "nan",
      "doi": "nan",
      "url": "https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=7888&context=open_access_etds",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "2DFRY422"
    },
    {
      "id": "Salinas_2025_What’s_in_a_name__Auditing_large_language_models_f",
      "filename": "Salinas_2025_What’s_in_a_name__Auditing_large_language_models_f.md",
      "title": "What’s in a name? Auditing large language models for race and gender bias",
      "author_year": "Salinas (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2402.14875v3",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This interdisciplinary audit of GPT-4 and other LLMs reveals systematic intersectional biases based on names signaling race and gender. Prompts with names suggesting a Black woman received less favorable advice compared to those with white male names. This disparity was robust across 42 prompt templates. The study found that adding quantitative anchors (facts, numbers) to the prompt largely eliminated this bias, whereas adding qualitative descriptive details had inconsistent effects and sometime",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "WIHW537P"
    },
    {
      "id": "Santos_2024_Explainability_through_systematicity__The_hard_sys",
      "filename": "Santos_2024_Explainability_through_systematicity__The_hard_sys.md",
      "title": "Explainability through systematicity: The hard systematicity challenge",
      "author_year": "Santos (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12307450/",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This philosophical paper argues that the pursuit of \"explainability\" in AI is too narrow. It proposes a richer ideal called \"systematicity,\" which demands that an AI's reasoning be consistent, coherent, comprehensive, and principled, akin to an integrated body of human thought. The author distinguishes this \"hard systematicity challenge\" from the historical Fodorian debate on connectionism and explores how the demand for AI to be systematic should be regulated by different rationales.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "4VZE4BCT"
    },
    {
      "id": "Santos_2025_How_large_language_models_judge_cooperation",
      "filename": "Santos_2025_How_large_language_models_judge_cooperation.md",
      "title": "How large language models judge cooperation",
      "author_year": "Santos (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/pdf/2507.00088",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This study investigates how 21 state-of-the-art LLMs make social and moral judgments about cooperative behavior. Using an evolutionary game-theory model and a dataset of 43,200 prompts, the authors find significant variation in how different models assign reputations, particularly when judging interactions with \"ill-reputed\" actors. Demonstrates that LLM social norms are highly malleable and can be consistently steered by different types of prompt interventions.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "6VI6NUNL"
    },
    {
      "id": "Santy_2023_NLPositionality__Characterizing_design_biases_of_d",
      "filename": "Santy_2023_NLPositionality__Characterizing_design_biases_of_d.md",
      "title": "NLPositionality: Characterizing design biases of datasets and models",
      "author_year": "Santy (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "https://aclanthology.org/2023.acl-long.530/",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": true,
      "summary_file": "summary_Santy_2023_NLPositionality.md",
      "abstract": "This study provides a theoretical and methodological framework for making bias visible in AI models and datasets. It introduces the concept of \"positionality\" - the assumption that developers' social, cultural, and political perspectives inevitably flow into AI artifacts. The authors develop a method to quantify design biases by analyzing which dialects, demographic groups, or social contexts are under- or over-represented in datasets.",
      "summary_section": "## Overview\n\nThis paper addresses a critical challenge in computational political science: predicting political ideology from text when training data is sparse and systematically biased toward extreme positions. Authored by Chen Chen (CUHK Shenzhen), Dylan Walker (Chapman University), and Venkatesh ",
      "source_tool": "Manual",
      "zotero_key": "9HXBT8YG"
    },
    {
      "id": "Sant_2024_The_power_of_prompts__Evaluating_and_mitigating_ge",
      "filename": "Sant_2024_The_power_of_prompts__Evaluating_and_mitigating_ge.md",
      "title": "The power of prompts: Evaluating and mitigating gender bias in MT with LLMs",
      "author_year": "Sant (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.18653/v1/2024.gebnlp-1.7",
      "url": "https://doi.org/10.18653/v1/2024.gebnlp-1.7",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Examines gender bias in machine translation through LLMs using four widely-used test sets. Develops specific prompting engineering techniques that reduce gender bias by up to 12% on WinoMT evaluation dataset. Identifies optimal prompt structures incorporating explicit fairness instructions and context-aware guidelines.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "CNWL34SP"
    },
    {
      "id": "Schneider_2018_Der_Einfluss_der_Algorithmen__Neue_Qualitäten_durc",
      "filename": "Schneider_2018_Der_Einfluss_der_Algorithmen__Neue_Qualitäten_durc.md",
      "title": "Der Einfluss der Algorithmen: Neue Qualitäten durch Big Data Analytics und Künstliche Intelligenz",
      "author_year": "Schneider (2018)",
      "authors": [],
      "publication_year": 2018.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1007/s12054-018-0046-y",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 1,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 3,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-bias-high",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Examines how algorithmic decision-making affects professional judgment formation and discretionary decision-making space for practitioners. Analyzes automation bias where professionals may over-rely on algorithmic recommendations without adequate critical evaluation. Stresses social work requires debate about what forms of knowledge new technologies can generate, where limits lie, and how it can meaningfully be incorporated into professional reflection and decision-making practices.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "UU6PITY6"
    },
    {
      "id": "Schneider_2022_Exploring_opportunities_and_risks_in_decision_supp",
      "filename": "Schneider_2022_Exploring_opportunities_and_risks_in_decision_supp.md",
      "title": "Exploring opportunities and risks in decision support technologies for social workers: An empirical study in the field of disabled people's services",
      "author_year": "Schneider (2022)",
      "authors": [],
      "publication_year": 2022.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1093/bjsw/bcab262",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Professional Context",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Empirical study investigating German social workers' perspectives on decision support systems in disability services through practitioner interviews. Identifies both opportunities (consistency across cases, evidence-based practice, administrative time-saving) and significant risks (deprofessionalization, data protection concerns, reduced professional autonomy, loss of holistic assessment capabilities). Social workers express ambivalence: recognizing potential for reducing subjective bias and imp",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "VJGGQZQS"
    },
    {
      "id": "Schneider_2024_AI_for_decision_support__What_are_possible_futures",
      "filename": "Schneider_2024_AI_for_decision_support__What_are_possible_futures.md",
      "title": "AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?",
      "author_year": "Schneider (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.14512/tatup.33.1.08",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Comprehensive examination of AI-based decision support systems across healthcare, legal systems, and border control. Provides critical analysis of technical, ethical, legal, and societal challenges when machines make or support decisions previously made by humans. Reviews regulatory attempts including EU AI Act. Examines key issues: opacity of algorithmic systems creating black box problems for accountability, professional deskilling risks when practitioners defer to AI, and potential for discri",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "ZBAYPR6D"
    },
    {
      "id": "Schneider_2024_Das_verflixte_Problem_mit_Klassifikationen__Zum_Ei",
      "filename": "Schneider_2024_Das_verflixte_Problem_mit_Klassifikationen__Zum_Ei.md",
      "title": "Das verflixte Problem mit Klassifikationen: Zum Einfluss der Digitalisierung auf die Soziale Diagnostik in der Sozialen Arbeit",
      "author_year": "Schneider (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "bookSection",
      "language": "nan",
      "doi": "nan",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 3,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Bias Analysis",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Examines how digitalization affects social diagnostics and assessment practices in social work. Analyzes fundamental problem of classification systems: tension between necessary categorization for resource allocation and profession's commitment to individualized, contextual understanding of clients. Digital systems intensify this tension by requiring standardized data inputs that may not capture social complexity or unique circumstances. Explores how algorithmic decision support systems rely on ",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "MN6E9Y5E"
    },
    {
      "id": "Schneider_2025_Indecision_on_the_use_of_artificial_intelligence_i",
      "filename": "Schneider_2025_Indecision_on_the_use_of_artificial_intelligence_i.md",
      "title": "Indecision on the use of artificial intelligence in healthcare: A qualitative study of patient perspectives on trust, responsibility and self-determination using AI-CDSS",
      "author_year": "Schneider (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1186/s12910-024-01143-5",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 1,
      "rel_praxis": 1,
      "rel_prof": 2,
      "total_relevance": 7,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-prof-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Empirical qualitative study exploring patient perspectives on AI-based clinical decision support systems (AI-CDSS) in healthcare, revealing significant ambivalence about AI use in medical decision-making. Through interviews examining trust, responsibility distribution, and self-determination when AI systems assist physicians, findings show patients worry about decreased human interaction, loss of holistic care perspectives, and unclear accountability when AI makes errors. Vulnerable populations ",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "5ZIDAC8C"
    },
    {
      "id": "Schönauer_2025_Akzeptanz_von_KI_und_organisationale_Rahmenbedingu",
      "filename": "Schönauer_2025_Akzeptanz_von_KI_und_organisationale_Rahmenbedingu.md",
      "title": "Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen Arbeit",
      "author_year": "Schönauer (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "de",
      "doi": "10.1007/s12054-025-00783-3",
      "url": "https://doi.org/10.1007/s12054-025-00783-3",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 2,
      "rel_bias": 1,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Professional Context",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-medium",
        "dim-vulnerable-medium",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Der Artikel untersucht die Akzeptanz und organisationalen Rahmenbedingungen digitaler Technologien, insbesondere Künstlicher Intelligenz (KI), in der Praxis der Sozialen Arbeit aus Sicht der Fachkräfte. Während digitale Technologien im administrativen Bereich bereits weit verbreitet sind, zeigt sich bei der Nutzung digitaler Technologien in der direkten Arbeit mit den Klient:innen noch Zurückhaltung. Durch die Entwicklungen im Bereich KI ergeben sich zunehmend neue Möglichkeiten digitale Technol",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "GCBMWEKM"
    },
    {
      "id": "Shah_2025_Gender_bias_in_artificial_intelligence__Empowering",
      "filename": "Shah_2025_Gender_bias_in_artificial_intelligence__Empowering.md",
      "title": "Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy",
      "author_year": "Shah (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.70389/PJAI.1000088",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-medium",
        "dim-vulnerable-high",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Narrative review examining interplay between gender bias in AI systems and digital literacy potential for empowering women in technology. Synthesizes research from 2010-2024 analyzing systematic gender biases in AI applications across recruitment, healthcare, and financial services. These biases stem from women's underrepresentation in AI development teams (only 22% globally), biased training data, and algorithmic design decisions. Digital literacy programs show promise as intervention fostering",
      "summary_section": "## Overview\n\nThis narrative review examines the relationship between gender bias in artificial intelligence systems and digital literacy as an empowerment mechanism for women in technology. Published in January 2025 by Syed Sibghatullah Shah from Quaid-i-Azam University (Islamabad, Pakistan), the st",
      "source_tool": "Manual",
      "zotero_key": "XT6XMMWT"
    },
    {
      "id": "Sharma_2024_Intersectional_analysis_of_visual_generative_AI__t",
      "filename": "Sharma_2024_Intersectional_analysis_of_visual_generative_AI__t.md",
      "title": "Intersectional analysis of visual generative AI: the case of stable diffusion",
      "author_year": "Sharma (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://link.springer.com/article/10.1007/s00146-025-02498-1",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Sharma_2024_Intersectional.md",
      "abstract": "This study conducts a critical intersectional analysis of Stable Diffusion, a widely used visual generative AI tool. The examination of 180 generated images shows how the system perpetuates prevailing power systems such as sexism, racism, heteronormativity, and ableism, assuming white, physically healthy, masculine-presenting individuals as the standard. The study identifies three levels of analysis: (1) the aesthetics of AI images (micro-level), (2) institutional contexts (meso-level), and (3) ",
      "summary_section": "## Overview\n\nThis theoretical paper by Overbye-Thompson and Rice addresses a critical gap in algorithmic justice research by shifting focus from technical bias documentation to user-level adaptive responses. While extensive literature demonstrates algorithmic bias across healthcare, hiring, criminal",
      "source_tool": "Manual",
      "zotero_key": "8TVALKIV"
    },
    {
      "id": "Shin_2024_Can_prompt_modifiers_control_bias__A_comparative_a",
      "filename": "Shin_2024_Can_prompt_modifiers_control_bias__A_comparative_a.md",
      "title": "Can prompt modifiers control bias? A comparative analysis of text-to-image generative models",
      "author_year": "Shin (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/abs/2406.05602",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This preprint investigates whether explicit prompt modifiers can reduce societal biases in text-to-image generative AI models. Authors evaluated three models (Stable Diffusion, DALL·E 3, Adobe Firefly) comparing baseline versus bias-mitigating prompts. Analysis revealed notable biases across all models, with inconsistent effectiveness of prompt modifiers. While diversity-reflective prompting can expose hidden biases and sometimes nudge outputs towards inclusivity, it is not a comprehensive fix a",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "Q6NX3MNX"
    },
    {
      "id": "Shin_2025_Mitigating_age-related_bias_in_large_language_mode",
      "filename": "Shin_2025_Mitigating_age-related_bias_in_large_language_mode.md",
      "title": "Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development",
      "author_year": "Shin (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://pubsonline.informs.org/doi/10.1287/ijoc.2024.0645",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "8SRB7NHJ"
    },
    {
      "id": "Shukla_2025_Investigating_AI_systems__examining_data_and_algor",
      "filename": "Shukla_2025_Investigating_AI_systems__examining_data_and_algor.md",
      "title": "Investigating AI systems: examining data and algorithmic bias through hermeneutic reverse engineering",
      "author_year": "Shukla (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.frontiersin.org/journals/communication/articles/10.3389/fcomm.2025.1380252/full",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This work presents hermeneutic reverse engineering as a framework for investigating bias in AI systems. The approach considers AI systems as boundary objects and analyzes cultural meanings and assumptions embedded in techno-cultural objects. The study proposes three research perspectives: (1) comparative exploration of algorithmic bias, (2) investigation of impacts on various social groups, and (3) participatory approaches to include users in AI design.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "XU4WU6E8"
    },
    {
      "id": "Siapka_2023_Towards_a_Feminist_Metaethics_of_AI",
      "filename": "Siapka_2023_Towards_a_Feminist_Metaethics_of_AI.md",
      "title": "Towards a Feminist Metaethics of AI",
      "author_year": "Siapka (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/abs/2311.14700",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-ai-komp-medium",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Siapka_2023_Towards.md",
      "abstract": "This work develops a research agenda for a feminist metaethics of AI. Unlike traditional metaethics that reflects on moral judgments non-normatively, feminist metaethics expands its scope to ask not only what ethics is, but also how our approach to it should be. The author argues that a feminist metaethics of AI should investigate four areas: (1) continuity between theory and action in AI ethics, (2) real-world impacts of AI ethics, (3) the role and profile of those involved in AI ethics, and (4",
      "summary_section": "## Overview\n\nAnastasia Siapka's paper identifies a fundamental paradox in contemporary AI ethics: despite overwhelming proliferation of ethical guidelines, boards, codes of conduct, and dedicated journals across multiple disciplines (law, philosophy, computer science, policy, industry), the field la",
      "source_tool": "Manual",
      "zotero_key": "DSPW44SK"
    },
    {
      "id": "Sinders_2017_Feminist_Data_Set",
      "filename": "Sinders_2017_Feminist_Data_Set.md",
      "title": "Feminist Data Set",
      "author_year": "Sinders (2017)",
      "authors": [],
      "publication_year": 2017.0,
      "item_type": "webpage",
      "language": "nan",
      "doi": "nan",
      "url": "https://carolinesinders.com/feminist-data-set/",
      "decision": "Exclude",
      "exclusion_reason": "Wrong publication type",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Multi-year art-research project directly addresses critical prompting practices by interrogating every AI development step—data collection, labeling, training, algorithm selection, and chatbot design—through feminist and intersectional lenses. Conducts public workshops to collaboratively build feminist datasets. Represents concrete critical prompting practice through community-based data creation as protest against biased AI systems, demonstrating practical approaches to feminist prompting by cr",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "IVTVGBKG"
    },
    {
      "id": "Singer_2023_AI_Creates_the_Message__Integrating_AI_Language_Le",
      "filename": "Singer_2023_AI_Creates_the_Message__Integrating_AI_Language_Le.md",
      "title": "AI Creates the Message: Integrating AI Language Learning Models into Social Work Education and Practice",
      "author_year": "Singer (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1080/10437797.2023.2189878",
      "url": "https://doi.org/10.1080/10437797.2023.2189878",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 1,
      "rel_bias": 2,
      "rel_praxis": 1,
      "rel_prof": 3,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Professional Context",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-ai-komp-medium",
        "dim-bias-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Commentary advocating cautious integration of LLMs in teaching and practice. Describes pedagogical uses (idea generation, material tailoring) and warns of bias, factual errors, confidentiality risks, and plagiarism. Emphasizes transparency policies and that engagement with AI is ethically preferable to avoidance.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "F5D34SBY"
    },
    {
      "id": "Singer_2023_ChatGPT_for_social_work_science__Ethical_challenge",
      "filename": "Singer_2023_ChatGPT_for_social_work_science__Ethical_challenge.md",
      "title": "ChatGPT for social work science: Ethical challenges and opportunities",
      "author_year": "Singer (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1086/726042",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 1,
      "rel_bias": 2,
      "rel_praxis": 1,
      "rel_prof": 3,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Professional Context",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-bias-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Invited paper exploring opportunities and ethical challenges of deploying ChatGPT and large language models specifically for social work science. Describes potential uses of ChatGPT in social work research while examining critical ethical concerns related to algorithmic bias, data quality, and risk of replacing human expertise with AI-generated content. Offers preliminary recommendations for ethical ChatGPT use in social work research, emphasizing need for researchers to critically evaluate AI o",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "M3RB6VDA"
    },
    {
      "id": "Singh_2025_A_reparative_turn_in_AI",
      "filename": "Singh_2025_A_reparative_turn_in_AI.md",
      "title": "A reparative turn in AI",
      "author_year": "Singh (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/pdf/2506.05687",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Singh_2025_reparative.md",
      "abstract": "Argues for a \"reparative turn\" in AI governance, moving beyond harm prevention to focus on remedying harm after it occurs. Based on thematic analysis of 1,060 real-world AI harm incidents, proposes taxonomy of reparative actions around four goals: acknowledging harm, attributing responsibility, providing remedies, and enabling systemic change. Finds significant \"accountability gap\" with most corporate responses limited to symbolic acknowledgments.",
      "summary_section": "![[summary_Singh_2025_reparative.md]]",
      "source_tool": "Manual",
      "zotero_key": "AAJM5NZG"
    },
    {
      "id": "Skilton_2024_Inclusive_prompt_engineering__A_methodology_for_ha",
      "filename": "Skilton_2024_Inclusive_prompt_engineering__A_methodology_for_ha.md",
      "title": "Inclusive prompt engineering: A methodology for hacking biased AI image generation",
      "author_year": "Skilton (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.1145/3641237.3691655",
      "url": "https://www.researchgate.net/publication/385325948_Inclusive_Prompt_Engineering_A_Methodology_for_Hacking_Biased_AI_Image_Generation",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This conference paper introduces \"inclusive prompt engineering\" as a strategy to probe and mitigate biases in generative AI image systems. Authors developed methodology to systematically modify prompts and provide tools for generating more diverse outputs. User studies revealed that when participants encountered stereotypical outputs, they tried adding negative qualifiers but models often failed to obey these negations. Findings underscore need for improved prompt interfaces that actively promot",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "5PC7KGXU"
    },
    {
      "id": "Slesinger_2024_Training_in_Co-Creation_as_a_Methodological_Approa",
      "filename": "Slesinger_2024_Training_in_Co-Creation_as_a_Methodological_Approa.md",
      "title": "Training in Co-Creation as a Methodological Approach to Improve AI Fairness",
      "author_year": "Slesinger (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://cris.unibo.it/retrieve/707c849f-3aeb-4ca3-8d1f-2817960064bd/societies-14-00259%20(1).pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 3,
      "rel_prof": 1,
      "total_relevance": 12,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-medium",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Slesinger_2024_Training.md",
      "abstract": "This study examines the integration of training components in co-creation processes with vulnerable and marginalized stakeholder groups as part of developing AI bias detection and mitigation tools. The research shows that training on AI definitions, terminology, and socio-technical impacts is necessary to enable non-technical stakeholders to clearly articulate their insights on AI fairness. The authors emphasize the importance of critical reflection on appropriate use of training in co-creation ",
      "summary_section": "## Overview\n\nThis academic paper addresses a critical gap in AI fairness governance by examining how training components integrated into co-creation (Co-C) processes can enable meaningful participation of vulnerable and marginalized groups in AI system design. Grounded in practical experience develo",
      "source_tool": "Manual",
      "zotero_key": "FL9N6Q3E"
    },
    {
      "id": "Small_2023_Generative_AI_and_opportunities_for_feminist_class",
      "filename": "Small_2023_Generative_AI_and_opportunities_for_feminist_class.md",
      "title": "Generative AI and opportunities for feminist classroom assignments",
      "author_year": "Small (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://digitalcommons.calpoly.edu/feministpedagogy/vol3/iss5/10/",
      "decision": "Unclear",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 1,
      "rel_bias": 0,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 6,
      "relevance_category": "medium",
      "top_dimensions": [
        "AI Literacy",
        "Practical Implementation"
      ],
      "tags": [
        "paper",
        "unclear",
        "medium-relevance",
        "dim-ai-komp-medium",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Addresses integration of generative AI into feminist classroom assignments to develop reflexivity and feminist epistemology. Proposes intentional feminist approaches where students collaborate with AI to develop understanding of feminist knowledge production, transforming AI from educational threat into tool for critical learning about power and epistemology.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "K5RYQDIP"
    },
    {
      "id": "Smith_2021_When_Good_Algorithms_Go_Sexist__Why_and_How_to_Adv",
      "filename": "Smith_2021_When_Good_Algorithms_Go_Sexist__Why_and_How_to_Adv.md",
      "title": "When Good Algorithms Go Sexist: Why and How to Advance AI Gender Equity",
      "author_year": "Smith (2021)",
      "authors": [],
      "publication_year": 2021.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.48558/A179-B138",
      "url": "https://ssir.org/articles/entry/when_good_algorithms_go_sexist_why_and_how_to_advance_ai_gender_equity",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Seven actions social change leaders and machine learning developers can take to build gender-smart artificial intelligence for a more just world.",
      "summary_section": "## Overview\n\nThe \"Mitigating Bias in Artificial Intelligence: An Equity Fluent Leadership Playbook\" is a translational research document developed by UC Berkeley's Center for Equity, Gender and Leadership that systematically bridges academic AI ethics research and industry practice. Authored by Gene",
      "source_tool": "Manual",
      "zotero_key": "A484RHDC"
    },
    {
      "id": "Spaulding_2023_Predicting_successful_placements_for_youth_in_chil",
      "filename": "Spaulding_2023_Predicting_successful_placements_for_youth_in_chil.md",
      "title": "Predicting successful placements for youth in child welfare with machine learning",
      "author_year": "Spaulding (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1016/j.childyouth.2023.107229",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 3,
      "rel_prof": 3,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Practical Implementation",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-high",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Study developed machine learning models to predict treatment success for youth in various child welfare placement settings using data from 4,788 youth served by Children's Hope Alliance in North Carolina. Models aimed to distinguish which youth would succeed in high-cost residential psychiatric treatment versus community-based alternatives. Using Treatment Outcome Package (TOP) assessment data and gradient boosting algorithms, models achieved AUROCs >0.70 in predicting placement success. Address",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "WNXD959W"
    },
    {
      "id": "Sperling_2024_In_search_of_artificial_intelligence_(AI)_literacy",
      "filename": "Sperling_2024_In_search_of_artificial_intelligence_(AI)_literacy.md",
      "title": "In search of artificial intelligence (AI) literacy in teacher education: A scoping review",
      "author_year": "Sperling (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1016/j.caeo.2024.100169",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2666557324000107",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "AEQCMKBI"
    },
    {
      "id": "Srinivasan_2024_Worst_of_both_worlds__A_comparative_analysis_of_er",
      "filename": "Srinivasan_2024_Worst_of_both_worlds__A_comparative_analysis_of_er.md",
      "title": "Worst of both worlds: A comparative analysis of error in language and vision-language models",
      "author_year": "Srinivasan (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2405.20152v1",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "RNXP7XFT"
    },
    {
      "id": "Srinivasan_2025_Mitigating_trust-induced_inappropriate_reliance_on",
      "filename": "Srinivasan_2025_Mitigating_trust-induced_inappropriate_reliance_on.md",
      "title": "Mitigating trust-induced inappropriate reliance on AI assistance",
      "author_year": "Srinivasan (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/pdf/2502.13321.pdf",
      "decision": "Unclear",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 0,
      "rel_bias": 1,
      "rel_praxis": 3,
      "rel_prof": 1,
      "total_relevance": 6,
      "relevance_category": "medium",
      "top_dimensions": [
        "Practical Implementation",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "unclear",
        "medium-relevance",
        "dim-praxis-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Investigates trust-adaptive interventions to reduce inappropriate reliance on AI recommendations in decision-support tasks. Argues that AI assistants should adapt behavior based on user trust to mitigate both under- and over-trust. In two decision scenarios shows that providing supportive explanations for low trust and counter-explanations for high trust leads to 38% reduction in inappropriate reliance and 20% improvement in decision accuracy. Demonstrates that adaptive insertion of forced pause",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "ZM97HKVW"
    },
    {
      "id": "Srivastava_2024_Algorithmic_Governance_and_the_International_Polit",
      "filename": "Srivastava_2024_Algorithmic_Governance_and_the_International_Polit.md",
      "title": "Algorithmic Governance and the International Politics of Big Tech",
      "author_year": "Srivastava (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.cambridge.org/core/journals/perspectives-on-politics/article/algorithmic-governance-and-the-international-politics-of-big-tech/3C04908735A5F2EE8A70AFED647741FB",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Structural analysis examines how Big Tech corporations exercise \"entrepreneurial private authority\" through algorithmic governance systems challenging state sovereignty. Moves beyond individual-focused approaches to analyze how technology firms exercise instrumental, structural, and discursive power globally. Critiques approaches focusing on individual user agency or technical fixes, examining instead how \"Big Tech's algorithmic governance incentivizes 'information pollution'\" and creates system",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "88UIN782"
    },
    {
      "id": "Steiner_2022_Künstliche_Intelligenz_in_der_Sozialen_Arbeit__Gru",
      "filename": "Steiner_2022_Künstliche_Intelligenz_in_der_Sozialen_Arbeit__Gru.md",
      "title": "Künstliche Intelligenz in der Sozialen Arbeit: Grundlagen, Entwicklungen, Herausforderungen",
      "author_year": "Steiner (2022)",
      "authors": [],
      "publication_year": 2022.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1007/s12054-022-00546-4",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 3,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Bias Analysis",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Systematically analyzes two key AI application scenarios: Predictive Risk Modeling (PRM) and chatbots in counseling. Discusses neural networks' black box problem, dangers of case labeling through standardization, ethical questions of responsibility and liability when AI predictions diverge from professional judgment, and algorithmic bias risks perpetuating social inequalities. Uses Jonas' ethical theory of responsibility to emphasize ethical responsibility as foundational to all AI implementatio",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "9T5KGGH6"
    },
    {
      "id": "Steyvers_2025_What_large_language_models_know_and_what_people_th",
      "filename": "Steyvers_2025_What_large_language_models_know_and_what_people_th.md",
      "title": "What large language models know and what people think they know",
      "author_year": "Steyvers (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1038/s42256-024-00976-7",
      "url": "https://www.nature.com/articles/s42256-024-00976-7",
      "decision": "Unclear",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 0,
      "rel_bias": 1,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 6,
      "relevance_category": "medium",
      "top_dimensions": [
        "AI Literacy",
        "Practical Implementation"
      ],
      "tags": [
        "paper",
        "unclear",
        "medium-relevance",
        "dim-ai-komp-medium",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "High-impact study investigating mismatch between LLMs' confidence and users' trust. Found users often over-trust LLM answers, especially with explanations. Longer, detailed explanations increased user confidence even when answers weren't more accurate. Demonstrated that prompt engineering to convey uncertainty accurately helps users calibrate trust better. Aligning explanation with model's true confidence narrowed \"calibration gap,\" improving accuracy in judging when to trust AI.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "MEYUKKTT"
    },
    {
      "id": "Strauß_2024_CAIL_–_Critical_AI_Literacy__Kritische_Technikkomp",
      "filename": "Strauß_2024_CAIL_–_Critical_AI_Literacy__Kritische_Technikkomp.md",
      "title": "CAIL – Critical AI Literacy: Kritische Technikkompetenz für konstruktiven Umgang mit KI-basierter Technologie in Betrieben",
      "author_year": "Strauß (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://wien.arbeiterkammer.at/service/digifonds/gefoerderte-projekte/CAIL_Bericht-FINAL.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 3,
      "rel_vulnerable": 1,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "AI Literacy",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-high",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This study develops a Critical AI Literacy framework defined as critical competency for assessing practical utility and limitations of AI applications in specific contexts. It emphasizes that AI-based automation is more complex, dynamic, and volatile than classical automation forms, creating new challenges. A central finding is that critical AI competency becomes part of knowledge work, as interpretation and verification of AI results remains an essential human task. The project identifies deep ",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "ZLVBN3R2"
    },
    {
      "id": "Struppek_2024_Homoglyph_unlearning__A_novel_approach_to_bias_mit",
      "filename": "Struppek_2024_Homoglyph_unlearning__A_novel_approach_to_bias_mit.md",
      "title": "Homoglyph unlearning: A novel approach to bias mitigation",
      "author_year": "Struppek (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2406.05602v1",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "RQEYJP43"
    },
    {
      "id": "Studeny_2025_Digitale_Werkzeuge_und_Machtasymmetrien_",
      "filename": "Studeny_2025_Digitale_Werkzeuge_und_Machtasymmetrien_.md",
      "title": "Digitale Werkzeuge und Machtasymmetrien?",
      "author_year": "Studeny (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.ogsa.at/wp-content/uploads/2025/03/ogsaTAGUNG2025_AG-Digitalisierung_Handout-Machtasymmetrien-Susanne-Studeny.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 3,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Bias Analysis",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-prof-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Studeny_2025_Digitale.md",
      "abstract": "Studeny analyzes power asymmetries in digital social work, emphasizing that digital tools and algorithms create new, often invisible forms of power and control. AI decisions remove influence from both professionals and clients while responsibility remains unclear. Algorithms reinforce discrimination as they work with biased data. The author demands that social work critically reflects on digital technologies, demands transparency, and ensures that technology serves people.",
      "summary_section": "## Overview\n\nThis academic document presents a critical examination of digital tools within social work practice, specifically addressing how technology mediates power relationships between practitioners and clients. Prepared for the OGSA conference in Graz (March 2025), the work challenges the prev",
      "source_tool": "Manual",
      "zotero_key": "JIQLZ5E6"
    },
    {
      "id": "Sūna_2024_Diskriminierung_durch_Algorithmen_–_Überlegungen_z",
      "filename": "Sūna_2024_Diskriminierung_durch_Algorithmen_–_Überlegungen_z.md",
      "title": "Diskriminierung durch Algorithmen – Überlegungen zur Stärkung KI-bezogener Kompetenzen",
      "author_year": "Sūna (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "bookSection",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.gmk-net.de/wp-content/uploads/2024/12/gmk60_suna_hoffmann_mollen.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Bias Analysis",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-medium",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": true,
      "summary_file": "summary_S_na_2024_Diskriminierung.md",
      "abstract": "Konzeptioneller Beitrag zu Ursachen und Formen algorithmischer Diskriminierung und zur Förderung kritischer KI-Kompetenzen. Plädiert für Aufklärung zu Datenbias, reflexive Nutzung und partizipative Trainings, um Benachteiligungen zu erkennen und digitale Teilhabe zu stärken.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "WLY34JHH"
    },
    {
      "id": "Taeihagh_2025_Governance_of_generative_AI__A_comprehensive_frame",
      "filename": "Taeihagh_2025_Governance_of_generative_AI__A_comprehensive_frame.md",
      "title": "Governance of generative AI: A comprehensive framework for navigating challenges and opportunities",
      "author_year": "Taeihagh (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://academic.oup.com/policyandsociety/article/44/1/1/7997395",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Provides comprehensive overview of governance challenges posed by generative AI, including bias amplification, privacy violations, misinformation, and exacerbation of power imbalances. Critiques inadequacy of voluntary self-regulation and proposes comprehensive governance framework that is proactive, adaptive, and participatory. Recommends improving data governance, mandating independent audits, enhancing public engagement, and fostering international cooperation.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "QMUWKIKK"
    },
    {
      "id": "Takaoka_2022_AI_implementation_science_for_social_issues__Pitfa",
      "filename": "Takaoka_2022_AI_implementation_science_for_social_issues__Pitfa.md",
      "title": "AI implementation science for social issues: Pitfalls and tips",
      "author_year": "Takaoka (2022)",
      "authors": [],
      "publication_year": 2022.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.2188/jea.JE20210380",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 3,
      "rel_prof": 3,
      "total_relevance": 11,
      "relevance_category": "high",
      "top_dimensions": [
        "Practical Implementation",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-high",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Case study documenting four-stage social implementation of AI system (AiCAN - Assistant of Intelligence for Child Abuse and Neglect) in Japanese Child Guidance Centers from 2012-2020. System uses machine learning to predict child abuse recurrence and Bayesian networks for real-time probabilistic inference to guide temporary protection decisions. Data from over 6,000 cases (2014-2018) were used to develop gradient boosting algorithms with AUROC >0.70. Implementation involved iterative stakeholder",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "NDKXZHTX"
    },
    {
      "id": "Thwaites_2024_Operationalizing_positive-constructive_pedagogy_to",
      "filename": "Thwaites_2024_Operationalizing_positive-constructive_pedagogy_to.md",
      "title": "Operationalizing positive-constructive pedagogy to artificial intelligence: the 3E model for scaffolding AI technology adoption",
      "author_year": "Thwaites (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.3389/feduc.2024.1293235",
      "url": "https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2024.1293235/full",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 3,
      "rel_vulnerable": 1,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "AI Literacy",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-ai-komp-high",
        "dim-bias-medium",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This article proposes the \"3E model\" (Expose, Explore, Exploit) as a pedagogical framework for developing critical AI literacy. The model aims to move students beyond passive use of AI to a more critical engagement. The \"Expose\" phase involves revealing the underlying mechanisms and biases of AI systems. \"Explore\" encourages students to test AI boundaries and critically question its outputs, a practice akin to critical prompting. \"Exploit\" focuses on using AI for creative and novel purposes. Whi",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "CUBA5DBW"
    },
    {
      "id": "Tinmaz_2022_A_systematic_review_on_digital_literacy",
      "filename": "Tinmaz_2022_A_systematic_review_on_digital_literacy.md",
      "title": "A systematic review on digital literacy",
      "author_year": "Tinmaz (2022)",
      "authors": [],
      "publication_year": 2022.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1186/s40561-022-00204-y",
      "url": "https://slejournal.springeropen.com/articles/10.1186/s40561-022-00204-y",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Abstract The purpose of this study is to discover the main themes and categories of the research studies regarding digital literacy. To serve this purpose, the databases of WoS/Clarivate Analytics, Proquest Central, Emerald Management Journals, Jstor Business College Collections and Scopus/Elsevier were searched with four keyword-combinations and final forty-three articles were included in the dataset. The researchers applied a systematic literature review method to the dataset. The preliminary ",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "CIPACAP5"
    },
    {
      "id": "Tint_2025_Guardrails,_not_guidance__Understanding_responses_",
      "filename": "Tint_2025_Guardrails,_not_guidance__Understanding_responses_.md",
      "title": "Guardrails, not guidance: Understanding responses to LGBTQ+ language in large language models",
      "author_year": "Tint (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "https://aclanthology.org/2025.queerinai-main.2.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Tint_2025_Guardrails.md",
      "abstract": "Examines how large language models respond to prompts involving LGBTQ+ terminology and how current safety measures handle such content. Finds disparity where LLMs invoke safety guardrails for overtly heteronormative prompts but exhibit subtle biases when handling queer slang or informal LGBTQ+ language, responding with more negative emotional tone without triggering content filters.",
      "summary_section": "## Overview\n\nJoshua Tint's research examines how Large Language Models respond to LGBTQ+ linguistic expression, investigating implicit biases in emotional content of model outputs across six major systems (GPT-3.5, GPT-4o, Llama2, Llama3, Gemma, Mistral). The paper addresses a critical gap in fairne",
      "source_tool": "Manual",
      "zotero_key": "CWBWUWYL"
    },
    {
      "id": "Toupin_2024_Shaping_feminist_artificial_intelligence",
      "filename": "Toupin_2024_Shaping_feminist_artificial_intelligence.md",
      "title": "Shaping feminist artificial intelligence",
      "author_year": "Toupin (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1177/14614448221150776",
      "url": "https://journals.sagepub.com/doi/full/10.1177/14614448221150776",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "AI Literacy",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-ai-komp-medium",
        "dim-vulnerable-medium",
        "dim-bias-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Comprehensive examination of feminist artificial intelligence through historical analysis and contemporary typology development. Provides detailed framework categorizing FAI as: model, design, policy, culture, discourse, and science. Traces FAI's evolution from foundational work to contemporary initiatives, analyzing tensions between commercialized approaches and community-oriented methods.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "AMAZDU2T"
    },
    {
      "id": "Tun_2025_Trust_in_artificial_intelligence–based_clinical_de",
      "filename": "Tun_2025_Trust_in_artificial_intelligence–based_clinical_de.md",
      "title": "Trust in artificial intelligence–based clinical decision support systems among health care workers: Systematic review",
      "author_year": "Tun (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.2196/69678",
      "url": "https://www.jmir.org/2025/1/e69678",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 1,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 7,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Practical Implementation"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-bias-medium",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Tun_2025_Trust.md",
      "abstract": "Systematic review synthesizing 27 studies on clinicians' trust in AI decision-support tools. Identifies eight key factors shaping professional trust, notably system transparency as primary enabler. Lack of transparency was recurring barrier with \"black-box\" algorithms undermining trust. Improving transparency, providing explainable recommendations, and addressing bias/fairness issues recommended to bolster trust. Concludes transparent, explainable AI with proper training and ethical safeguards c",
      "summary_section": "## Overview\n\nThe PRISMA 2020 Checklist is a **standardized reporting guideline**—not empirical research—that establishes normative requirements for documenting systematic reviews and meta-analyses. Comprising 27 mandatory reporting items distributed across six sections (Title, Abstract, Introduction",
      "source_tool": "Manual",
      "zotero_key": "FFAQTTR8"
    },
    {
      "id": "U.S._Bureau_of_Labor_Statistics_2023_Occupational_employment_statistics",
      "filename": "U.S._Bureau_of_Labor_Statistics_2023_Occupational_employment_statistics.md",
      "title": "Occupational employment statistics",
      "author_year": "U.S. Bureau of Labor Statistics (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.bls.gov/oes/",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "4A2Q3Y4H"
    },
    {
      "id": "Ulnicane_2024_Artificial_Intelligence_and_Intersectionality",
      "filename": "Ulnicane_2024_Artificial_Intelligence_and_Intersectionality.md",
      "title": "Artificial Intelligence and Intersectionality",
      "author_year": "Ulnicane (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://ecpr.eu/news/news/details/749",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Diese Analyse untersucht, wie KI-Dokumente Bedenken über Bias und Ungleichheit in KI rahmen und Empfehlungen zur Bekämpfung formulieren. Mittels intersektionaler Linse wird die Interaktion multipler Identitäten (Geschlecht, Rasse, Klasse) hervorgehoben, die zu Marginalisierung und Diskriminierung bestimmter sozialer Gruppen führt. Die Studie unterscheidet zwischen technischen und sozio-technischen Framings von KI-Bias und zeigt auf, dass technische Frames KI oft als objektiv und neutral darstell",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "4RNTYLA5"
    },
    {
      "id": "Ulnicane_2024_Intersectionality_in_artificial_intelligence__Fram",
      "filename": "Ulnicane_2024_Intersectionality_in_artificial_intelligence__Fram.md",
      "title": "Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action",
      "author_year": "Ulnicane (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.17645/si.7543",
      "url": "https://www.cogitatiopress.com/socialinclusion/article/view/7543",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Diese Studie analysiert die aufkommende Agenda zu Intersektionalität in AI durch Untersuchung von vier hochrangigen Berichten zu diesem Thema (2019-2021). Die Forschung zeigt, wie diese Dokumente Probleme rahmen und Empfehlungen zur Adressierung von Ungleichheiten formulieren. AI-Systeme verstärken und verschärfen oft menschliche Verzerrungen und Stereotypen, was zu Diskriminierung und Marginalisierung führt. Die Analyse deckt systematische Probleme auf: Diversitätskrisen in AI-Entwicklung, wo G",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "JXKVY242"
    },
    {
      "id": "UNESCO_2020_ARTIFICIAL_INTELLIGENCE_and_GENDER_EQUALITY",
      "filename": "UNESCO_2020_ARTIFICIAL_INTELLIGENCE_and_GENDER_EQUALITY.md",
      "title": "ARTIFICIAL INTELLIGENCE and GENDER EQUALITY",
      "author_year": "UNESCO (2020)",
      "authors": [],
      "publication_year": 2020.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "unesdoc.unesco.org/in/rest/annotationSVC/DownloadWatermarkedAttachment/attach_import_ab07646d-c784-4a4e-96a1-3be7855b6f76?_=374174eng.pdf&to=54&from=1",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "The present report builds on UNESCO’s previous work on gender equality and AI and aims to continue the conversation on this topic with a select group of experts from key stakeholder groups. In March 2019, UNESCO published a groundbreaking report, I’d Blush if I Could: closing gender divides in digital skills through education, based on research funded by the German Federal Ministry for Economic Cooperation and Development. This report featured recommendations on actions to overcome global gender",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "ICV9Q7XF"
    },
    {
      "id": "UNESCO_2021_Recommendation_on_the_Ethics_of_Artificial_Intelli",
      "filename": "UNESCO_2021_Recommendation_on_the_Ethics_of_Artificial_Intelli.md",
      "title": "Recommendation on the Ethics of Artificial Intelligence",
      "author_year": "UNESCO (2021)",
      "authors": [],
      "publication_year": 2021.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://unesdoc.unesco.org/ark:/48223/pf0000380455",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "First global standard on AI ethics adopted by 193 member states, establishing comprehensive policy frameworks addressing gender equality in AI development and deployment. Explicitly addresses gender stereotyping, discriminatory biases, and need for equitable participation across the AI lifecycle, with recent implementation including Women4Ethical AI platform and systematic bias studies.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "Y7HUDGKK"
    },
    {
      "id": "UNESCO_2024_Bias_against_women_and_girls_in_large_language_mod",
      "filename": "UNESCO_2024_Bias_against_women_and_girls_in_large_language_mod.md",
      "title": "Bias against women and girls in large language models: A UNESCO study",
      "author_year": "UNESCO (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.unesco.org/en/articles/generative-ai-unesco-study-reveals-alarming-evidence-regressive-gender-stereotypes",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_UNESCO_2024_Bias.md",
      "abstract": "Die Studie analysiert LLMs (z. B. GPT-3.5, Llama 2) und dokumentiert signifikante Stereotypisierungen gegen Frauen und queere Menschen in generierten Texten. Besondere Auffälligkeit bei Open-Source-Modellen.",
      "summary_section": "## Overview\n\nUNESCO's March 2024 press release presents a significant institutional investigation into systemic gender bias, homophobic content, and racial stereotyping within large language models (LLMs), the foundational technology underlying generative AI applications. The study examines three ma",
      "source_tool": "Manual",
      "zotero_key": "MMF4WBLF"
    },
    {
      "id": "UNESCO_2024_Challenging_systematic_prejudices__an_Investigatio",
      "filename": "UNESCO_2024_Challenging_systematic_prejudices__an_Investigatio.md",
      "title": "Challenging systematic prejudices: an Investigation into Gender Bias in Large Language Models",
      "author_year": "UNESCO (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://discovery.ucl.ac.uk/id/eprint/10188772/1/Unesco_results.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 7,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_UNESCO_2024_Challenging.md",
      "abstract": "This study explores biases in three significant large language models (LLMs): OpenAI’s GPT-2 and ChatGPT, along with Meta’s Llama 2, highlighting their role in both advanced decision-making systems and as user-facing conversational agents. Across multiple studies, the brief reveals how biases emerge in the text generated by LLMs, through gendered word associations, positive or negative regard for gendered subjects, or diversity in text generated by gender and culture. The research uncovers persi",
      "summary_section": "## Overview\n\nThis UNESCO-IRCAI study, funded by the European Union's Horizon 2020 programme and published in 2024, systematically investigates gender bias within three major large language models: OpenAI's GPT-2 and ChatGPT, and Meta's Llama 2. The research directly addresses UNESCO's Ethics of AI R",
      "source_tool": "Manual",
      "zotero_key": "WD9KQG9J"
    },
    {
      "id": "UNESCO_2024_Women4Ethical_AI__Global_cooperation_for_gender-in",
      "filename": "UNESCO_2024_Women4Ethical_AI__Global_cooperation_for_gender-in.md",
      "title": "Women4Ethical AI: Global cooperation for gender-inclusive AI",
      "author_year": "UNESCO (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.unesco.org/en/articles/unescos-women4ethical-ai-urges-global-cooperation-gender-inclusive-ai",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-medium",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_UNESCO_2024_Bias.md",
      "abstract": "UNESCO-Initiative zur Förderung genderinklusiver KI-Entwicklung. Fokus auf globale Zusammenarbeit, Menschenrechtsprinzipien und Expertinnenbeteiligung in allen Phasen.",
      "summary_section": "![[summary_UNESCO_2024_Bias.md]]",
      "source_tool": "Manual",
      "zotero_key": "2JTAQTYV"
    },
    {
      "id": "Unknown_2024_AI_competency_framework_for_students",
      "filename": "Unknown_2024_AI_competency_framework_for_students.md",
      "title": "AI competency framework for students",
      "author_year": "Unknown (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "book",
      "language": "nan",
      "doi": "nan",
      "url": "https://unesdoc.unesco.org/ark:/48223/pf0000391105",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "UDU8GUD2"
    },
    {
      "id": "Unknown_2024_Research_on_the_application_risks_and_countermeasu",
      "filename": "Unknown_2024_Research_on_the_application_risks_and_countermeasu.md",
      "title": "Research on the application risks and countermeasures of ChatGPT generative artificial intelligence in social work",
      "author_year": "Unknown (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.23977/jaip.2024.070222",
      "url": "https://www.clausiuspress.com/article/12988.html",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "S2IMUC8M"
    },
    {
      "id": "Unknown_2024_RETHINKING_SOCIAL_SERVICES_WITH_ARTIFICIAL_INTELLI",
      "filename": "Unknown_2024_RETHINKING_SOCIAL_SERVICES_WITH_ARTIFICIAL_INTELLI.md",
      "title": "RETHINKING SOCIAL SERVICES WITH ARTIFICIAL INTELLIGENCE: OPPORTUNITIES, RISKS, AND FUTURE PERSPECTIVES",
      "author_year": "Unknown (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "bookSection",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.researchgate.net/publication/393509196_RETHINKING_SOCIAL_SERVICES_WITH_ARTIFICIAL_INTELLIGENCE_OPPORTUNITIES_RISKS_AND_FUTURE_PERSPECTIVES",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 1,
      "rel_prof": 3,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Professional Context",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This chapter explores the transformative potential of artificial intelligence (AI) in the field of social services. It highlights how AI—through data analysis, predictive modeling, and administrative automation—can enhance the effectiveness, accessibility, and efficiency of social work practice. The chapter also presents significant ethical concerns, including risks of algorithmic bias, loss of human connection, and violations of privacy. The author emphasizes that while AI can complement social",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "UXUSIRST"
    },
    {
      "id": "Unknown_2025_Artificial_Intelligence_in_Social_Sciences_and_Soc",
      "filename": "Unknown_2025_Artificial_Intelligence_in_Social_Sciences_and_Soc.md",
      "title": "Artificial Intelligence in Social Sciences and Social Work: Bridging Technology and Humanity to Revolutionize Research, Policy, and Human Services",
      "author_year": "Unknown (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.multispecialityjournal.com/uploads/archives/20250920153430_MCR-2025-5-004.1.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 1,
      "rel_prof": 2,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-prof-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Unknown_2025_Artificial.md",
      "abstract": "This review explores the transformative role of artificial intelligence (AI) in the fields of social sciences and social work, with a focus on developments from 2022 to 2025. It examines how AI technologies—such as machine learning, natural language processing—enhance the analysis of complex social phenomena, support real-time forecasting, and inform data-driven policymaking. Within social work and human services, AI-driven tools facilitate case management, mental health interventions, crisis re",
      "summary_section": "## Overview\n\nThis academic review examines the transformative integration of artificial intelligence technologies into social sciences and human services during 2022-2025. The document addresses a critical contemporary question: how can AI be ethically implemented across research methodologies, poli",
      "source_tool": "Manual",
      "zotero_key": "5ML7W6AU"
    },
    {
      "id": "Unknown_2025_Artificial_Intelligence_in_Social_Work__An_EPIC_Mo",
      "filename": "Unknown_2025_Artificial_Intelligence_in_Social_Work__An_EPIC_Mo.md",
      "title": "Artificial Intelligence in Social Work: An EPIC Model for Practice",
      "author_year": "Unknown (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1080/0312407X.2025.2488345",
      "url": "https://www.tandfonline.com/doi/full/10.1080/0312407X.2025.2488345",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 11,
      "relevance_category": "high",
      "top_dimensions": [
        "Professional Context",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-medium",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-praxis-medium",
        "dim-prof-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Unknown_2025_Artificial.md",
      "abstract": "As artificial intelligence (AI) permeates the workplace environments of social workers, there is a need to understand the risks and benefits posed to the mission and values of the profession. This article examines the influence of artificial intelligence on the profession, including opportunities to advance socially just outcomes and challenges that risk ethical practice. A comprehensive review of literature was conducted to examine existing research on the intersection of AI and social work. Dr",
      "summary_section": "## Overview\n\nThis academic review examines the transformative integration of artificial intelligence technologies into social sciences and human services during 2022-2025. The document addresses a critical contemporary question: how can AI be ethically implemented across research methodologies, poli",
      "source_tool": "Manual",
      "zotero_key": "TBLSIEUC"
    },
    {
      "id": "Unknown__feminist_AI___ACADEMY",
      "filename": "Unknown__feminist_AI___ACADEMY.md",
      "title": "feminist AI | ACADEMY",
      "author_year": "Unknown ()",
      "authors": [],
      "publication_year": "nan",
      "item_type": "webpage",
      "language": "en",
      "doi": "nan",
      "url": "https://www.feminist-ai.com/academy",
      "decision": "Exclude",
      "exclusion_reason": "Wrong publication type",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Unknown_XXXX_feminist.md",
      "abstract": "Get ready for transforming power! We enable organizations to create more equitable AI through education. We hold workshops, give training, and provide learning material to raise awareness, build knowledge, and ease your creation of equitable AI.",
      "summary_section": "![[summary_Unknown_2025_Artificial.md]]",
      "source_tool": "Manual",
      "zotero_key": "H2W89APG"
    },
    {
      "id": "UN_Women_2024_Artificial_Intelligence_and_gender_equality",
      "filename": "UN_Women_2024_Artificial_Intelligence_and_gender_equality.md",
      "title": "Artificial Intelligence and gender equality",
      "author_year": "UN Women (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.unwomen.org/en/articles/explainer/artificial-intelligence-and-gender-equality",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Women_2024_Artificial.md",
      "abstract": "Comprehensive policy brief series analyzing how AI systems perpetuate gender inequalities while highlighting pathways for more equitable development. Documents that 44% of AI systems show gender bias, with 25% exhibiting both gender and racial bias, and provides evidence-based policy recommendations for addressing systematic underrepresentation and discriminatory outcomes.",
      "summary_section": "## Overview\n\nThis peer-reviewed paper by researchers at Epoch AI (with affiliations at University of Aberdeen, MIT CSAIL, Centre for the Governance of AI, and University of Tübingen) investigates a fundamental constraint facing artificial intelligence development: the potential exhaustion of publicl",
      "source_tool": "Manual",
      "zotero_key": "A4REJRN2"
    },
    {
      "id": "van_Toorn_2024_Introduction_to_the_digital_welfare_state__Contest",
      "filename": "van_Toorn_2024_Introduction_to_the_digital_welfare_state__Contest.md",
      "title": "Introduction to the digital welfare state: Contestations, considerations and entanglements",
      "author_year": "van Toorn (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1177/14407833241260890",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 2,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-prof-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Special issue introduction providing critical sociological analysis of digital welfare state, examining how datafication and automation amplify existing trends of surveillance and control over marginalized populations. Authors argue that contrary to neutral efficiency narratives, digital welfare technologies are embedded in fiscal austerity politics and criminalization of poverty. Employs power relations and human agency frameworks to demonstrate how algorithmic systems increase scrutiny of welf",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "RRVQGP5P"
    },
    {
      "id": "Vethman_2025_Fairness_Beyond_the_Algorithmic_Frame__Actionable_",
      "filename": "Vethman_2025_Fairness_Beyond_the_Algorithmic_Frame__Actionable_.md",
      "title": "Fairness Beyond the Algorithmic Frame: Actionable Recommendations for an Intersectional Approach",
      "author_year": "Vethman (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "https://facctconference.org/static/docs/facct2025-206archivalpdfs/facct2025-final2348-acmpaginated.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Vethman_2025_Fairness.md",
      "abstract": "This study develops actionable recommendations for AI experts to implement intersectional approaches beyond purely technical solutions. The authors criticize the reduction of intersectionality to algorithmic bias measurements between subgroups and develop a framework that emphasizes interdisciplinary teams, community participation, and analysis of power structures in social context. Their approach includes six core recommendations: responsible AI expert role, multidisciplinary team building, ref",
      "summary_section": "## Overview\n\nThis 2025 FAccT conference paper addresses a critical gap in AI fairness research: the reduction of intersectionality—a theoretical framework from Black Feminist scholarship examining interconnected systems of oppression—to narrow algorithmic bias metrics. The authors argue that current",
      "source_tool": "Manual",
      "zotero_key": "Q2HILDPI"
    },
    {
      "id": "Victor_2023_Recommendations_for_social_work_researchers_and_jo",
      "filename": "Victor_2023_Recommendations_for_social_work_researchers_and_jo.md",
      "title": "Recommendations for social work researchers and journal editors on the use of generative AI and large language models",
      "author_year": "Victor (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1086/726021",
      "url": "https://doi.org/10.1086/726021",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 1,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 11,
      "relevance_category": "high",
      "top_dimensions": [
        "Bias Analysis",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-medium",
        "dim-bias-high",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Develops \"disruptive-disrupting\" framework for analyzing generative AI in social work research with concrete recommendations for researchers and journal editors. Emphasizes transparency and accountability requiring researchers to fully document AI use, disclose prompts, and take responsibility for AI-generated content. Argues that continuous education about AI construction, limitations, and ethical considerations is essential for developing appropriate trust in LLM systems.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "HUDLW3NU"
    },
    {
      "id": "Voutyrakou_2025_Algorithmic_Governance__Gender_Bias_in_AI-Generate",
      "filename": "Voutyrakou_2025_Algorithmic_Governance__Gender_Bias_in_AI-Generate.md",
      "title": "Algorithmic Governance: Gender Bias in AI-Generated Policymaking?",
      "author_year": "Voutyrakou (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1007/s44230-025-00109-2",
      "url": "https://doi.org/10.1007/s44230-025-00109-2",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Examines whether gender-specific needs are reflected in AI-generated policies, demonstrating through GPT-4 and Copilot experiments that AI tends to overlook female-specific needs unless explicitly prompted. Highlights androcentric biases, advocating intersectionally-informed prompting to surface hidden biases but recognizing the limits of individual prompt-based solutions in addressing structural AI biases.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "LI4QASVU"
    },
    {
      "id": "Waag_2023_Rationalisierung_durch_Digitalisierung_",
      "filename": "Waag_2023_Rationalisierung_durch_Digitalisierung_.md",
      "title": "Rationalisierung durch Digitalisierung?",
      "author_year": "Waag (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1007/s12592-023-00472-6",
      "url": "nan",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Contributes labor sociology and interaction sociology perspectives (particularly Luhmann's interaction theory) to digitalization analyses in social work. Examines potential advantages and disadvantages from multiple stakeholder perspectives (professionals, service users, organizations), revealing that fears and hopes regarding rationalization through digitalization are overly simplistic. Highlights irreducible complexity of professional helping relationships and fundamental limitations of applyi",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "9NHF43IT"
    },
    {
      "id": "Wadmann_2020_'Meaningless_work'__How_the_datafication_of_health",
      "filename": "Wadmann_2020_'Meaningless_work'__How_the_datafication_of_health.md",
      "title": "'Meaningless work': How the datafication of health reconfigures knowledge about work and erodes professional judgement",
      "author_year": "Wadmann (2020)",
      "authors": [],
      "publication_year": 2020.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1177/0950017020950021",
      "url": "nan",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Critical ethnographic study examining how datafication and digitalization in Denmark's healthcare sector erode professional judgment and create meaningless work through surveillance and control mechanisms. Authors challenge policy narratives that more data leads to better, evidence-based healthcare decisions, revealing instead how data-intensive practices create Kafkaesque idiocy reconfiguring perceptions of work and undermining goal orientation. Key critical insights demonstrate how dual ambiti",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "X4F73VQZ"
    },
    {
      "id": "Wajcman_2023_Feminism_Confronts_AI__The_Gender_Relations_of_Dig",
      "filename": "Wajcman_2023_Feminism_Confronts_AI__The_Gender_Relations_of_Dig.md",
      "title": "Feminism Confronts AI: The Gender Relations of Digitalisation",
      "author_year": "Wajcman (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "bookSection",
      "language": "nan",
      "doi": "nan",
      "url": "https://academic.oup.com/book/55103/chapter/423909956",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Wajcman and Young provide a feminist critique of AI, arguing that the technology is not neutral but deeply embedded in existing gendered power structures. They highlight the severe underrepresentation of women in AI development as a key source of bias, leading to the creation of systems that reflect and amplify a masculine worldview. The authors contend that simply adding more women to the field is insufficient. Instead, they call for a fundamental shift in the culture of technology production, ",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "JL438SYR"
    },
    {
      "id": "Walgenbach_2023_Intersektionalität",
      "filename": "Walgenbach_2023_Intersektionalität.md",
      "title": "Intersektionalität",
      "author_year": "Walgenbach (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "bookSection",
      "language": "de",
      "doi": "nan",
      "url": "https://wb-erwachsenenbildung.net/intersektionalitaet/",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": true,
      "summary_file": "summary_Ma_2023_Intersectional.md",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "YVXN8TKM"
    },
    {
      "id": "Wang_2023_Measuring_user_competence_in_using_artificial_inte",
      "filename": "Wang_2023_Measuring_user_competence_in_using_artificial_inte.md",
      "title": "Measuring user competence in using artificial intelligence: validity and reliability of artificial intelligence literacy scale",
      "author_year": "Wang (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1080/0144929X.2022.2072768",
      "url": "https://www.tandfonline.com/doi/full/10.1080/0144929X.2022.2072768",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "IHB58BIG"
    },
    {
      "id": "Wang_2024_Algorithmic_discrimination__examining_its_types_an",
      "filename": "Wang_2024_Algorithmic_discrimination__examining_its_types_an.md",
      "title": "Algorithmic discrimination: examining its types and regulatory measures with emphasis on US legal practices",
      "author_year": "Wang (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.3389/frai.2024.1320277",
      "url": "https://doi.org/10.3389/frai.2024.1320277",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Diese umfassende Systematik identifiziert fünf primäre Typen algorithmischer Diskriminierung: Bias durch algorithmische Agenten, diskriminierende Merkmalsselektion, Proxy-Diskriminierung, disparate Auswirkungen und gezielte Werbung. Die Analyse der US-Rechtslandschaft offenbart einen mehrstufigen Regulierungsansatz aus prinzipieller Regulierung, präventiven Kontrollen, konsequenter Haftung und Selbstregulierung. Zentral ist die Erkenntnis, dass unbeabsichtigte Diskriminierung durch scheinbar neu",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "K4XSNE3E"
    },
    {
      "id": "Wang_2024_A_survey_on_fairness_in_large_language_models",
      "filename": "Wang_2024_A_survey_on_fairness_in_large_language_models.md",
      "title": "A survey on fairness in large language models",
      "author_year": "Wang (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://file.mixpaper.cn/paper_store/2023/5ddea1cd-c031-433f-a09b-14e7754f7826.pdf",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "G5GRGK8Y"
    },
    {
      "id": "Wang_2024_Multilingual_Prompting_for_Improving_LLM_Generatio",
      "filename": "Wang_2024_Multilingual_Prompting_for_Improving_LLM_Generatio.md",
      "title": "Multilingual Prompting for Improving LLM Generation Diversity",
      "author_year": "Wang (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2505.15229v1",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This paper introduces multilingual and multicultural prompting as methods to enhance the demographic and cultural diversity of Large Language Model outputs. The authors demonstrate these approaches outperform established diversity methods across multiple LLM architectures. Results indicate that prompting in culturally and linguistically aligned languages reduces hallucinated outputs and supports more representative generation.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "PMHP5747"
    },
    {
      "id": "Wang_2025_Multilingual_Prompting_for_Improving_LLM_Generatio",
      "filename": "Wang_2025_Multilingual_Prompting_for_Improving_LLM_Generatio.md",
      "title": "Multilingual Prompting for Improving LLM Generation Diversity",
      "author_year": "Wang (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2505.15229v1",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This study introduces multilingual prompting as a strategy to enhance narrative diversity in LLM outputs. By using prompts with diverse languages and cultural cues, models produced outputs with improved demographic and opinion diversity. Compared to temperature-based and persona prompting, multilingual prompting was more effective and reduced cultural hallucinations.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "N8GSM8AD"
    },
    {
      "id": "Washington_2025_Fragile_Foundations__Hidden_Risks_of_Generative_AI",
      "filename": "Washington_2025_Fragile_Foundations__Hidden_Risks_of_Generative_AI.md",
      "title": "Fragile Foundations: Hidden Risks of Generative AI",
      "author_year": "Washington (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.11586/2025078",
      "url": "https://www.bertelsmann-stiftung.de/doi/10.11586/2025078",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Washington_2025_Fragile.md",
      "abstract": "Foundation models are the backbone of generative AI and thus central to applications such as ChatGPT, Gemini, or Copilot. However, their use comes with risks: from randomly compiled training data and opaque processes to profit-driven business models. The new report Fragile Foundations: Hidden Risks of Generative AI by the Bertelsmann Stiftung shows why mission-driven organizations in particular should critically question the foundations of AI. It highlights the systemic weaknesses of foundation ",
      "summary_section": "## Overview\n\n\"Fragile Foundations: Hidden Risks of Generative AI\" is a critical policy analysis by Dr. Anne L. Washington (Duke University), published by Bertelsmann Stiftung in September 2025. The document systematically examines structural vulnerabilities in foundation models—large-scale AI system",
      "source_tool": "Manual",
      "zotero_key": "BALHXHPD"
    },
    {
      "id": "Weber_2023_Messung_von_AI_Literacy_–_Empirische_Evidenz_und_I",
      "filename": "Weber_2023_Messung_von_AI_Literacy_–_Empirische_Evidenz_und_I.md",
      "title": "Messung von AI Literacy – Empirische Evidenz und Implikationen",
      "author_year": "Weber (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://aisel.aisnet.org/wi2023/3",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "PRSE2699"
    },
    {
      "id": "West_2023_Discriminating_Systems__Gender,_Race,_and_Power_in",
      "filename": "West_2023_Discriminating_Systems__Gender,_Race,_and_Power_in.md",
      "title": "Discriminating Systems: Gender, Race, and Power in AI",
      "author_year": "West (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://ainowinstitute.org/wp-content/uploads/2023/04/discriminatingsystems.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_West_2023_Discriminating.md",
      "abstract": "Diese einflussreiche Studie argumentiert, dass die Diversitätskrise im KI-Sektor und Bias in KI-Systemen zwei Manifestationen desselben Problems sind und gemeinsam angegangen werden müssen. Die Autoren zeigen, dass rein technische Ansätze zur Bias-Behebung unzureichend sind, da sie die systemischen Machtverhältnisse ignorieren, die sowohl Arbeitsplätze als auch Technologien formen. Das \"Pipeline-Problem\"-Narrativ wird als zu eng kritisiert, da es tieferliegende Probleme mit Arbeitsplatzkultur, M",
      "summary_section": "## Overview\n\n\"Discriminating Systems: Gender, Race, and Power in AI\" is a 2019 report by West, Whittaker, and Crawford from the AI Now Institute that investigates systemic discrimination within artificial intelligence development and deployment. The document establishes a critical causal relationshi",
      "source_tool": "Manual",
      "zotero_key": "SIXWV7D4"
    },
    {
      "id": "Wilson_2024_AI_tools_show_biases_in_ranking_job_applicants'_na",
      "filename": "Wilson_2024_AI_tools_show_biases_in_ranking_job_applicants'_na.md",
      "title": "AI tools show biases in ranking job applicants' names according to perceived race and gender",
      "author_year": "Wilson (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "nan",
      "url": "https://ojs.aaai.org/index.php/AIES/article/view/31748",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Large-scale empirical study using over 550 resumes and 3+ million comparisons reveals that intersectional patterns of bias in AI resume screening cannot be understood as additive combinations of single-axis discrimination. Discovered unique harm against Black men invisible when examining race or gender independently—Black male names were never preferred over white male names (0% selection rate). Demonstrates co-constitutive nature of multiple discrimination where intersection of Blackness and ma",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "ITG64UXK"
    },
    {
      "id": "Wilson_2024_Gender,_race,_and_intersectional_bias_in_AI_resume",
      "filename": "Wilson_2024_Gender,_race,_and_intersectional_bias_in_AI_resume.md",
      "title": "Gender, race, and intersectional bias in AI resume screening via language model retrieval",
      "author_year": "Wilson (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.1609/aies.v7i1.31748",
      "url": "https://doi.org/10.1609/aies.v7i1.31748",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-high",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Analyzes bias in large language models used for resume screening, examining over 550 job descriptions and 550 resumes across multiple demographic combinations. Reveals significant intersectional discrimination with Black male candidates facing most severe disadvantages, validating three key hypotheses of intersectionality theory through over 40,000 comparisons.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "X873LLEU"
    },
    {
      "id": "World_Economic_Forum_2024_AI_for_impact__The_PRISM_framework_for_responsible",
      "filename": "World_Economic_Forum_2024_AI_for_impact__The_PRISM_framework_for_responsible.md",
      "title": "AI for impact: The PRISM framework for responsible AI in social innovation",
      "author_year": "World Economic Forum (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.weforum.org/publications/ai-for-impact-the-prism-framework-for-responsible-ai-in-social-innovation/",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 2,
      "rel_bias": 1,
      "rel_praxis": 3,
      "rel_prof": 2,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Practical Implementation",
        "AI Literacy"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-medium",
        "dim-vulnerable-medium",
        "dim-praxis-high",
        "dim-prof-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Institutional report introducing PRISM framework specifically designed for social innovators, impact enterprises, and intermediaries working in social services sectors. Building on Presidio Framework of AI Governance Alliance, PRISM provides adoption pathways through which organizations can filter their impact mission, capabilities, and risks against AI technology use. Framework includes AI-enabled readiness assessment matrix enabling organizations to evaluate current practices and develop actio",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "YSJRFESD"
    },
    {
      "id": "Wudel_2025_What_is_Feminist_AI_",
      "filename": "Wudel_2025_What_is_Feminist_AI_.md",
      "title": "What is Feminist AI?",
      "author_year": "Wudel (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://library.fes.de/pdf-files/bueros/bruessel/21888-20250304.pdf",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 10,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": true,
      "summary_file": "summary_Wudel_2025_What.md",
      "abstract": "Defines Feminist AI (FAI) as framework using intersectional feminism to address bias and inequalities in AI systems. Emphasizes interdisciplinary collaboration, systemic power analysis, and iterative theory-practice loops. Embeds feminist values of equality, freedom, and justice to transform AI development. Includes practical applications like FemAI advocacy for feminist perspectives in EU AI Act and MIRA diagnostic platform aligning AI tools with social justice goals. Distinguishes FAI from tra",
      "summary_section": "## Overview\n\n\"What is Feminist AI?\" is a policy-oriented academic document published by the Friedrich-Ebert-Stiftung's Competence Centre on the Future of Work in January 2025. Authored by Alexandra Wudel and Anna Ehrenberg, the paper introduces Feminist Artificial Intelligence (FAI) as a comprehensi",
      "source_tool": "Manual",
      "zotero_key": "6F8C5RXW"
    },
    {
      "id": "Wu_2025_Bias_in_decision-making_for_AI's_ethical_dilemmas_",
      "filename": "Wu_2025_Bias_in_decision-making_for_AI's_ethical_dilemmas_.md",
      "title": "Bias in decision-making for AI's ethical dilemmas: A comparative study of ChatGPT and Claude",
      "author_year": "Wu (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2501.10484v2",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "MKQZ27QE"
    },
    {
      "id": "Xu_2023_Transparency_enhances_positive_perceptions_of_soci",
      "filename": "Xu_2023_Transparency_enhances_positive_perceptions_of_soci.md",
      "title": "Transparency enhances positive perceptions of social artificial intelligence",
      "author_year": "Xu (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1155/2023/5550418",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 1,
      "rel_bias": 1,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 7,
      "relevance_category": "medium",
      "top_dimensions": [
        "AI Literacy",
        "Practical Implementation"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-ai-komp-medium",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Experimental study with 914 participants testing how transparent explanations about chatbot workings affect user perceptions. Results show transparency modestly improved trust-related perceptions: users found chatbot \"less creepy,\" felt more affinity, and perceived it as more socially intelligent when transparent explanation provided. Transparency's impact on perceived intelligence stronger for users with low prior AI knowledge. Suggests prompt-engineering for greater transparency can foster use",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "FLY5W3P5"
    },
    {
      "id": "Yan_2024_Promises_and_challenges_of_generative_artificial_i",
      "filename": "Yan_2024_Promises_and_challenges_of_generative_artificial_i.md",
      "title": "Promises and challenges of generative artificial intelligence for human learning",
      "author_year": "Yan (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1038/s41562-024-02004-5",
      "url": "https://www.nature.com/articles/s41562-024-02004-5",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "E2SAEKPK"
    },
    {
      "id": "Yuan_2025_The_cultural_stereotype_and_cultural_bias_of_ChatG",
      "filename": "Yuan_2025_The_cultural_stereotype_and_cultural_bias_of_ChatG.md",
      "title": "The cultural stereotype and cultural bias of ChatGPT",
      "author_year": "Yuan (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1177/18344909251355673",
      "url": "https://www.researchgate.net/publication/393408216_The_cultural_stereotype_and_cultural_bias_of_ChatGPT",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 0,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "dim-praxis-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "This article examines cultural biases in ChatGPT-3.5 and GPT-4. Study 1 measures alignment with human cultural values. Study 2 finds clear cultural stereotypes in GPT-3.5 but fewer in GPT-4. Study 3 tests four diversity-sensitive prompts (emphasizing individuality, fairness, egalitarian futures, or multiculturalism). All four strategies eliminated cultural stereotypes in GPT-3.5's outputs. For GPT-4, bias mitigation was more nuanced, requiring task-specific prompts. This indicates that while var",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "KVL2SBWV"
    },
    {
      "id": "Yunusov_2024_MirrorStories__Reflecting_Diversity_through_Person",
      "filename": "Yunusov_2024_MirrorStories__Reflecting_Diversity_through_Person.md",
      "title": "MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models",
      "author_year": "Yunusov (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/html/2409.13935v1",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high",
        "has-summary"
      ],
      "has_summary": true,
      "summary_file": "summary_Yunusov_2024_MirrorStories.md",
      "abstract": "This empirical study introduces a corpus of 1,500 personalized short stories generated with LLMs, incorporating identity features like gender, ethnicity, and age. Human judges rated these stories higher in engagement, diversity, and personalness. Narrative personalization increased textual diversity without harming moral comprehension. However, biases persist, such as preferential engagement for certain identities. The paper illustrates both potential and limitations of diversity-sensitive promp",
      "summary_section": "## Overview\n\nThis seminal paper presents critical improvements to the Skip-gram model, a neural network architecture for learning distributed word representations from large-scale unstructured text. The authors address two fundamental challenges: optimizing computational efficiency and representatio",
      "source_tool": "Manual",
      "zotero_key": "JEMUIGSX"
    },
    {
      "id": "Yu_2025_Algorithmic-Assisted_Decision-Making_Tools_in_Chil",
      "filename": "Yu_2025_Algorithmic-Assisted_Decision-Making_Tools_in_Chil.md",
      "title": "Algorithmic-Assisted Decision-Making Tools in Child Welfare Practice: A Systematic Review",
      "author_year": "Yu (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1177/10497315251350933",
      "url": "https://doi.org/10.1177/10497315251350933",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 3,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 12,
      "relevance_category": "high",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-vulnerable-high",
        "dim-bias-high",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "PRISMA-guided review of algorithmic tools in child welfare. Finds potential for consistency and early risk identification but significant concerns about bias, transparency, practitioner training, and stakeholder inclusion. Recommends audits, participatory design, and ethical guidelines; highlights evidence gaps.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "GIHHDECQ"
    },
    {
      "id": "Zakharova_2024_Tensions_in_digital_welfare_states__Three_perspect",
      "filename": "Zakharova_2024_Tensions_in_digital_welfare_states__Three_perspect.md",
      "title": "Tensions in digital welfare states: Three perspectives on care and control",
      "author_year": "Zakharova (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1177/14407833241226800",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 2,
      "rel_praxis": 1,
      "rel_prof": 2,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Vulnerable Groups",
        "Bias Analysis"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-medium",
        "dim-prof-medium"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Examines tensions between care and control in digital welfare states, analyzing how welfare services increasingly rely on digital technologies and data systems. Develops three analytical perspectives: datafied care practices, algorithmic governance, and digitalized welfare encounters. Demonstrates how digitalization reshapes welfare provision by intensifying surveillance while potentially enabling new forms of care. Reveals fundamental contradictions where care logics and control logics coexist ",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "EDM8628L"
    },
    {
      "id": "Zannone_2023_Intersectional_Fairness__A_Fractal_Approach",
      "filename": "Zannone_2023_Intersectional_Fairness__A_Fractal_Approach.md",
      "title": "Intersectional Fairness: A Fractal Approach",
      "author_year": "Zannone (2023)",
      "authors": [],
      "publication_year": 2023.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "https://arxiv.org/abs/2302.12683",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Diese Studie rahmt intersektionale Fairness in einem geometrischen Setting und projiziert Daten auf einen Hyperkubus. Die Autoren beweisen mathematisch, dass Fairness \"nach oben\" propagiert - die Sicherstellung von Fairness für alle Untergruppen auf der niedrigsten intersektionalen Ebene führt notwendigerweise zu Fairness auf allen höheren Ebenen. Sie definieren eine Familie von Metriken zur Erfassung intersektionaler Verzerrung und schlagen vor, Fairness als \"fraktales\" Problem zu betrachten, b",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "2MY2F9CI"
    },
    {
      "id": "Zayed_2024_Scaling_implicit_bias_analysis_across_transformer-",
      "filename": "Zayed_2024_Scaling_implicit_bias_analysis_across_transformer-.md",
      "title": "Scaling implicit bias analysis across transformer-based language models through embedding association test and prompt engineering",
      "author_year": "Zayed (2024)",
      "authors": [],
      "publication_year": 2024.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "nan",
      "url": "https://www.mdpi.com/2076-3417/14/8/3483",
      "decision": "Exclude",
      "exclusion_reason": "No full text",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "nan",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "VBREN7SI"
    },
    {
      "id": "Zeng_2025_Governing_discriminatory_content_in_conversational",
      "filename": "Zeng_2025_Governing_discriminatory_content_in_conversational.md",
      "title": "Governing discriminatory content in conversational AI: A cross-system, cross-lingual, and cross-topic audit",
      "author_year": "Zeng (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "nan",
      "doi": "10.1080/1369118X.2025.2537803",
      "url": "https://doi.org/10.1080/1369118X.2025.2537803",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 1,
      "rel_vulnerable": 2,
      "rel_bias": 3,
      "rel_praxis": 1,
      "rel_prof": 1,
      "total_relevance": 8,
      "relevance_category": "medium",
      "top_dimensions": [
        "Bias Analysis",
        "Vulnerable Groups"
      ],
      "tags": [
        "paper",
        "include",
        "medium-relevance",
        "dim-vulnerable-medium",
        "dim-bias-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Conducts mixed-method audit of how major conversational AI systems respond to and regulate discriminatory content. Analysis is cross-system, cross-lingual, and cross-topic, revealing that refusal sensitivity and answering strategies vary significantly across all three axes. Discusses value alignment process through reinforcement learning with human feedback and implementation of guardrails, highlighting tensions when tech platforms become arbiters of morality.",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "FDMJU7HF"
    },
    {
      "id": "Zhang_2025_Learning_About_AI__A_Systematic_Review_of_Reviews_",
      "filename": "Zhang_2025_Learning_About_AI__A_Systematic_Review_of_Reviews_.md",
      "title": "Learning About AI: A Systematic Review of Reviews on AI Literacy",
      "author_year": "Zhang (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "journalArticle",
      "language": "en",
      "doi": "10.1177/07356331251342081",
      "url": "https://journals.sagepub.com/doi/10.1177/07356331251342081",
      "decision": "Exclude",
      "exclusion_reason": "Not relevant topic",
      "rel_ai_komp": 0,
      "rel_vulnerable": 0,
      "rel_bias": 0,
      "rel_praxis": 0,
      "rel_prof": 0,
      "total_relevance": 0,
      "relevance_category": "low",
      "top_dimensions": [],
      "tags": [
        "paper",
        "exclude",
        "low-relevance"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Given the ubiquity of artificial intelligence (AI), it is essential to empower students to become creators, designers, and producers of AI technologies, rather than limiting them to the role of informed consumers. To achieve this, learners need to be equipped with AI knowledge and concepts and develop AI literacy. Paradoxically, it is largely unclear what AI literacy is and how we should learn and teach it. We address both of these questions through a systematic review of systematic reviews, als",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "2PCEEUM8"
    },
    {
      "id": "Zhao_2025_Thinking_like_a_scientist__Can_interactive_simulat",
      "filename": "Zhao_2025_Thinking_like_a_scientist__Can_interactive_simulat.md",
      "title": "Thinking like a scientist: Can interactive simulations foster critical AI literacy?",
      "author_year": "Zhao (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "conferencePaper",
      "language": "nan",
      "doi": "10.1007/978-3-031-98417-4_5",
      "url": "nan",
      "decision": "Unclear",
      "exclusion_reason": "nan",
      "rel_ai_komp": 3,
      "rel_vulnerable": 0,
      "rel_bias": 2,
      "rel_praxis": 3,
      "rel_prof": 1,
      "total_relevance": 9,
      "relevance_category": "medium",
      "top_dimensions": [
        "AI Literacy",
        "Practical Implementation"
      ],
      "tags": [
        "paper",
        "unclear",
        "medium-relevance",
        "dim-ai-komp-high",
        "dim-bias-medium",
        "dim-praxis-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Empirical study with 605 participants demonstrates that interactive simulations enhance critical AI literacy by engaging learners in scientific thinking processes including hypothesis testing and direct observation of AI behavior. Reveals that critical AI literacy requires understanding of fairness, dataset representativeness, and bias mechanisms in language models beyond technical knowledge. Establishes that effective AI literacy education must move beyond static instruction toward experiential",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "EM3RD6NJ"
    },
    {
      "id": "[Author_not_specified]_2025_Navigating_the_Nexus_of_Trust__Prompt_Engineering,",
      "filename": "[Author_not_specified]_2025_Navigating_the_Nexus_of_Trust__Prompt_Engineering,.md",
      "title": "Navigating the Nexus of Trust: Prompt Engineering, Professional Judgment, and the Integration of Large Language Models in Social Work",
      "author_year": "[Author not specified] (2025)",
      "authors": [],
      "publication_year": 2025.0,
      "item_type": "report",
      "language": "nan",
      "doi": "nan",
      "url": "nan",
      "decision": "Include",
      "exclusion_reason": "nan",
      "rel_ai_komp": 2,
      "rel_vulnerable": 1,
      "rel_bias": 3,
      "rel_praxis": 2,
      "rel_prof": 3,
      "total_relevance": 11,
      "relevance_category": "high",
      "top_dimensions": [
        "Bias Analysis",
        "Professional Context"
      ],
      "tags": [
        "paper",
        "include",
        "high-relevance",
        "dim-ai-komp-medium",
        "dim-bias-high",
        "dim-praxis-medium",
        "dim-prof-high"
      ],
      "has_summary": false,
      "summary_file": "",
      "abstract": "Comprehensive analysis examining how prompt engineering strategies designed to enhance transparency and mitigate bias influence trust that social work professionals place in LLM-generated case recommendations. Synthesizes literature from computer science, social work, ethics, and psychology to construct understanding of complex interplay between technology, human psychology, and professional practice. Develops framework for calibrated trust through responsible prompt engineering, positioning pro",
      "summary_section": "*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*",
      "source_tool": "Manual",
      "zotero_key": "UV2XJZZQ"
    }
  ],
  "statistics": {
    "total_papers": 264,
    "by_decision": {
      "Include": 169,
      "Exclude": 81,
      "Unclear": 14
    },
    "by_relevance": {
      "high": 74,
      "medium": 107,
      "low": 83
    },
    "by_dimension": {
      "ai_komp": {
        "high": 14,
        "medium": 32,
        "low": 118,
        "none": 100
      },
      "vulnerable": {
        "high": 75,
        "medium": 76,
        "low": 25,
        "none": 88
      },
      "bias": {
        "high": 100,
        "medium": 57,
        "low": 21,
        "none": 86
      },
      "praxis": {
        "high": 17,
        "medium": 92,
        "low": 74,
        "none": 81
      },
      "prof": {
        "high": 48,
        "medium": 18,
        "low": 115,
        "none": 83
      }
    },
    "with_summaries": 67,
    "average_relevance": 6.31
  },
  "generated": "2025-11-10",
  "total_papers": 264
}