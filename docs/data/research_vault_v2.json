{
  "meta": {
    "generated": "2026-02-18",
    "total_papers": 326,
    "papers_with_human": 292,
    "benchmark_papers": 199,
    "kappa_overall": 0.035,
    "kappa_interpretation": "slight",
    "llm_include_rate": 71.2,
    "llm_include_count": 232,
    "llm_exclude_count": 94,
    "human_include_rate": 43.2,
    "human_include_count": 82,
    "human_total_with_decision": 190,
    "disagreement_count": 102,
    "confusion_matrix": {
      "Include_Include": 63,
      "Include_Exclude": 19,
      "Exclude_Include": 145,
      "Exclude_Exclude": 65,
      "Other": 0
    }
  },
  "kappa_by_category": {
    "AI_Literacies": {
      "kappa": -0.0178,
      "agreement_pct": 54.5,
      "n": 165,
      "human_yes_rate": 32.7,
      "agent_yes_rate": 34.5
    },
    "Generative_KI": {
      "kappa": -0.0041,
      "agreement_pct": 49.4,
      "n": 164,
      "human_yes_rate": 53.7,
      "agent_yes_rate": 52.4
    },
    "Prompting": {
      "kappa": -0.0662,
      "agreement_pct": 52.4,
      "n": 164,
      "human_yes_rate": 34.8,
      "agent_yes_rate": 32.3
    },
    "KI_Sonstige": {
      "kappa": 0.0478,
      "agreement_pct": 51.5,
      "n": 163,
      "human_yes_rate": 66.3,
      "agent_yes_rate": 47.2
    },
    "Soziale_Arbeit": {
      "kappa": -0.0834,
      "agreement_pct": 68.9,
      "n": 164,
      "human_yes_rate": 24.4,
      "agent_yes_rate": 7.3
    },
    "Bias_Ungleichheit": {
      "kappa": -0.0969,
      "agreement_pct": 62.6,
      "n": 163,
      "human_yes_rate": 79.8,
      "agent_yes_rate": 76.7
    },
    "Gender": {
      "kappa": -0.098,
      "agreement_pct": 41.1,
      "n": 163,
      "human_yes_rate": 63.2,
      "agent_yes_rate": 36.2
    },
    "Diversitaet": {
      "kappa": 0.0242,
      "agreement_pct": 50.6,
      "n": 162,
      "human_yes_rate": 62.4,
      "agent_yes_rate": 47.5
    },
    "Feministisch": {
      "kappa": 0.0745,
      "agreement_pct": 64.2,
      "n": 162,
      "human_yes_rate": 22.2,
      "agent_yes_rate": 29.6
    },
    "Fairness": {
      "kappa": -0.1627,
      "agreement_pct": 43.2,
      "n": 162,
      "human_yes_rate": 52.5,
      "agent_yes_rate": 73.5
    }
  },
  "papers": [
    {
      "id": "BHXDU7VM",
      "title": "How People Use ChatGPT",
      "author_year": "Chatterji et al. (2025)",
      "authors": "Chatterji, Aaron; Cunningham, Thomas; Deming, David J.; Hitzig, Zoe; Ong, Christopher; Shan, Carl Yan; Wadman, Kevin",
      "year": 2025,
      "doi": "",
      "url": "https://www.nber.org/papers/w34255",
      "abstract": "Founded in 1920, the NBER is a private, non-profit, non-partisan organization dedicated to conducting economic research and to disseminating research findings among academics, public policy makers, and business professionals.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.3,
        "categories": [
          "AI_Literacies",
          "Generative_KI"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Nur Titel und Abstract vorhanden. Der Titel deutet auf Nutzungsforschung zu ChatGPT hin (Generative_KI, potentiell AI_Literacies). Jedoch fehlt der volle Text zur substanziellen Bewertung. Das Abstract ist nur eine NBER-Standardbeschreibung ohne inhaltliche Details. Keine erkennbaren sozialen Aspekt"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "I78CL6R5",
      "title": "What are artificial intelligence literacy and competency? A comprehensive framework to support them",
      "author_year": "Chiu et al. (2024)",
      "authors": "Chiu, Thomas K.F.; Ahmad, Zubair; Ismailov, Murod; Sanusi, Ismaila Temitayo",
      "year": 2024,
      "doi": "10.1016/j.caeo.2024.100171",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2666557324000120",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.7,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt AI Literacy und Kompetenzentwicklung im KI-Bereich (AI_Literacies = Ja), erfüllt aber die zweite Bedingung nicht: Es fehlt ein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität, Feminismus oder Fairness. Das Paper ist relevant für AI Literacy generell, nich"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "VP6SXQHY",
      "title": "The AI literacy development canvas: Assessing and building AI literacy in organizations",
      "author_year": "Benlian & Pinski (2025)",
      "authors": "Benlian, Alexander; Pinski, Marc",
      "year": 2025,
      "doi": "10.1016/j.bushor.2025.10.001",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S0007681325001673",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Titel deutet auf AI Literacy Framework hin - klare AI_Literacies-Kategorie. Kein Abstract vorhanden; basierend auf Titel keine Indikation für soziale Dimensionen (Bias, Ungleichheit, Gender, Fairness, Soziale Arbeit). Paper erfüllt nur Technik-Kriterium, nicht Sozial-Kriterium. EXCLUDE nach strikter"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "AXEIVEW3",
      "title": "Artificial Intelligence Competence Needs for Youth Workers",
      "author_year": "Lanzetta et al. (2024)",
      "authors": "Lanzetta, Miriam; Abbruzzese, Gianluca; ACOMI, OVIDIU; Acomi, Nicoleta; Machado, Jorge; Maravelaki, Sonia Anastasia",
      "year": 2024,
      "doi": "10.5281/ZENODO.11525357",
      "url": "https://zenodo.org/doi/10.5281/zenodo.11525357",
      "abstract": "The rapid developments in AI technology and the rise of accessible AI-powered tools are transforming the way we live, work and learn. While young people have already warmly embraced these solutions, with Gen Z being the most active users and experimenters of Generative AI (Microsoft, 2024), there is a sense of confusion and fear among youth workers about the future of how AI tools are going to be used in the youth sector, mixed with diverse emotions and viewpoints ranging from apprehension, scep",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper adressiert zentral AI-Kompetenzen von Fachkräften (AI_Literacies: Ja - Kompetenzrahmen für Jugendhilfe), behandelt Generative AI (Ja - explizit Gen Z und generative Tools), hat direkten Bezug zu Sozialarbeit/Jugendhilfe (Ja - Youth Workers, Jugendhilfesektor). Empirische Studie mit Fokusgruppe"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "UFE85SCV",
      "title": "AI literacy in K-12: a systematic literature review",
      "author_year": "Casal-Otero et al. (2023)",
      "authors": "Casal-Otero, Lorena; Catala, Alejandro; Fernández-Morante, Carmen; Taboada, Maria; Cebreiro, Beatriz; Barro, Senén",
      "year": 2023,
      "doi": "10.1186/s40594-023-00418-7",
      "url": "https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00418-7",
      "abstract": "Abstract\n            The successful irruption of AI-based technology in our daily lives has led to a growing educational, social, and political interest in training citizens in AI. Education systems now need to train students at the K-12 level to live in a society where they must interact with AI. Thus, AI literacy is a pedagogical and cognitive challenge at the K-12 level. This study aimed to understand how AI is being integrated into K-12 education worldwide. We conducted a search process foll",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper ist eine systematische Literaturreviw zu AI Literacy im K-12 Bildungskontext und behandelt substantiell AI-Kompetenzen, Curricula und Lernansätze (AI_Literacies = Ja). Es erfüllt jedoch KEINE sozialen Kriterien (Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "J9IZDVTW",
      "title": "Less knowledge, more trust? Exploring potentially uncritical attitudes towards AI in higher education",
      "author_year": "Biagini et al. (2024)",
      "authors": "Biagini, Gabriele; Cuomo, Stefano; Ranieri, Maria",
      "year": 2024,
      "doi": "10.17471/2499-4324/1337",
      "url": "https://doi.org/10.17471/2499-4324/1337",
      "abstract": "L'intelligenza artificiale (IA) ha il potenziale per trasformare vari aspetti delle nostre vite, ma il suo sviluppo è stato accompagnato da numerose preoccupazioni sociali ed etiche. Per comprendere le implicazioni e i meccanismi sottostanti, è essenziale acquisire una comprensione ampia dei suoi benefici e svantaggi. A questo scopo, l'alfabetizzazione all'IA è un fattore fondamentale per promuovere atteggiamenti più consapevoli verso lo sviluppo dell'IA e delle sue implicazioni. Tuttavia, la ri",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper untersucht AI Literacy bei Doktoranden auf vier Dimensionen (kognitiv, operativ, kritisch, ethisch). Dies erfüllt AI_Literacies-Kriterium substantiell. Jedoch adressiert das Paper keinen direkten Bezug zu Sozialer Arbeit, behandelt Bias/Ungleichheit nicht fokussiert und hat keinen Gender-, Div"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "P82X89Q4",
      "title": "A Competency Framework for AI Literacy: Variations by Different Learner Groups and an Implied Learning Pathway",
      "author_year": "Chee et al. (2025)",
      "authors": "Chee, Hyunkyung; Ahn, Solmoe; Lee, Jihyun",
      "year": 2025,
      "doi": "10.1111/bjet.13556",
      "url": "https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13556",
      "abstract": "This study aims to develop a comprehensive competency framework for artificial intelligence (AI) literacy, delineating essential competencies and sub‐competencies. This framework and its potential variations, tailored to different learner groups (by educational level and discipline), can serve as a crucial reference for designing and implementing AI curricula. However, the research on AI literacy by target learners is still in its infancy, and the findings of several existing studies provide inc",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper entwickelt ein umfassendes AI-Literacy-Kompetenzframework und erfüllt die TECHNIK-Bedingung substanziell (AI_Literacies=Ja). Allerdings fehlt jeglicher SOZIAL-Bezug: Es gibt keinen Fokus auf Soziale Arbeit, Bias/Ungleichheit, Gender, Diversität, feministische Perspektiven oder Fairness. Da"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "KPSF8EZE",
      "title": "AI literacy and competency: definitions, frameworks, development and future research directions",
      "author_year": "Chiu (2025)",
      "authors": "Chiu, Thomas K. F.",
      "year": 2025,
      "doi": "10.1080/10494820.2025.2514372",
      "url": "https://www.tandfonline.com/doi/full/10.1080/10494820.2025.2514372",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt substantiell AI Literacy und Kompetenzen (AI_Literacies = Ja). Allerdings fehlt jeglicher Bezug zu sozialen Aspekten wie Soziale Arbeit, Bias, Gender, Diversität, Fairness oder feministische Perspektiven. TECHNIK-Kriterium erfüllt, aber SOZIAL-Kriterium nicht. Daher: Exclude."
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "XCS4YCQH",
      "title": "DigComp 2.1: the digital competence framework for citizens with eight proficiency levels and examples of use.",
      "author_year": "European Commission. Joint Research Centre. (2017)",
      "authors": "",
      "year": 2017,
      "doi": "",
      "url": "https://data.europa.eu/doi/10.2760/38842",
      "abstract": "",
      "item_type": "book",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "DigComp 2.1 ist ein Rahmenwerk für digitale Kompetenzen im Allgemeinen, nicht spezifisch für KI/ML-Systeme. Es behandelt digitale Literalität umfassend, aber nicht AI Literacy im Sinne der Definition (KI-spezifische Kompetenzen, technisches KI-Verständnis). Keine Bezüge zu sozialen Arbeit, Bias, Fai"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "SY8LNID7",
      "title": "Digital literacy as a new determinant of health: A scoping review",
      "author_year": "Arias López et al. (2023)",
      "authors": "Arias López, Maria Del Pilar; Ong, Bradley A.; Borrat Frigola, Xavier; Fernández, Ariel L.; Hicklent, Rachel S.; Obeles, Arianne J. T.; Rocimo, Aubrey M.; Celi, Leo A.",
      "year": 2023,
      "doi": "10.1371/journal.pdig.0000279",
      "url": "https://dx.plos.org/10.1371/journal.pdig.0000279",
      "abstract": "Introduction\n              Harnessing new digital technologies can improve access to health care but can also widen the health divide for those with poor digital literacy. This scoping review aims to assess the current situation of low digital health literacy in terms of its definition, reach, impact on health and interventions for its mitigation.\n            \n            \n              Methods\n              A comprehensive literature search strategy was composed by a qualified medical librarian",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper ist ein Scoping Review zu Digital Health Literacy im Gesundheitssektor. Obwohl es marginale Überschneidungen mit AI_Literacies und Bias/Ungleichheit hat, fehlt jede substanzielle Verknüpfung zu KI-Technologien oder sozialer Arbeit. Es behandelt digitale Gesundheitskompetenz allgemein, nich"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "43CRXRRT",
      "title": "A systematic review on digital literacy",
      "author_year": "Tinmaz et al. (2022)",
      "authors": "Tinmaz, Hasan; Lee, Yoo-Taek; Fanea-Ivanovici, Mina; Baber, Hasnan",
      "year": 2022,
      "doi": "10.1186/s40561-022-00204-y",
      "url": "https://slejournal.springeropen.com/articles/10.1186/s40561-022-00204-y",
      "abstract": "Abstract\n            The purpose of this study is to discover the main themes and categories of the research studies regarding digital literacy. To serve this purpose, the databases of WoS/Clarivate Analytics, Proquest Central, Emerald Management Journals, Jstor Business College Collections and Scopus/Elsevier were searched with four keyword-combinations and final forty-three articles were included in the dataset. The researchers applied a systematic literature review method to the dataset. The ",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt digitale Literalität allgemein, NICHT AI Literacy im Sinne von Kompetenzen im Umgang mit KI-Systemen. Der Fokus liegt auf digital competencies, digital skills und digital thinking ohne spezifischen Bezug zu KI, generativen Modellen oder algorithmischen Systemen. Weder Technologie"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "HTXCWNQ9",
      "title": "A Literature Review of Digital Literacy over Two Decades",
      "author_year": "Peng et al. (2022)",
      "authors": "Peng, Danhua; Yu, Zhonggen",
      "year": 2022,
      "doi": "10.1155/2022/2533413",
      "url": "https://www.hindawi.com/journals/edri/2022/2533413/",
      "abstract": "The COVID-19 pandemic has forced online learning to be a “new normal” during the past three years, which highly emphasizes students’ improved digital literacy. This study aims to present a literature review of students’ digital literacy. Grounded on about twenty journal articles and other related publications from the Web of Science Core Collection, this paper focused on the definition of digital literacy; the factors affecting students’ digital literacy (age, gender, family socioeconomic status",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "Gender"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt digitale Literalität von Studierenden im Allgemeinen, nicht KI-spezifische Kompetenzen (AI Literacy). Gender wird erwähnt als Einflussfaktor, aber substantiell nicht fokussiert. Keine KI/ML-Technologie-Komponente. Kein Bezug zu Sozialer Arbeit. TECHNIK-Bedingung nicht erfüllt → E"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "7G78SADI",
      "title": "AI literacy for users – A comprehensive review and future research directions of learning methods, components, and effects",
      "author_year": "Pinski & Benlian (2024)",
      "authors": "Pinski, Marc; Benlian, Alexander",
      "year": 2024,
      "doi": "10.1016/j.chbah.2024.100062",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2949882124000227",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper fokussiert auf AI Literacy als Kompetenzbreich – ein eindeutiges TECHNIK-Thema (AI_Literacies: Ja). Der Titel und Fokus deuten auf eine Übersichtsarbeit zu Lernmethoden und Effekten hin, ohne soziale/kritische Dimensionen. Ohne Abstract und Hinweise auf Soziale Arbeit oder kritische Bias-/"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "3AQ2K3BX",
      "title": "Conceptualizing AI literacy: An exploratory review",
      "author_year": "Ng et al. (2021)",
      "authors": "Ng, Davy Tsz Kit; Leung, Jac Ka Lok; Chu, Samuel Kai Wah; Qiao, Maggie Shen",
      "year": 2021,
      "doi": "10.1016/j.caeai.2021.100041",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000357",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.75,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt AI Literacy conceptually durch eine exploratorische Literaturübersicht. Dies erfüllt die TECHNIK-Bedingung (AI_Literacies = Ja). Jedoch keine SOZIAL-Kategorie wird erfüllt. Ohne Abstract und nur Titel ist unklar, ob das Paper eine Sozialarbeitsperspektive, Bias-Diskussion, Divers"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "7VFNS5R3",
      "title": "Evaluation of an artificial intelligence literacy course for university students with diverse study backgrounds",
      "author_year": "Kong et al. (2021)",
      "authors": "Kong, Siu-Cheung; Man-Yin Cheung, William; Zhang, Guo",
      "year": 2021,
      "doi": "10.1016/j.caeai.2021.100026",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000205",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt AI Literacy als Bildungsintervention für Studierende mit unterschiedlichen Hintergründen. Während die AI_Literacies-Kategorie klar erfüllt ist (Evaluation eines Kompetenz-Kursus), fehlt jeder Bezug zu sozialen Aspekten wie Bias, Ungleichheit, Gender, Diversität, Fairness oder Soz"
      },
      "human": {
        "decision": "Unclear",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "23Y3627L",
      "title": "Measuring user competence in using artificial intelligence: validity and reliability of artificial intelligence literacy scale",
      "author_year": "Wang et al. (2023)",
      "authors": "Wang, Bingcheng; Rau, Pei-Luen Patrick; Yuan, Tianyi",
      "year": 2023,
      "doi": "10.1080/0144929X.2022.2072768",
      "url": "https://www.tandfonline.com/doi/full/10.1080/0144929X.2022.2072768",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt AI Literacy durch die Entwicklung und Validierung einer Messskala. Obwohl AI_Literacies erfüllt ist (Kompetenzen im Umgang mit KI), fehlt jeglicher Bezug zu sozialen Dimensionen (Soziale_Arbeit, Bias, Gender, Diversität, Fairness). Die Entscheidungslogik erfordert BEIDE Technik U"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "4BRSDIPP",
      "title": "Development of the “Scale for the assessment of non-experts’ AI literacy” – An exploratory factor analysis",
      "author_year": "Laupichler et al. (2023)",
      "authors": "Laupichler, Matthias Carl; Aster, Alexandra; Haverkamp, Nicolas; Raupach, Tobias",
      "year": 2023,
      "doi": "10.1016/j.chbr.2023.100338",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2451958823000714",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Der Titel deutet auf ein Instrument zur Messung von AI Literacy hin – ein TECHNIK-Thema. Jedoch fehlt jede Evidenz für soziale Dimensionen wie Anwendung in der Sozialen Arbeit oder Fokus auf Bias/Fairness. Das Paper erfüllt nur eine Bedingung der strikten Entscheidungslogik (TECHNIK_OK, aber nicht S"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "YDW5TX8M",
      "title": "Imagination, Algorithms and News: Developing AI Literacy for Journalism",
      "author_year": "Deuze & Beckett (2022)",
      "authors": "Deuze, Mark; Beckett, Charlie",
      "year": 2022,
      "doi": "10.1080/21670811.2022.2119152",
      "url": "https://www.tandfonline.com/doi/full/10.1080/21670811.2022.2119152",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.75,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper adressiert AI Literacy (TECHNIK erfüllt), aber der Fokus liegt auf Journalismus, nicht auf Sozialer Arbeit oder damit verbundenen sozialen Dimensionen wie Bias, Fairness oder Diversität. Keine der SOZIAL-Kategorien ist erfüllt. Ausschluss erforderlich."
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "BBLJ4RG3",
      "title": "Developing an artificial intelligence literacy framework: Evaluation of a literacy course for senior secondary students using a project-based learning approach",
      "author_year": "Kong et al. (2024)",
      "authors": "Kong, Siu-Cheung; Cheung, Man-Yin William; Tsang, Olson",
      "year": 2024,
      "doi": "10.1016/j.caeai.2024.100214",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2666920X24000158",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper entwickelt ein KI-Literacy-Framework und evaluiert einen Kurs für Sekundarschüler – klarer Fokus auf AI Literacies als Kompetenzentwicklung. Jedoch kein Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität, Feminismus oder Fairness erkennbar. Erfüllt nur TECHNIK-Bedingung, nicht"
      },
      "human": {
        "decision": "Unclear",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "FJQ6XYHH",
      "title": "Artificial Intelligence (AI) literacy – an argument for AI literacy in education",
      "author_year": "Kong et al. (2025)",
      "authors": "Kong, Siu-Cheung; Korte, Satu-Maarit; Burton, Steve; Keskitalo, Pigga; Turunen, Tuija; Smith, David; Wang, Lixun; Lee, John Chi-Kin; Beaton, Mhairi C.",
      "year": 2025,
      "doi": "10.1080/14703297.2024.2332744",
      "url": "https://www.tandfonline.com/doi/full/10.1080/14703297.2024.2332744",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Titel deutet auf AI Literacy in Bildung hin (TECHNIK-Kategorie erfüllt). Ohne Abstract/Volltext ist keine reliable Bewertung möglich. Kritisch: Kein erkennbarer Bezug zu Sozialer Arbeit, Bias, Gender, Diversität, Fairness oder feministischen Perspektiven. Das Paper adressiert offenbar allgemeine Bil"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "JMAWNUEV",
      "title": "GLAT: The generative AI literacy assessment test",
      "author_year": "Jin et al. (2025)",
      "authors": "Jin, Yueqiao; Martinez-Maldonado, Roberto; Gašević, Dragan; Yan, Lixiang",
      "year": 2025,
      "doi": "10.1016/j.caeai.2025.100436",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2666920X25000761",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Generative_KI"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt ein Assessment-Tool für generative KI-Kompetenzen und liegt damit im Bereich AI Literacies. Es fehlt jedoch jeder erkennbare Bezug zu sozialen Dimensionen (Soziale Arbeit, Bias, Gender, Diversität, Fairness). Das Paper adressiert rein bildungstechnische Aspekte von KI-Kompetenzer"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "7U29SIC8",
      "title": "In search of artificial intelligence (AI) literacy in teacher education: A scoping review",
      "author_year": "Sperling et al. (2024)",
      "authors": "Sperling, Katarina; Stenberg, Carl-Johan; McGrath, Cormac; Åkerfeldt, Anna; Heintz, Fredrik; Stenliden, Linnéa",
      "year": 2024,
      "doi": "10.1016/j.caeo.2024.100169",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2666557324000107",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt AI Literacy explizit, aber ausschließlich im Kontext von Lehrerausbildung (Teacher Education). Es fehlt jeder Bezug zu Sozialer Arbeit, Bias, Fairness, Gender oder Diversität. Die SOZIAL-Kategorie ist nicht erfüllt: Weder Soziale_Arbeit noch eine der Ungleichheits-/Fairness-Kateg"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "3K9RDWLY",
      "title": "AI Literacy - Towards Measuring Human Competency in Artificial Intelligence",
      "author_year": "Pinski & Benlian (2023)",
      "authors": "Pinski, Marc; Benlian, Alexander",
      "year": 2023,
      "doi": "10.24251/HICSS.2023.021",
      "url": "http://hdl.handle.net/10125/102649",
      "abstract": "",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper adressiert AI Literacy als Messung von KI-Kompetenzen (TECHNIK_OK). Es fehlt jedoch jede SOZIAL-Komponente: kein Bezug zu Sozialer Arbeit, keinen spezifischen Fokus auf Bias, Gender, Diversität oder Fairness. Ohne Sozialarbeitsbezug oder sozialwissenschaftliche Kritik wird es ausgeschlosse"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "2SNYUZG4",
      "title": "Promises and challenges of generative artificial intelligence for human learning",
      "author_year": "Yan et al. (2024)",
      "authors": "Yan, Lixiang; Greiff, Samuel; Teuber, Ziwen; Gašević, Dragan",
      "year": 2024,
      "doi": "10.1038/s41562-024-02004-5",
      "url": "https://www.nature.com/articles/s41562-024-02004-5",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.3,
        "categories": [
          "AI_Literacies",
          "Generative_KI"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Basierend auf dem Titel behandelt das Paper generative KI und menschliches Lernen, was auf AI_Literacies und Generative_KI hindeutet. Allerdings fehlt das Abstract und der Volltext für eine verlässliche Bewertung. Ohne substantielle Informationen kann nicht überprüft werden, ob ein echter Sozialarbe"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "SE579V7B",
      "title": "Conceptualizing AI literacy: An exploratory review",
      "author_year": "Ng et al. (2021)",
      "authors": "Ng, Davy Tsz Kit; Leung, Jac Ka Lok; Chu, Samuel Kai Wah; Qiao, Maggie Shen",
      "year": 2021,
      "doi": "10.1016/j.caeai.2021.100041",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000357",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper ist explizit ein exploratory review zu AI Literacy und behandelt somit substantiell KI-Kompetenzen und Wissenstransfer. Es erfüllt die TECHNIK-Bedingung. Jedoch gibt es keinen erkennbaren Bezug zu sozialen Dimensionen wie Soziale Arbeit, Bias/Ungleichheit, Gender, Diversität, Feminismus od"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "KI9GRGHB",
      "title": "Theorising algorithmic justice",
      "author_year": "Marjanovic et al. (2022)",
      "authors": "Marjanovic, O.; Cecez-Kecmanovic, D.; Vidgen, R.",
      "year": 2022,
      "doi": "10.1080/0960085X.2021.1934130",
      "url": "",
      "abstract": "Theoretical article developing framework for understanding algorithmic justice in automated decision-making systems with specific application to social welfare services. Authors examine how AI-driven automated algorithmic decision-making threatens core social justice principles in transformative services like welfare. Framework addresses WHAT matters in algorithmic justice (fairness, equity, human rights), WHO counts as subjects (vulnerable populations disproportionately affected), and HOW algor",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch KI_Sonstige (algorithmische Entscheidungssysteme im Sozialbereich). SOZIAL_OK durch Soziale_Arbeit (direkte Anwendung auf Wohlfahrtsdienste), Bias_Ungleichheit (Diskriminierung marginalisierter Gruppen), Diversitaet (vulnerable Populationen, marginal"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "AMYZFAPH",
      "title": "Recommendations for social work researchers and journal editors on the use of generative AI and large language models",
      "author_year": "Perron et al. (2023)",
      "authors": "Perron, B. E.; Victor, B. G.; Hodge, D. R.; Vaughn, M. G.; Salas-Wright, C. P.",
      "year": 2023,
      "doi": "10.1086/726021",
      "url": "",
      "abstract": "Evidence-based recommendations for social work researchers using generative AI and LLMs, addressing prompt engineering and critical engagement with AI outputs. Examines how LLMs can improve research efficiency through facilitating literature reviews, data analysis, and writing assistance while emphasizing need for critical evaluation of AI-generated content. Discusses concerns about over-reliance on AI potentially diminishing research quality when researchers don't engage in critical thinking or",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper adressiert substantiell AI_Literacies (kritische Reflexion über KI-Nutzung), Generative_KI (LLMs im Fokus) und Prompting (explizit prompt engineering erwähnt). Direkter Bezug zur Soziale_Arbeit durch Zielgruppe (social work researchers) und Empfehlungen für Praxis. TECHNIK_OK (3/4 Kategorien e"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "VSZM7CT6",
      "title": "Problematising artificial intelligence in social work education: Challenges, issues and possibilities",
      "author_year": "Hodgson et al. (2022)",
      "authors": "Hodgson, D.; Goldingay, S.; Boddy, J.; Nipperess, S.; Watts, L.",
      "year": 2022,
      "doi": "10.1093/bjsw/bcab168",
      "url": "",
      "abstract": "Critical examination of AI's fourth industrial revolution implications for social work education, questioning what skills and knowledge should be taught to prepare students for digital working lives. Adopts problematizing approach challenging both celebratory and catastrophic narratives about AI. Argues social work education must address fundamental tensions between training for technologically-driven environments and engaging students in critical theories acknowledging unequal power distributio",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper adressiert substantiell AI_Literacies durch kritische Diskussion von Kompetenzen und Wissen im Umgang mit KI in der Ausbildung (critical digital literacies). Direkter Bezug zu Soziale_Arbeit durch Fokus auf Social Work Education und berufliche Vorbereitung. Bias_Ungleichheit ist zentral: kriti"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "EXRF5629",
      "title": "Predicting successful placements for youth in child welfare with machine learning",
      "author_year": "Spaulding et al. (2023)",
      "authors": "Spaulding, A. D.; Valentine, E. J.; Skinner, S. R.; Gosnell, R. A.; Tran, K.; Hatfield, J.; Johnson, J. K.",
      "year": 2023,
      "doi": "10.1016/j.childyouth.2023.107229",
      "url": "",
      "abstract": "Study developed machine learning models to predict treatment success for youth in various child welfare placement settings using data from 4,788 youth served by Children's Hope Alliance in North Carolina. Models aimed to distinguish which youth would succeed in high-cost residential psychiatric treatment versus community-based alternatives. Using Treatment Outcome Package (TOP) assessment data and gradient boosting algorithms, models achieved AUROCs >0.70 in predicting placement success. Address",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt ML-basierte Entscheidungssysteme (Gradient Boosting) in der Jugendhilfe (KI_Sonstige=Ja). Direkter Bezug zu sozialarbeiterischer Praxis durch Vorhersage von Platzierungserfolgen im Child Welfare System (Soziale_Arbeit=Ja). TECHNIK und SOZIAL erfüllt → Include. Keine explizite Behandl"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "NSI6S5QE",
      "title": "NASW, ASWB, CSWE, & CSWA standards for technology in social work practice",
      "author_year": "National Association of Social Workers et al. (2017)",
      "authors": "",
      "year": 2017,
      "doi": "",
      "url": "https://www.socialworkers.org/Practice/NASW-Practice-Standards-Guidelines/Standards-for-Technology-in-Social-Work-Practice",
      "abstract": "Landmark 64-page collaborative document representing unprecedented coordination among four major U.S. social work organizations to establish comprehensive technology standards for the profession. Standards address four main areas: providing information to public, designing and delivering services, gathering/managing/storing/accessing client information, and educating and supervising social workers. Covers practitioner competence, informed consent, privacy and confidentiality, boundaries and dual",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper etabliert Standards für Technologienutzung in der Sozialarbeit, direkt relevant für KI-Anwendungen. AI_Literacies: Ja (Kompetenzanforderungen für Sozialarbeiter in Technologienutzung). KI_Sonstige: Ja (algorithmische Systeme im Sozialbereich adressiert durch Ethics-Framework). Soziale_Arbe"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "7FEFMCBZ",
      "title": "How child welfare workers reduce racial disparities in algorithmic decisions",
      "author_year": "Cheng et al. (2022)",
      "authors": "Cheng, H.-F.; Stapleton, L.; Kawakami, A.; Sivaraman, V.; Cheng, Y.; Qing, D.; Perer, A.; Holstein, K.; Wu, Z. S.; Zhu, H.",
      "year": 2022,
      "doi": "10.1145/3491102.3501831",
      "url": "",
      "abstract": "Mixed-methods study analyzing four years of child welfare call screening data alongside worker interviews to investigate how human-algorithm collaboration affects racial bias in decision-making. Demonstrates Allegheny Family Screening Tool algorithm alone would have created 20% disparity in screen-in rates between Black and white children, but workers reduced this to 9% through holistic risk assessments and adjustments for algorithmic limitations. Reveals critical discrimination risks: algorithm",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (KI_Sonstige: algorithmisches Entscheidungssystem in Jugendhilfe, Fairness: explizit Racial Bias und Fairness-Aspekte) und SOZIAL (Soziale_Arbeit: direkter Bezug zu Child Welfare Practice, Bias_Ungleichheit: Analyse rassistischer Diskriminierung, Fairness: Al"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "LR8Z3YHP",
      "title": "'Meaningless work': How the datafication of health reconfigures knowledge about work and erodes professional judgement",
      "author_year": "Wadmann & Hoeyer (2020)",
      "authors": "Wadmann, S.; Hoeyer, K.",
      "year": 2020,
      "doi": "10.1177/0950017020950021",
      "url": "",
      "abstract": "Critical ethnographic study examining how datafication and digitalization in Denmark's healthcare sector erode professional judgment and create meaningless work through surveillance and control mechanisms. Authors challenge policy narratives that more data leads to better, evidence-based healthcare decisions, revealing instead how data-intensive practices create Kafkaesque idiocy reconfiguring perceptions of work and undermining goal orientation. Key critical insights demonstrate how dual ambiti",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt Datafizierung und Digitalisierung im Gesundheitswesen, nicht aber KI/ML-Systeme spezifisch. Es fehlt jeglicher Bezug zu KI-Technologien (klassisches ML, generative KI, Prompting, AI Literacies). Ohne TECHNIK-Komponente kann die inklusive Entscheidungslogik nicht erfüllt werden. K"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "P4YQIKJX",
      "title": "Ethics & AI: A systematic review on ethical concerns and related strategies for designing with AI in healthcare",
      "author_year": "Li et al. (2023)",
      "authors": "Li, F.; Ruijs, N.; Lu, Y.",
      "year": 2023,
      "doi": "10.3390/ai4010003",
      "url": "",
      "abstract": "Systematic literature review (2010-2020) identifying 12 main ethical issues in AI healthcare applications with direct relevance to social services. Critical value tensions identified include justice and fairness (algorithmic bias causing discrimination), freedom and autonomy (control, respecting human autonomy, and informed consent challenges), privacy violations through data-driven systems, transparency conflicts with black-box algorithms, dignity concerns when AI reduces persons to data, and c",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper ist Ethik-Review zu KI im Healthcare mit substantiellem Fokus auf algorithmischen Bias und Fairness. Erfüllt TECHNIK-Bedingung (KI_Sonstige). Aber: Kein direkter Bezug zu Sozialer Arbeit oder deren Zielgruppen erkennbar; Bias/Fairness ohne sozialarbeiterischen Kontext. Orientierung an Healthca"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "R7V99ERA",
      "title": "Examining risks of racial biases in NLP tools for child protective services",
      "author_year": "Field et al. (2023)",
      "authors": "Field, A.; Coston, A.; Gandhi, N.; Chouldechova, A.; Putnam-Hornstein, E.; Steier, D.; Tsvetkov, Y.",
      "year": 2023,
      "doi": "10.1145/3593013.3594094",
      "url": "",
      "abstract": "Empirical study examining racial bias in natural language processing tools used to analyze child protective services case notes and make risk assessments. Demonstrates language models trained on case narratives exhibit systematic biases disadvantaging families of color. Testing multiple NLP architectures on real child welfare text data, finds models consistently predict higher risk scores for cases mentioning dialects or cultural contexts associated with Black and Latinx families, even when case",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Kriterien: (1) TECHNIK: KI_Sonstige=Ja (NLP für Risikobewertung, algorithmische Systeme im Sozialbereich); (2) SOZIAL: Soziale_Arbeit=Ja (Child Protective Services), Bias_Ungleichheit=Ja (racial bias, automated discrimination), Diversitaet=Ja (Black/Latinx families), Fairness=Ja "
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "4ZL5Q48E",
      "title": "Algorithmic management in a work context",
      "author_year": "Jarrahi et al. (2021)",
      "authors": "Jarrahi, M. H.; Newlands, G.; Lee, M. K.; Wolf, C. T.; Kinder, E.; Sutherland, W.",
      "year": 2021,
      "doi": "10.1177/20539517211020332",
      "url": "",
      "abstract": "Interdisciplinary analysis examining algorithmic management as sociotechnical phenomenon shaped by organizational choices and power structures rather than technological determinism. Authors critically analyze how algorithmic systems in standard work settings redefine pre-existing power dynamics between workers and managers, demanding new competencies while fostering oppositional attitudes. Key critical insights include risk of treating workers as programmable cogs through automation, commodifica",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt algorithimische Systeme (KI_Sonstige: Ja) und thematisiert Machtstrukturen sowie Ungleichheit am Arbeitsplatz (Bias_Ungleichheit: Ja). ABER: Kein direkter Bezug zu Sozialer Arbeit, deren Praxis, Theorie oder Zielgruppen. Der Fokus liegt auf allgemeinen Arbeitskontexten/Organisationen"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "ZQHP5G35",
      "title": "Data and rights in the digital welfare state: the case of Denmark",
      "author_year": "Jørgensen (2023)",
      "authors": "Jørgensen, R. F.",
      "year": 2023,
      "doi": "10.1080/1369118X.2021.1934069",
      "url": "",
      "abstract": "Critical analysis examining how surveillance capitalism logic manifests in public sector through Denmark's automated welfare decision-making systems. Argues that unless more human-centric approaches are adopted, digital welfare states advance digital technocracy treating citizens as data points for calculation and prediction rather than individuals with agency and rights. Employs theories of surveillance capitalism, digital-era governance, and data politics to analyze automated decision support ",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmische Entscheidungssysteme im Wohlfahrtssektor), SOZIAL_OK (Soziale_Arbeit: Wohlfahrtsadministration; Bias_Ungleichheit: Bürgerdisempowerment durch Überwachung; Fairness: Kritik an automatisierten Entscheidungen). Theoretische kritis"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "A2P8MXMY",
      "title": "Algorithmic justice in child protection: Statistical fairness, social justice and the implications for practice",
      "author_year": "Keddell (2019)",
      "authors": "Keddell, E.",
      "year": 2019,
      "doi": "10.3390/socsci8100281",
      "url": "",
      "abstract": "Critical analysis examining fairness and justice implications of predictive algorithms in child protection from both statistical and social justice perspectives. Identifies fundamental problems with using child protection system data: biased sample frames reflecting reporting patterns rather than actual abuse incidence, feedback loops amplifying discrimination, and spurious correlations conflating system surveillance with genuine risk. Demonstrates racial and socioeconomic disparities in child w",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: predictive algorithms in child protection), SOZIAL_OK (Soziale_Arbeit: child protection/welfare practice, Bias_Ungleichheit: racial und socioeconomic disparities, Diversitaet: marginalisierte Gruppen, Fairness: statistical fairness analysis)."
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "ZITLBM8A",
      "title": "ChatGPT for social work science: Ethical challenges and opportunities",
      "author_year": "Singer et al. (2023)",
      "authors": "Singer, J. B.; Victor, B. G.; Perron, B. E.",
      "year": 2023,
      "doi": "10.1086/726042",
      "url": "",
      "abstract": "Invited paper exploring opportunities and ethical challenges of deploying ChatGPT and large language models specifically for social work science. Describes potential uses of ChatGPT in social work research while examining critical ethical concerns related to algorithmic bias, data quality, and risk of replacing human expertise with AI-generated content. Offers preliminary recommendations for ethical ChatGPT use in social work research, emphasizing need for researchers to critically evaluate AI o",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Inklusionskriterien: TECHNIK_OK (Generative_KI: ChatGPT/LLMs, AI_Literacies: kritische Evaluationskompetenz für Forscher), SOZIAL_OK (Soziale_Arbeit: direkter Bezug zu sozialer Arbeit, Bias_Ungleichheit: algorithmischer Bias). Invited paper mit Fokus auf ethische Reflexion und pr"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "QI5AYE4V",
      "title": "Algorithmic decision-making in social work practice and pedagogy: Confronting the competency/critique dilemma",
      "author_year": "McDonald et al. (2023)",
      "authors": "McDonald, C.; Marston, G.; Buckley, A.",
      "year": 2023,
      "doi": "10.1080/02615479.2023.2195425",
      "url": "",
      "abstract": "Explores tensions between training social workers for technologically-driven environments and engaging students in critical theories acknowledging unequal power distributions that technological change reinforces. Introduces algorithmic literacy centered on understanding critical limitations of algorithmic decision-making systems. Argues that adding social work subjects on ADM alone proves insufficient; students need opportunities to develop algorithmic literacy enabling them to ask appropriate q",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper adressiert algorithmische Literacität (AI_Literacies) und algorithmic decision-making systems (KI_Sonstige) im Kontext der Sozialarbeiter:innen-Ausbildung (Soziale_Arbeit). Kritische Auseinandersetzung mit Machtverteilungen und strukturellen Ungleichheiten (Bias_Ungleichheit). Erfüllt beide Be"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "WTLVG29I",
      "title": "Artificial intelligence in social work: Emerging ethical issues",
      "author_year": "Reamer (2023)",
      "authors": "Reamer, F. G.",
      "year": 2023,
      "doi": "10.55521/10-020-205",
      "url": "",
      "abstract": "Seminal examination of critical ethical challenges related to social workers' AI use, addressing informed consent, privacy, transparency, algorithmic bias, client misdiagnosis, surveillance, and evidence-based tools. Provides detailed protocols for ethical AI implementation including establishing ethics-based governing principles, creating digital ethics steering committees, conducting diverse focus groups, and subjecting algorithms to peer review. Emphasizes developing competencies for understa",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper adressiert KI-Kompetenzen für Sozialarbeiter:innen (AI_Literacies), algorithmische Systeme in der Sozialarbeit wie Risikobewertung (KI_Sonstige), direkte sozialarbeiterische Praxis und Ethik (Soziale_Arbeit), algorithmischen Bias und Fairness bei Algorithmen (Bias_Ungleichheit, Fairness) sowie"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "J7V3AAQT",
      "title": "A chatbot for mental health support: Exploring the impact of Emohaa on reducing mental distress in China",
      "author_year": "Sabour et al. (2023)",
      "authors": "Sabour, S.; Zhang, W.; Xiao, X.; Zhang, Y.; Zheng, Y.; Wen, J.; Zhao, J.; Huang, M.",
      "year": 2023,
      "doi": "10.3389/fdgth.2023.1133987",
      "url": "",
      "abstract": "Three-arm randomized controlled trial examining Emohaa mental health chatbot system in China with 141 university students experiencing mental distress. Participants assigned to: CBT-Bot only (n=69), Full Emohaa with CBT-Bot and emotional support bot (n=31), or control group (n=41). 8-week intervention used AI-powered conversational agents providing cognitive-behavioral therapy techniques and emotional support. Results showed significant improvements in depression (p<.001, effect size η²p=.05) an",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.92,
        "categories": [
          "Generative_KI"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt Generative KI (Chatbot mit Sprachverarbeitung), erfüllt aber nicht die Sozial-Kriterien. Mental-Health-Anwendung für Universitätsstudenten ist kein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder Fairness. Psychologische Intervention ohne sozialarbeiteri"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "9WIGR47Y",
      "title": "Ethical issues related to the use of technology in social work practice: A systematic review",
      "author_year": "Rodríguez-Martínez et al. (2024)",
      "authors": "Rodríguez-Martínez, A.; Amezcua Aguilar, M. T.; Cortés Moreno, J.; Jiménez-Delgado, J. J.",
      "year": 2024,
      "doi": "10.1177/21582440241274842",
      "url": "",
      "abstract": "Systematic literature review examining ethical tensions arising from technology integration in social work practice. Review identifies three main categories of ethical challenges: effects of digitization on professional practice (tensions between efficiency and human connection), education, research and engagement challenges (digital literacy requirements conflicting with traditional social work training), and ethical challenges in digital professional practice (boundary issues, dual relationshi",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt ethische Herausforderungen von Technologie in der Sozialen Arbeit substantiell. KI_Sonstige erfüllt (automatisierte Systeme, digitale Technologien), Soziale_Arbeit erfüllt (direkte Praxisanalyse), Bias_Ungleichheit erfüllt (digital divide, soziale Gerechtigkeit, Selbstbestimmung). Sy"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "S8WGUVQT",
      "title": "Automated government benefits and welfare surveillance",
      "author_year": "Dencik et al. (2024)",
      "authors": "Dencik, L.; Hintz, A.; Redden, J.; Warne, H.",
      "year": 2024,
      "doi": "",
      "url": "https://ojs.library.queensu.ca/index.php/surveillance-and-society/article/view/16107",
      "abstract": "Critical surveillance studies analysis examining digital welfare state historically, presently, and prospectively, focusing on AI-driven welfare surveillance systems. Authors argue problems posed by AI in public administration are often misattributed to technological novelty when they actually represent historically familiar patterns of surveillance and control. Drawing on bureaucracy, welfare state, and automation scholarship, demonstrates how algorithmic fraud detection and chatbot assistance ",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Kriterien: TECHNIK_OK durch KI_Sonstige (algorithmische Systeme in öffentlicher Verwaltung: Fraud Detection, Chatbots). SOZIAL_OK durch Soziale_Arbeit (Wohlfahrtssysteme, marginalisierte Populationen) und Bias_Ungleichheit (Analyse von Machtsymmetrien, Surveillance von benachteil"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "LXDG4KQK",
      "title": "Coded injustice: Surveillance and discrimination in Denmark's automated welfare state",
      "author_year": "Amnesty International (2024)",
      "authors": "",
      "year": 2024,
      "doi": "",
      "url": "https://www.amnesty.org/en/documents/eur18/8709/2024/en/",
      "abstract": "Critical human rights investigation documenting how Denmark's AI-powered welfare fraud detection systems risk discriminating against people with disabilities, low-income individuals, migrants, refugees, and marginalized racial groups. Report examines up to 60 algorithmic models used to flag individuals for fraud investigations, revealing mass surveillance practices violating privacy rights and creating atmospheres of fear among welfare recipients. Key findings demonstrate how automated systems p",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Report untersucht substantiell algorithmische Entscheidungssysteme (KI_Sonstige: Ja) im Wohlfahrtsbereich, direkt relevant für Soziale Arbeit (Ja). Zentral sind Bias/Diskriminierung gegen vulnerable Gruppen (Bias_Ungleichheit: Ja), Diversität marginalisierter Bevölkerungen (Diversitaet: Ja) und Fair"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "BTLTEA6Y",
      "title": "Decision support and algorithmic support: The construction of algorithms and professional discretion in social work",
      "author_year": "Meilvang & Dahler (2024)",
      "authors": "Meilvang, M. L.; Dahler, A. M.",
      "year": 2024,
      "doi": "",
      "url": "",
      "abstract": "Critical analysis examining three decision-support algorithms developed for Danish municipalities in child and family social work, analyzing how they affect professional discretion despite claims to merely support professionals. Demonstrates how algorithmic systems designed to minimize subjective judgment and promote efficiency actually embody positivist assumptions that professional discretion can and should be eliminated. Key findings reveal how political actors favor standardized, automated a",
      "item_type": "booksection",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK-Ja (KI_Sonstige: algorithmische Entscheidungssysteme in der Jugendhilfe), SOZIAL-Ja (Soziale_Arbeit: direkter Fokus auf Auswirkungen von Algorithmen auf sozialarbeiterische Praxis und Professionalität; Bias_Ungleichheit und Fairness: kritische Analyse von Mac"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "7D3ICY7Z",
      "title": "Introduction to the digital welfare state: Contestations, considerations and entanglements",
      "author_year": "van Toorn et al. (2024)",
      "authors": "van Toorn, G.; Henman, P.; Soldatić, K.",
      "year": 2024,
      "doi": "10.1177/14407833241260890",
      "url": "",
      "abstract": "Special issue introduction providing critical sociological analysis of digital welfare state, examining how datafication and automation amplify existing trends of surveillance and control over marginalized populations. Authors argue that contrary to neutral efficiency narratives, digital welfare technologies are embedded in fiscal austerity politics and criminalization of poverty. Employs power relations and human agency frameworks to demonstrate how algorithmic systems increase scrutiny of welf",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt algorithmische Systeme im Sozialbereich (digital welfare, automated decision-making) substantiell → KI_Sonstige: Ja. Direkter Bezug zu Sozialarbeit und Welfare-Praxis → Soziale_Arbeit: Ja. Kritische Analyse von algorithmischer Diskriminierung marginalisierter Populationen → Bias_Ungl"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "XQM6WRU2",
      "title": "Exploring machine learning to support decision-making for placement stabilization and preservation in child welfare",
      "author_year": "Cher et al. (2024)",
      "authors": "Cher, K. H. B.; Adelson, S. L.; McCrae, J. S.; Barth, R. P.",
      "year": 2024,
      "doi": "10.1007/s10826-024-02993-x",
      "url": "",
      "abstract": "Statewide study analyzed 12,621 child welfare cases in large Midwestern state (2017-2020) to develop machine learning models predicting placement disruption risk. Goal was to identify youth who could benefit from placement stabilization services to prevent unnecessary residential care under Family First Prevention Services Act. Random forest models were compared with conventional logistic regression for predicting placement disruption and referral to stabilization programs. ML models demonstrate",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper entwickelt ML-Modelle (Random Forest, logistische Regression) für Vorhersage von Platzierungsabbruch in Jugendhilfe. Erfüllt TECHNIK-Kriterium durch KI_Sonstige (klassisches ML für algorithmische Entscheidungssysteme im Sozialbereich). Erfüllt SOZIAL-Kriterium durch: (1) direkter Bezug zu Sozi"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "E4G328PD",
      "title": "Clinical trial of an LLM-based conversational AI psychotherapy",
      "author_year": "Heinz et al. (2025)",
      "authors": "Heinz, M. V.; Fernandez-Mendoza, J.; Semien, G.; Brown, R.; Jacobson, N. C.",
      "year": 2025,
      "doi": "",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400802",
      "abstract": "Groundbreaking study representing first randomized controlled trial of generative AI-powered therapy chatbot, Therabot. Trial included 106 participants across United States diagnosed with major depressive disorder, generalized anxiety disorder, or eating disorders. Participants interacted with Therabot via smartphone app over 4-8 weeks. Results showed clinically significant symptom improvements: 51% reduction in depression symptoms, 31% reduction in anxiety symptoms, and 19% reduction in eating ",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "Generative_KI"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper untersucht LLM-basierte Psychotherapie in klinischem Kontext. Während es Generative KI behandelt (RCT mit Chatbot), fehlt der Bezug zur Sozialen Arbeit vollständig. Die Anwendung ist klinische Psychologie/Psychiatrie, nicht Soziale Arbeit. Keine Thematisierung von Bias, Fairness, Gender, D"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "GFXYER4F",
      "title": "AI implementation science for social issues: Pitfalls and tips",
      "author_year": "Takaoka (2022)",
      "authors": "Takaoka, K.",
      "year": 2022,
      "doi": "10.2188/jea.JE20210380",
      "url": "",
      "abstract": "Case study documenting four-stage social implementation of AI system (AiCAN - Assistant of Intelligence for Child Abuse and Neglect) in Japanese Child Guidance Centers from 2012-2020. System uses machine learning to predict child abuse recurrence and Bayesian networks for real-time probabilistic inference to guide temporary protection decisions. Data from over 6,000 cases (2014-2018) were used to develop gradient boosting algorithms with AUROC >0.70. Implementation involved iterative stakeholder",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK: KI_Sonstige (Machine Learning, Gradient Boosting für Prediction) + AI_Literacies (Training von Fachkräften, eXplainable AI für Transparenz). SOZIAL: Soziale_Arbeit (Jugendhilfe/Child Guidance Centers, direkte sozialarbeiterische Praxis und Zielgruppen). Impl"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "3B87U5LN",
      "title": "\"It happened to be the perfect thing\": Experiences of generative AI chatbots for mental health",
      "author_year": "Siddals et al. (2024)",
      "authors": "Siddals, S.; Torous, J.; Coxon, A.",
      "year": 2024,
      "doi": "10.1038/s44184-024-00097-4",
      "url": "",
      "abstract": "Qualitative study using semi-structured interviews with 19 individuals from 8 countries who used generative AI chatbots (primarily Pi, ChatGPT) for mental health support in real-world settings. Participants reported high engagement and positive impacts including improved relationships, healing from trauma and loss, and improved mood. Four themes emerged: emotional sanctuary (non-judgmental, always-available support), insightful guidance particularly for relationships, joy of connection, and comp",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper untersucht Nutzung von generativen KI-Chatbots (Pi, ChatGPT) für mentale Gesundheitsunterstützung. Generative_KI=Ja (LLM-basierte Chatbots im Fokus). Soziale_Arbeit=Ja (mentale Gesundheit, therapeutische Kontexte, Unterstützung von vulnerable Personen - Kernbereich Sozialer Arbeit). Beide TECH"
      },
      "human": {
        "decision": "Unclear",
        "categories": [
          "Generative_KI",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "C325Y32P",
      "title": "AI for impact: The PRISM framework for responsible AI in social innovation",
      "author_year": "World Economic Forum (2024)",
      "authors": "",
      "year": 2024,
      "doi": "",
      "url": "https://www.weforum.org/publications/ai-for-impact-the-prism-framework-for-responsible-ai-in-social-innovation/",
      "abstract": "Institutional report introducing PRISM framework specifically designed for social innovators, impact enterprises, and intermediaries working in social services sectors. Building on Presidio Framework of AI Governance Alliance, PRISM provides adoption pathways through which organizations can filter their impact mission, capabilities, and risks against AI technology use. Framework includes AI-enabled readiness assessment matrix enabling organizations to evaluate current practices and develop actio",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt KI-Kompetenzen und -Integration (AI_Literacies) sowie allgemeine KI-Governance (KI_Sonstige) im Kontext sozialer Innovationen und Dienstleistungen (Soziale_Arbeit). PRISM-Framework adressiert direkt Organisationen in sozialen Sektoren mit Readiness-Assessments. TECHNIK_OK (AI_Literac"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "7QQV7R9S",
      "title": "Generative AI & social work practice guidance",
      "author_year": "British Association of Social Workers (2025)",
      "authors": "",
      "year": 2025,
      "doi": "",
      "url": "https://basw.co.uk/policy-and-practice/resources/generative-ai-social-work-practice-guidance",
      "abstract": "BASW's initial practice guidance specifically addressing generative AI use in social work, offering reflection points for practitioners relating to ethical considerations under BASW Code of Ethics. Warns that AI tools are prone to replicating racist/sexist assumptions from training datasets, generating misleading information (hallucinations), and risking data privacy breaches. Emphasizes generative AI should create capacity for relationship-based practice rather than justify increased caseloads ",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + AI_Literacies durch Praxis-Guidance) und SOZIAL (Soziale_Arbeit + Bias_Ungleichheit durch Warnung vor racist/sexist assumptions). Es ist eine direkte Praxis-Orientierung für Sozialarbeiter:innen mit ethischen Reflexionspunkten und adressi"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "PI5H2LZ2",
      "title": "Failing our youngest: On the biases, pitfalls, and risks in a decision support algorithm used for child protection",
      "author_year": "Moreau et al. (2024)",
      "authors": "Moreau, T.; Sinatra, R.; Sekara, V.",
      "year": 2024,
      "doi": "10.1145/3630106.3658906",
      "url": "",
      "abstract": "Critical empirical study examining child protection decision support algorithm deployed in Danish municipalities, analyzing its biases and implementation challenges. Using real administrative data from Denmark's child welfare system, evaluated algorithm's predictions against actual case outcomes and found significant biases including disproportionate impacts on immigrant families and systematic errors in risk assessment. Results revealed concerning patterns of false positives for marginalized co",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Einschlusskriterien: (1) TECHNIK: KI_Sonstige=Ja (algorithmisches Entscheidungssystem in der Jugendhilfe). (2) SOZIAL: Soziale_Arbeit=Ja (direkter Bezug zu Child Protection/Jugendhilfe), Bias_Ungleichheit=Ja (Analyse von Diskriminierung gegenüber Immigrantenfamilien), Diversitaet"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "JC7X3MM7",
      "title": "Improving human-AI partnerships in child welfare: Understanding worker practices, challenges, and desires for algorithmic decision support",
      "author_year": "Kawakami et al. (2022)",
      "authors": "Kawakami, A.; Sivaraman, V.; Cheng, H.-F.; Stapleton, L.; Cheng, Y.; Qing, D.; Perer, A.; Wu, Z. S.; Zhu, H.; Holstein, K.",
      "year": 2022,
      "doi": "10.1145/3491102.3517439",
      "url": "",
      "abstract": "Empirical study examining how child maltreatment hotline workers interact with Allegheny Family Screening Tool, an AI-based decision support system. Through interviews and contextual inquiries, found workers' reliance on algorithmic predictions guided by four key factors: knowledge of contextual information beyond AI model capabilities, beliefs about system limitations, organizational pressures around tool use, and awareness of misalignments between algorithmic predictions and their own decision",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper adressiert Allegheny Family Screening Tool (algorithmisches Entscheidungssystem in der Jugendhilfe). TECHNIK erfüllt: KI_Sonstige (algorithmische Systeme), AI_Literacies (worker training, tool understanding). SOZIAL erfüllt: Soziale_Arbeit (child welfare practice), Bias_Ungleichheit (discrimin"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "TNLGELEQ",
      "title": "ReflectAI: Design and evaluation of an AI coach to support public servants' self-reflection",
      "author_year": "Näscher et al. (2025)",
      "authors": "Näscher, H.-H.; Schaffner, J.; Koddebusch, M.",
      "year": 2025,
      "doi": "10.1007/978-3-032-02515-9_7",
      "url": "",
      "abstract": "Design science research presenting ReflectAI, an LLM-based AI coach designed to support public servants in developing self-reflection competencies—critical skill for digital transformation in public administration. Two-week user study with seven public servants revealed three key benefits: increased awareness of self-reflection opportunities, improved thought structure, and valuable conversation documentation. Demonstrates how conversational AI can facilitate reflective practice through structur",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt TECHNIK- und SOZIAL-Kriterien: (1) Generative_KI (LLM-basierter AI Coach), Prompting (strukturiertes Prompting für reflektive Praxis), AI_Literacies (Entwicklung von Self-Reflection-Kompetenzen). (2) Soziale_Arbeit (öffentliche Verwaltung, human services context, Kompetenzentwicklung)."
      },
      "human": {
        "decision": "Unclear",
        "categories": [
          "Generative_KI"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "8MDXCTA6",
      "title": "A systematic review of sophisticated predictive and prescriptive analytics in child welfare: Accuracy, equity, and bias",
      "author_year": "Hall et al. (2024)",
      "authors": "Hall, S. F.; Sage, M.; Scott, C. F.; Magruder, K.; Powers, J.",
      "year": 2024,
      "doi": "10.1007/s10560-023-00931-2",
      "url": "",
      "abstract": "Systematic review examining how researchers address ethics, equity, bias, and model performance in predictive and prescriptive algorithms used in child welfare settings. Analyzing 67 articles published 2010-2020, reveals inconsistent approaches to measuring and mitigating algorithmic fairness in child welfare applications. Identifies that most predictive models use administrative data reflecting surveillance biases rather than true child maltreatment incidence, leading to discrimination against ",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch KI_Sonstige (predictive/prescriptive algorithms); SOZIAL_OK durch Soziale_Arbeit (child welfare, social workers), Bias_Ungleichheit (discrimination against low-income families and communities of color), Diversitaet (marginalisierte Communities), Fair"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "4GYXUS9Y",
      "title": "The end of the world as we know it? ChatGPT and social work",
      "author_year": "Goldkind et al. (2024)",
      "authors": "Goldkind, L.; Wolf, L.; Glennon, A.; Rios, J.; Nissen, L.",
      "year": 2024,
      "doi": "10.1093/sw/swad044",
      "url": "",
      "abstract": "Editorial in flagship journal Social Work providing critical reflection on ChatGPT's introduction and implications for social work practice. Addresses how ChatGPT, built on natural language processing, responds to prompts and generates text responses. Notes social work's historical reluctance to embrace new technologies and positions ChatGPT as opportunity to reflect on strategies promoting just technology use. Urges social workers to join cross-disciplinary conversations about AI evolution and ",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Soziale_Arbeit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK-OK (Generative_KI: ChatGPT; Prompting: explizit 'thoughtful prompting practices'; AI_Literacies: kritische Reflexion und kompetenter Umgang), SOZIAL-OK (Soziale_Arbeit: direkter Fokus auf Profession; Fairness: 'just technology use', 'fair use'). Editorial mit"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "CLKAD87H",
      "title": "Responsible prompting recommendation: Fostering responsible AI practices in prompting-time",
      "author_year": "James et al. (2025)",
      "authors": "James, P.; Meeussen, M.; Matthes, I.; Siemon, D.",
      "year": 2025,
      "doi": "10.1145/3706598.3713365",
      "url": "",
      "abstract": "Presents insights from interviews and user studies with IT professionals exploring prompting practices and develops open-source responsible prompting recommender system. Research reveals responsible prompt recommendations can support novice prompt engineers and raise awareness about Responsible AI in prompting-time, helping people reflect on responsible practices before LLM content generation. Demonstrates that finding right balance between adding social values to prompts and removing potentiall",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.87,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (Kompetenzentwicklung für Prompt Engineers), Generative_KI (LLM-Fokus), Prompting (Kernthema: Prompt Recommendations). SOZIAL: Soziale_Arbeit (explizite Relevanz für social services), Bias_Ungleichheit (harmful content removal), Fairness (Respo"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "CY5IMM6G",
      "title": "Artificial Intelligence (AI) literacy for social work: Implications for core competencies",
      "author_year": "Ahn et al. (2025)",
      "authors": "Ahn, E.; Choi, M.; Fowler, P.; Song, I. H.",
      "year": 2025,
      "doi": "10.1086/735187",
      "url": "",
      "abstract": "Comprehensive framework integrating AI literacy into CSWE's nine core competencies, arguing AI understanding is essential for recognizing algorithmic bias perpetuation of social inequalities, contributing to ethical governance, and thoughtfully integrating technology into practice. Emphasizes AI literacy enables social workers to recognize algorithmic bias, advocate for individuals navigating AI-driven systems, and address emerging vulnerabilities including digital divides and discriminatory pra",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies + KI_Sonstige für algorithmische Systeme); SOZIAL_OK (Soziale_Arbeit + Bias_Ungleichheit + Fairness). Zentral ist die Integration von KI-Kompetenzen in CSWE-Kernkompetenzen für Sozialarbeiter:innen mit explizitem Fokus auf algorithmischen Bi"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "UENFDPH9",
      "title": "Considering a unified model of artificial intelligence enhanced social work: A systematic review",
      "author_year": "Garkisch et al. (2024)",
      "authors": "Garkisch, M.; Langer, P. J.; Becher, T.",
      "year": 2024,
      "doi": "10.1007/s41134-024-00326-y",
      "url": "",
      "abstract": "Systematic review mapping research landscape of social work AI scholarship, analyzing 67 articles using qualitative analytic approaches to explore how social work researchers investigate AI. Identified themes consistent with Staub-Bernasconi's triple mandate covering profession level, social agencies/organizations, and clients. Emphasizes importance of enhancing computational thinking, AI literacy, and data literacy, and developing skills for evaluating automated systems. Stresses that professio",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Systematic Review mit substanziellem Fokus auf AI Literacy und Computational Thinking in der Sozialen Arbeit (AI_Literacies: Ja). Direkter Bezug zu sozialarbeiterischer Praxis und Profession via Staub-Bernasconi's Triple Mandate (Soziale_Arbeit: Ja). Thematisiert auch algorithmische Systeme/KI-Syste"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "AK7K4P97",
      "title": "Artificial intelligence in social work: An EPIC model for practice",
      "author_year": "Baker et al. (2025)",
      "authors": "Baker, S.; Garkisch, M.; Robards, F.",
      "year": 2025,
      "doi": "10.1080/0312407X.2025.2488345",
      "url": "",
      "abstract": "Presents EPIC model for integrating AI into social work consisting of four components: Ethics and justice, Policy development and advocacy, Intersectoral collaboration, and Community engagement and empowerment. Following comprehensive literature review, examines AI's influence on social work including opportunities to advance socially just outcomes and challenges risking ethical practice. Emphasizes community-based initiatives promoting AI digital literacy and partnerships with local organizatio",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK-OK durch AI_Literacies (community-based AI digital literacy), KI_Sonstige (AI in social work), SOZIAL-OK durch Soziale_Arbeit (direkter Fokus auf sozialarbeiterische Praxis), Bias_Ungleichheit (ethische Gerechtigkeit), Diversitaet (vulnerable populations, tec"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "6Y5EPQRR",
      "title": "Positionings, challenges, and ambivalences in children's and parents' perspectives in digitalized familial contexts",
      "author_year": "Kutscher (2023)",
      "authors": "Kutscher, N.",
      "year": 2023,
      "doi": "",
      "url": "",
      "abstract": "Examines family dynamics in digitalized contexts, analyzing tensions between children's digital participation rights and parental protection responsibilities. Presents research revealing ambivalences in both parental and children's perspectives: parents struggle between enabling children's digital competence and protecting them from risks; children experience tension between desire for autonomy and need for guidance. Addresses sharenting (parents sharing children's images/information online), ex",
      "item_type": "booksection",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt Digitalisierung in Familien und Sharenting, nicht aber KI-Systeme. Es adressiert zwar sozialarbeiterische Praxisbereiche (Familie, Kindeswohl), erfüllt aber die TECHNIK-Bedingung nicht: Weder AI_Literacies (kein fokussierter KI-Kompetenzrahmen), noch Generative_KI, Prompting oder"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "QKFG72IT",
      "title": "Bildungsteilhabe - Flucht - Digitalisierung: Eine multilokale Ethnografie im (digitalen) Alltag junger Geflüchteter",
      "author_year": "Fujii et al. (2024)",
      "authors": "Fujii, M. S.; Kutscher, N.; Friedrichs-Liesenkötter, H.; Hüttmann, J.",
      "year": 2024,
      "doi": "",
      "url": "https://www.beltz.de/fachmedien/sozialpaedagogik_soziale_arbeit/produkte/details/53146",
      "abstract": "Presents ethnographic research on how digital media shapes educational participation for young refugees, examining ambivalent role of digital technologies: enabling connection to educational resources and transnational networks while simultaneously creating new forms of exclusion through surveillance, documentation requirements, and digital skill barriers. Reveals digital access alone does not guarantee educational participation—digital literacy, linguistic competence, and social support remain ",
      "item_type": "book",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt digitale Medien, Bildungszugang und junge Geflüchtete. Erfüllt SOZIAL-Kriterien (Soziale_Arbeit, Bias_Ungleichheit, Diversität). ABER: Kein Fokus auf KI/ML-Systeme. Digitalisierung ≠ KI. Keine Kategorie aus TECHNIK-Bereich (AI_Literacies, Generative_KI, Prompting, KI_Sonstige) ist su"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "BKSU66QB",
      "title": "Digitalität und Digitalisierung als Gegenstand der Sozialen Arbeit",
      "author_year": "Kutscher (2024)",
      "authors": "Kutscher, N.",
      "year": 2024,
      "doi": "",
      "url": "",
      "abstract": "Differentiates between digitalization (technical processes of making things digital) and digitality (sociotechnical transformations of social practices and relations), arguing social work must engage with both technological changes and their social implications. Demonstrates how algorithms, data-driven systems, and digital platforms reshape professional practice, client relationships, and social inequalities. Argues digitalization fundamentally transforms social contexts where social work operat",
      "item_type": "booksection",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Inklusionskriterien: (1) TECHNIK-Kriterium: KI_Sonstige=Ja (Algorithmen, datengetriebene Systeme, digitale Plattformen sind zentral). (2) SOZIAL-Kriterium: Soziale_Arbeit=Ja (direkter Fokus auf sozialarbeiterische Praxis und deren Transformation) UND Bias_Ungleichheit=Ja (Analyse"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "M2FYV58I",
      "title": "Exploring opportunities and risks in decision support technologies for social workers: An empirical study in the field of disabled people's services",
      "author_year": "Schneider et al. (2022)",
      "authors": "Schneider, D.; Maier, A.; Cimiano, P.; Seelmeyer, U.",
      "year": 2022,
      "doi": "10.1093/bjsw/bcab262",
      "url": "",
      "abstract": "Empirical study investigating German social workers' perspectives on decision support systems in disability services through practitioner interviews. Identifies both opportunities (consistency across cases, evidence-based practice, administrative time-saving) and significant risks (deprofessionalization, data protection concerns, reduced professional autonomy, loss of holistic assessment capabilities). Social workers express ambivalence: recognizing potential for reducing subjective bias and imp",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Entscheidungsunterstützungssysteme/algorithmische Systeme) und SOZIAL_OK (Soziale_Arbeit: expliziter Fokus auf Sozialarbeiter und Behindertenservices; Bias_Ungleichheit: Reduktion subjektiver Vorurteile; Fairness: Transparenz in Ressourcenall"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "LMW8DZ78",
      "title": "Indecision on the use of artificial intelligence in healthcare: A qualitative study of patient perspectives on trust, responsibility and self-determination using AI-CDSS",
      "author_year": "Schneider et al. (2025)",
      "authors": "Schneider, D.; Liedtke, W.; Klausen, A. D.; Lipprandt, M.; Funer, F.; Langanke, M.; Heyen, N. B.; Aichinger, H.; Bratan, T.",
      "year": 2025,
      "doi": "10.1186/s12910-024-01143-5",
      "url": "",
      "abstract": "Empirical qualitative study exploring patient perspectives on AI-based clinical decision support systems (AI-CDSS) in healthcare, revealing significant ambivalence about AI use in medical decision-making. Through interviews examining trust, responsibility distribution, and self-determination when AI systems assist physicians, findings show patients worry about decreased human interaction, loss of holistic care perspectives, and unclear accountability when AI makes errors. Vulnerable populations ",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt AI-CDSS im Healthcare-Kontext (KI_Sonstige: Ja), adressiert algorithmische Fairness und Bias-Aspekte (Bias_Ungleichheit, Fairness: Ja). JEDOCH: Kein direkter Bezug zu Sozialer Arbeit. Healthcare-Setting mit Patient:innen-Perspektiven ist nicht Sozialarbeit (keine Zielgruppen-Überschn"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "2WHGF83D",
      "title": "AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?",
      "author_year": "Schneider & Weber (2024)",
      "authors": "Schneider, D.; Weber, K.",
      "year": 2024,
      "doi": "10.14512/tatup.33.1.08",
      "url": "",
      "abstract": "Comprehensive examination of AI-based decision support systems across healthcare, legal systems, and border control. Provides critical analysis of technical, ethical, legal, and societal challenges when machines make or support decisions previously made by humans. Reviews regulatory attempts including EU AI Act. Examines key issues: opacity of algorithmic systems creating black box problems for accountability, professional deskilling risks when practitioners defer to AI, and potential for discri",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt algorithmische Entscheidungssysteme (KI_Sonstige: Ja) in sensiblen Bereichen (Healthcare, Justiz, Grenzschutz). Substantielle Behandlung von Diskriminierungsrisiken in Trainingsdaten (Bias_Ungleichheit: Ja) und Fairness-Aspekten durch Analyse von Opazität und Accountability (Fairness"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "44AA6ETH",
      "title": "Das verflixte Problem mit Klassifikationen: Zum Einfluss der Digitalisierung auf die Soziale Diagnostik in der Sozialen Arbeit",
      "author_year": "Schneider (2024)",
      "authors": "Schneider, D.",
      "year": 2024,
      "doi": "",
      "url": "",
      "abstract": "Examines how digitalization affects social diagnostics and assessment practices in social work. Analyzes fundamental problem of classification systems: tension between necessary categorization for resource allocation and profession's commitment to individualized, contextual understanding of clients. Digital systems intensify this tension by requiring standardized data inputs that may not capture social complexity or unique circumstances. Explores how algorithmic decision support systems rely on ",
      "item_type": "booksection",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt Algorithmische Entscheidungssysteme (KI_Sonstige: Ja) im Kontext Sozialer Arbeit (Soziale_Arbeit: Ja). Expliziter Fokus auf Klassifikationssysteme und deren Limitations bezüglich Bias und Kontextverlust (Bias_Ungleichheit: Ja). TECHNIK und SOZIAL Bedingungen erfüllt → Include."
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "QG8IAB53",
      "title": "Who cares about data? Data care arrangements in everyday organisational practice",
      "author_year": "Jarke & Büchner (2024)",
      "authors": "Jarke, J.; Büchner, S.",
      "year": 2024,
      "doi": "10.1080/1369118X.2024.2320917",
      "url": "",
      "abstract": "Introduces data care arrangements concept to understand mundane data work in organizations. Demonstrates through empirical research in educational and social service organizations how data care work is distributed across organizational members with different, often conflicting care obligations. Reveals data quality maintenance involves complex sociomaterial configurations of people, infrastructures, routines, and practices. Shows data care work is frequently backgrounded and assumed effortless d",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt Datenmanagement und -pflege in Organisationen (Sozialarbeit, Bildung), nicht aber KI/ML-Systeme. Es fehlt jede technische KI-Komponente (keine AI_Literacies, Generative_KI, Prompting oder KI_Sonstige). Das Thema ist zu weit entfernt von KI-Anwendungen. Nur eine SOZIAL-Kategorie e"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "49PPZJ7Z",
      "title": "Tensions in digital welfare states: Three perspectives on care and control",
      "author_year": "Zakharova et al. (2024)",
      "authors": "Zakharova, I.; Jarke, J.; Kaun, A.",
      "year": 2024,
      "doi": "10.1177/14407833241226800",
      "url": "",
      "abstract": "Examines tensions between care and control in digital welfare states, analyzing how welfare services increasingly rely on digital technologies and data systems. Develops three analytical perspectives: datafied care practices, algorithmic governance, and digitalized welfare encounters. Demonstrates how digitalization reshapes welfare provision by intensifying surveillance while potentially enabling new forms of care. Reveals fundamental contradictions where care logics and control logics coexist ",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper adressiert algorithmische Governance und digitale Systeme in Wohlfahrtsstaat (KI_Sonstige: Ja). Direkter Bezug zu Sozialarbeit durch Analyse von Welfare Services und deren Digitalisierung (Soziale_Arbeit: Ja). Thematisiert Spannungen zwischen Care und Control, was Fragen zu Überwachung und alg"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "NQEUZQF9",
      "title": "Datafied ageing futures: Regimes of anticipation and participatory futuring",
      "author_year": "Jarke & Manchester (2025)",
      "authors": "Jarke, J.; Manchester, H.",
      "year": 2025,
      "doi": "10.1177/20539517241306363",
      "url": "",
      "abstract": "Challenges regimes of anticipation suggesting datafied futures are inevitable, arguing futures are actively made through sociotechnical imaginaries promoted by powerful actors. Explores how to democratize futures-making regarding aging populations, critiquing how current anticipations around data-driven systems and ageist assumptions dominate discussions without adequate participation from affected populations. Develops participatory futuring methodology enabling diverse stakeholders, particular",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.82,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt Datengetriebene Systeme (KI_Sonstige) und deren Auswirkungen auf vulnerable Bevölkerungsgruppen (ältere Erwachsene). Adressiert Bias durch ageist assumptions (Bias_Ungleichheit), partizipative Inklusion marginalisierter Gruppen (Diversitaet). Verwendet implizit-feministische Perspekt"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "L6PH7GDL",
      "title": "Künstliche Intelligenz in der Sozialen Arbeit: Grundlagen für Theorie und Praxis",
      "author_year": "Linnemann et al. (2025)",
      "authors": "Linnemann, G.; Löhe, J.; Rottkemper, B.",
      "year": 2025,
      "doi": "",
      "url": "https://doi.org/10.3262/978-3-7799-8562-4",
      "abstract": "First major German-language systematic treatment of AI in social work from multiple perspectives. Bridges technological progress and ethics, treating AI theoretically and in practice-oriented applications. Addresses technical AI basics for social work, ethical and legal frameworks, bias and discrimination in training data, automation bias risks, development of AI competencies in education and organizations, and specific application fields. Emphasizes responsible, reflective engagement with AI en",
      "item_type": "book",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (KI-Kompetenzen in Bildung/Organisationen), KI_Sonstige (technische Grundlagen, Automatisierungsbias). SOZIAL: Soziale_Arbeit (Direktbezug zu Praxis, Theorie und Anwendungsfeldern), Bias_Ungleichheit (Diskriminierung in Trainingsdaten), Fairnes"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "QLXLEUCG",
      "title": "Der Einfluss der Algorithmen: Neue Qualitäten durch Big Data Analytics und Künstliche Intelligenz",
      "author_year": "Schneider & Seelmeyer (2018)",
      "authors": "Schneider, D.; Seelmeyer, U.",
      "year": 2018,
      "doi": "10.1007/s12054-018-0046-y",
      "url": "",
      "abstract": "Examines how algorithmic decision-making affects professional judgment formation and discretionary decision-making space for practitioners. Analyzes automation bias where professionals may over-rely on algorithmic recommendations without adequate critical evaluation. Stresses social work requires debate about what forms of knowledge new technologies can generate, where limits lie, and how it can meaningfully be incorporated into professional reflection and decision-making practices.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt substantiell AI_Literacies (kritische Reflexion über KI-Kompetenzen von Fachkräften, Automation Bias), KI_Sonstige (Algorithmen, Big Data Analytics), Soziale_Arbeit (direkter Bezug zur sozialen Arbeit und professionellen Entscheidungsfindung) und Bias_Ungleichheit (Automation Bias). "
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "4JN7NIS4",
      "title": "Künstliche Intelligenz in der Sozialen Arbeit: Grundlagen, Entwicklungen, Herausforderungen",
      "author_year": "Steiner & Tschopp (2022)",
      "authors": "Steiner, O.; Tschopp, D.",
      "year": 2022,
      "doi": "10.1007/s12054-022-00546-4",
      "url": "",
      "abstract": "Systematically analyzes two key AI application scenarios: Predictive Risk Modeling (PRM) and chatbots in counseling. Discusses neural networks' black box problem, dangers of case labeling through standardization, ethical questions of responsibility and liability when AI predictions diverge from professional judgment, and algorithmic bias risks perpetuating social inequalities. Uses Jonas' ethical theory of responsibility to emphasize ethical responsibility as foundational to all AI implementatio",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt KI-Anwendungen in der Sozialen Arbeit (Chatbots, Predictive Risk Modeling) und analysiert algorithmischen Bias sowie ethische Fragen. Es erfüllt beide Anforderungen: TECHNIK (Generative_KI für Chatbots, KI_Sonstige für PRM und neuronale Netze) und SOZIAL (Soziale_Arbeit als Fokus"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "GZ9B3GPD",
      "title": "Rationalisierung durch Digitalisierung?",
      "author_year": "Waag (2023)",
      "authors": "Waag, P.",
      "year": 2023,
      "doi": "10.1007/s12592-023-00472-6",
      "url": "",
      "abstract": "Contributes labor sociology and interaction sociology perspectives (particularly Luhmann's interaction theory) to digitalization analyses in social work. Examines potential advantages and disadvantages from multiple stakeholder perspectives (professionals, service users, organizations), revealing that fears and hopes regarding rationalization through digitalization are overly simplistic. Highlights irreducible complexity of professional helping relationships and fundamental limitations of applyi",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt Digitalisierung in der Sozialen Arbeit (erfüllt Soziale_Arbeit = Ja), thematisiert aber keine KI-Systeme, ML-Algorithmen oder andere technische KI-Aspekte (alle Technik-Kategorien = Nein). Der Fokus liegt auf Rationalisierungslogik und Interaktionssoziologie. Ohne TECHNIK-Kompone"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "MA3LBJS6",
      "title": "Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit: Eine exemplarische arbeitsfeldübergreifende Betrachtung des Natural Language Processing (NLP)",
      "author_year": "Linnemann et al. (2023)",
      "authors": "Linnemann, G. A.; Löhe, J.; Rottkemper, B.",
      "year": 2023,
      "doi": "10.1007/s12592-023-00455-7",
      "url": "",
      "abstract": "Examines Natural Language Processing technologies' significance for social work practice using Staub-Bernasconi's action theory and media equation theory. Analyzes how NLP implementation creates opportunities (enhanced participation, low-threshold access, broader data analysis) and risks (modularized separation of social work activities, potential dehumanization). Emphasizes critical examination of whether AI supports or displaces authentic social work functions.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Kriterien: (1) TECHNIK_OK: Behandelt AI_Literacies (kritische Reflexion von NLP-Technologien in sozialer Arbeit) und KI_Sonstige (NLP als spezifisches ML-System). (2) SOZIAL_OK: Direkter Fokus auf Soziale_Arbeit (arbeitsfeldübergreifende Analyse, Auswirkungen auf sozialarbeiteris"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "2FUXNFZS",
      "title": "RETHINKING SOCIAL SERVICES WITH ARTIFICIAL INTELLIGENCE: OPPORTUNITIES, RISKS, AND FUTURE PERSPECTIVES",
      "author_year": "[Author not specified] (2024)",
      "authors": "",
      "year": 2024,
      "doi": "",
      "url": "https://www.researchgate.net/publication/393509196_RETHINKING_SOCIAL_SERVICES_WITH_ARTIFICIAL_INTELLIGENCE_OPPORTUNITIES_RISKS_AND_FUTURE_PERSPECTIVES",
      "abstract": "This chapter explores the transformative potential of artificial intelligence (AI) in the field of social services. It highlights how AI—through data analysis, predictive modeling, and administrative automation—can enhance the effectiveness, accessibility, and efficiency of social work practice. The chapter also presents significant ethical concerns, including risks of algorithmic bias, loss of human connection, and violations of privacy. The author emphasizes that while AI can complement social",
      "item_type": "booksection",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch AI_Literacies (Vorbereitung von Social Workers auf KI-Nutzung), KI_Sonstige (Predictive Modeling, Algorithmen in Social Services) und Fairness. SOZIAL_OK durch Soziale_Arbeit (direkter Bezug zu Social Work Practice), Bias_Ungleichheit (algorithmic bi"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "HSQW48VE",
      "title": "Artificial Intelligence in Social Work: An EPIC Model for Practice",
      "author_year": "[Author not specified] (2025)",
      "authors": "",
      "year": 2025,
      "doi": "10.1080/0312407X.2025.2488345",
      "url": "https://www.tandfonline.com/doi/full/10.1080/0312407X.2025.2488345",
      "abstract": "As artificial intelligence (AI) permeates the workplace environments of social workers, there is a need to understand the risks and benefits posed to the mission and values of the profession. This article examines the influence of artificial intelligence on the profession, including opportunities to advance socially just outcomes and challenges that risk ethical practice. A comprehensive review of literature was conducted to examine existing research on the intersection of AI and social work. Dr",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (Kompetenzen/Verständnis von KI-Risiken/-Nutzen in der Praxis), KI_Sonstige (allgemeine KI im Kontext). SOZIAL: Soziale_Arbeit (direkter Bezug zu Profession, Praxis, Werten), Bias_Ungleichheit (sozial gerechte Outcomes, ethische Praxis), Fairne"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "SS5HTYY6",
      "title": "Artificial Intelligence in Social Sciences and Social Work: Bridging Technology and Humanity to Revolutionize Research, Policy, and Human Services",
      "author_year": "[Author not specified] (2025)",
      "authors": "",
      "year": 2025,
      "doi": "",
      "url": "https://www.multispecialityjournal.com/uploads/archives/20250920153430_MCR-2025-5-004.1.pdf",
      "abstract": "This review explores the transformative role of artificial intelligence (AI) in the fields of social sciences and social work, with a focus on developments from 2022 to 2025. It examines how AI technologies—such as machine learning, natural language processing—enhance the analysis of complex social phenomena, support real-time forecasting, and inform data-driven policymaking. Within social work and human services, AI-driven tools facilitate case management, mental health interventions, crisis re",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper ist ein Literaturreview, das KI-Technologien (Machine Learning, NLP) substantiell im Kontext Sozialer Arbeit und Human Services behandelt. Es adressiert Bias, Fairness und Ethical Concerns—zentrale soziale Aspekte. TECHNIK erfüllt (KI_Sonstige + AI_Literacies: Diskurs über KI-Integration)."
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "H59BNSX8",
      "title": "Algorithmic decision-making in social work practice and pedagogy: confronting the competency/critique dilemma",
      "author_year": "James et al. (2023)",
      "authors": "James, P.; Lal, J.; Liao, A.; Magee, L.; Soldatic, K.",
      "year": 2023,
      "doi": "10.1080/02615479.2023.2195425",
      "url": "https://www.tandfonline.com/doi/full/10.1080/02615479.2023.2195425",
      "abstract": "The world is experiencing an accelerating digital transformation. One aspect of this is the implementation of...[source](https://www.google.com/search?q=https://www.aminer.cn/pub/645d0410d68f896efa94d024/algorithmic-decision-making-in-social-work-practice-and-pedagogy-confronting-the-competency)",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper adressiert algorithmic decision-making in Sozialarbeit (KI_Sonstige + Soziale_Arbeit). Der Titel hebt die Competency/Critique-Dilemma hervor, was auf AI Literacies verweist. Bias, Fairness und ethische Bedenken in Algorithmen sind zentral (Bias_Ungleichheit + Fairness). TECHNIK erfüllt (3x"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "RAV6DAFQ",
      "title": "The End of the World as We Know It? ChatGPT and Social Work",
      "author_year": "Goldkind et al. (2023)",
      "authors": "Goldkind, L.; Wolf, L.; Glennon, A.; Rios, J.; Nissen, L.",
      "year": 2023,
      "doi": "10.1093/sw/swad044",
      "url": "https://doi.org/10.1093/sw/swad044",
      "abstract": "Brief commentary marking ChatGPT as a pivotal moment for social work. Encourages proactive engagement to steer AI toward just, human-centered outcomes and warns that non-engagement risks value misalignment and inequity.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI: ChatGPT; AI_Literacies: proaktive Auseinandersetzung mit KI-Kompetenzen für Profession) + SOZIAL (Soziale_Arbeit: direkter Bezug zur Profession; Bias_Ungleichheit: Warnung vor inequitable outcomes). Kommentar-Format mit konzeptionellem Fokus a"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "YRBP6IEJ",
      "title": "Introducing Generative Artificial Intelligence into the MSW Curriculum: A Proposal for the 2029 Educational Policy and Accreditation Standards",
      "author_year": "Rodriguez et al. (2024)",
      "authors": "Rodriguez, M. Y.; Goldkind, L.; Victor, B. G.; Hiltz, B. S.; Perron, B. E.",
      "year": 2024,
      "doi": "10.1080/10437797.2024.2340931",
      "url": "https://doi.org/10.1080/10437797.2024.2340931",
      "abstract": "Proposal to add an explicit AI competency to MSW accreditation. Outlines benefits and risks of generative AI, recommends curricular content on ethics, bias, transparency, and responsible use, and frames AI literacy as essential for safeguarding client dignity and equity while leveraging innovation.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt substantiell KI-Kompetenzen (AI_Literacies) und generative KI im MSW-Curriculum. Direkter Bezug zu Sozialer Arbeit (Ausbildung, Akkreditierungsstandards). Adressiert Bias, Transparenz und Fairness als zentrale Themen für verantwortungsvolle AI-Nutzung im Kontext von Klientenschutz un"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "JI24FQPV",
      "title": "Clinical Social Workers’ Perceptions of Large Language Models in Practice: Resistance to Automation and Prospects for Integration",
      "author_year": "Creswell Báez et al. (2025)",
      "authors": "Creswell Báez, J.; Ahn, E.; Tamietti, A.; Victor, B. G.; Goldkind, L.",
      "year": 2025,
      "doi": "10.1080/26408066.2025.2542450",
      "url": "https://doi.org/10.1080/26408066.2025.2542450",
      "abstract": "Qualitative study (interviews, reflexive thematic analysis) of clinicians exposed to LLM-supported consultation. Identifies efficiency and documentation support alongside concerns over confidentiality, loss of nuance, and reduced empathy. Concludes AI should augment—not replace—clinical judgment, requiring training and ethics.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt TECHNIK- und SOZIAL-Kriterien: (1) TECHNIK: Generative_KI (LLMs in klinischer Praxis) und AI_Literacies (Training, Kompetenzentwicklung im Umgang mit LLMs) sind substantiell. (2) SOZIAL: Direkter Bezug zu Sozialarbeiter:innen (clinical social workers) und ihrer Praxis. Qualitative Stud"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "LGYFN6JK",
      "title": "ChatGPT for Social Work Science: Ethical Challenges and Opportunities",
      "author_year": "Patton et al. (2023)",
      "authors": "Patton, D. U.; Landau, A.; Mathiyazhagan, S.",
      "year": 2023,
      "doi": "10.1086/726042",
      "url": "https://doi.org/10.1086/726042",
      "abstract": "Ethical framework for using LLMs in social work research. Recommends transparency, verification, authorship integrity, anti-plagiarism, and inclusion/social justice to counter bias. Positions LLMs as assistive tools requiring critical human oversight.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI: LLM-Fokus; AI_Literacies: kritischer Umgang mit KI-Systemen) und SOZIAL_OK (Soziale_Arbeit: Ethik für sozialarbeiterische Forschung; Bias_Ungleichheit & Fairness: explizit Bias-Bekämpfung und Inklusion/Social Justice). Substantieller Bezug "
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "X2QQ4JH6",
      "title": "Artificial Intelligence in Social Work: Emerging Ethical Issues",
      "author_year": "Reamer (2023)",
      "authors": "Reamer, F. G.",
      "year": 2023,
      "doi": "",
      "url": "https://jswve.org/wp-content/uploads/2023/10/10-020-205-IJSWVE-2023.pdf",
      "abstract": "Comprehensive ethical analysis of AI in social work covering consent, autonomy, privacy, transparency, algorithmic bias, and professional competence. Maps issues to the NASW Code and urges policies, training, and client options to opt out of AI-mediated services.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch AI_Literacies (ethische Kompetenzen, professionelle Ausbildung), KI_Sonstige (algorithmische Systeme in Sozialbereich) und Fairness (Bias, Transparenz). SOZIAL_OK durch Soziale_Arbeit (direkte Anwendung in SW-Praxis), Bias_Ungleichheit (algorithmic b"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "GXZWBNJU",
      "title": "Artificial Intelligence in Social Work: An EPIC Model for Practice",
      "author_year": "Boetto (2025)",
      "authors": "Boetto, H.",
      "year": 2025,
      "doi": "10.1080/0312407X.2025.2488345",
      "url": "https://doi.org/10.1080/0312407X.2025.2488345",
      "abstract": "Narrative review proposing the EPIC model—Ethics & justice, Policy, Intersectoral collaboration, Community engagement—to guide ethical AI integration. Balances efficiency opportunities with risks of bias and value erosion, advocating structured, human-centered adoption aligned with social justice.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige) + SOZIAL_OK (Soziale_Arbeit, Bias_Ungleichheit, Diversitaet, Fairness, Feministisch). EPIC-Modell adressiert KI-Kompetenzen und strukturierte AI-Integration in Sozialarbeit. Expliziter Fokus auf Bias-Risiken, soziale Gerechtigk"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "ZJS2J7E7",
      "title": "Artificial Intelligence (AI) Literacy for Social Work: Implications for Core Competencies",
      "author_year": "Ahn et al. (2025)",
      "authors": "Ahn, E.; Choi, M.; Fowler, P.; Song, I.",
      "year": 2025,
      "doi": "10.1086/735187",
      "url": "https://doi.org/10.1086/735187](https://doi.org/10.1086/735187",
      "abstract": "Conceptual paper arguing that AI literacy should be a core social work competency. It links “algorithmic literacy” to existing educational standards and details how AI systems can amplify inequities without critical oversight. The authors propose integrating critical evaluation of AI tools, bias detection, and ethical safeguards into curricula and practice to protect vulnerable populations and uphold social justice.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (Kernthema: AI Literacy als Kompetenz in Curriculum), KI_Sonstige (algorithmische Systeme und deren Auswirkungen). SOZIAL: Soziale_Arbeit (direkter Bezug zu SW-Kompetenzentwicklung und Schutz vulnerabler Populationen), Bias_Ungleichheit (kr"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "Y6M97SWQ",
      "title": "Algorithmic-Assisted Decision-Making Tools in Child Welfare Practice: A Systematic Review",
      "author_year": "Yu & Rose (2025)",
      "authors": "Yu, M.-H.; Rose, R. A.",
      "year": 2025,
      "doi": "10.1177/10497315251350933",
      "url": "https://doi.org/10.1177/10497315251350933",
      "abstract": "PRISMA-guided review of algorithmic tools in child welfare. Finds potential for consistency and early risk identification but significant concerns about bias, transparency, practitioner training, and stakeholder inclusion. Recommends audits, participatory design, and ethical guidelines; highlights evidence gaps.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper adressiert algorithmic decision-making tools (KI_Sonstige) in child welfare (Soziale_Arbeit). Zentrale Themen sind Bias und Fairness. Nennt Practitioner Training (AI_Literacies). Systematischer Review mit PRISMA-Methode. Erfüllt beide Bedingungen: TECHNIK (KI_Sonstige + AI_Literacies) UND SOZI"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "VICS443I",
      "title": "AI Creates the Message: Integrating AI Language Learning Models into Social Work Education and Practice",
      "author_year": "Singer et al. (2023)",
      "authors": "Singer, J. B.; Creswell Báez, J.; Rios, J. A.",
      "year": 2023,
      "doi": "10.1080/10437797.2023.2189878",
      "url": "https://doi.org/10.1080/10437797.2023.2189878",
      "abstract": "Commentary advocating cautious integration of LLMs in teaching and practice. Describes pedagogical uses (idea generation, material tailoring) and warns of bias, factual errors, confidentiality risks, and plagiarism. Emphasizes transparency policies and that engagement with AI is ethically preferable to avoidance.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Bedingungen: (1) TECHNIK: Generative_KI (LLMs in Social Work) + AI_Literacies (Pädagogische Integration, Kompetenzentwicklung). (2) SOZIAL: Soziale_Arbeit (direkter Bezug zu Lehre und Praxis Sozialer Arbeit) + Bias_Ungleichheit (Warnung vor Bias, faktischen Fehlern). Substantiell"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "R5QQTD95",
      "title": "Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen Arbeit – Eine empirische Untersuchung aus der Perspektive von Berufseinsteiger:innen",
      "author_year": "Schönauer (2025)",
      "authors": "Schönauer, A.-L.",
      "year": 2025,
      "doi": "10.1007/s12054-025-00783-3",
      "url": "https://doi.org/10.1007/s12054-025-00783-3",
      "abstract": "Empirischer Kurzbeitrag zu Einstellungen von Berufseinsteiger:innen: überwiegend kritische Haltung gegenüber KI in direkter Klient:innenarbeit; Datenschutz- und Empathiefragen zentral. Akzeptanz steigt mit digitaler Kompetenz; Empfehlung: Aus- und Weiterbildung für kritische KI-Literacy und partizipative Technikgestaltung.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch AI_Literacies (Fokus auf digitale Kompetenz und kritische KI-Literacy in Aus- und Weiterbildung) und KI_Sonstige (Organisationale KI-Rahmenbedingungen). SOZIAL_OK durch Soziale_Arbeit (direkter Bezug zu Berufseinsteiger:innen und Klientenarbeit in de"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "JNLPSHD5",
      "title": "Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit: Eine exemplarische arbeitsfeldübergreifende Betrachtung des Natural Language Processing (NLP)",
      "author_year": "Linnemann et al. (2023)",
      "authors": "Linnemann, G. A.; Löhe, J.; Rottkemper, B.",
      "year": 2023,
      "doi": "10.1007/s12592-023-00455-7",
      "url": "https://link.springer.com/article/10.1007/s12592-023-00455-7",
      "abstract": "Theoriegeleitete Analyse zu Chancen und Risiken von NLP für die Soziale Arbeit. Diskutiert mögliche Auslagerung sozialarbeiterischer Tätigkeiten vs. Potenziale für Teilhabe und Zugänge; ordnet KI entlang handlungstheoretischer Konzepte ein und plädiert für kritische Auseinandersetzung im Einklang mit professionsethischen Werten.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt NLP als KI-Technologie (KI_Sonstige: Ja) und adressiert direkt sozialarbeiterische Praxis, Theorie und professionsethische Fragen (Soziale_Arbeit: Ja). Kritische Auseinandersetzung mit KI-Chancen/-Risiken impliziert auch AI Literacy-Anforderungen (AI_Literacies: Ja). TECHNIK-Kriteriu"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "H7E3N6VR",
      "title": "Diskriminierung durch Algorithmen – Überlegungen zur Stärkung KI-bezogener Kompetenzen",
      "author_year": "Sūna et al. (2024)",
      "authors": "Sūna, L.; Hoffmann, D.; Mollen, A.",
      "year": 2024,
      "doi": "",
      "url": "https://www.gmk-net.de/wp-content/uploads/2024/12/gmk60_suna_hoffmann_mollen.pdf",
      "abstract": "Konzeptioneller Beitrag zu Ursachen und Formen algorithmischer Diskriminierung und zur Förderung kritischer KI-Kompetenzen. Plädiert für Aufklärung zu Datenbias, reflexive Nutzung und partizipative Trainings, um Benachteiligungen zu erkennen und digitale Teilhabe zu stärken.",
      "item_type": "booksection",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt algorithmische Diskriminierung (KI_Sonstige: Ja) und kritische KI-Kompetenzen (AI_Literacies: Ja). Es adressiert substantiell Bias, Ungleichheit (Bias_Ungleichheit: Ja), digitale Teilhabe/Inklusion (Diversitaet: Ja) und implizit Fairness durch Fokus auf Benachteiligungen. TECHNIK"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "MITLDF9S",
      "title": "KI-basiertes Assistenzsystem im Kinderschutzverfahren",
      "author_year": "Feist-Ortmanns et al. (2025)",
      "authors": "Feist-Ortmanns, M.; Sauer, A.; Brinkmann, M.",
      "year": 2025,
      "doi": "",
      "url": "https://afet-ev.de/assets/themenplattform/KI-in-der-Kinder--und-Jugendhilfe-Naher-Grasshoff-Forum-2.pdf",
      "abstract": "Praxisnaher Bericht zu einem KI-Assistenzsystem für Gefährdungseinschätzung. Betont Human-in-the-Loop, Datenschutz, Organisationsentwicklung und Schulungsbedarf; zeigt Potenziale (präventive Hinweise) und Risiken (Bias, Fehlklassifikationen) im sensiblen Kinderschutzkontext.",
      "item_type": "booksection",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmisches Entscheidungssystem in Kinderschutz; AI_Literacies: Schulungsbedarf erwähnt) + SOZIAL_OK (Soziale_Arbeit: Gefährdungseinschätzung im Kinderschutz; Bias_Ungleichheit: explizite Bias-Risiken; Fairness: Human-in-the-Loop und Fehl"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "D4FY2S3R",
      "title": "Künstliche Intelligenz in der Sozialen Arbeit – Zwischen Bedenken und Optionen",
      "author_year": "Gravelmann (2024)",
      "authors": "Gravelmann, R.",
      "year": 2024,
      "doi": "",
      "url": "https://www.beltz.de/fachmedien/sozialpaedagogik_soziale_arbeit/zeitschriften/theorie_und_praxis_der_sozialen_arbeit/artikel/53521-kuenstliche-intelligenz-in-der-sozialen-arbeit-zwischen-bedenken-und-optionen.html",
      "abstract": "Überblicksbeitrag zu Status, Risiken und Potenzialen von KI in Praxisfeldern der Sozialen Arbeit. Betont Spannungen zwischen Effizienzgewinnen und Werten wie Menschenwürde; skizziert Handlungsfelder und fordert professionellen Diskurs sowie reflektierte Implementationsstrategien.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt KI in der Sozialen Arbeit und adressiert dabei sowohl Kompetenzentwicklung (AI Literacies: reflektierte Implementationsstrategien, professioneller Diskurs) als auch allgemeine KI/algorithmische Systeme (KI_Sonstige) in sozialarbeiterischen Praxisfeldern (Soziale_Arbeit: direkter "
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "DGLWM93D",
      "title": "Voll (dia)logisch? Ein Werkstattbericht über den Einsatz von generativer KI in der Hochschulbildung für Soziale Arbeit – Curriculare Überlegungen und veränderte Akteurskonstellationen",
      "author_year": "Engelhardt & Ley (2025)",
      "authors": "Engelhardt, E.; Ley, T.",
      "year": 2025,
      "doi": "",
      "url": "https://www.pedocs.de/volltexte/2025/33133/pdf/Engelhardt_Ley_2025_Voll_dia_logisch.pdf",
      "abstract": "Werkstattbericht zur curricularen Integration generativer KI. Positioniert Prompting als metakognitive Schlüsselkompetenz und diskutiert Rollenwandel von Lehr-/Lernakteuren; fordert reflektierte, ethisch eingebettete Nutzung mit Fokus auf kritische Validierung von KI-Ergebnissen.",
      "item_type": "booksection",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, Generative_KI, Prompting sind substantiell) + SOZIAL_OK (Soziale_Arbeit durch expliziten Hochschulbezug zur Soziale-Arbeit-Ausbildung). Prompting als metakognitive Schlüsselkompetenz ist Kernthema. Curriculare Integration in Soziale Arbeit "
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "EB7PZUZZ",
      "title": "Prompten nach Plan: Das PCRR-Framework als pädagogisches Werkzeug für den Einsatz von Künstlicher Intelligenz.",
      "author_year": "Freinhofer et al. (2025)",
      "authors": "Freinhofer, Dominik; Schwabl, Gerlinde; Aichinger, Susanne; Breitenberger, Sandra; Steindl, Sandra; Hechenberger, Tanja",
      "year": 2025,
      "doi": "10.21243/mi-01-25-26",
      "url": "https://journals.univie.ac.at/index.php/mp/article/view/9274",
      "abstract": "Die rasante Entwicklung generativer Künstlicher Intelligenz (KI) macht Prompt Engineering zu einer Schlüsselkompetenz für den kompetenten Umgang mit KI-Modellen. Während zahlreiche Prompting-Techniken und -Frameworks existieren, fehlt bislang eine systematische Integration in den schulischen Kontext. Diese Publikation stellt das PCRR-Framework (Plan – Create – Review – Reflect) vor, das als ganzheitlicher Ansatz für den Einsatz von Prompt Engineering im Unterricht dient. Basierend auf Erfahrunge",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper konzentriert sich auf AI Literacy und Prompting-Strategien (TECHNIK-Anforderung erfüllt), fehlt aber vollständig die SOZIAL-Komponente. Es diskutiert weder Bias, Ungleichheit, Gender-Perspektiven, Fairness noch direkte sozialarbeiterische Anwendungen. Rein schulpädagogisches Framework ohne"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "7FBB2KUC",
      "title": "Bedeutung von Künstlicher Intelligenz in der Sozialen Arbeit",
      "author_year": "Linnemann et al. (2023)",
      "authors": "Linnemann, Gesa Alena; Löhe, Julian; Rottkemper, Beate",
      "year": 2023,
      "doi": "10.1007/s12592-023-00455-7",
      "url": "https://doi.org/10.1007/s12592-023-00455-7",
      "abstract": "Die Bedeutung des Einsatzes von Verfahren, die unter dem Begriff der Künstlichen Intelligenz (KI) zusammenzufassen sind, wird sowohl für gesellschaftliche Prozesse als auch den Auftrag an die Soziale Arbeit zunehmend erkannt und diskutiert. Mit diesem Artikel wird ein Beitrag zum Diskurs geleistet, indem vertieft der Bereich der Sprachverarbeitung durch KI, das Natural Language Processing (NLP), in den Blick genommen wird. Verarbeitung natürlicher Sprache ist aufgrund der hohen Bedeutung kommuni",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper adressiert KI_Sonstige durch Natural Language Processing (NLP), ein klassisches ML-Thema, und hat starken direkten Bezug zu Sozialer Arbeit (Staub-Bernasconi, Klient*innen, sozialarbeiterische Praxis). Beide Bedingungen erfüllt (TECHNIK + SOZIAL). Theoretisch-diskursiver Charakter ohne empiris"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "6N2E242K",
      "title": "Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen Arbeit",
      "author_year": "Schönauer (2025)",
      "authors": "Schönauer, Anna-Lena",
      "year": 2025,
      "doi": "10.1007/s12054-025-00783-3",
      "url": "https://doi.org/10.1007/s12054-025-00783-3",
      "abstract": "Der Artikel untersucht die Akzeptanz und organisationalen Rahmenbedingungen digitaler Technologien, insbesondere Künstlicher Intelligenz (KI), in der Praxis der Sozialen Arbeit aus Sicht der Fachkräfte. Während digitale Technologien im administrativen Bereich bereits weit verbreitet sind, zeigt sich bei der Nutzung digitaler Technologien in der direkten Arbeit mit den Klient:innen noch Zurückhaltung. Durch die Entwicklungen im Bereich KI ergeben sich zunehmend neue Möglichkeiten digitale Technol",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt TECHNIK-Bedingung: AI_Literacies (Ja - untersucht digitale Kompetenzen und KI-Akzeptanz von Fachkräften) und KI_Sonstige (Ja - behandelt KI-Einsatz in Organisationen). Erfüllt SOZIAL-Bedingung: Soziale_Arbeit (Ja - direkter Fokus auf KI-Akzeptanz und Praxis in der Sozialen Arbeit, Bede"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "HT9ZSI9U",
      "title": "Handbuch Soziale Arbeit und Digitalisierung",
      "author_year": "Kutscher et al. (2020)",
      "authors": "Kutscher, Nadia; Ley, Thomas; Seelmeyer, Udo; Siller, Friederike; Tillmann, Angela; Zorn, Isabel",
      "year": 2020,
      "doi": "",
      "url": "https://www.beltz.de/fileadmin/beltz/leseproben/978-3-7799-5258-9.pdf",
      "abstract": "Dieses umfassende Handbuch mit über 50 Beiträgen behandelt erstmals systematisch Digitalisierung in Bezug auf Disziplin und Praxis der Sozialen Arbeit. Das 658-seitige Werk beleuchtet aus verschiedenen disziplinären Perspektiven gesellschaftliche Entwicklungen, Diskurse, digitalisierte Formen der Dienstleistungserbringung, Profession, Organisation und Handlungsfelder sowie neue Herausforderungen für Forschung. Zentrale Themen umfassen Mediatisierung, Akteur-Netzwerk-Theorie, ethische Fragen, inf",
      "item_type": "book",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Handbuch behandelt Digitalisierung in der Sozialen Arbeit substantiell (Soziale_Arbeit = Ja). Allerdings fehlt der erforderliche TECHNIK-Aspekt vollständig: Es gibt keinen Bezug zu KI-Systemen, Machine Learning, generativer KI oder KI-Kompetenzen. Das Werk konzentriert sich auf breite Digitalisi"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "79TL6HSB",
      "title": "Social work and artificial intelligence: Collaboration and challenges",
      "author_year": "Chen & Lin (2025)",
      "authors": "Chen, Y.-C.; Lin, C.-C.",
      "year": 2025,
      "doi": "",
      "url": "https://dmjr-journals.com/assets/article/1755892935-SOCIAL_WORK_&_ARTIFICIAL_INTELLIGENCE-_COLLABORATION_AND_CHALLENGES.pdf",
      "abstract": "This qualitative study explores current AI applications in social work through interviews with professionals, AI developers, and policymakers, identifying challenges including insufficient decision-making transparency, gaps in ethical frameworks, and inadequate technical literacy among professionals. The research reveals that while ninety percent of social work professionals acknowledge AI's auxiliary function in daily operations, concerns persist about automation bias and the potential undermin",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt substantiell AI Literacy (Gaps in technical literacy, educational training), KI allgemein (Automation Bias, explainable AI), Soziale Arbeit (AI applications in social work, professional autonomy), Bias (automation bias), und Fairness (ethical governance, transparency). Beide Bedi"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "PAZJHB8J",
      "title": "The role of artificial intelligence (AI) and machine learning in social work practice",
      "author_year": "Nuwasiima et al. (2024)",
      "authors": "Nuwasiima, M.; Ahonon, M. P.; Kadiri, C.",
      "year": 2024,
      "doi": "10.30574/wjarr.2024.24.1.2998",
      "url": "https://doi.org/10.30574/wjarr.2024.24.1.2998",
      "abstract": "This comprehensive review identifies algorithmic bias as a critical challenge in social work AI implementation, noting that algorithms trained on historical data may perpetuate existing inequalities by replicating racial, gender, and socio-economic disparities. The authors document specific cases where predictive analytics tools disproportionately flagged families of color for child welfare interventions despite lacking substantial evidence of higher abuse rates. The study emphasizes that addres",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Predictive Analytics, algorithmische Systeme; AI_Literacies: Involvement von Social Workers in AI-Entwicklung) und SOZIAL_OK (Soziale_Arbeit: direkter Bezug zu SW-Praxis und Kinderschutz; Bias_Ungleichheit: algorithmischer Bias gegen Fami"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "YTDMY5W4",
      "title": "A framework for the learning and teaching of Critical AI Literacy skills (Version 0.1)",
      "author_year": "Hauck et al. (2025)",
      "authors": "Hauck, M.; Moore, E.; Wright, C.",
      "year": 2025,
      "doi": "",
      "url": "https://www.open.ac.uk/blogs/learning-design/wp-content/uploads/2025/01/OU-Critical-AI-Literacy-framework-2025-external-sharing.pdf",
      "abstract": "This framework defines Critical AI Literacy as expanding beyond traditional AI literacy to examine how Large Language Models contribute to ongoing epistemic injustices that can lead to significant social and personal harm. It applies equality, diversity, inclusion, and accessibility principles to AI use, emphasizing the importance of critically evaluating AI-generated outputs and engaging in equitable and inclusive prompting practices. Critical AI Literacy is conceptualized as context-specific a",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper entwickelt ein Framework für Critical AI Literacy (TECHNIK: AI_Literacies, Generative_KI, Prompting). Substantieller Fokus auf Bias, Ungleichheit, Diversität und Fairness (SOZIAL) durch explizite Thematisierung von Epistemic Injustices, Power Relationships, Inequalities und Equity/Inclusion-Pr"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "PYN6HB3E",
      "title": "Social work in the age of artificial intelligence: A rights-based framework for evidence-based practice through social psychology, group dynamics, and institutional analysis",
      "author_year": "Alam (2025)",
      "authors": "Alam, N.",
      "year": 2025,
      "doi": "10.1080/26408066.2025.2547219",
      "url": "https://doi.org/10.1080/26408066.2025.2547219",
      "abstract": "This study develops a comprehensive rights-based framework for navigating AI integration in social work practice while addressing ethical implications across micro, meso, and macro practice levels. The framework bridges social work theory with interdisciplinary insights, demonstrating that AI systems profoundly impact vulnerable populations by mediating interpersonal relationships and constructing meaning in AI-mediated environments. It provides evidence-based guidance for practitioners to harne",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige) durch Framework für AI-Integration in Sozialarbeit; SOZIAL_OK (Soziale_Arbeit, Bias_Ungleichheit, Diversitaet, Fairness) durch expliziten Fokus auf vulnerable Populationen, Gerechtigkeit, Menschenrechte und ethische Implikation"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "THVKEETD",
      "title": "Artificial Intelligence (AI) literacy for social work",
      "author_year": "Ahn (2025)",
      "authors": "Ahn, E.",
      "year": 2025,
      "doi": "10.1086/735187",
      "url": "https://doi.org/10.1086/735187",
      "abstract": "This work explores how AI literacy, defined as the knowledge and skills to understand, use, and critically evaluate AI systems, can enhance social workers' ability to navigate technological integration while maintaining professional values. It emphasizes that embedding AI literacy into core competencies enables social workers to better address emerging challenges and promote equity in an AI-influenced society. The author argues that AI literacy must go beyond technical skills to include critical",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt AI Literacy substantiell als Kernkompetenz für Sozialarbeiter (AI_Literacies: Ja). Es hat direkten Bezug zu Soziale_Arbeit (Ja) durch explizite Fokussierung auf sozialarbeiterische Praxis und Professionalisierung. Bias und ethische Implikationen von KI werden adressiert (Bias_Ung"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "CF6T2RD7",
      "title": "Artificial intelligence in social work: Emerging ethical issues",
      "author_year": "Reamer (2023)",
      "authors": "Reamer, F. G.",
      "year": 2023,
      "doi": "",
      "url": "https://jswve.org/volume-20/issue-2/item-05/",
      "abstract": "This comprehensive analysis identifies key ethical challenges in social work's adoption of AI, including informed consent and client autonomy, privacy and confidentiality, transparency, client misdiagnosis, algorithmic bias and unfairness, and the need for evidence-based AI tools. The author proposes a concrete ethics-informed protocol for AI implementation, advocating for digital ethics steering committees and transparent, auditable methodologies. The work stresses that social workers must fami",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies: Fokus auf AI Literacy in Social Work Education; KI_Sonstige: algorithmische Systeme im Sozialbereich). SOZIAL (Soziale_Arbeit: direkter Bezug zu sozialarbeiterischer Praxis und Ausbildung; Bias_Ungleichheit: algorithmic bias; Fairness: unf"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "HZKE8T8I",
      "title": "Künstliche Intelligenz in der Sozialen Arbeit – Zwischen Bedenken und Optionen",
      "author_year": "Gravelmann (2024)",
      "authors": "Gravelmann, R.",
      "year": 2024,
      "doi": "",
      "url": "https://content-select.com/de/portal/media/view/66472512-3a7c-4fef-b8e6-a5d3ac1b0002",
      "abstract": "Gravelmann analyzes the impact of AI on the social work profession, identifying both opportunities and risks. Potential benefits include AI as communication aid, documentation tool, and data analysis instrument. Critical concerns include the danger of decision delegation to AI systems, potentially reducing professionals to executors. The author warns against automated AI-based procedures that massively intervene in people's lives, especially in child protection. Ethical problems are identified i",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: (1) TECHNIK: KI_Sonstige=Ja (algorithmische Systeme in Sozialer Arbeit, insbesondere in Kindesschutz). (2) SOZIAL: Soziale_Arbeit=Ja (direkter Fokus auf Auswirkungen auf Profession) und Bias_Ungleichheit=Ja (kritische Analyse von Diskriminierung und Machtstruktur"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "LXKXPD5H",
      "title": "Digitale Werkzeuge und Machtasymmetrien?",
      "author_year": "Studeny (2025)",
      "authors": "Studeny, S.",
      "year": 2025,
      "doi": "",
      "url": "https://www.ogsa.at/wp-content/uploads/2025/03/ogsaTAGUNG2025_AG-Digitalisierung_Handout-Machtasymmetrien-Susanne-Studeny.pdf",
      "abstract": "Studeny analyzes power asymmetries in digital social work, emphasizing that digital tools and algorithms create new, often invisible forms of power and control. AI decisions remove influence from both professionals and clients while responsibility remains unclear. Algorithms reinforce discrimination as they work with biased data. The author demands that social work critically reflects on digital technologies, demands transparency, and ensures that technology serves people.",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Einschlusskriterien: TECHNIK_OK (KI_Sonstige: Algorithmen in digitaler Sozialer Arbeit; AI_Literacies: kritische Reflexion digitaler Technologien) und SOZIAL_OK (Soziale_Arbeit: direkter Bezug zu sozialarbeiterischer Praxis; Bias_Ungleichheit: Diskriminierung durch biased data; F"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "9832ZJB7",
      "title": "Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden für KI in der Verwaltung, Version 1.0",
      "author_year": "Biegelbauer et al. (2023)",
      "authors": "Biegelbauer, P.; Lackinger, C.; Schlarb, S.; Subak, E.; Weinlinger, P.",
      "year": 2023,
      "doi": "",
      "url": "https://oeffentlicherdienst.gv.at/wp-content/uploads/2023/11/Leitfaden-Digitale-Verwaltung-Ethik.pdf",
      "abstract": "This guideline defines AI literacy as the ability to understand and use AI, emphasizing that safe, self-determined, and responsible use requires sufficient understanding of the technology's functioning, possibilities, and challenges. It identifies automation bias as a central risk and emphasizes competency building and training as the foundation for all further measures, recommending the creation of educational standards for AI procurement and application.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Der Leitfaden adressiert AI Literacy und KI-Risiken in der Verwaltung substantiell. Das Konzept von 'automation bias' ist technisch orientiert (KI-Versagen), nicht sozial (Diskriminierung von Gruppen). Fehlender Bezug zu Sozialer Arbeit, Gender, Diversität oder kritischen sozialen Perspektiven führt"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "DJEVSR8D",
      "title": "AI FORA – Artificial Intelligence for Assessment: Fairness bei der Verteilung öffentlicher sozialer Leistungen",
      "author_year": "Ahrweiler (2025)",
      "authors": "Ahrweiler, P.",
      "year": 2025,
      "doi": "",
      "url": "https://nachrichten.idw-online.de/2025/03/18/wie-koennte-kuenstliche-intelligenz-weltweit-fairness-bei-der-verteilung-oeffentlicher-sozialer-leistungen-erhoehen",
      "abstract": "This international research project examined AI-supported social assessments across nine countries on four continents. The study demonstrates that justice criteria for receiving state benefits are culture- and context-dependent. A central finding is that deploying a standardized AI system globally is insufficient; instead, flexible, dynamic, and adaptive systems are required. Development of such systems depends on contributions from all societal actors, including vulnerable groups, for designing",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper adressiert algorithmische Entscheidungssysteme (KI_Sonstige: Ja) in der Sozialleistungsverwaltung (Soziale_Arbeit: Ja). Zentrale Themen sind Fairness bei der Ressourcenverteilung (Fairness: Ja), kulturelle/kontextuelle Ungleichheiten (Bias_Ungleichheit: Ja) und inklusive Partizipation von marg"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "QVNJIQJG",
      "title": "CAIL – Critical AI Literacy: Kritische Technikkompetenz für konstruktiven Umgang mit KI-basierter Technologie in Betrieben",
      "author_year": "Strauß (2024)",
      "authors": "Strauß, S.",
      "year": 2024,
      "doi": "",
      "url": "https://wien.arbeiterkammer.at/service/digifonds/gefoerderte-projekte/CAIL_Bericht-FINAL.pdf",
      "abstract": "This study develops a Critical AI Literacy framework defined as critical competency for assessing practical utility and limitations of AI applications in specific contexts. It emphasizes that AI-based automation is more complex, dynamic, and volatile than classical automation forms, creating new challenges. A central finding is that critical AI competency becomes part of knowledge work, as interpretation and verification of AI results remains an essential human task. The project identifies deep ",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt Critical AI Literacy und Automation Bias (TECHNIK: AI_Literacies + KI_Sonstige erfüllt). Der Kontext ist jedoch betrieblich/wirtschaftlich orientiert, nicht sozialarbeiterisch. Bias_Ungleichheit ist nur marginal via 'automation bias' berührt, nicht substantiell. SOZIAL-Kriterien "
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "UHEM78MX",
      "title": "What is AI Literacy? Competencies and Design Considerations",
      "author_year": "Long & Magerko (2020)",
      "authors": "Long, Duri; Magerko, Brian",
      "year": 2020,
      "doi": "10.1145/3313831.3376727",
      "url": "https://dl.acm.org/doi/10.1145/3313831.3376727",
      "abstract": "",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.7,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt AI Literacy und Kompetenzenrahmen (erfüllt TECHNIK-Kriterium). Allerdings erfüllt es KEINE der SOZIAL-Kategorien: Es gibt keinen Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder Fairness. Es ist ein rein bildungstechnologisches Konzept-Paper ohne sozialarbeite"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "JHRVDXSD",
      "title": "Artificial Intelligence (AI) Literacy for Social Work: Implications for Core Competencies",
      "author_year": "Ahn et al. (2025)",
      "authors": "Ahn, Eunhye; Choi, Moon; Fowler, Patrick; Song, In Han",
      "year": 2025,
      "doi": "10.1086/735187",
      "url": "https://www.journals.uchicago.edu/doi/10.1086/735187",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt explizit AI Literacy im Kontext von Social Work und deren Implikationen für Kernkompetenzen. Dies erfüllt beide Bedingungen: (1) TECHNIK: AI_Literacies = Ja (KI-Kompetenzen für Sozialarbeit), (2) SOZIAL: Soziale_Arbeit = Ja (direkter Bezug zu sozialarbeiterischer Praxis und Ausbi"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "2MVSL3UV",
      "title": "Learning About AI: A Systematic Review of Reviews on AI Literacy",
      "author_year": "Zhang et al. (2025)",
      "authors": "Zhang, Shan; Ganapathy Prasad, Priyadharshini; Schroeder, Noah L.",
      "year": 2025,
      "doi": "10.1177/07356331251342081",
      "url": "https://journals.sagepub.com/doi/10.1177/07356331251342081",
      "abstract": "Given the ubiquity of artificial intelligence (AI), it is essential to empower students to become creators, designers, and producers of AI technologies, rather than limiting them to the role of informed consumers. To achieve this, learners need to be equipped with AI knowledge and concepts and develop AI literacy. Paradoxically, it is largely unclear what AI literacy is and how we should learn and teach it. We address both of these questions through a systematic review of systematic reviews, als",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt AI Literacy substantiell (Kategorie AI_Literacies = Ja) und ist ein hochwertiger Umbrella Review. Allerdings erfüllt es nicht die SOZIAL-Bedingung: Es gibt keinen direkten Bezug zu Sozialer Arbeit, keine Analyse von Bias/Ungleichheit, kein Gender/Diversitäts-Fokus und keine Fairn"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "UKKQKL7I",
      "title": "Fragile Foundations: Hidden Risks of Generative AI",
      "author_year": "Washington (2025)",
      "authors": "Washington, Anne L.",
      "year": 2025,
      "doi": "10.11586/2025078",
      "url": "https://www.bertelsmann-stiftung.de/doi/10.11586/2025078",
      "abstract": "Foundation models are the backbone of generative AI and thus central to applications such as ChatGPT, Gemini, or Copilot. However, their use comes with risks: from randomly compiled training data and opaque processes to profit-driven business models.\n\nThe new report Fragile Foundations: Hidden Risks of Generative AI by the Bertelsmann Stiftung shows why mission-driven organizations in particular should critically question the foundations of AI. It highlights the systemic weaknesses of foundation",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt substantiell Generative KI (Foundation Models, ChatGPT, Gemini) mit kritischem Fokus. Es adressiert Systemische Risiken und Gefahren für vulnerable Gruppen (Bias_Ungleichheit) sowie kritisches Verständnis der KI-Technologie (AI_Literacies). Die Kombination TECHNIK (Generative_KI "
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "8MRNK6FX",
      "title": "Research on the application risks and countermeasures of ChatGPT generative artificial intelligence in social work",
      "author_year": "[Author not specified] (2024)",
      "authors": "",
      "year": 2024,
      "doi": "10.23977/jaip.2024.070222",
      "url": "https://www.clausiuspress.com/article/12988.html",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Der Titel signalisiert explizit: (1) Generative KI (ChatGPT), (2) Soziale Arbeit, (3) Anwendungsrisiken und Gegenmaßnahmen. Dies erfüllt beide Inklusionsbedingungen: TECHNIK_OK (Generative_KI=Ja), SOZIAL_OK (Soziale_Arbeit=Ja). AI_Literacies wird mit Ja bewertet, da Risiken/Gegenmaßnahmen KI-Kompete"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "SSF5Q33W",
      "title": "Research on the application risks and countermeasures of ChatGPT generative artificial intelligence in social work",
      "author_year": "[Author not specified] (2024)",
      "authors": "",
      "year": 2024,
      "doi": "10.23977/jaip.2024.070222",
      "url": "https://www.clausiuspress.com/article/12988.html",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Der Titel deutet auf eine Analyse von ChatGPT (generative KI) in der Sozialen Arbeit hin. Dies erfüllt beide notwendigen Bedingungen: TECHNIK (Generative_KI = Ja, da ChatGPT explizit genannt) und SOZIAL (Soziale_Arbeit = Ja, da direkt im Titel). AI_Literacies ist ebenfalls Ja, da es um Kompetenzen/R"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "Prompting",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ]
      }
    },
    {
      "id": "LPMP8QAY",
      "title": "Messung von AI Literacy – Empirische Evidenz und Implikationen",
      "author_year": "Weber et al. (2023)",
      "authors": "Weber, Patrick; Baum, Lorenz; Pinski, Marc",
      "year": 2023,
      "doi": "",
      "url": "https://aisel.aisnet.org/wi2023/3",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.65,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Titel deutet auf AI Literacy hin (Technik-Komponente erfüllt). Jedoch fehlt Abstract und Volltextinformation. Ohne Textmaterial kann nicht überprüft werden, ob die SOZIAL-Komponente erfüllt ist (Bezug zu Sozialer Arbeit, Bias, Gender, Diversität, Fairness oder Feministik). Konservative Bewertung: Ex"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "KQ5A8D6E",
      "title": "AI competency framework for students",
      "author_year": "[Author not specified] (2024)",
      "authors": "",
      "year": 2024,
      "doi": "",
      "url": "https://unesdoc.unesco.org/ark:/48223/pf0000391105",
      "abstract": "",
      "item_type": "book",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.3,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Der Titel deutet auf AI Literacies (Kompetenzen für Studierende) hin, was die TECHNIK-Bedingung erfüllen könnte. Jedoch fehlt das Abstract und der vollständige Text, was eine fundierte Bewertung unmöglich macht. Zudem ist unklar, ob ein Sozialbezug vorliegt. Ohne Volltext kann nicht überprüft werden"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "K3YCLBXK",
      "title": "AI Literacy: A Framework to Understand, Evaluate, and Use Emerging Technology",
      "author_year": "Ruiz et al. (2024)",
      "authors": "Ruiz, Pati; Mills, Kelly; Lee, Keun-woo; Coenraad, Merijke; Fusco, Judi; Roschelle, Jeremy; Weisgrau, Josh",
      "year": 2024,
      "doi": "",
      "url": "https://hdl.handle.net/20.500.12265/218",
      "abstract": "To enable all who participate in educational settings to leverage AI tools for powerful learning, this paper describes a framework and strategies for educational leaders to design and implement a clear approach to AI Literacy for their specific audiences (e.g. learners, teachers, or others) that are safe and effective. The first part of the paper describes a framework that identifies essential components of AI Literacy and connects them to existing initiatives. The second part of the paper ident",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt AI Literacy im Bildungskontext (PK-12 Education) und erfüllt die TECHNIK-Bedingung durch AI_Literacies = Ja. Es fehlt jedoch jegliche SOZIAL-Komponente: Kein Bezug zu Sozialer Arbeit, keine Behandlung von Bias/Ungleichheit, Gender, Diversität, feministischen Perspektiven oder Fai"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "BFG8VUK3",
      "title": "Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy",
      "author_year": "Quaid-i-Azam University, Islamabad, Pakistan & Shah (2025)",
      "authors": "Shah, Syed",
      "year": 2025,
      "doi": "10.70389/PJAI.1000088",
      "url": "https://premierscience.com/pjai-24-524/",
      "abstract": "Purpose\n\nThis narrative review investigates the interplay between gender bias in artificial intelligence (AI) systems and the potential of digital literacy to empower women in technology. By synthesising research from 2010 to 2024, the study examines how gender bias manifests in AI, its impact on women’s participation in technology, and the effectiveness of digital literacy initiatives in addressing these disparities.\n\nMethods\n\nA systematic literature search was conducted across major academic d",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (digitale Kompetenzen zur Empowerment), KI_Sonstige (algorithmische Systeme in Recruitment, Healthcare, Finance). SOZIAL: Gender (expliziter Gender-Bias-Fokus), Bias_Ungleichheit (Diskriminierung durch biased datasets und unter-representati"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "FTJM5R8N",
      "title": "Defeating Nondeterminism in LLM Inference",
      "author_year": "[Author not specified] (n.d.)",
      "authors": "",
      "year": null,
      "doi": "",
      "url": "https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/",
      "abstract": "Reproducibility is a bedrock of scientific progress. However, it’s remarkably difficult to get reproducible results out of large language models.\nFor example, you might observe that asking ChatGPT the same question multiple times provides different results. This by itself is not surprising, since getting a result from a language model involves “sampling”, a process that converts the language model’s output into a probability distribution and probabilistically selects a token.\nWhat might be more ",
      "item_type": "blogpost",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "Generative_KI"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt rein technische Aspekte von LLM-Inferenz (Nondeterminismus, Sampling, Reproducibility). Während es Generative KI erwähnt, fehlt jeder soziale Bezug: keine AI Literacies, keine Bias/Fairness-Analyse, keine Bezüge zu Soziale Arbeit, Diversität oder Ungleichheit. Es ist ein rein tec"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ]
      }
    },
    {
      "id": "MPCZVZEW",
      "title": "Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room",
      "author_year": "Goellner et al. (2025)",
      "authors": "Goellner, J.; Kumar, V.; Aleven, V.",
      "year": 2025,
      "doi": "",
      "url": "https://arxiv.org/pdf/2504.16148",
      "abstract": "Identifies nine persistent challenges undermining responsible use of AI in education, including neglect of key learning processes, lack of stakeholder involvement, and use of unreliable XAI methods. Proposes hybrid human-AI methods, specifically neural-symbolic AI (NSAI), which integrates expert domain knowledge with data-driven approaches. This hybrid architecture allows for built-in transparency, stakeholder engagement, and modeling of complex pedagogically-grounded principles.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper adressiert AI Literacy durch Forderung nach Stakeholder-Involvement und Transparenz in KI-Systemen für Bildung. KI_Sonstige zutreffend: Neural-Symbolic AI ist ML-Ansatz. Bias_Ungleichheit und Fairness erfüllt durch Kritik an irresponsible KI-Use, unreliablen XAI-Methoden und Forderung nach Ver"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "XY2WVKBY",
      "title": "Governing discriminatory content in conversational AI: A cross-system, cross-lingual, and cross-topic audit",
      "author_year": "Zeng & van Es (2025)",
      "authors": "Zeng, J.; van Es, K.",
      "year": 2025,
      "doi": "10.1080/1369118X.2025.2537803",
      "url": "https://doi.org/10.1080/1369118X.2025.2537803",
      "abstract": "Conducts mixed-method audit of how major conversational AI systems respond to and regulate discriminatory content. Analysis is cross-system, cross-lingual, and cross-topic, revealing that refusal sensitivity and answering strategies vary significantly across all three axes. Discusses value alignment process through reinforcement learning with human feedback and implementation of guardrails, highlighting tensions when tech platforms become arbiters of morality.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper untersucht Diskriminierungsinhalte in conversational AI-Systemen (Generative_KI: Ja). Die Audit-Analyse fokussiert auf Bias und Diskriminierung (Bias_Ungleichheit: Ja), behandelt Unterschiede across Sprachen und Themen (Diversitaet: Ja) und diskutiert Fairness-Aspekte durch RLHF und Guardr"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "JPUCNHNU",
      "title": "LIBRA: Measuring bias of large language model from a local context",
      "author_year": "Pan et al. (2025)",
      "authors": "Pan, B.; Liu, H.; Hou, Y.; Yang, M.",
      "year": 2025,
      "doi": "",
      "url": "https://www.researchgate.net/publication/388686547_LIBRA_Measuring_Bias_of_Large_Language_Model_from_a_Local_Context",
      "abstract": "Critiques the U.S.-centricity of existing LLM bias evaluation methods. Proposes the Local Integrated Bias Recognition and Assessment (LIBRA) framework and develops dataset of over 360,000 test cases specific to New Zealand context. Results show models like BERT and GPT-2 struggle with local context, while Llama-3 responds better to different cultural contexts despite exhibiting larger bias overall.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt TECHNIK-Kriterium (Generative_KI, KI_Sonstige) und SOZIAL-Kriterium (Bias_Ungleichheit, Diversitaet, Fairness). Allerdings: Paper behandelt allgemeine LLM-Bias-Evaluation ohne direkten Bezug zu Sozialer Arbeit, sozialarbeiterischen Zielgruppen oder sozialarbeiterischer Praxis. Bias-Ana"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "YV53DKI2",
      "title": "Explainability through systematicity: The hard systematicity challenge",
      "author_year": "Santos (2024)",
      "authors": "Santos, J. D.",
      "year": 2024,
      "doi": "",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12307450/",
      "abstract": "This philosophical paper argues that the pursuit of \"explainability\" in AI is too narrow. It proposes a richer ideal called \"systematicity,\" which demands that an AI's reasoning be consistent, coherent, comprehensive, and principled, akin to an integrated body of human thought. The author distinguishes this \"hard systematicity challenge\" from the historical Fodorian debate on connectionism and explores how the demand for AI to be systematic should be regulated by different rationales.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt philosophische Grundlagen von AI-Explainability und Systematizität (KI_Sonstige: Ja). Dies ist jedoch rein technisch-philosophischer Natur ohne Bezug zu sozialen Auswirkungen, Bias, Ungleichheit, Gender, Diversität, Feminismus oder Fairness. Kein Bezug zu Sozialer Arbeit. TECHNIK erf"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Unclear",
        "severity": "low",
        "affected_categories": [
          "AI_Literacies",
          "Bias_Ungleichheit",
          "Gender"
        ]
      }
    },
    {
      "id": "BDBYDLVK",
      "title": "A reparative turn in AI",
      "author_year": "Singh et al. (2025)",
      "authors": "Singh, R.; Posada, J.; Penney, D.",
      "year": 2025,
      "doi": "",
      "url": "https://arxiv.org/pdf/2506.05687",
      "abstract": "Argues for a \"reparative turn\" in AI governance, moving beyond harm prevention to focus on remedying harm after it occurs. Based on thematic analysis of 1,060 real-world AI harm incidents, proposes taxonomy of reparative actions around four goals: acknowledging harm, attributing responsibility, providing remedies, and enabling systemic change. Finds significant \"accountability gap\" with most corporate responses limited to symbolic acknowledgments.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt KI-Governance und -Harms substantiell (KI_Sonstige: Ja). Adressiert algorithmische Diskriminierung und Schadensanalyse (Bias_Ungleichheit: Ja) sowie Accountability und Fairness in KI-Systemen (Fairness: Ja). Erfüllt beide Bedingungen: TECHNIK (KI_Sonstige) UND SOZIAL (Bias_Ungleichhe"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "T9KEZN3G",
      "title": "Governance of generative AI: A comprehensive framework for navigating challenges and opportunities",
      "author_year": "Taeihagh (2025)",
      "authors": "Taeihagh, A.",
      "year": 2025,
      "doi": "",
      "url": "https://academic.oup.com/policyandsociety/article/44/1/1/7997395",
      "abstract": "Provides comprehensive overview of governance challenges posed by generative AI, including bias amplification, privacy violations, misinformation, and exacerbation of power imbalances. Critiques inadequacy of voluntary self-regulation and proposes comprehensive governance framework that is proactive, adaptive, and participatory. Recommends improving data governance, mandating independent audits, enhancing public engagement, and fostering international cooperation.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt substantiell Generative KI (Kategorie erfüllt) und adressiert Bias-Amplifikation, Machtungleichgewichte und Fairness-Fragen (Bias_Ungleichheit und Fairness erfüllt). Es erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja) und SOZIAL_OK (Bias_Ungleichheit=Ja, Fairness=Ja). Kei"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ]
      }
    },
    {
      "id": "99QJDBSV",
      "title": "How large language models judge cooperation",
      "author_year": "Santos et al. (2025)",
      "authors": "Santos, F. P.; Lages, M.; Melo, F. S.",
      "year": 2025,
      "doi": "",
      "url": "https://arxiv.org/pdf/2507.00088",
      "abstract": "This study investigates how 21 state-of-the-art LLMs make social and moral judgments about cooperative behavior. Using an evolutionary game-theory model and a dataset of 43,200 prompts, the authors find significant variation in how different models assign reputations, particularly when judging interactions with \"ill-reputed\" actors. Demonstrates that LLM social norms are highly malleable and can be consistently steered by different types of prompt interventions.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.82,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper untersucht LLMs (Generative_KI: Ja) systematisch mit 43.200 Prompts (Prompting: Ja, substantiell). Zentrale Ergebnisse adressieren Bias und Malleabilität sozialer Normen (Bias_Ungleichheit: Ja) sowie Fairness-Aspekte bei moralischen Urteilen (Fairness: Ja). TECHNIK und SOZIAL erfüllt → Include"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "Prompting",
          "KI_Sonstige",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ]
      }
    },
    {
      "id": "R3VJVFCE",
      "title": "Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes",
      "author_year": "Kamruzzaman & Kim (2024)",
      "authors": "Kamruzzaman, M.; Kim, G. L.",
      "year": 2024,
      "doi": "",
      "url": "https://arxiv.org/abs/2404.17218",
      "abstract": "Explores how different prompt engineering strategies can mitigate social biases in LLM outputs by analogizing model's reasoning to human cognitive processes. Leverages dual-process cognition theory (System 1 vs System 2) to design prompts that encourage deliberative reasoning. Finds certain prompting techniques significantly reduce biased responses, with up to 13% reduction in stereotypes.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting substantiell behandelt), SOZIAL_OK (Bias_Ungleichheit + Fairness adressiert). Empirische Untersuchung von Prompt-Engineering-Strategien zur Bias-Reduktion in LLMs. Keine sozialen Arbeitskontexte, aber algorithmische Fairness ist "
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "GCQ8J9XF",
      "title": "Beyond transparency and explainability: On the need for adequate and contextualized user guidelines for LLM use",
      "author_year": "Barman et al. (2024)",
      "authors": "Barman, K. G.; Wood, N.; Pawlowski, P.",
      "year": 2024,
      "doi": "10.1007/s10676-024-09778-2",
      "url": "https://doi.org/10.1007/s10676-024-09778-2",
      "abstract": "Argues for user-centered approach to governing AI systems, contending that transparency alone is insufficient. Proposes contextualized guidelines and training for users including clear instructions on LLM reliability, diversity-sensitive prompting techniques, and iterative query refinement. Emphasizes shifting focus from AI's internal workings to human-AI interaction context.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt substantiell AI_Literacies (user guidelines und training für LLM-Nutzung), Generative_KI (LLM-fokussiert), Prompting (diversity-sensitive prompting techniques explizit genannt) und adressiert Bias_Ungleichheit sowie Diversität (diversity-sensitive approaches). Erfüllt TECHNIK (3 Kate"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Gender",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "EMZ33KFH",
      "title": "Unequal voices: How LLMs construct constrained queer narratives",
      "author_year": "Ghosal et al. (2025)",
      "authors": "Ghosal, A.; Gupta, A.; Srikumar, V.",
      "year": 2025,
      "doi": "",
      "url": "https://arxiv.org/abs/2507.15585",
      "abstract": "Investigates how large language models represent queer individuals in generated narratives, uncovering tendencies toward stereotyped and narrow portrayals. Identifies phenomena including narrow topic range, discursive othering, and identity foregrounding. Shows LLMs unconsciously reinforce divide where marginalized groups are not afforded same breadth of narrative roles as others.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper untersucht systematisch Bias und Diskriminierung in LLM-generierten Narrativen von queeren Personen. Erfüllt TECHNIK-Kriterium (Generative_KI: Fokus auf LLM-Outputs) und SOZIAL-Kriterium (Bias_Ungleichheit: Stereotypisierung marginalisierter Gruppen; Gender: queere Identitäten; Diversitaet: Re"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ]
      }
    },
    {
      "id": "WLV8L8PM",
      "title": "Guardrails, not guidance: Understanding responses to LGBTQ+ language in large language models",
      "author_year": "Tint (2025)",
      "authors": "Tint, J.",
      "year": 2025,
      "doi": "",
      "url": "https://aclanthology.org/2025.queerinai-main.2.pdf",
      "abstract": "Examines how large language models respond to prompts involving LGBTQ+ terminology and how current safety measures handle such content. Finds disparity where LLMs invoke safety guardrails for overtly heteronormative prompts but exhibit subtle biases when handling queer slang or informal LGBTQ+ language, responding with more negative emotional tone without triggering content filters.",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper untersucht substantiell LLM-Responses auf LGBTQ+-Prompts (Generative_KI: Ja; Prompting: Ja - systematische Prompt-Variation). Zeigt algorithmischen Bias gegen queer/marginalisierte Communities (Bias_Ungleichheit: Ja), thematisiert Gender/sexuelle Orientierung (Gender: Ja; Diversitaet: Ja - LGB"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Unclear",
        "severity": "low",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ]
      }
    },
    {
      "id": "38E5FZDV",
      "title": "Explicitly unbiased large language models still form biased associations",
      "author_year": "Bai et al. (2025)",
      "authors": "Bai, X.; Wang, A.; Sucholutsky, I.; Griffiths, T. L.",
      "year": 2025,
      "doi": "10.1073/pnas.2416228122",
      "url": "https://doi.org/10.1073/pnas.2416228122",
      "abstract": "Demonstrates that even when LLMs are aligned to avoid overt bias, they can still harbor implicit biases. Introduces novel evaluation methods inspired by psychology: LLM Word Association Test (LLM-WAT) and LLM Relative Decision Test (LLM-RDT) to probe automatic associations and subtle discrimination. Finds pervasive stereotype-consistent biases across multiple domains in eight state-of-the-art, value-aligned models.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper untersucht implizite Biase in LLMs durch neuartige Evaluationsmethoden (LLM-WAT, LLM-RDT). Erfüllt TECHNIK-Kriterium (Generative_KI=Ja) und SOZIAL-Kriterien (Bias_Ungleichheit=Ja, Fairness=Ja). Fokus auf stereotype Assoziationen und subtile Diskriminierung in State-of-the-Art Modellen. Kein ex"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "WNY526GN",
      "title": "Bias and fairness in large language models: A survey",
      "author_year": "Gallegos et al. (2024)",
      "authors": "Gallegos, I. O.; Rossi, R. A.; Barrow, J.; Tanjim, M. M.; Kim, S.; Dernoncourt, F.; Yu, T.; Zhang, R.; Ahmed, N. K.",
      "year": 2024,
      "doi": "10.1162/coli_a_00524",
      "url": "https://doi.org/10.1162/coli_a_00524",
      "abstract": "This comprehensive survey consolidates recent research on social biases in large language models (LLMs) and methods to mitigate them. The authors formalize key concepts of bias and fairness in NLP, presenting three taxonomies: metrics for bias evaluation organized by model level, datasets for bias evaluation categorized by structure and target social groups, and bias mitigation techniques classified by intervention stage.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper ist ein Überblicks-Survey zu Bias und Fairness in LLMs. Es erfüllt die TECHNIK-Bedingung (Generative_KI=Ja) und behandelt substantiell Bias_Ungleichheit und Fairness. Allerdings fehlt die SOZIAL-Bedingung im Sinne der definierten Kategorien: Es gibt keinen direkten Bezug zu Sozialer Arbeit"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Unclear",
        "severity": "low",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ]
      }
    },
    {
      "id": "22XEFRWP",
      "title": "Prejudiced interactions with large language models (LLMs) reduce trustworthiness and behavioral intentions among members of stigmatized groups",
      "author_year": "Petzel & Sowerby (2025)",
      "authors": "Petzel, Z. W.; Sowerby, M.",
      "year": 2025,
      "doi": "10.1016/j.chb.2025.108563",
      "url": "https://doi.org/10.1016/j.chb.2025.108563",
      "abstract": "Investigates how biased or prejudiced content in LLM responses affects user trust and willingness to use the system, particularly for users from marginalized communities. Through three preregistered experiments, finds that when AI responses exhibited prejudice, participants from marginalized groups reported significantly lower trust and decreased intentions to continue using the system.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI: LLMs sind zentral), SOZIAL_OK (Bias_Ungleichheit: Analyse von Prejudice in LLM-Outputs; Diversitaet: Fokus auf marginalized groups; Fairness: Implikationen für vertrauenswürdige KI-Nutzung). Drei preregistrierte experimentelle Studien mit k"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet"
        ]
      }
    },
    {
      "id": "QDUGSBPC",
      "title": "Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation",
      "author_year": "An et al. (2025)",
      "authors": "An, J.; Huang, D.; Lin, C.; Tai, M.",
      "year": 2025,
      "doi": "10.1093/pnasnexus/pgaf089",
      "url": "https://doi.org/10.1093/pnasnexus/pgaf089",
      "abstract": "Examines intersectional bias in LLM-based decision-making using AI-driven resume screening context. Multiple recent LLMs scored ~361,000 synthetic entry-level job resumes with systematically varied demographic attributes. Revealed significant biases: female candidates received higher competence scores than equally qualified males, while Black male candidates received markedly lower scores, demonstrating intersectional effects.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + KI_Sonstige: LLM-basierte Entscheidungssysteme), SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch: intersektionale Analyse von Geschlechts- und Rassenbias). Explizit intersektionales Design nach Crenshaw-Ansatz (Gender × Rasse). S"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Generative_KI"
        ]
      }
    },
    {
      "id": "7L78MV2V",
      "title": "Biases in large language models: Origins, inventory and discussion",
      "author_year": "Navigli et al. (2023)",
      "authors": "Navigli, R.; Conia, S.; Ross, B.",
      "year": 2023,
      "doi": "10.1145/3597307",
      "url": "https://doi.org/10.1145/3597307",
      "abstract": "Provides an overview of various social biases manifested by large language models and discusses their root causes. Examines how training data selection leads to bias and surveys different types of biases including gender, racial/ethnic, sexual orientation, age, religious and cultural biases. Compiles an inventory of biased behaviors and discusses emerging approaches to measure and mitigate such biases.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt TECHNIK-Kriterium (Generative_KI: LLMs) und SOZIAL-Kriterium (Bias_Ungleichheit, Gender, Diversitaet, Fairness). Systematische Analyse von sozialen Biases in LLMs mit Fokus auf Geschlecht, Ethnie, sexuelle Orientierung, Alter und Religion. Behandelt Messungs- und Mitigationsstrategien."
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Feministisch"
        ]
      }
    },
    {
      "id": "IW32JGWV",
      "title": "Advancing Accountability in AI",
      "author_year": "OECD (2023)",
      "authors": "",
      "year": 2023,
      "doi": "",
      "url": "https://www.oecd.org/content/dam/oecd/en/publications/reports/2023/02/advancing-accountability-in-ai_753bf8c8/2448f04b-en.pdf",
      "abstract": "Delivers a multi-level review of AI accountability, focusing on transparency, fairness, and privacy. Discusses trade-offs in adopting explainability and transparency measures while mitigating algorithmic bias and upholding fairness, framed within legal, social, and ethical requirements for inclusive, trustworthy AI.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt TECHNIK-Kriterium (KI_Sonstige=Ja) und SOZIAL-Kriterium (Bias_Ungleichheit=Ja, Fairness=Ja). ABER: Kein direkter Bezug zu Sozialer Arbeit, keine spezifische soziale Ungleichheit oder marginalisierte Gruppen adressiert. Allgemeine AI-Governance und Fairness-Diskussion ohne Sozialarbeits"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "LBLF9BCW",
      "title": "Dipper: Diversity in Prompts for Producing Large Language Model Outputs",
      "author_year": "Lau (2023)",
      "authors": "Lau, G. K. R.",
      "year": 2023,
      "doi": "",
      "url": "https://www.comp.nus.edu.sg/~greglau/assets/pdf/dipper_neurips_mint.pdf",
      "abstract": "Presents 'Dipper', an LLM prompting ensemble framework that systematically deploys a diverse set of prompts in parallel to improve the breadth of generated perspectives, including those of minority or marginalized groups. This training-free technique enhances demographic and perspective diversity without performance degradation.",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt Prompting (Dipper-Framework mit diversen Prompts) und Generative KI (LLMs) substantiell. Es adressiert Diversity (marginalisierte Gruppen, Perspektivenvielfalt), Bias_Ungleichheit (Verbesserung von Darstellung minorities) und Fairness (faire Output-Generierung). TECHNIK und SOZIA"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ]
      }
    },
    {
      "id": "QFPTW4VL",
      "title": "Explainable Artificial Intelligence",
      "author_year": "European Data Protection Supervisor (2023)",
      "authors": "",
      "year": 2023,
      "doi": "",
      "url": "https://www.edps.europa.eu/system/files/2023-11/23-11-16_techdispatch_xai_en.pdf",
      "abstract": "Emphasizes the necessity of Explainable AI (XAI) to ensure human-centered, transparent, and ethical AI systems. By increasing transparency and comprehensibility of algorithmic decisions, XAI enables individuals—including those from marginalized groups—to participate meaningfully in digital decision-making and challenge unjust outcomes.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt XAI als Kompetenz- und Transparenzthema (AI_Literacies), mit Fokus auf algorithmische Systeme (KI_Sonstige). Substantielle Thematisierung von Bias/Ungleichheit durch Marginalisierung, Diversität durch explizite Nennung marginalisierter Gruppen und Fairness durch ethische/gerechte Ent"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ]
      }
    },
    {
      "id": "DUV4TUG3",
      "title": "Improving diversity of demographic representation in people entities in Large Language Models",
      "author_year": "Lahoti et al. (2023)",
      "authors": "Lahoti, P.; Kiela, D.; Ahn, S.",
      "year": 2023,
      "doi": "",
      "url": "https://aclanthology.org/2023.emnlp-main.643/",
      "abstract": "Introduces the Collective-Critique and Self-Voting (CCSV) prompting method to systematically enhance demographic diversity in LLM outputs. The approach leverages LLMs' internal capacity for diversity reasoning and combines critique and self-voting mechanisms to iteratively improve output balance while maintaining model performance.",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper adressiert Generative KI (LLMs), Prompting (CCSV-Methode), sowie Bias/Ungleichheit durch demografische Unterrepräsentation, Diversität und Fairness. TECHNIK_OK (Generative_KI + Prompting), SOZIAL_OK (Bias_Ungleichheit + Diversitaet + Fairness). Erfüllt Einschlusskriterien. Kein expliziter Gend"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "6L6WSDC8",
      "title": "How Far Can We Extract Diverse Perspectives from Large Language Models?",
      "author_year": "Hayati et al. (2024)",
      "authors": "Hayati, S.; de Masson d'Autume, C.; Naradowsky, J.",
      "year": 2024,
      "doi": "",
      "url": "https://aclanthology.org/2024.emnlp-main.306.pdf",
      "abstract": "Systematically evaluates prompting strategies to extract diverse perspectives from LLMs and mitigate dominant group bias in outputs. Measuring subjective tasks such as argumentation and hate speech labeling, the study finds that diversity prompting increases perspective variety and reduces monocultural output tendencies.",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper adressiert LLMs (Generative_KI) und Prompting-Strategien (Diversity Prompting) substanziell. Zentraler Fokus liegt auf Mitigation von \"dominant group bias\" und Erhöhung von Perspektivenvielfalt - klare Bias_Ungleichheit und Diversitaet sowie Fairness-Themen. TECHNIK und SOZIAL erfüllt → Includ"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "3AHQEHDF",
      "title": "Multilingual Prompting for Improving LLM Generation Diversity",
      "author_year": "Wang et al. (2024)",
      "authors": "Wang, Q.; Pan, S.; Linzen, T.; Black, E.",
      "year": 2024,
      "doi": "",
      "url": "https://arxiv.org/html/2505.15229v1",
      "abstract": "This paper introduces multilingual and multicultural prompting as methods to enhance the demographic and cultural diversity of Large Language Model outputs. The authors demonstrate these approaches outperform established diversity methods across multiple LLM architectures. Results indicate that prompting in culturally and linguistically aligned languages reduces hallucinated outputs and supports more representative generation.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt TECHNIK-Kriterium: Generative_KI (LLM-Fokus) + Prompting (Multilingual Prompting als Kern). Erfüllt SOZIAL-Kriterium: Bias_Ungleichheit (Halluzinationen, Representation), Diversitaet (demografische und kulturelle Vielfalt), Fairness (representative generation). Kein Soziale_Arbeit- ode"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "PH7JBBC8",
      "title": "A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization",
      "author_year": "Bisconti et al. (2024)",
      "authors": "Bisconti, P.; Aquilino, L.; Marchetti, A.; Nardi, D.",
      "year": 2024,
      "doi": "",
      "url": "https://philarchive.org/archive/BISAFA",
      "abstract": "Develops formal conceptualization of AI trustworthiness connecting intrinsic and perceived trustworthiness in operationalizable schema. Argues that trustworthiness extends beyond inherent AI system capabilities to include significant influences from observer perceptions such as perceived transparency, agency locus, and human oversight. Identifies three central perceived characteristics: transparency (access to information about functionality), agency locus (perception of action autonomy source),",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper entwickelt formale Konzeptualisierung von KI-Vertrauenswürdigkeit (KI_Sonstige erfüllt). Es fehlt jedoch jeglicher Bezug zu sozialen Dimensionen: Keine Behandlung von Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversität, Fairness oder feministischen Perspektiven. Rein technisch-theoretisches "
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "7AMRV4DT",
      "title": "Mitigating trust-induced inappropriate reliance on AI assistance",
      "author_year": "Srinivasan & Thomason (2025)",
      "authors": "Srinivasan, T.; Thomason, J.",
      "year": 2025,
      "doi": "",
      "url": "https://arxiv.org/pdf/2502.13321.pdf",
      "abstract": "Investigates trust-adaptive interventions to reduce inappropriate reliance on AI recommendations in decision-support tasks. Argues that AI assistants should adapt behavior based on user trust to mitigate both under- and over-trust. In two decision scenarios shows that providing supportive explanations for low trust and counter-explanations for high trust leads to 38% reduction in inappropriate reliance and 20% improvement in decision accuracy. Demonstrates that adaptive insertion of forced pause",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper adressiert AI Literacy (kritisches Verständnis von KI-Limitations und vertrauensbasierter Reliance), KI_Sonstige (Decision-Support-Systeme), und Fairness (angemessenes Vertrauen, bias-freie Entscheidungen). ABER: Kein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität ode"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ]
      }
    },
    {
      "id": "ZGM7K3H6",
      "title": "Artificial intelligence in social work: An EPIC model for practice",
      "author_year": "Goldkind et al. (2024)",
      "authors": "Goldkind, L.; Hamama-Raz, Y.; Levitats, Z.; Levin, L.; Ben-Ezra, M.",
      "year": 2024,
      "doi": "10.1080/0312407X.2025.2488345",
      "url": "https://doi.org/10.1080/0312407X.2025.2488345",
      "abstract": "Develops EPIC model (Ethics and justice, Policy development and advocacy, Intersectoral collaboration, Community engagement and empowerment) as structured approach for AI integration in social work. Emphasizes that AI integration must align with social work mission and values through four overlapping components. Ethics and justice aspect addresses bias mitigation and transparent use, while policy development prioritizes client protection. Warns against excessive calibration of AI systems toward ",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK durch AI_Literacies (EPIC-Modell für KI-Integration), KI_Sonstige (algorithmische Systeme im Sozialbereich) UND SOZIAL durch Soziale_Arbeit (direkter Fokus auf sozialarbeiterische Praxis), Bias_Ungleichheit (Bias-Mitigation) und Fairness (Transparenz, Bias-Ad"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "Y4BMCI2J",
      "title": "Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review",
      "author_year": "Asseri et al. (2024)",
      "authors": "Asseri, B.; Abdelaziz, E.; Al-Wabil, A.",
      "year": 2024,
      "doi": "10.48550/arXiv.2506.18199",
      "url": "https://doi.org/10.48550/arXiv.2506.18199",
      "abstract": "Systematic review following PRISMA guidelines analyzing prompt-engineering techniques to reduce cultural bias against Arabs and Muslims in LLMs, evaluating eight empirical studies (2021-2024). Identifies five primary approaches: Cultural Prompting, Affective Priming, Self-Debiasing techniques, structured multi-stage pipelines, and parameter-optimized continuous prompts. Structured multi-stage pipelines most effective with up to 87.7% bias reduction. Emphasizes accessibility of prompt engineering",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI, Prompting substantiell behandelt), SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Fairness adressiert). Systematische Review zu Prompt-Engineering-Techniken für Bias-Mitigation gegen arabische/muslimische Minderheiten in LLMs. Direkter Fokus au"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ]
      }
    },
    {
      "id": "4PK8UN82",
      "title": "Recommendations for social work researchers and journal editors on the use of generative AI and large language models",
      "author_year": "Victor et al. (2023)",
      "authors": "Victor, B. G.; Sokol, R. L.; Goldkind, L.; Perron, B. E.",
      "year": 2023,
      "doi": "10.1086/726021",
      "url": "https://doi.org/10.1086/726021",
      "abstract": "Develops \"disruptive-disrupting\" framework for analyzing generative AI in social work research with concrete recommendations for researchers and journal editors. Emphasizes transparency and accountability requiring researchers to fully document AI use, disclose prompts, and take responsibility for AI-generated content. Argues that continuous education about AI construction, limitations, and ethical considerations is essential for developing appropriate trust in LLM systems.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper adressiert substantiell generative KI und LLMs (Generative_KI: Ja), entwickelt ein Framework für kritische Analyse (AI_Literacies: Ja) mit explizitem Fokus auf Prompt-Dokumentation und -Transparenz (Prompting: Ja). Direkter Bezug zu sozialarbeiterischer Forschungspraxis und Journal-Editoren in"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Unclear",
        "severity": "low",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "EXVG7MQR",
      "title": "Artificial intelligence in social work: Emerging ethical issues",
      "author_year": "Reamer (2023)",
      "authors": "Reamer, F. G.",
      "year": 2023,
      "doi": "10.55521/10-020-205",
      "url": "https://doi.org/10.55521/10-020-205",
      "abstract": "Systematic analysis of ethical challenges in AI use within social work, developing comprehensive framework for ethical AI implementation. Identifies eight central ethical areas including informed consent, data privacy, transparency, misdiagnosis, client neglect, surveillance, scientific misconduct, and algorithmic bias. Emphasizes transparency as fundamental principle requiring social workers to inform clients about AI use. Develops eight-step protocol for ethical AI implementation including eth",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies, KI_Sonstige, Fairness durch Bias-Thematisierung) und SOZIAL (Soziale_Arbeit, Bias_Ungleichheit, Fairness). Systematische ethische Analyse von KI in Sozialarbeit mit substantiellem Fokus auf algorithmischen Bias, Transparenz und Fairness-Implem"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "BCBWSU3Z",
      "title": "Can LLMs reason about trust? A pilot study",
      "author_year": "Debnath et al. (2024)",
      "authors": "Debnath, A.; Cranefield, S.; Lorini, E.; Savarimuthu, B. T. R.",
      "year": 2024,
      "doi": "",
      "url": "https://arxiv.org/html/2507.21075v1",
      "abstract": "Pilot study investigating LLMs' ability to reason about trust between individuals by capturing complex mental states that exceed traditional symbolic approaches. Evaluates whether LLMs can analyze conversations and assess trust levels based on willingness, competence, and security factors. Demonstrates that LLMs can identify individual goals and develop improvement suggestions for trust building, as well as generate strategic action plans for trust promotion. Suggests LLMs have potential to brid",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "Generative_KI"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt generative KI (LLMs) substantiell, erfüllt aber KEINE der sozialen Kategorien. Untersucht LLM-Fähigkeiten im Trust-Reasoning ohne Bezug zu Soziale Arbeit, Bias/Ungleichheit, Gender, Diversität, Feminismus oder Fairness. Keine direkte Relevanz für Sozialarbeitskontext oder marginalisi"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "G53MCF3W",
      "title": "Bias, accuracy, and trust: Gender-diverse perspectives on large language models",
      "author_year": "Gaba et al. (2025)",
      "authors": "Gaba, A.; Wall, E.; Babu, T. R.; Brun, Y.; Hall, K. W.; Xiong, C.",
      "year": 2025,
      "doi": "",
      "url": "https://arxiv.org/abs/2506.21898",
      "abstract": "Qualitative study with 25 interviews exploring how users of different gender identities perceive bias and trustworthiness in ChatGPT. Found perceived biases in LLM outputs undermine user trust, with non-binary participants encountering stereotyped responses that eroded confidence. Trust levels varied by group with male participants reporting higher trust. Participants recommended bias mitigation strategies including diversifying training data and implementing clarifying follow-up prompts to chec",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Inklusionskriterien: TECHNIK_OK (Generative_KI, Prompting), SOZIAL_OK (Bias_Ungleichheit, Gender, Fairness). Qualitative Studie zu LLM-Bias mit explizitem Gender-Fokus. Prompting wird als Bias-Mitigationsstrategie thematisiert. Kein Sozialarbeitsbezug, keine explizit feministisch"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ]
      }
    },
    {
      "id": "8JFZMD5G",
      "title": "The influence of mental state attributions on trust in large language models",
      "author_year": "Colombatto et al. (2025)",
      "authors": "Colombatto, C.; Birch, J.; Fleming, S. M.",
      "year": 2025,
      "doi": "10.1038/s44271-025-00262-1",
      "url": "https://www.nature.com/articles/s44271-025-00262-1",
      "abstract": "Empirical study examining how users' beliefs about LLM's \"mind\" affect trust in advice. Preregistered experiment (N=410) showing attributing intelligence to LLM strongly increased trust, whereas attributing human-like consciousness did not increase trust. Higher ascriptions of sentience correlated with less advice-taking. Participants trusted AI for perceived analytical competence, not for having feelings. Suggests prompt-engineering highlighting competence and reliability more effective than an",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt TECHNIK-Kriterium mit 3 Ja-Kategorien (AI_Literacies: User-Trust-Kompetenz; Generative_KI: LLMs; Prompting: Prompt-Engineering-Vorschläge). Jedoch keine SOZIAL-Kategorien erfüllt: Kein direkter Sozialarbeitsbezug, keine Ungleichheits-, Gender-, Diversitäts-, feministischen oder Fairnes"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ]
      }
    },
    {
      "id": "ERTJZW5M",
      "title": "Trust in artificial intelligence–based clinical decision support systems among health care workers: Systematic review",
      "author_year": "Tun et al. (2025)",
      "authors": "Tun, H. M.; Abdul Rahman, H.; Naing, L.; Malik, O. A.",
      "year": 2025,
      "doi": "10.2196/69678",
      "url": "https://www.jmir.org/2025/1/e69678",
      "abstract": "Systematic review synthesizing 27 studies on clinicians' trust in AI decision-support tools. Identifies eight key factors shaping professional trust, notably system transparency as primary enabler. Lack of transparency was recurring barrier with \"black-box\" algorithms undermining trust. Improving transparency, providing explainable recommendations, and addressing bias/fairness issues recommended to bolster trust. Concludes transparent, explainable AI with proper training and ethical safeguards c",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt TECHNIK-Kriterium (AI_Literacies, KI_Sonstige) und SOZIAL-Kriterium (Bias_Ungleichheit, Fairness). JEDOCH: Thema ist klinische Entscheidungsunterstützung im Gesundheitswesen, NICHT Soziale Arbeit. Keine Verknüpfung zu sozialarbeiterischer Praxis, Theorie oder Zielgruppen. Bias/Fairness"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Gender",
          "Diversitaet"
        ]
      }
    },
    {
      "id": "5ERYSQCK",
      "title": "Measuring and identifying factors of individuals' trust in large language models",
      "author_year": "De Duro et al. (2025)",
      "authors": "De Duro, E. S.; Veltri, G. A.; Golino, H.; Stella, M.",
      "year": 2025,
      "doi": "10.48550/arXiv.2502.21028",
      "url": "https://arxiv.org/html/2502.21028v1",
      "abstract": "Study developing \"Trust-In-LLMs Index (TILLMI)\" psychometric scale to quantify trust in conversational AI. Survey of ~1,000 U.S. adults identified two trust dimensions: affective component (emotional closeness) and cognitive component (reliance on competence). Found significant individual differences with personality factors influencing trust. Prior LLM experience increased trust while those with no direct use were more skeptical. Highlights need for prompt-engineering and system design to accou",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Generative_KI"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt Trust in LLMs und AI Literacies (Vertrauen/Kompetenzwahrnehmung), fokussiert auf generative KI. Prompting wird nur beiläufig erwähnt ('prompt-engineering'), nicht substantiell behandelt. Kritisch: Keine der SOZIAL-Kategorien erfüllt - kein Bezug zu Sozialer Arbeit, Bias/Ungleichheit,"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "RXNXJA8W",
      "title": "What large language models know and what people think they know",
      "author_year": "Steyvers et al. (2025)",
      "authors": "Steyvers, M.; Tejeda, H.; Kumar, A.; Belem, C.; Karny, S.; Smyth, P.",
      "year": 2025,
      "doi": "10.1038/s42256-024-00976-7",
      "url": "https://www.nature.com/articles/s42256-024-00976-7",
      "abstract": "High-impact study investigating mismatch between LLMs' confidence and users' trust. Found users often over-trust LLM answers, especially with explanations. Longer, detailed explanations increased user confidence even when answers weren't more accurate. Demonstrated that prompt engineering to convey uncertainty accurately helps users calibrate trust better. Aligning explanation with model's true confidence narrowed \"calibration gap,\" improving accuracy in judging when to trust AI.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper adressiert generative KI (LLMs), Prompting (uncertainty calibration durch Prompt Engineering) und AI Literacy (Vertrauen in LLM-Ausgaben verstehen). Behandelt auch Fairness-Aspekte durch Kalibrierung. ABER: Kein substantieller sozialer Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diver"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "JKF6VAQB",
      "title": "AI algorithm transparency, pipelines for trust not prisms: mitigating general negative attitudes and enhancing trust toward AI",
      "author_year": "Park & Yoon (2025)",
      "authors": "Park, K.; Yoon, H. Y.",
      "year": 2025,
      "doi": "10.1057/s41599-025-05116-z",
      "url": "https://www.nature.com/articles/s41599-025-05116-z",
      "abstract": "Peer-reviewed study examining how algorithmic transparency overcomes users' predisposed distrust in AI. 2×2 experiment showing transparency significantly increases trust, especially for those with initially negative AI attitudes. Making AI workings visible mitigated negative relationship between user AI-skepticism and organizational trust. Transparency acts as \"signal of trustworthiness,\" reducing skepticism and uncertainty. Emphasizing transparency in prompts and system design can reassure wary",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Prompting",
          "KI_Sonstige",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt AI-Vertrauen, Transparenz und Accountability (AI_Literacies, KI_Sonstige, Fairness) sowie Prompting im Design-Kontext. Erfüllt TECHNIK_OK. Fehlt aber SOZIAL_OK: Kein direkter Bezug zu Sozialer Arbeit/Zielgruppen, keine Analyse von Bias/Ungleichheit oder struktureller Benachteiligung."
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ]
      }
    },
    {
      "id": "EJEFPZGA",
      "title": "Navigating the Nexus of Trust: Prompt Engineering, Professional Judgment, and the Integration of Large Language Models in Social Work",
      "author_year": "[Author not specified] (2025)",
      "authors": "",
      "year": 2025,
      "doi": "",
      "url": "",
      "abstract": "Comprehensive analysis examining how prompt engineering strategies designed to enhance transparency and mitigate bias influence trust that social work professionals place in LLM-generated case recommendations. Synthesizes literature from computer science, social work, ethics, and psychology to construct understanding of complex interplay between technology, human psychology, and professional practice. Develops framework for calibrated trust through responsible prompt engineering, positioning pro",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper adressiert substantiell: (1) Prompting: Prompt-Engineering-Strategien zur Bias-Mitigation und Transparenzförderung sind zentral; (2) Generative_KI: LLMs in sozialer Praxis; (3) Soziale_Arbeit: direkter Bezug zu Case Recommendations und Professional Practice; (4) Bias_Ungleichheit und Fairness:"
      },
      "human": {
        "decision": "",
        "categories": [
          "Bias_Ungleichheit",
          "Gender"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "7W3RGSSG",
      "title": "Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes",
      "author_year": "Kamruzzaman & Kim (2024)",
      "authors": "Kamruzzaman, M.; Kim, G. L.",
      "year": 2024,
      "doi": "10.48550/arXiv.2404.17218",
      "url": "https://doi.org/10.48550/arXiv.2404.17218",
      "abstract": "Study evaluates 12 prompt-engineering strategies to reduce social biases in Large Language Models using dual-process theory. Findings show Chain-of-Thought prompting follows System 1 patterns rather than expected System 2 reasoning. Human Persona combined with System 2 prompting proved most effective, reducing stereotypical responses by up to 13%. Demonstrates that transparency in prompt design and systematic bias-mitigation approaches contribute significantly to building trust in LLM recommenda",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK-Bereich durch Generative_KI (LLMs) und Prompting (12 prompt-engineering Strategien sind substantieller Fokus). SOZIAL-Bereich durch Bias_Ungleichheit (explizit: 'social biases', 'stereotypical responses') und Fairness (Bias-Mitigation, Fairness-Strategien"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "NFW58AU8",
      "title": "Transparency enhances positive perceptions of social artificial intelligence",
      "author_year": "Xu et al. (2023)",
      "authors": "Xu, Y.; Bradford, N.; Garg, R.",
      "year": 2023,
      "doi": "10.1155/2023/5550418",
      "url": "",
      "abstract": "Experimental study with 914 participants testing how transparent explanations about chatbot workings affect user perceptions. Results show transparency modestly improved trust-related perceptions: users found chatbot \"less creepy,\" felt more affinity, and perceived it as more socially intelligent when transparent explanation provided. Transparency's impact on perceived intelligence stronger for users with low prior AI knowledge. Suggests prompt-engineering for greater transparency can foster use",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt Chatbot-Transparenz und Nutzerwahrnehmung mit Fokus auf AI Literacy (Effekt bei niedriger Vorkenntnis) und Prompting (Transparenz durch Prompt-Engineering). Technische Kriterien erfüllt. Es fehlt jedoch jeglicher Bezug zu sozialen Kategorien: Keine Soziale Arbeit, keine Analyse von B"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "AI_Literacies",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ]
      }
    },
    {
      "id": "ED6C8LD2",
      "title": "Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals",
      "author_year": "Choudhury & Chaudhry (2024)",
      "authors": "Choudhury, A.; Chaudhry, Z.",
      "year": 2024,
      "doi": "10.2196/56764",
      "url": "https://www.jmir.org/2024/1/e56764/",
      "abstract": "Peer-reviewed viewpoint discussing integration of LLMs into healthcare requiring balance between trust and skepticism. Warns that \"blind trust\" in LLM recommendations can lead to automation bias and confirmation bias as clinicians may accept AI outputs uncritically. Argues that prompting for transparency in LLM reasoning enhances professionals' trust by making AI reasoning visible and verifiable. Emphasizes need for human oversight and bias mitigation to ensure LLMs complement rather than compro",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt substantiell LLMs (Generative_KI), Prompt-Engineering für Transparenz (Prompting), AI-Literacies (kritisches Vertrauen, Deskilling), sowie Bias und Fairness. Allerdings adressiert es Healthcare-Professionals (Kliniker), nicht Soziale Arbeit oder deren Zielgruppen. Für einen Liter"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "XJCEMM3D",
      "title": "A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization",
      "author_year": "Bisconti et al. (2024)",
      "authors": "Bisconti, P.; Aquilino, L.; Marchetti, A.; Nardi, D.",
      "year": 2024,
      "doi": "",
      "url": "https://philarchive.org/archive/BISAFA",
      "abstract": "Develops formal conceptualization of AI trustworthiness connecting intrinsic and perceived trustworthiness in operationalizable schema. Argues that trustworthiness extends beyond inherent AI system capabilities to include significant influences from observer perceptions such as perceived transparency, agency locus, and human oversight. Identifies three central perceived characteristics: transparency (access to information about functionality), agency locus (perception of action autonomy source),",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt algorithmische Trustworthiness und Vertrauenskonzepte in KI-Systemen (KI_Sonstige = Ja). Es erfüllt aber keine sozialen Kategorien: Kein expliziter Bezug zu Sozialer Arbeit, keine Analyse von Bias/Ungleichheit, kein Gender- oder Diversitätsfokus, keine feministische Perspektive, "
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "RR5MJRBZ",
      "title": "Artificial intelligence in social work: An EPIC model for practice",
      "author_year": "Goldkind et al. (2024)",
      "authors": "Goldkind, L.; Hamama-Raz, Y.; Levitats, Z.; Levin, L.; Ben-Ezra, M.",
      "year": 2024,
      "doi": "10.1080/0312407X.2025.2488345",
      "url": "https://doi.org/10.1080/0312407X.2025.2488345",
      "abstract": "Develops EPIC model (Ethics and justice, Policy development and advocacy, Intersectoral collaboration, Community engagement and empowerment) as structured approach for AI integration in social work. Emphasizes that AI integration must align with social work mission and values through four overlapping components. Ethics and justice aspect addresses bias mitigation and transparent use, while policy development prioritizes client protection. Warns against excessive calibration of AI systems toward ",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: (1) TECHNIK: AI_Literacies (Rahmenmodell für KI-Integration in Praxis), KI_Sonstige (algorithmische Systeme im Sozialbereich), Fairness (Bias-Mitigation, Transparenz). (2) SOZIAL: Soziale_Arbeit (direkter Fokus auf Integrationsstrategie in sozialarbeiterischer Praxis"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "Soziale_Arbeit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ]
      }
    },
    {
      "id": "3GB9B4IJ",
      "title": "Can LLMs reason about trust? A pilot study",
      "author_year": "Debnath et al. (2024)",
      "authors": "Debnath, A.; Cranefield, S.; Lorini, E.; Savarimuthu, B. T. R.",
      "year": 2024,
      "doi": "",
      "url": "https://arxiv.org/html/2507.21075v1",
      "abstract": "Pilot study investigating LLMs' ability to reason about trust between individuals by capturing complex mental states that exceed traditional symbolic approaches. Evaluates whether LLMs can analyze conversations and assess trust levels based on willingness, competence, and security factors. Demonstrates that LLMs can identify individual goals and develop improvement suggestions for trust building, as well as generate strategic action plans for trust promotion. Suggests LLMs have potential to brid",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "Generative_KI"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt LLMs (Generative_KI = Ja) und untersucht deren Fähigkeiten zur Analyse von Vertrauen. Es erfüllt aber KEINE der erforderlichen SOZIAL-Kategorien: Kein Bezug zu Sozialer Arbeit, keine Behandlung von Bias/Ungleichheit, Gender, Diversität, feministischen Perspektiven oder Fairness. "
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ]
      }
    },
    {
      "id": "25XSMXKT",
      "title": "Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review",
      "author_year": "Asseri et al. (2024)",
      "authors": "Asseri, B.; Abdelaziz, E.; Al-Wabil, A.",
      "year": 2024,
      "doi": "10.48550/arXiv.2506.18199",
      "url": "https://doi.org/10.48550/arXiv.2506.18199",
      "abstract": "Systematic review following PRISMA guidelines analyzing prompt-engineering techniques to reduce cultural bias against Arabs and Muslims in LLMs, evaluating eight empirical studies (2021-2024). Identifies five primary approaches: Cultural Prompting, Affective Priming, Self-Debiasing techniques, structured multi-stage pipelines, and parameter-optimized continuous prompts. Structured multi-stage pipelines most effective with up to 87.7% bias reduction. Emphasizes accessibility of prompt engineering",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt TECHNIK-Bedingung (Generative_KI=Ja, Prompting=Ja) und SOZIAL-Bedingung (Bias_Ungleichheit=Ja, Diversitaet=Ja, Fairness=Ja). Systematische Review zu Prompt-Engineering-Techniken zur Reduktion kultureller Vorurteile gegen Araber und Muslime in LLMs mit substantiellem Fokus auf Fairness "
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "9YYPYEGY",
      "title": "Recommendations for social work researchers and journal editors on the use of generative AI and large language models",
      "author_year": "Victor et al. (2023)",
      "authors": "Victor, B. G.; Sokol, R. L.; Goldkind, L.; Perron, B. E.",
      "year": 2023,
      "doi": "10.1086/726021",
      "url": "https://doi.org/10.1086/726021",
      "abstract": "Develops \"disruptive-disrupting\" framework for analyzing generative AI in social work research with concrete recommendations for researchers and journal editors. Emphasizes transparency and accountability requiring researchers to fully document AI use, disclose prompts, and take responsibility for AI-generated content. Argues that continuous education about AI construction, limitations, and ethical considerations is essential for developing appropriate trust in LLM systems.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, Prompting=Ja, AI_Literacies=Ja durch Fokus auf Bildung und kritisches Verständnis von LLMs) und SOZIAL_OK (Soziale_Arbeit=Ja, direkter Bezug zu Forschung und Praxis in Social Work). Framework für verantwortungsvollen KI-Einsatz mit Dokum"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ]
      }
    },
    {
      "id": "Q8YPNNKL",
      "title": "Mitigating trust-induced inappropriate reliance on AI assistance",
      "author_year": "Srinivasan & Thomason (2025)",
      "authors": "Srinivasan, T.; Thomason, J.",
      "year": 2025,
      "doi": "",
      "url": "https://arxiv.org/pdf/2502.13321.pdf",
      "abstract": "Investigates trust-adaptive interventions to reduce inappropriate reliance on AI recommendations in decision-support tasks. Argues that AI assistants should adapt behavior based on user trust to mitigate both under- and over-trust. In two decision scenarios shows that providing supportive explanations for low trust and counter-explanations for high trust leads to 38% reduction in inappropriate reliance and 20% improvement in decision accuracy. Demonstrates that adaptive insertion of forced pause",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper adressiert Mensch-KI-Interaktion und Trust-Adaptivität in Decision-Support-Systemen (AI_Literacies, KI_Sonstige, Fairness). Allerdings fehlt der erforderliche SOZIAL-Anker komplett: kein Bezug zu Soziale Arbeit, keine Analyse von strukturellen Ungleichheiten, keine marginalisierten Gruppen im "
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "IUN7Z56I",
      "title": "Artificial intelligence in social work: Emerging ethical issues",
      "author_year": "Reamer (2023)",
      "authors": "Reamer, F. G.",
      "year": 2023,
      "doi": "10.55521/10-020-205",
      "url": "https://doi.org/10.55521/10-020-205",
      "abstract": "Systematic analysis of ethical challenges in AI use within social work, developing comprehensive framework for ethical AI implementation. Identifies eight central ethical areas including informed consent, data privacy, transparency, misdiagnosis, client neglect, surveillance, scientific misconduct, and algorithmic bias. Emphasizes transparency as fundamental principle requiring social workers to inform clients about AI use. Develops eight-step protocol for ethical AI implementation including eth",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt systematisch ethische Herausforderungen von KI in der Sozialen Arbeit und entwickelt einen umfassenden Implementierungsrahmen. TECHNIK_OK: AI_Literacies (Kompetenzrahmen für ethische KI-Implementierung) + KI_Sonstige (algorithmische Systeme im Sozialkontext). SOZIAL_OK: Soziale_A"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "F7WNRWIC",
      "title": "Measuring and identifying factors of individuals' trust in large language models",
      "author_year": "De Duro et al. (2025)",
      "authors": "De Duro, E. S.; Veltri, G. A.; Golino, H.; Stella, M.",
      "year": 2025,
      "doi": "10.48550/arXiv.2502.21028",
      "url": "https://arxiv.org/html/2502.21028v1",
      "abstract": "Study developing \"Trust-In-LLMs Index (TILLMI)\" psychometric scale to quantify trust in conversational AI. Survey of ~1,000 U.S. adults identified two trust dimensions: affective component (emotional closeness) and cognitive component (reliance on competence). Found significant individual differences with personality factors influencing trust. Prior LLM experience increased trust while those with no direct use were more skeptical. Highlights need for prompt-engineering and system design to accou",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Generative_KI"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt LLM-Vertrauen und individuelle Unterschiede (AI_Literacies, Generative_KI erfüllt). ABER: Kein Bezug zu sozialen Ungleichheiten, Bias, Gender, Diversität, Fairness oder Sozialer Arbeit. Prompting wird nur beiläufig erwähnt ('prompt-engineering'), nicht substantiell behandelt. Psychol"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "Prompting",
          "Bias_Ungleichheit"
        ]
      }
    },
    {
      "id": "GFHALQS2",
      "title": "Trust in artificial intelligence–based clinical decision support systems among health care workers: Systematic review",
      "author_year": "Tun et al. (2025)",
      "authors": "Tun, H. M.; Abdul Rahman, H.; Naing, L.; Malik, O. A.",
      "year": 2025,
      "doi": "10.2196/69678",
      "url": "https://www.jmir.org/2025/1/e69678",
      "abstract": "Systematic review synthesizing 27 studies on clinicians' trust in AI decision-support tools. Identifies eight key factors shaping professional trust, notably system transparency as primary enabler. Lack of transparency was recurring barrier with \"black-box\" algorithms undermining trust. Improving transparency, providing explainable recommendations, and addressing bias/fairness issues recommended to bolster trust. Concludes transparent, explainable AI with proper training and ethical safeguards c",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt AI-Kompetenzen (Vertrauen, Transparenzverstaendnis) und KI-Sonstige (klinische Entscheidungssysteme) sowie Bias/Fairness. ABER: Fokus liegt auf Healthcare/klinischem Kontext, nicht auf Soziale Arbeit. Keine direkter Bezug zu sozialarbeiterischer Praxis, Theorie oder Zielgruppen. SOZI"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "Soziale_Arbeit",
          "Diversitaet"
        ]
      }
    },
    {
      "id": "MUBZ8XJL",
      "title": "Bias, accuracy, and trust: Gender-diverse perspectives on large language models",
      "author_year": "Gaba et al. (2025)",
      "authors": "Gaba, A.; Wall, E.; Babu, T. R.; Brun, Y.; Hall, K. W.; Xiong, C.",
      "year": 2025,
      "doi": "",
      "url": "https://arxiv.org/abs/2506.21898",
      "abstract": "Qualitative study with 25 interviews exploring how users of different gender identities perceive bias and trustworthiness in ChatGPT. Found perceived biases in LLM outputs undermine user trust, with non-binary participants encountering stereotyped responses that eroded confidence. Trust levels varied by group with male participants reporting higher trust. Participants recommended bias mitigation strategies including diversifying training data and implementing clarifying follow-up prompts to chec",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt TECHNIK-Kriterien durch Generative_KI (ChatGPT-Fokus) und Prompting (Empfehlungen zu clarifying prompts). Erfüllt SOZIAL-Kriterien durch expliziten Gender-Fokus (unterschiedliche Geschlechteridentitäten), Bias_Ungleichheit (stereotype responses, verringerte Vertrauenswerte) und Fairnes"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Unclear",
        "severity": "low",
        "affected_categories": [
          "Prompting",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ]
      }
    },
    {
      "id": "7IY7AX7D",
      "title": "The influence of mental state attributions on trust in large language models",
      "author_year": "Colombatto et al. (2025)",
      "authors": "Colombatto, C.; Birch, J.; Fleming, S. M.",
      "year": 2025,
      "doi": "10.1038/s44271-025-00262-1",
      "url": "https://www.nature.com/articles/s44271-025-00262-1",
      "abstract": "Empirical study examining how users' beliefs about LLM's \"mind\" affect trust in advice. Preregistered experiment (N=410) showing attributing intelligence to LLM strongly increased trust, whereas attributing human-like consciousness did not increase trust. Higher ascriptions of sentience correlated with less advice-taking. Participants trusted AI for perceived analytical competence, not for having feelings. Suggests prompt-engineering highlighting competence and reliability more effective than an",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt substantiell AI_Literacies (Vertrauen, Kompetenzen im Umgang mit LLMs) und Generative_KI sowie Prompting-Strategien. Jedoch fehlt vollständig der zweite erforderliche Bereich (Sozial): keine direkte Soziale_Arbeit, keine Bias_Ungleichheit, kein Gender/Diversität/Feminismus/Fairne"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ]
      }
    },
    {
      "id": "KRUQB7L2",
      "title": "What large language models know and what people think they know",
      "author_year": "Steyvers et al. (2025)",
      "authors": "Steyvers, M.; Tejeda, H.; Kumar, A.; Belem, C.; Karny, S.; Smyth, P.",
      "year": 2025,
      "doi": "10.1038/s42256-024-00976-7",
      "url": "https://www.nature.com/articles/s42256-024-00976-7",
      "abstract": "High-impact study investigating mismatch between LLMs' confidence and users' trust. Found users often over-trust LLM answers, especially with explanations. Longer, detailed explanations increased user confidence even when answers weren't more accurate. Demonstrated that prompt engineering to convey uncertainty accurately helps users calibrate trust better. Aligning explanation with model's true confidence narrowed \"calibration gap,\" improving accuracy in judging when to trust AI.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt AI Literacy (Benutzervertrauen, Kalibrierung), Generative KI (LLMs) und Prompting-Strategien (Unsicherheitskonveyance). Fairness-relevant bezueglich Transparenz. Jedoch kein direkter Bezug zu Sozialer Arbeit, Bias/Ungleichheit, Gender, Diversitaet oder feministischer Perspektive."
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Unclear",
        "severity": "low",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ]
      }
    },
    {
      "id": "WAYCKUZ8",
      "title": "Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals",
      "author_year": "Choudhury & Chaudhry (2024)",
      "authors": "Choudhury, A.; Chaudhry, Z.",
      "year": 2024,
      "doi": "10.2196/56764",
      "url": "https://www.jmir.org/2024/1/e56764/",
      "abstract": "Peer-reviewed viewpoint discussing integration of LLMs into healthcare requiring balance between trust and skepticism. Warns that \"blind trust\" in LLM recommendations can lead to automation bias and confirmation bias as clinicians may accept AI outputs uncritically. Argues that prompting for transparency in LLM reasoning enhances professionals' trust by making AI reasoning visible and verifiable. Emphasizes need for human oversight and bias mitigation to ensure LLMs complement rather than compro",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.75,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt LLMs, Prompting und Bias-Mitigation substantiell (TECHNIK erfüllt). Es adressiert Fairness und Bias-Ungleichheit. Jedoch ist Healthcare-Kontext nicht identisch mit Sozialer Arbeit, und es gibt keinen expliziten Fokus auf marginalisierte Gruppen oder strukturelle Ungleichheit im S"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "UIIDCXLB",
      "title": "Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes",
      "author_year": "Kamruzzaman & Kim (2024)",
      "authors": "Kamruzzaman, M.; Kim, G. L.",
      "year": 2024,
      "doi": "10.48550/arXiv.2404.17218",
      "url": "https://doi.org/10.48550/arXiv.2404.17218",
      "abstract": "Study evaluates 12 prompt-engineering strategies to reduce social biases in Large Language Models using dual-process theory. Findings show Chain-of-Thought prompting follows System 1 patterns rather than expected System 2 reasoning. Human Persona combined with System 2 prompting proved most effective, reducing stereotypical responses by up to 13%. Demonstrates that transparency in prompt design and systematic bias-mitigation approaches contribute significantly to building trust in LLM recommenda",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, Prompting=Ja) und SOZIAL_OK (Bias_Ungleichheit=Ja, Fairness=Ja). Substantieller Fokus auf Prompt-Engineering-Strategien zur Bias-Reduktion in LLMs. Direkte Behandlung sozialer Bias und Fairness-Ansätze. Nicht auf Soziale Arbeit fokussier"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Gender",
          "Diversitaet"
        ]
      }
    },
    {
      "id": "ZHQMHHPQ",
      "title": "AI algorithm transparency, pipelines for trust not prisms: mitigating general negative attitudes and enhancing trust toward AI",
      "author_year": "Park & Yoon (2025)",
      "authors": "Park, K.; Yoon, H. Y.",
      "year": 2025,
      "doi": "10.1057/s41599-025-05116-z",
      "url": "https://www.nature.com/articles/s41599-025-05116-z",
      "abstract": "Peer-reviewed study examining how algorithmic transparency overcomes users' predisposed distrust in AI. 2×2 experiment showing transparency significantly increases trust, especially for those with initially negative AI attitudes. Making AI workings visible mitigated negative relationship between user AI-skepticism and organizational trust. Transparency acts as \"signal of trustworthiness,\" reducing skepticism and uncertainty. Emphasizing transparency in prompts and system design can reassure wary",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Prompting",
          "KI_Sonstige",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper adressiert AI-Vertrauen, Transparenz und Fairness-Aspekte (TECHNIK-Seite erfüllt). Jedoch fehlt der Sozialarbeitsbezug und kein expliziter Fokus auf Ungleichheit oder Marginalisierung. Allgemeine organisationale Trust-Studie ohne Sozialbereich-Spezifika."
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "Prompting",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ]
      }
    },
    {
      "id": "NUYCHW2T",
      "title": "Transparency enhances positive perceptions of social artificial intelligence",
      "author_year": "Xu et al. (2023)",
      "authors": "Xu, Y.; Bradford, N.; Garg, R.",
      "year": 2023,
      "doi": "10.1155/2023/5550418",
      "url": "",
      "abstract": "Experimental study with 914 participants testing how transparent explanations about chatbot workings affect user perceptions. Results show transparency modestly improved trust-related perceptions: users found chatbot \"less creepy,\" felt more affinity, and perceived it as more socially intelligent when transparent explanation provided. Transparency's impact on perceived intelligence stronger for users with low prior AI knowledge. Suggests prompt-engineering for greater transparency can foster use",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt TECHNIK-Kriterium (AI_Literacies: Ja - Nutzer-Kompetenzen/Wissen über KI; Generative_KI: Ja - Chatbot-fokussiert; Prompting: Ja - Transparenz-Strategien für Prompt-Engineering erwähnt). Aber SOZIAL-Kriterium nicht erfüllt: Kein direkter Bezug zu Sozialer Arbeit, keine substantielle Beh"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "Bias_Ungleichheit",
          "Fairness"
        ]
      }
    },
    {
      "id": "7AS5MAU9",
      "title": "Mitigating trust-induced inappropriate reliance on AI assistance",
      "author_year": "Srinivasan & Thomason (2025)",
      "authors": "Srinivasan, T.; Thomason, J.",
      "year": 2025,
      "doi": "",
      "url": "https://arxiv.org/pdf/2502.13321.pdf",
      "abstract": "Investigates trust-adaptive interventions to reduce inappropriate reliance on AI recommendations in decision-support tasks. Argues that AI assistants should adapt behavior based on user trust to mitigate both under- and over-trust. In two decision scenarios shows that providing supportive explanations for low trust and counter-explanations for high trust leads to 38% reduction in inappropriate reliance and 20% improvement in decision accuracy. Demonstrates that adaptive insertion of forced pause",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt Trust-Management in AI-Systemen und Fairness durch adaptive Interventionen (TECHNIK: AI_Literacies, KI_Sonstige, Fairness erfüllt). Es befasst sich jedoch nicht mit sozialer Arbeit, Bias/Ungleichheit, Gender, Diversität oder feministischen Perspektiven (SOZIAL nicht erfüllt). Fokus l"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ]
      }
    },
    {
      "id": "L48P8FBG",
      "title": "Artificial intelligence in social work: An EPIC model for practice",
      "author_year": "Goldkind et al. (2024)",
      "authors": "Goldkind, L.; Hamama-Raz, Y.; Levitats, Z.; Levin, L.; Ben-Ezra, M.",
      "year": 2024,
      "doi": "10.1080/0312407X.2025.2488345",
      "url": "https://doi.org/10.1080/0312407X.2025.2488345",
      "abstract": "Develops EPIC model (Ethics and justice, Policy development and advocacy, Intersectoral collaboration, Community engagement and empowerment) as structured approach for AI integration in social work. Emphasizes that AI integration must align with social work mission and values through four overlapping components. Ethics and justice aspect addresses bias mitigation and transparent use, while policy development prioritizes client protection. Warns against excessive calibration of AI systems toward ",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Inklusionskriterien: TECHNIK (AI_Literacies, KI_Sonstige) durch EPIC-Framework für KI-Integration und Bias-Mitigation; SOZIAL (Soziale_Arbeit, Bias_Ungleichheit, Fairness) durch direkten Bezug zu Sozialer Arbeit, Ethik, Transparenz und Fairness-Thematisierung. Substantielle Behan"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "U9ACKGB4",
      "title": "A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization",
      "author_year": "Bisconti et al. (2024)",
      "authors": "Bisconti, P.; Aquilino, L.; Marchetti, A.; Nardi, D.",
      "year": 2024,
      "doi": "",
      "url": "https://philarchive.org/archive/BISAFA",
      "abstract": "Develops formal conceptualization of AI trustworthiness connecting intrinsic and perceived trustworthiness in operationalizable schema. Argues that trustworthiness extends beyond inherent AI system capabilities to include significant influences from observer perceptions such as perceived transparency, agency locus, and human oversight. Identifies three central perceived characteristics: transparency (access to information about functionality), agency locus (perception of action autonomy source),",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt AI trustworthiness aus theoretischer, KI-ethischer Perspektive (KI_Sonstige: Ja). Es bietet jedoch keinen Bezug zu sozialen Anwendungsdomänen, Bias, Gender, Diversität, Fairness oder Soziale Arbeit. Es ist eine rein konzeptionelle Formalisierung von Vertrauen in KI-Systemen ohne "
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "2YS85B49",
      "title": "Can LLMs reason about trust? A pilot study",
      "author_year": "Debnath et al. (2024)",
      "authors": "Debnath, A.; Cranefield, S.; Lorini, E.; Savarimuthu, B. T. R.",
      "year": 2024,
      "doi": "",
      "url": "https://arxiv.org/html/2507.21075v1",
      "abstract": "Pilot study investigating LLMs' ability to reason about trust between individuals by capturing complex mental states that exceed traditional symbolic approaches. Evaluates whether LLMs can analyze conversations and assess trust levels based on willingness, competence, and security factors. Demonstrates that LLMs can identify individual goals and develop improvement suggestions for trust building, as well as generate strategic action plans for trust promotion. Suggests LLMs have potential to brid",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "Generative_KI"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper untersucht LLM-Fähigkeiten zur Vertrauensanalyse zwischen Individuen. Während Generative_KI (LLMs) relevant ist, fehlt jeglicher substantieller Bezug zu den sozialen Kategorien: kein direkter Sozialarbeitsbezug, keine Bias/Ungleichheit-Analyse, kein Gender-, Diversitäts- oder Fairness-Fokus. D"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ]
      }
    },
    {
      "id": "Z4YXX9PZ",
      "title": "Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes",
      "author_year": "Kamruzzaman & Kim (2024)",
      "authors": "Kamruzzaman, M.; Kim, G. L.",
      "year": 2024,
      "doi": "10.48550/arXiv.2404.17218",
      "url": "https://doi.org/10.48550/arXiv.2404.17218",
      "abstract": "Study evaluates 12 prompt-engineering strategies to reduce social biases in Large Language Models using dual-process theory. Findings show Chain-of-Thought prompting follows System 1 patterns rather than expected System 2 reasoning. Human Persona combined with System 2 prompting proved most effective, reducing stereotypical responses by up to 13%. Demonstrates that transparency in prompt design and systematic bias-mitigation approaches contribute significantly to building trust in LLM recommenda",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting), SOZIAL_OK (Bias_Ungleichheit + Fairness). Substantieller Fokus auf Prompt-Engineering-Strategien zur Bias-Reduktion in LLMs mit empirischer Evaluation. Adressiert soziale Bias und Fairness-Aspekte von KI-Systemen, nicht aber So"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "GUMWKBN6",
      "title": "Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review",
      "author_year": "Asseri et al. (2024)",
      "authors": "Asseri, B.; Abdelaziz, E.; Al-Wabil, A.",
      "year": 2024,
      "doi": "10.48550/arXiv.2506.18199",
      "url": "https://doi.org/10.48550/arXiv.2506.18199",
      "abstract": "Systematic review following PRISMA guidelines analyzing prompt-engineering techniques to reduce cultural bias against Arabs and Muslims in LLMs, evaluating eight empirical studies (2021-2024). Identifies five primary approaches: Cultural Prompting, Affective Priming, Self-Debiasing techniques, structured multi-stage pipelines, and parameter-optimized continuous prompts. Structured multi-stage pipelines most effective with up to 87.7% bias reduction. Emphasizes accessibility of prompt engineering",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, Prompting=Ja: systematische Analyse von Prompt-Engineering-Techniken für LLMs). SOZIAL_OK (Bias_Ungleichheit=Ja: kultureller Bias gegen Araber und Muslime; Diversitaet=Ja: kulturelle Repräsentation; Fairness=Ja: Bias-Reduktion und Fairne"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "U3AJIXAJ",
      "title": "Recommendations for social work researchers and journal editors on the use of generative AI and large language models",
      "author_year": "Victor et al. (2023)",
      "authors": "Victor, B. G.; Sokol, R. L.; Goldkind, L.; Perron, B. E.",
      "year": 2023,
      "doi": "10.1086/726021",
      "url": "https://doi.org/10.1086/726021",
      "abstract": "Develops \"disruptive-disrupting\" framework for analyzing generative AI in social work research with concrete recommendations for researchers and journal editors. Emphasizes transparency and accountability requiring researchers to fully document AI use, disclose prompts, and take responsibility for AI-generated content. Argues that continuous education about AI construction, limitations, and ethical considerations is essential for developing appropriate trust in LLM systems.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Soziale_Arbeit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, Prompting=Ja, AI_Literacies=Ja durch Fokus auf Wissen/Kompetenzen für Forschende) und SOZIAL_OK (Soziale_Arbeit=Ja durch expliziten Bezug zu Social-Work-Forschung und -Praxis). Entwickelt Framework für KI-Einsatz in Sozialarbeit mit Empf"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Soziale_Arbeit"
        ]
      }
    },
    {
      "id": "HN7KKNYV",
      "title": "Artificial intelligence in social work: Emerging ethical issues",
      "author_year": "Reamer (2023)",
      "authors": "Reamer, F. G.",
      "year": 2023,
      "doi": "10.55521/10-020-205",
      "url": "https://doi.org/10.55521/10-020-205",
      "abstract": "Systematic analysis of ethical challenges in AI use within social work, developing comprehensive framework for ethical AI implementation. Identifies eight central ethical areas including informed consent, data privacy, transparency, misdiagnosis, client neglect, surveillance, scientific misconduct, and algorithmic bias. Emphasizes transparency as fundamental principle requiring social workers to inform clients about AI use. Develops eight-step protocol for ethical AI implementation including eth",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies + KI_Sonstige: Bezug zu ethischen Rahmenbedingungen und Implementierung von KI-Systemen in sozialer Arbeit) und SOZIAL_OK (Soziale_Arbeit + Bias_Ungleichheit + Fairness: Direkter Bezug zu Sozialarbeitspraxis, explizite Behandlung von algorit"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "UB9NK8KI",
      "title": "Scaling implicit bias analysis across transformer-based language models through embedding association test and prompt engineering",
      "author_year": "Zayed (2024)",
      "authors": "Zayed, O.",
      "year": 2024,
      "doi": "",
      "url": "https://www.mdpi.com/2076-3417/14/8/3483",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper adressiert impliziten Bias in Transformer-Sprachmodellen mittels Embedding Association Test und Prompt Engineering. Dies deckt Generative_KI, Prompting und Bias ab. Jedoch fehlt der Sozial-Kontext: Keine Bezüge zu Sozialer Arbeit, struktureller Benachteiligung oder konkreten sozialen Auswirkun"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "XG7RFFC7",
      "title": "Occupational employment statistics",
      "author_year": "U.S. Bureau of Labor Statistics (2023)",
      "authors": "",
      "year": 2023,
      "doi": "",
      "url": "https://www.bls.gov/oes/",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Dies ist ein Statistikbericht des U.S. Bureau of Labor Statistics über Beschäftigungsstatistiken nach Beruf. Es behandelt weder KI-Themen (TECHNIK-Kriterien nicht erfüllt) noch soziale Gerechtigkeit/Bias-Probleme im KI-Kontext (SOZIAL-Kriterien nicht erfüllt). Kein Bezug zu KI-Systemen oder deren Au"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "JRG3B3LE",
      "title": "Homoglyph unlearning: A novel approach to bias mitigation",
      "author_year": "Struppek (2024)",
      "authors": "Struppek, T.",
      "year": 2024,
      "doi": "",
      "url": "https://arxiv.org/html/2406.05602v1",
      "abstract": "",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.65,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt 'Homoglyph unlearning' als Bias-Mitigationsmethode (KI_Sonstige: Ja, technischer Fokus). Der Titel signalisiert explizit Bias Mitigation (Bias_Ungleichheit + Fairness: Ja). Allerdings fehlt das Abstract für vollständige Bewertung. TECHNIK_OK (KI_Sonstige) + SOZIAL_OK (Bias/Fairne"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "JGZDWMN3",
      "title": "Bias in decision-making for AI's ethical dilemmas: A comparative study of ChatGPT and Claude",
      "author_year": "Wu (2025)",
      "authors": "Wu, Z.",
      "year": 2025,
      "doi": "",
      "url": "https://arxiv.org/html/2501.10484v2",
      "abstract": "",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.3,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Titel deutet auf Generative KI und Bias hin, aber ohne Abstract oder Volltext ist die Einordnung spekulativ. Sicher sind: Generative_KI=Ja (ChatGPT/Claude), Bias_Ungleichheit=Ja (Titel erwähnt 'Bias'). ABER: Kein Bezug zu Sozialer Arbeit erkennbar. Die Themen 'ethical dilemmas' sind philosophisch, n"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "Prompting",
          "Gender",
          "Diversitaet"
        ]
      }
    },
    {
      "id": "NVZA58ML",
      "title": "A survey on fairness in large language models",
      "author_year": "Wang (2024)",
      "authors": "Wang, Z.",
      "year": 2024,
      "doi": "",
      "url": "https://file.mixpaper.cn/paper_store/2023/5ddea1cd-c031-433f-a09b-14e7754f7826.pdf",
      "abstract": "",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt Fairness in Large Language Models (Generative_KI: Ja, Fairness: Ja, Bias_Ungleichheit: Ja). Jedoch fehlt der erforderliche SOZIAL-Kontext: Es gibt keinen direkten Bezug zu Sozialer Arbeit, Gender-Perspektive, Diversität oder feministischen Ansätzen. Allgemeine Fairness-Diskussionen i"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "Prompting",
          "Gender",
          "Diversitaet",
          "Fairness"
        ]
      }
    },
    {
      "id": "Z9DNTBFF",
      "title": "Worst of both worlds: A comparative analysis of error in language and vision-language models",
      "author_year": "Srinivasan & Bisk (2024)",
      "authors": "Srinivasan, V.; Bisk, Y.",
      "year": 2024,
      "doi": "",
      "url": "https://arxiv.org/html/2405.20152v1",
      "abstract": "",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.4,
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Der Titel deutet auf eine technische Vergleichsstudie zu Sprachmodellen und Vision-Language-Modellen hin. Generative_KI und KI_Sonstige sind relevant; 'error' könnte auf Bias hinweisen. Allerdings: Ohne Abstract/Volltext kann nicht eingeschätzt werden, ob ein Sozialkontext (Soziale_Arbeit, Bias_Ungl"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "W8DFWR9L",
      "title": "Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development",
      "author_year": "Shin et al. (2025)",
      "authors": "Shin, J.; You, J.; Birhane, A.",
      "year": 2025,
      "doi": "",
      "url": "https://pubsonline.informs.org/doi/10.1287/ijoc.2024.0645",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.72,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt Bias-Mitigation in LLMs mit Fokus auf altersbedingte Diskriminierung. Generative_KI (LLMs), Bias_Ungleichheit (Age Bias), Diversitaet (Age-related groups), und Fairness (Debiasing-Strategien) sind substantiell adressiert. TECHNIK_OK (Generative_KI=Ja) und SOZIAL_OK (Bias_Ungleich"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "M7AGB7LI",
      "title": "Model explanations for gender and ethnicity bias mitigation in AI-generated narratives",
      "author_year": "Salecha & Srijith (2025)",
      "authors": "Salecha, A.; Srijith, P. K.",
      "year": 2025,
      "doi": "",
      "url": "https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=7888&context=open_access_etds",
      "abstract": "",
      "item_type": "thesis",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.75,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper adressiert explizit Bias-Mitigation in KI-generierten Narrativen mit Fokus auf Gender und Ethnizität. Generative_KI (narratives generation), Bias_Ungleichheit (gender and ethnicity bias), Gender (expliziter Gender-Fokus), Diversitaet (ethnicity representation) und Fairness (bias mitigation"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "6MJYP7ZX",
      "title": "Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review",
      "author_year": "Prakash & Lee (2023)",
      "authors": "Prakash, A.; Lee, S.",
      "year": 2023,
      "doi": "",
      "url": "https://www.researchgate.net/publication/392942981_Prompt_Engineering_Techniques_for_Mitigating_Cultural_Bias_Against_Arabs_and_Muslims_in_Large_Language_Models_A_Systematic_Review",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt substantiell Prompting-Techniken (TECHNIK) zur Bias-Mitigation gegen Arabs/Muslims in LLMs (SOZIAL). Erfüllt beide Bedingungen: Generative_KI + Prompting (TECHNIK) und Bias_Ungleichheit + Diversitaet + Fairness (SOZIAL). Kein expliziter Soziale_Arbeit-Bezug, aber kulturelle Diskr"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "YUVR5YNQ",
      "title": "Self-debiasing large language models: Zero-shot recognition and reduction of stereotypes",
      "author_year": "Parrish & de-Arteaga (2025)",
      "authors": "Parrish, A.; de-Arteaga, M.",
      "year": 2025,
      "doi": "",
      "url": "https://aclanthology.org/2025.naacl-short.74.pdf",
      "abstract": "",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt deutlich Generative KI (LLMs) und Prompting-Strategien zur Bias-Reduktion (Self-Debiasing). Es adressiert auch Fairness und Bias. Jedoch: Keine direkten sozialen Aspekte im Sinne der Review-Kriterien (Soziale_Arbeit, strukturelle Ungleichheit, Gender-Fokus, Diversität, oder femin"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "YPYQ2TCL",
      "title": "Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia",
      "author_year": "Mei et al. (2023)",
      "authors": "Mei, K.; Li, J.; Zhao, J.",
      "year": 2023,
      "doi": "",
      "url": "https://arxiv.org/pdf/2505.17045",
      "abstract": "",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper untersucht Bias in GPT (Generative_KI: Ja) durch eine intersektionale Fallstudie (Feministisch: Ja - Intersektionalitätsansatz nach Crenshaw). Es thematisiert Diskriminierung gegen stigmatisierte Gruppen (Bias_Ungleichheit: Ja) und Diversität von Gruppen (Diversitaet: Ja). Fairness ist imp"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Gender",
          "Feministisch"
        ]
      }
    },
    {
      "id": "64DQYVVB",
      "title": "More or less wrong: A benchmark for directional bias in LLM comparative reasoning",
      "author_year": "Liu et al. (2025)",
      "authors": "Liu, H.; Sferrazza, C.; Lupu, Y.",
      "year": 2025,
      "doi": "",
      "url": "https://arxiv.org/html/2506.03923v1",
      "abstract": "",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt Bias und Fairness in LLMs durch ein Benchmark für direktionalen Bias. Dies erfüllt TECHNIK-Bedingung (Generative_KI, Bias_Ungleichheit, Fairness). JEDOCH: Keine Verbindung zu Soziale_Arbeit erkennbar. Das Paper fokussiert auf technische Bias-Metriken in LLMs, nicht auf sozialarbeiter"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "RAY6G2R7",
      "title": "SWIFTSAGE: A new dual-module framework for better action planning in complex interactive reasoning tasks",
      "author_year": "Lin et al. (2024)",
      "authors": "Lin, S.; Chen, M.; Yih, W.",
      "year": 2024,
      "doi": "",
      "url": "https://arxiv.org/html/2404.17218v3",
      "abstract": "",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.4,
        "categories": [
          "Generative_KI",
          "Prompting"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper adressiert offenbar generative KI und Prompting-Strategien (Dual-Module Framework für Action Planning mit LLMs). Jedoch: Kein Abstract, fehlender Volltext. Titel suggeriert rein technisches ML-Framework ohne erkennbaren Bezug zu Sozialer Arbeit, Fairness, Bias oder verwandten sozialen Dimensio"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "QY6P4RGQ",
      "title": "BBQ: A hand-built bias benchmark for question answering",
      "author_year": "Parrish et al. (2022)",
      "authors": "Parrish, A.; Chen, A.; Nangia, N.; Padmakumar, V.; Bowman, S. R.",
      "year": 2022,
      "doi": "",
      "url": "",
      "abstract": "",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "BBQ ist ein etablierter Benchmark für Bias-Evaluationen in QA-Systemen (KI-Sonstige: Ja; Generative_KI: Ja, da oft für LLM-Evaluation genutzt). Das Paper adressiert substantiell Bias, Fairness und Diversität mit explizitem Gender-Fokus. Keine Soziale_Arbeit. Technik+Sozial erfüllt → Include."
      },
      "human": {
        "decision": "Include",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "H3STST88",
      "title": "Large language models are zero-shot reasoners",
      "author_year": "Kojima et al. (2022)",
      "authors": "Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; Iwasawa, Y.",
      "year": 2022,
      "doi": "",
      "url": "",
      "abstract": "",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Prompting"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt Chain-of-Thought Prompting für LLMs (Generative_KI=Ja, Prompting=Ja). Dies ist eine rein technische Methoden-Studie ohne Bezug zu sozialen Themen. Keine Kategorien aus dem SOZIAL-Set (Soziale_Arbeit, Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness) sind erfüllt. Damit "
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Generative_KI",
          "Prompting"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Unclear",
        "severity": "low",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "RM833D5N",
      "title": "On the steerability of large language models",
      "author_year": "He et al. (2024)",
      "authors": "He, Z.; Bhargava, P.; D’Amour, A.",
      "year": 2024,
      "doi": "",
      "url": "https://aclanthology.org/2025.naacl-long.400.pdf",
      "abstract": "",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.6,
        "categories": [
          "Generative_KI",
          "Prompting"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt Steerability von LLMs (Generative_KI: Ja, Prompting: Ja). Dies sind Technik-Kategorien. Jedoch fehlt jeglicher Bezug zu sozialen Aspekten (Bias, Fairness, Soziale Arbeit, etc.). Ohne Abstract kann keine substantielle Einschätzung des Inhalts erfolgen, aber auf Basis des Titels liegt "
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ]
      }
    },
    {
      "id": "ACDF4FL9",
      "title": "Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes",
      "author_year": "Chisca et al. (2024)",
      "authors": "Chisca, R.; Saltaformaggio, A.; Verma, G.",
      "year": 2024,
      "doi": "",
      "url": "https://arxiv.org/html/2404.17218v3",
      "abstract": "",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper fokussiert auf Prompting-Techniken (substantiell im Titel) und Generative KI (LLMs). Adressiert social bias reduction - direkt relevant für Bias_Ungleichheit und Fairness. TECHNIK erfüllt (Generative_KI + Prompting), SOZIAL erfüllt (Bias_Ungleichheit + Fairness). Kein Soziale_Arbeit-Bezug, abe"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Generative_KI",
          "Prompting",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ]
      }
    },
    {
      "id": "VEJBIZRR",
      "title": "Exploring complex mental health symptoms via classifying social media data with explainable LLMs",
      "author_year": "Chen et al. (2024)",
      "authors": "Chen, K.; Lim, N.; Lee, C.; Guerzhoy, M.",
      "year": 2024,
      "doi": "",
      "url": "https://www.arxivdaily.com/thread/62478",
      "abstract": "",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.35,
        "categories": [
          "Generative_KI",
          "KI_Sonstige"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Der Titel deutet auf LLM-basierte Klassifikation von Mentalhygiene-Daten hin (Generative_KI + KI_Sonstige erfüllt, TECHNIK_OK). Jedoch: kein Abstract verfügbar, daher nicht erkennbar, ob ein substantieller Bezug zu sozialen/ethischen Dimensionen (SOZIAL_OK) besteht. Mental Health ist nicht automatis"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "AI_Literacies",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ]
      }
    },
    {
      "id": "QIQR449A",
      "title": "A sociolinguistic approach to stereotype assessment in large language models",
      "author_year": "Klinge & Kjeldsen (2024)",
      "authors": "Klinge, A.; Kjeldsen, K.",
      "year": 2024,
      "doi": "",
      "url": "https://facctconference.org/static/docs/facct2025-206archivalpdfs/facct2025-final1618-acmpaginated.pdf",
      "abstract": "",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.75,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Gender"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper analysiert Stereotypen in LLMs aus soziolinguistischer Perspektive. Erfüllt TECHNIK-Bedingung durch Generative_KI (Fokus auf LLMs). Erfüllt SOZIAL-Bedingung durch Bias_Ungleichheit (Stereotype-Assessment) und Gender (implizit durch Stereotype-Analyse). Sociolinguistischer Zugang deutet auf Ana"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "55ZR64VU",
      "title": "Debiasing prompts for gender bias in large language models",
      "author_year": "Kaneko & Bollegala (2024)",
      "authors": "Kaneko, K.; Bollegala, D.",
      "year": 2024,
      "doi": "",
      "url": "https://arxiv.org/html/2404.17218v3",
      "abstract": "",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt Prompting-Strategien zur Reduktion von Gender-Bias in LLMs—klare technische Komponente mit Fairness-Fokus. Allerdings fehlt jeder Bezug zu Sozialer Arbeit, Zielgruppen sozialer Dienste oder strukturellen sozialen Implikationen. Es ist eine Standard-Debiasing-Studie ohne sozialarbeitl"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "Diversitaet"
        ]
      }
    },
    {
      "id": "7IVS7X63",
      "title": "Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia",
      "author_year": "Jiang et al. (2022)",
      "authors": "Jiang, H.; Kim, B.; Lipton, Z. C.",
      "year": 2022,
      "doi": "",
      "url": "https://arxiv.org/pdf/2505.17045",
      "abstract": "",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper untersucht systematisch Bias in GPT gegenüber stigmatisierten Gruppen (Generative_KI: Ja). Der Fokus auf Nationalität und psychische Erkrankungen adressiert Bias und Ungleichheit (Bias_Ungleichheit: Ja). Der explizit erwähnte intersektionale Ansatz deutet auf feministische Methodik hin (Fe"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "ST9UCTJE",
      "title": "Counterfactual fairness in text classification through robustness",
      "author_year": "Garg et al. (2019)",
      "authors": "Garg, S.; Perot, V.; Lim, T.; Lipton, Z. C.",
      "year": 2019,
      "doi": "",
      "url": "",
      "abstract": "",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.75,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper adressiert Counterfactual Fairness in Text Classification - ein KI/ML-Thema (KI_Sonstige: Ja). Fokus auf Fairness und Robustheit deutet auf Bias-Mitigation hin (Fairness: Ja, Bias_Ungleichheit: Ja). TECHNIK und SOZIAL sind erfüllt. Ohne Abstract ist Confidence begrenzt, aber Titel und Autorsch"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "SQYLQFRU",
      "title": "Reasoning towards fairness: Mitigating bias in language models through reasoning-guided fine-tuning",
      "author_year": "Furniturewala et al. (2024)",
      "authors": "Furniturewala, A.; Zhang, A.; Chang, K.",
      "year": 2024,
      "doi": "",
      "url": "https://powerdrill.ai/discover/summary-reasoning-towards-fairness-mitigating-bias-in-cm9af0g1h7sb507pn7f5qh32q",
      "abstract": "",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt Sprachmodelle (Generative_KI: Ja) mit explizitem Fokus auf Bias-Mitigation und Fairness (Bias_Ungleichheit: Ja, Fairness: Ja). Der Titel signalisiert einen technischen Ansatz zur Reduktion von Bias durch Fine-Tuning. TECHNIK_OK (Generative_KI) und SOZIAL_OK (Fairness + Bias_Ungle"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "KG8JRLRQ",
      "title": "Measuring and mitigating unintended bias in text data",
      "author_year": "Dixon et al. (2018)",
      "authors": "Dixon, L.; Li, J.; Sorensen, J.; Thain, N.; Vasserman, L.",
      "year": 2018,
      "doi": "",
      "url": "",
      "abstract": "",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.75,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt Bias-Mitigation und Fairness in ML-Systemen (KI_Sonstige: Ja, Bias_Ungleichheit: Ja, Fairness: Ja). Dies ist ein etabliertes Paper zur algorithmischen Fairness. TECHNIK-Bedingung erfüllt (KI_Sonstige), SOZIAL-Bedingung erfüllt (Bias_Ungleichheit + Fairness). Kein Sozialarbeitsbezug e"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "GCN42PAM",
      "title": "Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development",
      "author_year": "Birru et al. (2024)",
      "authors": "Birru, G. T.; Shin, J.; You, J.; Birhane, A.",
      "year": 2024,
      "doi": "",
      "url": "https://pubsonline.informs.org/doi/10.1287/ijoc.2024.0645",
      "abstract": "",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.75,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt Large Language Models (Generative_KI=Ja) und fokussiert auf Bias-Mitigation bezüglich Altersmerkmalen (Bias_Ungleichheit=Ja, Diversitaet=Ja, Fairness=Ja). Titel und Fokus auf 'responsible AI development' und 'mitigating bias' sind substantiell. TECHNIK_OK (Generative_KI) + SOZIAL"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ]
      }
    },
    {
      "id": "CYZQ6XPK",
      "title": "Evaluating gender bias in large language models via chain-of-thought prompting",
      "author_year": "Kaneko et al. (2024)",
      "authors": "Kaneko, M.; Bollegala, D.; Okazaki, N.; Baldwin, T.",
      "year": 2024,
      "doi": "",
      "url": "https://arxiv.org/abs/2401.15585",
      "abstract": "This study investigates if Chain-of-Thought (CoT) prompting reduces implicit gender bias in LLMs. Using a synthetic task of counting gendered words, the authors found that without step-by-step prompting, models made biased errors. CoT prompting, which forced the model to explicitly label each word's gender before counting, significantly reduced these mistakes. This suggests that guiding the model through an explicit reasoning process makes it rely more on logic than on stereotypes, thereby mitig",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt substantiell Generative KI (LLMs), Prompting (Chain-of-Thought als zentrale Intervention), Gender-Bias in LLM-Outputs und Fairness-Aspekte (Bias-Reduktion). Erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting) und SOZIAL_OK (Bias_Ungleichheit + Gender + Fairness). Experi"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ]
      }
    },
    {
      "id": "EHQBHVYV",
      "title": "Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review",
      "author_year": "Asseri et al. (2025)",
      "authors": "Asseri, B.; Abdelaziz, E.; Al-Wabil, A.",
      "year": 2025,
      "doi": "",
      "url": "https://arxiv.org/html/2506.18199",
      "abstract": "This systematic review of 8 studies (2021–2024) identifies five prompt engineering approaches to mitigate bias against Arabs and Muslims: self-debiasing, cultural context prompting, affective priming, structured multi-step pipelines, and continuous prompt tuning. Multi-step pipelines were most effective, reducing biased content by up to ~88%, while simpler methods like cultural prompts showed ~71–81% improvement. The review concludes that while prompt engineering can mitigate biases without retr",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting substantiell behandelt), SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Fairness adressiert). Systematische Review zu Prompt-Engineering-Techniken für Bias-Mitigation gegen Arabs/Muslims in LLMs mit quantifizierten Ergebnissen. Fokus"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "22KJL3PC",
      "title": "The cultural stereotype and cultural bias of ChatGPT",
      "author_year": "Yuan et al. (2025)",
      "authors": "Yuan, H.; Che, Z.; Zhang, Y.; Li, S.; Yuan, X.; Huang, L.; Luo, S.",
      "year": 2025,
      "doi": "10.1177/18344909251355673",
      "url": "https://www.researchgate.net/publication/393408216_The_cultural_stereotype_and_cultural_bias_of_ChatGPT",
      "abstract": "This article examines cultural biases in ChatGPT-3.5 and GPT-4. Study 1 measures alignment with human cultural values. Study 2 finds clear cultural stereotypes in GPT-3.5 but fewer in GPT-4. Study 3 tests four diversity-sensitive prompts (emphasizing individuality, fairness, egalitarian futures, or multiculturalism). All four strategies eliminated cultural stereotypes in GPT-3.5's outputs. For GPT-4, bias mitigation was more nuanced, requiring task-specific prompts. This indicates that while var",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt TECHNIK-Kriterium: Fokus auf Generative KI (ChatGPT-3.5/GPT-4), substantielle Analyse von Prompting-Strategien zur Bias-Reduktion. Erfüllt SOZIAL-Kriterium: Explizite Behandlung von Bias (kulturelle Stereotype), Diversität (diversity-sensitive prompts) und Fairness (Bias-Mitigation-Str"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "RARE5UFC",
      "title": "Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes",
      "author_year": "Kamruzzaman & Kim (2024)",
      "authors": "Kamruzzaman, M.; Kim, G. L.",
      "year": 2024,
      "doi": "",
      "url": "https://arxiv.org/html/2404.17218v1",
      "abstract": "This study evaluates 12 prompt strategies across five LLMs, finding that instructing a model to adopt a System 2 (deliberative) reasoning style and a \"human persona\" most effectively reduces stereotypes. Combining these two strategies yielded up to a 13% reduction in stereotypical responses. Contrary to prior assumptions, Chain-of-Thought (CoT) prompting alone was not as effective, showing bias levels similar to a default prompt. The results suggest that prompts encouraging careful, human-like r",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt systematisch Prompting-Strategien (Ja: substantieller Fokus auf 12 Prompt-Varianten) zur Bias-Reduktion in LLMs (Ja: Generative_KI). Bias-Reduktion und Fairness (Ja: beide Kategorien) sind zentrale Themen. TECHNIK-Bedingung erfüllt (Prompting + Generative_KI), SOZIAL-Bedingung erfüll"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Fairness"
        ]
      }
    },
    {
      "id": "YMABYPKF",
      "title": "What’s in a name? Auditing large language models for race and gender bias",
      "author_year": "Salinas et al. (2025)",
      "authors": "Salinas, A.; Haim, A.; Nyarko, J.",
      "year": 2025,
      "doi": "",
      "url": "https://arxiv.org/html/2402.14875v3",
      "abstract": "This interdisciplinary audit of GPT-4 and other LLMs reveals systematic intersectional biases based on names signaling race and gender. Prompts with names suggesting a Black woman received less favorable advice compared to those with white male names. This disparity was robust across 42 prompt templates. The study found that adding quantitative anchors (facts, numbers) to the prompt largely eliminated this bias, whereas adding qualitative descriptive details had inconsistent effects and sometime",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper adressiert Generative KI (GPT-4, LLMs) mit Fokus auf Prompting (42 Prompt-Templates als Audit-Instrument). Es thematisiert explizit intersektionalen Bias (Rasse × Geschlecht nach Crenshaw-Logik), Gender-Bias und Fairness. Die intersektionale Analyse erfüllt den feministischen Kriterium. TE"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "ZLMLP53P",
      "title": "Queer in AI: A case study in community-led participatory AI",
      "author_year": "Cvoelcker (2023)",
      "authors": "Cvoelcker, C.",
      "year": 2023,
      "doi": "",
      "url": "https://cvoelcker.de/assets/pdf/paper_queer_in_ai.pdf",
      "abstract": "Fallstudie zu Queer in AI, dokumentiert Schäden durch KI-Systeme an queeren Menschen und beschreibt community-geleitete Strategien für partizipative, faire KI.",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt algorithmische Systeme und deren Schäden (KI_Sonstige=Ja). Community-geleiteter Ansatz zur fairen KI adressiert Fairness, Bias/Ungleichheit und Diversität. Queer-Fokus involviert Gender-Perspektive und intersektionale Analysen (feministisch). Erfüllt beide Bedingungen: Technik (KI_So"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "4LY3SA4E",
      "title": "Flexible intersectional stereotype extraction (FISE): Analyzing intersectional biases in large language models",
      "author_year": "Charlesworth (2024)",
      "authors": "Charlesworth, T.",
      "year": 2024,
      "doi": "",
      "url": "https://news.northwestern.edu/stories/2024/03/kellogg-study-suggests-that-some-intersectional-groups-are-more-represented-than-others-in-internet-text/",
      "abstract": "Studie entwickelt FISE-Methode zur Messung intersektionaler Repräsentationsverzerrungen. Zeigt massive Dominanz weißer Männer in Internettexten und Ableitung entsprechender LLM-Biases.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper analysiert LLM-Biases mit FISE-Methode (Generative_KI: Ja). Fokus auf intersektionale Repräsentationsverzerrungen erfüllt Bias_Ungleichheit, Gender (Geschlechterstereotype), Diversitaet (Repräsentation). Intersektionale Analyse nach Crenshaw ist explizit feministisch (Feministisch: Ja). Messun"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "TSYJ3Y57",
      "title": "Women4Ethical AI: Global cooperation for gender-inclusive AI",
      "author_year": "UNESCO (2024)",
      "authors": "",
      "year": 2024,
      "doi": "",
      "url": "https://www.unesco.org/en/articles/unescos-women4ethical-ai-urges-global-cooperation-gender-inclusive-ai",
      "abstract": "UNESCO-Initiative zur Förderung genderinklusiver KI-Entwicklung. Fokus auf globale Zusammenarbeit, Menschenrechtsprinzipien und Expertinnenbeteiligung in allen Phasen.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt TECHNIK-Kriterien (AI_Literacies für KI-Kompetenzen/Beteiligung; KI_Sonstige für allgemeine KI-Entwicklung; Fairness für genderinklusive/faire KI-Systeme) und SOZIAL-Kriterien (Gender explizit im Titel; Bias_Ungleichheit durch Menschenrechtsfokus; Diversitaet durch globale Kooperation "
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "A776TPGG",
      "title": "The EU artificial intelligence act through a gender lens",
      "author_year": "Friedrich-Ebert-Stiftung (2025)",
      "authors": "",
      "year": 2025,
      "doi": "",
      "url": "https://library.fes.de/pdf-files/bueros/bruessel/21887-20250304.pdf",
      "abstract": "Politikanalyse des EU AI Acts mit Fokus auf Geschlechtergerechtigkeit. Identifiziert Potenziale und Lücken im Gesetzestext und gibt konkrete Empfehlungen zur Umsetzung.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.75,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper analysiert EU AI Act durch Gender-Lens: KI_Sonstige (algorithmische Systeme unter Regulierung) + Gender (expliziter Geschlechterfokus) + Bias_Ungleichheit (Geschlechtergerechtigkeit) + Fairness (Regulierungsanalyse). Beide Bedingungen erfüllt: TECHNIK_OK (KI_Sonstige), SOZIAL_OK (Gender + Bias"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "MTMU9UPJ",
      "title": "DR.GAP: Mitigating bias in large language models using gender-aware prompting with demonstration and reasoning",
      "author_year": "Qiu et al. (2025)",
      "authors": "Qiu, H.; Xu, Y.; Qiu, M.; Wang, W.",
      "year": 2025,
      "doi": "",
      "url": "https://arxiv.org/html/2502.11603v1",
      "abstract": "DR.GAP ist eine prompting-basierte Methode zur Bias-Reduktion in LLMs. Sie nutzt Beispielfälle und strukturierte Reasoning-Schritte, um gendergerechtere Antworten zu erzielen.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt TECHNIK-Bedingung (Generative_KI, Prompting). Jedoch fehlt SOZIAL-Bedingung für Anwendungsgebiet mit sozialem Fokus: Bias_Ungleichheit, Gender und Fairness behandeln allgemeine LLM-Bias, nicht spezifisch Soziale Arbeit, marginalisierte Communities oder sozialarbeiterische Kontexte. Rei"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "Diversitaet",
          "Fairness"
        ]
      }
    },
    {
      "id": "CSJS9JGH",
      "title": "AI gender bias, disparities, and fairness: Does training data matter?",
      "author_year": "Latif et al. (2023)",
      "authors": "Latif, E.; Zhai, X.; Liu, L.",
      "year": 2023,
      "doi": "",
      "url": "https://arxiv.org/html/2312.10833v2",
      "abstract": "Empirische Analyse von Geschlechterbias in Bewertungssystemen mit BERT und GPT-3.5. Mixed-gender Trainingsdaten reduzierten Bias, aber verstärkten Unterschiede. Drei Bias-Metriken angewendet.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI: GPT-3.5, KI_Sonstige: BERT, Fairness-Metriken) und SOZIAL_OK (Gender-Bias explizit im Titel, Bias_Ungleichheit als Analysefokus, Fairness-Metriken substantiell behandelt). Empirische Studie zu Gender-Bias in KI-Systemen mit direktem Fairnes"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "BR2LG8LD",
      "title": "How AI hype impacts the LGBTQ+ community",
      "author_year": "Kumar & Gartner (2024)",
      "authors": "Kumar, A.; Gartner, M.",
      "year": 2024,
      "doi": "10.1007/s43681-024-00423-8",
      "url": "https://doi.org/10.1007/s43681-024-00423-8",
      "abstract": "Die Studie analysiert, wie der Hype um KI heteronormative Annahmen verstärkt. Sie führt Fallstudien zur Gesichtserkennung, Content-Moderation und Geschlechtsklassifikation durch und zeigt auf, wie queere Identitäten algorithmisch marginalisiert werden.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt KI-Sonstige (Computer Vision, Content-Moderation, Algorithmen) und adressiert multiple soziale Dimensionen: Bias_Ungleichheit (algorithmische Marginalisierung queerer Identitäten), Gender (Geschlechtsklassifikation, heteronormative Annahmen), Diversitaet (queere Communities), Feminis"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "5G2QTZYD",
      "title": "Feminist perspectives on AI: Ethical considerations in algorithmic decision-making",
      "author_year": "Ahmed (2024)",
      "authors": "Ahmed, U.",
      "year": 2024,
      "doi": "",
      "url": "https://www.researchcorridor.org/index.php/jgsi/article/download/330/314",
      "abstract": "Theoretische Arbeit zur feministischen KI-Ethik. Argumentiert, dass KI strukturell eingebettete Diskriminierung reproduziert und plädiert für partizipative, intersektionale Entwicklungspraxis.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja; algorithmische Entscheidungssysteme), SOZIAL_OK (Bias_Ungleichheit=Ja, Gender=Ja, Diversitaet=Ja, Feministisch=Ja). Explizit feministische Perspektive mit intersektionaler Analyse und Fokus auf strukturelle Diskriminierung. Kein Bezug zu S"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "6QPLNNQK",
      "title": "Bias against women and girls in large language models: A UNESCO study",
      "author_year": "UNESCO (2024)",
      "authors": "",
      "year": 2024,
      "doi": "",
      "url": "https://www.unesco.org/en/articles/generative-ai-unesco-study-reveals-alarming-evidence-regressive-gender-stereotypes",
      "abstract": "Die Studie analysiert LLMs (z. B. GPT-3.5, Llama 2) und dokumentiert signifikante Stereotypisierungen gegen Frauen und queere Menschen in generierten Texten. Besondere Auffälligkeit bei Open-Source-Modellen.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI=Ja, LLM-Analyse), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversität, Fairness alle Ja). Dokumentiert Gender-Stereotypen und algorithmischen Bias in LLMs systematisch. Kein explizit feministischer Theorie-Einsatz, daher Feministisch=Nein. Kein"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Unclear",
        "severity": "low",
        "affected_categories": [
          "Soziale_Arbeit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ]
      }
    },
    {
      "id": "WS4KQPWN",
      "title": "Intersectionality in artificial intelligence: Framing concerns and recommendations for action",
      "author_year": "Ulnicane (2024)",
      "authors": "Ulnicane, I.",
      "year": 2024,
      "doi": "10.17645/si.v12i1.7543",
      "url": "https://doi.org/10.17645/si.v12i1.7543",
      "abstract": "Die Arbeit analysiert vier Berichte zur Intersektionalität in KI, zeigt wie mangelnde Diversität zu biased KI-Systemen führt, und dokumentiert Vorurteile in Robotern, Sprachassistenten und HR-Tools.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (KI_Sonstige: algorithmische Systeme in Robotik, HR-Tools) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch: Intersektionalität ist etablierter feministischer Forschungsrahmen nach Crenshaw; Fairness). Substantielle Behandlung von Vorurteilen "
      },
      "human": {
        "decision": "Include",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "QD2CXE9I",
      "title": "Measuring gender and racial biases in large language models",
      "author_year": "An & et al. (2025)",
      "authors": "An, J.",
      "year": 2025,
      "doi": "10.1093/pnasnexus/pgaf089",
      "url": "https://academic.oup.com/pnasnexus/article/4/3/pgaf089/8071848",
      "abstract": "This study quantifies intersectional race–gender biases in LLMs, showing non-additive stereotype effects and supporting intersectionality theory. Results recommend layered prompt interventions to address these complex biases in narrative generation.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt generative KI (LLMs) mit substantiellem Fokus auf intersektionale Gender-Rassen-Biases. Explizite Bezüge zu Intersektionalitätstheorie (feministische Perspektive nach Crenshaw). Prompting wird als Interventionsstrategie adressiert. Erfüllt beide Bedingungen: TECHNIK (Generative_KI + "
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "HMDFMBV3",
      "title": "Intersectional Stereotypes in Large Language Models: Dataset and Analysis",
      "author_year": "Ma et al. (2023)",
      "authors": "Ma, W.; Chiang, B.; Wu, T.; Wang, L.; Vosoughi, S.",
      "year": 2023,
      "doi": "10.18653/v1/2023.findings-emnlp.575",
      "url": "https://aclanthology.org/2023.findings-emnlp.575.pdf",
      "abstract": "This EMNLP paper introduces a dataset for studying intersectional stereotypes and applies it to three LLMs. Results reveal emergent stereotypes not predictable from single-attribute analysis. Prompt engineering reduces but does not eliminate such patterns, highlighting persistent biases in generated narratives.",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt substantiell Generative KI (LLMs) und Prompting (Prompt Engineering zur Bias-Reduktion). SOZIAL-Kriterien erfüllt: (1) Bias_Ungleichheit (intersektionale Stereotypes analyse), (2) Gender (Geschlechter-Stereotypen in LLMs), (3) Diversitaet (multiple Attribute/Gruppen), (4) Feministisc"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "5UAHQESQ",
      "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review",
      "author_year": "Basseri et al. (2025)",
      "authors": "Basseri, K.; Abdelaziz, E.; Al-Wabil, A.",
      "year": 2025,
      "doi": "",
      "url": "https://arxiv.org/html/2506.18199v1",
      "abstract": "This systematic review identifies five major prompt engineering strategies to reduce cultural and intersectional bias in LLMs. Structured multi-step pipelines were most effective but complex, while cultural prompting offered a practical balance. Results show varying mitigation success depending on stereotype type, and emphasize trade-offs between bias reduction and performance.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + Prompting substantiell fokussiert) und SOZIAL (Bias_Ungleichheit + Diversitaet + Fairness adressieren kulturelle und intersektionale Diskriminierung gegen Araber und Muslime). Systematischer Review von Prompt-Engineering zur Bias-Mitigation i"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "UDZLIJWX",
      "title": "MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models",
      "author_year": "Yunusov et al. (2024)",
      "authors": "Yunusov, S.; Sidat, H.; Emami, A.",
      "year": 2024,
      "doi": "",
      "url": "https://arxiv.org/html/2409.13935v1",
      "abstract": "This empirical study introduces a corpus of 1,500 personalized short stories generated with LLMs, incorporating identity features like gender, ethnicity, and age. Human judges rated these stories higher in engagement, diversity, and personalness. Narrative personalization increased textual diversity without harming moral comprehension. However, biases persist, such as preferential engagement for certain identities. The paper illustrates both potential and limitations of diversity-sensitive promp",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + Prompting substantiell behandelt) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Fairness). Fokus auf LLM-generierte Stories mit explizitem Prompting für Diversität. Empirische Analyse von Bias und Engagement-Disparitäten nach Identitäts"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "6BZ5353S",
      "title": "Multilingual Prompting for Improving LLM Generation Diversity",
      "author_year": "Wang et al. (2025)",
      "authors": "Wang, Q.; Pan, S.; Linzen, T.; Black, E.",
      "year": 2025,
      "doi": "",
      "url": "https://arxiv.org/html/2505.15229v1",
      "abstract": "This study introduces multilingual prompting as a strategy to enhance narrative diversity in LLM outputs. By using prompts with diverse languages and cultural cues, models produced outputs with improved demographic and opinion diversity. Compared to temperature-based and persona prompting, multilingual prompting was more effective and reduced cultural hallucinations.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper adressiert TECHNIK-Aspekte durch Generative_KI (LLMs) und substantiell Prompting (multilingual prompting als zentrale Strategie). SOZIAL-Aspekte sind erfüllt durch Bias_Ungleichheit (kulturelle Halluzinationen, demographische Bias), Diversitaet (narrative und demographische Diversität) und"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "Z9BIKVS3",
      "title": "Avoiding Catastrophe Through Intersectionality in Global AI Governance",
      "author_year": "Laine & McCrory (2025)",
      "authors": "Laine, S.; McCrory, M.",
      "year": 2025,
      "doi": "",
      "url": "https://www.cigionline.org/publications/avoiding-catastrophe-through-intersectionality-in-global-ai-governance/",
      "abstract": "Dieses Working Paper nutzt einen feministischen Policy-Analyse-Rahmen, der auf fünf thematischen Bereichen basiert: Intersektionalität, Kontext, Neutralität, Macht und Gerechtigkeit. Die Forschung schlägt einen feministischen KI-Policy-Rahmen vor, der Entscheidungsträger und Stakeholder ermutigt, potenzielle KI-Sicherheitsprojekte in Übereinstimmung mit vier Zielen zu bewerten: Förderung der Intersektionalität, Bereitstellung diverser Kontexte, Bekämpfung der Neutralität und transformative Gerec",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt KI-Governance (KI_Sonstige: Ja) mit explizit feministischer Theorie (intersektionale Analyse nach Crenshaw-Tradition, Feministisch: Ja). Adressiert strukturelle Ungleichheiten, Bias und Gerechtigkeit (Bias_Ungleichheit, Gender, Diversitaet, Fairness alle Ja). Erfüllt beide Bedingunge"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "SSH3LVN6",
      "title": "Artificial Intelligence and Intersectionality",
      "author_year": "Ulnicane (2024)",
      "authors": "Ulnicane, I.",
      "year": 2024,
      "doi": "",
      "url": "https://ecpr.eu/news/news/details/749",
      "abstract": "Diese Analyse untersucht, wie KI-Dokumente Bedenken über Bias und Ungleichheit in KI rahmen und Empfehlungen zur Bekämpfung formulieren. Mittels intersektionaler Linse wird die Interaktion multipler Identitäten (Geschlecht, Rasse, Klasse) hervorgehoben, die zu Marginalisierung und Diskriminierung bestimmter sozialer Gruppen führt. Die Studie unterscheidet zwischen technischen und sozio-technischen Framings von KI-Bias und zeigt auf, dass technische Frames KI oft als objektiv und neutral darstell",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "TECHNIK_OK: KI_Sonstige=Ja (algorithmische Systeme und Bias-Analyse). SOZIAL_OK: Bias_Ungleichheit=Ja, Gender=Ja, Diversitaet=Ja, Feministisch=Ja (explizite intersektionale Perspektive nach Crenshaw), Fairness=Ja. Alle Bedingungen erfüllt. Paper nutzt feministische/intersektionale Theorie substantie"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "QZJL6KBZ",
      "title": "Algorithms, artificial intelligence and discrimination",
      "author_year": "Lund (2025)",
      "authors": "Lund, O.",
      "year": 2025,
      "doi": "",
      "url": "https://ldo.no/content/uploads/2025/05/Algorithms-artificial-intelligence-and-discrimination-report.pdf",
      "abstract": "Dieser norwegische Regierungsbericht überprüft Schlüsselelemente des norwegischen Gleichstellungs- und Antidiskriminierungsgesetzes mit primärem Fokus auf algorithmische Diskriminierung. Der Bericht diskutiert die mögliche Einführung spezifischer Definitionen direkter und indirekter algorithmischer Diskriminierung und schlägt die Schaffung einer spezifischen Bestimmung zu rechtmäßiger algorithmischer Differenzialbehandlung vor. Die Komplexität algorithmischer Systeme erschwert die Unterscheidung",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Der Bericht adressiert algorithmische Diskriminierung (KI_Sonstige: Ja) und behandelt substantiell Bias/Ungleichheit sowie Fairness im Kontext von Antidiskriminierungsrecht. Er erfüllt beide Bedingungen (TECHNIK + SOZIAL) und ist damit included, obwohl er keinen direkten Soziale-Arbeit-Bezug hat."
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "UBYTNGNV",
      "title": "The AI Act, gender equality and non-discrimination: what role for the AI Office?",
      "author_year": "Giannoni Adielsson & Öberg (2024)",
      "authors": "Giannoni Adielsson, M.; Öberg, J.",
      "year": 2024,
      "doi": "10.1007/s12027-024-00785-w",
      "url": "https://doi.org/10.1007/s12027-024-00785-w",
      "abstract": "Diese Analyse bewertet, ob der EU AI Act Fragen der Geschlechtergerechtigkeit und Nichtdiskriminierung ausreichend adressiert. Die substantiellen Bestimmungen des AI Acts werden durch die Linse von Gleichstellungs- und Antidiskriminierungsrecht analysiert, wobei vorgeschlagene Tools wie grundrechtliche Folgenabschätzungen und Bias-Audits zur Reduzierung von Geschlechterverzerrungen und Diskriminierungsrisiken hervorgehoben werden. Die Rolle des AI Office und seine Kooperation mit nationalen, eur",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper analysiert EU AI Act durch die Linse von Geschlechtergerechtigkeit und Nichtdiskriminierung. TECHNIK erfüllt: KI_Sonstige (algorithmische Systeme reguliert durch AI Act). SOZIAL erfüllt: Bias_Ungleichheit (Geschlechterverzerrungen, Diskriminierungsrisiken), Gender (expliziter Gender-Fokus), Fa"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "RZ4QFXQI",
      "title": "Discriminating Systems: Gender, Race, and Power in AI",
      "author_year": "West et al. (2023)",
      "authors": "West, S.M.; Whittaker, M.; Crawford, K.",
      "year": 2023,
      "doi": "",
      "url": "https://ainowinstitute.org/wp-content/uploads/2023/04/discriminatingsystems.pdf",
      "abstract": "Diese einflussreiche Studie argumentiert, dass die Diversitätskrise im KI-Sektor und Bias in KI-Systemen zwei Manifestationen desselben Problems sind und gemeinsam angegangen werden müssen. Die Autoren zeigen, dass rein technische Ansätze zur Bias-Behebung unzureichend sind, da sie die systemischen Machtverhältnisse ignorieren, die sowohl Arbeitsplätze als auch Technologien formen. Das \"Pipeline-Problem\"-Narrativ wird als zu eng kritisiert, da es tieferliegende Probleme mit Arbeitsplatzkultur, M",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Bias in KI-Systemen), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch als intersektionale Kritik an Machtverhältnissen, Fairness). Kritische Analyse von algorithmischen Systemen mit explizitem Gender- und Rassismus-Fokus sowie"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Unclear",
        "severity": "low",
        "affected_categories": [
          "Gender",
          "Feministisch"
        ]
      }
    },
    {
      "id": "K2KL8WH8",
      "title": "Revisiting Technical Bias Mitigation Strategies",
      "author_year": "Djiberou Mahamadou (2024)",
      "authors": "Djiberou Mahamadou, A. J.",
      "year": 2024,
      "doi": "",
      "url": "https://arxiv.org/abs/2410.17433",
      "abstract": "Diese systematische Überprüfung identifiziert praktische Limitationen technischer Bias-Mitigation-Strategien im Gesundheitswesen entlang fünf Schlüsseldimensionen: wer Bias und Fairness definiert, welche Mitigation-Strategie zu verwenden und zu priorisieren ist, wann in den KI-Entwicklungsstadien die Lösungen am effektivsten sind, für welche Populationen und in welchem Kontext die Lösungen entworfen sind. Die Studie zeigt mathematische Inkonsistenzen und Inkompatibilitäten zwischen verschiedenen",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt technische Bias-Mitigation und Fairness-Metriken (TECHNIK_OK), aber ausschließlich im Gesundheitswesen ohne Bezug zu Sozialer Arbeit. Während Bias_Ungleichheit und Fairness adressiert werden, fehlt ein direkter Bezug zu den definierten SOZIAL-Kategorien (Soziale_Arbeit, Gender, D"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "DWS4KXBW",
      "title": "Towards Substantive Equality in Artificial Intelligence: Transformative AI Policy for Gender Equality and Diversity",
      "author_year": "Ovalle (2024)",
      "authors": "Ovalle, A.",
      "year": 2024,
      "doi": "",
      "url": "https://datapopalliance.org/publications/towards-real-diversity-and-gender-equality-in-ai/",
      "abstract": "Dieser GPAI-Bericht, basierend auf Konsultationen mit über 200 Teilnehmern aus mehr als 50 Ländern, entwickelt einen menschenrechtsbasierten Rahmen für substantielle Gleichberechtigung in der KI. Der Bericht betont, dass KI ohne Intervention das Risiko birgt, gesellschaftliche Verzerrungen zu perpetuieren und zu verstärken, insbesondere gegen historisch marginalisierte Gruppen. Die Empfehlungen zielen darauf ab, die strukturellen Wurzeln der Ungleichheit zu bekämpfen und transformative Veränderu",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt KI-Systeme substantiell unter der Linse sozialer Gerechtigkeit. KI_Sonstige: Ja (algorithmische Systeme und deren gesellschaftliche Auswirkungen). Bias_Ungleichheit: Ja (expliziter Fokus auf Verzerrungen gegenüber marginalisierten Gruppen). Gender: Ja (Geschlechtergleichstellung "
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "K6JQ7SVA",
      "title": "Intersectional Fairness: A Fractal Approach",
      "author_year": "Zannone (2023)",
      "authors": "Zannone, S.",
      "year": 2023,
      "doi": "",
      "url": "https://arxiv.org/abs/2302.12683",
      "abstract": "Diese Studie rahmt intersektionale Fairness in einem geometrischen Setting und projiziert Daten auf einen Hyperkubus. Die Autoren beweisen mathematisch, dass Fairness \"nach oben\" propagiert - die Sicherstellung von Fairness für alle Untergruppen auf der niedrigsten intersektionalen Ebene führt notwendigerweise zu Fairness auf allen höheren Ebenen. Sie definieren eine Familie von Metriken zur Erfassung intersektionaler Verzerrung und schlagen vor, Fairness als \"fraktales\" Problem zu betrachten, b",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt KI/ML-Systeme (KI_Sonstige: mathematische Fairness-Metriken für Klassifikation). Substantieller Fokus auf intersektionale Fairness und algorithmischen Bias (Bias_Ungleichheit). Intersektionale Analyse nach Crenshaw ist explizit feministisches Konzept (Feministisch). Fairness-Metriken"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "NBYNRKBL",
      "title": "What is Feminist AI?",
      "author_year": "Wudel & Ehrenberg (2025)",
      "authors": "Wudel, A.; Ehrenberg, A.",
      "year": 2025,
      "doi": "",
      "url": "https://library.fes.de/pdf-files/bueros/bruessel/21888-20250304.pdf",
      "abstract": "Das Papier entwickelt einen Rahmen für Feminist AI (FAI), der intersektionale feministische Methodologie zur Adressierung von Bias und Ungleichheit in KI-Systemen nutzt. FAI betont interdisziplinäre Zusammenarbeit, systematische Machtanalyse und iterative Theorie-Praxis-Schleifen. Durch die Einbettung feministischer Werte (Gleichberechtigung, Freiheit, Gerechtigkeit) zielt FAI darauf ab, KI-Entwicklung zu transformieren und Inklusivität sowie soziale Nachhaltigkeit sicherzustellen. Praktische An",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige) durch Framework-Entwicklung für KI-Kompetenzentwicklung und Machtanalyse in KI-Systemen. SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness) durch explizite feministische Methodologie nach intersektio"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "XK6G84V7",
      "title": "Algorithmic discrimination: examining its types and regulatory measures with emphasis on US legal practices",
      "author_year": "Wang et al. (2024)",
      "authors": "Wang, X.; Wu, Y. C.; Ji, X.; Fu, H.",
      "year": 2024,
      "doi": "10.3389/frai.2024.1320277",
      "url": "https://doi.org/10.3389/frai.2024.1320277",
      "abstract": "Diese umfassende Systematik identifiziert fünf primäre Typen algorithmischer Diskriminierung: Bias durch algorithmische Agenten, diskriminierende Merkmalsselektion, Proxy-Diskriminierung, disparate Auswirkungen und gezielte Werbung. Die Analyse der US-Rechtslandschaft offenbart einen mehrstufigen Regulierungsansatz aus prinzipieller Regulierung, präventiven Kontrollen, konsequenter Haftung und Selbstregulierung. Zentral ist die Erkenntnis, dass unbeabsichtigte Diskriminierung durch scheinbar neu",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmische Diskriminierungssysteme), SOZIAL_OK (Bias_Ungleichheit + Fairness: systematische Analyse von Diskriminierungstypen, Regelungsansätze, strukturelle Ungleichheiten). Kein direkter Sozialarbeitsbezug, daher nicht als SA-Paper klas"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "T83KNEQZ",
      "title": "Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness",
      "author_year": "Ovalle et al. (2023)",
      "authors": "Ovalle, A.; Subramonian, A.; Gautam, V.; Gee, G.; Chang, K.-W.",
      "year": 2023,
      "doi": "10.1145/3600211.3604705",
      "url": "https://doi.org/10.1145/3600211.3604705",
      "abstract": "Diese kritische Literaturanalyse von 30 Arbeiten zur intersektionalen KI-Fairness deckt eine fundamentale Diskrepanz zwischen der Konzeptualisierung und Operationalisierung von Intersektionalität auf. Die Autoren zeigen, dass Forscher Intersektionalität überwiegend auf die Optimierung von Fairness-Metriken über demografische Untergruppen reduzieren, dabei aber die strukturellen Machtverhältnisse und den historischen Kontext vernachlässigen. Die Studie demonstriert, dass tiefgreifendes Engagement",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja: algorithmische Fairness-Systeme) und SOZIAL_OK (Bias_Ungleichheit=Ja, Diversitaet=Ja, Feministisch=Ja [explizite Verwendung von Intersektionalität nach Crenshaw, Analyse von Machtstrukturen], Fairness=Ja). Kritische intersektionale Analyse"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "TRAN2GJU",
      "title": "How can feminism inform AI governance in practice?",
      "author_year": "Ricaurte (2024)",
      "authors": "Ricaurte, P.",
      "year": 2024,
      "doi": "",
      "url": "https://www.unesco.org/en/articles/how-can-feminism-inform-ai-governance-practice",
      "abstract": "Diese UNESCO-Publikation definiert feministische KI-Governance als einen aufkommenden Bereich von Politik, Forschung und Entwicklung, der darauf abzielt, KI-Systeme gerecht, gleichberechtigt und inklusiv zu gestalten. Feministische KI-Governance zielt darauf ab, Machtungleichgewichte im KI-Ökosystem zu adressieren und strukturelle Ungleichheiten, koloniale Vermächtnisse und multidimensionale Schäden zu berücksichtigen, die überproportional Gemeinschaften der globalen Mehrheit betreffen. Der Ansa",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper adressiert KI-Governance als algorithmisches System (KI_Sonstige) mit explizitem Fokus auf feministische Theorie und Methodik (Feministisch). Substantielle Behandlung von Machtungleichgewichten, Bias, strukturellen Ungleichheiten (Bias_Ungleichheit), Gender-Perspektive und Diversität/Inklusion"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "Q4LK53XW",
      "title": "Are \"Intersectionally Fair\" AI Algorithms Really Fair to Women of Color? A Philosophical Analysis",
      "author_year": "Kong (2022)",
      "authors": "Kong, Y.",
      "year": 2022,
      "doi": "10.1145/3531146.3533074",
      "url": "https://doi.org/10.1145/3531146.3533074",
      "abstract": "Diese philosophische Analyse identifiziert drei fundamentale Probleme mit der dominanten Interpretation intersektionaler Fairness in der KI: Die Fokussierung auf Identitätskategorien statt Unterdrückungsstrukturen, ein Dilemma zwischen statistischer Parität und substanzieller Gerechtigkeit, sowie die Vernachlässigung historischer Kontexte. Kong argumentiert, dass echte intersektionale Fairness über statistische Metriken hinausgehen und strukturelle Unterdrückungssysteme (Rassismus, Sexismus) dir",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper kombiniert Technik (KI_Sonstige: algorithmische Fairness-Systeme) mit starkem sozialen Bezug. Erfüllt mehrere SOZIAL-Kategorien: Bias_Ungleichheit (strukturelle Unterdrückungssysteme), Gender (Fokus auf Women of Color), Diversitaet (intersektionale Analyse), Feministisch (kritisch-theoretische"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "XIYX5HJS",
      "title": "Towards substantive equality in artificial intelligence: Transformative AI policy for gender equality and diversity",
      "author_year": "Ricaurte Quijano & Prud’homme (2024)",
      "authors": "Ricaurte Quijano, P.; Prud’homme, B.",
      "year": 2024,
      "doi": "",
      "url": "https://wp.oecd.ai/app/uploads/2025/05/towards-substantive-equality-in-artificial-intelligence_Transformative-AI-policy-for-gender-equality-and-diversity.pdf",
      "abstract": "This extensive report – developed through the Global Partnership on AI (GPAI) with contributors from academia and policy – sets out a vision for “substantive equality” in AI as opposed to mere formal equality. It recognizes that AI systems can replicate and even amplify societal power imbalances (“algorithmic discrimination”), thus requiring proactive governance to ensure historically marginalized groups are not left behind in the AI era. The report argues that purely technical bias mitigation i",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK: AI_Literacies (Policy-Rahmen für AI-Governance), KI_Sonstige (algorithmische Diskriminierung). SOZIAL: Bias_Ungleichheit (Fokus auf algorithmische Diskriminierung), Gender (explizite intersektionale Genderanalyse), Diversitaet (inklusive Repräsentation),"
      },
      "human": {
        "decision": "Unclear",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "3QUWCYVW",
      "title": "Assessing trustworthy AI: Technical and legal perspectives of fairness in AI",
      "author_year": "Kattnig et al. (2024)",
      "authors": "Kattnig, M.; Angerschmid, A.; Reichel, T.; Kern, R.",
      "year": 2024,
      "doi": "10.1016/j.clsr.2024.106053",
      "url": "https://graz.elsevierpure.com/ws/portalfiles/portal/89954869/1-s2.0-S0267364924001195-main.pdf",
      "abstract": "Kattnig et al. examine the disconnect between existing AI fairness techniques and the requirements of non-discrimination law, highlighting that many algorithmic bias mitigation methods do not meet legal standards for equality. Focusing on the EU context (with particular attention to the forthcoming AI Act and established non-discrimination directives), the authors review state-of-the-art bias mitigation approaches – from pre-processing data fixes to in-processing algorithms – and evaluate them a",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt algorithmische Systeme (KI_Sonstige: Ja) mit substantiellem Fokus auf Bias-Mitigation und Diskriminierung (Bias_Ungleichheit: Ja) sowie Fairness-Konzepte (Fairness: Ja). Kritische Analyse von Fairness-Definitionen und Bias-Mitigation-Methoden erfüllt beide Bedingungen (TECHNIK + SOZI"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "5F7D9PEB",
      "title": "Policy advice and best practices on bias and fairness in AI",
      "author_year": "Alvarez et al. (2024)",
      "authors": "Alvarez, J. M.; Bringas Colmenarejo, A.; Elobaid, A.; Fabbrizzi, S.; Fahimi, M.; Ferrara, A.; Ruggieri, S.",
      "year": 2024,
      "doi": "10.1007/s10676-024-09746-w",
      "url": "https://link.springer.com/article/10.1007/s10676-024-09746-w",
      "abstract": "This open-access paper provides a comprehensive overview of fairness in AI, bridging technical bias mitigation methods with legal and policy considerations. Alvarez et al. survey the state-of-the-art in fair AI techniques and review major policy initiatives and standards on algorithmic bias. A key contribution is the NoBIAS architecture introduced in the paper, which comprises a “Legal Layer” (focusing on EU non-discrimination law and human rights requirements) and a “Bias Management Layer” (cov",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt algorithmische Fairness und Bias-Mitigation mit Policy-Fokus (KI_Sonstige: Ja, Fairness: Ja). Es erfüllt die TECHNIK-Bedingung. Allerdings fehlt der explizite Bezug zu Sozialer Arbeit oder marginalisierten Gruppen im Kontext sozialer Dienste. Intersektionale Diskriminierung wird "
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "2SLISKSW",
      "title": "Avoiding catastrophe through intersectionality in global AI governance",
      "author_year": "McCrory (2024)",
      "authors": "McCrory, L.",
      "year": 2024,
      "doi": "",
      "url": "https://www.cigionline.org/documents/3375/DPH-paper-Laine_McCrory.pdf",
      "abstract": "In this working paper, McCrory argues that prevailing global AI governance efforts (especially those focused on “AI safety” and existential risks) lack an adequate intersectional, feminist perspective, which is needed to avert not just hypothetical future catastrophes but ongoing injustices. The paper analyzes seven prominent international AI policy initiatives (e.g. global AI accords and principles) against criteria derived from feminist theory, such as intersectionality, context, and power dyn",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmische Governance und AI-Systeme sind substantiell adressiert) und SOZIAL_OK (Bias_Ungleichheit, Gender, Diversität und explizit Feministisch durch Intersektionalitätsanalyse nach Crenshaw-Perspektive, Fairness durch Kritik unglei"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "8U23BX2J",
      "title": "Intersectionality in artificial intelligence: Framing concerns and recommendations for action",
      "author_year": "Ulnicane (2024)",
      "authors": "Ulnicane, I.",
      "year": 2024,
      "doi": "10.17645/si.v12.7543",
      "url": "https://www.cogitatiopress.com/socialinclusion/article/download/7543/3744",
      "abstract": "Ulnicane’s article investigates how intersectionality – the overlapping of gender, race, class and other social inequalities – is being addressed in debates about AI bias. Through an analysis of four high-profile reports on AI and discrimination, the study finds that AI is often incorrectly portrayed as neutral, whereas in reality it amplifies existing societal biases, leading to discriminatory outcomes. A core issue identified is the tech sector’s “diversity crisis”: a homogenous AI workforce (",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: algorithmische Systeme und Bias), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch via intersektionale Analyse nach Crenshaw-Rahmen, Fairness). Intersektionalität und kritische Machtanalyse sind explizit feministisch. Fokus"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "7L93JBLR",
      "title": "Gender in a stereo-(gender)typical EU AI law: A feminist reading of the AI Act",
      "author_year": "Karagianni (2025)",
      "authors": "Karagianni, A.",
      "year": 2025,
      "doi": "10.1017/cfl.2025.12",
      "url": "https://www.cambridge.org/core/journals/cambridge-forum-on-ai-law-and-governance/article/gender-in-astereogendertypical-eu-ai-law-a-feminist-reading-of-the-ai-act/E9DEFC1E114CBE577D737EC616610921",
      "abstract": "This peer-reviewed article offers a feminist critique of the European Union’s AI Act, arguing that the law’s current bias mitigation provisions only superficially address gendered risks and fail to confront deeply embedded structural biases. Using frameworks like hermeneutical injustice and feminist legal theory, Karagianni shows that AI systems can perpetuate historical power imbalances and systemic discrimination against women and marginalized groups. The AI Act’s pre-market and post-market me",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt TECHNIK (KI_Sonstige: algorithmische Systeme und deren Regulierung) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness). Es nutzt explizit feministische Theorie (hermeneutical injustice, feminist legal theory, Intersektionalität) zur Kritik von KI-Governance"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "NUVZI357",
      "title": "Feministische Netzpolitik und Künstliche Intelligenz in der politischen Bildung [Feminist network politics and artificial intelligence in political education]",
      "author_year": "Mosene (2023)",
      "authors": "Mosene, K.",
      "year": 2023,
      "doi": "",
      "url": "https://www.politische-medienkompetenz.de/debatte/ki-und-intersektionalitaet/",
      "abstract": "Examines intersections of feminist network politics and AI within political education, emphasizing intersectional feminist perspectives on digital technologies. Argues feminist network politics involves supporting AI researchers and activists working to eliminate bias in development and outcomes. Discusses how traditional gender roles are reinforced through AI systems and advocates for political education helping users understand how technologies function, emerged, which societal ideas they refl",
      "item_type": "webpage",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige) + SOZIAL_OK (Bias_Ungleichheit, Gender, Feministisch, Fairness). Behandelt substantiell feministische Perspektive auf KI (intersektional), Gender-Bias in KI-Systemen, AI-Literacy in politischer Bildung. Kein Sozialarb.-Bezug, d"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "2565PPJW",
      "title": "Feminist reflections for the development of artificial intelligence",
      "author_year": "Guerra et al. (2023)",
      "authors": "Guerra, J.; Venturini, J.; Castrillón, A.; Porras Sepúlveda, M.J.",
      "year": 2023,
      "doi": "",
      "url": "https://www.derechosdigitales.org/fair-2023-en/",
      "abstract": "Comprehensive synthesis of Latin American women's conversations developing AI under feminist frameworks establishes methodological commitments for co-design with communities, gender perspective integration in data science projects, and strategies for women crowd workers. Key feminist AI principles include building diverse intersectional teams, establishing community collaboration agreements, choosing technology based on context rather than consumption, and protecting autonomy through strong anon",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt substantiell feministische Perspektiven auf KI-Entwicklung (Feministisch: Ja), thematisiert Gender-Perspektiven und Geschlechtergerechtigkeit (Gender: Ja), adressiert Diversität und Inklusion durch intersektionale Teams und Community-Partizipation (Diversitaet: Ja), diskutiert Bias u"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "DA6T4Z5B",
      "title": "Feminist Data Set",
      "author_year": "Sinders (2017)",
      "authors": "Sinders, C.",
      "year": 2017,
      "doi": "",
      "url": "https://carolinesinders.com/feminist-data-set/",
      "abstract": "Multi-year art-research project directly addresses critical prompting practices by interrogating every AI development step—data collection, labeling, training, algorithm selection, and chatbot design—through feminist and intersectional lenses. Conducts public workshops to collaboratively build feminist datasets. Represents concrete critical prompting practice through community-based data creation as protest against biased AI systems, demonstrating practical approaches to feminist prompting by cr",
      "item_type": "webpage",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK-Seite mit AI_Literacies (kritische Kompetenzentwicklung), Generative_KI (Chatbot-Design), Prompting (explizit critical prompting practices), und KI_Sonstige (Datensammlung, Training, Algorithmenauswahl). SOZIAL-Seite mit starkem Feministisch-Fokus (expliz"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "UFJ7ERFF",
      "title": "Data Feminism for AI",
      "author_year": "D'Ignazio & Klein (2024)",
      "authors": "D'Ignazio, C.; Klein, L.",
      "year": 2024,
      "doi": "",
      "url": "https://arxiv.org/html/2405.01286v1",
      "abstract": "Comprehensive feminist framework directly critiques individualized approaches to AI ethics, challenging the \"liberal framework of making algorithms unbiased and inclusive\" in favor of structural \"remediation\" addressing \"systemic and structural dimensions of discrimination.\" Examines how AI research is captured by \"racial, gendered capitalism\" and proposes nine principles focusing on structural power analysis including examining power, challenging power, and making labor visible. Explicitly posi",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja: algorithmische Systeme und KI-Diskriminierung), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet und Feministisch alle Ja). Data Feminism ist ein kanonales Werk der feministischen Technikforschung (D'Ignazio explizit), das strukturell"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "BJZYGNEE",
      "title": "Shaping feminist artificial intelligence",
      "author_year": "Toupin (2024)",
      "authors": "Toupin, S.",
      "year": 2024,
      "doi": "10.1177/14614448221150776",
      "url": "https://journals.sagepub.com/doi/full/10.1177/14614448221150776",
      "abstract": "Comprehensive typology establishes six ways feminism and AI intersect: model, design, policy, culture, discourse, and science. The \"design\" category most directly addresses prompting practices, exploring how feminist approaches emphasize participatory design, community involvement, and challenging masculine-coded AI systems. Traces feminist AI from 1990s theoretical foundations through contemporary projects, establishing frameworks for feminist human-AI interaction informing critical prompting p",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies, Prompting) + SOZIAL (Feministisch, Gender, Bias_Ungleichheit, Diversitaet). Explizit feministische Theorie (Rahmenwerk mit 6 Kategorien), Fokus auf Design und Prompting-Praktiken sowie Geschlechterperspektive auf KI-Systeme. Typen-orientierter"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "N7J2ZRFP",
      "title": "Algorithmic Governance and the International Politics of Big Tech",
      "author_year": "Srivastava (2024)",
      "authors": "Srivastava, S.",
      "year": 2024,
      "doi": "",
      "url": "https://www.cambridge.org/core/journals/perspectives-on-politics/article/algorithmic-governance-and-the-international-politics-of-big-tech/3C04908735A5F2EE8A70AFED647741FB",
      "abstract": "Structural analysis examines how Big Tech corporations exercise \"entrepreneurial private authority\" through algorithmic governance systems challenging state sovereignty. Moves beyond individual-focused approaches to analyze how technology firms exercise instrumental, structural, and discursive power globally. Critiques approaches focusing on individual user agency or technical fixes, examining instead how \"Big Tech's algorithmic governance incentivizes 'information pollution'\" and creates system",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt algorithmische Systeme (KI_Sonstige: Ja) und Machtungleichgewichte durch Big Tech (Bias_Ungleichheit: Ja). Allerdings fehlt ein direkter Bezug zu Sozialer Arbeit oder deren Praxis/Zielgruppen. Die Analyse fokussiert auf politische Wirtschaft und staatliche Souveränität, nicht auf"
      },
      "human": {
        "decision": "Unclear",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "HJ7BHX8J",
      "title": "Engineers on responsibility: feminist approaches to who's responsible for ethical AI",
      "author_year": "Browne et al. (2024)",
      "authors": "Browne, J.; Drage, E.; McInerney, K.",
      "year": 2024,
      "doi": "10.1007/s10676-023-09739-1",
      "url": "",
      "abstract": "Through interviews with AI practitioners interpreted via feminist political thought, reimagines responsibility in AI development beyond individualized approaches. Critiques current AI responsibility frameworks focused on individual competency and technical solutions, proposing instead \"responsibility as the product of work cultures that enable tech workers to be responsive and answerable for their products.\" Moves beyond \"individual competency approaches\" toward understanding responsibility as e",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 1,
          "Fairness": 0
        },
        "reasoning": "Paper kombiniert feministische Theorie (Browne et al.) mit KI-Verantwortungsdiskurs. KI_Sonstige=Ja (KI-Entwicklung, Verantwortungsrahmen), Feministisch=Ja (explizit feministische politische Theorie). Jedoch: Soziale_Arbeit=Nein (kein Bezug zu SW-Praxis/Zielgruppen), Bias_Ungleichheit=Nein (nicht su"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "VFD9ENG6",
      "title": "A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges",
      "author_year": "Gohar & Cheng (2023)",
      "authors": "Gohar, U.; Cheng, L.",
      "year": 2023,
      "doi": "",
      "url": "https://arxiv.org/abs/2305.06969",
      "abstract": "Comprehensive survey provides the first taxonomy of intersectional fairness notions in machine learning, explicitly grounded in Crenshaw's legal intersectionality framework. Demonstrates how multiple sensitive attributes interact to create distinct algorithmic bias forms that traditional single-axis approaches cannot capture. Shows how intersectional identities amplify biases not present in constituent groups, revealing co-constitutive discrimination mechanisms. Systematically reviews mitigation",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt substantiell algorithmische Fairness und Bias in ML-Systemen (KI_Sonstige, Fairness). Es greift EXPLIZIT auf Crenshaw's intersektionale Theorie zurück und analysiert Diskriminierungsmechanismen aus intersektionaler Perspektive (Feministisch, Bias_Ungleichheit, Gender, Diversitaet"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Fairness"
        ]
      }
    },
    {
      "id": "THGC3PA2",
      "title": "Feministische KI – Künstliche Intelligenz für alle? [Feminist AI – Artificial intelligence for everyone?]",
      "author_year": "Kubes (2024)",
      "authors": "Kubes, T.",
      "year": 2024,
      "doi": "",
      "url": "https://blogs.fu-berlin.de/toolbox/2024/04/08/feministische-ki-kuenstliche-intelligenz-fuer-alle/",
      "abstract": "Innovative interdisciplinary seminar teaches students to critically analyze everyday AI applications from sociotechnical feminist perspectives across four domains: love, robots, work, and creativity. Students analyze AI within androcentric, Eurocentric, anthropocentric, and capitalist-patriarchal structures. Curriculum combines theoretical foundations with practical application through \"queerbot\" design workshops that reimagine AI beyond normative dichotomies, demonstrating concrete pedagogical ",
      "item_type": "webpage",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt substantiell AI Literacy durch feministische Perspektive (Kubes, Gender-kritisch). Analysiert KI-Anwendungen im Kontext Bias (androzentrisch, eurozentrisch, kapitalistisch-patriarchal), Geschlechterperspektive und Inklusion. Praktischer Bezug durch Curriculum und Workshops. Beide Bed"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies"
        ]
      }
    },
    {
      "id": "MT54YHER",
      "title": "Thinking like a scientist: Can interactive simulations foster critical AI literacy?",
      "author_year": "Zhao et al. (2025)",
      "authors": "Zhao, Y.; Michal, A.; Thain, N.; Subramonyam, H.",
      "year": 2025,
      "doi": "10.1007/978-3-031-98417-4_5",
      "url": "",
      "abstract": "Empirical study with 605 participants demonstrates that interactive simulations enhance critical AI literacy by engaging learners in scientific thinking processes including hypothesis testing and direct observation of AI behavior. Reveals that critical AI literacy requires understanding of fairness, dataset representativeness, and bias mechanisms in language models beyond technical knowledge. Establishes that effective AI literacy education must move beyond static instruction toward experiential",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt TECHNIK-Kriterium durch AI_Literacies (Fokus auf kritische KI-Kompetenzentwicklung), Generative_KI (Erwähnung von Language Models) und SOZIAL-Kriterium durch Bias_Ungleichheit (expliziter Fokus auf Bias-Mechanismen) und Fairness (Verständnis von Fairness als zentrales Element). Empiris"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "KI_Sonstige",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "T8R8RKX9",
      "title": "AI tools show biases in ranking job applicants' names according to perceived race and gender",
      "author_year": "Wilson & Caliskan (2024)",
      "authors": "Wilson, K.; Caliskan, A.",
      "year": 2024,
      "doi": "",
      "url": "https://ojs.aaai.org/index.php/AIES/article/view/31748",
      "abstract": "Large-scale empirical study using over 550 resumes and 3+ million comparisons reveals that intersectional patterns of bias in AI resume screening cannot be understood as additive combinations of single-axis discrimination. Discovered unique harm against Black men invisible when examining race or gender independently—Black male names were never preferred over white male names (0% selection rate). Demonstrates co-constitutive nature of multiple discrimination where intersection of Blackness and ma",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (KI_Sonstige: algorithmische Systeme für Recruitment/Resume-Screening) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet: explizite Analyse von Rassismus und Geschlechterdiskriminierung; Feministisch: intersektionale Analyse nach Crenshaw-Logik; Fairness: al"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Fairness"
        ]
      }
    },
    {
      "id": "X54V3JMF",
      "title": "Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action",
      "author_year": "Ulnicane (2024)",
      "authors": "Ulnicane, I.",
      "year": 2024,
      "doi": "10.17645/si.7543",
      "url": "",
      "abstract": "Applies Crenshaw's intersectionality theory to examine four high-profile AI policy reports, revealing how diversity crises in AI workforce create \"negative feedback loops\" where homogeneous development teams embed biases into systems. Demonstrates how multiple forms of discrimination co-constitute each other through voice assistants, robots, and hiring tools, showing that intersectional experiences cannot be reduced to single identity categories. Argues for moving beyond simple diversity initiat",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: analysiert AI-Systeme wie Voice Assistants, Robots, Hiring Tools). SOZIAL_OK (Bias_Ungleichheit: Diskriminierung durch homogene Dev-Teams; Gender + Diversitaet: mehrere Diskriminierungsformen; Feministisch: explizite Anwendung von Crenshaw's "
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "EN6GNKL3",
      "title": "AI literacy in teacher education: Empowering educators through critical co-discovery",
      "author_year": "Dilek et al. (2025)",
      "authors": "Dilek, M.; Baran, E.; Aleman, E.",
      "year": 2025,
      "doi": "10.1177/00224871251325083",
      "url": "",
      "abstract": "Implements critical co-discovery approaches within AI teacher education to move beyond technical automation toward critical pedagogical engagement. Through co-discovery activities, educators developed understanding of AI concepts, ethical considerations, and context-specific applications while co-constructing knowledge. Emphasizes that prolonged engagement with AI literacy integrated into teacher education programs enables educators to critically navigate AI systems and examine broader pedagogic",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt substantiell AI Literacy durch teacher education und co-discovery-Ansätze (TECHNIK erfüllt). Expliziter Fokus auf kritische Examination von AI's power dynamics und social justice implications erfüllt Bias_Ungleichheit-Kategorie (SOZIAL erfüllt). Beide Bedingungen der Inklusionskriter"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "QUU8QDPQ",
      "title": "Keynote Summary: The New Jim Code: Reimagining the Default Settings of Technology & Society",
      "author_year": "Benjamin (2023)",
      "authors": "Benjamin, R.",
      "year": 2023,
      "doi": "10.1145/3589139",
      "url": "https://dl.acm.org/doi/10.1145/3589139",
      "abstract": "In this summary of her foundational work, Ruha Benjamin introduces the concept of the \"New Jim Code,\" which describes how new technologies, including AI, can reproduce and even deepen existing racial hierarchies and discrimination under a veneer of neutrality and progress. She argues that discrimination becomes embedded in the very architecture of these systems. This framework is crucial for understanding how various forms of discrimination are co-constituted in AI, not as accidental bugs, but a",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt die SOZIAL-Bedingung (Bias_Ungleichheit, Diversitaet, Feministisch, Fairness alle Ja) und auch die TECHNIK-Bedingung (KI_Sonstige Ja). Jedoch ist dies eine Keynote-Zusammenfassung von fundamentaler Kritischer Techniktheorie/Philosophie ohne direkten Bezug zu Sozialer Arbeit als Pra"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Include_Agent_Exclude",
        "severity": "high",
        "affected_categories": [
          "AI_Literacies",
          "Soziale_Arbeit",
          "Gender",
          "Feministisch"
        ]
      }
    },
    {
      "id": "PQC9G5EU",
      "title": "Feminism Confronts AI: The Gender Relations of Digitalisation",
      "author_year": "Wajcman & Young (2023)",
      "authors": "Wajcman, J.; Young, E.",
      "year": 2023,
      "doi": "",
      "url": "https://academic.oup.com/book/55103/chapter/423909956",
      "abstract": "Wajcman and Young provide a feminist critique of AI, arguing that the technology is not neutral but deeply embedded in existing gendered power structures. They highlight the severe underrepresentation of women in AI development as a key source of bias, leading to the creation of systems that reflect and amplify a masculine worldview. The authors contend that simply adding more women to the field is insufficient. Instead, they call for a fundamental shift in the culture of technology production, ",
      "item_type": "booksection",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja: kritische Analyse von KI-Systemen und deren Entwicklung) UND SOZIAL_OK (Bias_Ungleichheit=Ja: Fokus auf Gender Bias in AI; Gender=Ja: expliziter Gender-Fokus; Diversitaet=Ja: Unterrepräsentation von Frauen; Feministisch=Ja: verwendet f"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "U75LP6SV",
      "title": "The Algorithmic Auditing Landscape: A Social Justice Approach",
      "author_year": "Raji & Buolamwini (2024)",
      "authors": "Raji, I. D.; Buolamwini, J.",
      "year": 2024,
      "doi": "10.1145/3630659.3630671",
      "url": "https://dl.acm.org/doi/10.1145/3630659.3630671",
      "abstract": "Raji and Buolamwini, pioneers of algorithmic auditing, argue for an approach rooted in social justice. They critique audits that focus solely on technical metrics, advocating instead for methods that center the lived experiences of marginalized communities. This involves a multi-stakeholder process, transparency, and a focus on real-world harms. Such an audit practice inherently makes the co-constitution of discrimination visible by investigating not just the algorithm's output, but the entire s",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt TECHNIK-Kriterium über KI_Sonstige (algorithmische Auditing-Systeme) und SOZIAL-Kriterium über multiple Kategorien: Bias_Ungleichheit (Fokus auf Diskriminierung und marginalisierte Communities), Diversitaet (Multi-Stakeholder-Prozess, Community-Involvement), Feministisch (intersektiona"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "JZ4P8V8S",
      "title": "Operationalizing positive-constructive pedagogy to artificial intelligence: the 3E model for scaffolding AI technology adoption",
      "author_year": "Thwaites et al. (2024)",
      "authors": "Thwaites, A.; Maas, C.; Zhang, Y.",
      "year": 2024,
      "doi": "10.3389/feduc.2024.1293235",
      "url": "https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2024.1293235/full",
      "abstract": "This article proposes the \"3E model\" (Expose, Explore, Exploit) as a pedagogical framework for developing critical AI literacy. The model aims to move students beyond passive use of AI to a more critical engagement. The \"Expose\" phase involves revealing the underlying mechanisms and biases of AI systems. \"Explore\" encourages students to test AI boundaries and critically question its outputs, a practice akin to critical prompting. \"Exploit\" focuses on using AI for creative and novel purposes. Whi",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Prompting",
          "Bias_Ungleichheit"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies Ja: Pedagogisches Framework für kritische KI-Kompetenzentwicklung; Prompting Ja: Expliziter Fokus auf 'critical prompting' in Explore-Phase). SOZIAL_OK (Bias_Ungleichheit Ja: Substantielle Behandlung von Bias-Erkennung und Sichtbarmachung vo"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "SJLJ2GHC",
      "title": "Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action",
      "author_year": "Ulnicane (2024)",
      "authors": "Ulnicane, I.",
      "year": 2024,
      "doi": "10.17645/si.7543",
      "url": "https://www.cogitatiopress.com/socialinclusion/article/view/7543",
      "abstract": "This article critically examines how intersectionality is understood and applied in European Union AI policy documents. Ulnicane argues that current policy approaches often reduce intersectionality to a mere \"multi-category\" perspective, focusing on adding more diversity variables without addressing the underlying structural power dynamics that create inequalities. The study reveals that while policies acknowledge bias, they fail to adequately address the systemic nature of discrimination co-con",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Kritische Analyse von KI-Policy), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch: Explizit intersektionale Analyse mit Fokus auf strukturelle Machtdynamiken; Fairness: Kritik an unzureichenden Fairness-Ansätzen). Die femi"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "3XMBE43Z",
      "title": "Algorithmic Governance: Gender Bias in AI-Generated Policymaking?",
      "author_year": "Voutyrakou & Skordoulis (2025)",
      "authors": "Voutyrakou, D. A.; Skordoulis, C.",
      "year": 2025,
      "doi": "10.1007/s44230-025-00109-2",
      "url": "https://doi.org/10.1007/s44230-025-00109-2",
      "abstract": "Examines whether gender-specific needs are reflected in AI-generated policies, demonstrating through GPT-4 and Copilot experiments that AI tends to overlook female-specific needs unless explicitly prompted. Highlights androcentric biases, advocating intersectionally-informed prompting to surface hidden biases but recognizing the limits of individual prompt-based solutions in addressing structural AI biases.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (Generative_KI + Prompting), SOZIAL_OK (Bias_Ungleichheit + Gender + Feministisch + Fairness). Empirische Analyse von Gender-Bias in GPT-4/Copilot-generierten Policies mit intersektionalem Rahmen. Fokus auf Androzentrizität und strukturelle Biases entspric"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "SHDNTZJZ",
      "title": "Shaping feminist artificial intelligence",
      "author_year": "Toupin (2024)",
      "authors": "Toupin, S.",
      "year": 2024,
      "doi": "10.1177/14614448221150776",
      "url": "https://doi.org/10.1177/14614448221150776",
      "abstract": "Toupin reviews historical and conceptual developments of feminist AI, identifying six key forms—model, design, policy, culture, discourse, and science. She demonstrates how feminist initiatives historically and currently surface hidden biases and power dynamics in AI. Advocates for integrating feminist perspectives directly into AI discourse and development.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 1,
          "Fairness": 0
        },
        "reasoning": "Das Paper behandelt substantiell feministische Perspektiven auf KI (KI_Sonstige: Ja), thematisiert Gender-Bias und Machtdynamiken in AI-Systemen (Bias_Ungleichheit: Ja, Gender: Ja) und nutzt explizit feministische Theorie als Analyserahmen (Feministisch: Ja). Erfüllt beide Bedingungen: TECHNIK (KI_S"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "9YCFMPVT",
      "title": "Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action",
      "author_year": "Ulnicane (2024)",
      "authors": "Ulnicane, I.",
      "year": 2024,
      "doi": "10.17645/si.v12i2.7543",
      "url": "https://doi.org/10.17645/si.v12i2.7543",
      "abstract": "Ulnicane analyzes four high-profile reports on intersectionality in AI, identifying a “vicious cycle” of bias perpetuated by a homogeneous AI workforce. She argues previous diversity initiatives have largely failed and advocates a holistic approach to altering power structures and cultures within AI development. Intersectional perspectives must move from the periphery to challenge core AI agendas.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        },
        "reasoning": "Paper analysiert intersektionale Perspektiven in KI-Entwicklung und befasst sich substantiell mit Bias, Geschlechterperspektiven und struktureller Unterrepräsentation. Intersektionalität (Crenshaw-Framework) ist eine feministische Methode. Erfüllt TECHNIK (KI_Sonstige) und SOZIAL (Bias_Ungleichheit,"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "C485NKYA",
      "title": "AI Countergovernance: Lessons Learned from Canada and Paris",
      "author_year": "Attard-Frost et al. (2025)",
      "authors": "Attard-Frost, B.; Brandusescu, A.; Widder, D. G.; Tessono, C.",
      "year": 2025,
      "doi": "",
      "url": "https://techpolicy.press/ai-countergovernance-lessons-learned-from-canada-and-paris/",
      "abstract": "Argues against superficial \"AI literacy\" programs, promoting instead grassroots critical AI literacies that engage directly with structural inequalities related to race, gender, and labor. Stresses collectiv",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.82,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 1,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt kritische AI Literacies (TECHNIK) und thematisiert strukturelle Ungleichheiten bezüglich Rasse, Geschlecht und Arbeit (SOZIAL). Expliziter Fokus auf Bias/Ungleichheit und Gender. Kritische Perspektive auf Strukturen deutet auf feministische Ansätze hin. Beide Bedingungen erfüllt → In"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies"
        ]
      }
    },
    {
      "id": "XZ6Z8A9C",
      "title": "Trustworthy AI and the Logics of Intersectional Resistance",
      "author_year": "Knowles et al. (2023)",
      "authors": "Knowles, B.; Fledderjohann, J.; Richards, J. T.; Varshney, K. R.",
      "year": 2023,
      "doi": "10.1145/3593013.3593986",
      "url": "https://doi.org/10.1145/3593013.3593986",
      "abstract": "Critically examines mainstream \"Trustworthy AI\" frameworks from an intersectional feminist perspective, arguing that traditional AI ethics often privilege dominant groups and fail marginalized communities. Suggests reframing trustworthy AI principles to incorporate stewardship, care, humility, and empowerment, addressing intersectional injustices through power-sharing and structural change.",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Ja - Kritik an AI-Ethics-Frameworks als algorithmische Systeme) und SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness alle Ja). Explizit intersektional-feministischer Ansatz mit Fokus auf marginalisierte Gruppen und st"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "GJF776AY",
      "title": "Investigating AI systems: examining data and algorithmic bias through hermeneutic reverse engineering",
      "author_year": "Shukla (2025)",
      "authors": "Shukla, N.",
      "year": 2025,
      "doi": "",
      "url": "https://www.frontiersin.org/journals/communication/articles/10.3389/fcomm.2025.1380252/full",
      "abstract": "This work presents hermeneutic reverse engineering as a framework for investigating bias in AI systems. The approach considers AI systems as boundary objects and analyzes cultural meanings and assumptions embedded in techno-cultural objects. The study proposes three research perspectives: (1) comparative exploration of algorithmic bias, (2) investigation of impacts on various social groups, and (3) participatory approaches to include users in AI design.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt Bias in AI-Systemen (KI_Sonstige: Ja) mittels hermeneutischer Reverse Engineering. Fokus auf algorithmischen Bias (Bias_Ungleichheit: Ja), Auswirkungen auf verschiedene soziale Gruppen (Diversitaet: Ja) und Fairness-Aspekte durch participatory design (Fairness: Ja). Beide Bedingungen"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "PC75954S",
      "title": "An empirical study of structural social and ethical challenges in AI",
      "author_year": "Ghosal (2024)",
      "authors": "Ghosal, A.",
      "year": 2024,
      "doi": "",
      "url": "https://link.springer.com/article/10.1007/s00146-025-02207-y",
      "abstract": "This empirical study examines how professionals (N=32) in AI development perceive structural ethical challenges such as injustices and inequalities. The research identifies three main themes: (1) barriers to responsibility in a changing ecosystem, (2) the need for holistic consideration of AI products and their harms, and (3) structural obstacles that prevent engineers from taking personal responsibility.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja, da es sich um Studie zu AI-Development befasst) und SOZIAL_OK (Bias_Ungleichheit=Ja: 'injustices and inequalities'; Fairness=Ja: Responsibility und strukturelle Herausforderungen). Kein Bezug zu Sozialer Arbeit oder feministischer Theorie "
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Generative_KI",
          "Prompting",
          "Soziale_Arbeit",
          "Gender",
          "Fairness"
        ]
      }
    },
    {
      "id": "ZAU6P4BK",
      "title": "Shaping feminist artificial intelligence",
      "author_year": "Toupin (2024)",
      "authors": "Toupin, S.",
      "year": 2024,
      "doi": "",
      "url": "",
      "abstract": "This study examines the historical and contemporary shaping of feminist AI (FAI) through a typology of six approaches: FAI as model, design, politics, culture, discourse, and science. Toupin analyzes how feminist perspectives are implemented in various areas of AI development and identifies both potentials and limitations of feminist approaches. The work shows that FAI is not only a technological concept but also a movement that aims to transform power relations in AI development and promote soc",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt substantiell feministische Theorie und Perspektiven im KI-Kontext (KI_Sonstige: Ja). Es adressiert explizit feministische Ansätze, Machtstrukturen und soziale Gerechtigkeit (Feministisch: Ja). Gender und Diversität sind zentral (Gender: Ja, Diversitaet: Ja). Bias/Ungleichheit wir"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Generative_KI",
          "Prompting",
          "Soziale_Arbeit",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "Y8J3HI9J",
      "title": "Artificial Intelligence in a Structurally Unjust Society",
      "author_year": "Lin & Chen (2022)",
      "authors": "Lin, T.-A.; Chen, P.-H. C.",
      "year": 2022,
      "doi": "",
      "url": "https://ojs.lib.uwo.ca/index.php/fpq/article/download/14191/12139/38443",
      "abstract": "This study argues that AI bias represents a form of structural injustice that arises when AI systems interact with other social factors to exacerbate existing social inequalities. Using the example of AI in healthcare, the authors show that the goal of AI fairness is to strive for a more just social structure through appropriate development and use of AI systems. They argue that all actors involved in the unjust social structure bear shared responsibility to join collective actions for structura",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja, algorithmische Systeme im Gesundheitskontext), SOZIAL_OK (Bias_Ungleichheit=Ja zu struktureller Ungerechtigkeit, Fairness=Ja zu AI Fairness). Kein direkter Soziale_Arbeit-Bezug, daher keine entsprechende Kategorie."
      },
      "human": {
        "decision": "Unclear",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "VVEEL68I",
      "title": "Training in Co-Creation as a Methodological Approach to Improve AI Fairness",
      "author_year": "Slesinger et al. (2024)",
      "authors": "Slesinger, I.; Yalaz, E.; Rizouli, S.; Komninos, E.; Papadopoulos, S.; Gibin, M.",
      "year": 2024,
      "doi": "",
      "url": "https://cris.unibo.it/retrieve/707c849f-3aeb-4ca3-8d1f-2817960064bd/societies-14-00259%20(1).pdf",
      "abstract": "This study examines the integration of training components in co-creation processes with vulnerable and marginalized stakeholder groups as part of developing AI bias detection and mitigation tools. The research shows that training on AI definitions, terminology, and socio-technical impacts is necessary to enable non-technical stakeholders to clearly articulate their insights on AI fairness. The authors emphasize the importance of critical reflection on appropriate use of training in co-creation ",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies: Training zu KI-Konzepten; KI_Sonstige: AI Bias Detection/Mitigation Tools; Fairness: Algorithmen-Fairness) + SOZIAL (Bias_Ungleichheit: Fokus auf Bias Detection; Diversitaet: vulnerable/marginalized stakeholder groups, inklusive AI-Entwicklung"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "HLBXNWAZ",
      "title": "Intersectional analysis of visual generative AI: the case of stable diffusion",
      "author_year": "Sharma et al. (2024)",
      "authors": "Sharma, S.; Ovalle, A.; Subramonian, A.",
      "year": 2024,
      "doi": "",
      "url": "https://link.springer.com/article/10.1007/s00146-025-02498-1",
      "abstract": "This study conducts a critical intersectional analysis of Stable Diffusion, a widely used visual generative AI tool. The examination of 180 generated images shows how the system perpetuates prevailing power systems such as sexism, racism, heteronormativity, and ableism, assuming white, physically healthy, masculine-presenting individuals as the standard. The study identifies three levels of analysis: (1) the aesthetics of AI images (micro-level), (2) institutional contexts (meso-level), and (3) ",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Bedingungen: (1) TECHNIK: Generative_KI=Ja (Stable Diffusion als visuelles generatives KI-System). (2) SOZIAL: Bias_Ungleichheit=Ja (Analyse von Sexismus, Rassismus, Heteronormativität, Ableismus), Gender=Ja (expliziter Fokus auf Geschlechterstereotype/masculine-presenting standa"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "Y6SAPNT2",
      "title": "Fairness Beyond the Algorithmic Frame: Actionable Recommendations for an Intersectional Approach",
      "author_year": "Vethman et al. (2025)",
      "authors": "Vethman, S.; Smit, Q. T. S.; van Liebergen, N. M.; Veenman, C. J.",
      "year": 2025,
      "doi": "",
      "url": "https://facctconference.org/static/docs/facct2025-206archivalpdfs/facct2025-final2348-acmpaginated.pdf",
      "abstract": "This study develops actionable recommendations for AI experts to implement intersectional approaches beyond purely technical solutions. The authors criticize the reduction of intersectionality to algorithmic bias measurements between subgroups and develop a framework that emphasizes interdisciplinary teams, community participation, and analysis of power structures in social context. Their approach includes six core recommendations: responsible AI expert role, multidisciplinary team building, ref",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, KI_Sonstige, Fairness behandeln KI-Systeme und Kompetenzentwicklung). SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Feministisch durch intersektionale Perspektive, expliziter Fokus auf Machtstrukturen und marginalisierte Gruppen). Intersektion"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Bias_Ungleichheit",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "XXCDL3A3",
      "title": "Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness",
      "author_year": "Ovalle et al. (2023)",
      "authors": "Ovalle, A.; Subramonian, A.; Gautam, V.; Gee, G.; Chang, K.-W.",
      "year": 2023,
      "doi": "",
      "url": "https://www.lsv.uni-saarland.de/wp-content/uploads/2023/12/Ovalle-et-al.-2023-Factoring-the-Matrix-of-Domination-A-Critical-Rev.pdf",
      "abstract": "This critical review examines how intersectionality is discussed in 30 works of AI fairness literature. The study shows that researchers predominantly reduce intersectionality to optimizing fairness metrics across demographic subgroups while neglecting social context and power structures. The authors develop a framework for re-conceptualizing intersectionality in AI fairness based on relationality, social power, and structural analysis. They argue that genuine intersectional approaches must cons",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt substantiell AI Fairness und Bias/Ungleichheit im KI-Kontext (KI_Sonstige erfüllt Technik-Bedingung). Bias_Ungleichheit, Diversitaet und Fairness sind zentrale Themen. Explizite feministische Perspektive durch Bezug zu Crenshaw's Intersectionality-Framework und kritische Analyse "
      },
      "human": {
        "decision": "Unclear",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "QXDK8Z6I",
      "title": "Artificial Intelligence and Structural Injustice: Foundations for Equity, Values, and Responsibility",
      "author_year": "Himmelreich & Lim (2022)",
      "authors": "Himmelreich, J.; Lim, D.",
      "year": 2022,
      "doi": "",
      "url": "https://arxiv.org/abs/2205.02389",
      "abstract": "This work develops a structural injustice approach to AI governance based on Iris Marion Young's theory of structural injustice. The authors argue that structural injustice is a powerful conceptual tool that enables researchers and practitioners to identify, articulate, and potentially even anticipate AI bias. The approach includes both an analytical component (structural explanations) and an evaluative component (justice theory) and provides methodological and normative foundations for diversit",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch KI_Sonstige (algorithmische Systeme und AI Bias); SOZIAL_OK durch Bias_Ungleichheit (strukturelle Gerechtigkeit), Diversitaet (Diversity/Equity/Inclusion) und Fairness (Frameworks für faire KI-Governance). Theoretischer Ansatz basierend auf Young's S"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "LT3D3ZQ2",
      "title": "Towards a Feminist Metaethics of AI",
      "author_year": "Siapka (2023)",
      "authors": "Siapka, A.",
      "year": 2023,
      "doi": "",
      "url": "https://arxiv.org/abs/2311.14700",
      "abstract": "This work develops a research agenda for a feminist metaethics of AI. Unlike traditional metaethics that reflects on moral judgments non-normatively, feminist metaethics expands its scope to ask not only what ethics is, but also how our approach to it should be. The author argues that a feminist metaethics of AI should investigate four areas: (1) continuity between theory and action in AI ethics, (2) real-world impacts of AI ethics, (3) the role and profile of those involved in AI ethics, and (4",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 1,
          "Fairness": 0
        },
        "reasoning": "Das Paper entwickelt einen explizit feministischen theoretischen Rahmen (feminist metaethics) für KI-Ethik und untersucht Machtstrukturen, Kontexte und die Rolle von Akteur:innen. KI_Sonstige=Ja (KI-Ethik als Teilbereich), Feministisch=Ja (explizite feministische Metaethik-Perspektive), Bias_Ungleic"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "D2EGAVKZ",
      "title": "Data Feminism for AI",
      "author_year": "Klein & D'Ignazio (2024)",
      "authors": "Klein, L.; D'Ignazio, C.",
      "year": 2024,
      "doi": "",
      "url": "https://facctconference.org/static/papers24/facct24-7.pdf",
      "abstract": "This work extends the seven principles of Data Feminism to the AI context and introduces two additional principles on environmental impacts and consent. The authors argue that feminist perspectives are essential for understanding and combating the unequal, undemocratic, extractive, and exclusionary forces in AI research. Their intersectional feminist principles aim to: (1) identify unequal power relations in AI systems, (2) mitigate predictable harms proactively, and (3) inspire creative, collec",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: Ja - behandelt AI-Systeme im Allgemeinen), SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness alle Ja). Explizit feministische Arbeit (D'Ignazio ist Autorin von Data Feminism), intersektionale Perspektive auf KI-Sys"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": []
      }
    },
    {
      "id": "NHIZN4QJ",
      "title": "What is Feminist AI?",
      "author_year": "Wudel & Ehrenberg (2025)",
      "authors": "Wudel, A.; Ehrenberg, A.",
      "year": 2025,
      "doi": "",
      "url": "https://library.fes.de/pdf-files/bueros/bruessel/21888-20250304.pdf",
      "abstract": "This publication examines Feminist Artificial Intelligence (FAI) as a framework that utilizes intersectional feminism to address biases and injustices in AI systems. FAI emphasizes interdisciplinary collaboration, systemic power analysis, and iterative theory-practice loops. By embedding feminist values (justice, freedom, and equity), FAI aims to transform AI development to ensure inclusivity and social sustainability. Practical applications include FemAI's advocacy for feminist perspectives in ",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies: Ja – Framework für KI-Kompetenzen/Werte-Integration; KI_Sonstige: Ja – Systemische Analyse von AI-Biases). SOZIAL_OK (Bias_Ungleichheit: Ja – adressiert Biases und Injustices; Gender: Ja – expliziter Gender-Fokus; Diversitaet: Ja – Inklusio"
      },
      "human": {
        "decision": "Unclear",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "MIT8HTC6",
      "title": "PreciseDebias: An automatic prompt engineering approach for generative AI to mitigate image demographic biases",
      "author_year": "Clemmer et al. (2024)",
      "authors": "Clemmer, C.; Ding, J.; Feng, Y.",
      "year": 2024,
      "doi": "",
      "url": "https://openaccess.thecvf.com/content/WACV2024/html/Clemmer_PreciseDebias_An_Automatic_Prompt_Engineering_Approach_for_Generative_AI_WACV_2024_paper.html",
      "abstract": "This paper presents a technical solution for reducing demographic bias in AI image generators through \"PreciseDebias,\" an automated approach that rewrites simple prompts into more complex, diversity-reflective prompts. The system analyzes model bias and strategically adds attributes like ethnicity, gender, or age to enforce fairer representation distributions, demonstrating that diversity-reflective prompting can be automated at the system level.",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt Generative KI (Bildgeneratoren) mit substantiellem Fokus auf Prompt-Engineering durch automatisierte Prompt-Rewriting. Adressiert demografische Bias-Reduktion explizit durch Gender-, Ethnizität- und Altersrepräsentation. Fairness-Metriken zur Bias-Messung zentral. TECHNIK erfüllt (Ge"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Fairness"
        ]
      }
    },
    {
      "id": "YN9JAREI",
      "title": "Faires KI-Prompting – Ein Leitfaden für Unternehmen",
      "author_year": "Gengler et al. (2024)",
      "authors": "Gengler, E.; Kraus, A.; Bodrožić-Brnić, K.",
      "year": 2024,
      "doi": "",
      "url": "https://www.digitalzentrum-zukunftskultur.de/wp-content/uploads/2024/05/Faires-KI-Prompting-Ein-Leitfaden-fuer-Unternehmen.pdf",
      "abstract": "This practical guide argues that feminist AI competency results in conscious prompt design to actively reduce stereotypes and discrimination in AI-generated content. It presents the \"KI-FAIRNESS\" framework for diversity-reflective prompting and demonstrates how specific, context-rich prompts lead to fairer and more representative results by compensating for AI's \"blind spots\" through targeted instructions.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies, Generative_KI, Prompting alle Ja) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness alle Ja). Fokus auf feministisches KI-Prompting mit KI-FAIRNESS-Framework für faire, diskriminierungsvermeidende Prompt-Gestaltung. Su"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "UJ7DXK8Y",
      "title": "NLPositionality: Characterizing design biases of datasets and models",
      "author_year": "Santy et al. (2023)",
      "authors": "Santy, S.; O'Connor, A.; Shi, E.; Wang, A.; Dai, J.; Klein, D.",
      "year": 2023,
      "doi": "",
      "url": "https://aclanthology.org/2023.acl-long.530/",
      "abstract": "This study provides a theoretical and methodological framework for making bias visible in AI models and datasets. It introduces the concept of \"positionality\" - the assumption that developers' social, cultural, and political perspectives inevitably flow into AI artifacts. The authors develop a method to quantify design biases by analyzing which dialects, demographic groups, or social contexts are under- or over-represented in datasets.",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt substantiell algorithmische Bias und Repräsentationsprobleme in KI-Datasets/Modellen (KI_Sonstige: Ja). Das Konzept der 'Positionality' basiert auf einer kritischen Perspektive, die implizit feministische Technikforschung (d'Ignazio & Klein) widerspiegelt - Fokus auf positionality, m"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "MJDTRLAI",
      "title": "Can prompt modifiers control bias? A comparative analysis of text-to-image generative models",
      "author_year": "Shin et al. (2024)",
      "authors": "Shin, P. W.; Ahn, J. J.; Yin, W.; Sampson, J.; Narayanan, V.",
      "year": 2024,
      "doi": "",
      "url": "https://arxiv.org/abs/2406.05602",
      "abstract": "This preprint investigates whether explicit prompt modifiers can reduce societal biases in text-to-image generative AI models. Authors evaluated three models (Stable Diffusion, DALL·E 3, Adobe Firefly) comparing baseline versus bias-mitigating prompts. Analysis revealed notable biases across all models, with inconsistent effectiveness of prompt modifiers. While diversity-reflective prompting can expose hidden biases and sometimes nudge outputs towards inclusivity, it is not a comprehensive fix a",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: (1) TECHNIK: Generative_KI (Text-to-Image Modelle), Prompting (Analyse von Prompt-Modifizierern zur Bias-Reduktion) → Ja. (2) SOZIAL: Bias_Ungleichheit (explizit: Analyse gesellschaftlicher Biases), Diversitaet (diversity-reflective prompting), Fairness (ethische"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ]
      }
    },
    {
      "id": "6L85PRUW",
      "title": "Inclusive prompt engineering: A methodology for hacking biased AI image generation",
      "author_year": "Skilton & Cardinal (2024)",
      "authors": "Skilton, R.; Cardinal, A.",
      "year": 2024,
      "doi": "10.1145/3641237.3691655",
      "url": "https://www.researchgate.net/publication/385325948_Inclusive_Prompt_Engineering_A_Methodology_for_Hacking_Biased_AI_Image_Generation",
      "abstract": "This conference paper introduces \"inclusive prompt engineering\" as a strategy to probe and mitigate biases in generative AI image systems. Authors developed methodology to systematically modify prompts and provide tools for generating more diverse outputs. User studies revealed that when participants encountered stereotypical outputs, they tried adding negative qualifiers but models often failed to obey these negations. Findings underscore need for improved prompt interfaces that actively promot",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt substantiell Prompting-Strategien zur Bias-Mitigation in generativen KI-Systemen (Bildgeneratoren). TECHNIK erfüllt: AI_Literacies (Kompetenzentwicklung für inklusives Prompting), Generative_KI (Bildgeneratoren), Prompting (Prompt-Engineering als Kernmethodologie). SOZIAL erfüllt: Bi"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "YLAKP7Z2",
      "title": "Intersectional analysis of visual generative AI: The case of Stable Diffusion",
      "author_year": "Jääskeläinen et al. (2025)",
      "authors": "Jääskeläinen, P.; Sharma, N. K.; Pallett, H.; Åsberg, C.",
      "year": 2025,
      "doi": "10.1007/s00146-025-02207-y",
      "url": "https://link.springer.com/article/10.1007/s00146-025-02207-y",
      "abstract": "This open-access paper provides a feminist intersectional critique of Stable Diffusion through qualitative visual analysis of 180 AI-generated images. Authors examined how power systems like racism, sexism, heteronormativity, ableism, colonialism, and capitalism are reflected in AI outputs. Found that default outputs frequently perpetuate harmful stereotypes and assume a \"white, able-bodied, masculine-presenting\" default subject position. Advocates for social justice-oriented approach to AI by a",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch Generative_KI (Stable Diffusion-Analyse). SOZIAL_OK durch Bias_Ungleichheit (Stereotype, Diskriminierung), Gender (Sexismus-Fokus), Diversitaet (Inklusion marginalisierter Gruppen), und Feministisch (explizite feministische intersektionale Methodik n"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "8WBUGXRR",
      "title": "Reflexive prompt engineering: A framework for responsible prompt engineering and AI interaction design",
      "author_year": "Djeffal (2025)",
      "authors": "Djeffal, C.",
      "year": 2025,
      "doi": "10.1145/3715275.3732118",
      "url": "",
      "abstract": "This paper proposes \"Reflexive Prompt Engineering\" as a comprehensive framework to embed ethical and inclusive principles into prompt crafting for generative AI. Framework consists of five components: prompt design, system selection, system configuration, performance evaluation, and prompt management, each considered from social responsibility perspective. Positions responsible prompt engineering as essential component of AI literacy, bridging gap between AI development and deployment by alignin",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies, Generative_KI, Prompting alle Ja), SOZIAL_OK (Bias_Ungleichheit, Diversitaet, Fairness alle Ja). Framework für verantwortungsvolles Prompt Engineering mit explizitem Fokus auf ethische Prinzipien, Menschenrechte und Diversität. Keine Sozial"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Unclear",
        "severity": "low",
        "affected_categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "KI_Sonstige",
          "Gender",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "IDW7QSYG",
      "title": "Female perspectives on algorithmic bias: Implications for AI researchers and practitioners",
      "author_year": "Fraile-Rojas et al. (2025)",
      "authors": "Fraile-Rojas, B.; De-Pablos-Heredero, C.; Méndez-Suárez, M.",
      "year": 2025,
      "doi": "10.1108/MD-04-2024-0884",
      "url": "https://colab.ws/articles/10.1108%2Fmd-04-2024-0884",
      "abstract": "This study uses NLP and machine learning to analyze 172,041 tweets from female users discussing gender inequality in AI. It identifies prominent themes including the future of AI technologies and women's active role in ensuring gender-balanced systems. Findings show that algorithmic bias directly affects women's experiences, prompting engagement in online discourse about injustices. Women lead constructive conversations and create entrepreneurial solutions when faced with bias, demonstrating how",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: NLP/ML für Datenanalyse; AI_Literacies: feministische digitale Kompetenzen) und SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch, Fairness substantiell behandelt). Empirische Analyse von 172K Tweets zu Geschlechter-Perspektiven"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "Fairness"
        ]
      }
    },
    {
      "id": "AIZGTQKG",
      "title": "Gender bias in artificial intelligence: Empowering women through digital literacy",
      "author_year": "Shah (2025)",
      "authors": "Shah, S. S.",
      "year": 2025,
      "doi": "10.70389/PJAI.1000088",
      "url": "https://premierscience.com/pjai-24-524/",
      "abstract": "This narrative review examines how systemic gender biases are embedded in AI systems across domains (e.g. hiring, healthcare, finance) and explores digital literacy as a tool to combat these biases. Key findings indicate that biases arise from underrepresentation of women in AI development, biased training data, and algorithmic design choices. Digital literacy programs for women are highlighted as a promising intervention that raises critical awareness of AI bias, encourages women's participatio",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.88,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt BEIDE Bedingungen: TECHNIK (AI_Literacies: digitale Literacy als Intervention + KI_Sonstige: Bias in Hiring/Healthcare/Finance-Systemen) UND SOZIAL (Gender: expliziter Gender-Bias-Fokus; Bias_Ungleichheit: Underrepresentation von Frauen; Diversitaet: Women-led AI projects; Fairness: al"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "X7ZGW6CN",
      "title": "Intersectional Artificial Intelligence Is Essential: Polyvocal, Multimodal, Experimental Methods to Save AI",
      "author_year": "Ciston (2024)",
      "authors": "Ciston, S.",
      "year": 2024,
      "doi": "10.7559/CITARJ.V11I2.665",
      "url": "",
      "abstract": "Arguments for applying intersectional strategies to AI at all levels - from data through design to implementation. Intersectionality, integrating institutional power analysis and queer-feminist plus critical race theories, can contribute to AI reconceptualization. Intersectional framework enables analysis of existing AI biases and uncovering alternative ethics from counter-narratives. Research presents intersectional strategies as polyvocal, multimodal, and experimental, with community-focused a",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (AI_Literacies, Generative_KI, KI_Sonstige durch intersektionale AI-Strategien und Chatbot-Beispiel) und SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch durch explizite intersektionale und queer-feministische Theorie nach Crenshaw; Fairness durch"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ]
      }
    },
    {
      "id": "CHJQ52DC",
      "title": "AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?",
      "author_year": "Latif et al. (2024)",
      "authors": "Latif, E.; Zhai, X.; Liu, L.",
      "year": 2024,
      "doi": "",
      "url": "https://arxiv.org/html/2312.10833v4",
      "abstract": "Empirical study examining gender bias in large language models through analysis of over 6000 evaluated student responses from 70 male and 70 female participants. Research uses fine-tuned BERT models and GPT-3.5 to evaluate various training data configurations: gender-specific versus mixed datasets. Three evaluation metrics applied: Scoring Accuracy Difference for bias assessment, Mean Score Gaps (MSG) for gender disparities, and Equalized Odds (EO) for fairness measurement. Results show mixed-ge",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt TECHNIK-Bedingung (Generative_KI, KI_Sonstige) und SOZIAL-Bedingung (Bias_Ungleichheit, Gender, Fairness). ABER: Kein Bezug zu Sozialer Arbeit oder deren Zielgruppen/Praxis. Rein technische Studie zu LLM-Bias ohne Anwendungskontext in Sozialbereich. Für Literature Review Soziale Arbeit"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Gender"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "AIGLDZ4C",
      "title": "How to Create Inclusive AI Images: A Guide to Bias-Free Prompting",
      "author_year": "Articulate (2025)",
      "authors": "",
      "year": 2025,
      "doi": "",
      "url": "https://www.articulate.com/blog/how-to-create-inclusive-ai-images-a-guide-to-bias-free-prompting/",
      "abstract": "Practical guide examining prompt engineering strategies for creating inclusive AI-generated images and avoiding biased default outputs. Analysis shows AI image generators often produce stereotypical representations (white, male, slim, young, physically able) due to training on unbalanced internet datasets. Presents concrete techniques for inclusive prompt engineering: specifying visible identity characteristics (age, race, gender, body size, visible disabilities), using diversity-promoting terms",
      "item_type": "webpage",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt TECHNIK-Kriterium (Generative_KI + Prompting + AI_Literacies) und SOZIAL-Kriterium (Bias_Ungleichheit + Gender + Diversitaet + Fairness). Substantieller Fokus auf Prompt-Engineering-Strategien zur Reduktion von Bias in generativen KI-Bildern. Adressiert stereotype Darstellungen und prä"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "AFDLFCIL",
      "title": "Feminist Perspectives on AI: Ethical Considerations in Algorithmic Decision-Making",
      "author_year": "Ahmed (2024)",
      "authors": "Ahmed, U.",
      "year": 2024,
      "doi": "",
      "url": "https://www.researchcorridor.org/index.php/jgsi/article/download/330/314",
      "abstract": "Explores ethical implications of algorithmic decision-making from feminist perspective, examining data bias, discrimination in automated systems, and underrepresentation of women in AI development. Algorithmic biases disproportionately affect marginalized groups and reinforce societal inequalities in areas like hiring, healthcare, and law enforcement. Feminist ethical framework emphasizes transparency, fairness, and inclusivity while questioning patriarchal and corporate-driven narratives in AI ",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt TECHNIK-Kriterium durch KI_Sonstige (algorithmische Entscheidungssysteme in Hiring, Healthcare, Law Enforcement). SOZIAL-Kriterium erfüllt durch: Bias_Ungleichheit (Diskriminierung, Bias in automatisierten Systemen), Gender (Unterrepräsentation von Frauen in AI), Diversitaet (marginali"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "FDR5APIU",
      "title": "Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action",
      "author_year": "Ulnicane (2024)",
      "authors": "Ulnicane, I.",
      "year": 2024,
      "doi": "10.17645/si.v12.7543",
      "url": "",
      "abstract": "Analyzes emerging intersectionality agenda in AI through examination of four high-level reports on this topic (2019-2021). Research shows how these documents frame problems and formulate recommendations for addressing inequalities. AI systems often amplify and exacerbate human biases and stereotypes, leading to discrimination and marginalization. Analysis reveals systematic problems including diversity crises in AI development where founders and employees mainly come from homogeneous groups of w",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper analysiert Intersektionalität in KI-Systemen explizit mit feministischer Perspektive (Intersektionalitätsagenda nach Crenshaw-Tradition). Thematisiert substantiell: algorithmische Bias/Diskriminierung (KI_Sonstige), strukturelle Ungleichheit (Bias_Ungleichheit), Gender-Perspektive (homogene Gr"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "SWB86AYC",
      "title": "Generative AI and the Future of Digital Literacy: Opportunities for Gender Inclusion",
      "author_year": "Hartshorne & Cohen (2025)",
      "authors": "Hartshorne, R.; Cohen, J.",
      "year": 2025,
      "doi": "",
      "url": "",
      "abstract": "Examines generative AI potential for promoting female inclusion in primary and secondary education while addressing inherent risks of reinforcing gender-specific biases. Qualitative research analyzes systemic effects of generative AI tools on teaching and learning through interviews and focus groups with students and teachers. Results show complex interactions between generative AI technologies and gender dynamics. Key factors for effective AI-supported learning environments include personalized",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (AI_Literacies: Ja, Generative_KI: Ja); SOZIAL_OK (Bias_Ungleichheit: Ja, Gender: Ja, Diversitaet: Ja). Qualitative empirische Studie zu generativen KI in Bildung mit explizitem Gender-Fokus und Analyse von Bias-Risiken. Kein explizit feministischer Theori"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "GP4JDSI8",
      "title": "Data Feminism for AI",
      "author_year": "Klein & D'Ignazio (2024)",
      "authors": "Klein, L.; D'Ignazio, C.",
      "year": 2024,
      "doi": "10.1145/3630106.3658543",
      "url": "",
      "abstract": "Presents intersectional feminist principles for just, ethical, and sustainable AI research. Extends seven Data Feminism principles to AI contexts: examine power, challenge power, rethink binaries and hierarchies, elevate emotion and embodiment, consider context, embrace pluralism, and make work visible. Proposes two additional principles on environmental impacts and consent. Framework helps identify and mitigate predictable harms before releasing discriminatory systems. Practical applications in",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch AI_Literacies (Framework für KI-Kompetenzen/ethische Prinzipien) und KI_Sonstige (ML-Design, algorithmische Systeme). SOZIAL_OK durch Feministisch (explizite Data Feminism Theorie nach D'Ignazio), Bias_Ungleichheit (Fokus auf diskriminatorische Syste"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "KI_Sonstige"
        ]
      }
    },
    {
      "id": "GPSB87RN",
      "title": "What is Feminist AI?",
      "author_year": "Wudel & Ehrenberg (2025)",
      "authors": "Wudel, A.; Ehrenberg, A.",
      "year": 2025,
      "doi": "",
      "url": "https://library.fes.de/pdf-files/bueros/bruessel/21888-20250304.pdf",
      "abstract": "Defines Feminist AI (FAI) as framework using intersectional feminism to address bias and inequalities in AI systems. Emphasizes interdisciplinary collaboration, systemic power analysis, and iterative theory-practice loops. Embeds feminist values of equality, freedom, and justice to transform AI development. Includes practical applications like FemAI advocacy for feminist perspectives in EU AI Act and MIRA diagnostic platform aligning AI tools with social justice goals. Distinguishes FAI from tra",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK-OK durch KI_Sonstige (Bias- und Fairness-Fokus auf AI-Systeme). SOZIAL-OK durch Bias_Ungleichheit, Gender, Diversitaet, Feministisch (explizit intersektionales Feminist AI Framework) und Fairness. Verwendet feministische Theorie substantiell als Kernrahmen zu"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "SHJQQTI6",
      "title": "A Survey on Intersectional Fairness in Machine Learning: Opportunities and Challenges",
      "author_year": "Gohar & Cheng (2023)",
      "authors": "Gohar, U.; Cheng, L.",
      "year": 2023,
      "doi": "10.24963/ijcai.2023/742",
      "url": "",
      "abstract": "Comprehensive survey examining intersectional fairness in machine learning systems beyond traditional binary fairness approaches. Presents taxonomy of intersectional fairness concepts showing how multiple sensitive attributes interact to create unique discrimination forms. Identifies challenges including data sparsity for intersectional groups and bias mitigation complexity. Demonstrates applications in NLP systems, ranking algorithms, and image recognition. Shows traditional fairness metrics fa",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige: ML-Systeme, Fairness-Metriken); SOZIAL_OK (Bias_Ungleichheit, Gender, Diversitaet, Feministisch via intersektionale Analyse). Explizit intersektionales Framing (Crenshaw-Tradition) mit Fokus auf marginalisierte Gruppen und Machtstrukturen ist"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "9Y7ZFGI5",
      "title": "Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy",
      "author_year": "Shah (2025)",
      "authors": "Shah, S. S.",
      "year": 2025,
      "doi": "10.70389/PJAI.1000088",
      "url": "",
      "abstract": "Narrative review examining interplay between gender bias in AI systems and digital literacy potential for empowering women in technology. Synthesizes research from 2010-2024 analyzing systematic gender biases in AI applications across recruitment, healthcare, and financial services. These biases stem from women's underrepresentation in AI development teams (only 22% globally), biased training data, and algorithmic design decisions. Digital literacy programs show promise as intervention fostering",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: (1) TECHNIK: AI_Literacies (Digital Literacy Programme), KI_Sonstige (algorithmische Systeme in Recruitment/Healthcare/Finance); (2) SOZIAL: Bias_Ungleichheit (systematische Gender-Biases), Gender (expliziter Gender-Fokus), Diversitaet (Unterrepräsentation von Frauen"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "9BDIJE9B",
      "title": "Artificial Intelligence and gender equality",
      "author_year": "UN Women (2024)",
      "authors": "",
      "year": 2024,
      "doi": "",
      "url": "https://www.unwomen.org/en/articles/explainer/artificial-intelligence-and-gender-equality",
      "abstract": "Comprehensive policy brief series analyzing how AI systems perpetuate gender inequalities while highlighting pathways for more equitable development. Documents that 44% of AI systems show gender bias, with 25% exhibiting both gender and racial bias, and provides evidence-based policy recommendations for addressing systematic underrepresentation and discriminatory outcomes.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "TECHNIK_OK: KI_Sonstige=Ja (algorithmische Systeme und deren Auswirkungen). SOZIAL_OK: Bias_Ungleichheit=Ja (Gender-Bias in 44% der KI-Systeme), Gender=Ja (expliziter Gender-Fokus), Fairness=Ja (Policy-Empfehlungen). Kein direkter Soziale_Arbeit-Bezug erkennbar. Nicht feministisch theoretisch fundie"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": "Human_Exclude_Agent_Include",
        "severity": "medium",
        "affected_categories": [
          "AI_Literacies",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ]
      }
    },
    {
      "id": "V4GTLMED",
      "title": "Tech workers' perspectives on ethical issues in AI development: Foregrounding feminist approaches",
      "author_year": "Browne et al. (2024)",
      "authors": "Browne, J.; Drage, E.; McInerney, K.",
      "year": 2024,
      "doi": "10.1177/20539517231221780",
      "url": "https://doi.org/10.1177/20539517231221780",
      "abstract": "Empirical study examining tech workers' understanding of ethical AI development through feminist lens, revealing critical gaps in current AI ethics approaches. Finds that the term \"bias\" creates confusion among tech workers, undermining AI ethics initiatives, and argues for moving beyond diversity narratives toward \"design justice\" that centers marginalized voices.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper kombiniert TECHNIK (AI_Literacies: KI-Ethik-Verständnis; KI_Sonstige: algorithmische Bias) mit SOZIAL (Bias_Ungleichheit: Analyse von Bias-Konzepten; Gender + Diversität + Feministisch: expliziter feministischer Ansatz; Fairness: Design Justice). Empirische Studie mit Tech-Worker-Perspektive u"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "YMYHLMFS",
      "title": "Data feminism for AI",
      "author_year": "Klein & D'Ignazio (2024)",
      "authors": "Klein, L.; D'Ignazio, C.",
      "year": 2024,
      "doi": "10.1145/3630106.3658543",
      "url": "https://doi.org/10.1145/3630106.3658543",
      "abstract": "Extends the influential \"Data Feminism\" framework to AI research, presenting intersectional feminist principles for conducting equitable, ethical, and sustainable AI research. Rearticulates original seven data feminism principles specifically for AI contexts and introduces two new principles addressing environmental impact and consent, providing concrete methodological guidance.",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK durch KI_Sonstige (allgemeiner KI/ML-Bezug); SOZIAL_OK durch Bias_Ungleichheit, Gender, Diversitaet und Feministisch (explizite Verwendung des Data-Feminism-Frameworks von D'Ignazio, intersektionale feministische Perspektive). Starker konzeptioneller Beitr"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "QM6L6XLZ",
      "title": "Incubating Feminist AI: Executive Summary 2021-2024",
      "author_year": "A+ Alliance (2024)",
      "authors": "",
      "year": 2024,
      "doi": "",
      "url": "https://aplusalliance.org/incubatingfeministai2024/",
      "abstract": "Documents outcomes from $2 million CAD Feminist AI Research Network project across Latin America, Middle East, and Asia. Developed 12 feminist AI prototypes addressing gender-based violence, transit safety, bias detection in NLP systems, and judicial transparency, demonstrating practical applications of feminist AI principles through community-centered design and intersectional analysis capabilities.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt beide Bedingungen: TECHNIK (KI_Sonstige: Bias-Detection in NLP, algorithmische Systeme im sozialen Kontext) und SOZIAL (Feministisch: explizit feminist AI; Gender: gender-based violence; Soziale_Arbeit: praktische Anwendungen bei Gewalt und Justiz; Bias_Ungleichheit: Bias-Detection"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "ZNHUCA4B",
      "title": "Recommendation on the Ethics of Artificial Intelligence",
      "author_year": "UNESCO (2021)",
      "authors": "",
      "year": 2021,
      "doi": "",
      "url": "https://unesdoc.unesco.org/ark:/48223/pf0000380455",
      "abstract": "First global standard on AI ethics adopted by 193 member states, establishing comprehensive policy frameworks addressing gender equality in AI development and deployment. Explicitly addresses gender stereotyping, discriminatory biases, and need for equitable participation across the AI lifecycle, with recent implementation including Women4Ethical AI platform and systematic bias studies.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "UNESCO-Empfehlung behandelt AI Ethics als globale Governance-Frage (KI_Sonstige: Ja). Substantieller Fokus auf Gender Equality, Geschlechterstereotypen und diskriminatorische Biases (Gender: Ja, Bias_Ungleichheit: Ja), sowie equitable participation und Fairness (Diversitaet: Ja, Fairness: Ja). Erfül"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Soziale_Arbeit",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 1,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "SQ38TTWQ",
      "title": "Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation",
      "author_year": "An et al. (2025)",
      "authors": "An, J.; Huang, D.; Lin, C.; Tai, M.",
      "year": 2025,
      "doi": "10.1093/pnasnexus/pgaf089",
      "url": "https://doi.org/10.1093/pnasnexus/pgaf089",
      "abstract": "Large-scale experimental study examining bias across five major LLMs using over 361,000 randomized resumes. Reveals complex intersectional patterns where LLMs favor female candidates but discriminate against Black male candidates, with bias effects translating to 1-3 percentage point differences in hiring probabilities, validating intersectionality theory through three key findings.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper kombiniert Technik (Generative_KI: Evaluation von LLMs) mit mehreren sozialen Kategorien. Explizit intersektionale Analyse (Feministisch: Crenshaw'sche Intersektionalität), Gender- und Diversitätsfokus, algorithmischer Bias in Hiring-Kontexten (Bias_Ungleichheit, Fairness). Erfüllt beide Bedin"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "ZMW228P6",
      "title": "Factoring the matrix of domination: A critical review and reimagination of intersectionality in AI fairness",
      "author_year": "Ovalle et al. (2023)",
      "authors": "Ovalle, A.; Subramonian, A.; Gautam, V.; Gee, G.; Chang, K. W.",
      "year": 2023,
      "doi": "10.1145/3600211.3604705",
      "url": "https://dl.acm.org/doi/abs/10.1145/3600211.3604705",
      "abstract": "Critical review examining how intersectionality is conceptualized and operationalized within AI fairness research, analyzing 30 papers from the field. Identifies significant gaps between intersectionality theory and technical applications, proposing six key tenets for properly applying intersectionality in AI research drawn from Patricia Hill Collins and Sirma Bilge's framework.",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper adressiert AI Fairness (KI_Sonstige) mit explizit feministisch-theoretischem Rahmen (Collins, Bilge). Substantielle Behandlung von Intersektionalität (Feministisch, Gender, Diversität, Bias_Ungleichheit). Kritische Analyse algorithmischer Systeme unter intersektionaler Perspektive. TECHNIK (KI"
      },
      "human": {
        "decision": "Exclude",
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "XW8NHCIE",
      "title": "Gender, race, and intersectional bias in AI resume screening via language model retrieval",
      "author_year": "Wilson & Caliskan (2024)",
      "authors": "Wilson, K.; Caliskan, A.",
      "year": 2024,
      "doi": "10.1609/aies.v7i1.31748",
      "url": "https://doi.org/10.1609/aies.v7i1.31748",
      "abstract": "Analyzes bias in large language models used for resume screening, examining over 550 job descriptions and 550 resumes across multiple demographic combinations. Reveals significant intersectional discrimination with Black male candidates facing most severe disadvantages, validating three key hypotheses of intersectionality theory through over 40,000 comparisons.",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK (Generative_KI + KI_Sonstige: LLM-basierte Systeme für Resume Screening), SOZIAL (Bias_Ungleichheit, Gender, Diversitaet, Feministisch via explizite intersektionale Analyse nach Crenshaw-Theorie, Fairness). Substanzielle empirische Analyse von Diskriminierung"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "ZLM537Z4",
      "title": "Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligent Machines",
      "author_year": "Browne et al. (2023)",
      "authors": "Browne, J.; Cave, S.; Drage, E.; McInerney, K.",
      "year": 2023,
      "doi": "",
      "url": "https://doi.org/10.1093/oso/9780192889898.001.0001",
      "abstract": "First comprehensive collection bringing together leading feminist thinkers across disciplines to examine AI's societal impact. Features 21 chapters covering topics from techno-racial capitalism to AI's military applications, examining how feminist scholarship can hold the AI sector accountable for designing systems that further social justice through diverse feminist approaches.",
      "item_type": "book",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Sammlung mit explizit feministischer Perspektive (TECHNIK: KI_Sonstige durch algorithmische Systeme und deren gesellschaftliche Auswirkungen; SOZIAL: Bias_Ungleichheit, Gender, Diversitaet und Feministisch alle substantiell durch Fokus auf Gerechtigkeit, Diskriminierung und feministische Theorie). B"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "JZN2I6J5",
      "title": "Prompting fairness: Learning prompts for debiasing large language models",
      "author_year": "Chisca et al. (2024)",
      "authors": "Chisca, A.-V.; Rad, A.-C.; Lemnaru, C.",
      "year": 2024,
      "doi": "",
      "url": "https://aclanthology.org/2024.ltedi-1.6/",
      "abstract": "Introduces novel prompt-tuning method for reducing biases in encoder models like BERT and RoBERTa through training small sets of additional reusable token embeddings. Demonstrates state-of-the-art performance while maintaining minimal impact on language modeling capabilities through parameter-efficient approach applicable across different models and tasks.",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.92,
        "categories": [
          "Prompting",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper behandelt Prompt-Tuning für Debiasing von LLMs und adressiert Fairness/Bias-Reduktion. Dies sind technische Kategorien. Allerdings fehlt ein substantieller Bezug zu sozialen Auswirkungen, sozialarbeiterischer Praxis oder marginalisierten Gruppen. Die Bias-Thematisierung ist rein algorithmisch-"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "MS3CNU3S",
      "title": "Bias and fairness in large language models: A survey",
      "author_year": "Gallegos et al. (2024)",
      "authors": "Gallegos, I. O.; Rossi, R. A.; Barrow, J.; Tanjim, M. M.; Kim, S.; Dernoncourt, F.; Yu, T.; Zhang, R.; Ahmed, N. K.",
      "year": 2024,
      "doi": "10.1162/coli_a_00524",
      "url": "https://doi.org/10.1162/coli_a_00524",
      "abstract": "Comprehensive survey consolidating notions of social bias and fairness in NLP, defining distinct facets of harm and introducing desiderata to operationalize fairness for LLMs. Proposes three taxonomies: metrics for bias evaluation, datasets for bias evaluation, and techniques for bias mitigation classified by intervention timing.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper erfüllt die TECHNIK-Bedingung (Generative_KI, Bias_Ungleichheit, Fairness alle Ja). Jedoch fehlt die SOZIAL-Bedingung im Sinne des Literature Reviews: Bias_Ungleichheit und Fairness sind zwar vorhanden, aber das Paper behandelt allgemeine KI-Ethik und -Fairness OHNE spezifischen Bezug zu S"
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "7G73H3KM",
      "title": "Generative AI and opportunities for feminist classroom assignments",
      "author_year": "Small (2023)",
      "authors": "Small, S. F.",
      "year": 2023,
      "doi": "",
      "url": "https://digitalcommons.calpoly.edu/feministpedagogy/vol3/iss5/10/",
      "abstract": "Addresses integration of generative AI into feminist classroom assignments to develop reflexivity and feminist epistemology. Proposes intentional feminist approaches where students collaborate with AI to develop understanding of feminist knowledge production, transforming AI from educational threat into tool for critical learning about power and epistemology.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Gender",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 1,
          "Fairness": 0
        },
        "reasoning": "Paper behandelt Integration generativer KI in Bildung (AI_Literacies: Ja, reflexive Kompetenzentwicklung). Fokus auf generative AI-Tools (Ja). Explizit feministische Perspektive und Epistemologie (Feministisch: Ja). Gender-Fokus durch feministische Theorieentwicklung (Gender: Ja). TECHNIK erfüllt (A"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": true,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "KNQYFQ6B",
      "title": "The power of prompts: Evaluating and mitigating gender bias in MT with LLMs",
      "author_year": "Sant et al. (2024)",
      "authors": "Sant, A.; Escolano, C.; Mash, A.; De Luca Fornaciari, F.; Melero, M.",
      "year": 2024,
      "doi": "10.18653/v1/2024.gebnlp-1.7",
      "url": "https://doi.org/10.18653/v1/2024.gebnlp-1.7",
      "abstract": "Examines gender bias in machine translation through LLMs using four widely-used test sets. Develops specific prompting engineering techniques that reduce gender bias by up to 12% on WinoMT evaluation dataset. Identifies optimal prompt structures incorporating explicit fairness instructions and context-aware guidelines.",
      "item_type": "conferencepaper",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.85,
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt LLMs (Generative_KI=Ja), Prompt-Engineering (Prompting=Ja), Gender-Bias (Gender=Ja) und Fairness-Metriken (Fairness=Ja). Dies erfüllt die TECHNIK-Bedingung klar. SOZIAL-Bedingung: Bias_Ungleichheit=Ja ist erfüllt. Allerdings liegt der Fokus ausschließlich auf technischer Sprachüb"
      },
      "human": {
        "decision": "Include",
        "categories": [
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Gender"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "8BUHU5EP",
      "title": "Feminist reflections for the development of Artificial Intelligence",
      "author_year": "Derechos Digitales (2023)",
      "authors": "",
      "year": 2023,
      "doi": "",
      "url": "https://www.derechosdigitales.org/fair-2023-en/",
      "abstract": "Synthesizes conversations among Latin American women developing AI systems, providing methodological frameworks for feminist AI development based on four real-world projects. Emphasizes co-design methodologies, digital autonomy, data sovereignty, and intersectional approaches through community agreements and power-balancing strategies.",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        },
        "reasoning": "Paper erfüllt beide Bedingungen: TECHNIK_OK (KI_Sonstige=Ja, da methodologische Frameworks für AI-Entwicklung), SOZIAL_OK (Feministisch=Ja durch explizite feministische Perspektive; Gender=Ja, Latin American women focus; Diversitaet=Ja, intersektionale Ansätze; Bias_Ungleichheit=Ja, power-balancing "
      },
      "human": {
        "decision": "",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "IGFYSIV8",
      "title": "Shaping feminist artificial intelligence",
      "author_year": "Toupin (2024)",
      "authors": "Toupin, S.",
      "year": 2024,
      "doi": "10.1177/14614448221150776",
      "url": "https://journals.sagepub.com/doi/full/10.1177/14614448221150776",
      "abstract": "Comprehensive examination of feminist artificial intelligence through historical analysis and contemporary typology development. Provides detailed framework categorizing FAI as: model, design, policy, culture, discourse, and science. Traces FAI's evolution from foundational work to contemporary initiatives, analyzing tensions between commercialized approaches and community-oriented methods.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.92,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 0
        },
        "reasoning": "Das Paper analysiert Feminist AI (FAI) mit explizit feministischer Theorie und Perspektive (erfüllt Feministisch=Ja). Es behandelt algorithmische Systeme und deren gesellschaftliche Auswirkungen (KI_Sonstige=Ja, Bias_Ungleichheit=Ja). Gender und Diversität sind zentral für die Analyse von FAI-Framew"
      },
      "human": {
        "decision": "Exclude",
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        }
      },
      "benchmark": {
        "has_human": true,
        "agreement": false,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "BDI6XU5A",
      "title": "Gender und KI-Anwendungen. Trägt KI zum Genderproblem oder zu seiner Lösung bei?",
      "author_year": "Franken & Mauritz (n.d.)",
      "authors": "Franken, Swetlana; Mauritz, Nina",
      "year": null,
      "doi": "",
      "url": "https://www.fh-bielefeld.de/multimedia/Fachbereiche/Wirtschaft+und+Gesundheit/ Forschung/Denkfabrik+Digitalisierte+Arbeitswelt/geki_Abschlussbericht.pdf",
      "abstract": "",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.75,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Der Titel signalisiert explizit einen Gender-Fokus im KI-Kontext ('Gender und KI-Anwendungen', Frage zu Genderproblemen). Dies aktiviert KI_Sonstige (allgemeine KI-Anwendungen) und Gender sowie Bias_Ungleichheit (Gender-Bias ist strukturelle Ungleichheit). TECHNIK + SOZIAL erfüllt → Include. Ohne Ab"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "EQV4DNQR",
      "title": "Challenging systematic prejudices: an Investigation into Gender Bias in Large Language Models",
      "author_year": "UNESCO, IRCAI (2024)",
      "authors": "UNESCO, IRCAI",
      "year": 2024,
      "doi": "",
      "url": "https://discovery.ucl.ac.uk/id/eprint/10188772/1/Unesco_results.pdf",
      "abstract": "This study explores biases in three significant large language models (LLMs): OpenAI’s GPT-2\nand ChatGPT, along with Meta’s Llama 2, highlighting their role in both advanced decision-making\nsystems and as user-facing conversational agents. Across multiple studies, the brief reveals how\nbiases emerge in the text generated by LLMs, through gendered word associations, positive or\nnegative regard for gendered subjects, or diversity in text generated by gender and culture.\nThe research uncovers persi",
      "item_type": "document",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "Generative_KI",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 1,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Paper untersucht Gender-Bias in LLMs (ChatGPT, GPT-2, Llama 2) – erfüllt TECHNIK-Kriterium (Generative_KI). Expliziter Fokus auf Geschlechter-Stereotypen, soziale Vorurteile und Fairness in generierten Texten erfüllt SOZIAL-Kriterien (Bias_Ungleichheit, Gender, Diversitaet, Fairness). Kein feministi"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "8NG4ZEWE",
      "title": "Intersektionalität",
      "author_year": "Arnold et al. (2023)",
      "authors": "Walgenbach, Katharina",
      "year": 2023,
      "doi": "",
      "url": "https://wb-erwachsenenbildung.net/intersektionalitaet/",
      "abstract": "",
      "item_type": "booksection",
      "journal": "",
      "llm": {
        "decision": "Exclude",
        "confidence": 0.95,
        "categories": [],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 0,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Keine ausreichende Information vorhanden. Titel allein ('Intersektionalität') ermöglicht keine verlässliche Kategorisierung. Ohne Abstract und Volltext können KI-Technologie-Bezug und spezifisches thematisches Fokus nicht bewertet werden. Restriktives Klassifizieren erfordert Ausschluss bei unzureic"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "5T55I5Z7",
      "title": "ARTIFICIAL INTELLIGENCE and GENDER EQUALITY",
      "author_year": "UNESCO (2020)",
      "authors": "UNESCO",
      "year": 2020,
      "doi": "",
      "url": "unesdoc.unesco.org/in/rest/annotationSVC/DownloadWatermarkedAttachment/attach_import_ab07646d-c784-4a4e-96a1-3be7855b6f76?_=374174eng.pdf&to=54&from=1",
      "abstract": "The present report builds on UNESCO’s previous\nwork on gender equality and AI and aims to continue\nthe conversation on this topic with a select group\nof experts from key stakeholder groups. In March\n2019, UNESCO published a groundbreaking report,\nI’d Blush if I Could: closing gender divides in digital\nskills through education, based on research funded\nby the German Federal Ministry for Economic\nCooperation and Development. This report featured\nrecommendations on actions to overcome\nglobal gender",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "AI_Literacies",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 0
        },
        "reasoning": "Das UNESCO-Paper behandelt substantiell die Themen Gender Equality und AI (Kategorie Gender: Ja). Es adressiert digitale Kompetenzen und Geschlechterkluft (AI_Literacies: Ja), Gender Bias in KI-Systemen (KI_Sonstige: Ja, Bias_Ungleichheit: Ja) sowie Diversität. Beide Bedingungen (Technik + Sozial) s"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "QUV5DQH3",
      "title": "When Good Algorithms Go Sexist: Why and How to Advance AI Gender Equity",
      "author_year": "Smith & Rustagi (2021)",
      "authors": "Smith, Genevieve; Rustagi, Ishita",
      "year": 2021,
      "doi": "10.48558/A179-B138",
      "url": "https://ssir.org/articles/entry/when_good_algorithms_go_sexist_why_and_how_to_advance_ai_gender_equity",
      "abstract": "Seven actions social change leaders and machine learning developers can take to build gender-smart artificial intelligence for a more just world.",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.85,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 0,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper adressiert algorithmen-basierte Systeme (KI_Sonstige) mit explizitem Fokus auf Gender-Bias und Geschlechter-Gerechtigkeit (Gender, Bias_Ungleichheit). Es behandelt konkrete Strategien für faire und gender-sensible KI-Entwicklung (Fairness). Beide Bedingungen erfüllt: TECHNIK (KI_Sonstige=J"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "4KMMPA6A",
      "title": "Faires KIPrompting – Ein Leitfaden für Unternehmen. BSP Business and Law School – Hochschule für Management und Recht",
      "author_year": "Gengler, & Bodrožić-Brnić, (2024)",
      "authors": "Gengler,, E.; Bodrožić-Brnić,, K.",
      "year": 2024,
      "doi": "",
      "url": "https://www.businessschool-berlin.de/files/Business-School-Berlin/Publikation/Faires-KI-Prompting-Ein-Leitfaden-fuer-Unternehmen%202024.pdf",
      "abstract": "Der vorliegende Leitfaden möchte Sie auf eine Reise durch die Welt der Generativen KI mitnehmen und Ihnen Werkzeuge an die Hand geben, um diese Technolo-\ngien verantwortungsvoll und bewusst zu nutzen. Wir möchten Verständnis für die positive wie negative Wirkung von Generativer KI schaffen, zugleich aber auch den Weg für einen diversen und fairen Einsatz ebnen. Dieser Guide kann Ihr Kompass sein, um nicht nur zu navigieren, sondern die digitale Zukunft mitzugestalten",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.78,
        "categories": [
          "AI_Literacies",
          "Generative_KI",
          "Prompting",
          "Bias_Ungleichheit",
          "Diversitaet",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 1,
          "Prompting": 1,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 1,
          "Feministisch": 0,
          "Fairness": 1
        },
        "reasoning": "Das Paper behandelt Kompetenzen im Umgang mit generativer KI (Leitfaden für verantwortungsvolle Nutzung), fokussiert explizit auf Prompting und Fairness (‚faires KI-Prompting'), sowie Diversität und Bias-Mitigation. TECHNIK-Bedingung erfüllt (AI_Literacies, Generative_KI, Prompting). SOZIAL-Bedingun"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "3ZNMTJ5B",
      "title": "feminist AI | ACADEMY",
      "author_year": "[Author not specified] (n.d.)",
      "authors": "",
      "year": null,
      "doi": "",
      "url": "https://www.feminist-ai.com/academy",
      "abstract": "Get ready for transforming power! We enable organizations to create more equitable AI through education. We hold workshops, give training, and provide learning material to raise awareness, build knowledge, and ease your creation of equitable AI.",
      "item_type": "webpage",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.72,
        "categories": [
          "AI_Literacies",
          "Bias_Ungleichheit",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 0,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 0,
          "Diversitaet": 0,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Material behandelt KI-Kompetenzen/Literacies durch Workshops und Schulungen zur Erstellung von 'equitable AI'. Der Titel 'feminist AI' und der explizite Fokus auf Gerechtigkeit und Fairness deuten auf feministische und Fairness-Perspektiven hin. Es erfüllt TECHNIK (AI_Literacies) und SOZIAL (Bia"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "J5EF9W6M",
      "title": "AI & Intersectionality: A Toolkit For Fairness & Inclusion",
      "author_year": "DIVERSIFAIR Project (2024)",
      "authors": "",
      "year": 2024,
      "doi": "",
      "url": "https://diversifair-project.eu/wp-content/uploads/2025/01/Copie-de-INDUSTRY-Educ-Kits-A4-V2.pdf",
      "abstract": "Das DIVERSIFAIR-Toolkit ist eine praktische Ressource, die sich an politische Entscheidungsträger*innen, die Industrie und die Zivilgesellschaft richtet. Es zielt darauf ab, ein Bewusstsein für intersektionale Diskriminierung in KI-Systemen zu schaffen und konkrete Handlungsstrategien zur Risikominderung anzubieten. Das Toolkit betont die Notwendigkeit, über einzelne Diskriminierungsachsen (wie Geschlecht oder Herkunft) hinauszudenken und deren Verschränkungen zu analysieren. Es fördert eine KI-",
      "item_type": "report",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "AI_Literacies",
          "Prompting",
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 1,
          "Generative_KI": 0,
          "Prompting": 1,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Das Toolkit adressiert substantiell KI-Kompetenzen (Bewusstsein und Handlungsstrategien für Stakeholder), Prompting-Strategien (intersektionale Szenarien zur Bias-Aufdeckung) und KI-Systeme generell. Die intersektionale Perspektive erfüllt implizit feministische Ansätze (Crenshaw'sche Intersektional"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    },
    {
      "id": "2EBHMYU4",
      "title": "Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action",
      "author_year": "Ulnicane (2024)",
      "authors": "Ulnicane, Inga",
      "year": 2024,
      "doi": "10.17645/si.7543",
      "url": "https://www.cogitatiopress.com/socialinclusion/article/view/7543",
      "abstract": "Diese Studie analysiert die aufkommende Agenda zu Intersektionalität in AI durch Untersuchung von vier hochrangigen Berichten zu diesem Thema (2019-2021). Die Forschung zeigt, wie diese Dokumente Probleme rahmen und Empfehlungen zur Adressierung von Ungleichheiten formulieren. AI-Systeme verstärken und verschärfen oft menschliche Verzerrungen und Stereotypen, was zu Diskriminierung und Marginalisierung führt. Die Analyse deckt systematische Probleme auf: Diversitätskrisen in AI-Entwicklung, wo G",
      "item_type": "journalarticle",
      "journal": "",
      "llm": {
        "decision": "Include",
        "confidence": 0.95,
        "categories": [
          "KI_Sonstige",
          "Bias_Ungleichheit",
          "Gender",
          "Diversitaet",
          "Feministisch",
          "Fairness"
        ],
        "all_categories": {
          "AI_Literacies": 0,
          "Generative_KI": 0,
          "Prompting": 0,
          "KI_Sonstige": 1,
          "Soziale_Arbeit": 0,
          "Bias_Ungleichheit": 1,
          "Gender": 1,
          "Diversitaet": 1,
          "Feministisch": 1,
          "Fairness": 1
        },
        "reasoning": "Paper erfüllt beide Bedingungen: (1) TECHNIK_OK: KI_Sonstige=Ja (algorithmische Systeme und ihre sozialen Auswirkungen). (2) SOZIAL_OK: Bias_Ungleichheit=Ja (systematische Diskriminierung durch AI), Gender=Ja (Geschlechterstereotypen), Diversitaet=Ja (Diversitätskrisen in AI-Entwicklung), Feministis"
      },
      "human": null,
      "benchmark": {
        "has_human": false,
        "agreement": null,
        "disagreement_type": null,
        "severity": null,
        "affected_categories": []
      }
    }
  ]
}