---
title: "Artificial Intelligence in a Structurally Unjust Society"
zotero_key: WC6JF3VD
author_year: "Lin (2022)"
authors: []

# Publication
publication_year: 2022.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://ojs.lib.uwo.ca/index.php/fpq/article/download/14191/12139/38443"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Lin_2022_Artificial.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Artificial Intelligence in a Structurally Unjust Society

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2022.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ⭐ Low |
| Vulnerable Groups & Digital Equity | 2/3 | ⭐⭐ Medium |
| Bias & Discrimination Analysis | 3/3 | ⭐⭐⭐ High |
| Practical Implementation | 1/3 | ⭐ Low |
| Professional/Social Work Context | 1/3 | ⭐ Low |


## Abstract

This study argues that AI bias represents a form of structural injustice that arises when AI systems interact with other social factors to exacerbate existing social inequalities. Using the example of AI in healthcare, the authors show that the goal of AI fairness is to strive for a more just social structure through appropriate development and use of AI systems. They argue that all actors involved in the unjust social structure bear shared responsibility to join collective actions for structural reform and offer practical recommendations for various social positions.


## AI Summary

## Overview

This paper by Lin and Chen presents a philosophical critique of mainstream AI fairness approaches by reframing AI bias as **structural injustice** rather than a technical problem. The authors argue that contemporary efforts—which pursue statistical parity and algorithmic debiasing—fundamentally misdiagnose the problem and cannot adequately address its ethical dimensions. They propose that AI bias emerges when AI systems interact with existing social inequalities to amplify disadvantages for certain groups while conferring unearned benefits to others. The paper bridges computer science ethics with social philosophy, establishing that meaningful progress requires **collective action targeting systemic social reform** rather than isolated technical interventions. Healthcare applications serve as the primary case study demonstrating this structural dynamic.

## Main Findings

The authors establish three critical findings: (1) **AI bias is structural**, not algorithmic—it results from AI systems operating within and reinforcing unjust social structures rather than from defective code; (2) **The dominant fairness paradigm is inadequate** because it mislocates the problem within algorithms, pursues disconnected statistical metrics, and obscures the distributed responsibility necessary for reform; (3) **Shared responsibility is essential**—all participating agents (developers, deployers, policymakers, affected communities, and institutions) bear moral obligation to contribute collective action according to their social position. The paper explicitly defines the goal of AI fairness as pursuing "a more just social structure with the development and use of AI systems when appropriate." Critically, the authors distinguish between two injustice mechanisms: undeserved burdens imposed on marginalized groups and unearned benefits conferred on privileged groups. Healthcare examples illustrate how technical fixes cannot overcome systemic injustices embedded in medical institutions, data practices, and resource allocation.

## Methodology/Approach

The paper employs **philosophical conceptual analysis** grounded in social justice theory rather than empirical research. The methodology combines: (1) theoretical reconceptualization of documented AI bias cases (recruiting algorithms discriminating against women, recidivism prediction systems targeting Black defendants, search engine stereotyping of women of color), (2) normative ethical reasoning about justice and responsibility distribution, and (3) case study analysis of healthcare AI applications. This approach prioritizes conceptual clarity and theoretical coherence, reflecting the authors' position that the fundamental problem is one of **problem-framing and understanding** rather than measurement or technical optimization.

## Relevant Concepts

**Structural Injustice:** Systemic disadvantage produced through interaction of multiple social institutions and practices, not reducible to individual wrongdoing or discrete policy failures.

**AI Bias (redefined):** The reproduction and amplification of existing social inequalities through AI systems interacting with unjust social structures, creating both undeserved burdens and unearned benefits.

**Shared Responsibility:** Distributed moral obligation across all participating agents in an unjust structure to contribute to collective reform efforts, differentiated by social position.

**Social Structure (in AI context):** The interconnected systems of institutions, practices, and social factors that AI systems operate within and reinforce.

**Technical-Fix Approach (critique):** The dominant paradigm treating AI fairness as achievable through algorithmic debiasing and statistical parity measures, which the authors argue is fundamentally insufficient.

## Significance

This work significantly advances **critical AI studies** by establishing that AI fairness cannot be achieved through engineering alone—it requires structural social change. The theoretical contribution clarifies why statistical parity measures fail to produce justice: they address symptoms rather than causes. The practical significance emerges through the authors' differentiated responsibility framework, offering guidance for diverse stakeholders to contribute according to their position. By grounding AI ethics in social philosophy, the paper challenges techno-optimistic assumptions and creates space for comprehensive policy approaches. The reframing has substantial implications: organizations pursuing AI fairness must recognize that technical solutions to structural problems represent a category error. This work establishes that meaningful AI fairness requires simultaneous attention to algorithmic design, institutional reform, policy change, and collective action—making it foundational for emerging critical perspectives in AI ethics and governance.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://ojs.lib.uwo.ca/index.php/fpq/article/download/14191/12139/38443
- **Zotero:** [Open in Zotero](zotero://select/items/WC6JF3VD)

## Related Concepts

- [[Concepts/Structural_Injustice|Structural Injustice]]
- [[Concepts/AI_Bias_redefined|AI Bias (redefined)]]
- [[Concepts/Shared_Responsibility|Shared Responsibility]]
- [[Concepts/Social_Structure_in_AI_context|Social Structure (in AI context)]]
- [[Concepts/Technical_Fix_Approach_critique|Technical-Fix Approach (critique)]]

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

