---
title: "Fairness Beyond the Algorithmic Frame: Actionable Recommendations for an Intersectional Approach"
zotero_key: Q2HILDPI
author_year: "Vethman (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "https://facctconference.org/static/docs/facct2025-206archivalpdfs/facct2025-final2348-acmpaginated.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Vethman_2025_Fairness.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Fairness Beyond the Algorithmic Frame: Actionable Recommendations for an Intersectional Approach

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ⭐ Low |
| Vulnerable Groups & Digital Equity | 3/3 | ⭐⭐⭐ High |
| Bias & Discrimination Analysis | 3/3 | ⭐⭐⭐ High |
| Practical Implementation | 2/3 | ⭐⭐ Medium |
| Professional/Social Work Context | 1/3 | ⭐ Low |


## Abstract

This study develops actionable recommendations for AI experts to implement intersectional approaches beyond purely technical solutions. The authors criticize the reduction of intersectionality to algorithmic bias measurements between subgroups and develop a framework that emphasizes interdisciplinary teams, community participation, and analysis of power structures in social context. Their approach includes six core recommendations: responsible AI expert role, multidisciplinary team building, reflection on societal positioning, participatory community engagement, power and context analysis, and data-sensitive metrics.


## AI Summary

## Overview

This 2025 FAccT conference paper addresses a critical gap in AI fairness research: the reduction of intersectionality—a theoretical framework from Black Feminist scholarship examining interconnected systems of oppression—to narrow algorithmic bias metrics. The authors argue that current AI fairness approaches focus exclusively on technical solutions (bias detection, fairness metrics) while sidelining essential intersectional dimensions: power relations, social justice, and structural inequality. Real-world cases including the 2019 Dutch childcare benefits scandal (disproportionately targeting 26,000 families with dual nationality) and France's automated welfare fraud detection system (flagging low-income and immigrant populations) demonstrate how algorithmic systems compound historical discrimination against marginalized communities. The paper proposes that AI experts, occupying central roles in system development, bear responsibility for limiting unjust outcomes through genuinely intersectional approaches that extend beyond technical optimization.

## Main Findings

The study identifies five actionable themes for implementing intersectional AI practices:

1. **Mandate interdisciplinary collaboration** incorporating expertise from social sciences, humanities, and affected communities
2. **Embed reflexivity and recognize positionality** to acknowledge researchers' social positions and inherent biases
3. **Approach communities as co-owners** in design and deployment, not passive research subjects
4. **Engage with power dynamics and social context** to understand historical discrimination and structural inequality
5. **Critically assess data framing and metric limitations** to prevent bias perpetuation through measurement systems

Significant implementation barriers emerged: tech-optimism (belief that technical solutions suffice for social problems) and experts' fear of insufficient knowledge in social justice domains. Conversely, participating AI experts responded positively, valuing recommendations as practical tools for communicating intersectionality's importance and catalyzing institutional change toward more just AI practices.

## Methodology/Approach

The authors employed participatory mixed-methods research combining systematic thematic analysis of AI fairness literature with community engagement involving AI experts. This approach grounds abstract theoretical critique in institutional practice contexts. The theoretical framework explicitly draws from Black Feminist intersectionality scholarship, emphasizing how multiple identity categories (race, gender, class, disability) interact to create compounded discrimination, rather than treating demographic categories as isolated variables. Critically, the literature analysis was evaluated through expert participation, ensuring recommendations reflected both theoretical rigor and practical feasibility. This participatory dimension distinguishes the work from purely theoretical critique.

## Relevant Concepts

**Intersectionality:** Theoretical framework examining how multiple identity categories interact to create compounded forms of discrimination and privilege; originated in Black Feminist scholarship to address interconnected systems of oppression.

**Algorithmic frame:** The dominant approach in AI fairness research that reduces intersectionality to technical problems addressable through bias metrics and algorithmic adjustments, sidelining social justice and power dynamics.

**Algorithmic bias:** Systematic errors in AI systems disproportionately harming specific demographic groups, often reflecting historical discrimination embedded in training data and design choices.

**Positionality:** Recognition that researchers and practitioners occupy specific social positions shaped by identity, experience, and power relations, influencing their perspectives, decisions, and blind spots.

**Tech-optimism:** Belief that technological solutions can resolve complex social problems without addressing underlying structural inequalities and power imbalances.

**Co-ownership:** Collaborative models where affected communities participate as equal partners in decision-making, design, and governance rather than serving as research subjects or passive stakeholders.

**Structural inequality:** Systemic disadvantages embedded in institutions, policies, and practices that perpetuate discrimination across generations and demographic groups.

## Significance

This work makes substantial theoretical and practical contributions to responsible AI discourse by fundamentally challenging the dominant technical paradigm in AI fairness research. Rather than proposing new algorithms or metrics, it advocates for epistemological and structural transformation in how fairness is conceptualized and practiced. The paper bridges computer science and social justice scholarship, demonstrating that technical expertise alone cannot address discrimination rooted in historical and structural inequality. By grounding recommendations in expert feedback and community engagement, the authors translate critical theory into actionable institutional practice—essential for meaningful implementation beyond academic discourse. The work explicitly positions itself against "algorithmic solutionism," arguing that genuine fairness requires interdisciplinary collaboration, reflexivity, power analysis, and community co-ownership. This represents a significant contribution to FAccT scholarship and broader conversations about responsible AI development, offering concrete guidance for practitioners while maintaining theoretical rigor grounded in social justice frameworks.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://facctconference.org/static/docs/facct2025-206archivalpdfs/facct2025-final2348-acmpaginated.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/Q2HILDPI)

## Related Concepts

- [[Concepts/Intersectionality|Intersectionality]]
- [[Concepts/Algorithmic_frame|Algorithmic frame]]
- [[Concepts/Algorithmic_bias|Algorithmic bias]]
- [[Concepts/Positionality|Positionality]]
- [[Concepts/Tech_optimism|Tech-optimism]]
- [[Concepts/Co_ownership|Co-ownership]]
- [[Concepts/Structural_inequality|Structural inequality]]

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

