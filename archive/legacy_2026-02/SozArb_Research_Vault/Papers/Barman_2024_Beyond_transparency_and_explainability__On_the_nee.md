---
title: "Beyond transparency and explainability: On the need for adequate and contextualized user guidelines for LLM use"
zotero_key: DMTJNKKH
author_year: "Barman (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1007/s10676-024-09778-2"
url: "https://doi.org/10.1007/s10676-024-09778-2"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 1
rel_bias: 2
rel_praxis: 2
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["AI Literacy", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-ai-komp-medium", "dim-bias-medium", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Barman_2024_Beyond.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Beyond transparency and explainability: On the need for adequate and contextualized user guidelines for LLM use

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | AI Literacy, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ⭐⭐ Medium |
| Vulnerable Groups & Digital Equity | 1/3 | ⭐ Low |
| Bias & Discrimination Analysis | 2/3 | ⭐⭐ Medium |
| Practical Implementation | 2/3 | ⭐⭐ Medium |
| Professional/Social Work Context | 1/3 | ⭐ Low |


## Abstract

Argues for user-centered approach to governing AI systems, contending that transparency alone is insufficient. Proposes contextualized guidelines and training for users including clear instructions on LLM reliability, diversity-sensitive prompting techniques, and iterative query refinement. Emphasizes shifting focus from AI's internal workings to human-AI interaction context.


## AI Summary

## Overview

The South Australian Government's Generative AI Guideline establishes a comprehensive governance framework for integrating generative AI and large language model (LLM) tools—including OpenAI's ChatGPT and Google Bard—across all SA Government agencies, personnel, and non-government suppliers accessing government resources. This policy-pragmatic document addresses institutional integration of emerging AI technologies while protecting sensitive information, maintaining data integrity, ensuring cybersecurity, and preserving record-keeping and privacy compliance. Rather than academic research, it provides precautionary governance guidance acknowledging both transformative productivity potential and substantial organizational hazards during rapid technological evolution, requiring agencies to conduct individualized risk assessments before authorization.

## Main Findings

The analysis reveals a fundamental tension: generative AI offers significant operational value through task automation (report generation, content creation, code debugging, brainstorming) that could enhance productivity, yet presents critical vulnerabilities requiring managed implementation. The most consequential finding concerns information confidentiality—user inputs into consumer-oriented platforms automatically enter the public domain, creating unprecedented exposure risks for sensitive government data. The document establishes a critical technical distinction: LLMs prioritize plausibility over accuracy, generating outputs resembling training data rather than factual information, directly contradicting government requirements for reliable, verifiable information. Organizational security maturity emerges as a prerequisite variable; agencies with underdeveloped privacy and cybersecurity programs face amplified risks. The guideline identifies multiple risk categories: information confidentiality, data integrity, cyber security, and record-keeping compliance. Crucially, the document acknowledges "unknown and unknowable risks," positioning this as justification for mandatory individual risk assessments. Employees retain full accountability for maintaining record-keeping, privacy, confidentiality, and integrity obligations regardless of AI tool usage.

## Methodology/Approach

The guideline employs a **risk-benefit analysis framework** grounded in the precautionary principle rather than empirical research. This approach systematically applies organizational maturity assessment as a gating mechanism for adoption, implements scope-based governance uniformly across heterogeneous agencies and external suppliers, develops categorical risk taxonomies addressing confidentiality, integrity, cybersecurity, and compliance dimensions, and distinguishes between consumer-oriented and enterprise-grade tools. The framework emphasizes human accountability and organizational responsibility, requiring agencies to balance productivity gains against documented limitations and emerging threats through structured risk assessment protocols.

## Relevant Concepts

**Generative AI/LLMs**: AI systems trained on textual associations to produce similar content, prioritizing plausibility over factual accuracy.

**Information Confidentiality Risk**: Exposure of sensitive government data through user inputs to public-domain platforms.

**Data Integrity**: Accuracy and reliability of information—compromised when LLM outputs prioritize similarity over factual correctness.

**Organizational Maturity Assessment**: Evaluation of existing security, privacy, and compliance program development as prerequisite for safe AI implementation.

**Precautionary Principle**: Conservative governance approach acknowledging unknown risks justifies restrictive guidance during technological uncertainty.

**Record-keeping and Privacy Obligations**: Employee responsibilities for maintaining compliance regardless of AI tool usage.

**Consumer-oriented vs. Enterprise Tools**: Distinction affecting risk profiles and data exposure potential.

**Risk-Managed Implementation**: Conditional adoption requiring mandatory risk assessments and governance protocols before authorization.

## Significance

This guideline reflects institutional responses to AI disruption emerging in 2023-2024, prioritizing governance and risk mitigation over enthusiastic adoption. Its significance lies in establishing a replicable model for public sector AI governance balancing innovation with institutional protection, emphasizing organizational accountability and employee responsibility rather than technological inevitability. The document's cautious positioning increasingly represents mainstream public sector perspectives, influencing how government organizations globally approach generative AI integration while protecting citizen data, maintaining operational integrity, and ensuring compliance with record-keeping and privacy obligations.


## Links & Resources

- **DOI:** [10.1007/s10676-024-09778-2](https://doi.org/10.1007/s10676-024-09778-2)
- **URL:** https://doi.org/10.1007/s10676-024-09778-2
- **Zotero:** [Open in Zotero](zotero://select/items/DMTJNKKH)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

