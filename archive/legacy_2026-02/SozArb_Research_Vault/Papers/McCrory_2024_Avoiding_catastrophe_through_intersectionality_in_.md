---
title: "Avoiding catastrophe through intersectionality in global AI governance"
zotero_key: NMD5P5LN
author_year: "McCrory (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://www.cigionline.org/documents/3375/DPH-paper-Laine_McCrory.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_McCrory_2024_Avoiding.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Avoiding catastrophe through intersectionality in global AI governance

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ⭐ Low |
| Vulnerable Groups & Digital Equity | 3/3 | ⭐⭐⭐ High |
| Bias & Discrimination Analysis | 2/3 | ⭐⭐ Medium |
| Practical Implementation | 1/3 | ⭐ Low |
| Professional/Social Work Context | 1/3 | ⭐ Low |


## Abstract

In this working paper, McCrory argues that prevailing global AI governance efforts (especially those focused on “AI safety” and existential risks) lack an adequate intersectional, feminist perspective, which is needed to avert not just hypothetical future catastrophes but ongoing injustices. The paper analyzes seven prominent international AI policy initiatives (e.g. global AI accords and principles) against criteria derived from feminist theory, such as intersectionality, context, and power dynamics. McCrory finds that these high-level policies often remain techno-centric: they invoke abstract risks or neutrality, but fail to engage with how AI harms are unevenly distributed along lines of gender, race, and class. For example, current AI “safety” pledges seldom consider the lived experiences of marginalized communities or the way existing structural inequalities are mirrored in AI systems. The author contends that treating AI governance as a purely technical, top-down process is misguided; instead, governance should include meaningful participation from under-represented groups and incorporate feminist insights about power and oppression. The paper’s recommendations call for centering intersectionality in AI policy: explicitly addressing how AI-related risks and harms intersect with social identity and historical injustices, and ensuring that any frameworks for AI risk management or ethics actively involve those who have been marginalized by past technological developments.


## AI Summary

## Overview

This working paper from CIGI's Digital Policy Hub, supported by Mitacs partnership, addresses a critical gap in artificial intelligence governance by integrating feminist intersectional analysis into AI safety discourse. The research, authored by Laine McCrory, challenges the predominant AI safety movement—which focuses on existential risks and system alignment with human values—for treating these risks as universally impactful while overlooking how marginalized communities already experience disproportionate harms from current AI systems. The paper argues that meaningful AI governance requires moving beyond technocratic approaches that claim neutrality, instead recognizing how future AI risks are fundamentally interconnected with existing structural inequalities and power dynamics. By applying feminist policy analysis frameworks organized around five thematic dimensions (intersectionality, context, neutrality, control, and power), the work bridges traditionally separate scholarly domains to propose more equitable governance pathways.

## Main Findings

The analysis reveals five systematic deficiencies in current global AI safety initiatives:

1. **Limited feminist engagement:** AI safety policies demonstrate insufficient meaningful engagement with feminist principles and accountability mechanisms.

2. **Disconnected temporality:** Governance frameworks fail to establish explicit connections between hypothetical future existential risks and observable present-day harms already experienced by marginalized communities.

3. **Unacknowledged differential impacts:** Current AI systems replicate existing social biases and power hierarchies, yet policy frameworks inadequately recognize how marginalized groups face disproportionate existential risks from these systems.

4. **Exclusionary participation:** Affected communities lack meaningful participation mechanisms in policy development processes, limiting governance legitimacy and effectiveness.

5. **False universalism:** AI safety discourse operates with implicit universalism—assuming risks affect all populations equally—thereby obscuring how current technological harms are distributed unequally across social groups.

The research demonstrates that treating future AI risks as separate from current inequities prevents governance frameworks from achieving genuine accountability or addressing root causes of technological harm.

## Methodology/Approach

The paper employs feminist policy analysis as its primary analytical framework, systematically examining AI safety governance initiatives through five thematic dimensions. **Intersectionality** interrogates how multiple overlapping social identities create compounded disadvantages. **Context** examines how policies reflect specific historical and social circumstances. **Neutrality** questions claims of objectivity in governance. **Control** analyzes who holds decision-making power. **Power** investigates how policies reinforce or challenge existing hierarchies. This multidimensional approach enables evaluation of how initiatives address structural inequalities and whose voices shape policy development. The framework moves beyond surface-level critique to interrogate underlying assumptions about risk, universality, and governance legitimacy, revealing how AI safety policies often inadvertently reinforce existing power imbalances while claiming neutrality.

## Relevant Concepts

**Intersectionality:** Framework recognizing how multiple social identities (race, gender, class, disability, etc.) interact to create distinct, compounded experiences of marginalization and technological risk.

**AI Safety:** Discipline addressing existential threats from artificial intelligence through development of systems aligned with human values, ethics, explainability, and external control mechanisms.

**Feminist Policy Analysis:** Critical approach examining how policies reflect and reinforce power dynamics, whose interests are centered, how structural inequalities are addressed or perpetuated, and what accountability mechanisms exist.

**Existential Risk:** Threats to human civilization or marginalized populations' futures from advanced AI systems operating without adequate value alignment or human oversight.

**Technocratic Governance:** Top-down policy approaches prioritizing technical expertise and universal frameworks while marginalizing affected communities' participation, knowledge, and differential needs.

**Current Harms:** Observable, present-day negative impacts of existing AI systems on marginalized groups, including algorithmic bias, discriminatory outcomes, and exclusion from decision-making.

## Significance

This work holds substantial significance for academic, policy, and affected communities. It provides concrete frameworks for integrating equity considerations into AI safety governance rather than treating them as separate concerns. The research validates AI safety's core concerns about technological risks while demonstrating how universalist framings obscure differential impacts across social groups. For policymakers, the paper offers actionable recommendations: (1) integrate accountability and participation mechanisms into research and policy development, (2) ensure meaningful participation from marginalized communities, (3) explicitly connect future AI risks to current harms, and (4) acknowledge how current biases shape future AI impacts. By bridging AI safety and critical technology studies, the work establishes intersectionality as essential to legitimate, effective governance. This contribution challenges the field to move beyond technocratic approaches toward more inclusive, equitable AI development processes that acknowledge how technological futures are inseparable from present inequalities. The timing is critical: as AI systems rapidly proliferate, governance frameworks established now will determine whether future development perpetuates or disrupts existing power hierarchies.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.cigionline.org/documents/3375/DPH-paper-Laine_McCrory.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/NMD5P5LN)

## Related Concepts

- [[Concepts/Intersectionality|Intersectionality]]
- [[Concepts/AI_Safety|AI Safety]]
- [[Concepts/Feminist_Policy_Analysis|Feminist Policy Analysis]]
- [[Concepts/Existential_Risk|Existential Risk]]
- [[Concepts/Technocratic_Governance|Technocratic Governance]]
- [[Concepts/Current_Harms|Current Harms]]

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

