---
title: "Assessing trustworthy AI: Technical and legal perspectives of fairness in AI"
zotero_key: 9A73EE79
author_year: "Kattnig (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1016/j.clsr.2024.106053"
url: "https://graz.elsevierpure.com/ws/portalfiles/portal/89954869/1-s2.0-S0267364924001195-main.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Kattnig_2024_Assessing.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Assessing trustworthy AI: Technical and legal perspectives of fairness in AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ⭐ Low |
| Vulnerable Groups & Digital Equity | 2/3 | ⭐⭐ Medium |
| Bias & Discrimination Analysis | 3/3 | ⭐⭐⭐ High |
| Practical Implementation | 2/3 | ⭐⭐ Medium |
| Professional/Social Work Context | 1/3 | ⭐ Low |


## Abstract

Kattnig et al. examine the disconnect between existing AI fairness techniques and the requirements of non-discrimination law, highlighting that many algorithmic bias mitigation methods do not meet legal standards for equality. Focusing on the EU context (with particular attention to the forthcoming AI Act and established non-discrimination directives), the authors review state-of-the-art bias mitigation approaches – from pre-processing data fixes to in-processing algorithms – and evaluate them against legal concepts of fairness and equality. They discuss how ambiguous legal frameworks and the difficulty of defining “fairness” pose challenges: for instance, fairness has multiple interpretations (individual vs. group fairness, formal vs. substantive equality) and is understood differently across disciplines. The paper argues for an interdisciplinary legal methodology to complement technical solutions. In practice, this means moving beyond purely quantitative parity metrics and ensuring AI systems comply with human rights and equality principles (e.g. ensuring de facto non-discrimination for all data subjects). By contrasting algorithms with legal norms, the study underlines that trustworthy AI requires more than technical robustness – it demands alignment with justice and accountability frameworks.


## AI Summary

## Overview

This academic paper addresses a critical contemporary challenge in artificial intelligence: ensuring fairness and non-discrimination in AI systems through integrated technical and legal analysis. Published in *Computer Law & Security Review*, the work by Kattnig et al. from Graz University of Technology examines the fundamental gap between technical bias mitigation methods and legal compliance requirements, particularly within the European Union regulatory framework and the emerging AI Act. The research recognizes that as AI systems increasingly influence consequential decisions affecting human lives—from hiring algorithms to criminal risk assessment—the imperative to ensure fair, unbiased decision-making has become paramount. The paper's central premise challenges the prevailing assumption that technical solutions for fairness automatically satisfy legal standards, arguing instead for an integrated approach that bridges computer science and legal scholarship to establish trustworthy AI governance.

## Main Findings

The research reveals several critical gaps between technical and legal approaches to AI fairness. Most significantly, few existing bias mitigation methods adequately meet legal requirements for non-discrimination under EU regulations and the AI Act. The paper identifies that bias—defined as systematic and unfair behavior in AI systems—often emerges when training data contains historical inequalities, inadvertently perpetuating discrimination against already disadvantaged groups. The COMPAS algorithm case study exemplifies this problem, demonstrating how racial bias becomes embedded in decision-making systems with serious social consequences. Additional findings indicate that bias identification remains technically challenging despite widespread AI deployment, creating operational and compliance risks. The paper reveals that fairness definitions remain contested across technical and legal disciplines, creating conceptual confusion that impedes implementation. Crucially, AI systems lack "common sense" or causal reasoning capabilities, compounding fairness challenges beyond statistical bias mitigation. The authors conclude that comprehensive legal methodology is essential for proper AI fairness assessment, and that technical bias mitigation alone proves insufficient without legal validation and alignment with regulatory requirements.

## Methodology/Approach

The paper employs a comparative analytical framework that systematically reviews state-of-the-art bias mitigation technical methods while contrasting them against legal requirements. The geographic scope is limited to the European Union, with particular emphasis on AI Act compliance and existing legal frameworks. The methodology examines both fairness definitions and measurement approaches, identifying conceptual challenges in operationalizing fairness across disciplines. The framework analyzes both group fairness (equitable treatment across demographic groups) and individual fairness (similar treatment for similar individuals). This comparative approach deliberately bridges disciplinary boundaries, recognizing that neither purely technical optimization nor purely legal analysis suffices independently. The methodology acknowledges that bias identification remains technically challenging, requiring integrated expertise from computer science, statistics, and legal scholarship.

## Relevant Concepts

**Bias**: Systematic and unfair behavior or errors in AI systems leading to discriminatory outcomes and unjust decisions.

**Group Fairness**: Ensuring equitable treatment across demographic groups or protected categories in AI decision-making.

**Individual Fairness**: Ensuring similar individuals receive similar treatment from AI systems regardless of protected characteristics.

**Trustworthy AI**: AI systems operating transparently, safely, and in compliance with legal, ethical, and regulatory standards.

**Bias Mitigation**: Technical methods (pre-processing, in-processing, post-processing) designed to reduce or eliminate discriminatory outcomes.

**Non-discrimination**: Legal principle ensuring AI decisions do not unfairly disadvantage protected groups or individuals.

**Causality**: AI system's ability to infer cause-and-effect relationships, enabling "common sense" reasoning beyond statistical patterns.

**Legal Compliance Gap**: Mismatch between technical fairness metrics and actual legal requirements under EU regulations and AI Act.

**Fairness Metrics**: Quantitative measures assessing whether AI systems meet fairness standards across different definitions.

## Significance

This work significantly advances the emerging field of trustworthy AI governance by challenging disciplinary silos and advocating for integrated expertise. It contributes to responsible AI discourse by emphasizing that legal compliance must guide technical implementation, not follow it. The paper's position as a "bridge-builder" between computer science and legal studies reflects growing recognition that AI regulation requires interdisciplinary collaboration. By examining AI Act requirements specifically, the research provides practical guidance for EU compliance while advancing theoretical understanding of fairness in AI. The work establishes that data subjects' rights to fair, non-discriminatory treatment demand systematic solutions transcending technical optimization alone. For organizations deploying AI systems, the research highlights that achieving fairness requires alignment between technical bias mitigation approaches and legal frameworks, not merely technical excellence. The paper establishes a foundation for future regulatory frameworks, technical standards development, and organizational compliance strategies in trustworthy AI implementation.


## Links & Resources

- **DOI:** [10.1016/j.clsr.2024.106053](https://doi.org/10.1016/j.clsr.2024.106053)
- **URL:** https://graz.elsevierpure.com/ws/portalfiles/portal/89954869/1-s2.0-S0267364924001195-main.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/9A73EE79)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

