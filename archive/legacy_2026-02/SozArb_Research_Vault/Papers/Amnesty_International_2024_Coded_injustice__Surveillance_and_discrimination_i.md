---
title: "Coded injustice: Surveillance and discrimination in Denmark's automated welfare state"
zotero_key: C2MFBS5K
author_year: "Amnesty International (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://www.amnesty.org/en/documents/eur18/8709/2024/en/"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 2
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-prof-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Amnesty_International_2024_Coded.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Coded injustice: Surveillance and discrimination in Denmark's automated welfare state

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ⭐ Low |
| Vulnerable Groups & Digital Equity | 3/3 | ⭐⭐⭐ High |
| Bias & Discrimination Analysis | 3/3 | ⭐⭐⭐ High |
| Practical Implementation | 1/3 | ⭐ Low |
| Professional/Social Work Context | 2/3 | ⭐⭐ Medium |


## Abstract

Critical human rights investigation documenting how Denmark's AI-powered welfare fraud detection systems risk discriminating against people with disabilities, low-income individuals, migrants, refugees, and marginalized racial groups. Report examines up to 60 algorithmic models used to flag individuals for fraud investigations, revealing mass surveillance practices violating privacy rights and creating atmospheres of fear among welfare recipients. Key findings demonstrate how automated systems paired with extensive data collection from multiple government agencies create what approaches prohibited social scoring. Investigation reveals harmful psychological tolls on surveilled populations and argues automation exacerbates pre-existing structural inequalities rather than creating fair or efficient systems.


## AI Summary

## Overview

Amnesty International's 2024 report "Coded Injustice" examines Denmark's automated welfare administration system, specifically investigating how Udbetaling Danmark (UDK) deploys fraud-control algorithms that create discriminatory surveillance mechanisms targeting vulnerable populations. The document addresses a critical accountability gap by analyzing whether algorithmic decision-making in welfare systems complies with international human rights obligations regarding privacy, equality, and non-discrimination. The report positions algorithms as active agents capable of perpetuating and amplifying structural discrimination rather than neutral technical tools. It reveals systemic failures in state oversight, corporate transparency, and individual remedy mechanisms, demonstrating how technological automation institutionalizes discrimination against marginalized groups including foreign-affiliated individuals, ethnic minorities, atypical households, and welfare recipients within Denmark's hostile policy environment toward vulnerable populations.

## Main Findings

The report identifies violations across multiple dimensions. First, **structural discrimination**: UDK's algorithms employ discriminatory proxy variables—household composition patterns, foreign affiliations, residency status—that systematically target vulnerable populations. Second, **surveillance expansion**: "Duvet lifting" practices extend monitoring beyond individual applicants to their entire social networks, creating expansive surveillance infrastructure. Third, **digital exclusion paradox**: marginalized populations face simultaneous exclusion from services and forced mandatory digital compliance. Fourth, **transparency deficits**: affected individuals cannot understand algorithmic decisions or access meaningful remedies. Fifth, **accountability gaps**: existing state oversight mechanisms prove inadequate, violating international human rights law (privacy, equality, non-discrimination rights) and anticipating violations of the EU AI Act. Sixth, **corporate responsibility failures**: insufficient corporate transparency prevents accountability for algorithmic harms.

## Methodology/Approach

The analysis employs a rigorous human rights framework grounded in international law obligations. The methodology combines document analysis of UDK's algorithmic systems with empirical investigation of discriminatory impacts. Research incorporates stakeholder engagement including responses from authorities and companies, ensuring multi-perspectival analysis. An intersectional lens examines how discrimination compounds across overlapping marginalized identities. The approach treats algorithmic systems as measurable actors with demonstrable human rights implications. The framework integrates critical algorithm studies with human rights jurisprudence, bridging academic scholarship and policy advocacy.

## Relevant Concepts

**Structural discrimination**: Systemic patterns embedded in institutional practices that disadvantage specific groups independent of individual intent.

**Algorithmic discrimination**: Automated systems perpetuating structural discrimination through proxy variables correlating with protected characteristics.

**Duvet lifting**: Invasive monitoring extending beyond individual welfare recipients to social networks.

**Digital exclusion paradox**: Simultaneous service exclusion and forced mandatory digital compliance.

**Proxy variables**: Data inputs indirectly discriminating by correlating with protected characteristics.

**State oversight mechanisms**: Regulatory frameworks and accountability structures for algorithmic systems.

**Corporate responsibility**: Corporate accountability for algorithmic harms affecting human rights.

**Remedy mechanisms**: Access to justice and redress for individuals harmed by algorithmic discrimination.

## Significance

This report challenges techno-solutionist narratives positioning automation as neutral and efficiency-enhancing. It establishes algorithmic discrimination as a systemic justice issue requiring robust human rights safeguards in welfare contexts. Key recommendations include mandatory algorithmic impact assessments, enhanced transparency requirements, strengthened due process protections, and improved state oversight mechanisms. The work advances scholarly consensus that automated decision-making demands human rights compliance frameworks. By positioning algorithmic systems within human rights law rather than purely technical domains, the report establishes precedent for holding state and corporate actors accountable for algorithmic harms affecting vulnerable populations, particularly within hostile policy environments.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.amnesty.org/en/documents/eur18/8709/2024/en/
- **Zotero:** [Open in Zotero](zotero://select/items/C2MFBS5K)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

