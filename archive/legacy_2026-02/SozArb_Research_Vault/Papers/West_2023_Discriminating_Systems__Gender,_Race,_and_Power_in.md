---
title: "Discriminating Systems: Gender, Race, and Power in AI"
zotero_key: SIXWV7D4
author_year: "West (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: report
language: nan
doi: "nan"
url: "https://ainowinstitute.org/wp-content/uploads/2023/04/discriminatingsystems.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_West_2023_Discriminating.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Discriminating Systems: Gender, Race, and Power in AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ⭐ Low |
| Vulnerable Groups & Digital Equity | 3/3 | ⭐⭐⭐ High |
| Bias & Discrimination Analysis | 3/3 | ⭐⭐⭐ High |
| Practical Implementation | 1/3 | ⭐ Low |
| Professional/Social Work Context | 1/3 | ⭐ Low |


## Abstract

Diese einflussreiche Studie argumentiert, dass die Diversitätskrise im KI-Sektor und Bias in KI-Systemen zwei Manifestationen desselben Problems sind und gemeinsam angegangen werden müssen. Die Autoren zeigen, dass rein technische Ansätze zur Bias-Behebung unzureichend sind, da sie die systemischen Machtverhältnisse ignorieren, die sowohl Arbeitsplätze als auch Technologien formen. Das "Pipeline-Problem"-Narrativ wird als zu eng kritisiert, da es tieferliegende Probleme mit Arbeitsplatzkultur, Machtasymmetrien und struktureller Diskriminierung nicht adressiert. Die Studie fordert eine Verschiebung von technischer "Debiasing" zu breiterer sozialer Analyse.


## AI Summary

## Overview

"Discriminating Systems: Gender, Race, and Power in AI" is a 2019 report by West, Whittaker, and Crawford from the AI Now Institute that investigates systemic discrimination within artificial intelligence development and deployment. The document establishes a critical causal relationship between workforce underrepresentation and algorithmic bias, arguing that the homogeneous demographics of AI creators directly produce discriminatory AI systems that reflect and amplify historical discrimination patterns. Rather than treating diversity and algorithmic fairness as separate concerns, the authors position them as interconnected manifestations of structural inequality requiring simultaneous institutional transformation. The report fundamentally challenges technocratic approaches to bias mitigation, asserting that technical solutions alone cannot address problems rooted in power asymmetries, workplace cultures, and systemic exclusion.

## Main Findings

The research reveals an acute diversity crisis across the AI sector with extreme underrepresentation. Women constitute only 18% of authors at leading AI conferences and represent merely 10-15% of AI research staff at major technology companies (Google and Facebook respectively). The situation is dramatically worse for Black workers, comprising only 2.5-4% of workforces at major tech firms, while no public data exists regarding trans or non-binary workers. The document demonstrates that decades of "pipeline problem" research and intervention have failed to produce meaningful progress, indicating that recruitment-focused solutions are fundamentally inadequate. Instead, the authors identify workplace culture, power asymmetries, harassment, exclusionary hiring practices, unfair compensation, and tokenization as primary drivers of attrition and exclusion. Critically, the research highlights how AI systems are actively deployed for "classification, detection, and prediction of race and gender," practices requiring urgent re-evaluation given their discriminatory potential. Additionally, narrow "women in tech" frameworks inadvertently privilege white women while marginalizing other intersecting identities, and binary gender assumptions in AI research systematically erase non-binary and transgender experiences.

## Methodology/Approach

The document employs mixed analytical methods combining empirical data aggregation with critical institutional analysis. Authors compile diversity statistics from leading AI conferences and major technology companies, providing quantitative evidence of disparity. Simultaneously, they employ intersectional feminist theory to examine how race, gender, and power dynamics intersect within institutional contexts. The framework incorporates historical contextualization, connecting contemporary algorithmic bias to historical patterns of discrimination and "race science." Critically, the methodology rejects technocratic problem-framing, instead emphasizing structural power dynamics, institutional barriers, and the interconnection between workforce composition and system outputs as primary analytical lenses.

## Relevant Concepts

**Intersectionality**: The analytical framework recognizing how multiple marginalized identities (race, gender, sexuality, etc.) interact and compound discrimination experiences, rather than existing independently.

**Pipeline Problem**: The conventional framing attributing diversity gaps to insufficient recruitment of underrepresented groups from educational pipelines, which the authors critique as inadequate and masking deeper structural issues.

**Discrimination Feedback Loop**: The bidirectional mechanism whereby homogeneous AI development teams create systems reflecting their biases and historical discrimination patterns, which subsequently reinforce and amplify inequality in broader society through algorithmic deployment.

**Tokenization**: The practice of including minimal representation of marginalized individuals without addressing systemic barriers, power structures, or workplace culture.

**Algorithmic Bias as Social Justice Issue**: The conceptual reframing positioning AI discrimination not as technical problem requiring engineering solutions, but as structural inequality requiring institutional transformation.

## Significance

This report represents a paradigm shift in AI ethics discourse, establishing that algorithmic bias is fundamentally a social justice issue rather than merely a technical problem. By demonstrating the causal relationship between workforce demographics and system outputs, the authors provide empirical grounding for structural critiques of technology development. The work challenges industry narratives of meritocracy and incremental progress, instead advocating for profound institutional transformation addressing workplace power dynamics, hiring practices, and retention simultaneously with algorithmic fairness. The document's emphasis on intersectionality and rejection of narrow "women in tech" frameworks has substantially influenced subsequent policy discussions and corporate accountability frameworks. Published by influential scholars at a leading AI ethics institute, this work has become foundational to contemporary AI ethics scholarship and policy development.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://ainowinstitute.org/wp-content/uploads/2023/04/discriminatingsystems.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/SIXWV7D4)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

