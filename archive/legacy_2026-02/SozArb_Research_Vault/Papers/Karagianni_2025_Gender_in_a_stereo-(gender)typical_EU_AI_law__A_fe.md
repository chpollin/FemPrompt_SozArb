---
title: "Gender in a stereo-(gender)typical EU AI law: A feminist reading of the AI Act"
zotero_key: F8GURS3C
author_year: "Karagianni (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1017/cfl.2025.12"
url: "https://www.cambridge.org/core/journals/cambridge-forum-on-ai-law-and-governance/article/gender-in-astereogendertypical-eu-ai-law-a-feminist-reading-of-the-ai-act/E9DEFC1E114CBE577D737EC616610921"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Karagianni_2025_Gender.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Gender in a stereo-(gender)typical EU AI law: A feminist reading of the AI Act

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ⭐ Low |
| Vulnerable Groups & Digital Equity | 3/3 | ⭐⭐⭐ High |
| Bias & Discrimination Analysis | 3/3 | ⭐⭐⭐ High |
| Practical Implementation | 1/3 | ⭐ Low |
| Professional/Social Work Context | 1/3 | ⭐ Low |


## Abstract

This peer-reviewed article offers a feminist critique of the European Union’s AI Act, arguing that the law’s current bias mitigation provisions only superficially address gendered risks and fail to confront deeply embedded structural biases. Using frameworks like hermeneutical injustice and feminist legal theory, Karagianni shows that AI systems can perpetuate historical power imbalances and systemic discrimination against women and marginalized groups. The AI Act’s pre-market and post-market measures are analyzed to reveal how they often reinforce, rather than challenge, algorithmic discrimination. The author concludes that effective AI governance must go beyond technical fixes, incorporating an intersectional perspective and substantive equality principles. She calls for feminist-informed revisions to the AI Act – emphasizing gender inclusivity, intersectionality, and accountability – to ensure AI regulation actively dismantles (instead of inadvertently upholding) existing power asymmetries.


## AI Summary

## Overview

Anastasia Karagianni's research article, published in 2025, provides a critical feminist examination of the European Union's AI Act (Regulation 2024/1689), questioning whether this landmark regulatory framework adequately protects marginalized communities from gender-based discrimination embedded in artificial intelligence systems. Conducted within the Law Science Technology and Society (LSTS) Research Group at Vrije Universiteit Brussels, the paper challenges the assumption that formal regulatory provisions automatically translate into substantive protection against algorithmic bias. By applying feminist legal theory to specific AI Act provisions—particularly those addressing "special categories of personal data" processing—Karagianni argues that the regulation, while well-intentioned, perpetuates structural inequalities by failing to address the systemic power imbalances that shape AI development and deployment. The work positions itself within emerging critical scholarship that demands transformative rather than incremental approaches to technology governance.

## Main Findings

The analysis reveals significant gaps between the AI Act's stated objectives and its practical capacity to protect vulnerable populations. While the regulation permits processing "special categories of personal data" (including gender identity information) to prevent algorithmic discrimination, this mechanism proves insufficient for addressing deeply embedded structural biases. The paper identifies four critical deficiencies: first, existing provisions fail to challenge underlying structural inequalities that predate and shape AI systems; second, binary gender frameworks persist throughout AI development and deployment, marginalizing transgender and non-binary individuals; third, accountability mechanisms lack enforceability for gender-based harms; and fourth, the regulation inadequately addresses gender-based violence enabled by generative AI, particularly non-consensual deepfakes. The research demonstrates that current regulatory approaches treat discrimination as isolated technical problems rather than manifestations of systemic power hierarchies. Concrete examples—including Amazon's biased recruitment algorithm disadvantaging women, healthcare AI systems operating within binary gender frameworks that misgendered patients, and non-consensual deepfake technology constituting gender-based violence—illustrate how AI amplifies existing societal inequalities. Karagianni concludes that intersectional perspectives remain absent from regulatory design, limiting the AI Act's capacity to address compounded discrimination experienced by individuals with multiple marginalized identities (women, LGBTQIA+ people, and other marginalized communities).

## Methodology/Approach

The paper employs sophisticated feminist legal methods grounded in four complementary theoretical frameworks. Miranda Fricker's hermeneutical injustice theory illuminates how epistemic marginalization prevents certain groups from contributing to knowledge production about AI harms. Catharine MacKinnon's feminist legal theory reveals how male dominance structures become embedded in technological systems and legal frameworks. Aníbal Quijano's coloniality of power concept exposes how historical hierarchies persist within contemporary technology governance. Walter Mignolo's decolonial epistemology challenges whose knowledge counts in law and technology development. This multidisciplinary theoretical architecture enables systematic critical examination of specific AI Act articles, revealing how regulatory language either reinforces or fails to challenge algorithmic discrimination. The methodology combines textual analysis of regulatory provisions with theoretical critique of underlying epistemological assumptions.

## Relevant Concepts

**Hermeneutical injustice:** Epistemic marginalization preventing certain groups from contributing to collective understanding of AI harms and discrimination.

**Structural bias:** Systemic inequalities embedded within AI systems that disproportionately harm marginalized communities, rooted in biased training data and design choices.

**Intersectionality:** Framework recognizing how multiple marginalized identities (gender, sexuality, race, disability) compound discrimination experiences in AI systems.

**Coloniality of power:** Historical power hierarchies and epistemological dominance persisting within contemporary institutions, technologies, and regulatory frameworks.

**Algorithmic discrimination:** Discriminatory outcomes produced through automated decision-making systems, often reproducing historical biases at scale.

**Special categories of personal data:** EU legal concept referring to sensitive data (including gender identity) whose processing is restricted but permitted under specific conditions for non-discrimination purposes.

**Gender-based violence:** Harm targeting individuals based on gender, including non-consensual deepfakes and sexualized content enabled by generative AI.

## Significance

This research significantly contributes to critical AI governance scholarship by demonstrating that formal regulatory frameworks require feminist-informed revision to achieve substantive equity. The paper challenges technocratic optimism surrounding the AI Act, arguing that regulatory provisions alone cannot address discrimination rooted in epistemological and structural inequalities. By proposing feminist-informed revisions emphasizing gender inclusivity, intersectionality, and accountability mechanisms with enforcement capacity, Karagianni advances scholarship demanding transformative approaches to technology governance. The work proves particularly significant for policymakers, legal scholars, technology ethicists, and marginalized communities seeking to develop regulations that genuinely protect vulnerable populations rather than merely appearing to do so. It contributes to broader EU policy discourse on AI governance by identifying specific gaps requiring legislative amendment before the AI Act's full implementation.


## Links & Resources

- **DOI:** [10.1017/cfl.2025.12](https://doi.org/10.1017/cfl.2025.12)
- **URL:** https://www.cambridge.org/core/journals/cambridge-forum-on-ai-law-and-governance/article/gender-in-astereogendertypical-eu-ai-law-a-feminist-reading-of-the-ai-act/E9DEFC1E114CBE577D737EC616610921
- **Zotero:** [Open in Zotero](zotero://select/items/F8GURS3C)

## Related Concepts

- [[Concepts/Hermeneutical_injustice|Hermeneutical injustice]]
- [[Concepts/Structural_bias|Structural bias]]
- [[Concepts/Intersectionality|Intersectionality]]
- [[Concepts/Coloniality_of_power|Coloniality of power]]
- [[Concepts/Algorithmic_discrimination|Algorithmic discrimination]]
- [[Concepts/Special_categories_of_personal_data|Special categories of personal data]]
- [[Concepts/Gender_based_violence|Gender-based violence]]

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

