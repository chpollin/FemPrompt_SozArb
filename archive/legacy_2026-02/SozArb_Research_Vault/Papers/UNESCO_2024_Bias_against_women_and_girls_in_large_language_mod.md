---
title: "Bias against women and girls in large language models: A UNESCO study"
zotero_key: MMF4WBLF
author_year: "UNESCO (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://www.unesco.org/en/articles/generative-ai-unesco-study-reveals-alarming-evidence-regressive-gender-stereotypes"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_UNESCO_2024_Bias.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Bias against women and girls in large language models: A UNESCO study

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ⭐ Low |
| Vulnerable Groups & Digital Equity | 3/3 | ⭐⭐⭐ High |
| Bias & Discrimination Analysis | 3/3 | ⭐⭐⭐ High |
| Practical Implementation | 1/3 | ⭐ Low |
| Professional/Social Work Context | 1/3 | ⭐ Low |


## Abstract

Die Studie analysiert LLMs (z. B. GPT-3.5, Llama 2) und dokumentiert signifikante Stereotypisierungen gegen Frauen und queere Menschen in generierten Texten. Besondere Auffälligkeit bei Open-Source-Modellen.


## AI Summary

## Overview

UNESCO's March 2024 press release presents a significant institutional investigation into systemic gender bias, homophobic content, and racial stereotyping within large language models (LLMs), the foundational technology underlying generative AI applications. The study examines three major systems—GPT-3.5, GPT-2, and Llama 2—to determine whether these widely-used AI tools perpetuate retrograde gender stereotypes (outdated, regressive gender role expectations) and discriminatory patterns affecting women, LGBTQ+ individuals, and racial minorities. The research emerges strategically before International Women's Day, positioning AI bias as a contemporary social justice concern requiring multi-stakeholder intervention. UNESCO Director-General Audrey Azoulay emphasizes that millions of users interact daily with these systems in professional, educational, and domestic contexts, making even subtle biases consequential for real-world inequality amplification—the mechanism through which algorithmic discrimination scales globally and reinforces existing social hierarchies.

## Main Findings

The research reveals quantifiable disparities across all examined models. **Gender findings:** Women appear in domestic roles at four times the frequency of men, with consistent semantic associations linking women to "home," "family," and "children," while men correlate with "work," "executive positions," "salary," and "employment." Narrative analysis demonstrates qualitative differences: AI-generated stories about men employ adventurous language ("sea," "forest," "treasure," "adventure"), while female narratives emphasize romantic and domestic vocabulary ("love," "garden," "cute," "husband," "rose"). **LGBTQ+ findings:** 70% of Llama 2's completions about gay individuals expressed negative sentiments, while 60% of GPT-2's outputs similarly generated homophobic content. **Model comparison:** Open-source models (Llama 2, GPT-2) exhibited more pronounced bias than proprietary systems (GPT-3.5, GPT-4, Google Gemini), though this transparency paradoxically enables collaborative correction opportunities. **Racial stereotyping:** The study documents systematic racial bias in content generation, though specific percentages remain undisclosed in this press release.

## Methodology/Approach

The study employs mixed-methods comparative content analysis combining quantitative and qualitative approaches across three major LLM systems. Researchers conducted systematic prompt completion tasks, requesting AI systems to generate narratives about diverse demographic groups across racial, sexual orientation, and gender categories. Quantitative measurement tracked role representation frequencies and semantic associations through linguistic pattern analysis (word frequency correlations). Qualitative narrative analysis examined generated content for thematic patterns and stereotypical framings. The methodological framework assumes that training data biases become embedded in model parameters during training, subsequently influencing user perceptions through repeated exposure to biased outputs at scale.

## Relevant Concepts

**Algorithmic bias:** Systematic discrimination embedded in AI systems through biased training data and design choices, producing discriminatory outputs at scale affecting millions of users.

**Large Language Models (LLMs):** Neural networks trained on vast text corpora to predict and generate human language, serving as infrastructure for generative AI applications like ChatGPT and Gemini.

**Semantic associations:** Linguistic patterns linking concepts through statistical co-occurrence in training data, reinforcing stereotypical connections (e.g., "woman" + "domestic," "man" + "professional").

**Bias amplification:** The mechanism through which algorithmic discrimination magnifies existing social inequalities through widespread deployment, user influence, and perception-shaping at population scale.

**Retrograde stereotypes:** Outdated, regressive gender role expectations that contradict contemporary gender equality standards, perpetuating historical power imbalances.

## Significance

This research carries substantial policy implications, advocating for explicit regulatory frameworks, continuous corporate bias monitoring, and international research collaboration aligned with UNESCO's 2021 AI ethics recommendations. The study directs recommendations toward three stakeholder groups: governments (regulatory framework development), private corporations (systematic bias monitoring), and research communities (collaborative bias mitigation). By demonstrating that open-source transparency enables collaborative bias correction while closed models obscure accountability, the research rejects technological determinism while emphasizing multi-stakeholder responsibility. The findings inform emerging governance debates about AI accountability, corporate responsibility, and international regulatory harmonization, establishing UNESCO as an authoritative voice in AI ethics discourse and positioning AI bias within critical social justice frameworks rather than treating it as purely technical.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.unesco.org/en/articles/generative-ai-unesco-study-reveals-alarming-evidence-regressive-gender-stereotypes
- **Zotero:** [Open in Zotero](zotero://select/items/MMF4WBLF)

## Related Concepts

- [[Concepts/Algorithmic_bias|Algorithmic bias]]
- [[Concepts/Large_Language_Models_LLMs|Large Language Models (LLMs)]]
- [[Concepts/Semantic_associations|Semantic associations]]
- [[Concepts/Bias_amplification|Bias amplification]]
- [[Concepts/Retrograde_stereotypes|Retrograde stereotypes]]

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

