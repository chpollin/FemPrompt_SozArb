---
title: "Guardrails, not guidance: Understanding responses to LGBTQ+ language in large language models"
zotero_key: CWBWUWYL
author_year: "Tint (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "https://aclanthology.org/2025.queerinai-main.2.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Tint_2025_Guardrails.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Guardrails, not guidance: Understanding responses to LGBTQ+ language in large language models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ⭐ Low |
| Vulnerable Groups & Digital Equity | 3/3 | ⭐⭐⭐ High |
| Bias & Discrimination Analysis | 3/3 | ⭐⭐⭐ High |
| Practical Implementation | 1/3 | ⭐ Low |
| Professional/Social Work Context | 1/3 | ⭐ Low |


## Abstract

Examines how large language models respond to prompts involving LGBTQ+ terminology and how current safety measures handle such content. Finds disparity where LLMs invoke safety guardrails for overtly heteronormative prompts but exhibit subtle biases when handling queer slang or informal LGBTQ+ language, responding with more negative emotional tone without triggering content filters.


## AI Summary

## Overview

Joshua Tint's research examines how Large Language Models respond to LGBTQ+ linguistic expression, investigating implicit biases in emotional content of model outputs across six major systems (GPT-3.5, GPT-4o, Llama2, Llama3, Gemma, Mistral). The paper addresses a critical gap in fairness-in-AI research by distinguishing between two linguistic contexts: heteronormative language (normative expressions assuming heterosexuality as default) and LGBTQ+ slang (community-specific linguistic markers). Rather than analyzing how LLMs handle queer *topics*, this study examines how models process language *used by* queer communities. This distinction is significant because LGBTQ+ individuals disproportionately rely on online spaces mediated by LLMs for connection and support, making systemic bias particularly consequential. The research challenges the assumption that current safety mechanisms and debiasing techniques adequately protect marginalized communities, arguing that these "guardrails" primarily address overt discrimination while overlooking subtler forms of bias affecting non-standard linguistic features.

## Main Findings

The study reveals a critical paradox: heteronormative language triggers safety mechanisms producing neutral or corrective responses, while LGBTQ+ slang elicits disproportionately negative emotional labels. This counterintuitive finding indicates that safety guardrails, designed to prevent harm, may inadvertently disadvantage minority linguistic communities. Specifically, the research demonstrates that current fairness approaches fail to address systemic biases in emotional responses to queer slang—models assign more negative sentiment to LGBTQ+ linguistic markers than to heteronormative equivalents. The findings suggest that equitable LLM outcomes require moving beyond eliminating explicit bigotry to addressing how models emotionally respond to marginalized communities' authentic language use. Critically, the paper emphasizes that safety mechanisms function as "guardrails" (restrictive boundaries) rather than "guidance" (constructive support), limiting rather than enabling equitable outcomes for minority communities.

## Methodology/Approach

The research employs two experiments measuring emotional content in LLM responses using sentiment analysis across six major models. The methodology operationalizes "emotional content" through sentiment valence measurement—quantifying positive versus negative emotional tone in model outputs. Rather than measuring explicit discriminatory outputs or safety filter triggers, this approach captures implicit bias through emotional labeling patterns. The study compares responses to heteronormative versus non-heteronormative prompts and examines how LGBTQ+ slang influences emotional valence. This measurement innovation enables detection of systemic inequities manifesting through emotional tone rather than overt content filtering. The approach distinguishes itself by focusing on language *used by* queer people rather than queer topics, and by examining implicit bias through emotional analysis rather than surface-level fairness assessments.

## Relevant Concepts

**Heteronormative Language**: Linguistic expressions assuming heterosexuality as the default or normative orientation, often invisible as a linguistic marker.

**LGBTQ+ Slang/Argot**: Community-specific linguistic markers and non-standard language features used by queer communities for identity expression and in-group communication.

**Implicit Bias**: Unconscious prejudices reflected in system outputs that don't constitute explicit discrimination but systematically disadvantage marginalized groups through subtle mechanisms.

**Emotional Content/Sentiment Valence**: Quantifiable measurement of positive versus negative emotional tone embedded in model responses, used as an indicator of systemic bias.

**Safety Mechanisms as Guardrails**: Automated restrictions designed to prevent harmful outputs that may inadvertently neutralize or negatively respond to marginalized communities' authentic language.

**Community-Driven Evaluation**: Fairness assessment frameworks incorporating perspectives from affected communities rather than relying solely on technical metrics.

## Significance

This research advances fairness-in-AI scholarship by introducing emotional content measurement as a bias detection tool and demonstrating that existing debiasing approaches are fundamentally insufficient. The findings have direct implications for LLM developers: achieving equitable outcomes requires redesigning safety mechanisms to avoid disadvantaging minority linguistic communities. The work contributes to emerging community-driven evaluation frameworks, emphasizing that technical solutions must be complemented by structural changes addressing how language technologies process marginalized communities' authentic expression. By demonstrating that guardrails can paradoxically harm the communities they're designed to protect, this research fundamentally challenges assumptions about fairness in contemporary language technologies. The distinction between guardrails (restrictive) and guidance (constructive) suggests that future approaches must shift from preventing harm to actively supporting equitable outcomes for marginalized linguistic communities.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://aclanthology.org/2025.queerinai-main.2.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/CWBWUWYL)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

