---
title: "Vethman 2025 Fairness"
original_document: Vethman_2025_Fairness.md
document_type: Conference Paper
research_domain: AI Ethics
methodology: Mixed Methods
keywords: Intersectionality, AI Fairness, Algorithmic Bias, Community Engagement, Structural Inequality
mini_abstract: "This paper critiques narrow technical approaches to AI fairness and proposes five actionable recommendations for AI experts to operationalize intersectionality, addressing interconnected discrimination through interdisciplinary collaboration and community co-ownership rather than algorithmic metrics alone."
target_audience: Researchers, Practitioners, Policymakers
key_contributions: "Bridging intersectionality theory with practical AI development guidance"
geographic_focus: Europe
publication_year: 2025
related_fields: Black Feminist Theory, Science and Technology Studies, Social Justice
summary_date: 2025-11-07
language: English
ai_model: claude-haiku-4-5
---

# Summary: Vethman 2025 Fairness

## Overview

This 2025 FAccT conference paper addresses a critical gap in AI fairness research: the reduction of intersectionality—a theoretical framework from Black Feminist scholarship examining interconnected systems of oppression—to narrow algorithmic bias metrics. The authors argue that current AI fairness approaches focus exclusively on technical solutions (bias detection, fairness metrics) while sidelining essential intersectional dimensions: power relations, social justice, and structural inequality. Real-world cases including the 2019 Dutch childcare benefits scandal (disproportionately targeting 26,000 families with dual nationality) and France's automated welfare fraud detection system (flagging low-income and immigrant populations) demonstrate how algorithmic systems compound historical discrimination against marginalized communities. The paper proposes that AI experts, occupying central roles in system development, bear responsibility for limiting unjust outcomes through genuinely intersectional approaches that extend beyond technical optimization.

## Main Findings

The study identifies five actionable themes for implementing intersectional AI practices:

1. **Mandate interdisciplinary collaboration** incorporating expertise from social sciences, humanities, and affected communities
2. **Embed reflexivity and recognize positionality** to acknowledge researchers' social positions and inherent biases
3. **Approach communities as co-owners** in design and deployment, not passive research subjects
4. **Engage with power dynamics and social context** to understand historical discrimination and structural inequality
5. **Critically assess data framing and metric limitations** to prevent bias perpetuation through measurement systems

Significant implementation barriers emerged: tech-optimism (belief that technical solutions suffice for social problems) and experts' fear of insufficient knowledge in social justice domains. Conversely, participating AI experts responded positively, valuing recommendations as practical tools for communicating intersectionality's importance and catalyzing institutional change toward more just AI practices.

## Methodology/Approach

The authors employed participatory mixed-methods research combining systematic thematic analysis of AI fairness literature with community engagement involving AI experts. This approach grounds abstract theoretical critique in institutional practice contexts. The theoretical framework explicitly draws from Black Feminist intersectionality scholarship, emphasizing how multiple identity categories (race, gender, class, disability) interact to create compounded discrimination, rather than treating demographic categories as isolated variables. Critically, the literature analysis was evaluated through expert participation, ensuring recommendations reflected both theoretical rigor and practical feasibility. This participatory dimension distinguishes the work from purely theoretical critique.

## Relevant Concepts

**Intersectionality:** Theoretical framework examining how multiple identity categories interact to create compounded forms of discrimination and privilege; originated in Black Feminist scholarship to address interconnected systems of oppression.

**Algorithmic frame:** The dominant approach in AI fairness research that reduces intersectionality to technical problems addressable through bias metrics and algorithmic adjustments, sidelining social justice and power dynamics.

**Algorithmic bias:** Systematic errors in AI systems disproportionately harming specific demographic groups, often reflecting historical discrimination embedded in training data and design choices.

**Positionality:** Recognition that researchers and practitioners occupy specific social positions shaped by identity, experience, and power relations, influencing their perspectives, decisions, and blind spots.

**Tech-optimism:** Belief that technological solutions can resolve complex social problems without addressing underlying structural inequalities and power imbalances.

**Co-ownership:** Collaborative models where affected communities participate as equal partners in decision-making, design, and governance rather than serving as research subjects or passive stakeholders.

**Structural inequality:** Systemic disadvantages embedded in institutions, policies, and practices that perpetuate discrimination across generations and demographic groups.

## Significance

This work makes substantial theoretical and practical contributions to responsible AI discourse by fundamentally challenging the dominant technical paradigm in AI fairness research. Rather than proposing new algorithms or metrics, it advocates for epistemological and structural transformation in how fairness is conceptualized and practiced. The paper bridges computer science and social justice scholarship, demonstrating that technical expertise alone cannot address discrimination rooted in historical and structural inequality. By grounding recommendations in expert feedback and community engagement, the authors translate critical theory into actionable institutional practice—essential for meaningful implementation beyond academic discourse. The work explicitly positions itself against "algorithmic solutionism," arguing that genuine fairness requires interdisciplinary collaboration, reflexivity, power analysis, and community co-ownership. This represents a significant contribution to FAccT scholarship and broader conversations about responsible AI development, offering concrete guidance for practitioners while maintaining theoretical rigor grounded in social justice frameworks.
