# SozArb Research Vault - Complete Export

**Generated:** 2025-11-20 10:33:23

**Contents:** 266 Papers, 144 Concepts, 14 MOCs

---

# Table of Contents

## Maps of Content (MOCs)
- [Master Map of Content](#master_moc)
- [Dimension: AI Literacy & Competencies](#dimension_ai_literacy)
- [Dimension: Bias & Discrimination Analysis](#dimension_bias_analysis)
- [Dimension: Practical Implementation](#dimension_practical_implementation)
- [Dimension: Professional/Social Work Context](#dimension_professional_context)
- [Dimension: Vulnerable Groups & Digital Equity](#dimension_vulnerable_groups)
- [üìï Exclude Papers](#papers_exclude)
- [‚≠ê‚≠ê‚≠ê High Relevance Papers](#papers_high_relevance)
- [üìó Include Papers](#papers_include)
- [‚≠ê Low Relevance Papers](#papers_low_relevance)
- [‚≠ê‚≠ê Medium Relevance Papers](#papers_medium_relevance)
- [üìô Unclear Papers](#papers_unclear)
- [üìù Papers with AI Summaries](#papers_with_summaries)
- [üèÜ Top 20 Papers by Total Relevance](#top_papers)

## Papers (266)
- [Navigating the Nexus of Trust: Prompt Engineering, Professional Judgment, and the Integration of Large Language Models in Social Work](#[author_not_specified]_2025_navigating_the_nexus_of_trust__prompt_engineering,)
- [Incubating Feminist AI: Executive Summary 2021-2024](#a+_alliance_2024_incubating_feminist_ai__executive_summary_2021-202)
- [Feminist Perspectives on AI: Ethical Considerations in Algorithmic Decision-Making](#ahmed_2024_feminist_perspectives_on_ai__ethical_consideration)
- [Artificial Intelligence (AI) Literacy for Social Work: Implications for Core Competencies](#ahn_2025_artificial_intelligence_(ai)_literacy_for_social_w)
- [AI FORA ‚Äì Artificial Intelligence for Assessment: Fairness bei der Verteilung √∂ffentlicher sozialer Leistungen](#ahrweiler_2025_ai_fora_‚Äì_artificial_intelligence_for_assessment__)
- [Social work in the age of artificial intelligence: A rights-based framework for evidence-based practice through social psychology, group dynamics, and institutional analysis](#alam_2025_social_work_in_the_age_of_artificial_intelligence_)
- [Policy advice and best practices on bias and fairness in AI](#alvarez_2024_policy_advice_and_best_practices_on_bias_and_fairn)
- [Coded injustice: Surveillance and discrimination in Denmark's automated welfare state](#amnesty_international_2024_coded_injustice__surveillance_and_discrimination_i)
- [Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation](#an_2025_measuring_gender_and_racial_biases_in_large_langua)
- [Digital literacy as a new determinant of health: A scoping review](#arias_l√≥pez_2023_digital_literacy_as_a_new_determinant_of_health__a)
- [How to Create Inclusive AI Images: A Guide to Bias-Free Prompting](#articulate_2025_how_to_create_inclusive_ai_images__a_guide_to_bias)
- [Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review](#asseri_2024_prompt_engineering_techniques_for_mitigating_cultu)
- [Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review](#asseri_2025_prompt_engineering_techniques_for_mitigating_cultu)
- [AI Countergovernance: Lessons Learned from Canada and Paris](#attard-frost_2025_ai_countergovernance__lessons_learned_from_canada_)
- [Explicitly unbiased large language models still form biased associations](#bai_2025_explicitly_unbiased_large_language_models_still_fo)
- [Artificial intelligence in social work: An EPIC model for practice](#baker_2025_artificial_intelligence_in_social_work__an_epic_mo)
- [Beyond transparency and explainability: On the need for adequate and contextualized user guidelines for LLM use](#barman_2024_beyond_transparency_and_explainability__on_the_nee)
- [Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review](#basseri_2025_prompt_engineering_techniques_for_mitigating_cultu)
- [Keynote Summary: The New Jim Code: Reimagining the Default Settings of Technology & Society](#benjamin_2023_keynote_summary__the_new_jim_code__reimagining_the)
- [The AI literacy development canvas: Assessing and building AI literacy in organizations](#benlian_2025_the_ai_literacy_development_canvas__assessing_and_)
- [Less knowledge, more trust? Exploring potentially uncritical attitudes towards AI in higher education](#biagini_2024_less_knowledge,_more_trust__exploring_potentially_)
- [Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden f√ºr KI in der Verwaltung, Version 1.0](#biegelbauer_2023_leitfaden_digitale_verwaltung_und_ethik__praxislei)
- [Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development](#birru_2024_mitigating_age-related_bias_in_large_language_mode)
- [A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization](#bisconti_2024_a_formal_account_of_ai_trustworthiness__connecting)
- [Artificial Intelligence in Social Work: An EPIC Model for Practice](#boetto_2025_artificial_intelligence_in_social_work__an_epic_mo)
- [Generative AI & social work practice guidance](#british_association_of_social_workers_2025_generative_ai_&_social_work_practice_guidance)
- [Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligent Machines](#browne_2023_feminist_ai__critical_perspectives_on_algorithms,_)
- [Engineers on responsibility: feminist approaches to who's responsible for ethical AI](#browne_2024_engineers_on_responsibility__feminist_approaches_t)
- [Tech workers' perspectives on ethical issues in AI development: Foregrounding feminist approaches](#browne_2024_tech_workers'_perspectives_on_ethical_issues_in_ai)
- [AI literacy in K-12: a systematic literature review](#casal-otero_2023_ai_literacy_in_k-12__a_systematic_literature_revie)
- [Flexible intersectional stereotype extraction (FISE): Analyzing intersectional biases in large language models](#charlesworth_2024_flexible_intersectional_stereotype_extraction_(fis)
- [How People Use ChatGPT](#chatterji_2025_how_people_use_chatgpt)
- [A Competency Framework for AI Literacy: Variations by Different Learner Groups and an Implied Learning Pathway](#chee_2025_a_competency_framework_for_ai_literacy__variations)
- [Exploring complex mental health symptoms via classifying social media data with explainable LLMs](#chen_2024_exploring_complex_mental_health_symptoms_via_class)
- [Social work and artificial intelligence: Collaboration and challenges](#chen_2025_social_work_and_artificial_intelligence__collabora)
- [How child welfare workers reduce racial disparities in algorithmic decisions](#cheng_2022_how_child_welfare_workers_reduce_racial_disparitie)
- [Exploring machine learning to support decision-making for placement stabilization and preservation in child welfare](#cher_2024_exploring_machine_learning_to_support_decision-mak)
- [Prompting fairness: Learning prompts for debiasing large language models](#chisca_2024_prompting_fairness__learning_prompts_for_debiasing)
- [Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes](#chisca_2024_prompting_techniques_for_reducing_social_bias_in_l)
- [What are artificial intelligence literacy and competency? A comprehensive framework to support them](#chiu_2024_what_are_artificial_intelligence_literacy_and_comp)
- [AI literacy and competency: definitions, frameworks, development and future research directions](#chiu_2025_ai_literacy_and_competency__definitions,_framework)
- [Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals](#choudhury_2024_large_language_models_and_user_trust__consequence_)
- [Intersectional Artificial Intelligence Is Essential: Polyvocal, Multimodal, Experimental Methods to Save AI](#ciston_2024_intersectional_artificial_intelligence_is_essentia)
- [PreciseDebias: An automatic prompt engineering approach for generative AI to mitigate image demographic biases](#clemmer_2024_precisedebias__an_automatic_prompt_engineering_app)
- [The influence of mental state attributions on trust in large language models](#colombatto_2025_the_influence_of_mental_state_attributions_on_trus)
- [Clinical Social Workers‚Äô Perceptions of Large Language Models in Practice: Resistance to Automation and Prospects for Integration](#creswell_b√°ez_2025_clinical_social_workers‚Äô_perceptions_of_large_lang)
- [Queer in AI: A case study in community-led participatory AI](#cvoelcker_2023_queer_in_ai__a_case_study_in_community-led_partici)
- [Data Feminism for AI](#d'ignazio_2024_data_feminism_for_ai)
- [Measuring and identifying factors of individuals' trust in large language models](#de_duro_2025_measuring_and_identifying_factors_of_individuals'_)
- [Can LLMs reason about trust? A pilot study](#debnath_2024_can_llms_reason_about_trust__a_pilot_study)
- ... and 216 more papers

## Concepts (144)
- [Accountability Asymmetry](#accountability_asymmetry)
- [Ai Accountability](#ai_accountability)
- [Ai Act](#ai_act)
- [Ai Bias Mitigation](#ai_bias_mitigation)
- [AI Bias (redefined)](#ai_bias_redefined)
- [Ai Ethics](#ai_ethics)
- [Ai Governance](#ai_governance)
- [AI Safety](#ai_safety)
- [Ai Transparency](#ai_transparency)
- [AI Workforce Diversity](#ai_workforce_diversity)
- [Algorithmic bias](#algorithmic_bias)
- [Algorithmic Decision-Making](#algorithmic_decision-making)
- [Algorithmic discrimination](#algorithmic_discrimination)
- [Algorithmic Fairness](#algorithmic_fairness)
- [Algorithmic frame](#algorithmic_frame)
- [Algorithmic Opacity](#algorithmic_opacity)
- [Algorithmic Reparation](#algorithmic_reparation)
- [Artificial Intelligence](#artificial_intelligence)
- [Automated Decision-Making](#automated_decision_making)
- [Batch Inference](#batch_inference)
- [Bi-modal Priors](#bi_modal_priors)
- [Bias amplification](#bias_amplification)
- [Bias Mitigation](#bias_mitigation)
- [Black Box Effect](#black_box_effect)
- [Chain-of-Thought (CoT)](#chain_of_thought_cot)
- [Chain-of-Thought (CoT) Prompting](#chain_of_thought_cot_prompting)
- [Client Confidentiality](#client_confidentiality)
- [Co-ownership](#co_ownership)
- [Colonial Epistemology](#colonial_epistemology)
- [Coloniality of power](#coloniality_of_power)
- [Community Co-Design](#community_co-design)
- [Compositionality failure](#compositionality_failure)
- [Consent Principle](#consent_principle)
- [Context Vector](#context_vector)
- [Contextual Filtering](#contextual_filtering)
- [Critical-Pragmatic Positioning](#critical_pragmatic_positioning)
- [Cultural associations](#cultural_associations)
- [Current Harms](#current_harms)
- [Data Justice](#data_justice)
- [Data monocultures](#data_monocultures)
- [Data recycling](#data_recycling)
- [Decision-making Transparency](#decision_making_transparency)
- [Deliberate representation](#deliberate_representation)
- [Digital Literacy](#digital_literacy)
- [Digital Twin Models](#digital_twin_models)
- [Direct Debiasing](#direct_debiasing)
- [Distributed representations](#distributed_representations)
- [Distributed Responsibility](#distributed_responsibility)
- [Diversity In Ai](#diversity_in_ai)
- [Dual-Component Framework](#dual_component_framework)
- ... and 94 more concepts

---

# Maps of Content (MOCs)

<a id='master_moc'></a>

## Master Map of Content

---
title: "Master MOC - SozArb Literature Research"
type: master-moc
tags: [moc, master, navigation]
generated: 2025-11-10 07:10
---

# Master MOC - SozArb Literature Research

Complete navigation for AI Literacy in Social Work research corpus.

## Research Question

**How can social workers develop AI literacy to serve vulnerable populations ethically and effectively, particularly in addressing bias and discrimination in AI systems?**

---

## Vault Statistics

| Metric | Count | Percentage |
|--------|-------|------------|
| **Total Papers** | 325 | 100% |
| Papers with AI Summaries | 67 | 20.6% |
| Concept Pages | 40 | ‚Äî |
| | | |
| **PRISMA Decisions** | | |
| ‚úÖ Include | 222 | 68.3% |
| ‚ùå Exclude | 83 | 25.5% |
| ‚ùì Unclear | 20 | 6.2% |
| | | |
| **Relevance Categories** | | |
| ‚≠ê‚≠ê‚≠ê High (‚â•10) | 95 | 29.2% |
| ‚≠ê‚≠ê Medium (5-9) | 142 | 43.7% |
| ‚≠ê Low (<5) | 88 | 27.1% |

---

## Navigation

### By Assessment Decision

- üìó Included Papers (222)
- üìï Excluded Papers (83)
- üìô Unclear Papers (20)

### By Relevance Dimension

- ü§ñ AI Literacy & Competencies
- üõ°Ô∏è Vulnerable Groups & Digital Equity
- ‚öñÔ∏è Bias & Discrimination Analysis
- üîß Practical Implementation
- üë• Professional/Social Work Context

### By Relevance Level

- ‚≠ê‚≠ê‚≠ê High Relevance Papers (95)
- ‚≠ê‚≠ê Medium Relevance Papers (142)
- ‚≠ê Low Relevance Papers (88)

### Special Collections

- üìù Papers with AI Summaries (67)
- üèÜ Top 20 Papers by Total Relevance

### By Concept

- üîñ Concept Index (40 concepts)
- Key concepts:
  - Algorithmic Fairness (14 papers)
  - Intersectionality (12 papers)
  - Algorithmic Bias (12 papers)
  - Responsible AI (11 papers)
  - Large Language Models (10 papers)
  - Generative AI (9 papers)

---

## Quick Searches

Use these in Obsidian search:

- `tag:#include` - All included papers
- `tag:#high-relevance` - High relevance papers
- `tag:#has-summary` - Papers with AI summaries
- `tag:#dim-bias-high` - Papers with high bias dimension score
- `tag:#dim-vulnerable-high` - Papers focused on vulnerable groups

---

## Dataview Queries

### Top 10 Papers by Total Relevance

\`\`\`dataview
TABLE author_year, title, total_relevance, top_dimensions
FROM "Papers"
WHERE decision = "Include"
SORT total_relevance DESC
LIMIT 10
\`\`\`

### Papers by Dimension Score

\`\`\`dataview
TABLE author_year, rel_bias, rel_vulnerable, total_relevance
FROM "Papers"
WHERE rel_bias >= 2 OR rel_vulnerable >= 2
SORT total_relevance DESC
\`\`\`

---

## Your Research Workspace

- üìì Your Research Notes
- üí° Key Insights
- ‚ùì Open Questions
- üîç Identified Gaps

---

*Vault generated: 2025-11-10 07:10*
*Last updated: 2025-11-10*
*Scripts: `generate_research_vault_with_assessment.py`, `sync_summary_metadata.py`, `extract_concepts_from_summaries.py`*
*Total files: 325 papers + 73 summaries + 40 concepts + 13 MOCs*

---

<a id='dimension_ai_literacy'></a>

## Dimension: AI Literacy & Competencies

---
title: "Dimension: AI Literacy & Competencies"
type: dimension-moc
dimension: AI_Literacy
date_created: 2025-11-10
tags: [moc, dimension]
---

# Dimension: AI Literacy & Competencies

Research papers organized by relevance score for this dimension.

## Statistics

| Score | Count | Percentage |
|-------|-------|------------|
| ‚≠ê‚≠ê‚≠ê High (3) | 17 | 5.2% |
| ‚≠ê‚≠ê Medium (2) | 46 | 14.2% |
| ‚≠ê Low (1) | 151 | 46.5% |
| ‚Äî None (0) | 111 | 34.2% |

Average score: 0.90

---


## ‚≠ê‚≠ê‚≠ê High (Score 3)

- McDonald (2023): Algorithmic decision-making in social work practice and peda... (Total: 12/15)
- Ahn (2025): Artificial Intelligence (AI) literacy for social work: Impli... (Total: 12/15)
- Linnemann (2025): K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen f√º... (Total: 12/15)
- Ahn (2025): Artificial Intelligence (AI) Literacy for Social Work: Impli... (Total: 12/15)
- Rodriguez (2024): Introducing Generative Artificial Intelligence into the MSW ... (Total: 12/15)
- Lanzetta (2024): Artificial Intelligence Competence Needs for Youth Workers... (Total: 11/15)
- Ahn (2025): Artificial Intelligence (AI) literacy for social work... (Total: 11/15)
- Kubes (2024): Feministische KI ‚Äì K√ºnstliche Intelligenz f√ºr alle? [Feminis... (Total: 11/15)
- Engelhardt (2025): Voll (dia)logisch? Ein Werkstattbericht √ºber den Einsatz von... (Total: 10/15)
- Hauck (2025): A framework for the learning and teaching of Critical AI Lit... (Total: 10/15)
- Strau√ü (2024): CAIL ‚Äì Critical AI Literacy: Kritische Technikkompetenz f√ºr ... (Total: 10/15)
- Djeffal (2025): Reflexive prompt engineering: A framework for responsible pr... (Total: 10/15)
- Biegelbauer (2023): Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden f√ºr... (Total: 9/15)
- Thwaites (2024): Operationalizing positive-constructive pedagogy to artificia... (Total: 9/15)
- Zhao (2025): Thinking like a scientist: Can interactive simulations foste... (Total: 9/15)
- Chee (2025): A Competency Framework for AI Literacy: Variations by Differ... (Total: 7/15)
- Biagini (2024): Less knowledge, more trust? Exploring potentially uncritical... (Total: 7/15)

## ‚≠ê‚≠ê Medium (Score 2)

- Baker (2025): Artificial intelligence in social work: An EPIC model for pr... (Total: 12/15)
- Slesinger (2024): Training in Co-Creation as a Methodological Approach to Impr... (Total: 12/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit... (Total: 11/15)
- Unknown (2025): Artificial Intelligence in Social Work: An EPIC Model for Pr... (Total: 11/15)
- James (2023): Algorithmic decision-making in social work practice and peda... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Victor (2023): Recommendations for social work researchers and journal edit... (Total: 11/15)
- [Author not specified] (2025): Navigating the Nexus of Trust: Prompt Engineering, Professio... (Total: 11/15)
- Victor (2023): Recommendations for social work researchers and journal edit... (Total: 11/15)
- Victor (2023): Recommendations for social work researchers and journal edit... (Total: 11/15)
- Shah (2025): Gender bias in artificial intelligence: Empowering women thr... (Total: 11/15)
- DIVERSIFAIR Project (2024): AI & Intersectionality: A Toolkit For Fairness & Inclusion... (Total: 11/15)
- World Economic Forum (2024): AI for impact: The PRISM framework for responsible AI in soc... (Total: 10/15)
- Perron (2023): Recommendations for social work researchers and journal edit... (Total: 10/15)
- Hodgson (2022): Problematising artificial intelligence in social work educat... (Total: 10/15)
- Creswell B√°ez (2025): Clinical Social Workers‚Äô Perceptions of Large Language Model... (Total: 10/15)
- S≈´na (2024): Diskriminierung durch Algorithmen ‚Äì √úberlegungen zur St√§rkun... (Total: 10/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit... (Total: 10/15)
- Sch√∂nauer (2025): Akzeptanz von KI und organisationale Rahmenbedingungen in de... (Total: 10/15)
- Chen (2025): Social work and artificial intelligence: Collaboration and c... (Total: 10/15)
- Quaid-i-Azam University (2025): Gender Bias in Artificial Intelligence: Empowering Women Thr... (Total: 10/15)
- Hartshorne (2025): Generative AI and the Future of Digital Literacy: Opportunit... (Total: 10/15)
- Shah (2025): Gender Bias in Artificial Intelligence: Empowering Women Thr... (Total: 10/15)
- James (2025): Responsible prompting recommendation: Fostering responsible ... (Total: 9/15)
- Singer (2023): AI Creates the Message: Integrating AI Language Learning Mod... (Total: 9/15)
- Sch√∂nauer (2025): Akzeptanz von KI und organisationale Rahmenbedingungen in de... (Total: 9/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit... (Total: 9/15)
- Siapka (2023): Towards a Feminist Metaethics of AI... (Total: 9/15)
- Attard-Frost (2025): AI Countergovernance: Lessons Learned from Canada and Paris... (Total: 9/15)
- Garkisch (2024): Considering a unified model of artificial intelligence enhan... (Total: 8/15)
- Barman (2024): Beyond transparency and explainability: On the need for adeq... (Total: 8/15)
- Colombatto (2025): The influence of mental state attributions on trust in large... (Total: 8/15)
- Colombatto (2025): The influence of mental state attributions on trust in large... (Total: 8/15)
- Toupin (2024): Shaping feminist artificial intelligence... (Total: 8/15)
- Toupin (2024): Shaping feminist artificial intelligence... (Total: 8/15)
- N√§scher (2025): ReflectAI: Design and evaluation of an AI coach to support p... (Total: 7/15)
- Goellner (2025): Towards responsible AI for education: Hybrid human-AI to con... (Total: 7/15)
- Xu (2023): Transparency enhances positive perceptions of social artific... (Total: 7/15)
- Xu (2023): Transparency enhances positive perceptions of social artific... (Total: 7/15)
- Debnath (2024): Can LLMs reason about trust? A pilot study... (Total: 6/15)
- Steyvers (2025): What large language models know and what people think they k... (Total: 6/15)
- Debnath (2024): Can LLMs reason about trust? A pilot study... (Total: 6/15)
- Steyvers (2025): What large language models know and what people think they k... (Total: 6/15)
- Debnath (2024): Can LLMs reason about trust? A pilot study... (Total: 6/15)
- Small (2023): Generative AI and opportunities for feminist classroom assig... (Total: 6/15)

## ‚≠ê Low (Score 1)

- Cheng (2022): How child welfare workers reduce racial disparities in algor... (Total: 13/15)
- Hall (2024): A systematic review of sophisticated predictive and prescrip... (Total: 12/15)
- Kawakami (2022): Improving human-AI partnerships in child welfare: Understand... (Total: 12/15)
- Yu (2025): Algorithmic-Assisted Decision-Making Tools in Child Welfare ... (Total: 12/15)
- Nuwasiima (2024): The role of artificial intelligence (AI) and machine learnin... (Total: 12/15)
- Takaoka (2022): AI implementation science for social issues: Pitfalls and ti... (Total: 11/15)
- Moreau (2024): Failing our youngest: On the biases, pitfalls, and risks in ... (Total: 11/15)
- Feist-Ortmanns (2025): KI-basiertes Assistenzsystem im Kinderschutzverfahren... (Total: 11/15)
- Alam (2025): Social work in the age of artificial intelligence: A rights-... (Total: 11/15)
- Ahrweiler (2025): AI FORA ‚Äì Artificial Intelligence for Assessment: Fairness b... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Marjanovic (2022): Theorising algorithmic justice... (Total: 10/15)
- Li (2023): Ethics & AI: A systematic review on ethical concerns and rel... (Total: 10/15)
- Dencik (2024): Automated government benefits and welfare surveillance... (Total: 10/15)
- Amnesty International (2024): Coded injustice: Surveillance and discrimination in Denmark'... (Total: 10/15)
- van Toorn (2024): Introduction to the digital welfare state: Contestations, co... (Total: 10/15)
- Field (2023): Examining risks of racial biases in NLP tools for child prot... (Total: 10/15)
- British Association of Social Workers (2025): Generative AI & social work practice guidance... (Total: 10/15)
- Schneider (2022): Exploring opportunities and risks in decision support techno... (Total: 10/15)
- Schneider (2024): Das verflixte Problem mit Klassifikationen: Zum Einfluss der... (Total: 10/15)
- Steiner (2022): K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen, E... (Total: 10/15)
- Patton (2023): ChatGPT for Social Work Science: Ethical Challenges and Oppo... (Total: 10/15)
- Reamer (2023): Artificial Intelligence in Social Work: Emerging Ethical Iss... (Total: 10/15)
- Boetto (2025): Artificial Intelligence in Social Work: An EPIC Model for Pr... (Total: 10/15)
- Studeny (2025): Digitale Werkzeuge und Machtasymmetrien?... (Total: 10/15)
- Gravelmann (2024): K√ºnstliche Intelligenz in der Sozialen Arbeit ‚Äì Zwischen Bed... (Total: 10/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs throug... (Total: 10/15)
- Lau (2023): Dipper: Diversity in Prompts for Producing Large Language Mo... (Total: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for pr... (Total: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for pr... (Total: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for pr... (Total: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Salinas (2025): What‚Äôs in a name? Auditing large language models for race an... (Total: 10/15)
- Asseri (2025): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs throug... (Total: 10/15)
- Qiu (2025): DR.GAP: Mitigating bias in large language models using gende... (Total: 10/15)
- Cvoelcker (2023): Queer in AI: A case study in community-led participatory AI... (Total: 10/15)
- Basseri (2025): Prompt Engineering Techniques for Mitigating Cultural Bias A... (Total: 10/15)
- Ovalle (2024): Towards Substantive Equality in Artificial Intelligence: Tra... (Total: 10/15)
- Giannoni Adielsson (2024): The AI Act, gender equality and non-discrimination: what rol... (Total: 10/15)
- Wudel (2025): What is Feminist AI?... (Total: 10/15)
- Ricaurte Quijano (2024): Towards substantive equality in artificial intelligence: Tra... (Total: 10/15)
- Wudel (2025): What is Feminist AI?... (Total: 10/15)
- Vethman (2025): Fairness Beyond the Algorithmic Frame: Actionable Recommenda... (Total: 10/15)
- Raji (2024): The Algorithmic Auditing Landscape: A Social Justice Approac... (Total: 10/15)
- Ciston (2024): Intersectional Artificial Intelligence Is Essential: Polyvoc... (Total: 10/15)

---

<a id='dimension_bias_analysis'></a>

## Dimension: Bias & Discrimination Analysis

---
title: "Dimension: Bias & Discrimination Analysis"
type: dimension-moc
dimension: Bias_Analysis
date_created: 2025-11-10
tags: [moc, dimension]
---

# Dimension: Bias & Discrimination Analysis

Research papers organized by relevance score for this dimension.

## Statistics

| Score | Count | Percentage |
|-------|-------|------------|
| ‚≠ê‚≠ê‚≠ê High (3) | 133 | 40.9% |
| ‚≠ê‚≠ê Medium (2) | 71 | 21.8% |
| ‚≠ê Low (1) | 29 | 8.9% |
| ‚Äî None (0) | 92 | 28.3% |

Average score: 1.75

---


## ‚≠ê‚≠ê‚≠ê High (Score 3)

- Cheng (2022): How child welfare workers reduce racial disparities in algor... (Total: 13/15)
- Hall (2024): A systematic review of sophisticated predictive and prescrip... (Total: 12/15)
- Kawakami (2022): Improving human-AI partnerships in child welfare: Understand... (Total: 12/15)
- Yu (2025): Algorithmic-Assisted Decision-Making Tools in Child Welfare ... (Total: 12/15)
- Nuwasiima (2024): The role of artificial intelligence (AI) and machine learnin... (Total: 12/15)
- Slesinger (2024): Training in Co-Creation as a Methodological Approach to Impr... (Total: 12/15)
- Moreau (2024): Failing our youngest: On the biases, pitfalls, and risks in ... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Victor (2023): Recommendations for social work researchers and journal edit... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- [Author not specified] (2025): Navigating the Nexus of Trust: Prompt Engineering, Professio... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Victor (2023): Recommendations for social work researchers and journal edit... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Victor (2023): Recommendations for social work researchers and journal edit... (Total: 11/15)
- Shah (2025): Gender bias in artificial intelligence: Empowering women thr... (Total: 11/15)
- DIVERSIFAIR Project (2024): AI & Intersectionality: A Toolkit For Fairness & Inclusion... (Total: 11/15)
- Marjanovic (2022): Theorising algorithmic justice... (Total: 10/15)
- Li (2023): Ethics & AI: A systematic review on ethical concerns and rel... (Total: 10/15)
- Dencik (2024): Automated government benefits and welfare surveillance... (Total: 10/15)
- Amnesty International (2024): Coded injustice: Surveillance and discrimination in Denmark'... (Total: 10/15)
- van Toorn (2024): Introduction to the digital welfare state: Contestations, co... (Total: 10/15)
- Field (2023): Examining risks of racial biases in NLP tools for child prot... (Total: 10/15)
- Schneider (2024): Das verflixte Problem mit Klassifikationen: Zum Einfluss der... (Total: 10/15)
- Steiner (2022): K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen, E... (Total: 10/15)
- S≈´na (2024): Diskriminierung durch Algorithmen ‚Äì √úberlegungen zur St√§rkun... (Total: 10/15)
- Studeny (2025): Digitale Werkzeuge und Machtasymmetrien?... (Total: 10/15)
- Gravelmann (2024): K√ºnstliche Intelligenz in der Sozialen Arbeit ‚Äì Zwischen Bed... (Total: 10/15)
- Strau√ü (2024): CAIL ‚Äì Critical AI Literacy: Kritische Technikkompetenz f√ºr ... (Total: 10/15)
- Quaid-i-Azam University (2025): Gender Bias in Artificial Intelligence: Empowering Women Thr... (Total: 10/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs throug... (Total: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Salinas (2025): What‚Äôs in a name? Auditing large language models for race an... (Total: 10/15)
- Asseri (2025): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs throug... (Total: 10/15)
- Qiu (2025): DR.GAP: Mitigating bias in large language models using gende... (Total: 10/15)
- Cvoelcker (2023): Queer in AI: A case study in community-led participatory AI... (Total: 10/15)
- Basseri (2025): Prompt Engineering Techniques for Mitigating Cultural Bias A... (Total: 10/15)
- Ovalle (2024): Towards Substantive Equality in Artificial Intelligence: Tra... (Total: 10/15)
- Giannoni Adielsson (2024): The AI Act, gender equality and non-discrimination: what rol... (Total: 10/15)
- Wudel (2025): What is Feminist AI?... (Total: 10/15)
- Ricaurte Quijano (2024): Towards substantive equality in artificial intelligence: Tra... (Total: 10/15)
- Wudel (2025): What is Feminist AI?... (Total: 10/15)
- Vethman (2025): Fairness Beyond the Algorithmic Frame: Actionable Recommenda... (Total: 10/15)
- Raji (2024): The Algorithmic Auditing Landscape: A Social Justice Approac... (Total: 10/15)
- Ciston (2024): Intersectional Artificial Intelligence Is Essential: Polyvoc... (Total: 10/15)
- Klein (2024): Data Feminism for AI... (Total: 10/15)
- Wudel (2025): What is Feminist AI?... (Total: 10/15)

## ‚≠ê‚≠ê Medium (Score 2)

- McDonald (2023): Algorithmic decision-making in social work practice and peda... (Total: 12/15)
- Ahn (2025): Artificial Intelligence (AI) literacy for social work: Impli... (Total: 12/15)
- Baker (2025): Artificial intelligence in social work: An EPIC model for pr... (Total: 12/15)
- Linnemann (2025): K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen f√º... (Total: 12/15)
- Ahn (2025): Artificial Intelligence (AI) Literacy for Social Work: Impli... (Total: 12/15)
- Rodriguez (2024): Introducing Generative Artificial Intelligence into the MSW ... (Total: 12/15)
- Takaoka (2022): AI implementation science for social issues: Pitfalls and ti... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit... (Total: 11/15)
- Unknown (2025): Artificial Intelligence in Social Work: An EPIC Model for Pr... (Total: 11/15)
- James (2023): Algorithmic decision-making in social work practice and peda... (Total: 11/15)
- Feist-Ortmanns (2025): KI-basiertes Assistenzsystem im Kinderschutzverfahren... (Total: 11/15)
- Ahn (2025): Artificial Intelligence (AI) literacy for social work... (Total: 11/15)
- Alam (2025): Social work in the age of artificial intelligence: A rights-... (Total: 11/15)
- Ahrweiler (2025): AI FORA ‚Äì Artificial Intelligence for Assessment: Fairness b... (Total: 11/15)
- Kubes (2024): Feministische KI ‚Äì K√ºnstliche Intelligenz f√ºr alle? [Feminis... (Total: 11/15)
- Spaulding (2023): Predicting successful placements for youth in child welfare ... (Total: 10/15)
- Cher (2024): Exploring machine learning to support decision-making for pl... (Total: 10/15)
- British Association of Social Workers (2025): Generative AI & social work practice guidance... (Total: 10/15)
- Perron (2023): Recommendations for social work researchers and journal edit... (Total: 10/15)
- Hodgson (2022): Problematising artificial intelligence in social work educat... (Total: 10/15)
- Schneider (2022): Exploring opportunities and risks in decision support techno... (Total: 10/15)
- Creswell B√°ez (2025): Clinical Social Workers‚Äô Perceptions of Large Language Model... (Total: 10/15)
- Patton (2023): ChatGPT for Social Work Science: Ethical Challenges and Oppo... (Total: 10/15)
- Reamer (2023): Artificial Intelligence in Social Work: Emerging Ethical Iss... (Total: 10/15)
- Boetto (2025): Artificial Intelligence in Social Work: An EPIC Model for Pr... (Total: 10/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit... (Total: 10/15)
- Hauck (2025): A framework for the learning and teaching of Critical AI Lit... (Total: 10/15)
- Chen (2025): Social work and artificial intelligence: Collaboration and c... (Total: 10/15)
- Lau (2023): Dipper: Diversity in Prompts for Producing Large Language Mo... (Total: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for pr... (Total: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for pr... (Total: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for pr... (Total: 10/15)
- Djeffal (2025): Reflexive prompt engineering: A framework for responsible pr... (Total: 10/15)
- Hartshorne (2025): Generative AI and the Future of Digital Literacy: Opportunit... (Total: 10/15)
- A+ Alliance (2024): Incubating Feminist AI: Executive Summary 2021-2024... (Total: 10/15)
- Rodr√≠guez-Mart√≠nez (2024): Ethical issues related to the use of technology in social wo... (Total: 9/15)
- Meilvang (2024): Decision support and algorithmic support: The construction o... (Total: 9/15)
- J√∏rgensen (2023): Data and rights in the digital welfare state: the case of De... (Total: 9/15)
- James (2025): Responsible prompting recommendation: Fostering responsible ... (Total: 9/15)
- Kutscher (2024): Digitalit√§t und Digitalisierung als Gegenstand der Sozialen ... (Total: 9/15)
- Unknown (2024): RETHINKING SOCIAL SERVICES WITH ARTIFICIAL INTELLIGENCE: OPP... (Total: 9/15)
- Singer (2023): AI Creates the Message: Integrating AI Language Learning Mod... (Total: 9/15)
- Gravelmann (2024): K√ºnstliche Intelligenz in der Sozialen Arbeit ‚Äì Zwischen Bed... (Total: 9/15)
- Biegelbauer (2023): Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden f√ºr... (Total: 9/15)
- UNESCO (2024): Women4Ethical AI: Global cooperation for gender-inclusive AI... (Total: 9/15)
- Ricaurte (2024): How can feminism inform AI governance in practice?... (Total: 9/15)
- Ghosal (2024): An empirical study of structural social and ethical challeng... (Total: 9/15)
- Attard-Frost (2025): AI Countergovernance: Lessons Learned from Canada and Paris... (Total: 9/15)
- Thwaites (2024): Operationalizing positive-constructive pedagogy to artificia... (Total: 9/15)

## ‚≠ê Low (Score 1)

- Lanzetta (2024): Artificial Intelligence Competence Needs for Youth Workers... (Total: 11/15)
- World Economic Forum (2024): AI for impact: The PRISM framework for responsible AI in soc... (Total: 10/15)
- Engelhardt (2025): Voll (dia)logisch? Ein Werkstattbericht √ºber den Einsatz von... (Total: 10/15)
- Sch√∂nauer (2025): Akzeptanz von KI und organisationale Rahmenbedingungen in de... (Total: 10/15)
- Heinz (2025): Clinical trial of an LLM-based conversational AI psychothera... (Total: 9/15)
- National Association of Social Workers (2017): NASW, ASWB, CSWE, & CSWA standards for technology in social ... (Total: 9/15)
- Sch√∂nauer (2025): Akzeptanz von KI und organisationale Rahmenbedingungen in de... (Total: 9/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit... (Total: 9/15)
- Sabour (2023): A chatbot for mental health support: Exploring the impact of... (Total: 8/15)
- Siddals (2024): "It happened to be the perfect thing": Experiences of genera... (Total: 8/15)
- Garkisch (2024): Considering a unified model of artificial intelligence enhan... (Total: 8/15)
- Jarke (2025): Datafied ageing futures: Regimes of anticipation and partici... (Total: 8/15)
- Kutscher (2020): Handbuch Soziale Arbeit und Digitalisierung... (Total: 8/15)
- Chee (2025): A Competency Framework for AI Literacy: Variations by Differ... (Total: 7/15)
- Biagini (2024): Less knowledge, more trust? Exploring potentially uncritical... (Total: 7/15)
- Schneider (2025): Indecision on the use of artificial intelligence in healthca... (Total: 7/15)
- Goldkind (2023): The End of the World as We Know It? ChatGPT and Social Work... (Total: 7/15)
- Goellner (2025): Towards responsible AI for education: Hybrid human-AI to con... (Total: 7/15)
- Xu (2023): Transparency enhances positive perceptions of social artific... (Total: 7/15)
- Xu (2023): Transparency enhances positive perceptions of social artific... (Total: 7/15)
- Browne (2024): Engineers on responsibility: feminist approaches to who's re... (Total: 7/15)
- Srinivasan (2025): Mitigating trust-induced inappropriate reliance on AI assist... (Total: 6/15)
- Debnath (2024): Can LLMs reason about trust? A pilot study... (Total: 6/15)
- Steyvers (2025): What large language models know and what people think they k... (Total: 6/15)
- Srinivasan (2025): Mitigating trust-induced inappropriate reliance on AI assist... (Total: 6/15)
- Debnath (2024): Can LLMs reason about trust? A pilot study... (Total: 6/15)
- Steyvers (2025): What large language models know and what people think they k... (Total: 6/15)
- Srinivasan (2025): Mitigating trust-induced inappropriate reliance on AI assist... (Total: 6/15)
- Debnath (2024): Can LLMs reason about trust? A pilot study... (Total: 6/15)

---

<a id='dimension_practical_implementation'></a>

## Dimension: Practical Implementation

---
title: "Dimension: Practical Implementation"
type: dimension-moc
dimension: Practical_Implementation
date_created: 2025-11-10
tags: [moc, dimension]
---

# Dimension: Practical Implementation

Research papers organized by relevance score for this dimension.

## Statistics

| Score | Count | Percentage |
|-------|-------|------------|
| ‚≠ê‚≠ê‚≠ê High (3) | 23 | 7.1% |
| ‚≠ê‚≠ê Medium (2) | 126 | 38.8% |
| ‚≠ê Low (1) | 93 | 28.6% |
| ‚Äî None (0) | 83 | 25.5% |

Average score: 1.27

---


## ‚≠ê‚≠ê‚≠ê High (Score 3)

- Cheng (2022): How child welfare workers reduce racial disparities in algor... (Total: 13/15)
- Slesinger (2024): Training in Co-Creation as a Methodological Approach to Impr... (Total: 12/15)
- Takaoka (2022): AI implementation science for social issues: Pitfalls and ti... (Total: 11/15)
- Feist-Ortmanns (2025): KI-basiertes Assistenzsystem im Kinderschutzverfahren... (Total: 11/15)
- Spaulding (2023): Predicting successful placements for youth in child welfare ... (Total: 10/15)
- Cher (2024): Exploring machine learning to support decision-making for pl... (Total: 10/15)
- World Economic Forum (2024): AI for impact: The PRISM framework for responsible AI in soc... (Total: 10/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs throug... (Total: 10/15)
- Lau (2023): Dipper: Diversity in Prompts for Producing Large Language Mo... (Total: 10/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs throug... (Total: 10/15)
- Qiu (2025): DR.GAP: Mitigating bias in large language models using gende... (Total: 10/15)
- A+ Alliance (2024): Incubating Feminist AI: Executive Summary 2021-2024... (Total: 10/15)
- Heinz (2025): Clinical trial of an LLM-based conversational AI psychothera... (Total: 9/15)
- James (2025): Responsible prompting recommendation: Fostering responsible ... (Total: 9/15)
- Zhao (2025): Thinking like a scientist: Can interactive simulations foste... (Total: 9/15)
- Sabour (2023): A chatbot for mental health support: Exploring the impact of... (Total: 8/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs throug... (Total: 8/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs throug... (Total: 8/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs throug... (Total: 8/15)
- N√§scher (2025): ReflectAI: Design and evaluation of an AI coach to support p... (Total: 7/15)
- Srinivasan (2025): Mitigating trust-induced inappropriate reliance on AI assist... (Total: 6/15)
- Srinivasan (2025): Mitigating trust-induced inappropriate reliance on AI assist... (Total: 6/15)
- Srinivasan (2025): Mitigating trust-induced inappropriate reliance on AI assist... (Total: 6/15)

## ‚≠ê‚≠ê Medium (Score 2)

- Hall (2024): A systematic review of sophisticated predictive and prescrip... (Total: 12/15)
- Kawakami (2022): Improving human-AI partnerships in child welfare: Understand... (Total: 12/15)
- McDonald (2023): Algorithmic decision-making in social work practice and peda... (Total: 12/15)
- Ahn (2025): Artificial Intelligence (AI) literacy for social work: Impli... (Total: 12/15)
- Baker (2025): Artificial intelligence in social work: An EPIC model for pr... (Total: 12/15)
- Linnemann (2025): K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen f√º... (Total: 12/15)
- Yu (2025): Algorithmic-Assisted Decision-Making Tools in Child Welfare ... (Total: 12/15)
- Ahn (2025): Artificial Intelligence (AI) Literacy for Social Work: Impli... (Total: 12/15)
- Rodriguez (2024): Introducing Generative Artificial Intelligence into the MSW ... (Total: 12/15)
- Nuwasiima (2024): The role of artificial intelligence (AI) and machine learnin... (Total: 12/15)
- Lanzetta (2024): Artificial Intelligence Competence Needs for Youth Workers... (Total: 11/15)
- Moreau (2024): Failing our youngest: On the biases, pitfalls, and risks in ... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit... (Total: 11/15)
- Unknown (2025): Artificial Intelligence in Social Work: An EPIC Model for Pr... (Total: 11/15)
- James (2023): Algorithmic decision-making in social work practice and peda... (Total: 11/15)
- Alam (2025): Social work in the age of artificial intelligence: A rights-... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Ahrweiler (2025): AI FORA ‚Äì Artificial Intelligence for Assessment: Fairness b... (Total: 11/15)
- Victor (2023): Recommendations for social work researchers and journal edit... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- [Author not specified] (2025): Navigating the Nexus of Trust: Prompt Engineering, Professio... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Victor (2023): Recommendations for social work researchers and journal edit... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Victor (2023): Recommendations for social work researchers and journal edit... (Total: 11/15)
- Kubes (2024): Feministische KI ‚Äì K√ºnstliche Intelligenz f√ºr alle? [Feminis... (Total: 11/15)
- Shah (2025): Gender bias in artificial intelligence: Empowering women thr... (Total: 11/15)
- DIVERSIFAIR Project (2024): AI & Intersectionality: A Toolkit For Fairness & Inclusion... (Total: 11/15)
- Li (2023): Ethics & AI: A systematic review on ethical concerns and rel... (Total: 10/15)
- British Association of Social Workers (2025): Generative AI & social work practice guidance... (Total: 10/15)
- Perron (2023): Recommendations for social work researchers and journal edit... (Total: 10/15)
- Schneider (2022): Exploring opportunities and risks in decision support techno... (Total: 10/15)
- Creswell B√°ez (2025): Clinical Social Workers‚Äô Perceptions of Large Language Model... (Total: 10/15)
- Patton (2023): ChatGPT for Social Work Science: Ethical Challenges and Oppo... (Total: 10/15)
- Reamer (2023): Artificial Intelligence in Social Work: Emerging Ethical Iss... (Total: 10/15)
- Boetto (2025): Artificial Intelligence in Social Work: An EPIC Model for Pr... (Total: 10/15)
- Engelhardt (2025): Voll (dia)logisch? Ein Werkstattbericht √ºber den Einsatz von... (Total: 10/15)
- S≈´na (2024): Diskriminierung durch Algorithmen ‚Äì √úberlegungen zur St√§rkun... (Total: 10/15)
- Sch√∂nauer (2025): Akzeptanz von KI und organisationale Rahmenbedingungen in de... (Total: 10/15)
- Hauck (2025): A framework for the learning and teaching of Critical AI Lit... (Total: 10/15)
- Chen (2025): Social work and artificial intelligence: Collaboration and c... (Total: 10/15)
- Strau√ü (2024): CAIL ‚Äì Critical AI Literacy: Kritische Technikkompetenz f√ºr ... (Total: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for pr... (Total: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for pr... (Total: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for pr... (Total: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Salinas (2025): What‚Äôs in a name? Auditing large language models for race an... (Total: 10/15)

## ‚≠ê Low (Score 1)

- Ahn (2025): Artificial Intelligence (AI) literacy for social work... (Total: 11/15)
- Marjanovic (2022): Theorising algorithmic justice... (Total: 10/15)
- Dencik (2024): Automated government benefits and welfare surveillance... (Total: 10/15)
- Amnesty International (2024): Coded injustice: Surveillance and discrimination in Denmark'... (Total: 10/15)
- van Toorn (2024): Introduction to the digital welfare state: Contestations, co... (Total: 10/15)
- Field (2023): Examining risks of racial biases in NLP tools for child prot... (Total: 10/15)
- Hodgson (2022): Problematising artificial intelligence in social work educat... (Total: 10/15)
- Schneider (2024): Das verflixte Problem mit Klassifikationen: Zum Einfluss der... (Total: 10/15)
- Steiner (2022): K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen, E... (Total: 10/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit... (Total: 10/15)
- Studeny (2025): Digitale Werkzeuge und Machtasymmetrien?... (Total: 10/15)
- Gravelmann (2024): K√ºnstliche Intelligenz in der Sozialen Arbeit ‚Äì Zwischen Bed... (Total: 10/15)
- Quaid-i-Azam University (2025): Gender Bias in Artificial Intelligence: Empowering Women Thr... (Total: 10/15)
- Shah (2025): Gender Bias in Artificial Intelligence: Empowering Women Thr... (Total: 10/15)
- Rodr√≠guez-Mart√≠nez (2024): Ethical issues related to the use of technology in social wo... (Total: 9/15)
- Meilvang (2024): Decision support and algorithmic support: The construction o... (Total: 9/15)
- J√∏rgensen (2023): Data and rights in the digital welfare state: the case of De... (Total: 9/15)
- Keddell (2019): Algorithmic justice in child protection: Statistical fairnes... (Total: 9/15)
- Kutscher (2024): Digitalit√§t und Digitalisierung als Gegenstand der Sozialen ... (Total: 9/15)
- Unknown (2024): RETHINKING SOCIAL SERVICES WITH ARTIFICIAL INTELLIGENCE: OPP... (Total: 9/15)
- Schneider (2018): Der Einfluss der Algorithmen: Neue Qualit√§ten durch Big Data... (Total: 9/15)
- Singer (2023): AI Creates the Message: Integrating AI Language Learning Mod... (Total: 9/15)
- Gravelmann (2024): K√ºnstliche Intelligenz in der Sozialen Arbeit ‚Äì Zwischen Bed... (Total: 9/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit... (Total: 9/15)
- Ghosal (2025): Unequal voices: How LLMs construct constrained queer narrati... (Total: 9/15)
- Tint (2025): Guardrails, not guidance: Understanding responses to LGBTQ+ ... (Total: 9/15)
- Petzel (2025): Prejudiced interactions with large language models (LLMs) re... (Total: 9/15)
- Ahmed (2024): Feminist perspectives on AI: Ethical considerations in algor... (Total: 9/15)
- Charlesworth (2024): Flexible intersectional stereotype extraction (FISE): Analyz... (Total: 9/15)
- UNESCO (2024): Bias against women and girls in large language models: A UNE... (Total: 9/15)
- Ma (2023): Intersectional Stereotypes in Large Language Models: Dataset... (Total: 9/15)
- West (2023): Discriminating Systems: Gender, Race, and Power in AI... (Total: 9/15)
- Kong (2022): Are "Intersectionally Fair" AI Algorithms Really Fair to Wom... (Total: 9/15)
- Ovalle (2023): Factoring the Matrix of Domination: A Critical Review and Re... (Total: 9/15)
- Ulnicane (2024): Intersectionality in artificial intelligence: Framing concer... (Total: 9/15)
- Karagianni (2025): Gender in a stereo-(gender)typical EU AI law: A feminist rea... (Total: 9/15)
- Klein (2024): Data Feminism for AI... (Total: 9/15)
- Siapka (2023): Towards a Feminist Metaethics of AI... (Total: 9/15)
- Ovalle (2023): Factoring the Matrix of Domination: A Critical Review and Re... (Total: 9/15)
- Sharma (2024): Intersectional analysis of visual generative AI: the case of... (Total: 9/15)
- Attard-Frost (2025): AI Countergovernance: Lessons Learned from Canada and Paris... (Total: 9/15)
- Ulnicane (2024): Intersectionality in Artificial Intelligence: Framing Concer... (Total: 9/15)
- Wajcman (2023): Feminism Confronts AI: The Gender Relations of Digitalisatio... (Total: 9/15)
- Benjamin (2023): Keynote Summary: The New Jim Code: Reimagining the Default S... (Total: 9/15)
- Ulnicane (2024): Intersectionality in Artificial Intelligence: Framing Concer... (Total: 9/15)
- D'Ignazio (2024): Data Feminism for AI... (Total: 9/15)
- J√§√§skel√§inen (2025): Intersectional analysis of visual generative AI: The case of... (Total: 9/15)
- Fraile-Rojas (2025): Female perspectives on algorithmic bias: Implications for AI... (Total: 9/15)
- Ahmed (2024): Feminist Perspectives on AI: Ethical Considerations in Algor... (Total: 9/15)
- Gohar (2023): A Survey on Intersectional Fairness in Machine Learning: Opp... (Total: 9/15)

---

<a id='dimension_professional_context'></a>

## Dimension: Professional/Social Work Context

---
title: "Dimension: Professional/Social Work Context"
type: dimension-moc
dimension: Professional_Context
date_created: 2025-11-10
tags: [moc, dimension]
---

# Dimension: Professional/Social Work Context

Research papers organized by relevance score for this dimension.

## Statistics

| Score | Count | Percentage |
|-------|-------|------------|
| ‚≠ê‚≠ê‚≠ê High (3) | 64 | 19.7% |
| ‚≠ê‚≠ê Medium (2) | 20 | 6.2% |
| ‚≠ê Low (1) | 156 | 48.0% |
| ‚Äî None (0) | 85 | 26.2% |

Average score: 1.19

---


## ‚≠ê‚≠ê‚≠ê High (Score 3)

- Cheng (2022): How child welfare workers reduce racial disparities in algor... (Total: 13/15)
- Hall (2024): A systematic review of sophisticated predictive and prescrip... (Total: 12/15)
- Kawakami (2022): Improving human-AI partnerships in child welfare: Understand... (Total: 12/15)
- McDonald (2023): Algorithmic decision-making in social work practice and peda... (Total: 12/15)
- Ahn (2025): Artificial Intelligence (AI) literacy for social work: Impli... (Total: 12/15)
- Baker (2025): Artificial intelligence in social work: An EPIC model for pr... (Total: 12/15)
- Linnemann (2025): K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen f√º... (Total: 12/15)
- Yu (2025): Algorithmic-Assisted Decision-Making Tools in Child Welfare ... (Total: 12/15)
- Ahn (2025): Artificial Intelligence (AI) Literacy for Social Work: Impli... (Total: 12/15)
- Rodriguez (2024): Introducing Generative Artificial Intelligence into the MSW ... (Total: 12/15)
- Nuwasiima (2024): The role of artificial intelligence (AI) and machine learnin... (Total: 12/15)
- Lanzetta (2024): Artificial Intelligence Competence Needs for Youth Workers... (Total: 11/15)
- Takaoka (2022): AI implementation science for social issues: Pitfalls and ti... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit... (Total: 11/15)
- Unknown (2025): Artificial Intelligence in Social Work: An EPIC Model for Pr... (Total: 11/15)
- James (2023): Algorithmic decision-making in social work practice and peda... (Total: 11/15)
- Feist-Ortmanns (2025): KI-basiertes Assistenzsystem im Kinderschutzverfahren... (Total: 11/15)
- Ahn (2025): Artificial Intelligence (AI) literacy for social work... (Total: 11/15)
- Alam (2025): Social work in the age of artificial intelligence: A rights-... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Ahrweiler (2025): AI FORA ‚Äì Artificial Intelligence for Assessment: Fairness b... (Total: 11/15)
- Victor (2023): Recommendations for social work researchers and journal edit... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- [Author not specified] (2025): Navigating the Nexus of Trust: Prompt Engineering, Professio... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Victor (2023): Recommendations for social work researchers and journal edit... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Victor (2023): Recommendations for social work researchers and journal edit... (Total: 11/15)
- Spaulding (2023): Predicting successful placements for youth in child welfare ... (Total: 10/15)
- Cher (2024): Exploring machine learning to support decision-making for pl... (Total: 10/15)
- British Association of Social Workers (2025): Generative AI & social work practice guidance... (Total: 10/15)
- Perron (2023): Recommendations for social work researchers and journal edit... (Total: 10/15)
- Hodgson (2022): Problematising artificial intelligence in social work educat... (Total: 10/15)
- Schneider (2022): Exploring opportunities and risks in decision support techno... (Total: 10/15)
- Schneider (2024): Das verflixte Problem mit Klassifikationen: Zum Einfluss der... (Total: 10/15)
- Steiner (2022): K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen, E... (Total: 10/15)
- Creswell B√°ez (2025): Clinical Social Workers‚Äô Perceptions of Large Language Model... (Total: 10/15)
- Patton (2023): ChatGPT for Social Work Science: Ethical Challenges and Oppo... (Total: 10/15)
- Reamer (2023): Artificial Intelligence in Social Work: Emerging Ethical Iss... (Total: 10/15)
- Boetto (2025): Artificial Intelligence in Social Work: An EPIC Model for Pr... (Total: 10/15)
- Engelhardt (2025): Voll (dia)logisch? Ein Werkstattbericht √ºber den Einsatz von... (Total: 10/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit... (Total: 10/15)
- Sch√∂nauer (2025): Akzeptanz von KI und organisationale Rahmenbedingungen in de... (Total: 10/15)
- Chen (2025): Social work and artificial intelligence: Collaboration and c... (Total: 10/15)
- Studeny (2025): Digitale Werkzeuge und Machtasymmetrien?... (Total: 10/15)
- Gravelmann (2024): K√ºnstliche Intelligenz in der Sozialen Arbeit ‚Äì Zwischen Bed... (Total: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for pr... (Total: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for pr... (Total: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for pr... (Total: 10/15)

## ‚≠ê‚≠ê Medium (Score 2)

- Moreau (2024): Failing our youngest: On the biases, pitfalls, and risks in ... (Total: 11/15)
- Marjanovic (2022): Theorising algorithmic justice... (Total: 10/15)
- Li (2023): Ethics & AI: A systematic review on ethical concerns and rel... (Total: 10/15)
- Dencik (2024): Automated government benefits and welfare surveillance... (Total: 10/15)
- Amnesty International (2024): Coded injustice: Surveillance and discrimination in Denmark'... (Total: 10/15)
- van Toorn (2024): Introduction to the digital welfare state: Contestations, co... (Total: 10/15)
- World Economic Forum (2024): AI for impact: The PRISM framework for responsible AI in soc... (Total: 10/15)
- Field (2023): Examining risks of racial biases in NLP tools for child prot... (Total: 10/15)
- J√∏rgensen (2023): Data and rights in the digital welfare state: the case of De... (Total: 9/15)
- Heinz (2025): Clinical trial of an LLM-based conversational AI psychothera... (Total: 9/15)
- Keddell (2019): Algorithmic justice in child protection: Statistical fairnes... (Total: 9/15)
- Choudhury (2024): Large Language Models and User Trust: Consequence of Self-Re... (Total: 9/15)
- Choudhury (2024): Large Language Models and User Trust: Consequence of Self-Re... (Total: 9/15)
- Ghosal (2024): An empirical study of structural social and ethical challeng... (Total: 9/15)
- Sabour (2023): A chatbot for mental health support: Exploring the impact of... (Total: 8/15)
- Siddals (2024): "It happened to be the perfect thing": Experiences of genera... (Total: 8/15)
- Zakharova (2024): Tensions in digital welfare states: Three perspectives on ca... (Total: 8/15)
- Unknown (2025): Artificial Intelligence in Social Sciences and Social Work: ... (Total: 8/15)
- N√§scher (2025): ReflectAI: Design and evaluation of an AI coach to support p... (Total: 7/15)
- Schneider (2025): Indecision on the use of artificial intelligence in healthca... (Total: 7/15)

## ‚≠ê Low (Score 1)

- Slesinger (2024): Training in Co-Creation as a Methodological Approach to Impr... (Total: 12/15)
- Kubes (2024): Feministische KI ‚Äì K√ºnstliche Intelligenz f√ºr alle? [Feminis... (Total: 11/15)
- Shah (2025): Gender bias in artificial intelligence: Empowering women thr... (Total: 11/15)
- DIVERSIFAIR Project (2024): AI & Intersectionality: A Toolkit For Fairness & Inclusion... (Total: 11/15)
- S≈´na (2024): Diskriminierung durch Algorithmen ‚Äì √úberlegungen zur St√§rkun... (Total: 10/15)
- Hauck (2025): A framework for the learning and teaching of Critical AI Lit... (Total: 10/15)
- Strau√ü (2024): CAIL ‚Äì Critical AI Literacy: Kritische Technikkompetenz f√ºr ... (Total: 10/15)
- Quaid-i-Azam University (2025): Gender Bias in Artificial Intelligence: Empowering Women Thr... (Total: 10/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs throug... (Total: 10/15)
- Lau (2023): Dipper: Diversity in Prompts for Producing Large Language Mo... (Total: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Salinas (2025): What‚Äôs in a name? Auditing large language models for race an... (Total: 10/15)
- Asseri (2025): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs throug... (Total: 10/15)
- Qiu (2025): DR.GAP: Mitigating bias in large language models using gende... (Total: 10/15)
- Cvoelcker (2023): Queer in AI: A case study in community-led participatory AI... (Total: 10/15)
- Basseri (2025): Prompt Engineering Techniques for Mitigating Cultural Bias A... (Total: 10/15)
- Ovalle (2024): Towards Substantive Equality in Artificial Intelligence: Tra... (Total: 10/15)
- Giannoni Adielsson (2024): The AI Act, gender equality and non-discrimination: what rol... (Total: 10/15)
- Wudel (2025): What is Feminist AI?... (Total: 10/15)
- Ricaurte Quijano (2024): Towards substantive equality in artificial intelligence: Tra... (Total: 10/15)
- Wudel (2025): What is Feminist AI?... (Total: 10/15)
- Vethman (2025): Fairness Beyond the Algorithmic Frame: Actionable Recommenda... (Total: 10/15)
- Raji (2024): The Algorithmic Auditing Landscape: A Social Justice Approac... (Total: 10/15)
- Djeffal (2025): Reflexive prompt engineering: A framework for responsible pr... (Total: 10/15)
- Ciston (2024): Intersectional Artificial Intelligence Is Essential: Polyvoc... (Total: 10/15)
- Hartshorne (2025): Generative AI and the Future of Digital Literacy: Opportunit... (Total: 10/15)
- Klein (2024): Data Feminism for AI... (Total: 10/15)
- Wudel (2025): What is Feminist AI?... (Total: 10/15)
- Shah (2025): Gender Bias in Artificial Intelligence: Empowering Women Thr... (Total: 10/15)
- A+ Alliance (2024): Incubating Feminist AI: Executive Summary 2021-2024... (Total: 10/15)
- UN Women (2024): Artificial Intelligence and gender equality... (Total: 10/15)
- UNESCO (2021): Recommendation on the Ethics of Artificial Intelligence... (Total: 10/15)
- Browne (2024): Tech workers' perspectives on ethical issues in AI developme... (Total: 10/15)
- Smith (2021): When Good Algorithms Go Sexist: Why and How to Advance AI Ge... (Total: 10/15)
- James (2025): Responsible prompting recommendation: Fostering responsible ... (Total: 9/15)
- Biegelbauer (2023): Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden f√ºr... (Total: 9/15)
- Singh (2025): A reparative turn in AI... (Total: 9/15)
- Taeihagh (2025): Governance of generative AI: A comprehensive framework for n... (Total: 9/15)
- Ghosal (2025): Unequal voices: How LLMs construct constrained queer narrati... (Total: 9/15)
- Tint (2025): Guardrails, not guidance: Understanding responses to LGBTQ+ ... (Total: 9/15)
- Petzel (2025): Prejudiced interactions with large language models (LLMs) re... (Total: 9/15)
- Gallegos (2024): Bias and fairness in large language models: A survey... (Total: 9/15)
- OECD (2023): Advancing Accountability in AI... (Total: 9/15)
- Hayati (2024): How Far Can We Extract Diverse Perspectives from Large Langu... (Total: 9/15)
- UNESCO (2024): Women4Ethical AI: Global cooperation for gender-inclusive AI... (Total: 9/15)
- Friedrich-Ebert-Stiftung (2025): The EU artificial intelligence act through a gender lens... (Total: 9/15)
- Ahmed (2024): Feminist perspectives on AI: Ethical considerations in algor... (Total: 9/15)

---

<a id='dimension_vulnerable_groups'></a>

## Dimension: Vulnerable Groups & Digital Equity

---
title: "Dimension: Vulnerable Groups & Digital Equity"
type: dimension-moc
dimension: Vulnerable_Groups
date_created: 2025-11-10
tags: [moc, dimension]
---

# Dimension: Vulnerable Groups & Digital Equity

Research papers organized by relevance score for this dimension.

## Statistics

| Score | Count | Percentage |
|-------|-------|------------|
| ‚≠ê‚≠ê‚≠ê High (3) | 95 | 29.2% |
| ‚≠ê‚≠ê Medium (2) | 95 | 29.2% |
| ‚≠ê Low (1) | 37 | 11.4% |
| ‚Äî None (0) | 98 | 30.2% |

Average score: 1.58

---


## ‚≠ê‚≠ê‚≠ê High (Score 3)

- Cheng (2022): How child welfare workers reduce racial disparities in algor... (Total: 13/15)
- Hall (2024): A systematic review of sophisticated predictive and prescrip... (Total: 12/15)
- Kawakami (2022): Improving human-AI partnerships in child welfare: Understand... (Total: 12/15)
- Baker (2025): Artificial intelligence in social work: An EPIC model for pr... (Total: 12/15)
- Yu (2025): Algorithmic-Assisted Decision-Making Tools in Child Welfare ... (Total: 12/15)
- Nuwasiima (2024): The role of artificial intelligence (AI) and machine learnin... (Total: 12/15)
- Slesinger (2024): Training in Co-Creation as a Methodological Approach to Impr... (Total: 12/15)
- Moreau (2024): Failing our youngest: On the biases, pitfalls, and risks in ... (Total: 11/15)
- Alam (2025): Social work in the age of artificial intelligence: A rights-... (Total: 11/15)
- Ahrweiler (2025): AI FORA ‚Äì Artificial Intelligence for Assessment: Fairness b... (Total: 11/15)
- Kubes (2024): Feministische KI ‚Äì K√ºnstliche Intelligenz f√ºr alle? [Feminis... (Total: 11/15)
- Shah (2025): Gender bias in artificial intelligence: Empowering women thr... (Total: 11/15)
- DIVERSIFAIR Project (2024): AI & Intersectionality: A Toolkit For Fairness & Inclusion... (Total: 11/15)
- Marjanovic (2022): Theorising algorithmic justice... (Total: 10/15)
- Dencik (2024): Automated government benefits and welfare surveillance... (Total: 10/15)
- Amnesty International (2024): Coded injustice: Surveillance and discrimination in Denmark'... (Total: 10/15)
- van Toorn (2024): Introduction to the digital welfare state: Contestations, co... (Total: 10/15)
- Field (2023): Examining risks of racial biases in NLP tools for child prot... (Total: 10/15)
- Quaid-i-Azam University (2025): Gender Bias in Artificial Intelligence: Empowering Women Thr... (Total: 10/15)
- Lau (2023): Dipper: Diversity in Prompts for Producing Large Language Mo... (Total: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Salinas (2025): What‚Äôs in a name? Auditing large language models for race an... (Total: 10/15)
- Asseri (2025): Prompt engineering techniques for mitigating cultural bias a... (Total: 10/15)
- Cvoelcker (2023): Queer in AI: A case study in community-led participatory AI... (Total: 10/15)
- Basseri (2025): Prompt Engineering Techniques for Mitigating Cultural Bias A... (Total: 10/15)
- Ovalle (2024): Towards Substantive Equality in Artificial Intelligence: Tra... (Total: 10/15)
- Giannoni Adielsson (2024): The AI Act, gender equality and non-discrimination: what rol... (Total: 10/15)
- Wudel (2025): What is Feminist AI?... (Total: 10/15)
- Ricaurte Quijano (2024): Towards substantive equality in artificial intelligence: Tra... (Total: 10/15)
- Wudel (2025): What is Feminist AI?... (Total: 10/15)
- Vethman (2025): Fairness Beyond the Algorithmic Frame: Actionable Recommenda... (Total: 10/15)
- Raji (2024): The Algorithmic Auditing Landscape: A Social Justice Approac... (Total: 10/15)
- Ciston (2024): Intersectional Artificial Intelligence Is Essential: Polyvoc... (Total: 10/15)
- Hartshorne (2025): Generative AI and the Future of Digital Literacy: Opportunit... (Total: 10/15)
- Klein (2024): Data Feminism for AI... (Total: 10/15)
- Wudel (2025): What is Feminist AI?... (Total: 10/15)
- Shah (2025): Gender Bias in Artificial Intelligence: Empowering Women Thr... (Total: 10/15)
- A+ Alliance (2024): Incubating Feminist AI: Executive Summary 2021-2024... (Total: 10/15)
- UN Women (2024): Artificial Intelligence and gender equality... (Total: 10/15)
- UNESCO (2021): Recommendation on the Ethics of Artificial Intelligence... (Total: 10/15)
- Browne (2024): Tech workers' perspectives on ethical issues in AI developme... (Total: 10/15)
- Smith (2021): When Good Algorithms Go Sexist: Why and How to Advance AI Ge... (Total: 10/15)
- J√∏rgensen (2023): Data and rights in the digital welfare state: the case of De... (Total: 9/15)
- Keddell (2019): Algorithmic justice in child protection: Statistical fairnes... (Total: 9/15)
- Ghosal (2025): Unequal voices: How LLMs construct constrained queer narrati... (Total: 9/15)
- Tint (2025): Guardrails, not guidance: Understanding responses to LGBTQ+ ... (Total: 9/15)
- Petzel (2025): Prejudiced interactions with large language models (LLMs) re... (Total: 9/15)
- UNESCO (2024): Women4Ethical AI: Global cooperation for gender-inclusive AI... (Total: 9/15)

## ‚≠ê‚≠ê Medium (Score 2)

- McDonald (2023): Algorithmic decision-making in social work practice and peda... (Total: 12/15)
- Ahn (2025): Artificial Intelligence (AI) literacy for social work: Impli... (Total: 12/15)
- Linnemann (2025): K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen f√º... (Total: 12/15)
- Ahn (2025): Artificial Intelligence (AI) Literacy for Social Work: Impli... (Total: 12/15)
- Rodriguez (2024): Introducing Generative Artificial Intelligence into the MSW ... (Total: 12/15)
- Lanzetta (2024): Artificial Intelligence Competence Needs for Youth Workers... (Total: 11/15)
- Takaoka (2022): AI implementation science for social issues: Pitfalls and ti... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit... (Total: 11/15)
- Unknown (2025): Artificial Intelligence in Social Work: An EPIC Model for Pr... (Total: 11/15)
- James (2023): Algorithmic decision-making in social work practice and peda... (Total: 11/15)
- Feist-Ortmanns (2025): KI-basiertes Assistenzsystem im Kinderschutzverfahren... (Total: 11/15)
- Ahn (2025): Artificial Intelligence (AI) literacy for social work... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Li (2023): Ethics & AI: A systematic review on ethical concerns and rel... (Total: 10/15)
- Spaulding (2023): Predicting successful placements for youth in child welfare ... (Total: 10/15)
- Cher (2024): Exploring machine learning to support decision-making for pl... (Total: 10/15)
- World Economic Forum (2024): AI for impact: The PRISM framework for responsible AI in soc... (Total: 10/15)
- British Association of Social Workers (2025): Generative AI & social work practice guidance... (Total: 10/15)
- Hodgson (2022): Problematising artificial intelligence in social work educat... (Total: 10/15)
- Schneider (2022): Exploring opportunities and risks in decision support techno... (Total: 10/15)
- Schneider (2024): Das verflixte Problem mit Klassifikationen: Zum Einfluss der... (Total: 10/15)
- Steiner (2022): K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen, E... (Total: 10/15)
- Patton (2023): ChatGPT for Social Work Science: Ethical Challenges and Oppo... (Total: 10/15)
- Reamer (2023): Artificial Intelligence in Social Work: Emerging Ethical Iss... (Total: 10/15)
- Boetto (2025): Artificial Intelligence in Social Work: An EPIC Model for Pr... (Total: 10/15)
- S≈´na (2024): Diskriminierung durch Algorithmen ‚Äì √úberlegungen zur St√§rkun... (Total: 10/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit... (Total: 10/15)
- Sch√∂nauer (2025): Akzeptanz von KI und organisationale Rahmenbedingungen in de... (Total: 10/15)
- Hauck (2025): A framework for the learning and teaching of Critical AI Lit... (Total: 10/15)
- Studeny (2025): Digitale Werkzeuge und Machtasymmetrien?... (Total: 10/15)
- Gravelmann (2024): K√ºnstliche Intelligenz in der Sozialen Arbeit ‚Äì Zwischen Bed... (Total: 10/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs throug... (Total: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for pr... (Total: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for pr... (Total: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for pr... (Total: 10/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs throug... (Total: 10/15)
- Qiu (2025): DR.GAP: Mitigating bias in large language models using gende... (Total: 10/15)
- Djeffal (2025): Reflexive prompt engineering: A framework for responsible pr... (Total: 10/15)
- Rodr√≠guez-Mart√≠nez (2024): Ethical issues related to the use of technology in social wo... (Total: 9/15)
- Meilvang (2024): Decision support and algorithmic support: The construction o... (Total: 9/15)
- Heinz (2025): Clinical trial of an LLM-based conversational AI psychothera... (Total: 9/15)
- National Association of Social Workers (2017): NASW, ASWB, CSWE, & CSWA standards for technology in social ... (Total: 9/15)
- Kutscher (2024): Digitalit√§t und Digitalisierung als Gegenstand der Sozialen ... (Total: 9/15)
- Unknown (2024): RETHINKING SOCIAL SERVICES WITH ARTIFICIAL INTELLIGENCE: OPP... (Total: 9/15)
- Gravelmann (2024): K√ºnstliche Intelligenz in der Sozialen Arbeit ‚Äì Zwischen Bed... (Total: 9/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit... (Total: 9/15)
- Singh (2025): A reparative turn in AI... (Total: 9/15)

## ‚≠ê Low (Score 1)

- Reamer (2023): Artificial intelligence in social work: Emerging ethical iss... (Total: 11/15)
- Victor (2023): Recommendations for social work researchers and journal edit... (Total: 11/15)
- [Author not specified] (2025): Navigating the Nexus of Trust: Prompt Engineering, Professio... (Total: 11/15)
- Victor (2023): Recommendations for social work researchers and journal edit... (Total: 11/15)
- Victor (2023): Recommendations for social work researchers and journal edit... (Total: 11/15)
- Perron (2023): Recommendations for social work researchers and journal edit... (Total: 10/15)
- Creswell B√°ez (2025): Clinical Social Workers‚Äô Perceptions of Large Language Model... (Total: 10/15)
- Engelhardt (2025): Voll (dia)logisch? Ein Werkstattbericht √ºber den Einsatz von... (Total: 10/15)
- Chen (2025): Social work and artificial intelligence: Collaboration and c... (Total: 10/15)
- Strau√ü (2024): CAIL ‚Äì Critical AI Literacy: Kritische Technikkompetenz f√ºr ... (Total: 10/15)
- James (2025): Responsible prompting recommendation: Fostering responsible ... (Total: 9/15)
- Schneider (2018): Der Einfluss der Algorithmen: Neue Qualit√§ten durch Big Data... (Total: 9/15)
- Singer (2023): AI Creates the Message: Integrating AI Language Learning Mod... (Total: 9/15)
- Sch√∂nauer (2025): Akzeptanz von KI und organisationale Rahmenbedingungen in de... (Total: 9/15)
- Biegelbauer (2023): Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden f√ºr... (Total: 9/15)
- Choudhury (2024): Large Language Models and User Trust: Consequence of Self-Re... (Total: 9/15)
- Choudhury (2024): Large Language Models and User Trust: Consequence of Self-Re... (Total: 9/15)
- Thwaites (2024): Operationalizing positive-constructive pedagogy to artificia... (Total: 9/15)
- Singer (2023): ChatGPT for social work science: Ethical challenges and oppo... (Total: 8/15)
- Garkisch (2024): Considering a unified model of artificial intelligence enhan... (Total: 8/15)
- Barman (2024): Beyond transparency and explainability: On the need for adeq... (Total: 8/15)
- Colombatto (2025): The influence of mental state attributions on trust in large... (Total: 8/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs throug... (Total: 8/15)
- Colombatto (2025): The influence of mental state attributions on trust in large... (Total: 8/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs throug... (Total: 8/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs throug... (Total: 8/15)
- Chee (2025): A Competency Framework for AI Literacy: Variations by Differ... (Total: 7/15)
- Biagini (2024): Less knowledge, more trust? Exploring potentially uncritical... (Total: 7/15)
- Goldkind (2023): The End of the World as We Know It? ChatGPT and Social Work... (Total: 7/15)
- Goellner (2025): Towards responsible AI for education: Hybrid human-AI to con... (Total: 7/15)
- Tun (2025): Trust in artificial intelligence‚Äìbased clinical decision sup... (Total: 7/15)
- Xu (2023): Transparency enhances positive perceptions of social artific... (Total: 7/15)
- Tun (2025): Trust in artificial intelligence‚Äìbased clinical decision sup... (Total: 7/15)
- Xu (2023): Transparency enhances positive perceptions of social artific... (Total: 7/15)
- Small (2023): Generative AI and opportunities for feminist classroom assig... (Total: 6/15)
- Park (2025): AI algorithm transparency, pipelines for trust not prisms: m... (Total: 5/15)
- Park (2025): AI algorithm transparency, pipelines for trust not prisms: m... (Total: 5/15)

---

<a id='papers_exclude'></a>

## üìï Exclude Papers

---
title: "Exclude Papers"
type: decision-moc
decision: Exclude
count: 83
date_created: 2025-11-10
tags: [moc, decision, exclude]
---

# üìï Exclude Papers

Total: 83 papers

- Chatterji (2025): How People Use ChatGPT...  (Relevance: 0/15)
- Benlian (2025): The AI literacy development canvas: Assessing and building AI literacy...  (Relevance: 0/15)
- Chiu (2025): AI literacy and competency: definitions, frameworks, development and f...  (Relevance: 0/15)
- Casal-Otero (2023): AI literacy in K-12: a systematic literature review...  (Relevance: 0/15)
- Chiu (2024): What are artificial intelligence literacy and competency? A comprehens...  (Relevance: 0/15)
- Kong (2021): Evaluation of an artificial intelligence literacy course for universit...  (Relevance: 0/15)
- Ng (2021): Conceptualizing AI literacy: An exploratory review...  (Relevance: 0/15)
- Peng (2022): A Literature Review of Digital Literacy over Two Decades...  (Relevance: 0/15)
- Tinmaz (2022): A systematic review on digital literacy...  (Relevance: 0/15)
- European Commission. Joint Research Centre. (2017): DigComp 2.1: the digital competence framework for citizens with eight ...  (Relevance: 0/15)
- Ng (2021): Conceptualizing AI literacy: An exploratory review...  (Relevance: 0/15)
- Pinski (2024): AI literacy for users ‚Äì A comprehensive review and future research dir...  (Relevance: 0/15)
- Arias L√≥pez (2023): Digital literacy as a new determinant of health: A scoping review...  (Relevance: 0/15)
- Yan (2024): Promises and challenges of generative artificial intelligence for huma...  (Relevance: 0/15)
- Pinski (2023): AI Literacy - Towards Measuring Human Competency in Artificial Intelli...  (Relevance: 0/15)
- Sperling (2024): In search of artificial intelligence (AI) literacy in teacher educatio...  (Relevance: 0/15)
- Jin (2025): GLAT: The generative AI literacy assessment test...  (Relevance: 0/15)
- Deuze (2022): Imagination, Algorithms and News: Developing AI Literacy for Journalis...  (Relevance: 0/15)
- Laupichler (2023): Development of the ‚ÄúScale for the assessment of non-experts‚Äô AI litera...  (Relevance: 0/15)
- Wang (2023): Measuring user competence in using artificial intelligence: validity a...  (Relevance: 0/15)
- Kong (2025): Artificial Intelligence (AI) literacy ‚Äì an argument for AI literacy in...  (Relevance: 0/15)
- Kong (2024): Developing an artificial intelligence literacy framework: Evaluation o...  (Relevance: 0/15)
- Wadmann (2020): 'Meaningless work': How the datafication of health reconfigures knowle...  (Relevance: 0/15)
- Goldkind (2024): The end of the world as we know it? ChatGPT and social work...  (Relevance: 0/15)
- Kutscher (2023): Positionings, challenges, and ambivalences in children's and parents' ...  (Relevance: 0/15)
- Fujii (2024): Bildungsteilhabe - Flucht - Digitalisierung: Eine multilokale Ethnogra...  (Relevance: 0/15)
- Jarke (2024): Who cares about data? Data care arrangements in everyday organisationa...  (Relevance: 0/15)
- Waag (2023): Rationalisierung durch Digitalisierung?...  (Relevance: 0/15)
- Freinhofer (2025): Prompten nach Plan: Das PCRR-Framework als p√§dagogisches Werkzeug f√ºr ...  (Relevance: 0/15)
- Weber (2023): Messung von AI Literacy ‚Äì Empirische Evidenz und Implikationen...  (Relevance: 0/15)
- Unknown (2024): Research on the application risks and countermeasures of ChatGPT gener...  (Relevance: 0/15)
- Ruiz (2024): AI Literacy: A Framework to Understand, Evaluate, and Use Emerging Tec...  (Relevance: 0/15)
- Unknown (2024): AI competency framework for students...  (Relevance: 0/15)
- Unknown (2024): Research on the application risks and countermeasures of ChatGPT gener...  (Relevance: 0/15)
- Zhang (2025): Learning About AI: A Systematic Review of Reviews on AI Literacy...  (Relevance: 0/15)
- Ahn (2025): Artificial Intelligence (AI) Literacy for Social Work: Implications fo...  (Relevance: 0/15)
- Long (2020): What is AI Literacy? Competencies and Design Considerations...  (Relevance: 0/15)
- Santos (2025): How large language models judge cooperation...  (Relevance: 0/15)
- Santos (2024): Explainability through systematicity: The hard systematicity challenge...  (Relevance: 0/15)
- Lahoti (2023): Improving diversity of demographic representation in people entities i...  (Relevance: 0/15)
- Wang (2024): Multilingual Prompting for Improving LLM Generation Diversity...  (Relevance: 0/15)
- Wu (2025): Bias in decision-making for AI's ethical dilemmas: A comparative study...  (Relevance: 0/15)
- Struppek (2024): Homoglyph unlearning: A novel approach to bias mitigation...  (Relevance: 0/15)
- Srinivasan (2024): Worst of both worlds: A comparative analysis of error in language and ...  (Relevance: 0/15)
- Prakash (2023): Prompt engineering techniques for mitigating cultural bias against Ara...  (Relevance: 0/15)
- Zayed (2024): Scaling implicit bias analysis across transformer-based language model...  (Relevance: 0/15)
- Wang (2024): A survey on fairness in large language models...  (Relevance: 0/15)
- U.S. Bureau of Labor Statistics (2023): Occupational employment statistics...  (Relevance: 0/15)
- Shin (2025): Mitigating age-related bias in large language models: Strategies for r...  (Relevance: 0/15)
- Salecha (2025): Model explanations for gender and ethnicity bias mitigation in AI-gene...  (Relevance: 0/15)
- Parrish (2025): Self-debiasing large language models: Zero-shot recognition and reduct...  (Relevance: 0/15)
- Mei (2023): Assessing GPT's bias towards stigmatized social groups: An intersectio...  (Relevance: 0/15)
- Kojima (2022): Large language models are zero-shot reasoners...  (Relevance: 0/15)
- Klinge (2024): A sociolinguistic approach to stereotype assessment in large language ...  (Relevance: 0/15)
- Kaneko (2024): Debiasing prompts for gender bias in large language models...  (Relevance: 0/15)
- He (2024): On the steerability of large language models...  (Relevance: 0/15)
- Parrish (2022): BBQ: A hand-built bias benchmark for question answering...  (Relevance: 0/15)
- Liu (2025): More or less wrong: A benchmark for directional bias in LLM comparativ...  (Relevance: 0/15)
- Lin (2024): SWIFTSAGE: A new dual-module framework for better action planning in c...  (Relevance: 0/15)
- Jiang (2022): Assessing GPT's bias towards stigmatized social groups: An intersectio...  (Relevance: 0/15)
- Garg (2019): Counterfactual fairness in text classification through robustness...  (Relevance: 0/15)
- Furniturewala (2024): Reasoning towards fairness: Mitigating bias in language models through...  (Relevance: 0/15)
- Dixon (2018): Measuring and mitigating unintended bias in text data...  (Relevance: 0/15)
- Chisca (2024): Prompting techniques for reducing social bias in LLMs through System 1...  (Relevance: 0/15)
- Kaneko (2024): Evaluating gender bias in large language models via chain-of-thought p...  (Relevance: 0/15)
- Chen (2024): Exploring complex mental health symptoms via classifying social media ...  (Relevance: 0/15)
- Birru (2024): Mitigating age-related bias in large language models: Strategies for r...  (Relevance: 0/15)
- Wang (2025): Multilingual Prompting for Improving LLM Generation Diversity...  (Relevance: 0/15)
- Zannone (2023): Intersectional Fairness: A Fractal Approach...  (Relevance: 0/15)
- Dilek (2025): AI literacy in teacher education: Empowering educators through critica...  (Relevance: 0/15)
- Srivastava (2024): Algorithmic Governance and the International Politics of Big Tech...  (Relevance: 0/15)
- Sinders (2017): Feminist Data Set...  (Relevance: 0/15)
- Clemmer (2024): PreciseDebias: An automatic prompt engineering approach for generative...  (Relevance: 0/15)
- Gengler (2024): Faires KI-Prompting ‚Äì Ein Leitfaden f√ºr Unternehmen...  (Relevance: 0/15)
- Skilton (2024): Inclusive prompt engineering: A methodology for hacking biased AI imag...  (Relevance: 0/15)
- Articulate (2025): How to Create Inclusive AI Images: A Guide to Bias-Free Prompting...  (Relevance: 0/15)
- Chisca (2024): Prompting fairness: Learning prompts for debiasing large language mode...  (Relevance: 0/15)
- Sant (2024): The power of prompts: Evaluating and mitigating gender bias in MT with...  (Relevance: 0/15)
- Gallegos (2024): Bias and fairness in large language models: A survey...  (Relevance: 0/15)
- Walgenbach (2023): Intersektionalit√§t...  (Relevance: 0/15)
- Franken (): Gender und KI-Anwendungen. Tr√§gt KI zum Genderproblem oder zu seiner L...  (Relevance: 0/15)
- _ACADEMY|Unknown (): feminist AI | ACADEMY... üìù (Relevance: 0/15)
- Gengler (2024): Faires KIPrompting ‚Äì Ein Leitfaden f√ºr Unternehmen. BSP Business and L...  (Relevance: 0/15)

---

<a id='papers_high_relevance'></a>

## ‚≠ê‚≠ê‚≠ê High Relevance Papers

---
title: "High Relevance Papers"
type: relevance-moc
relevance_level: High
count: 95
date_created: 2025-11-10
tags: [moc, relevance, high-relevance]
---

# ‚≠ê‚≠ê‚≠ê High Relevance Papers

Total: 95 papers

- Cheng (2022): How child welfare workers reduce racial disparities in algorithmic dec... (Score: 13/15, Include)
- Hall (2024): A systematic review of sophisticated predictive and prescriptive analy... (Score: 12/15, Include)
- Kawakami (2022): Improving human-AI partnerships in child welfare: Understanding worker... (Score: 12/15, Include)
- McDonald (2023): Algorithmic decision-making in social work practice and pedagogy: Conf... (Score: 12/15, Include)
- Ahn (2025): Artificial Intelligence (AI) literacy for social work: Implications fo... (Score: 12/15, Include)
- Baker (2025): Artificial intelligence in social work: An EPIC model for practice... (Score: 12/15, Include)
- Linnemann (2025): K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen f√ºr Theorie ... (Score: 12/15, Include)
- Yu (2025): Algorithmic-Assisted Decision-Making Tools in Child Welfare Practice: ... (Score: 12/15, Include)
- Ahn (2025): Artificial Intelligence (AI) Literacy for Social Work: Implications fo... (Score: 12/15, Include)
- Rodriguez (2024): Introducing Generative Artificial Intelligence into the MSW Curriculum... (Score: 12/15, Include)
- Nuwasiima (2024): The role of artificial intelligence (AI) and machine learning in socia... (Score: 12/15, Include)
- Slesinger (2024): Training in Co-Creation as a Methodological Approach to Improve AI Fai... (Score: 12/15, Include)
- Lanzetta (2024): Artificial Intelligence Competence Needs for Youth Workers... (Score: 11/15, Include)
- Takaoka (2022): AI implementation science for social issues: Pitfalls and tips... (Score: 11/15, Include)
- Moreau (2024): Failing our youngest: On the biases, pitfalls, and risks in a decision... (Score: 11/15, Include)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical issues... (Score: 11/15, Include)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit: Eine exe... (Score: 11/15, Include)
- Unknown (2025): Artificial Intelligence in Social Work: An EPIC Model for Practice... (Score: 11/15, Include)
- James (2023): Algorithmic decision-making in social work practice and pedagogy: conf... (Score: 11/15, Include)
- Feist-Ortmanns (2025): KI-basiertes Assistenzsystem im Kinderschutzverfahren... (Score: 11/15, Include)
- Ahn (2025): Artificial Intelligence (AI) literacy for social work... (Score: 11/15, Include)
- Alam (2025): Social work in the age of artificial intelligence: A rights-based fram... (Score: 11/15, Include)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical issues... (Score: 11/15, Include)
- Ahrweiler (2025): AI FORA ‚Äì Artificial Intelligence for Assessment: Fairness bei der Ver... (Score: 11/15, Include)
- Victor (2023): Recommendations for social work researchers and journal editors on the... (Score: 11/15, Include)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical issues... (Score: 11/15, Include)
- [Author not specified] (2025): Navigating the Nexus of Trust: Prompt Engineering, Professional Judgme... (Score: 11/15, Include)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical issues... (Score: 11/15, Include)
- Victor (2023): Recommendations for social work researchers and journal editors on the... (Score: 11/15, Include)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical issues... (Score: 11/15, Include)
- Victor (2023): Recommendations for social work researchers and journal editors on the... (Score: 11/15, Include)
- Kubes (2024): Feministische KI ‚Äì K√ºnstliche Intelligenz f√ºr alle? [Feminist AI ‚Äì Art... (Score: 11/15, Unclear)
- Shah (2025): Gender bias in artificial intelligence: Empowering women through digit... (Score: 11/15, Include)
- DIVERSIFAIR Project (2024): AI & Intersectionality: A Toolkit For Fairness & Inclusion... (Score: 11/15, Include)
- Marjanovic (2022): Theorising algorithmic justice... (Score: 10/15, Include)
- Li (2023): Ethics & AI: A systematic review on ethical concerns and related strat... (Score: 10/15, Include)
- Dencik (2024): Automated government benefits and welfare surveillance... (Score: 10/15, Include)
- Amnesty International (2024): Coded injustice: Surveillance and discrimination in Denmark's automate... (Score: 10/15, Include)
- van Toorn (2024): Introduction to the digital welfare state: Contestations, consideratio... (Score: 10/15, Include)
- Spaulding (2023): Predicting successful placements for youth in child welfare with machi... (Score: 10/15, Include)
- Cher (2024): Exploring machine learning to support decision-making for placement st... (Score: 10/15, Include)
- World Economic Forum (2024): AI for impact: The PRISM framework for responsible AI in social innova... (Score: 10/15, Include)
- Field (2023): Examining risks of racial biases in NLP tools for child protective ser... (Score: 10/15, Include)
- British Association of Social Workers (2025): Generative AI & social work practice guidance... (Score: 10/15, Include)
- Perron (2023): Recommendations for social work researchers and journal editors on the... (Score: 10/15, Include)
- Hodgson (2022): Problematising artificial intelligence in social work education: Chall... (Score: 10/15, Include)
- Schneider (2022): Exploring opportunities and risks in decision support technologies for... (Score: 10/15, Include)
- Schneider (2024): Das verflixte Problem mit Klassifikationen: Zum Einfluss der Digitalis... (Score: 10/15, Include)
- Steiner (2022): K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen, Entwicklung... (Score: 10/15, Include)
- Creswell B√°ez (2025): Clinical Social Workers‚Äô Perceptions of Large Language Models in Pract... (Score: 10/15, Include)
- Patton (2023): ChatGPT for Social Work Science: Ethical Challenges and Opportunities... (Score: 10/15, Include)
- Reamer (2023): Artificial Intelligence in Social Work: Emerging Ethical Issues... (Score: 10/15, Include)
- Boetto (2025): Artificial Intelligence in Social Work: An EPIC Model for Practice... (Score: 10/15, Include)
- Engelhardt (2025): Voll (dia)logisch? Ein Werkstattbericht √ºber den Einsatz von generativ... (Score: 10/15, Include)
- S≈´na (2024): Diskriminierung durch Algorithmen ‚Äì √úberlegungen zur St√§rkung KI-bezog... (Score: 10/15, Include)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit: Eine exe... (Score: 10/15, Include)
- Sch√∂nauer (2025): Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen... (Score: 10/15, Include)
- Hauck (2025): A framework for the learning and teaching of Critical AI Literacy skil... (Score: 10/15, Include)
- Chen (2025): Social work and artificial intelligence: Collaboration and challenges... (Score: 10/15, Include)
- Studeny (2025): Digitale Werkzeuge und Machtasymmetrien?... (Score: 10/15, Include)
- Gravelmann (2024): K√ºnstliche Intelligenz in der Sozialen Arbeit ‚Äì Zwischen Bedenken und ... (Score: 10/15, Include)
- Strau√ü (2024): CAIL ‚Äì Critical AI Literacy: Kritische Technikkompetenz f√ºr konstrukti... (Score: 10/15, Include)
- Quaid-i-Azam University (2025): Gender Bias in Artificial Intelligence: Empowering Women Through Digit... (Score: 10/15, Include)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs through System 1... (Score: 10/15, Include)
- Lau (2023): Dipper: Diversity in Prompts for Producing Large Language Model Output... (Score: 10/15, Include)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias against Ara... (Score: 10/15, Include)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for practice... (Score: 10/15, Include)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for practice... (Score: 10/15, Include)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias against Ara... (Score: 10/15, Include)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for practice... (Score: 10/15, Include)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias against Ara... (Score: 10/15, Include)
- Salinas (2025): What‚Äôs in a name? Auditing large language models for race and gender b... (Score: 10/15, Include)
- Asseri (2025): Prompt engineering techniques for mitigating cultural bias against Ara... (Score: 10/15, Include)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs through System 1... (Score: 10/15, Include)
- Qiu (2025): DR.GAP: Mitigating bias in large language models using gender-aware pr... (Score: 10/15, Include)
- Cvoelcker (2023): Queer in AI: A case study in community-led participatory AI... (Score: 10/15, Include)
- Basseri (2025): Prompt Engineering Techniques for Mitigating Cultural Bias Against Ara... (Score: 10/15, Include)
- Ovalle (2024): Towards Substantive Equality in Artificial Intelligence: Transformativ... (Score: 10/15, Include)
- Giannoni Adielsson (2024): The AI Act, gender equality and non-discrimination: what role for the ... (Score: 10/15, Include)
- Wudel (2025): What is Feminist AI?... (Score: 10/15, Include)
- Ricaurte Quijano (2024): Towards substantive equality in artificial intelligence: Transformativ... (Score: 10/15, Include)
- Wudel (2025): What is Feminist AI?... (Score: 10/15, Include)
- Vethman (2025): Fairness Beyond the Algorithmic Frame: Actionable Recommendations for ... (Score: 10/15, Include)
- Raji (2024): The Algorithmic Auditing Landscape: A Social Justice Approach... (Score: 10/15, Include)
- Djeffal (2025): Reflexive prompt engineering: A framework for responsible prompt engin... (Score: 10/15, Include)
- Ciston (2024): Intersectional Artificial Intelligence Is Essential: Polyvocal, Multim... (Score: 10/15, Include)
- Hartshorne (2025): Generative AI and the Future of Digital Literacy: Opportunities for Ge... (Score: 10/15, Include)
- Klein (2024): Data Feminism for AI... (Score: 10/15, Include)
- Wudel (2025): What is Feminist AI?... (Score: 10/15, Include)
- Shah (2025): Gender Bias in Artificial Intelligence: Empowering Women Through Digit... (Score: 10/15, Include)
- A+ Alliance (2024): Incubating Feminist AI: Executive Summary 2021-2024... (Score: 10/15, Include)
- UN Women (2024): Artificial Intelligence and gender equality... (Score: 10/15, Include)
- UNESCO (2021): Recommendation on the Ethics of Artificial Intelligence... (Score: 10/15, Include)
- Browne (2024): Tech workers' perspectives on ethical issues in AI development: Foregr... (Score: 10/15, Include)
- Smith (2021): When Good Algorithms Go Sexist: Why and How to Advance AI Gender Equit... (Score: 10/15, Include)

---

<a id='papers_include'></a>

## üìó Include Papers

---
title: "Include Papers"
type: decision-moc
decision: Include
count: 222
date_created: 2025-11-10
tags: [moc, decision, include]
---

# üìó Include Papers

Total: 222 papers

- Cheng (2022): How child welfare workers reduce racial disparities in algorithmic dec...  (Relevance: 13/15)
- Hall (2024): A systematic review of sophisticated predictive and prescriptive analy... üìù (Relevance: 12/15)
- Kawakami (2022): Improving human-AI partnerships in child welfare: Understanding worker...  (Relevance: 12/15)
- McDonald (2023): Algorithmic decision-making in social work practice and pedagogy: Conf...  (Relevance: 12/15)
- Ahn (2025): Artificial Intelligence (AI) literacy for social work: Implications fo...  (Relevance: 12/15)
- Baker (2025): Artificial intelligence in social work: An EPIC model for practice...  (Relevance: 12/15)
- Linnemann (2025): K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen f√ºr Theorie ...  (Relevance: 12/15)
- Yu (2025): Algorithmic-Assisted Decision-Making Tools in Child Welfare Practice: ...  (Relevance: 12/15)
- Ahn (2025): Artificial Intelligence (AI) Literacy for Social Work: Implications fo...  (Relevance: 12/15)
- Rodriguez (2024): Introducing Generative Artificial Intelligence into the MSW Curriculum...  (Relevance: 12/15)
- Nuwasiima (2024): The role of artificial intelligence (AI) and machine learning in socia... üìù (Relevance: 12/15)
- Slesinger (2024): Training in Co-Creation as a Methodological Approach to Improve AI Fai... üìù (Relevance: 12/15)
- Lanzetta (2024): Artificial Intelligence Competence Needs for Youth Workers...  (Relevance: 11/15)
- Takaoka (2022): AI implementation science for social issues: Pitfalls and tips...  (Relevance: 11/15)
- Moreau (2024): Failing our youngest: On the biases, pitfalls, and risks in a decision...  (Relevance: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical issues... üìù (Relevance: 11/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit: Eine exe... üìù (Relevance: 11/15)
- Unknown (2025): Artificial Intelligence in Social Work: An EPIC Model for Practice... üìù (Relevance: 11/15)
- James (2023): Algorithmic decision-making in social work practice and pedagogy: conf...  (Relevance: 11/15)
- Feist-Ortmanns (2025): KI-basiertes Assistenzsystem im Kinderschutzverfahren... üìù (Relevance: 11/15)
- Ahn (2025): Artificial Intelligence (AI) literacy for social work...  (Relevance: 11/15)
- Alam (2025): Social work in the age of artificial intelligence: A rights-based fram...  (Relevance: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical issues... üìù (Relevance: 11/15)
- Ahrweiler (2025): AI FORA ‚Äì Artificial Intelligence for Assessment: Fairness bei der Ver...  (Relevance: 11/15)
- Victor (2023): Recommendations for social work researchers and journal editors on the...  (Relevance: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical issues... üìù (Relevance: 11/15)
- [Author not specified] (2025): Navigating the Nexus of Trust: Prompt Engineering, Professional Judgme...  (Relevance: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical issues... üìù (Relevance: 11/15)
- Victor (2023): Recommendations for social work researchers and journal editors on the...  (Relevance: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical issues... üìù (Relevance: 11/15)
- Victor (2023): Recommendations for social work researchers and journal editors on the...  (Relevance: 11/15)
- Shah (2025): Gender bias in artificial intelligence: Empowering women through digit...  (Relevance: 11/15)
- DIVERSIFAIR Project (2024): AI & Intersectionality: A Toolkit For Fairness & Inclusion... üìù (Relevance: 11/15)
- Marjanovic (2022): Theorising algorithmic justice...  (Relevance: 10/15)
- Li (2023): Ethics & AI: A systematic review on ethical concerns and related strat...  (Relevance: 10/15)
- Dencik (2024): Automated government benefits and welfare surveillance...  (Relevance: 10/15)
- Amnesty International (2024): Coded injustice: Surveillance and discrimination in Denmark's automate... üìù (Relevance: 10/15)
- van Toorn (2024): Introduction to the digital welfare state: Contestations, consideratio...  (Relevance: 10/15)
- Spaulding (2023): Predicting successful placements for youth in child welfare with machi...  (Relevance: 10/15)
- Cher (2024): Exploring machine learning to support decision-making for placement st...  (Relevance: 10/15)
- World Economic Forum (2024): AI for impact: The PRISM framework for responsible AI in social innova...  (Relevance: 10/15)
- Field (2023): Examining risks of racial biases in NLP tools for child protective ser...  (Relevance: 10/15)
- British Association of Social Workers (2025): Generative AI & social work practice guidance...  (Relevance: 10/15)
- Perron (2023): Recommendations for social work researchers and journal editors on the...  (Relevance: 10/15)
- Hodgson (2022): Problematising artificial intelligence in social work education: Chall...  (Relevance: 10/15)
- Schneider (2022): Exploring opportunities and risks in decision support technologies for...  (Relevance: 10/15)
- Schneider (2024): Das verflixte Problem mit Klassifikationen: Zum Einfluss der Digitalis...  (Relevance: 10/15)
- Steiner (2022): K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen, Entwicklung...  (Relevance: 10/15)
- Creswell B√°ez (2025): Clinical Social Workers‚Äô Perceptions of Large Language Models in Pract...  (Relevance: 10/15)
- Patton (2023): ChatGPT for Social Work Science: Ethical Challenges and Opportunities...  (Relevance: 10/15)
- Reamer (2023): Artificial Intelligence in Social Work: Emerging Ethical Issues... üìù (Relevance: 10/15)
- Boetto (2025): Artificial Intelligence in Social Work: An EPIC Model for Practice...  (Relevance: 10/15)
- Engelhardt (2025): Voll (dia)logisch? Ein Werkstattbericht √ºber den Einsatz von generativ... üìù (Relevance: 10/15)
- S≈´na (2024): Diskriminierung durch Algorithmen ‚Äì √úberlegungen zur St√§rkung KI-bezog...  (Relevance: 10/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit: Eine exe... üìù (Relevance: 10/15)
- Sch√∂nauer (2025): Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen...  (Relevance: 10/15)
- Hauck (2025): A framework for the learning and teaching of Critical AI Literacy skil...  (Relevance: 10/15)
- Chen (2025): Social work and artificial intelligence: Collaboration and challenges... üìù (Relevance: 10/15)
- Studeny (2025): Digitale Werkzeuge und Machtasymmetrien?... üìù (Relevance: 10/15)
- Gravelmann (2024): K√ºnstliche Intelligenz in der Sozialen Arbeit ‚Äì Zwischen Bedenken und ...  (Relevance: 10/15)
- Strau√ü (2024): CAIL ‚Äì Critical AI Literacy: Kritische Technikkompetenz f√ºr konstrukti...  (Relevance: 10/15)
- Quaid-i-Azam University (2025): Gender Bias in Artificial Intelligence: Empowering Women Through Digit... üìù (Relevance: 10/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs through System 1... üìù (Relevance: 10/15)
- Lau (2023): Dipper: Diversity in Prompts for Producing Large Language Model Output... üìù (Relevance: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias against Ara...  (Relevance: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for practice...  (Relevance: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for practice...  (Relevance: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias against Ara...  (Relevance: 10/15)
- Goldkind (2024): Artificial intelligence in social work: An EPIC model for practice...  (Relevance: 10/15)
- Asseri (2024): Prompt engineering techniques for mitigating cultural bias against Ara...  (Relevance: 10/15)
- Salinas (2025): What‚Äôs in a name? Auditing large language models for race and gender b...  (Relevance: 10/15)
- Asseri (2025): Prompt engineering techniques for mitigating cultural bias against Ara...  (Relevance: 10/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs through System 1... üìù (Relevance: 10/15)
- Qiu (2025): DR.GAP: Mitigating bias in large language models using gender-aware pr... üìù (Relevance: 10/15)
- Cvoelcker (2023): Queer in AI: A case study in community-led participatory AI... üìù (Relevance: 10/15)
- Basseri (2025): Prompt Engineering Techniques for Mitigating Cultural Bias Against Ara...  (Relevance: 10/15)
- Ovalle (2024): Towards Substantive Equality in Artificial Intelligence: Transformativ...  (Relevance: 10/15)
- Giannoni Adielsson (2024): The AI Act, gender equality and non-discrimination: what role for the ...  (Relevance: 10/15)
- Wudel (2025): What is Feminist AI?...  (Relevance: 10/15)
- Ricaurte Quijano (2024): Towards substantive equality in artificial intelligence: Transformativ... üìù (Relevance: 10/15)
- Wudel (2025): What is Feminist AI?...  (Relevance: 10/15)
- Vethman (2025): Fairness Beyond the Algorithmic Frame: Actionable Recommendations for ... üìù (Relevance: 10/15)
- Raji (2024): The Algorithmic Auditing Landscape: A Social Justice Approach...  (Relevance: 10/15)
- Djeffal (2025): Reflexive prompt engineering: A framework for responsible prompt engin...  (Relevance: 10/15)
- Ciston (2024): Intersectional Artificial Intelligence Is Essential: Polyvocal, Multim...  (Relevance: 10/15)
- Hartshorne (2025): Generative AI and the Future of Digital Literacy: Opportunities for Ge...  (Relevance: 10/15)
- Klein (2024): Data Feminism for AI... üìù (Relevance: 10/15)
- Wudel (2025): What is Feminist AI?...  (Relevance: 10/15)
- Shah (2025): Gender Bias in Artificial Intelligence: Empowering Women Through Digit...  (Relevance: 10/15)
- A+ Alliance (2024): Incubating Feminist AI: Executive Summary 2021-2024...  (Relevance: 10/15)
- UN Women (2024): Artificial Intelligence and gender equality... üìù (Relevance: 10/15)
- UNESCO (2021): Recommendation on the Ethics of Artificial Intelligence...  (Relevance: 10/15)
- Browne (2024): Tech workers' perspectives on ethical issues in AI development: Foregr...  (Relevance: 10/15)
- Smith (2021): When Good Algorithms Go Sexist: Why and How to Advance AI Gender Equit...  (Relevance: 10/15)
- Rodr√≠guez-Mart√≠nez (2024): Ethical issues related to the use of technology in social work practic...  (Relevance: 9/15)
- Meilvang (2024): Decision support and algorithmic support: The construction of algorith...  (Relevance: 9/15)
- J√∏rgensen (2023): Data and rights in the digital welfare state: the case of Denmark...  (Relevance: 9/15)
- Heinz (2025): Clinical trial of an LLM-based conversational AI psychotherapy...  (Relevance: 9/15)
- Keddell (2019): Algorithmic justice in child protection: Statistical fairness, social ...  (Relevance: 9/15)
- National Association of Social Workers (2017): NASW, ASWB, CSWE, & CSWA standards for technology in social work pract...  (Relevance: 9/15)
- James (2025): Responsible prompting recommendation: Fostering responsible AI practic...  (Relevance: 9/15)
- Kutscher (2024): Digitalit√§t und Digitalisierung als Gegenstand der Sozialen Arbeit...  (Relevance: 9/15)
- Unknown (2024): RETHINKING SOCIAL SERVICES WITH ARTIFICIAL INTELLIGENCE: OPPORTUNITIES...  (Relevance: 9/15)
- Schneider (2018): Der Einfluss der Algorithmen: Neue Qualit√§ten durch Big Data Analytics...  (Relevance: 9/15)
- Singer (2023): AI Creates the Message: Integrating AI Language Learning Models into S...  (Relevance: 9/15)
- Sch√∂nauer (2025): Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen...  (Relevance: 9/15)
- Gravelmann (2024): K√ºnstliche Intelligenz in der Sozialen Arbeit ‚Äì Zwischen Bedenken und ...  (Relevance: 9/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit... üìù (Relevance: 9/15)
- Biegelbauer (2023): Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden f√ºr KI in der... üìù (Relevance: 9/15)
- Singh (2025): A reparative turn in AI... üìù (Relevance: 9/15)
- Taeihagh (2025): Governance of generative AI: A comprehensive framework for navigating ...  (Relevance: 9/15)
- Ghosal (2025): Unequal voices: How LLMs construct constrained queer narratives... üìù (Relevance: 9/15)
- Tint (2025): Guardrails, not guidance: Understanding responses to LGBTQ+ language i... üìù (Relevance: 9/15)
- Petzel (2025): Prejudiced interactions with large language models (LLMs) reduce trust...  (Relevance: 9/15)
- Gallegos (2024): Bias and fairness in large language models: A survey...  (Relevance: 9/15)
- OECD (2023): Advancing Accountability in AI... üìù (Relevance: 9/15)
- Hayati (2024): How Far Can We Extract Diverse Perspectives from Large Language Models... üìù (Relevance: 9/15)
- Choudhury (2024): Large Language Models and User Trust: Consequence of Self-Referential ...  (Relevance: 9/15)
- Choudhury (2024): Large Language Models and User Trust: Consequence of Self-Referential ...  (Relevance: 9/15)
- UNESCO (2024): Women4Ethical AI: Global cooperation for gender-inclusive AI... üìù (Relevance: 9/15)
- Friedrich-Ebert-Stiftung (2025): The EU artificial intelligence act through a gender lens... üìù (Relevance: 9/15)
- Ahmed (2024): Feminist perspectives on AI: Ethical considerations in algorithmic dec...  (Relevance: 9/15)
- Charlesworth (2024): Flexible intersectional stereotype extraction (FISE): Analyzing inters...  (Relevance: 9/15)
- An (2025): Measuring gender and racial biases in large language models... üìù (Relevance: 9/15)
- UNESCO (2024): Bias against women and girls in large language models: A UNESCO study... üìù (Relevance: 9/15)
- Ulnicane (2024): Intersectionality in artificial intelligence: Framing concerns and rec...  (Relevance: 9/15)
- Ma (2023): Intersectional Stereotypes in Large Language Models: Dataset and Analy... üìù (Relevance: 9/15)
- West (2023): Discriminating Systems: Gender, Race, and Power in AI... üìù (Relevance: 9/15)
- Kong (2022): Are "Intersectionally Fair" AI Algorithms Really Fair to Women of Colo...  (Relevance: 9/15)
- Ricaurte (2024): How can feminism inform AI governance in practice?...  (Relevance: 9/15)
- Ovalle (2023): Factoring the Matrix of Domination: A Critical Review and Reimaginatio... üìù (Relevance: 9/15)
- Ulnicane (2024): Intersectionality in artificial intelligence: Framing concerns and rec...  (Relevance: 9/15)
- Kattnig (2024): Assessing trustworthy AI: Technical and legal perspectives of fairness... üìù (Relevance: 9/15)
- Karagianni (2025): Gender in a stereo-(gender)typical EU AI law: A feminist reading of th... üìù (Relevance: 9/15)
- Alvarez (2024): Policy advice and best practices on bias and fairness in AI... üìù (Relevance: 9/15)
- Klein (2024): Data Feminism for AI... üìù (Relevance: 9/15)
- Siapka (2023): Towards a Feminist Metaethics of AI... üìù (Relevance: 9/15)
- Ovalle (2023): Factoring the Matrix of Domination: A Critical Review and Reimaginatio... üìù (Relevance: 9/15)
- Sharma (2024): Intersectional analysis of visual generative AI: the case of stable di... üìù (Relevance: 9/15)
- Ghosal (2024): An empirical study of structural social and ethical challenges in AI... üìù (Relevance: 9/15)
- Shukla (2025): Investigating AI systems: examining data and algorithmic bias through ...  (Relevance: 9/15)
- Voutyrakou (2025): Algorithmic Governance: Gender Bias in AI-Generated Policymaking?...  (Relevance: 9/15)
- Thwaites (2024): Operationalizing positive-constructive pedagogy to artificial intellig...  (Relevance: 9/15)
- Ulnicane (2024): Intersectionality in Artificial Intelligence: Framing Concerns and Rec...  (Relevance: 9/15)
- Wajcman (2023): Feminism Confronts AI: The Gender Relations of Digitalisation...  (Relevance: 9/15)
- Benjamin (2023): Keynote Summary: The New Jim Code: Reimagining the Default Settings of...  (Relevance: 9/15)
- Ulnicane (2024): Intersectionality in Artificial Intelligence: Framing Concerns and Rec...  (Relevance: 9/15)
- D'Ignazio (2024): Data Feminism for AI...  (Relevance: 9/15)
- Guerra (2023): Feminist reflections for the development of artificial intelligence...  (Relevance: 9/15)
- Santy (2023): NLPositionality: Characterizing design biases of datasets and models...  (Relevance: 9/15)
- Shin (2024): Can prompt modifiers control bias? A comparative analysis of text-to-i...  (Relevance: 9/15)
- J√§√§skel√§inen (2025): Intersectional analysis of visual generative AI: The case of Stable Di...  (Relevance: 9/15)
- Fraile-Rojas (2025): Female perspectives on algorithmic bias: Implications for AI researche...  (Relevance: 9/15)
- Ahmed (2024): Feminist Perspectives on AI: Ethical Considerations in Algorithmic Dec...  (Relevance: 9/15)
- Gohar (2023): A Survey on Intersectional Fairness in Machine Learning: Opportunities... üìù (Relevance: 9/15)
- Klein (2024): Data feminism for AI... üìù (Relevance: 9/15)
- Browne (2023): Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligen...  (Relevance: 9/15)
- Ovalle (2023): Factoring the matrix of domination: A critical review and reimaginatio... üìù (Relevance: 9/15)
- Derechos Digitales (2023): Feminist reflections for the development of Artificial Intelligence...  (Relevance: 9/15)
- Ulnicane (2024): Intersectionality in Artificial Intelligence: Framing Concerns and Rec...  (Relevance: 9/15)
- UNESCO (2020): ARTIFICIAL INTELLIGENCE and GENDER EQUALITY...  (Relevance: 9/15)
- Sabour (2023): A chatbot for mental health support: Exploring the impact of Emohaa on...  (Relevance: 8/15)
- Siddals (2024): "It happened to be the perfect thing": Experiences of generative AI ch...  (Relevance: 8/15)
- Singer (2023): ChatGPT for social work science: Ethical challenges and opportunities...  (Relevance: 8/15)
- Garkisch (2024): Considering a unified model of artificial intelligence enhanced social...  (Relevance: 8/15)
- Schneider (2024): AI for decision support: What are possible futures, social impacts, re...  (Relevance: 8/15)
- Zakharova (2024): Tensions in digital welfare states: Three perspectives on care and con...  (Relevance: 8/15)
- Jarke (2025): Datafied ageing futures: Regimes of anticipation and participatory fut...  (Relevance: 8/15)
- Unknown (2025): Artificial Intelligence in Social Sciences and Social Work: Bridging T... üìù (Relevance: 8/15)
- Kutscher (2020): Handbuch Soziale Arbeit und Digitalisierung... üìù (Relevance: 8/15)
- Washington (2025): Fragile Foundations: Hidden Risks of Generative AI... üìù (Relevance: 8/15)
- Zeng (2025): Governing discriminatory content in conversational AI: A cross-system,...  (Relevance: 8/15)
- Barman (2024): Beyond transparency and explainability: On the need for adequate and c... üìù (Relevance: 8/15)
- An (2025): Measuring gender and racial biases in large language models: Intersect... üìù (Relevance: 8/15)
- European Data Protection Supervisor (2023): Explainable Artificial Intelligence... üìù (Relevance: 8/15)
- Colombatto (2025): The influence of mental state attributions on trust in large language ... üìù (Relevance: 8/15)
- Gaba (2025): Bias, accuracy, and trust: Gender-diverse perspectives on large langua... üìù (Relevance: 8/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs through System 1... üìù (Relevance: 8/15)
- Gaba (2025): Bias, accuracy, and trust: Gender-diverse perspectives on large langua... üìù (Relevance: 8/15)
- Colombatto (2025): The influence of mental state attributions on trust in large language ... üìù (Relevance: 8/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs through System 1... üìù (Relevance: 8/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs through System 1... üìù (Relevance: 8/15)
- Yuan (2025): The cultural stereotype and cultural bias of ChatGPT...  (Relevance: 8/15)
- Kumar (2024): How AI hype impacts the LGBTQ+ community...  (Relevance: 8/15)
- Yunusov (2024): MirrorStories: Reflecting Diversity through Personalized Narrative Gen... üìù (Relevance: 8/15)
- Laine (2025): Avoiding Catastrophe Through Intersectionality in Global AI Governance...  (Relevance: 8/15)
- Ulnicane (2024): Artificial Intelligence and Intersectionality...  (Relevance: 8/15)
- Djiberou Mahamadou (2024): Revisiting Technical Bias Mitigation Strategies... üìù (Relevance: 8/15)
- Lund (2025): Algorithms, artificial intelligence and discrimination... üìù (Relevance: 8/15)
- Wang (2024): Algorithmic discrimination: examining its types and regulatory measure...  (Relevance: 8/15)
- McCrory (2024): Avoiding catastrophe through intersectionality in global AI governance... üìù (Relevance: 8/15)
- Himmelreich (2022): Artificial Intelligence and Structural Injustice: Foundations for Equi... üìù (Relevance: 8/15)
- Lin (2022): Artificial Intelligence in a Structurally Unjust Society... üìù (Relevance: 8/15)
- Knowles (2023): Trustworthy AI and the Logics of Intersectional Resistance...  (Relevance: 8/15)
- Toupin (2024): Shaping feminist artificial intelligence...  (Relevance: 8/15)
- Ulnicane (2024): Intersectionality in Artificial Intelligence: Framing Concerns and Rec...  (Relevance: 8/15)
- Wilson (2024): AI tools show biases in ranking job applicants' names according to per...  (Relevance: 8/15)
- Gohar (2023): A Survey on Intersectional Fairness in Machine Learning: Notions, Miti... üìù (Relevance: 8/15)
- Toupin (2024): Shaping feminist artificial intelligence...  (Relevance: 8/15)
- Ulnicane (2024): Intersectionality in Artificial Intelligence: Framing Concerns and Rec...  (Relevance: 8/15)
- Wilson (2024): Gender, race, and intersectional bias in AI resume screening via langu...  (Relevance: 8/15)
- An (2025): Measuring gender and racial biases in large language models: Intersect... üìù (Relevance: 8/15)
- Toupin (2024): Shaping feminist artificial intelligence...  (Relevance: 8/15)
- N√§scher (2025): ReflectAI: Design and evaluation of an AI coach to support public serv...  (Relevance: 7/15)
- Schneider (2025): Indecision on the use of artificial intelligence in healthcare: A qual...  (Relevance: 7/15)
- Goldkind (2023): The End of the World as We Know It? ChatGPT and Social Work...  (Relevance: 7/15)
- Bai (2025): Explicitly unbiased large language models still form biased associatio...  (Relevance: 7/15)
- Navigli (2023): Biases in large language models: Origins, inventory and discussion...  (Relevance: 7/15)
- Tun (2025): Trust in artificial intelligence‚Äìbased clinical decision support syste... üìù (Relevance: 7/15)
- Xu (2023): Transparency enhances positive perceptions of social artificial intell...  (Relevance: 7/15)
- Tun (2025): Trust in artificial intelligence‚Äìbased clinical decision support syste... üìù (Relevance: 7/15)
- Xu (2023): Transparency enhances positive perceptions of social artificial intell...  (Relevance: 7/15)
- Latif (2023): AI gender bias, disparities, and fairness: Does training data matter?...  (Relevance: 7/15)
- Toupin (2024): Shaping feminist artificial intelligence...  (Relevance: 7/15)
- Browne (2024): Engineers on responsibility: feminist approaches to who's responsible ...  (Relevance: 7/15)
- Latif (2024): AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?...  (Relevance: 7/15)
- UNESCO (2024): Challenging systematic prejudices: an Investigation into Gender Bias i... üìù (Relevance: 7/15)
- Debnath (2024): Can LLMs reason about trust? A pilot study... üìù (Relevance: 6/15)
- Debnath (2024): Can LLMs reason about trust? A pilot study... üìù (Relevance: 6/15)
- Debnath (2024): Can LLMs reason about trust? A pilot study... üìù (Relevance: 6/15)
- Park (2025): AI algorithm transparency, pipelines for trust not prisms: mitigating ... üìù (Relevance: 5/15)
- Park (2025): AI algorithm transparency, pipelines for trust not prisms: mitigating ... üìù (Relevance: 5/15)

---

<a id='papers_low_relevance'></a>

## ‚≠ê Low Relevance Papers

---
title: "Low Relevance Papers"
type: relevance-moc
relevance_level: Low
count: 88
date_created: 2025-11-10
tags: [moc, relevance, low-relevance]
---

# ‚≠ê Low Relevance Papers

Total: 88 papers

- Bisconti (2024): A formal account of AI trustworthiness: Connecting intrinsic and perce... (Score: 4/15, Unclear)
- De Duro (2025): Measuring and identifying factors of individuals' trust in large langu... (Score: 4/15, Unclear)
- Bisconti (2024): A formal account of AI trustworthiness: Connecting intrinsic and perce... (Score: 4/15, Unclear)
- De Duro (2025): Measuring and identifying factors of individuals' trust in large langu... (Score: 4/15, Unclear)
- Bisconti (2024): A formal account of AI trustworthiness: Connecting intrinsic and perce... (Score: 4/15, Unclear)
- Chatterji (2025): How People Use ChatGPT... (Score: 0/15, Exclude)
- Benlian (2025): The AI literacy development canvas: Assessing and building AI literacy... (Score: 0/15, Exclude)
- Chiu (2025): AI literacy and competency: definitions, frameworks, development and f... (Score: 0/15, Exclude)
- Casal-Otero (2023): AI literacy in K-12: a systematic literature review... (Score: 0/15, Exclude)
- Chiu (2024): What are artificial intelligence literacy and competency? A comprehens... (Score: 0/15, Exclude)
- Kong (2021): Evaluation of an artificial intelligence literacy course for universit... (Score: 0/15, Exclude)
- Ng (2021): Conceptualizing AI literacy: An exploratory review... (Score: 0/15, Exclude)
- Peng (2022): A Literature Review of Digital Literacy over Two Decades... (Score: 0/15, Exclude)
- Tinmaz (2022): A systematic review on digital literacy... (Score: 0/15, Exclude)
- European Commission. Joint Research Centre. (2017): DigComp 2.1: the digital competence framework for citizens with eight ... (Score: 0/15, Exclude)
- Ng (2021): Conceptualizing AI literacy: An exploratory review... (Score: 0/15, Exclude)
- Pinski (2024): AI literacy for users ‚Äì A comprehensive review and future research dir... (Score: 0/15, Exclude)
- Arias L√≥pez (2023): Digital literacy as a new determinant of health: A scoping review... (Score: 0/15, Exclude)
- Yan (2024): Promises and challenges of generative artificial intelligence for huma... (Score: 0/15, Exclude)
- Pinski (2023): AI Literacy - Towards Measuring Human Competency in Artificial Intelli... (Score: 0/15, Exclude)
- Sperling (2024): In search of artificial intelligence (AI) literacy in teacher educatio... (Score: 0/15, Exclude)
- Jin (2025): GLAT: The generative AI literacy assessment test... (Score: 0/15, Exclude)
- Deuze (2022): Imagination, Algorithms and News: Developing AI Literacy for Journalis... (Score: 0/15, Exclude)
- Laupichler (2023): Development of the ‚ÄúScale for the assessment of non-experts‚Äô AI litera... (Score: 0/15, Exclude)
- Wang (2023): Measuring user competence in using artificial intelligence: validity a... (Score: 0/15, Exclude)
- Kong (2025): Artificial Intelligence (AI) literacy ‚Äì an argument for AI literacy in... (Score: 0/15, Exclude)
- Kong (2024): Developing an artificial intelligence literacy framework: Evaluation o... (Score: 0/15, Exclude)
- Wadmann (2020): 'Meaningless work': How the datafication of health reconfigures knowle... (Score: 0/15, Exclude)
- Goldkind (2024): The end of the world as we know it? ChatGPT and social work... (Score: 0/15, Exclude)
- Kutscher (2023): Positionings, challenges, and ambivalences in children's and parents' ... (Score: 0/15, Exclude)
- Fujii (2024): Bildungsteilhabe - Flucht - Digitalisierung: Eine multilokale Ethnogra... (Score: 0/15, Exclude)
- Jarke (2024): Who cares about data? Data care arrangements in everyday organisationa... (Score: 0/15, Exclude)
- Waag (2023): Rationalisierung durch Digitalisierung?... (Score: 0/15, Exclude)
- Freinhofer (2025): Prompten nach Plan: Das PCRR-Framework als p√§dagogisches Werkzeug f√ºr ... (Score: 0/15, Exclude)
- Weber (2023): Messung von AI Literacy ‚Äì Empirische Evidenz und Implikationen... (Score: 0/15, Exclude)
- Unknown (2024): Research on the application risks and countermeasures of ChatGPT gener... (Score: 0/15, Exclude)
- Ruiz (2024): AI Literacy: A Framework to Understand, Evaluate, and Use Emerging Tec... (Score: 0/15, Exclude)
- Unknown (2024): AI competency framework for students... (Score: 0/15, Exclude)
- Unknown (2024): Research on the application risks and countermeasures of ChatGPT gener... (Score: 0/15, Exclude)
- Zhang (2025): Learning About AI: A Systematic Review of Reviews on AI Literacy... (Score: 0/15, Exclude)
- Ahn (2025): Artificial Intelligence (AI) Literacy for Social Work: Implications fo... (Score: 0/15, Exclude)
- Long (2020): What is AI Literacy? Competencies and Design Considerations... (Score: 0/15, Exclude)
- Santos (2025): How large language models judge cooperation... (Score: 0/15, Exclude)
- Santos (2024): Explainability through systematicity: The hard systematicity challenge... (Score: 0/15, Exclude)
- Lahoti (2023): Improving diversity of demographic representation in people entities i... (Score: 0/15, Exclude)
- Wang (2024): Multilingual Prompting for Improving LLM Generation Diversity... (Score: 0/15, Exclude)
- Wu (2025): Bias in decision-making for AI's ethical dilemmas: A comparative study... (Score: 0/15, Exclude)
- Struppek (2024): Homoglyph unlearning: A novel approach to bias mitigation... (Score: 0/15, Exclude)
- Srinivasan (2024): Worst of both worlds: A comparative analysis of error in language and ... (Score: 0/15, Exclude)
- Prakash (2023): Prompt engineering techniques for mitigating cultural bias against Ara... (Score: 0/15, Exclude)
- Zayed (2024): Scaling implicit bias analysis across transformer-based language model... (Score: 0/15, Exclude)
- Wang (2024): A survey on fairness in large language models... (Score: 0/15, Exclude)
- U.S. Bureau of Labor Statistics (2023): Occupational employment statistics... (Score: 0/15, Exclude)
- Shin (2025): Mitigating age-related bias in large language models: Strategies for r... (Score: 0/15, Exclude)
- Salecha (2025): Model explanations for gender and ethnicity bias mitigation in AI-gene... (Score: 0/15, Exclude)
- Parrish (2025): Self-debiasing large language models: Zero-shot recognition and reduct... (Score: 0/15, Exclude)
- Mei (2023): Assessing GPT's bias towards stigmatized social groups: An intersectio... (Score: 0/15, Exclude)
- Kojima (2022): Large language models are zero-shot reasoners... (Score: 0/15, Exclude)
- Klinge (2024): A sociolinguistic approach to stereotype assessment in large language ... (Score: 0/15, Exclude)
- Kaneko (2024): Debiasing prompts for gender bias in large language models... (Score: 0/15, Exclude)
- He (2024): On the steerability of large language models... (Score: 0/15, Exclude)
- Parrish (2022): BBQ: A hand-built bias benchmark for question answering... (Score: 0/15, Exclude)
- Liu (2025): More or less wrong: A benchmark for directional bias in LLM comparativ... (Score: 0/15, Exclude)
- Lin (2024): SWIFTSAGE: A new dual-module framework for better action planning in c... (Score: 0/15, Exclude)
- Jiang (2022): Assessing GPT's bias towards stigmatized social groups: An intersectio... (Score: 0/15, Exclude)
- Garg (2019): Counterfactual fairness in text classification through robustness... (Score: 0/15, Exclude)
- Furniturewala (2024): Reasoning towards fairness: Mitigating bias in language models through... (Score: 0/15, Exclude)
- Dixon (2018): Measuring and mitigating unintended bias in text data... (Score: 0/15, Exclude)
- Chisca (2024): Prompting techniques for reducing social bias in LLMs through System 1... (Score: 0/15, Exclude)
- Kaneko (2024): Evaluating gender bias in large language models via chain-of-thought p... (Score: 0/15, Exclude)
- Chen (2024): Exploring complex mental health symptoms via classifying social media ... (Score: 0/15, Exclude)
- Birru (2024): Mitigating age-related bias in large language models: Strategies for r... (Score: 0/15, Exclude)
- Wang (2025): Multilingual Prompting for Improving LLM Generation Diversity... (Score: 0/15, Exclude)
- Zannone (2023): Intersectional Fairness: A Fractal Approach... (Score: 0/15, Exclude)
- Dilek (2025): AI literacy in teacher education: Empowering educators through critica... (Score: 0/15, Exclude)
- Srivastava (2024): Algorithmic Governance and the International Politics of Big Tech... (Score: 0/15, Exclude)
- Sinders (2017): Feminist Data Set... (Score: 0/15, Exclude)
- Clemmer (2024): PreciseDebias: An automatic prompt engineering approach for generative... (Score: 0/15, Exclude)
- Gengler (2024): Faires KI-Prompting ‚Äì Ein Leitfaden f√ºr Unternehmen... (Score: 0/15, Exclude)
- Skilton (2024): Inclusive prompt engineering: A methodology for hacking biased AI imag... (Score: 0/15, Exclude)
- Articulate (2025): How to Create Inclusive AI Images: A Guide to Bias-Free Prompting... (Score: 0/15, Exclude)
- Chisca (2024): Prompting fairness: Learning prompts for debiasing large language mode... (Score: 0/15, Exclude)
- Sant (2024): The power of prompts: Evaluating and mitigating gender bias in MT with... (Score: 0/15, Exclude)
- Gallegos (2024): Bias and fairness in large language models: A survey... (Score: 0/15, Exclude)
- Walgenbach (2023): Intersektionalit√§t... (Score: 0/15, Exclude)
- Franken (): Gender und KI-Anwendungen. Tr√§gt KI zum Genderproblem oder zu seiner L... (Score: 0/15, Exclude)
- _ACADEMY|Unknown (): feminist AI | ACADEMY... (Score: 0/15, Exclude)
- Gengler (2024): Faires KIPrompting ‚Äì Ein Leitfaden f√ºr Unternehmen. BSP Business and L... (Score: 0/15, Exclude)

---

<a id='papers_medium_relevance'></a>

## ‚≠ê‚≠ê Medium Relevance Papers

---
title: "Medium Relevance Papers"
type: relevance-moc
relevance_level: Medium
count: 142
date_created: 2025-11-10
tags: [moc, relevance, medium-relevance]
---

# ‚≠ê‚≠ê Medium Relevance Papers

Total: 142 papers

- Rodr√≠guez-Mart√≠nez (2024): Ethical issues related to the use of technology in social work practic... (Score: 9/15, Include)
- Meilvang (2024): Decision support and algorithmic support: The construction of algorith... (Score: 9/15, Include)
- J√∏rgensen (2023): Data and rights in the digital welfare state: the case of Denmark... (Score: 9/15, Include)
- Heinz (2025): Clinical trial of an LLM-based conversational AI psychotherapy... (Score: 9/15, Include)
- Keddell (2019): Algorithmic justice in child protection: Statistical fairness, social ... (Score: 9/15, Include)
- National Association of Social Workers (2017): NASW, ASWB, CSWE, & CSWA standards for technology in social work pract... (Score: 9/15, Include)
- James (2025): Responsible prompting recommendation: Fostering responsible AI practic... (Score: 9/15, Include)
- Kutscher (2024): Digitalit√§t und Digitalisierung als Gegenstand der Sozialen Arbeit... (Score: 9/15, Include)
- Unknown (2024): RETHINKING SOCIAL SERVICES WITH ARTIFICIAL INTELLIGENCE: OPPORTUNITIES... (Score: 9/15, Include)
- Schneider (2018): Der Einfluss der Algorithmen: Neue Qualit√§ten durch Big Data Analytics... (Score: 9/15, Include)
- Singer (2023): AI Creates the Message: Integrating AI Language Learning Models into S... (Score: 9/15, Include)
- Sch√∂nauer (2025): Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen... (Score: 9/15, Include)
- Gravelmann (2024): K√ºnstliche Intelligenz in der Sozialen Arbeit ‚Äì Zwischen Bedenken und ... (Score: 9/15, Include)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit... (Score: 9/15, Include)
- Biegelbauer (2023): Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden f√ºr KI in der... (Score: 9/15, Include)
- Singh (2025): A reparative turn in AI... (Score: 9/15, Include)
- Taeihagh (2025): Governance of generative AI: A comprehensive framework for navigating ... (Score: 9/15, Include)
- Ghosal (2025): Unequal voices: How LLMs construct constrained queer narratives... (Score: 9/15, Include)
- Tint (2025): Guardrails, not guidance: Understanding responses to LGBTQ+ language i... (Score: 9/15, Include)
- Petzel (2025): Prejudiced interactions with large language models (LLMs) reduce trust... (Score: 9/15, Include)
- Gallegos (2024): Bias and fairness in large language models: A survey... (Score: 9/15, Include)
- OECD (2023): Advancing Accountability in AI... (Score: 9/15, Include)
- Hayati (2024): How Far Can We Extract Diverse Perspectives from Large Language Models... (Score: 9/15, Include)
- Choudhury (2024): Large Language Models and User Trust: Consequence of Self-Referential ... (Score: 9/15, Include)
- Choudhury (2024): Large Language Models and User Trust: Consequence of Self-Referential ... (Score: 9/15, Include)
- UNESCO (2024): Women4Ethical AI: Global cooperation for gender-inclusive AI... (Score: 9/15, Include)
- Friedrich-Ebert-Stiftung (2025): The EU artificial intelligence act through a gender lens... (Score: 9/15, Include)
- Ahmed (2024): Feminist perspectives on AI: Ethical considerations in algorithmic dec... (Score: 9/15, Include)
- Charlesworth (2024): Flexible intersectional stereotype extraction (FISE): Analyzing inters... (Score: 9/15, Include)
- An (2025): Measuring gender and racial biases in large language models... (Score: 9/15, Include)
- UNESCO (2024): Bias against women and girls in large language models: A UNESCO study... (Score: 9/15, Include)
- Ulnicane (2024): Intersectionality in artificial intelligence: Framing concerns and rec... (Score: 9/15, Include)
- Ma (2023): Intersectional Stereotypes in Large Language Models: Dataset and Analy... (Score: 9/15, Include)
- West (2023): Discriminating Systems: Gender, Race, and Power in AI... (Score: 9/15, Include)
- Kong (2022): Are "Intersectionally Fair" AI Algorithms Really Fair to Women of Colo... (Score: 9/15, Include)
- Ricaurte (2024): How can feminism inform AI governance in practice?... (Score: 9/15, Include)
- Ovalle (2023): Factoring the Matrix of Domination: A Critical Review and Reimaginatio... (Score: 9/15, Include)
- Ulnicane (2024): Intersectionality in artificial intelligence: Framing concerns and rec... (Score: 9/15, Include)
- Kattnig (2024): Assessing trustworthy AI: Technical and legal perspectives of fairness... (Score: 9/15, Include)
- Karagianni (2025): Gender in a stereo-(gender)typical EU AI law: A feminist reading of th... (Score: 9/15, Include)
- Alvarez (2024): Policy advice and best practices on bias and fairness in AI... (Score: 9/15, Include)
- Klein (2024): Data Feminism for AI... (Score: 9/15, Include)
- Siapka (2023): Towards a Feminist Metaethics of AI... (Score: 9/15, Include)
- Ovalle (2023): Factoring the Matrix of Domination: A Critical Review and Reimaginatio... (Score: 9/15, Include)
- Sharma (2024): Intersectional analysis of visual generative AI: the case of stable di... (Score: 9/15, Include)
- Ghosal (2024): An empirical study of structural social and ethical challenges in AI... (Score: 9/15, Include)
- Attard-Frost (2025): AI Countergovernance: Lessons Learned from Canada and Paris... (Score: 9/15, Unclear)
- Shukla (2025): Investigating AI systems: examining data and algorithmic bias through ... (Score: 9/15, Include)
- Voutyrakou (2025): Algorithmic Governance: Gender Bias in AI-Generated Policymaking?... (Score: 9/15, Include)
- Thwaites (2024): Operationalizing positive-constructive pedagogy to artificial intellig... (Score: 9/15, Include)
- Ulnicane (2024): Intersectionality in Artificial Intelligence: Framing Concerns and Rec... (Score: 9/15, Include)
- Wajcman (2023): Feminism Confronts AI: The Gender Relations of Digitalisation... (Score: 9/15, Include)
- Benjamin (2023): Keynote Summary: The New Jim Code: Reimagining the Default Settings of... (Score: 9/15, Include)
- Ulnicane (2024): Intersectionality in Artificial Intelligence: Framing Concerns and Rec... (Score: 9/15, Include)
- Zhao (2025): Thinking like a scientist: Can interactive simulations foster critical... (Score: 9/15, Unclear)
- D'Ignazio (2024): Data Feminism for AI... (Score: 9/15, Include)
- Guerra (2023): Feminist reflections for the development of artificial intelligence... (Score: 9/15, Include)
- Santy (2023): NLPositionality: Characterizing design biases of datasets and models... (Score: 9/15, Include)
- Shin (2024): Can prompt modifiers control bias? A comparative analysis of text-to-i... (Score: 9/15, Include)
- J√§√§skel√§inen (2025): Intersectional analysis of visual generative AI: The case of Stable Di... (Score: 9/15, Include)
- Fraile-Rojas (2025): Female perspectives on algorithmic bias: Implications for AI researche... (Score: 9/15, Include)
- Ahmed (2024): Feminist Perspectives on AI: Ethical Considerations in Algorithmic Dec... (Score: 9/15, Include)
- Gohar (2023): A Survey on Intersectional Fairness in Machine Learning: Opportunities... (Score: 9/15, Include)
- Klein (2024): Data feminism for AI... (Score: 9/15, Include)
- Browne (2023): Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligen... (Score: 9/15, Include)
- Ovalle (2023): Factoring the matrix of domination: A critical review and reimaginatio... (Score: 9/15, Include)
- Derechos Digitales (2023): Feminist reflections for the development of Artificial Intelligence... (Score: 9/15, Include)
- Ulnicane (2024): Intersectionality in Artificial Intelligence: Framing Concerns and Rec... (Score: 9/15, Include)
- UNESCO (2020): ARTIFICIAL INTELLIGENCE and GENDER EQUALITY... (Score: 9/15, Include)
- Sabour (2023): A chatbot for mental health support: Exploring the impact of Emohaa on... (Score: 8/15, Include)
- Siddals (2024): "It happened to be the perfect thing": Experiences of generative AI ch... (Score: 8/15, Include)
- Singer (2023): ChatGPT for social work science: Ethical challenges and opportunities... (Score: 8/15, Include)
- Garkisch (2024): Considering a unified model of artificial intelligence enhanced social... (Score: 8/15, Include)
- Schneider (2024): AI for decision support: What are possible futures, social impacts, re... (Score: 8/15, Include)
- Zakharova (2024): Tensions in digital welfare states: Three perspectives on care and con... (Score: 8/15, Include)
- Jarke (2025): Datafied ageing futures: Regimes of anticipation and participatory fut... (Score: 8/15, Include)
- Unknown (2025): Artificial Intelligence in Social Sciences and Social Work: Bridging T... (Score: 8/15, Include)
- Kutscher (2020): Handbuch Soziale Arbeit und Digitalisierung... (Score: 8/15, Include)
- Washington (2025): Fragile Foundations: Hidden Risks of Generative AI... (Score: 8/15, Include)
- Zeng (2025): Governing discriminatory content in conversational AI: A cross-system,... (Score: 8/15, Include)
- Pan (2025): LIBRA: Measuring bias of large language model from a local context... (Score: 8/15, Unclear)
- Barman (2024): Beyond transparency and explainability: On the need for adequate and c... (Score: 8/15, Include)
- An (2025): Measuring gender and racial biases in large language models: Intersect... (Score: 8/15, Include)
- European Data Protection Supervisor (2023): Explainable Artificial Intelligence... (Score: 8/15, Include)
- Colombatto (2025): The influence of mental state attributions on trust in large language ... (Score: 8/15, Include)
- Gaba (2025): Bias, accuracy, and trust: Gender-diverse perspectives on large langua... (Score: 8/15, Include)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs through System 1... (Score: 8/15, Include)
- Gaba (2025): Bias, accuracy, and trust: Gender-diverse perspectives on large langua... (Score: 8/15, Include)
- Colombatto (2025): The influence of mental state attributions on trust in large language ... (Score: 8/15, Include)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs through System 1... (Score: 8/15, Include)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs through System 1... (Score: 8/15, Include)
- Yuan (2025): The cultural stereotype and cultural bias of ChatGPT... (Score: 8/15, Include)
- Kumar (2024): How AI hype impacts the LGBTQ+ community... (Score: 8/15, Include)
- Yunusov (2024): MirrorStories: Reflecting Diversity through Personalized Narrative Gen... (Score: 8/15, Include)
- Laine (2025): Avoiding Catastrophe Through Intersectionality in Global AI Governance... (Score: 8/15, Include)
- Ulnicane (2024): Artificial Intelligence and Intersectionality... (Score: 8/15, Include)
- Djiberou Mahamadou (2024): Revisiting Technical Bias Mitigation Strategies... (Score: 8/15, Include)
- Lund (2025): Algorithms, artificial intelligence and discrimination... (Score: 8/15, Include)
- Wang (2024): Algorithmic discrimination: examining its types and regulatory measure... (Score: 8/15, Include)
- McCrory (2024): Avoiding catastrophe through intersectionality in global AI governance... (Score: 8/15, Include)
- Himmelreich (2022): Artificial Intelligence and Structural Injustice: Foundations for Equi... (Score: 8/15, Include)
- Lin (2022): Artificial Intelligence in a Structurally Unjust Society... (Score: 8/15, Include)
- Knowles (2023): Trustworthy AI and the Logics of Intersectional Resistance... (Score: 8/15, Include)
- Toupin (2024): Shaping feminist artificial intelligence... (Score: 8/15, Include)
- Ulnicane (2024): Intersectionality in Artificial Intelligence: Framing Concerns and Rec... (Score: 8/15, Include)
- Wilson (2024): AI tools show biases in ranking job applicants' names according to per... (Score: 8/15, Include)
- Gohar (2023): A Survey on Intersectional Fairness in Machine Learning: Notions, Miti... (Score: 8/15, Include)
- Toupin (2024): Shaping feminist artificial intelligence... (Score: 8/15, Include)
- Ulnicane (2024): Intersectionality in Artificial Intelligence: Framing Concerns and Rec... (Score: 8/15, Include)
- Wilson (2024): Gender, race, and intersectional bias in AI resume screening via langu... (Score: 8/15, Include)
- An (2025): Measuring gender and racial biases in large language models: Intersect... (Score: 8/15, Include)
- Toupin (2024): Shaping feminist artificial intelligence... (Score: 8/15, Include)
- Chee (2025): A Competency Framework for AI Literacy: Variations by Different Learne... (Score: 7/15, Unclear)
- Biagini (2024): Less knowledge, more trust? Exploring potentially uncritical attitudes... (Score: 7/15, Unclear)
- N√§scher (2025): ReflectAI: Design and evaluation of an AI coach to support public serv... (Score: 7/15, Include)
- Schneider (2025): Indecision on the use of artificial intelligence in healthcare: A qual... (Score: 7/15, Include)
- Goldkind (2023): The End of the World as We Know It? ChatGPT and Social Work... (Score: 7/15, Include)
- Goellner (2025): Towards responsible AI for education: Hybrid human-AI to confront the ... (Score: 7/15, Unclear)
- Bai (2025): Explicitly unbiased large language models still form biased associatio... (Score: 7/15, Include)
- Navigli (2023): Biases in large language models: Origins, inventory and discussion... (Score: 7/15, Include)
- Tun (2025): Trust in artificial intelligence‚Äìbased clinical decision support syste... (Score: 7/15, Include)
- Xu (2023): Transparency enhances positive perceptions of social artificial intell... (Score: 7/15, Include)
- Tun (2025): Trust in artificial intelligence‚Äìbased clinical decision support syste... (Score: 7/15, Include)
- Xu (2023): Transparency enhances positive perceptions of social artificial intell... (Score: 7/15, Include)
- Latif (2023): AI gender bias, disparities, and fairness: Does training data matter?... (Score: 7/15, Include)
- Toupin (2024): Shaping feminist artificial intelligence... (Score: 7/15, Include)
- Browne (2024): Engineers on responsibility: feminist approaches to who's responsible ... (Score: 7/15, Include)
- Mosene (2023): Feministische Netzpolitik und K√ºnstliche Intelligenz in der politische... (Score: 7/15, Unclear)
- Latif (2024): AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?... (Score: 7/15, Include)
- UNESCO (2024): Challenging systematic prejudices: an Investigation into Gender Bias i... (Score: 7/15, Include)
- Jarrahi (2021): Algorithmic management in a work context... (Score: 6/15, Unclear)
- Srinivasan (2025): Mitigating trust-induced inappropriate reliance on AI assistance... (Score: 6/15, Unclear)
- Debnath (2024): Can LLMs reason about trust? A pilot study... (Score: 6/15, Include)
- Steyvers (2025): What large language models know and what people think they know... (Score: 6/15, Unclear)
- Srinivasan (2025): Mitigating trust-induced inappropriate reliance on AI assistance... (Score: 6/15, Unclear)
- Debnath (2024): Can LLMs reason about trust? A pilot study... (Score: 6/15, Include)
- Steyvers (2025): What large language models know and what people think they know... (Score: 6/15, Unclear)
- Srinivasan (2025): Mitigating trust-induced inappropriate reliance on AI assistance... (Score: 6/15, Unclear)
- Debnath (2024): Can LLMs reason about trust? A pilot study... (Score: 6/15, Include)
- Small (2023): Generative AI and opportunities for feminist classroom assignments... (Score: 6/15, Unclear)
- Park (2025): AI algorithm transparency, pipelines for trust not prisms: mitigating ... (Score: 5/15, Include)
- Park (2025): AI algorithm transparency, pipelines for trust not prisms: mitigating ... (Score: 5/15, Include)

---

<a id='papers_unclear'></a>

## üìô Unclear Papers

---
title: "Unclear Papers"
type: decision-moc
decision: Unclear
count: 20
date_created: 2025-11-10
tags: [moc, decision, unclear]
---

# üìô Unclear Papers

Total: 20 papers

- Kubes (2024): Feministische KI ‚Äì K√ºnstliche Intelligenz f√ºr alle? [Feminist AI ‚Äì Art...  (Relevance: 11/15)
- Attard-Frost (2025): AI Countergovernance: Lessons Learned from Canada and Paris...  (Relevance: 9/15)
- Zhao (2025): Thinking like a scientist: Can interactive simulations foster critical...  (Relevance: 9/15)
- Pan (2025): LIBRA: Measuring bias of large language model from a local context...  (Relevance: 8/15)
- Chee (2025): A Competency Framework for AI Literacy: Variations by Different Learne...  (Relevance: 7/15)
- Biagini (2024): Less knowledge, more trust? Exploring potentially uncritical attitudes...  (Relevance: 7/15)
- Goellner (2025): Towards responsible AI for education: Hybrid human-AI to confront the ...  (Relevance: 7/15)
- Mosene (2023): Feministische Netzpolitik und K√ºnstliche Intelligenz in der politische...  (Relevance: 7/15)
- Jarrahi (2021): Algorithmic management in a work context...  (Relevance: 6/15)
- Srinivasan (2025): Mitigating trust-induced inappropriate reliance on AI assistance...  (Relevance: 6/15)
- Steyvers (2025): What large language models know and what people think they know...  (Relevance: 6/15)
- Srinivasan (2025): Mitigating trust-induced inappropriate reliance on AI assistance...  (Relevance: 6/15)
- Steyvers (2025): What large language models know and what people think they know...  (Relevance: 6/15)
- Srinivasan (2025): Mitigating trust-induced inappropriate reliance on AI assistance...  (Relevance: 6/15)
- Small (2023): Generative AI and opportunities for feminist classroom assignments...  (Relevance: 6/15)
- Bisconti (2024): A formal account of AI trustworthiness: Connecting intrinsic and perce...  (Relevance: 4/15)
- De Duro (2025): Measuring and identifying factors of individuals' trust in large langu...  (Relevance: 4/15)
- Bisconti (2024): A formal account of AI trustworthiness: Connecting intrinsic and perce...  (Relevance: 4/15)
- De Duro (2025): Measuring and identifying factors of individuals' trust in large langu...  (Relevance: 4/15)
- Bisconti (2024): A formal account of AI trustworthiness: Connecting intrinsic and perce...  (Relevance: 4/15)

---

<a id='papers_with_summaries'></a>

## üìù Papers with AI Summaries

---
title: "Papers with AI Summaries"
type: summaries-moc
count: 83
date_created: 2025-11-10
tags: [moc, summaries]
---

# üìù Papers with AI Summaries

Papers that have been processed through the AI summarization pipeline (Claude Haiku 4.5).

Total: 83 papers

- Hall (2024): A systematic review of sophisticated predictive and prescriptive analy... (Relevance: 12/15)
- Nuwasiima (2024): The role of artificial intelligence (AI) and machine learning in socia... (Relevance: 12/15)
- Slesinger (2024): Training in Co-Creation as a Methodological Approach to Improve AI Fai... (Relevance: 12/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical issues... (Relevance: 11/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit: Eine exe... (Relevance: 11/15)
- Unknown (2025): Artificial Intelligence in Social Work: An EPIC Model for Practice... (Relevance: 11/15)
- Feist-Ortmanns (2025): KI-basiertes Assistenzsystem im Kinderschutzverfahren... (Relevance: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical issues... (Relevance: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical issues... (Relevance: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical issues... (Relevance: 11/15)
- Reamer (2023): Artificial intelligence in social work: Emerging ethical issues... (Relevance: 11/15)
- DIVERSIFAIR Project (2024): AI & Intersectionality: A Toolkit For Fairness & Inclusion... (Relevance: 11/15)
- Amnesty International (2024): Coded injustice: Surveillance and discrimination in Denmark's automate... (Relevance: 10/15)
- Reamer (2023): Artificial Intelligence in Social Work: Emerging Ethical Issues... (Relevance: 10/15)
- Engelhardt (2025): Voll (dia)logisch? Ein Werkstattbericht √ºber den Einsatz von generativ... (Relevance: 10/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit: Eine exe... (Relevance: 10/15)
- Chen (2025): Social work and artificial intelligence: Collaboration and challenges... (Relevance: 10/15)
- Studeny (2025): Digitale Werkzeuge und Machtasymmetrien?... (Relevance: 10/15)
- Quaid-i-Azam University (2025): Gender Bias in Artificial Intelligence: Empowering Women Through Digit... (Relevance: 10/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs through System 1... (Relevance: 10/15)
- Lau (2023): Dipper: Diversity in Prompts for Producing Large Language Model Output... (Relevance: 10/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs through System 1... (Relevance: 10/15)
- Qiu (2025): DR.GAP: Mitigating bias in large language models using gender-aware pr... (Relevance: 10/15)
- Cvoelcker (2023): Queer in AI: A case study in community-led participatory AI... (Relevance: 10/15)
- Ricaurte Quijano (2024): Towards substantive equality in artificial intelligence: Transformativ... (Relevance: 10/15)
- Vethman (2025): Fairness Beyond the Algorithmic Frame: Actionable Recommendations for ... (Relevance: 10/15)
- Klein (2024): Data Feminism for AI... (Relevance: 10/15)
- UN Women (2024): Artificial Intelligence and gender equality... (Relevance: 10/15)
- Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit... (Relevance: 9/15)
- Biegelbauer (2023): Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden f√ºr KI in der... (Relevance: 9/15)
- Singh (2025): A reparative turn in AI... (Relevance: 9/15)
- Ghosal (2025): Unequal voices: How LLMs construct constrained queer narratives... (Relevance: 9/15)
- Tint (2025): Guardrails, not guidance: Understanding responses to LGBTQ+ language i... (Relevance: 9/15)
- OECD (2023): Advancing Accountability in AI... (Relevance: 9/15)
- Hayati (2024): How Far Can We Extract Diverse Perspectives from Large Language Models... (Relevance: 9/15)
- UNESCO (2024): Women4Ethical AI: Global cooperation for gender-inclusive AI... (Relevance: 9/15)
- Friedrich-Ebert-Stiftung (2025): The EU artificial intelligence act through a gender lens... (Relevance: 9/15)
- An (2025): Measuring gender and racial biases in large language models... (Relevance: 9/15)
- UNESCO (2024): Bias against women and girls in large language models: A UNESCO study... (Relevance: 9/15)
- Ma (2023): Intersectional Stereotypes in Large Language Models: Dataset and Analy... (Relevance: 9/15)
- West (2023): Discriminating Systems: Gender, Race, and Power in AI... (Relevance: 9/15)
- Ovalle (2023): Factoring the Matrix of Domination: A Critical Review and Reimaginatio... (Relevance: 9/15)
- Kattnig (2024): Assessing trustworthy AI: Technical and legal perspectives of fairness... (Relevance: 9/15)
- Karagianni (2025): Gender in a stereo-(gender)typical EU AI law: A feminist reading of th... (Relevance: 9/15)
- Alvarez (2024): Policy advice and best practices on bias and fairness in AI... (Relevance: 9/15)
- Klein (2024): Data Feminism for AI... (Relevance: 9/15)
- Siapka (2023): Towards a Feminist Metaethics of AI... (Relevance: 9/15)
- Ovalle (2023): Factoring the Matrix of Domination: A Critical Review and Reimaginatio... (Relevance: 9/15)
- Sharma (2024): Intersectional analysis of visual generative AI: the case of stable di... (Relevance: 9/15)
- Ghosal (2024): An empirical study of structural social and ethical challenges in AI... (Relevance: 9/15)
- Gohar (2023): A Survey on Intersectional Fairness in Machine Learning: Opportunities... (Relevance: 9/15)
- Klein (2024): Data feminism for AI... (Relevance: 9/15)
- Ovalle (2023): Factoring the matrix of domination: A critical review and reimaginatio... (Relevance: 9/15)
- Unknown (2025): Artificial Intelligence in Social Sciences and Social Work: Bridging T... (Relevance: 8/15)
- Kutscher (2020): Handbuch Soziale Arbeit und Digitalisierung... (Relevance: 8/15)
- Washington (2025): Fragile Foundations: Hidden Risks of Generative AI... (Relevance: 8/15)
- Barman (2024): Beyond transparency and explainability: On the need for adequate and c... (Relevance: 8/15)
- An (2025): Measuring gender and racial biases in large language models: Intersect... (Relevance: 8/15)
- European Data Protection Supervisor (2023): Explainable Artificial Intelligence... (Relevance: 8/15)
- Colombatto (2025): The influence of mental state attributions on trust in large language ... (Relevance: 8/15)
- Gaba (2025): Bias, accuracy, and trust: Gender-diverse perspectives on large langua... (Relevance: 8/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs through System 1... (Relevance: 8/15)
- Gaba (2025): Bias, accuracy, and trust: Gender-diverse perspectives on large langua... (Relevance: 8/15)
- Colombatto (2025): The influence of mental state attributions on trust in large language ... (Relevance: 8/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs through System 1... (Relevance: 8/15)
- Kamruzzaman (2024): Prompting techniques for reducing social bias in LLMs through System 1... (Relevance: 8/15)
- Yunusov (2024): MirrorStories: Reflecting Diversity through Personalized Narrative Gen... (Relevance: 8/15)
- Djiberou Mahamadou (2024): Revisiting Technical Bias Mitigation Strategies... (Relevance: 8/15)
- Lund (2025): Algorithms, artificial intelligence and discrimination... (Relevance: 8/15)
- McCrory (2024): Avoiding catastrophe through intersectionality in global AI governance... (Relevance: 8/15)
- Himmelreich (2022): Artificial Intelligence and Structural Injustice: Foundations for Equi... (Relevance: 8/15)
- Lin (2022): Artificial Intelligence in a Structurally Unjust Society... (Relevance: 8/15)
- Gohar (2023): A Survey on Intersectional Fairness in Machine Learning: Notions, Miti... (Relevance: 8/15)
- An (2025): Measuring gender and racial biases in large language models: Intersect... (Relevance: 8/15)
- Tun (2025): Trust in artificial intelligence‚Äìbased clinical decision support syste... (Relevance: 7/15)
- Tun (2025): Trust in artificial intelligence‚Äìbased clinical decision support syste... (Relevance: 7/15)
- UNESCO (2024): Challenging systematic prejudices: an Investigation into Gender Bias i... (Relevance: 7/15)
- Debnath (2024): Can LLMs reason about trust? A pilot study... (Relevance: 6/15)
- Debnath (2024): Can LLMs reason about trust? A pilot study... (Relevance: 6/15)
- Debnath (2024): Can LLMs reason about trust? A pilot study... (Relevance: 6/15)
- Park (2025): AI algorithm transparency, pipelines for trust not prisms: mitigating ... (Relevance: 5/15)
- Park (2025): AI algorithm transparency, pipelines for trust not prisms: mitigating ... (Relevance: 5/15)
- _ACADEMY|Unknown (): feminist AI | ACADEMY... (Relevance: 0/15)

---

<a id='top_papers'></a>

## üèÜ Top 20 Papers by Total Relevance

---
title: "Top 20 Papers by Relevance"
type: top-papers-moc
count: 20
date_created: 2025-11-10
tags: [moc, top-papers]
---

# üèÜ Top 20 Papers by Total Relevance

The most relevant papers for AI literacy in social work research.


## 1. Cheng (2022): How child welfare workers reduce racial disparities in algorithmic decisions

- **Link:** Cheng_2022_How_child_welfare_workers_reduce_racial_disparitie
- **Total Relevance:** 13/15
- **Top Dimensions:** Vulnerable Groups, Bias Analysis
- **Decision:** Include
- **Has Summary:** No

## 2. Hall (2024): A systematic review of sophisticated predictive and prescriptive analytics in child welfare: Accuracy, equity, and bias

- **Link:** Hall_2024_A_systematic_review_of_sophisticated_predictive_an
- **Total Relevance:** 12/15
- **Top Dimensions:** Vulnerable Groups, Bias Analysis
- **Decision:** Include
- **Has Summary:** Yes üìù

## 3. Kawakami (2022): Improving human-AI partnerships in child welfare: Understanding worker practices, challenges, and desires for algorithmic decision support

- **Link:** Kawakami_2022_Improving_human-AI_partnerships_in_child_welfare__
- **Total Relevance:** 12/15
- **Top Dimensions:** Vulnerable Groups, Bias Analysis
- **Decision:** Include
- **Has Summary:** No

## 4. McDonald (2023): Algorithmic decision-making in social work practice and pedagogy: Confronting the competency/critique dilemma

- **Link:** McDonald_2023_Algorithmic_decision-making_in_social_work_practic
- **Total Relevance:** 12/15
- **Top Dimensions:** AI Literacy, Professional Context
- **Decision:** Include
- **Has Summary:** No

## 5. Ahn (2025): Artificial Intelligence (AI) literacy for social work: Implications for core competencies

- **Link:** Ahn_2025_Artificial_Intelligence_(AI)_literacy_for_social_w
- **Total Relevance:** 12/15
- **Top Dimensions:** AI Literacy, Professional Context
- **Decision:** Include
- **Has Summary:** No

## 6. Baker (2025): Artificial intelligence in social work: An EPIC model for practice

- **Link:** Baker_2025_Artificial_intelligence_in_social_work__An_EPIC_mo
- **Total Relevance:** 12/15
- **Top Dimensions:** Vulnerable Groups, Professional Context
- **Decision:** Include
- **Has Summary:** No

## 7. Linnemann (2025): K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen f√ºr Theorie und Praxis

- **Link:** Linnemann_2025_K√ºnstliche_Intelligenz_in_der_Sozialen_Arbeit__Gru
- **Total Relevance:** 12/15
- **Top Dimensions:** AI Literacy, Professional Context
- **Decision:** Include
- **Has Summary:** No

## 8. Yu (2025): Algorithmic-Assisted Decision-Making Tools in Child Welfare Practice: A Systematic Review

- **Link:** Yu_2025_Algorithmic-Assisted_Decision-Making_Tools_in_Chil
- **Total Relevance:** 12/15
- **Top Dimensions:** Vulnerable Groups, Bias Analysis
- **Decision:** Include
- **Has Summary:** No

## 9. Ahn (2025): Artificial Intelligence (AI) Literacy for Social Work: Implications for Core Competencies

- **Link:** Ahn_2025_Artificial_Intelligence_(AI)_Literacy_for_Social_W
- **Total Relevance:** 12/15
- **Top Dimensions:** AI Literacy, Professional Context
- **Decision:** Include
- **Has Summary:** No

## 10. Rodriguez (2024): Introducing Generative Artificial Intelligence into the MSW Curriculum: A Proposal for the 2029 Educational Policy and Accreditation Standards

- **Link:** Rodriguez_2024_Introducing_Generative_Artificial_Intelligence_int
- **Total Relevance:** 12/15
- **Top Dimensions:** AI Literacy, Professional Context
- **Decision:** Include
- **Has Summary:** No

## 11. Nuwasiima (2024): The role of artificial intelligence (AI) and machine learning in social work practice

- **Link:** Nuwasiima_2024_The_role_of_artificial_intelligence_(AI)_and_machi
- **Total Relevance:** 12/15
- **Top Dimensions:** Vulnerable Groups, Bias Analysis
- **Decision:** Include
- **Has Summary:** Yes üìù

## 12. Slesinger (2024): Training in Co-Creation as a Methodological Approach to Improve AI Fairness

- **Link:** Slesinger_2024_Training_in_Co-Creation_as_a_Methodological_Approa
- **Total Relevance:** 12/15
- **Top Dimensions:** Vulnerable Groups, Bias Analysis
- **Decision:** Include
- **Has Summary:** Yes üìù

## 13. Lanzetta (2024): Artificial Intelligence Competence Needs for Youth Workers

- **Link:** Lanzetta_2024_Artificial_Intelligence_Competence_Needs_for_Youth
- **Total Relevance:** 11/15
- **Top Dimensions:** AI Literacy, Professional Context
- **Decision:** Include
- **Has Summary:** No

## 14. Takaoka (2022): AI implementation science for social issues: Pitfalls and tips

- **Link:** Takaoka_2022_AI_implementation_science_for_social_issues__Pitfa
- **Total Relevance:** 11/15
- **Top Dimensions:** Practical Implementation, Professional Context
- **Decision:** Include
- **Has Summary:** No

## 15. Moreau (2024): Failing our youngest: On the biases, pitfalls, and risks in a decision support algorithm used for child protection

- **Link:** Moreau_2024_Failing_our_youngest__On_the_biases,_pitfalls,_and
- **Total Relevance:** 11/15
- **Top Dimensions:** Vulnerable Groups, Bias Analysis
- **Decision:** Include
- **Has Summary:** No

## 16. Reamer (2023): Artificial intelligence in social work: Emerging ethical issues

- **Link:** Reamer_2023_Artificial_intelligence_in_social_work__Emerging_e
- **Total Relevance:** 11/15
- **Top Dimensions:** Professional Context, AI Literacy
- **Decision:** Include
- **Has Summary:** Yes üìù

## 17. Linnemann (2023): Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit: Eine exemplarische arbeitsfeld√ºbergreifende Betrachtung des Natural Language Processing (NLP)

- **Link:** Linnemann_2023_Bedeutung_von_K√ºnstlicher_Intelligenz_in_der_Sozia
- **Total Relevance:** 11/15
- **Top Dimensions:** Professional Context, AI Literacy
- **Decision:** Include
- **Has Summary:** Yes üìù

## 18. Unknown (2025): Artificial Intelligence in Social Work: An EPIC Model for Practice

- **Link:** Unknown_2025_Artificial_Intelligence_in_Social_Work__An_EPIC_Mo
- **Total Relevance:** 11/15
- **Top Dimensions:** Professional Context, AI Literacy
- **Decision:** Include
- **Has Summary:** Yes üìù

## 19. James (2023): Algorithmic decision-making in social work practice and pedagogy: confronting the competency/critique dilemma

- **Link:** James_2023_Algorithmic_decision-making_in_social_work_practic
- **Total Relevance:** 11/15
- **Top Dimensions:** Professional Context, AI Literacy
- **Decision:** Include
- **Has Summary:** No

## 20. Feist-Ortmanns (2025): KI-basiertes Assistenzsystem im Kinderschutzverfahren

- **Link:** Feist-Ortmanns_2025_KI-basiertes_Assistenzsystem_im_Kinderschutzverfah
- **Total Relevance:** 11/15
- **Top Dimensions:** Practical Implementation, Professional Context
- **Decision:** Include
- **Has Summary:** Yes üìù

---

# Papers (266)

<a id='[author_not_specified]_2025_navigating_the_nexus_of_trust__prompt_engineering,'></a>

## Paper 1/266: Navigating the Nexus of Trust: Prompt Engineering, Professional Judgment, and the Integration of Large Language Models in Social Work

**Source file:** `[Author_not_specified]_2025_Navigating_the_Nexus_of_Trust__Prompt_Engineering,.md`

---
title: "Navigating the Nexus of Trust: Prompt Engineering, Professional Judgment, and the Integration of Large Language Models in Social Work"
zotero_key: UV2XJZZQ
author_year: "[Author not specified] (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 1
rel_bias: 3
rel_praxis: 2
rel_prof: 3
total_relevance: 11

# Categorization
relevance_category: high
top_dimensions: ["Bias Analysis", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-medium", "dim-bias-high", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Navigating the Nexus of Trust: Prompt Engineering, Professional Judgment, and the Integration of Large Language Models in Social Work

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **11/15** (high) |
| **Top Dimensions** | Bias Analysis, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Comprehensive analysis examining how prompt engineering strategies designed to enhance transparency and mitigate bias influence trust that social work professionals place in LLM-generated case recommendations. Synthesizes literature from computer science, social work, ethics, and psychology to construct understanding of complex interplay between technology, human psychology, and professional practice. Develops framework for calibrated trust through responsible prompt engineering, positioning professionals as active directors rather than passive consumers of AI outputs. Proposes that trust emerges from three-way interaction between intentional prompting strategies, professional disposition and expertise, and perceived system trustworthiness across ability, benevolence, and integrity dimensions.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/UV2XJZZQ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='a+_alliance_2024_incubating_feminist_ai__executive_summary_2021-202'></a>

## Paper 2/266: Incubating Feminist AI: Executive Summary 2021-2024

**Source file:** `A+_Alliance_2024_Incubating_Feminist_AI__Executive_Summary_2021-202.md`

---
title: "Incubating Feminist AI: Executive Summary 2021-2024"
zotero_key: KALSNSNC
author_year: "A+ Alliance (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://aplusalliance.org/incubatingfeministai2024/"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 3
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Practical Implementation"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-medium", "dim-praxis-high"]

# Summary
has_summary: true
summary_file: "summary_Alliance_2024_Incubating.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Incubating Feminist AI: Executive Summary 2021-2024

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Practical Implementation |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Documents outcomes from $2 million CAD Feminist AI Research Network project across Latin America, Middle East, and Asia. Developed 12 feminist AI prototypes addressing gender-based violence, transit safety, bias detection in NLP systems, and judicial transparency, demonstrating practical applications of feminist AI principles through community-centered design and intersectional analysis capabilities.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://aplusalliance.org/incubatingfeministai2024/
- **Zotero:** [Open in Zotero](zotero://select/items/KALSNSNC)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='ahmed_2024_feminist_perspectives_on_ai__ethical_consideration'></a>

## Paper 3/266: Feminist Perspectives on AI: Ethical Considerations in Algorithmic Decision-Making

**Source file:** `Ahmed_2024_Feminist_perspectives_on_AI__Ethical_consideration.md`

---
title: "Feminist Perspectives on AI: Ethical Considerations in Algorithmic Decision-Making"
zotero_key: HIX79B5L
author_year: "Ahmed (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://www.researchcorridor.org/index.php/jgsi/article/download/330/314"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Feminist Perspectives on AI: Ethical Considerations in Algorithmic Decision-Making

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Explores ethical implications of algorithmic decision-making from feminist perspective, examining data bias, discrimination in automated systems, and underrepresentation of women in AI development. Algorithmic biases disproportionately affect marginalized groups and reinforce societal inequalities in areas like hiring, healthcare, and law enforcement. Feminist ethical framework emphasizes transparency, fairness, and inclusivity while questioning patriarchal and corporate-driven narratives in AI research and policy. Analysis shows AI ethics must go beyond technical solutions to address systemic power imbalances and cultural biases in data.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.researchcorridor.org/index.php/jgsi/article/download/330/314
- **Zotero:** [Open in Zotero](zotero://select/items/HIX79B5L)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='ahn_2025_artificial_intelligence_(ai)_literacy_for_social_w'></a>

## Paper 4/266: Artificial Intelligence (AI) Literacy for Social Work: Implications for Core Competencies

**Source file:** `Ahn_2025_Artificial_Intelligence_(AI)_literacy_for_social_w.md`

---
title: "Artificial Intelligence (AI) Literacy for Social Work: Implications for Core Competencies"
zotero_key: T9NBQNPV
author_year: "Ahn (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: en
doi: "10.1086/735187"
url: "https://www.journals.uchicago.edu/doi/10.1086/735187"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Artificial Intelligence (AI) Literacy for Social Work: Implications for Core Competencies

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1086/735187](https://doi.org/10.1086/735187)
- **URL:** https://www.journals.uchicago.edu/doi/10.1086/735187
- **Zotero:** [Open in Zotero](zotero://select/items/T9NBQNPV)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='ahrweiler_2025_ai_fora_‚Äì_artificial_intelligence_for_assessment__'></a>

## Paper 5/266: AI FORA ‚Äì Artificial Intelligence for Assessment: Fairness bei der Verteilung √∂ffentlicher sozialer Leistungen

**Source file:** `Ahrweiler_2025_AI_FORA_‚Äì_Artificial_Intelligence_for_Assessment__.md`

---
title: "AI FORA ‚Äì Artificial Intelligence for Assessment: Fairness bei der Verteilung √∂ffentlicher sozialer Leistungen"
zotero_key: FTR33WLG
author_year: "Ahrweiler (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://nachrichten.idw-online.de/2025/03/18/wie-koennte-kuenstliche-intelligenz-weltweit-fairness-bei-der-verteilung-oeffentlicher-sozialer-leistungen-erhoehen"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 2
rel_prof: 3
total_relevance: 11

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-medium", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI FORA ‚Äì Artificial Intelligence for Assessment: Fairness bei der Verteilung √∂ffentlicher sozialer Leistungen

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **11/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

This international research project examined AI-supported social assessments across nine countries on four continents. The study demonstrates that justice criteria for receiving state benefits are culture- and context-dependent. A central finding is that deploying a standardized AI system globally is insufficient; instead, flexible, dynamic, and adaptive systems are required. Development of such systems depends on contributions from all societal actors, including vulnerable groups, for designing participatory, context-specific, and fair AI.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://nachrichten.idw-online.de/2025/03/18/wie-koennte-kuenstliche-intelligenz-weltweit-fairness-bei-der-verteilung-oeffentlicher-sozialer-leistungen-erhoehen
- **Zotero:** [Open in Zotero](zotero://select/items/FTR33WLG)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='alam_2025_social_work_in_the_age_of_artificial_intelligence_'></a>

## Paper 6/266: Social work in the age of artificial intelligence: A rights-based framework for evidence-based practice through social psychology, group dynamics, and institutional analysis

**Source file:** `Alam_2025_Social_work_in_the_age_of_artificial_intelligence_.md`

---
title: "Social work in the age of artificial intelligence: A rights-based framework for evidence-based practice through social psychology, group dynamics, and institutional analysis"
zotero_key: 63XWTA3J
author_year: "Alam (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1080/26408066.2025.2547219"
url: "https://doi.org/10.1080/26408066.2025.2547219"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 2
rel_prof: 3
total_relevance: 11

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-medium", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Social work in the age of artificial intelligence: A rights-based framework for evidence-based practice through social psychology, group dynamics, and institutional analysis

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **11/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

This study develops a comprehensive rights-based framework for navigating AI integration in social work practice while addressing ethical implications across micro, meso, and macro practice levels. The framework bridges social work theory with interdisciplinary insights, demonstrating that AI systems profoundly impact vulnerable populations by mediating interpersonal relationships and constructing meaning in AI-mediated environments. It provides evidence-based guidance for practitioners to harness AI's potential while safeguarding core social work values of human dignity, self-determination, and social justice, offering concrete strategies for social work education and research methodologies that center community voices.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1080/26408066.2025.2547219](https://doi.org/10.1080/26408066.2025.2547219)
- **URL:** https://doi.org/10.1080/26408066.2025.2547219
- **Zotero:** [Open in Zotero](zotero://select/items/63XWTA3J)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='alvarez_2024_policy_advice_and_best_practices_on_bias_and_fairn'></a>

## Paper 7/266: Policy advice and best practices on bias and fairness in AI

**Source file:** `Alvarez_2024_Policy_advice_and_best_practices_on_bias_and_fairn.md`

---
title: "Policy advice and best practices on bias and fairness in AI"
zotero_key: V3TWQUZC
author_year: "Alvarez (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1007/s10676-024-09746-w"
url: "https://link.springer.com/article/10.1007/s10676-024-09746-w"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Alvarez_2024_Policy.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Policy advice and best practices on bias and fairness in AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This open-access paper provides a comprehensive overview of fairness in AI, bridging technical bias mitigation methods with legal and policy considerations. Alvarez et al. survey the state-of-the-art in fair AI techniques and review major policy initiatives and standards on algorithmic bias. A key contribution is the NoBIAS architecture introduced in the paper, which comprises a ‚ÄúLegal Layer‚Äù (focusing on EU non-discrimination law and human rights requirements) and a ‚ÄúBias Management Layer‚Äù (covering bias understanding, mitigation, and accountability). The authors note that AI systems have produced real-world harms, including illegal discrimination against protected groups, and they highlight challenges such as intersectional discrimination that current EU law does not explicitly address. By organizing existing knowledge and best practices, the article guides researchers and practitioners in aligning technical solutions with ethical and legal norms ‚Äì underscoring that managing AI bias requires not just algorithmic techniques but also adherence to equality principles and governance frameworks.


## AI Summary

## Overview

This 2024 paper, authored by 13 international researchers and published in April 2024, addresses a critical gap in AI governance by providing comprehensive guidance on bias and fairness in artificial intelligence systems. Grounded in the NoBIAS research project‚Äîa European initiative examining legal and technical dimensions of bias‚Äîthe work responds to the exponential growth of AI applications in socially sensitive domains since the post-2010 AI renaissance. The paper serves dual purposes: offering a bird's-eye survey of fair-AI methods, resources, and policies for researchers and practitioners, and contributing original policy advice and best practices. The authors recognize that AI systems are not neutral tools but inherently value-laden technologies capable of producing real-world harms, particularly through both intentional and unintentional discrimination against legally protected social groups. This recognition necessitates integrated approaches combining technical innovation with legal frameworks and ethical considerations.

## Main Findings

The research identifies several critical insights: First, documented AI incident databases reveal tangible harms resulting from biased algorithmic systems, challenging widespread assumptions about algorithmic neutrality. Second, a substantial gap exists between rapid technical advances in AI development and underdeveloped comprehensive policy frameworks, particularly regarding operationalization of fairness principles. Third, effective bias management requires simultaneous integration of EU non-discrimination legislation with technical mitigation strategies‚Äîneither legal nor technical approaches alone suffice. Fourth, algorithmic systems create moral consequences, reinforce or undercut ethical principles, and enable or diminish stakeholder rights and dignity, requiring explicit value consideration. Fifth, the proliferation of fair-AI literature creates barriers for new researchers and practitioners seeking comprehensive understanding. Finally, structured guidance translating abstract fairness principles into actionable procedures, standards, and practices remains insufficiently developed in existing literature.

## Methodology/Approach

The paper employs a sophisticated dual-layer analytical framework derived from the NoBIAS project. The **Legal Layer** examines the European Union regulatory context and non-discrimination law, providing normative foundations for bias governance within EU legislation. The **Bias Management Layer** addresses three interconnected operations: understanding bias mechanisms, mitigating bias effects, and accounting for bias in system design and deployment. This multidisciplinary approach synthesizes perspectives from computer science, law, ethics, and policy studies across 13 international authors and multiple institutions. Rather than attempting exhaustive coverage of the extensive literature, the methodology strategically prioritizes survey papers and recent works, enabling comprehensive yet manageable guidance. This approach acknowledges the field's rapid expansion while maintaining accessibility for diverse audiences including researchers, practitioners, and policymakers.

## Relevant Concepts

**Algorithmic Bias**: Systematic errors in AI systems producing discriminatory outcomes, whether intentional or unintentional, affecting legally protected groups.

**Fairness**: Multifaceted concept addressing equitable treatment, non-discrimination compliance, and respect for stakeholder rights and dignity in algorithmic decision-making.

**Value-Laden Systems**: Technologies that inherently embody moral consequences, reinforce or undercut ethical principles, and enable or diminish stakeholder rights and dignity through design choices.

**Bias Management**: Systematic processes comprising three operations‚Äîunderstanding, mitigating, and accounting for bias‚Äîthroughout AI system lifecycles.

**Policy Operationalization**: Translation of abstract fairness principles into concrete procedures, standards, and actionable practices suitable for organizational implementation.

**Bird's-Eye View**: Comprehensive, high-level perspective providing orientation across multidisciplinary literature and fragmented research domains.

## Significance

This work holds substantial significance for multiple stakeholders. For researchers, it provides essential orientation within an increasingly complex landscape, facilitating informed research directions and identifying underdeveloped areas. For practitioners, it offers practical guidance for implementing fair-AI principles within organizational contexts through structured frameworks. For policymakers, particularly within the EU, it bridges technical and legal domains, supporting evidence-based regulation development aligned with non-discrimination law. The paper's emphasis on integrating legal and technical perspectives reflects emerging consensus that responsible AI governance requires multidisciplinary collaboration. By proposing the NoBIAS architecture as a governance model combining legal and technical layers, the authors contribute to operationalizing fairness beyond theoretical discussion. The work ultimately advances the field's maturation, moving from isolated technical solutions toward comprehensive, integrated approaches to bias management that acknowledge AI's profound societal implications and legal obligations.


## Links & Resources

- **DOI:** [10.1007/s10676-024-09746-w](https://doi.org/10.1007/s10676-024-09746-w)
- **URL:** https://link.springer.com/article/10.1007/s10676-024-09746-w
- **Zotero:** [Open in Zotero](zotero://select/items/V3TWQUZC)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='amnesty_international_2024_coded_injustice__surveillance_and_discrimination_i'></a>

## Paper 8/266: Coded injustice: Surveillance and discrimination in Denmark's automated welfare state

**Source file:** `Amnesty_International_2024_Coded_injustice__Surveillance_and_discrimination_i.md`

---
title: "Coded injustice: Surveillance and discrimination in Denmark's automated welfare state"
zotero_key: C2MFBS5K
author_year: "Amnesty International (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://www.amnesty.org/en/documents/eur18/8709/2024/en/"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 2
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-prof-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Amnesty_International_2024_Coded.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Coded injustice: Surveillance and discrimination in Denmark's automated welfare state

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

Critical human rights investigation documenting how Denmark's AI-powered welfare fraud detection systems risk discriminating against people with disabilities, low-income individuals, migrants, refugees, and marginalized racial groups. Report examines up to 60 algorithmic models used to flag individuals for fraud investigations, revealing mass surveillance practices violating privacy rights and creating atmospheres of fear among welfare recipients. Key findings demonstrate how automated systems paired with extensive data collection from multiple government agencies create what approaches prohibited social scoring. Investigation reveals harmful psychological tolls on surveilled populations and argues automation exacerbates pre-existing structural inequalities rather than creating fair or efficient systems.


## AI Summary

## Overview

Amnesty International's 2024 report "Coded Injustice" examines Denmark's automated welfare administration system, specifically investigating how Udbetaling Danmark (UDK) deploys fraud-control algorithms that create discriminatory surveillance mechanisms targeting vulnerable populations. The document addresses a critical accountability gap by analyzing whether algorithmic decision-making in welfare systems complies with international human rights obligations regarding privacy, equality, and non-discrimination. The report positions algorithms as active agents capable of perpetuating and amplifying structural discrimination rather than neutral technical tools. It reveals systemic failures in state oversight, corporate transparency, and individual remedy mechanisms, demonstrating how technological automation institutionalizes discrimination against marginalized groups including foreign-affiliated individuals, ethnic minorities, atypical households, and welfare recipients within Denmark's hostile policy environment toward vulnerable populations.

## Main Findings

The report identifies violations across multiple dimensions. First, **structural discrimination**: UDK's algorithms employ discriminatory proxy variables‚Äîhousehold composition patterns, foreign affiliations, residency status‚Äîthat systematically target vulnerable populations. Second, **surveillance expansion**: "Duvet lifting" practices extend monitoring beyond individual applicants to their entire social networks, creating expansive surveillance infrastructure. Third, **digital exclusion paradox**: marginalized populations face simultaneous exclusion from services and forced mandatory digital compliance. Fourth, **transparency deficits**: affected individuals cannot understand algorithmic decisions or access meaningful remedies. Fifth, **accountability gaps**: existing state oversight mechanisms prove inadequate, violating international human rights law (privacy, equality, non-discrimination rights) and anticipating violations of the EU AI Act. Sixth, **corporate responsibility failures**: insufficient corporate transparency prevents accountability for algorithmic harms.

## Methodology/Approach

The analysis employs a rigorous human rights framework grounded in international law obligations. The methodology combines document analysis of UDK's algorithmic systems with empirical investigation of discriminatory impacts. Research incorporates stakeholder engagement including responses from authorities and companies, ensuring multi-perspectival analysis. An intersectional lens examines how discrimination compounds across overlapping marginalized identities. The approach treats algorithmic systems as measurable actors with demonstrable human rights implications. The framework integrates critical algorithm studies with human rights jurisprudence, bridging academic scholarship and policy advocacy.

## Relevant Concepts

**Structural discrimination**: Systemic patterns embedded in institutional practices that disadvantage specific groups independent of individual intent.

**Algorithmic discrimination**: Automated systems perpetuating structural discrimination through proxy variables correlating with protected characteristics.

**Duvet lifting**: Invasive monitoring extending beyond individual welfare recipients to social networks.

**Digital exclusion paradox**: Simultaneous service exclusion and forced mandatory digital compliance.

**Proxy variables**: Data inputs indirectly discriminating by correlating with protected characteristics.

**State oversight mechanisms**: Regulatory frameworks and accountability structures for algorithmic systems.

**Corporate responsibility**: Corporate accountability for algorithmic harms affecting human rights.

**Remedy mechanisms**: Access to justice and redress for individuals harmed by algorithmic discrimination.

## Significance

This report challenges techno-solutionist narratives positioning automation as neutral and efficiency-enhancing. It establishes algorithmic discrimination as a systemic justice issue requiring robust human rights safeguards in welfare contexts. Key recommendations include mandatory algorithmic impact assessments, enhanced transparency requirements, strengthened due process protections, and improved state oversight mechanisms. The work advances scholarly consensus that automated decision-making demands human rights compliance frameworks. By positioning algorithmic systems within human rights law rather than purely technical domains, the report establishes precedent for holding state and corporate actors accountable for algorithmic harms affecting vulnerable populations, particularly within hostile policy environments.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.amnesty.org/en/documents/eur18/8709/2024/en/
- **Zotero:** [Open in Zotero](zotero://select/items/C2MFBS5K)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='an_2025_measuring_gender_and_racial_biases_in_large_langua'></a>

## Paper 9/266: Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation

**Source file:** `An_2025_Measuring_gender_and_racial_biases_in_large_langua.md`

---
title: "Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation"
zotero_key: 5WVV8IRY
author_year: "An (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1093/pnasnexus/pgaf089"
url: "https://doi.org/10.1093/pnasnexus/pgaf089"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Vethman_2025_Fairness.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Large-scale experimental study examining bias across five major LLMs using over 361,000 randomized resumes. Reveals complex intersectional patterns where LLMs favor female candidates but discriminate against Black male candidates, with bias effects translating to 1-3 percentage point differences in hiring probabilities, validating intersectionality theory through three key findings.


## AI Summary

!summary_Vethman_2025_Fairness.md


## Links & Resources

- **DOI:** [10.1093/pnasnexus/pgaf089](https://doi.org/10.1093/pnasnexus/pgaf089)
- **URL:** https://doi.org/10.1093/pnasnexus/pgaf089
- **Zotero:** [Open in Zotero](zotero://select/items/5WVV8IRY)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='arias_l√≥pez_2023_digital_literacy_as_a_new_determinant_of_health__a'></a>

## Paper 10/266: Digital literacy as a new determinant of health: A scoping review

**Source file:** `Arias_L√≥pez_2023_Digital_literacy_as_a_new_determinant_of_health__A.md`

---
title: "Digital literacy as a new determinant of health: A scoping review"
zotero_key: 5WQ2JR8V
author_year: "Arias L√≥pez (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: en
doi: "10.1371/journal.pdig.0000279"
url: "https://dx.plos.org/10.1371/journal.pdig.0000279"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Digital literacy as a new determinant of health: A scoping review

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Introduction Harnessing new digital technologies can improve access to health care but can also widen the health divide for those with poor digital literacy. This scoping review aims to assess the current situation of low digital health literacy in terms of its definition, reach, impact on health and interventions for its mitigation. Methods A comprehensive literature search strategy was composed by a qualified medical librarian. Literature databases [Medline (Ovid), Embase (Ovid), Scopus, and Google Scholar] were queried using appropriate natural language and controlled vocabulary terms along with hand-searching and citation chaining. We focused on recent and highly cited references published in English. Reviews were excluded. This scoping review was conducted following the methodological framework of Arksey and O‚ÄôMalley. Results A total of 268 articles were identified (263 from the initial search and 5 more added from the references of the original papers), 53 of which were finally selected for full text analysis. Digital health literacy is the most frequently used descriptor to refer to the ability to find and use health information with the goal of addressing or solving a health problem using technology. The most utilized tool to assess digital health literacy is the eHealth literacy scale (eHEALS), a self-reported measurement tool that evaluates six core dimensions and is available in various languages. Individuals with higher digital health literacy scores have better self-management and participation in their own medical decisions, mental and psychological state and quality of life. Effective interventions addressing poor digital health literacy included education/training and social support. Conclusions Although there is interest in the study and impact of poor digital health literacy, there is still a long way to go to improve measurement tools and find effective interventions to reduce the digital health divide.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1371/journal.pdig.0000279](https://doi.org/10.1371/journal.pdig.0000279)
- **URL:** https://dx.plos.org/10.1371/journal.pdig.0000279
- **Zotero:** [Open in Zotero](zotero://select/items/5WQ2JR8V)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='articulate_2025_how_to_create_inclusive_ai_images__a_guide_to_bias'></a>

## Paper 11/266: How to Create Inclusive AI Images: A Guide to Bias-Free Prompting

**Source file:** `Articulate_2025_How_to_Create_Inclusive_AI_Images__A_Guide_to_Bias.md`

---
title: "How to Create Inclusive AI Images: A Guide to Bias-Free Prompting"
zotero_key: 3VBJ2BPX
author_year: "Articulate (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: webpage
language: nan
doi: "nan"
url: "https://www.articulate.com/blog/how-to-create-inclusive-ai-images-a-guide-to-bias-free-prompting/"

# Assessment
decision: Exclude
exclusion_reason: "Wrong publication type"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# How to Create Inclusive AI Images: A Guide to Bias-Free Prompting

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Wrong publication type


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Practical guide examining prompt engineering strategies for creating inclusive AI-generated images and avoiding biased default outputs. Analysis shows AI image generators often produce stereotypical representations (white, male, slim, young, physically able) due to training on unbalanced internet datasets. Presents concrete techniques for inclusive prompt engineering: specifying visible identity characteristics (age, race, gender, body size, visible disabilities), using diversity-promoting terms like "multicultural" and "gender-diverse," and providing additional context to break stereotypical associations.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.articulate.com/blog/how-to-create-inclusive-ai-images-a-guide-to-bias-free-prompting/
- **Zotero:** [Open in Zotero](zotero://select/items/3VBJ2BPX)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='asseri_2024_prompt_engineering_techniques_for_mitigating_cultu'></a>

## Paper 12/266: Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review

**Source file:** `Asseri_2024_Prompt_engineering_techniques_for_mitigating_cultu.md`

---
title: "Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review"
zotero_key: FEC83VIS
author_year: "Asseri (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.48550/arXiv.2506.18199"
url: "https://doi.org/10.48550/arXiv.2506.18199"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Systematic review following PRISMA guidelines analyzing prompt-engineering techniques to reduce cultural bias against Arabs and Muslims in LLMs, evaluating eight empirical studies (2021-2024). Identifies five primary approaches: Cultural Prompting, Affective Priming, Self-Debiasing techniques, structured multi-stage pipelines, and parameter-optimized continuous prompts. Structured multi-stage pipelines most effective with up to 87.7% bias reduction. Emphasizes accessibility of prompt engineering for bias mitigation and importance of transparent methodology for trust building.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.48550/arXiv.2506.18199](https://doi.org/10.48550/arXiv.2506.18199)
- **URL:** https://doi.org/10.48550/arXiv.2506.18199
- **Zotero:** [Open in Zotero](zotero://select/items/FEC83VIS)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='asseri_2025_prompt_engineering_techniques_for_mitigating_cultu'></a>

## Paper 13/266: Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review

**Source file:** `Asseri_2025_Prompt_engineering_techniques_for_mitigating_cultu.md`

---
title: "Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review"
zotero_key: VP7WHIQM
author_year: "Asseri (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/html/2506.18199"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This systematic review of 8 studies (2021‚Äì2024) identifies five prompt engineering approaches to mitigate bias against Arabs and Muslims: self-debiasing, cultural context prompting, affective priming, structured multi-step pipelines, and continuous prompt tuning. Multi-step pipelines were most effective, reducing biased content by up to ~88%, while simpler methods like cultural prompts showed ~71‚Äì81% improvement. The review concludes that while prompt engineering can mitigate biases without retraining, deep-seated biases may persist, and fixes can be superficial.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2506.18199
- **Zotero:** [Open in Zotero](zotero://select/items/VP7WHIQM)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='attard-frost_2025_ai_countergovernance__lessons_learned_from_canada_'></a>

## Paper 14/266: AI Countergovernance: Lessons Learned from Canada and Paris

**Source file:** `Attard-Frost_2025_AI_Countergovernance__Lessons_Learned_from_Canada_.md`

---
title: "AI Countergovernance: Lessons Learned from Canada and Paris"
zotero_key: MIMGY67J
author_year: "Attard-Frost (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://techpolicy.press/ai-countergovernance-lessons-learned-from-canada-and-paris/"

# Assessment
decision: Unclear
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "AI Literacy"]

# Tags
tags: ["paper", "unclear", "medium-relevance", "dim-ai-komp-medium", "dim-vulnerable-high", "dim-bias-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI Countergovernance: Lessons Learned from Canada and Paris

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Unclear** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Argues against superficial "AI literacy" programs, promoting instead grassroots critical AI literacies that engage directly with structural inequalities related to race, gender, and labor. Stresses collectiv


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://techpolicy.press/ai-countergovernance-lessons-learned-from-canada-and-paris/
- **Zotero:** [Open in Zotero](zotero://select/items/MIMGY67J)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='bai_2025_explicitly_unbiased_large_language_models_still_fo'></a>

## Paper 15/266: Explicitly unbiased large language models still form biased associations

**Source file:** `Bai_2025_Explicitly_unbiased_large_language_models_still_fo.md`

---
title: "Explicitly unbiased large language models still form biased associations"
zotero_key: IWMJYP2I
author_year: "Bai (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1073/pnas.2416228122"
url: "https://doi.org/10.1073/pnas.2416228122"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 7

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Explicitly unbiased large language models still form biased associations

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **7/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Demonstrates that even when LLMs are aligned to avoid overt bias, they can still harbor implicit biases. Introduces novel evaluation methods inspired by psychology: LLM Word Association Test (LLM-WAT) and LLM Relative Decision Test (LLM-RDT) to probe automatic associations and subtle discrimination. Finds pervasive stereotype-consistent biases across multiple domains in eight state-of-the-art, value-aligned models.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1073/pnas.2416228122](https://doi.org/10.1073/pnas.2416228122)
- **URL:** https://doi.org/10.1073/pnas.2416228122
- **Zotero:** [Open in Zotero](zotero://select/items/IWMJYP2I)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='baker_2025_artificial_intelligence_in_social_work__an_epic_mo'></a>

## Paper 16/266: Artificial intelligence in social work: An EPIC model for practice

**Source file:** `Baker_2025_Artificial_intelligence_in_social_work__An_EPIC_mo.md`

---
title: "Artificial intelligence in social work: An EPIC model for practice"
zotero_key: KQPFZ6DZ
author_year: "Baker (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1080/0312407X.2025.2488345"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 2
rel_prof: 3
total_relevance: 12

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-medium", "dim-vulnerable-high", "dim-bias-medium", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Artificial intelligence in social work: An EPIC model for practice

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **12/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Presents EPIC model for integrating AI into social work consisting of four components: Ethics and justice, Policy development and advocacy, Intersectoral collaboration, and Community engagement and empowerment. Following comprehensive literature review, examines AI's influence on social work including opportunities to advance socially just outcomes and challenges risking ethical practice. Emphasizes community-based initiatives promoting AI digital literacy and partnerships with local organizations to improve technology access for vulnerable populations.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1080/0312407X.2025.2488345](https://doi.org/10.1080/0312407X.2025.2488345)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/KQPFZ6DZ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='barman_2024_beyond_transparency_and_explainability__on_the_nee'></a>

## Paper 17/266: Beyond transparency and explainability: On the need for adequate and contextualized user guidelines for LLM use

**Source file:** `Barman_2024_Beyond_transparency_and_explainability__On_the_nee.md`

---
title: "Beyond transparency and explainability: On the need for adequate and contextualized user guidelines for LLM use"
zotero_key: DMTJNKKH
author_year: "Barman (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1007/s10676-024-09778-2"
url: "https://doi.org/10.1007/s10676-024-09778-2"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 1
rel_bias: 2
rel_praxis: 2
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["AI Literacy", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-ai-komp-medium", "dim-bias-medium", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Barman_2024_Beyond.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Beyond transparency and explainability: On the need for adequate and contextualized user guidelines for LLM use

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | AI Literacy, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Argues for user-centered approach to governing AI systems, contending that transparency alone is insufficient. Proposes contextualized guidelines and training for users including clear instructions on LLM reliability, diversity-sensitive prompting techniques, and iterative query refinement. Emphasizes shifting focus from AI's internal workings to human-AI interaction context.


## AI Summary

## Overview

The South Australian Government's Generative AI Guideline establishes a comprehensive governance framework for integrating generative AI and large language model (LLM) tools‚Äîincluding OpenAI's ChatGPT and Google Bard‚Äîacross all SA Government agencies, personnel, and non-government suppliers accessing government resources. This policy-pragmatic document addresses institutional integration of emerging AI technologies while protecting sensitive information, maintaining data integrity, ensuring cybersecurity, and preserving record-keeping and privacy compliance. Rather than academic research, it provides precautionary governance guidance acknowledging both transformative productivity potential and substantial organizational hazards during rapid technological evolution, requiring agencies to conduct individualized risk assessments before authorization.

## Main Findings

The analysis reveals a fundamental tension: generative AI offers significant operational value through task automation (report generation, content creation, code debugging, brainstorming) that could enhance productivity, yet presents critical vulnerabilities requiring managed implementation. The most consequential finding concerns information confidentiality‚Äîuser inputs into consumer-oriented platforms automatically enter the public domain, creating unprecedented exposure risks for sensitive government data. The document establishes a critical technical distinction: LLMs prioritize plausibility over accuracy, generating outputs resembling training data rather than factual information, directly contradicting government requirements for reliable, verifiable information. Organizational security maturity emerges as a prerequisite variable; agencies with underdeveloped privacy and cybersecurity programs face amplified risks. The guideline identifies multiple risk categories: information confidentiality, data integrity, cyber security, and record-keeping compliance. Crucially, the document acknowledges "unknown and unknowable risks," positioning this as justification for mandatory individual risk assessments. Employees retain full accountability for maintaining record-keeping, privacy, confidentiality, and integrity obligations regardless of AI tool usage.

## Methodology/Approach

The guideline employs a **risk-benefit analysis framework** grounded in the precautionary principle rather than empirical research. This approach systematically applies organizational maturity assessment as a gating mechanism for adoption, implements scope-based governance uniformly across heterogeneous agencies and external suppliers, develops categorical risk taxonomies addressing confidentiality, integrity, cybersecurity, and compliance dimensions, and distinguishes between consumer-oriented and enterprise-grade tools. The framework emphasizes human accountability and organizational responsibility, requiring agencies to balance productivity gains against documented limitations and emerging threats through structured risk assessment protocols.

## Relevant Concepts

**Generative AI/LLMs**: AI systems trained on textual associations to produce similar content, prioritizing plausibility over factual accuracy.

**Information Confidentiality Risk**: Exposure of sensitive government data through user inputs to public-domain platforms.

**Data Integrity**: Accuracy and reliability of information‚Äîcompromised when LLM outputs prioritize similarity over factual correctness.

**Organizational Maturity Assessment**: Evaluation of existing security, privacy, and compliance program development as prerequisite for safe AI implementation.

**Precautionary Principle**: Conservative governance approach acknowledging unknown risks justifies restrictive guidance during technological uncertainty.

**Record-keeping and Privacy Obligations**: Employee responsibilities for maintaining compliance regardless of AI tool usage.

**Consumer-oriented vs. Enterprise Tools**: Distinction affecting risk profiles and data exposure potential.

**Risk-Managed Implementation**: Conditional adoption requiring mandatory risk assessments and governance protocols before authorization.

## Significance

This guideline reflects institutional responses to AI disruption emerging in 2023-2024, prioritizing governance and risk mitigation over enthusiastic adoption. Its significance lies in establishing a replicable model for public sector AI governance balancing innovation with institutional protection, emphasizing organizational accountability and employee responsibility rather than technological inevitability. The document's cautious positioning increasingly represents mainstream public sector perspectives, influencing how government organizations globally approach generative AI integration while protecting citizen data, maintaining operational integrity, and ensuring compliance with record-keeping and privacy obligations.


## Links & Resources

- **DOI:** [10.1007/s10676-024-09778-2](https://doi.org/10.1007/s10676-024-09778-2)
- **URL:** https://doi.org/10.1007/s10676-024-09778-2
- **Zotero:** [Open in Zotero](zotero://select/items/DMTJNKKH)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='basseri_2025_prompt_engineering_techniques_for_mitigating_cultu'></a>

## Paper 18/266: Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review

**Source file:** `Basseri_2025_Prompt_Engineering_Techniques_for_Mitigating_Cultu.md`

---
title: "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review"
zotero_key: G53VSXJU
author_year: "Basseri (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/html/2506.18199v1"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This systematic review identifies five major prompt engineering strategies to reduce cultural and intersectional bias in LLMs. Structured multi-step pipelines were most effective but complex, while cultural prompting offered a practical balance. Results show varying mitigation success depending on stereotype type, and emphasize trade-offs between bias reduction and performance.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2506.18199v1
- **Zotero:** [Open in Zotero](zotero://select/items/G53VSXJU)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='benjamin_2023_keynote_summary__the_new_jim_code__reimagining_the'></a>

## Paper 19/266: Keynote Summary: The New Jim Code: Reimagining the Default Settings of Technology & Society

**Source file:** `Benjamin_2023_Keynote_Summary__The_New_Jim_Code__Reimagining_the.md`

---
title: "Keynote Summary: The New Jim Code: Reimagining the Default Settings of Technology & Society"
zotero_key: NXQ4CGF6
author_year: "Benjamin (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "10.1145/3589139"
url: "https://dl.acm.org/doi/10.1145/3589139"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Keynote Summary: The New Jim Code: Reimagining the Default Settings of Technology & Society

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

In this summary of her foundational work, Ruha Benjamin introduces the concept of the "New Jim Code," which describes how new technologies, including AI, can reproduce and even deepen existing racial hierarchies and discrimination under a veneer of neutrality and progress. She argues that discrimination becomes embedded in the very architecture of these systems. This framework is crucial for understanding how various forms of discrimination are co-constituted in AI, not as accidental bugs, but as features of a system designed within a society that has not resolved its structural inequalities. The concept inherently critiques individual competence, showing how even well-intentioned developers can create discriminatory systems if the underlying societal "default settings" are not challenged.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1145/3589139](https://doi.org/10.1145/3589139)
- **URL:** https://dl.acm.org/doi/10.1145/3589139
- **Zotero:** [Open in Zotero](zotero://select/items/NXQ4CGF6)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='benlian_2025_the_ai_literacy_development_canvas__assessing_and_'></a>

## Paper 20/266: The AI literacy development canvas: Assessing and building AI literacy in organizations

**Source file:** `Benlian_2025_The_AI_literacy_development_canvas__Assessing_and_.md`

---
title: "The AI literacy development canvas: Assessing and building AI literacy in organizations"
zotero_key: AWM5HCGV
author_year: "Benlian (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: en
doi: "10.1016/j.bushor.2025.10.001"
url: "https://linkinghub.elsevier.com/retrieve/pii/S0007681325001673"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# The AI literacy development canvas: Assessing and building AI literacy in organizations

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1016/j.bushor.2025.10.001](https://doi.org/10.1016/j.bushor.2025.10.001)
- **URL:** https://linkinghub.elsevier.com/retrieve/pii/S0007681325001673
- **Zotero:** [Open in Zotero](zotero://select/items/AWM5HCGV)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='biagini_2024_less_knowledge,_more_trust__exploring_potentially_'></a>

## Paper 21/266: Less knowledge, more trust? Exploring potentially uncritical attitudes towards AI in higher education

**Source file:** `Biagini_2024_Less_knowledge,_more_trust__Exploring_potentially_.md`

---
title: "Less knowledge, more trust? Exploring potentially uncritical attitudes towards AI in higher education"
zotero_key: MQEHCQMW
author_year: "Biagini (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: en
doi: "10.17471/2499-4324/1337"
url: "https://doi.org/10.17471/2499-4324/1337"

# Assessment
decision: Unclear
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 3
rel_vulnerable: 1
rel_bias: 1
rel_praxis: 1
rel_prof: 1
total_relevance: 7

# Categorization
relevance_category: medium
top_dimensions: ["AI Literacy", "Vulnerable Groups"]

# Tags
tags: ["paper", "unclear", "medium-relevance", "dim-ai-komp-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Less knowledge, more trust? Exploring potentially uncritical attitudes towards AI in higher education

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Unclear** |
| **Total Relevance** | **7/15** (medium) |
| **Top Dimensions** | AI Literacy, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

L'intelligenza artificiale (IA) ha il potenziale per trasformare vari aspetti delle nostre vite, ma il suo sviluppo √® stato accompagnato da numerose preoccupazioni sociali ed etiche. Per comprendere le implicazioni e i meccanismi sottostanti, √® essenziale acquisire una comprensione ampia dei suoi benefici e svantaggi. A questo scopo, l'alfabetizzazione all'IA √® un fattore fondamentale per promuovere atteggiamenti pi√π consapevoli verso lo sviluppo dell'IA e delle sue implicazioni. Tuttavia, la ricerca sulla literacy all'IA √® ancora agli esordi. Per contribuire ai progressi del settore, questo articolo presenta i risultati di uno studio volto a valutare l'alfabetizzazione all'IA degli studenti nel contesto dell'istruzione universitaria, concentrandosi su dei dottorandi. L‚Äôindagine sulla loro literacy all‚ÄôIA √® stata condotta su quattro dimensioni: cognitiva, operativa, critica ed etica. I risultati mostrano che, sebbene i partecipanti avessero poca conoscenza dell'IA, erano eccessivamente fiduciosi nelle capacit√† della tecnologia. Lo studio evidenzia la necessit√† di un approccio pi√π completo all'alfabetizzazione all'IA, che includa una comprensione pi√π profonda delle sue implicazioni etiche, sociali ed economiche.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.17471/2499-4324/1337](https://doi.org/10.17471/2499-4324/1337)
- **URL:** https://doi.org/10.17471/2499-4324/1337
- **Zotero:** [Open in Zotero](zotero://select/items/MQEHCQMW)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='biegelbauer_2023_leitfaden_digitale_verwaltung_und_ethik__praxislei'></a>

## Paper 22/266: Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden f√ºr KI in der Verwaltung, Version 1.0

**Source file:** `Biegelbauer_2023_Leitfaden_Digitale_Verwaltung_und_Ethik__Praxislei.md`

---
title: "Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden f√ºr KI in der Verwaltung, Version 1.0"
zotero_key: DI8XTFWR
author_year: "Biegelbauer (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: report
language: nan
doi: "nan"
url: "https://oeffentlicherdienst.gv.at/wp-content/uploads/2023/11/Leitfaden-Digitale-Verwaltung-Ethik.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 3
rel_vulnerable: 1
rel_bias: 2
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["AI Literacy", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-ai-komp-high", "dim-bias-medium", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Biegelbauer_2023_Leitfaden.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Leitfaden Digitale Verwaltung und Ethik: Praxisleitfaden f√ºr KI in der Verwaltung, Version 1.0

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | AI Literacy, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This guideline defines AI literacy as the ability to understand and use AI, emphasizing that safe, self-determined, and responsible use requires sufficient understanding of the technology's functioning, possibilities, and challenges. It identifies automation bias as a central risk and emphasizes competency building and training as the foundation for all further measures, recommending the creation of educational standards for AI procurement and application.


## AI Summary

## Overview

The Austrian government's "Guide for Digital Administration and Ethics" (Version 1.0, 2023) represents an institutional policy document establishing frameworks for integrating artificial intelligence and digital technologies into public administration while maintaining ethical standards and democratic accountability. Published by the Federal Ministry for Art, Culture, Public Service and Sport, this guide addresses the accelerating digital transformation of governmental institutions through a normative-prescriptive approach. The document's foundational premise positions digitalization not as a purely technical challenge but as fundamentally an ethical and governance question requiring explicit institutional coordination. Significantly, the Version 1.0 designation indicates iterative development with ongoing feedback mechanisms, reflecting the document's role as a living framework rather than static guidance. The guide emphasizes comprehensive ("fl√§chendeckend") application across all administrative levels, ensuring universal rather than selective digitalization governance.

## Main Findings

The guide establishes that responsible digital administration requires five interconnected elements. First, ethical frameworks must be embedded within technological implementation processes‚Äîdigitalization cannot proceed autonomously from explicit ethical standards and transparency mechanisms. Second, citizen control over personal data ("Kontrolle √ºber die eigenen Daten") and governmental transparency constitute foundational prerequisites for legitimate digital governance, distinguishing between political transparency and algorithmic transparency. Third, digital transformation presents a dual nature: simultaneously offering efficiency gains and citizen-proximity service improvements ("b√ºrgern√§her") while presenting ethical and legal challenges requiring proactive governance. Fourth, algorithmic decision-making in public administration demands explicit transparency enabling citizens to understand automated system impacts. Fifth, successful implementation depends on institutional capacity-building ensuring public servants possess competencies navigating complex ethical-technical terrain. The Vice Chancellor's foreword emphasizes that digitalization "richtig verstanden" (properly understood) requires human-centered approaches prioritizing citizen agency over technical optimization.

## Methodology/Approach

The document employs a **normative-prescriptive methodology** combining multidisciplinary expertise with governmental institutional authority. The authorship team comprises academics (PD Dr. Peter Biegelbauer as project lead), policy specialists, and administrative practitioners, reflecting deliberate integration of theoretical and practical perspectives. Rather than conducting empirical research, the guide synthesizes legal, ethical, and administrative knowledge into actionable guidance. The theoretical framework draws from governance ethics, digital governance theory, and human-centered design principles. Notably, the document establishes contact mechanisms (ralf.tatto@bmkoes.gv.at) for ongoing feedback, indicating commitment to iterative refinement. This methodology prioritizes practitioner applicability while maintaining grounding in established governance theory and international standards (EU AI Act, OECD recommendations).

## Relevant Concepts

**Ethical Governance**: Systematic integration of ethical principles into institutional decision-making, ensuring technology serves democratic values rather than purely technical efficiency.

**Data Sovereignty**: Citizens' fundamental right to control personal information and understand governmental usage, constituting a prerequisite for legitimate digital administration.

**Human-Centered Digitalization ("Menschenzentrierter Ansatz")**: Technology implementation prioritizing citizen needs, democratic participation, and human dignity over technical optimization, requiring citizen control and transparency.

**Citizen-Proximity ("B√ºrgern√§he")**: Austrian administrative value emphasizing service delivery accessibility and responsiveness to citizen needs through digital means.

**Political and Algorithmic Transparency**: Dual requirement that both governmental decision-making processes and automated systems be comprehensible and subject to democratic oversight.

**Institutional Responsibility**: Government's obligation to establish regulatory frameworks ensuring both technological effectiveness and ethical correctness, with comprehensive application across administrative levels.

**Preventive Governance**: Proactive establishment of ethical guardrails before widespread AI deployment, rather than reactive management of technological failures.

## Significance

This guide occupies strategic importance within contemporary governance discourse at multiple levels. Institutionally, it demonstrates Austrian governmental commitment to responsible AI implementation aligned with international standards while addressing implementation gaps in digital governance. The document's Version 1.0 status and feedback mechanisms indicate recognition that digital ethics frameworks require ongoing refinement‚Äîa sophisticated acknowledgment of complexity often absent from policy documents. Methodologically, the guide models how governments can integrate theoretical ethics with practical administration through multidisciplinary collaboration. Substantively, it challenges technocratic approaches treating digitalization as value-neutral by positioning ethics as foundational rather than supplementary. The emphasis on citizen-proximity and data sovereignty reflects distinctly Austrian administrative values while contributing to broader European discourse on democratic accountability in technological societies. The preventive orientation‚Äîaddressing ethical challenges before deployment‚Äîrepresents emerging best practice influencing policy development across administrations. Finally, the document's legitimization of public sector caution regarding AI adoption establishes that responsible governance sometimes requires deliberate implementation constraints, countering narratives of inevitable technological acceleration.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://oeffentlicherdienst.gv.at/wp-content/uploads/2023/11/Leitfaden-Digitale-Verwaltung-Ethik.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/DI8XTFWR)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='birru_2024_mitigating_age-related_bias_in_large_language_mode'></a>

## Paper 23/266: Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development

**Source file:** `Birru_2024_Mitigating_age-related_bias_in_large_language_mode.md`

---
title: "Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development"
zotero_key: 6SMMDL8E
author_year: "Birru (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://pubsonline.informs.org/doi/10.1287/ijoc.2024.0645"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://pubsonline.informs.org/doi/10.1287/ijoc.2024.0645
- **Zotero:** [Open in Zotero](zotero://select/items/6SMMDL8E)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='bisconti_2024_a_formal_account_of_ai_trustworthiness__connecting'></a>

## Paper 24/266: A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization

**Source file:** `Bisconti_2024_A_formal_account_of_AI_trustworthiness__Connecting.md`

---
title: "A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization"
zotero_key: 464WKRRQ
author_year: "Bisconti (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://philarchive.org/archive/BISAFA"

# Assessment
decision: Unclear
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 2
rel_prof: 1
total_relevance: 4

# Categorization
relevance_category: low
top_dimensions: ["Practical Implementation", "AI Literacy"]

# Tags
tags: ["paper", "unclear", "low-relevance", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# A formal account of AI trustworthiness: Connecting intrinsic and perceived trustworthiness in an operational schematization

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Unclear** |
| **Total Relevance** | **4/15** (low) |
| **Top Dimensions** | Practical Implementation, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Develops formal conceptualization of AI trustworthiness connecting intrinsic and perceived trustworthiness in operationalizable schema. Argues that trustworthiness extends beyond inherent AI system capabilities to include significant influences from observer perceptions such as perceived transparency, agency locus, and human oversight. Identifies three central perceived characteristics: transparency (access to information about functionality), agency locus (perception of action autonomy source), and human oversight (presence of human control). Mathematical formalization defines overall trustworthiness as function of discrepancy between intrinsic and perceived trustworthiness.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://philarchive.org/archive/BISAFA
- **Zotero:** [Open in Zotero](zotero://select/items/464WKRRQ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='boetto_2025_artificial_intelligence_in_social_work__an_epic_mo'></a>

## Paper 25/266: Artificial Intelligence in Social Work: An EPIC Model for Practice

**Source file:** `Boetto_2025_Artificial_Intelligence_in_Social_Work__An_EPIC_Mo.md`

---
title: "Artificial Intelligence in Social Work: An EPIC Model for Practice"
zotero_key: 95CGSWND
author_year: "Boetto (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1080/0312407X.2025.2488345"
url: "https://doi.org/10.1080/0312407X.2025.2488345"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 2
rel_prof: 3
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Professional Context", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Artificial Intelligence in Social Work: An EPIC Model for Practice

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Professional Context, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Narrative review proposing the EPIC model‚ÄîEthics & justice, Policy, Intersectoral collaboration, Community engagement‚Äîto guide ethical AI integration. Balances efficiency opportunities with risks of bias and value erosion, advocating structured, human-centered adoption aligned with social justice.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1080/0312407X.2025.2488345](https://doi.org/10.1080/0312407X.2025.2488345)
- **URL:** https://doi.org/10.1080/0312407X.2025.2488345
- **Zotero:** [Open in Zotero](zotero://select/items/95CGSWND)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='british_association_of_social_workers_2025_generative_ai_&_social_work_practice_guidance'></a>

## Paper 26/266: Generative AI & social work practice guidance

**Source file:** `British_Association_of_Social_Workers_2025_Generative_AI_&_social_work_practice_guidance.md`

---
title: "Generative AI & social work practice guidance"
zotero_key: 8IXX8VAB
author_year: "British Association of Social Workers (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://basw.co.uk/policy-and-practice/resources/generative-ai-social-work-practice-guidance"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 2
rel_prof: 3
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Professional Context", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Generative AI & social work practice guidance

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Professional Context, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

BASW's initial practice guidance specifically addressing generative AI use in social work, offering reflection points for practitioners relating to ethical considerations under BASW Code of Ethics. Warns that AI tools are prone to replicating racist/sexist assumptions from training datasets, generating misleading information (hallucinations), and risking data privacy breaches. Emphasizes generative AI should create capacity for relationship-based practice rather than justify increased caseloads or redundancies. Key recommendations include avoiding entry of sensitive personal information into generic tools without explicit consent, conducting data protection assessments, maintaining human oversight and critical assessment of AI outputs.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://basw.co.uk/policy-and-practice/resources/generative-ai-social-work-practice-guidance
- **Zotero:** [Open in Zotero](zotero://select/items/8IXX8VAB)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='browne_2023_feminist_ai__critical_perspectives_on_algorithms,_'></a>

## Paper 27/266: Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligent Machines

**Source file:** `Browne_2023_Feminist_AI__Critical_Perspectives_on_Algorithms,_.md`

---
title: "Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligent Machines"
zotero_key: 8LG58R7T
author_year: "Browne (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: book
language: nan
doi: "nan"
url: "https://doi.org/10.1093/oso/9780192889898.001.0001"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Feminist AI: Critical Perspectives on Algorithms, Data, and Intelligent Machines

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

First comprehensive collection bringing together leading feminist thinkers across disciplines to examine AI's societal impact. Features 21 chapters covering topics from techno-racial capitalism to AI's military applications, examining how feminist scholarship can hold the AI sector accountable for designing systems that further social justice through diverse feminist approaches.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://doi.org/10.1093/oso/9780192889898.001.0001
- **Zotero:** [Open in Zotero](zotero://select/items/8LG58R7T)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='browne_2024_engineers_on_responsibility__feminist_approaches_t'></a>

## Paper 28/266: Engineers on responsibility: feminist approaches to who's responsible for ethical AI

**Source file:** `Browne_2024_Engineers_on_responsibility__feminist_approaches_t.md`

---
title: "Engineers on responsibility: feminist approaches to who's responsible for ethical AI"
zotero_key: 5TV6QMRD
author_year: "Browne (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1007/s10676-023-09739-1"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 1
rel_praxis: 2
rel_prof: 1
total_relevance: 7

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Practical Implementation"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Engineers on responsibility: feminist approaches to who's responsible for ethical AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **7/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Practical Implementation |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Through interviews with AI practitioners interpreted via feminist political thought, reimagines responsibility in AI development beyond individualized approaches. Critiques current AI responsibility frameworks focused on individual competency and technical solutions, proposing instead "responsibility as the product of work cultures that enable tech workers to be responsive and answerable for their products." Moves beyond "individual competency approaches" toward understanding responsibility as embedded in structural power relations and organizational cultures.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1007/s10676-023-09739-1](https://doi.org/10.1007/s10676-023-09739-1)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/5TV6QMRD)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='browne_2024_tech_workers'_perspectives_on_ethical_issues_in_ai'></a>

## Paper 29/266: Tech workers' perspectives on ethical issues in AI development: Foregrounding feminist approaches

**Source file:** `Browne_2024_Tech_workers'_perspectives_on_ethical_issues_in_AI.md`

---
title: "Tech workers' perspectives on ethical issues in AI development: Foregrounding feminist approaches"
zotero_key: GFL24IE5
author_year: "Browne (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1177/20539517231221780"
url: "https://doi.org/10.1177/20539517231221780"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Tech workers' perspectives on ethical issues in AI development: Foregrounding feminist approaches

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Empirical study examining tech workers' understanding of ethical AI development through feminist lens, revealing critical gaps in current AI ethics approaches. Finds that the term "bias" creates confusion among tech workers, undermining AI ethics initiatives, and argues for moving beyond diversity narratives toward "design justice" that centers marginalized voices.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1177/20539517231221780](https://doi.org/10.1177/20539517231221780)
- **URL:** https://doi.org/10.1177/20539517231221780
- **Zotero:** [Open in Zotero](zotero://select/items/GFL24IE5)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='casal-otero_2023_ai_literacy_in_k-12__a_systematic_literature_revie'></a>

## Paper 30/266: AI literacy in K-12: a systematic literature review

**Source file:** `Casal-Otero_2023_AI_literacy_in_K-12__a_systematic_literature_revie.md`

---
title: "AI literacy in K-12: a systematic literature review"
zotero_key: EDU8F937
author_year: "Casal-Otero (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: en
doi: "10.1186/s40594-023-00418-7"
url: "https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00418-7"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI literacy in K-12: a systematic literature review

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Abstract The successful irruption of AI-based technology in our daily lives has led to a growing educational, social, and political interest in training citizens in AI. Education systems now need to train students at the K-12 level to live in a society where they must interact with AI. Thus, AI literacy is a pedagogical and cognitive challenge at the K-12 level. This study aimed to understand how AI is being integrated into K-12 education worldwide. We conducted a search process following the systematic literature review method using Scopus. 179 documents were reviewed, and two broad groups of AI literacy approaches were identified, namely learning experience and theoretical perspective. The first group covered experiences in learning technical, conceptual and applied skills in a particular domain of interest. The second group revealed that significant efforts are being made to design models that frame AI literacy proposals. There were hardly any experiences that assessed whether students understood AI concepts after the learning experience. Little attention has been paid to the undesirable consequences of an indiscriminate and insufficiently thought-out application of AI. A competency framework is required to guide the didactic proposals designed by educational institutions and define a curriculum reflecting the sequence and academic continuity, which should be modular, personalized and adjusted to the conditions of the schools. Finally, AI literacy can be leveraged to enhance the learning of disciplinary core subjects by integrating AI into the teaching process of those subjects, provided the curriculum is co-designed with teachers.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1186/s40594-023-00418-7](https://doi.org/10.1186/s40594-023-00418-7)
- **URL:** https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00418-7
- **Zotero:** [Open in Zotero](zotero://select/items/EDU8F937)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='charlesworth_2024_flexible_intersectional_stereotype_extraction_(fis'></a>

## Paper 31/266: Flexible intersectional stereotype extraction (FISE): Analyzing intersectional biases in large language models

**Source file:** `Charlesworth_2024_Flexible_intersectional_stereotype_extraction_(FIS.md`

---
title: "Flexible intersectional stereotype extraction (FISE): Analyzing intersectional biases in large language models"
zotero_key: 48NCCNDH
author_year: "Charlesworth (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://news.northwestern.edu/stories/2024/03/kellogg-study-suggests-that-some-intersectional-groups-are-more-represented-than-others-in-internet-text/"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Flexible intersectional stereotype extraction (FISE): Analyzing intersectional biases in large language models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Studie entwickelt FISE-Methode zur Messung intersektionaler Repr√§sentationsverzerrungen. Zeigt massive Dominanz wei√üer M√§nner in Internettexten und Ableitung entsprechender LLM-Biases.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://news.northwestern.edu/stories/2024/03/kellogg-study-suggests-that-some-intersectional-groups-are-more-represented-than-others-in-internet-text/
- **Zotero:** [Open in Zotero](zotero://select/items/48NCCNDH)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='chatterji_2025_how_people_use_chatgpt'></a>

## Paper 32/266: How People Use ChatGPT

**Source file:** `Chatterji_2025_How_People_Use_ChatGPT.md`

---
title: "How People Use ChatGPT"
zotero_key: 49VKYVJV
author_year: "Chatterji (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: en
doi: "nan"
url: "https://www.nber.org/papers/w34255"

# Assessment
decision: Exclude
exclusion_reason: "Wrong publication type"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# How People Use ChatGPT

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Wrong publication type


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Founded in 1920, the NBER is a private, non-profit, non-partisan organization dedicated to conducting economic research and to disseminating research findings among academics, public policy makers, and business professionals.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.nber.org/papers/w34255
- **Zotero:** [Open in Zotero](zotero://select/items/49VKYVJV)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='chee_2025_a_competency_framework_for_ai_literacy__variations'></a>

## Paper 33/266: A Competency Framework for AI Literacy: Variations by Different Learner Groups and an Implied Learning Pathway

**Source file:** `Chee_2025_A_Competency_Framework_for_AI_Literacy__Variations.md`

---
title: "A Competency Framework for AI Literacy: Variations by Different Learner Groups and an Implied Learning Pathway"
zotero_key: 9YRIUFDC
author_year: "Chee (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: en
doi: "10.1111/bjet.13556"
url: "https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13556"

# Assessment
decision: Unclear
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 3
rel_vulnerable: 1
rel_bias: 1
rel_praxis: 2
rel_prof: 0
total_relevance: 7

# Categorization
relevance_category: medium
top_dimensions: ["AI Literacy", "Practical Implementation"]

# Tags
tags: ["paper", "unclear", "medium-relevance", "dim-ai-komp-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# A Competency Framework for AI Literacy: Variations by Different Learner Groups and an Implied Learning Pathway

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Unclear** |
| **Total Relevance** | **7/15** (medium) |
| **Top Dimensions** | AI Literacy, Practical Implementation |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

This study aims to develop a comprehensive competency framework for artificial intelligence (AI) literacy, delineating essential competencies and sub‚Äêcompetencies. This framework and its potential variations, tailored to different learner groups (by educational level and discipline), can serve as a crucial reference for designing and implementing AI curricula. However, the research on AI literacy by target learners is still in its infancy, and the findings of several existing studies provide inconsistent guidelines for educational practices. Following the 2020 PRISMA guidelines, we searched the Web of Science, Scopus, and ScienceDirect databases to identify relevant studies published between January 2012 and October 2024. The quality of the included studies was evaluated using QualSyst. A total of 29 studies were identified, and their research findings were synthesized. Results show that at the K‚Äê12 level, the required competencies include basic AI knowledge, device usage, and AI ethics. For higher education, the focus shifts to understanding data and algorithms, problem‚Äêsolving, and career‚Äêrelated competencies. For general workforce, emphasis is placed on the interpretation and utilization of data and AI tools for specific careers, along with error detection and AI‚Äêbased decision‚Äêmaking. This study connects the progression of specific learning objectives, which should be intensively addressed at each stage, to propose an AI literacy education pathway. We discuss the findings, potentials, and limitations of the derived competency framework for AI literacy, including its theoretical and practical implications and future research suggestions. Practitioner notes What is already known about this topic AI literacy is becoming increasingly important as AI technologies are integrated into various aspects of life and work. Research on AI literacy competencies across diverse learner groups and disciplines remains fragmented and inconsistent to guide educational practices. Studies providing a coherent pathway for AI literacy development throughout educational and working life are lacking. What this paper adds A comprehensive AI literacy competency framework consisting of 8 competencies and 18 sub‚Äêcompetencies. Variations in AI literacy competencies with tailored configuration and prioritization across different learner groups by school levels and disciplines. A proposed pathway for developing AI literacy from K‚Äê12 to higher education and workforce levels. Implications for practice and policy The framework can guide the design and implementation of AI curricula tailored to different learner characteristics and needs. Education should shift focus from teaching how to use AI to fostering competencies for critical, strategic, responsible and ethical integration of AI. Policies are needed to support a systematic pathway for lifelong AI literacy development from K‚Äê12 education to workforce training.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1111/bjet.13556](https://doi.org/10.1111/bjet.13556)
- **URL:** https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13556
- **Zotero:** [Open in Zotero](zotero://select/items/9YRIUFDC)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='chen_2024_exploring_complex_mental_health_symptoms_via_class'></a>

## Paper 34/266: Exploring complex mental health symptoms via classifying social media data with explainable LLMs

**Source file:** `Chen_2024_Exploring_complex_mental_health_symptoms_via_class.md`

---
title: "Exploring complex mental health symptoms via classifying social media data with explainable LLMs"
zotero_key: YRTQN7KB
author_year: "Chen (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://www.arxivdaily.com/thread/62478"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Exploring complex mental health symptoms via classifying social media data with explainable LLMs

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.arxivdaily.com/thread/62478
- **Zotero:** [Open in Zotero](zotero://select/items/YRTQN7KB)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='chen_2025_social_work_and_artificial_intelligence__collabora'></a>

## Paper 35/266: Social work and artificial intelligence: Collaboration and challenges

**Source file:** `Chen_2025_Social_work_and_artificial_intelligence__Collabora.md`

---
title: "Social work and artificial intelligence: Collaboration and challenges"
zotero_key: 9I8UUAMQ
author_year: "Chen (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://dmjr-journals.com/assets/article/1755892935-SOCIAL_WORK_&_ARTIFICIAL_INTELLIGENCE-_COLLABORATION_AND_CHALLENGES.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 1
rel_bias: 2
rel_praxis: 2
rel_prof: 3
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Professional Context", "AI Literacy"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-medium", "dim-bias-medium", "dim-praxis-medium", "dim-prof-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Chen_2025_Social.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Social work and artificial intelligence: Collaboration and challenges

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Professional Context, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

This qualitative study explores current AI applications in social work through interviews with professionals, AI developers, and policymakers, identifying challenges including insufficient decision-making transparency, gaps in ethical frameworks, and inadequate technical literacy among professionals. The research reveals that while ninety percent of social work professionals acknowledge AI's auxiliary function in daily operations, concerns persist about automation bias and the potential undermining of professional autonomy. The study proposes educational training and policy recommendations while advocating for explainable AI systems and strengthened ethical governance to foster harmonious development between technological applications and humanistic values.


## AI Summary

## Overview

This peer-reviewed academic study, published in the Journals of Business & Management Studies (Vol. 1, Issue 2, July 2025), examines the integration of artificial intelligence technology within social work practice through qualitative research involving social work professionals, AI developers, and policymakers. The research addresses a critical gap in contemporary professional literature by moving beyond purely technical discussions to investigate how AI can be responsibly incorporated while preserving social work's core ethical values, professional judgment, and humanistic care principles. Conducted by researchers from Chang Jung Christian University in Taiwan, this study provides empirical evidence for developing balanced integration strategies that prioritize both technological efficiency and professional integrity in human-centered service delivery.

## Main Findings

The study reveals that AI technology demonstrates significant potential for enhancing service efficiency, case management, and practice effectiveness in social work contexts. However, this efficiency gain accompanies substantial challenges across multiple dimensions. Critical findings include: (1) insufficient decision-making transparency in AI systems, preventing practitioners from understanding algorithmic reasoning; (2) inadequate ethical and regulatory frameworks governing AI use in social work, creating governance vacuums; (3) widespread technical literacy gaps among practitioners that impede responsible implementation; (4) social workers' insufficient trust and acceptance of AI systems as fundamental adoption barriers; and (5) risks to client confidentiality and sensitive information protection. The research identifies that previous scholarship concentrated narrowly on technical applications‚Äîsuch as risk assessment algorithms and automated case classification‚Äîwhile neglecting the profession's distinctive ethical dimensions, contextual complexity, and nuanced client understanding requirements. The study proposes that effective integration requires: developing explainable AI (XAI) systems clarifying algorithmic decision-making; establishing strengthened ethical governance structures; implementing comprehensive professional training programs addressing technical literacy; fostering genuine interdisciplinary collaboration; and prioritizing humanistic values alongside technological innovation.

## Methodology/Approach

The research employs qualitative methodologies designed to capture nuanced stakeholder perspectives through multiple data collection approaches. Primary methods include semi-structured interviews with social work professionals and focus group discussions involving AI technology developers and policymakers. This multi-stakeholder approach enables comprehensive understanding of integration challenges from diverse professional viewpoints, capturing practitioner concerns, technical feasibility perspectives, and policy considerations simultaneously. The theoretical framework integrates social work ethics, AI governance principles, and organizational change perspectives, reflecting an "integrated theoretical and practical perspective" rather than technology-centric analysis. This interdisciplinary approach acknowledges that responsible AI integration requires simultaneous attention to technical feasibility, ethical compliance, professional values, organizational readiness, and humanistic care preservation.

## Relevant Concepts

**Explainable AI (XAI):** AI systems designed to make their decision-making processes transparent and understandable to human users, addressing the "black box" problem in algorithmic decision-making and enabling professional accountability.

**Ethical Governance:** Institutional frameworks, policies, and oversight mechanisms ensuring AI applications comply with professional ethical standards, regulatory requirements, and social work values.

**Technical Literacy:** Practitioners' knowledge, competence, and confidence in understanding, appropriately utilizing, and critically evaluating AI technologies within professional contexts.

**Decision-making Transparency:** The clarity with which AI systems communicate their reasoning, recommendations, limitations, and confidence levels to social work professionals and clients.

**Humanistic Care:** Social work's core commitment to client dignity, contextual understanding, individual agency, and ethical judgment that transcends algorithmic decision-making.

**Client Confidentiality:** Protection of sensitive personal information and privacy rights in AI-mediated social work practice, ensuring data security and ethical information handling.

## Significance

This research holds substantial significance for multiple audiences and professional contexts. For social work practitioners, it provides evidence-based guidance for responsible AI adoption and identifies critical implementation barriers. For policymakers, it offers concrete recommendations for developing regulatory frameworks and governance structures. For AI developers, it clarifies social work-specific requirements for ethical system design and transparency standards. For academic discourse, it contributes to contemporary scholarship on responsible AI governance by demonstrating that technological advancement and humanistic practice are not mutually exclusive but require deliberate integration strategies. Broader significance lies in modeling how human-centered professions can thoughtfully integrate transformative technologies while maintaining core professional values. The study advances understanding of interdisciplinary collaboration requirements and establishes frameworks applicable beyond social work to other ethics-intensive professions requiring nuanced human judgment and client confidentiality protection.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://dmjr-journals.com/assets/article/1755892935-SOCIAL_WORK_&_ARTIFICIAL_INTELLIGENCE-_COLLABORATION_AND_CHALLENGES.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/9I8UUAMQ)

## Related Concepts

- Explainable AI (XAI)
- Ethical Governance
- Technical Literacy
- Decision-making Transparency
- Humanistic Care
- Client Confidentiality

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='cheng_2022_how_child_welfare_workers_reduce_racial_disparitie'></a>

## Paper 36/266: How child welfare workers reduce racial disparities in algorithmic decisions

**Source file:** `Cheng_2022_How_child_welfare_workers_reduce_racial_disparitie.md`

---
title: "How child welfare workers reduce racial disparities in algorithmic decisions"
zotero_key: C3UQDJJA
author_year: "Cheng (2022)"
authors: []

# Publication
publication_year: 2022.0
item_type: conferencePaper
language: nan
doi: "10.1145/3491102.3501831"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 3
rel_prof: 3
total_relevance: 13

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-high", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# How child welfare workers reduce racial disparities in algorithmic decisions

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2022.0 |
| **Decision** | **Include** |
| **Total Relevance** | **13/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Mixed-methods study analyzing four years of child welfare call screening data alongside worker interviews to investigate how human-algorithm collaboration affects racial bias in decision-making. Demonstrates Allegheny Family Screening Tool algorithm alone would have created 20% disparity in screen-in rates between Black and white children, but workers reduced this to 9% through holistic risk assessments and adjustments for algorithmic limitations. Reveals critical discrimination risks: algorithm disproportionately flags Black families for investigation due to biased training data reflecting historical over-policing and surveillance of communities of color.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1145/3491102.3501831](https://doi.org/10.1145/3491102.3501831)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/C3UQDJJA)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='cher_2024_exploring_machine_learning_to_support_decision-mak'></a>

## Paper 37/266: Exploring machine learning to support decision-making for placement stabilization and preservation in child welfare

**Source file:** `Cher_2024_Exploring_machine_learning_to_support_decision-mak.md`

---
title: "Exploring machine learning to support decision-making for placement stabilization and preservation in child welfare"
zotero_key: IAW426ZK
author_year: "Cher (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1007/s10826-024-02993-x"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 3
rel_prof: 3
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Practical Implementation", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-high", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Exploring machine learning to support decision-making for placement stabilization and preservation in child welfare

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Practical Implementation, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Statewide study analyzed 12,621 child welfare cases in large Midwestern state (2017-2020) to develop machine learning models predicting placement disruption risk. Goal was to identify youth who could benefit from placement stabilization services to prevent unnecessary residential care under Family First Prevention Services Act. Random forest models were compared with conventional logistic regression for predicting placement disruption and referral to stabilization programs. ML models demonstrated moderate predictive validity with practical implications for proactive service allocation. Results showed ML could support but not replace caseworker judgment in placement decisions. Study used administrative child welfare data and evaluated model fairness considerations.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1007/s10826-024-02993-x](https://doi.org/10.1007/s10826-024-02993-x)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/IAW426ZK)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='chisca_2024_prompting_fairness__learning_prompts_for_debiasing'></a>

## Paper 38/266: Prompting fairness: Learning prompts for debiasing large language models

**Source file:** `Chisca_2024_Prompting_fairness__Learning_prompts_for_debiasing.md`

---
title: "Prompting fairness: Learning prompts for debiasing large language models"
zotero_key: MHZTET9D
author_year: "Chisca (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "https://aclanthology.org/2024.ltedi-1.6/"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Prompting fairness: Learning prompts for debiasing large language models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Introduces novel prompt-tuning method for reducing biases in encoder models like BERT and RoBERTa through training small sets of additional reusable token embeddings. Demonstrates state-of-the-art performance while maintaining minimal impact on language modeling capabilities through parameter-efficient approach applicable across different models and tasks.


## AI Summary

## Overview

This research addresses a critical challenge in contemporary NLP: mitigating social biases embedded in Large Language Models (LLMs) such as BERT and RoBERTa. The paper proposes an innovative prompt-tuning approach that efficiently reduces gender bias without requiring computationally expensive full model retraining. By training only small, reusable token embeddings that concatenate to input sequences, the authors present a practical solution to bias mitigation that maintains competitive performance while minimizing computational overhead. This work is particularly timely given the widespread deployment of LLMs in high-stakes applications where bias perpetuation poses significant risks to underrepresented groups.

## Main Findings

The research demonstrates three primary contributions: (1) a prompt-tuning method for encoder-only models that trains only small additional token embeddings rather than modifying full model parameters; (2) a novel KL-divergence-based loss function balancing bias reduction against language modeling performance preservation; and (3) an extensible template framework for gender bias that generalizes to other bias categories. Evaluations on SEAT and StereoSet benchmarks confirm competitive performance with state-of-the-art debiasing methods while maintaining minimal degradation of language modeling ability. The approach requires substantially fewer computational resources than existing alternatives, making it viable for resource-constrained environments. The template-based framework demonstrates extensibility beyond gender bias, suggesting applicability to other demographic and social bias categories, though specific performance metrics on alternative bias types remain to be established.

## Methodology/Approach

The methodology targets encoder-only models (BERT, RoBERTa) through prompt-tuning rather than parameter modification. The approach trains learnable token embeddings functioning as reusable prompts, concatenated to input sequences to guide bias reduction. A custom loss function based on Kullback-Leibler (KL) divergence balances competing objectives: reducing bias while preserving language modeling performance. Training utilizes template-based examples (e.g., "<GenderedWord> is a <Target>") systematically exposing gender stereotypes. Evaluation employs two complementary benchmarks: SEAT measures associations between demographic and target attributes in contextual embeddings, while StereoSet quantifies stereotypical word selection frequency. This dual-benchmark approach ensures comprehensive assessment of both bias reduction efficacy and model utility preservation, with results compared against existing state-of-the-art debiasing methods.

## Relevant Concepts

**Representational Harms**: Disparate system performance, exclusion, or stereotyping disadvantaging specific demographic groups in model outputs.

**Allocation Harms**: Discriminatory resource distribution resulting from biased model decisions.

**Encoder-Only Models**: Transformer architectures (BERT, RoBERTa) using bidirectional context without generative capabilities, suitable for classification and embedding tasks.

**SEAT (Sentence Embedding Association Test)**: Embedding-based metric measuring bias by quantifying associations between demographic attribute groups and target attribute groups in contextual embeddings.

**StereoSet**: Probability-based benchmark measuring bias frequency through stereotypical word selection in masked token prediction tasks.

**Prompt-Tuning**: Parameter-efficient fine-tuning approach training only small additional embeddings rather than full model parameters, reducing computational requirements.

**KL-Divergence**: Information-theoretic measure quantifying distribution divergence; here applied to balance bias reduction objectives against language modeling performance preservation.

## Significance

This work addresses a critical gap between theoretical fairness research and practical implementation constraints. By offering a computationally lightweight debiasing method, it democratizes bias mitigation for practitioners with limited computational resources. The research contributes to three interconnected domains: bias mitigation literature, prompt-learning paradigms, and practical fairness deployment. Most significantly, it demonstrates that competitive debiasing performance need not require substantial computational investment, making fairness considerations accessible across diverse organizational contexts. The extensible template framework provides a foundation for addressing multiple bias categories, advancing toward comprehensive fairness solutions in LLM applications. However, generalization to non-gender biases and scalability across diverse model architectures require further investigation.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://aclanthology.org/2024.ltedi-1.6/
- **Zotero:** [Open in Zotero](zotero://select/items/MHZTET9D)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='chisca_2024_prompting_techniques_for_reducing_social_bias_in_l'></a>

## Paper 39/266: Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes

**Source file:** `Chisca_2024_Prompting_techniques_for_reducing_social_bias_in_L.md`

---
title: "Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes"
zotero_key: UDWBF53B
author_year: "Chisca (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/html/2404.17218v3"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: true
summary_file: "summary_Chisca_2024_Prompting.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

## Overview

This research addresses a critical challenge in contemporary NLP: mitigating social biases embedded in Large Language Models (LLMs) such as BERT and RoBERTa. The paper proposes an innovative prompt-tuning approach that efficiently reduces gender bias without requiring computationally expensive full model retraining. By training only small, reusable token embeddings that concatenate to input sequences, the authors present a practical solution to bias mitigation that maintains competitive performance while minimizing computational overhead. This work is particularly timely given the widespread deployment of LLMs in high-stakes applications where bias perpetuation poses significant risks to underrepresented groups.

## Main Findings

The research demonstrates three primary contributions: (1) a prompt-tuning method for encoder-only models that trains only small additional token embeddings rather than modifying full model parameters; (2) a novel KL-divergence-based loss function balancing bias reduction against language modeling performance preservation; and (3) an extensible template framework for gender bias that generalizes to other bias categories. Evaluations on SEAT and StereoSet benchmarks confirm competitive performance with state-of-the-art debiasing methods while maintaining minimal degradation of language modeling ability. The approach requires substantially fewer computational resources than existing alternatives, making it viable for resource-constrained environments. The template-based framework demonstrates extensibility beyond gender bias, suggesting applicability to other demographic and social bias categories, though specific performance metrics on alternative bias types remain to be established.

## Methodology/Approach

The methodology targets encoder-only models (BERT, RoBERTa) through prompt-tuning rather than parameter modification. The approach trains learnable token embeddings functioning as reusable prompts, concatenated to input sequences to guide bias reduction. A custom loss function based on Kullback-Leibler (KL) divergence balances competing objectives: reducing bias while preserving language modeling performance. Training utilizes template-based examples (e.g., "<GenderedWord> is a <Target>") systematically exposing gender stereotypes. Evaluation employs two complementary benchmarks: SEAT measures associations between demographic and target attributes in contextual embeddings, while StereoSet quantifies stereotypical word selection frequency. This dual-benchmark approach ensures comprehensive assessment of both bias reduction efficacy and model utility preservation, with results compared against existing state-of-the-art debiasing methods.

## Relevant Concepts

**Representational Harms**: Disparate system performance, exclusion, or stereotyping disadvantaging specific demographic groups in model outputs.

**Allocation Harms**: Discriminatory resource distribution resulting from biased model decisions.

**Encoder-Only Models**: Transformer architectures (BERT, RoBERTa) using bidirectional context without generative capabilities, suitable for classification and embedding tasks.

**SEAT (Sentence Embedding Association Test)**: Embedding-based metric measuring bias by quantifying associations between demographic attribute groups and target attribute groups in contextual embeddings.

**StereoSet**: Probability-based benchmark measuring bias frequency through stereotypical word selection in masked token prediction tasks.

**Prompt-Tuning**: Parameter-efficient fine-tuning approach training only small additional embeddings rather than full model parameters, reducing computational requirements.

**KL-Divergence**: Information-theoretic measure quantifying distribution divergence; here applied to balance bias reduction objectives against language modeling performance preservation.

## Significance

This work addresses a critical gap between theoretical fairness research and practical implementation constraints. By offering a computationally lightweight debiasing method, it democratizes bias mitigation for practitioners with limited computational resources. The research contributes to three interconnected domains: bias mitigation literature, prompt-learning paradigms, and practical fairness deployment. Most significantly, it demonstrates that competitive debiasing performance need not require substantial computational investment, making fairness considerations accessible across diverse organizational contexts. The extensible template framework provides a foundation for addressing multiple bias categories, advancing toward comprehensive fairness solutions in LLM applications. However, generalization to non-gender biases and scalability across diverse model architectures require further investigation.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2404.17218v3
- **Zotero:** [Open in Zotero](zotero://select/items/UDWBF53B)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='chiu_2024_what_are_artificial_intelligence_literacy_and_comp'></a>

## Paper 40/266: What are artificial intelligence literacy and competency? A comprehensive framework to support them

**Source file:** `Chiu_2024_What_are_artificial_intelligence_literacy_and_comp.md`

---
title: "What are artificial intelligence literacy and competency? A comprehensive framework to support them"
zotero_key: I49J8KMK
author_year: "Chiu (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: en
doi: "10.1016/j.caeo.2024.100171"
url: "https://linkinghub.elsevier.com/retrieve/pii/S2666557324000120"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# What are artificial intelligence literacy and competency? A comprehensive framework to support them

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1016/j.caeo.2024.100171](https://doi.org/10.1016/j.caeo.2024.100171)
- **URL:** https://linkinghub.elsevier.com/retrieve/pii/S2666557324000120
- **Zotero:** [Open in Zotero](zotero://select/items/I49J8KMK)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='chiu_2025_ai_literacy_and_competency__definitions,_framework'></a>

## Paper 41/266: AI literacy and competency: definitions, frameworks, development and future research directions

**Source file:** `Chiu_2025_AI_literacy_and_competency__definitions,_framework.md`

---
title: "AI literacy and competency: definitions, frameworks, development and future research directions"
zotero_key: 3FXYBAWC
author_year: "Chiu (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: en
doi: "10.1080/10494820.2025.2514372"
url: "https://www.tandfonline.com/doi/full/10.1080/10494820.2025.2514372"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI literacy and competency: definitions, frameworks, development and future research directions

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1080/10494820.2025.2514372](https://doi.org/10.1080/10494820.2025.2514372)
- **URL:** https://www.tandfonline.com/doi/full/10.1080/10494820.2025.2514372
- **Zotero:** [Open in Zotero](zotero://select/items/3FXYBAWC)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='choudhury_2024_large_language_models_and_user_trust__consequence_'></a>

## Paper 42/266: Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals

**Source file:** `Choudhury_2024_Large_Language_Models_and_User_Trust__Consequence_.md`

---
title: "Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals"
zotero_key: HR4KR4YL
author_year: "Choudhury (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.2196/56764"
url: "https://www.jmir.org/2024/1/e56764/"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 1
rel_bias: 3
rel_praxis: 2
rel_prof: 2
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Practical Implementation"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-bias-high", "dim-praxis-medium", "dim-prof-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Bias Analysis, Practical Implementation |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

Peer-reviewed viewpoint discussing integration of LLMs into healthcare requiring balance between trust and skepticism. Warns that "blind trust" in LLM recommendations can lead to automation bias and confirmation bias as clinicians may accept AI outputs uncritically. Argues that prompting for transparency in LLM reasoning enhances professionals' trust by making AI reasoning visible and verifiable. Emphasizes need for human oversight and bias mitigation to ensure LLMs complement rather than compromise expert decision-making.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.2196/56764](https://doi.org/10.2196/56764)
- **URL:** https://www.jmir.org/2024/1/e56764/
- **Zotero:** [Open in Zotero](zotero://select/items/HR4KR4YL)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='ciston_2024_intersectional_artificial_intelligence_is_essentia'></a>

## Paper 43/266: Intersectional Artificial Intelligence Is Essential: Polyvocal, Multimodal, Experimental Methods to Save AI

**Source file:** `Ciston_2024_Intersectional_Artificial_Intelligence_Is_Essentia.md`

---
title: "Intersectional Artificial Intelligence Is Essential: Polyvocal, Multimodal, Experimental Methods to Save AI"
zotero_key: 8GG7KEEY
author_year: "Ciston (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.7559/CITARJ.V11I2.665"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Intersectional Artificial Intelligence Is Essential: Polyvocal, Multimodal, Experimental Methods to Save AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Arguments for applying intersectional strategies to AI at all levels - from data through design to implementation. Intersectionality, integrating institutional power analysis and queer-feminist plus critical race theories, can contribute to AI reconceptualization. Intersectional framework enables analysis of existing AI biases and uncovering alternative ethics from counter-narratives. Research presents intersectional strategies as polyvocal, multimodal, and experimental, with community-focused and artistic practices helping explore AI's intersectional possibilities. Practical examples include Data Nutrition Label for bias assessment in datasets and experimental projects like "ladymouth," a chatbot explaining feminism.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.7559/CITARJ.V11I2.665](https://doi.org/10.7559/CITARJ.V11I2.665)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/8GG7KEEY)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='clemmer_2024_precisedebias__an_automatic_prompt_engineering_app'></a>

## Paper 44/266: PreciseDebias: An automatic prompt engineering approach for generative AI to mitigate image demographic biases

**Source file:** `Clemmer_2024_PreciseDebias__An_automatic_prompt_engineering_app.md`

---
title: "PreciseDebias: An automatic prompt engineering approach for generative AI to mitigate image demographic biases"
zotero_key: I32VV8SK
author_year: "Clemmer (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "https://openaccess.thecvf.com/content/WACV2024/html/Clemmer_PreciseDebias_An_Automatic_Prompt_Engineering_Approach_for_Generative_AI_WACV_2024_paper.html"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# PreciseDebias: An automatic prompt engineering approach for generative AI to mitigate image demographic biases

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

This paper presents a technical solution for reducing demographic bias in AI image generators through "PreciseDebias," an automated approach that rewrites simple prompts into more complex, diversity-reflective prompts. The system analyzes model bias and strategically adds attributes like ethnicity, gender, or age to enforce fairer representation distributions, demonstrating that diversity-reflective prompting can be automated at the system level.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://openaccess.thecvf.com/content/WACV2024/html/Clemmer_PreciseDebias_An_Automatic_Prompt_Engineering_Approach_for_Generative_AI_WACV_2024_paper.html
- **Zotero:** [Open in Zotero](zotero://select/items/I32VV8SK)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='colombatto_2025_the_influence_of_mental_state_attributions_on_trus'></a>

## Paper 45/266: The influence of mental state attributions on trust in large language models

**Source file:** `Colombatto_2025_The_influence_of_mental_state_attributions_on_trus.md`

---
title: "The influence of mental state attributions on trust in large language models"
zotero_key: 7XBEESMD
author_year: "Colombatto (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1038/s44271-025-00262-1"
url: "https://www.nature.com/articles/s44271-025-00262-1"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 1
rel_bias: 2
rel_praxis: 2
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["AI Literacy", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-ai-komp-medium", "dim-bias-medium", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Colombatto_2025_influence.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# The influence of mental state attributions on trust in large language models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | AI Literacy, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Empirical study examining how users' beliefs about LLM's "mind" affect trust in advice. Preregistered experiment (N=410) showing attributing intelligence to LLM strongly increased trust, whereas attributing human-like consciousness did not increase trust. Higher ascriptions of sentience correlated with less advice-taking. Participants trusted AI for perceived analytical competence, not for having feelings. Suggests prompt-engineering highlighting competence and reliability more effective than anthropomorphism for building appropriate trust.


## AI Summary

!summary_Colombatto_2025_influence.md


## Links & Resources

- **DOI:** [10.1038/s44271-025-00262-1](https://doi.org/10.1038/s44271-025-00262-1)
- **URL:** https://www.nature.com/articles/s44271-025-00262-1
- **Zotero:** [Open in Zotero](zotero://select/items/7XBEESMD)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='creswell_b√°ez_2025_clinical_social_workers‚Äô_perceptions_of_large_lang'></a>

## Paper 46/266: Clinical Social Workers‚Äô Perceptions of Large Language Models in Practice: Resistance to Automation and Prospects for Integration

**Source file:** `Creswell_B√°ez_2025_Clinical_Social_Workers‚Äô_Perceptions_of_Large_Lang.md`

---
title: "Clinical Social Workers‚Äô Perceptions of Large Language Models in Practice: Resistance to Automation and Prospects for Integration"
zotero_key: TKHRE6ZY
author_year: "Creswell B√°ez (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1080/26408066.2025.2542450"
url: "https://doi.org/10.1080/26408066.2025.2542450"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 1
rel_bias: 2
rel_praxis: 2
rel_prof: 3
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Professional Context", "AI Literacy"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-medium", "dim-bias-medium", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Clinical Social Workers‚Äô Perceptions of Large Language Models in Practice: Resistance to Automation and Prospects for Integration

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Professional Context, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Qualitative study (interviews, reflexive thematic analysis) of clinicians exposed to LLM-supported consultation. Identifies efficiency and documentation support alongside concerns over confidentiality, loss of nuance, and reduced empathy. Concludes AI should augment‚Äînot replace‚Äîclinical judgment, requiring training and ethics.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1080/26408066.2025.2542450](https://doi.org/10.1080/26408066.2025.2542450)
- **URL:** https://doi.org/10.1080/26408066.2025.2542450
- **Zotero:** [Open in Zotero](zotero://select/items/TKHRE6ZY)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='cvoelcker_2023_queer_in_ai__a_case_study_in_community-led_partici'></a>

## Paper 47/266: Queer in AI: A case study in community-led participatory AI

**Source file:** `Cvoelcker_2023_Queer_in_AI__A_case_study_in_community-led_partici.md`

---
title: "Queer in AI: A case study in community-led participatory AI"
zotero_key: ZQAKXVE6
author_year: "Cvoelcker (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "https://cvoelcker.de/assets/pdf/paper_queer_in_ai.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Cvoelcker_2023_Queer.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Queer in AI: A case study in community-led participatory AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Fallstudie zu Queer in AI, dokumentiert Sch√§den durch KI-Systeme an queeren Menschen und beschreibt community-geleitete Strategien f√ºr partizipative, faire KI.


## AI Summary

## Overview

The Queer in AI case study documents a globally distributed, community-led participatory initiative addressing systemic underrepresentation of LGBTQ+ professionals within artificial intelligence. Comprising 50+ contributors across 15+ countries embedded in leading research institutions (Carnegie Mellon, MIT, UCLA, UC San Diego), the project exemplifies grassroots activism within technology sectors. The research investigates how marginalized communities can self-organize to create sustainable support networks, advocacy mechanisms, and systemic change‚Äîimplicitly questioning whether community-driven approaches prove more effective than traditional top-down institutional diversity initiatives. The work positions community members as simultaneous researchers and subjects, challenging conventional academic hierarchies.

## Main Findings

The document reveals five critical findings about community-led organizing in technology. First, **distributed international networks successfully coordinate collective action across 15+ countries**, demonstrating scalability beyond single institutions. Second, **strategic institutional embeddedness amplifies impact**: members' positions within prestigious universities provide credibility and resource access while maintaining organizational autonomy from institutional control. Third, **intersectional organizing addresses compounded marginalization**‚Äîtargeting overlapping barriers faced by LGBTQ+ professionals in male-dominated, heteronormative AI fields. Fourth, **participatory authorship constitutes methodological innovation**: the 50+ author structure proves that rigorous academic work can center community voice and distributed leadership simultaneously. Fifth, **the model demonstrates replicability**: the organizational structure and geographic distribution suggest applicability to other marginalized communities within technology sectors.

## Methodology/Approach

The case study employs **participatory action research** where community members simultaneously document and shape their own organizing‚Äîinverting traditional research hierarchies. This methodology integrates **social movement theory** (analyzing collective mobilization of marginalized groups) and **organizational studies** (examining distributed governance without hierarchical centralization). The 50+ distributed authorship across multiple continents operationalizes participatory methodology, treating community members as legitimate knowledge producers rather than research subjects. Data sources include organizational structure documentation, membership mapping across institutions and geographies, and institutional affiliation analysis. The approach privileges **epistemic justice**‚Äîrecognizing that LGBTQ+ AI professionals possess essential insights about systemic barriers inaccessible to external researchers. This methodology validates lived experience as rigorous empirical evidence.

## Relevant Concepts

**Participatory action research**: Research conducted *with* communities rather than *about* them, emphasizing community agency in problem-solving and knowledge production.

**Distributed leadership**: Organizational governance without hierarchical centralization, enabling coordination across geographic and institutional boundaries while preventing power concentration.

**Intersectionality**: Framework analyzing how overlapping social identities (LGBTQ+ status, professional position, geographic location) create compounded experiences of marginalization and unique organizing needs.

**Epistemic justice**: Recognition of marginalized groups as legitimate knowledge producers; rejection of epistemic hierarchies that privilege institutional over community expertise.

**Grassroots activism**: Bottom-up social change initiated and controlled by affected communities rather than institutional authorities.

**Strategic institutional embeddedness**: Positioning community members within established institutions to access resources and credibility while maintaining organizational independence.

## Significance

This work advances multiple scholarly domains simultaneously. Within **AI ethics and fairness**, it provides empirical evidence that community-led models generate insights institutional approaches miss. For **science and technology studies**, it demonstrates how marginalized groups reshape technical fields through collective organizing and knowledge production. In **organizational sociology**, it documents viable alternatives to hierarchical governance with measurable global reach. Critically, the document challenges epistemic hierarchies by positioning community members as researchers‚Äîwith direct implications for academic publishing, institutional credibility assessment, and diversity initiative design. By centering LGBTQ+ voices in AI discourse while modeling participatory scholarship, the work advances both equity and methodological innovation. The demonstrated replicability suggests other marginalized communities could adopt this organizational model, potentially transforming how technology fields address systemic exclusion.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://cvoelcker.de/assets/pdf/paper_queer_in_ai.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/ZQAKXVE6)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='d'ignazio_2024_data_feminism_for_ai'></a>

## Paper 48/266: Data Feminism for AI

**Source file:** `D'Ignazio_2024_Data_Feminism_for_AI.md`

---
title: "Data Feminism for AI"
zotero_key: G82RHBDD
author_year: "D'Ignazio (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/html/2405.01286v1"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: true
summary_file: "summary_D_Ignazio_2024_Data.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Data Feminism for AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Comprehensive feminist framework directly critiques individualized approaches to AI ethics, challenging the "liberal framework of making algorithms unbiased and inclusive" in favor of structural "remediation" addressing "systemic and structural dimensions of discrimination." Examines how AI research is captured by "racial, gendered capitalism" and proposes nine principles focusing on structural power analysis including examining power, challenging power, and making labor visible. Explicitly positions against technical solutions ignoring power structures.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2405.01286v1
- **Zotero:** [Open in Zotero](zotero://select/items/G82RHBDD)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='de_duro_2025_measuring_and_identifying_factors_of_individuals'_'></a>

## Paper 49/266: Measuring and identifying factors of individuals' trust in large language models

**Source file:** `De_Duro_2025_Measuring_and_identifying_factors_of_individuals'_.md`

---
title: "Measuring and identifying factors of individuals' trust in large language models"
zotero_key: 4EI9H33G
author_year: "De Duro (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.48550/arXiv.2502.21028"
url: "https://arxiv.org/html/2502.21028v1"

# Assessment
decision: Unclear
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 2
rel_prof: 1
total_relevance: 4

# Categorization
relevance_category: low
top_dimensions: ["Practical Implementation", "AI Literacy"]

# Tags
tags: ["paper", "unclear", "low-relevance", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Measuring and identifying factors of individuals' trust in large language models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Unclear** |
| **Total Relevance** | **4/15** (low) |
| **Top Dimensions** | Practical Implementation, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Study developing "Trust-In-LLMs Index (TILLMI)" psychometric scale to quantify trust in conversational AI. Survey of ~1,000 U.S. adults identified two trust dimensions: affective component (emotional closeness) and cognitive component (reliance on competence). Found significant individual differences with personality factors influencing trust. Prior LLM experience increased trust while those with no direct use were more skeptical. Highlights need for prompt-engineering and system design to account for user differences.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.48550/arXiv.2502.21028](https://doi.org/10.48550/arXiv.2502.21028)
- **URL:** https://arxiv.org/html/2502.21028v1
- **Zotero:** [Open in Zotero](zotero://select/items/4EI9H33G)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='debnath_2024_can_llms_reason_about_trust__a_pilot_study'></a>

## Paper 50/266: Can LLMs reason about trust? A pilot study

**Source file:** `Debnath_2024_Can_LLMs_reason_about_trust__A_pilot_study.md`

---
title: "Can LLMs reason about trust? A pilot study"
zotero_key: VLC4TVQT
author_year: "Debnath (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://arxiv.org/html/2507.21075v1"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 0
rel_bias: 1
rel_praxis: 2
rel_prof: 1
total_relevance: 6

# Categorization
relevance_category: medium
top_dimensions: ["AI Literacy", "Practical Implementation"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-ai-komp-medium", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Debnath_2024_LLMs.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Can LLMs reason about trust? A pilot study

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **6/15** (medium) |
| **Top Dimensions** | AI Literacy, Practical Implementation |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Pilot study investigating LLMs' ability to reason about trust between individuals by capturing complex mental states that exceed traditional symbolic approaches. Evaluates whether LLMs can analyze conversations and assess trust levels based on willingness, competence, and security factors. Demonstrates that LLMs can identify individual goals and develop improvement suggestions for trust building, as well as generate strategic action plans for trust promotion. Suggests LLMs have potential to bridge the gap between computational and socio-cognitive trust models.


## AI Summary

!summary_Debnath_2024_LLMs.md


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2507.21075v1
- **Zotero:** [Open in Zotero](zotero://select/items/VLC4TVQT)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='dencik_2024_automated_government_benefits_and_welfare_surveill'></a>

## Paper 51/266: Automated government benefits and welfare surveillance

**Source file:** `Dencik_2024_Automated_government_benefits_and_welfare_surveill.md`

---
title: "Automated government benefits and welfare surveillance"
zotero_key: TIX2ZW6A
author_year: "Dencik (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://ojs.library.queensu.ca/index.php/surveillance-and-society/article/view/16107"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 2
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-prof-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Automated government benefits and welfare surveillance

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

Critical surveillance studies analysis examining digital welfare state historically, presently, and prospectively, focusing on AI-driven welfare surveillance systems. Authors argue problems posed by AI in public administration are often misattributed to technological novelty when they actually represent historically familiar patterns of surveillance and control. Drawing on bureaucracy, welfare state, and automation scholarship, demonstrates how algorithmic fraud detection and chatbot assistance systems extend long-standing practices of scrutinizing and disciplining marginalized populations. Critical insights include analysis of how automation enables unprecedented scale of surveillance while maintaining opacity through algorithmic systems' auditable veneer. Examines cases from Netherlands and other jurisdictions showing how automated welfare systems amplify existing power asymmetries and inequality.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://ojs.library.queensu.ca/index.php/surveillance-and-society/article/view/16107
- **Zotero:** [Open in Zotero](zotero://select/items/TIX2ZW6A)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='derechos_digitales_2023_feminist_reflections_for_the_development_of_artifi'></a>

## Paper 52/266: Feminist reflections for the development of Artificial Intelligence

**Source file:** `Derechos_Digitales_2023_Feminist_reflections_for_the_development_of_Artifi.md`

---
title: "Feminist reflections for the development of Artificial Intelligence"
zotero_key: Q5VIXYTS
author_year: "Derechos Digitales (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: report
language: nan
doi: "nan"
url: "https://www.derechosdigitales.org/fair-2023-en/"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-medium", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Feminist reflections for the development of Artificial Intelligence

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Synthesizes conversations among Latin American women developing AI systems, providing methodological frameworks for feminist AI development based on four real-world projects. Emphasizes co-design methodologies, digital autonomy, data sovereignty, and intersectional approaches through community agreements and power-balancing strategies.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.derechosdigitales.org/fair-2023-en/
- **Zotero:** [Open in Zotero](zotero://select/items/Q5VIXYTS)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='deuze_2022_imagination,_algorithms_and_news__developing_ai_li'></a>

## Paper 53/266: Imagination, Algorithms and News: Developing AI Literacy for Journalism

**Source file:** `Deuze_2022_Imagination,_Algorithms_and_News__Developing_AI_Li.md`

---
title: "Imagination, Algorithms and News: Developing AI Literacy for Journalism"
zotero_key: 4CK7LDP3
author_year: "Deuze (2022)"
authors: []

# Publication
publication_year: 2022.0
item_type: journalArticle
language: en
doi: "10.1080/21670811.2022.2119152"
url: "https://www.tandfonline.com/doi/full/10.1080/21670811.2022.2119152"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Imagination, Algorithms and News: Developing AI Literacy for Journalism

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2022.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1080/21670811.2022.2119152](https://doi.org/10.1080/21670811.2022.2119152)
- **URL:** https://www.tandfonline.com/doi/full/10.1080/21670811.2022.2119152
- **Zotero:** [Open in Zotero](zotero://select/items/4CK7LDP3)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='dilek_2025_ai_literacy_in_teacher_education__empowering_educa'></a>

## Paper 54/266: AI literacy in teacher education: Empowering educators through critical co-discovery

**Source file:** `Dilek_2025_AI_literacy_in_teacher_education__Empowering_educa.md`

---
title: "AI literacy in teacher education: Empowering educators through critical co-discovery"
zotero_key: TGIXGKR6
author_year: "Dilek (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1177/00224871251325083"
url: "nan"

# Assessment
decision: Exclude
exclusion_reason: "Wrong publication type"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI literacy in teacher education: Empowering educators through critical co-discovery

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Wrong publication type


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Implements critical co-discovery approaches within AI teacher education to move beyond technical automation toward critical pedagogical engagement. Through co-discovery activities, educators developed understanding of AI concepts, ethical considerations, and context-specific applications while co-constructing knowledge. Emphasizes that prolonged engagement with AI literacy integrated into teacher education programs enables educators to critically navigate AI systems and examine broader pedagogical and ethical implications, prioritizing critical examination of AI's power dynamics and social justice implications.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1177/00224871251325083](https://doi.org/10.1177/00224871251325083)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/TGIXGKR6)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='diversifair_project_2024_ai_&_intersectionality__a_toolkit_for_fairness_&_i'></a>

## Paper 55/266: AI & Intersectionality: A Toolkit For Fairness & Inclusion

**Source file:** `DIVERSIFAIR_Project_2024_AI_&_Intersectionality__A_Toolkit_For_Fairness_&_I.md`

---
title: "AI & Intersectionality: A Toolkit For Fairness & Inclusion"
zotero_key: 9XWIAWFN
author_year: "DIVERSIFAIR Project (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://diversifair-project.eu/wp-content/uploads/2025/01/Copie-de-INDUSTRY-Educ-Kits-A4-V2.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 11

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-medium", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_DIVERSIFAIR_Project_2024_Intersectionality.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI & Intersectionality: A Toolkit For Fairness & Inclusion

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **11/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Das DIVERSIFAIR-Toolkit ist eine praktische Ressource, die sich an politische Entscheidungstr√§ger*innen, die Industrie und die Zivilgesellschaft richtet. Es zielt darauf ab, ein Bewusstsein f√ºr intersektionale Diskriminierung in KI-Systemen zu schaffen und konkrete Handlungsstrategien zur Risikominderung anzubieten. Das Toolkit betont die Notwendigkeit, √ºber einzelne Diskriminierungsachsen (wie Geschlecht oder Herkunft) hinauszudenken und deren Verschr√§nkungen zu analysieren. Es f√∂rdert eine KI-Kompetenz, die es Stakeholdern erm√∂glicht, KI-Systeme √ºber ihren gesamten Lebenszyklus hinweg ‚Äì von der Datensammlung √ºber das Design bis zur Anwendung ‚Äì auf intersektionale Risiken zu pr√ºfen. F√ºr das Prompting bedeutet dies, gezielt Szenarien zu entwerfen, die marginalisierte Identit√§ten an der Schnittstelle mehrerer Merkmale repr√§sentieren, um blinde Flecken und stereotype Assoziationen in KI-Modellen aufzudecken.


## AI Summary

!summary_DIVERSIFAIR_Project_2024_Intersectionality.md


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://diversifair-project.eu/wp-content/uploads/2025/01/Copie-de-INDUSTRY-Educ-Kits-A4-V2.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/9XWIAWFN)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='dixon_2018_measuring_and_mitigating_unintended_bias_in_text_d'></a>

## Paper 56/266: Measuring and mitigating unintended bias in text data

**Source file:** `Dixon_2018_Measuring_and_mitigating_unintended_bias_in_text_d.md`

---
title: "Measuring and mitigating unintended bias in text data"
zotero_key: 95UW59KT
author_year: "Dixon (2018)"
authors: []

# Publication
publication_year: 2018.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "nan"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Measuring and mitigating unintended bias in text data

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2018.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/95UW59KT)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='djeffal_2025_reflexive_prompt_engineering__a_framework_for_resp'></a>

## Paper 57/266: Reflexive prompt engineering: A framework for responsible prompt engineering and AI interaction design

**Source file:** `Djeffal_2025_Reflexive_prompt_engineering__A_framework_for_resp.md`

---
title: "Reflexive prompt engineering: A framework for responsible prompt engineering and AI interaction design"
zotero_key: YH8KK797
author_year: "Djeffal (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: conferencePaper
language: nan
doi: "10.1145/3715275.3732118"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 3
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["AI Literacy", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-high", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Reflexive prompt engineering: A framework for responsible prompt engineering and AI interaction design

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | AI Literacy, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This paper proposes "Reflexive Prompt Engineering" as a comprehensive framework to embed ethical and inclusive principles into prompt crafting for generative AI. Framework consists of five components: prompt design, system selection, system configuration, performance evaluation, and prompt management, each considered from social responsibility perspective. Positions responsible prompt engineering as essential component of AI literacy, bridging gap between AI development and deployment by aligning AI behavior with human rights and diversity values.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1145/3715275.3732118](https://doi.org/10.1145/3715275.3732118)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/YH8KK797)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='djiberou_mahamadou_2024_revisiting_technical_bias_mitigation_strategies'></a>

## Paper 58/266: Revisiting Technical Bias Mitigation Strategies

**Source file:** `Djiberou_Mahamadou_2024_Revisiting_Technical_Bias_Mitigation_Strategies.md`

---
title: "Revisiting Technical Bias Mitigation Strategies"
zotero_key: KQU9D6DL
author_year: "Djiberou Mahamadou (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/abs/2410.17433"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Djiberou_Mahamadou_2024_Revisiting.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Revisiting Technical Bias Mitigation Strategies

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Diese systematische √úberpr√ºfung identifiziert praktische Limitationen technischer Bias-Mitigation-Strategien im Gesundheitswesen entlang f√ºnf Schl√ºsseldimensionen: wer Bias und Fairness definiert, welche Mitigation-Strategie zu verwenden und zu priorisieren ist, wann in den KI-Entwicklungsstadien die L√∂sungen am effektivsten sind, f√ºr welche Populationen und in welchem Kontext die L√∂sungen entworfen sind. Die Studie zeigt mathematische Inkonsistenzen und Inkompatibilit√§ten zwischen verschiedenen Fairness-Metriken auf und diskutiert, wie werteorientierte KI stakeholder-bezogene Ans√§tze zur Bias-Mitigation erm√∂glichen kann.


## AI Summary

## Overview

This Stanford-based review systematically examines why technical approaches to bias mitigation in healthcare AI frequently fail in real-world implementation despite their theoretical soundness. The authors conduct a critical analysis of practical limitations inherent in current bias mitigation strategies, departing from the dominant engineering-centric paradigm by arguing that bias and fairness are fundamentally socio-technical challenges requiring stakeholder engagement and contextual sensitivity. The review is structured around five critical dimensions determining implementation success: (1) who defines bias and fairness, (2) which mitigation strategies to prioritize among incompatible alternatives, (3) when interventions should occur in the development pipeline, (4) which populations benefit from specific solutions, and (5) the broader organizational and clinical contexts shaping deployment. The work is particularly urgent given healthcare's high stakes‚Äîwhere algorithmic errors have life-or-death consequences‚Äîand the potential for AI to either reduce or exacerbate existing health inequities, especially in low-resource settings.

## Main Findings

The authors identify critical implementation gaps that technical solutions alone cannot address. First, bias operates through compounding mechanisms where data biases, algorithmic biases (minority bias, label bias), and user-interaction biases (clinician and patient biases) interact synergistically, creating detection and mitigation challenges exceeding technical scope. Second, regulatory frameworks‚Äîincluding federal civil rights laws‚Äîstruggle to establish causality between algorithmic decisions and patient harm, creating legal ambiguity that undermines enforcement and accountability. Third, the proliferation of incompatible and inconsistent mitigation strategies lacks clear prioritization mechanisms, forcing practitioners to make ad-hoc choices without principled guidance. Fourth, the effectiveness of bias mitigation strategies varies unpredictably across different patient populations and healthcare contexts, suggesting universal technical solutions are inappropriate. Fifth, current technical approaches systematically exclude stakeholder values‚Äîthose of patients, clinicians, and affected communities‚Äîfrom solution design. Finally, the review identifies that despite AI's demonstrated superiority in medical imaging and other applications, and its substantial economic potential, these advances risk amplifying health inequities without proper governance frameworks.

## Methodology/Approach

The review employs structured dimensional analysis examining five implementation factors across empirical healthcare case studies rather than theoretical abstraction. The theoretical framework integrates value-sensitive design‚Äîa technology ethics approach emphasizing stakeholder participation‚Äîas a corrective to purely technical perspectives. This methodological choice represents a deliberate shift from engineering-centric to socio-technical analysis. The authors ground arguments in concrete biomedical applications, demonstrating how technical solutions fail in practice. The review also examines how algorithmic anti-discrimination laws and AI regulation initiatives attempt to address bias but remain insufficient without addressing underlying implementation challenges.

## Relevant Concepts

**Technical Solutionism**: The assumption that complex social problems can be resolved through technical interventions alone, without addressing underlying value conflicts or contextual factors.

**Value-Sensitive Design**: A framework ensuring that stakeholder values are systematically incorporated into technology design processes, moving beyond purely functional considerations.

**Compounding Bias**: The synergistic interaction of multiple bias sources (data biases, algorithmic biases, user-interaction biases) that amplifies discriminatory effects beyond individual components.

**Health Inequities**: Systematic disparities in health outcomes across populations, which AI systems can either mitigate or exacerbate through biased decision-making.

**Socio-Technical Systems**: Integrated frameworks recognizing that technology implementation depends equally on social, organizational, and institutional factors alongside technical capabilities.

**Algorithmic Anti-Discrimination**: Legal and regulatory approaches attempting to prevent AI systems from systematically discriminating against protected groups.

## Significance

This work challenges the prevailing assumption that bias mitigation is primarily an engineering problem, reframing it as a governance and values challenge requiring stakeholder engagement. By positioning healthcare as a critical test case with life-or-death stakes, the authors demonstrate that high-stakes domains require fundamentally different approaches than general AI applications. The emphasis on the five implementation dimensions provides practitioners with structured frameworks for identifying where technical solutions fail. The review's significance lies in bridging technical AI ethics literature with social science critiques, offering healthcare institutions evidence-based rationales for integrating value-sensitive frameworks into bias mitigation strategies. The work also highlights the particular urgency for low-resource settings where AI deployment could either democratize healthcare access or perpetuate existing inequities. By providing practical recommendations alongside theoretical critique, the review positions itself as actionable guidance for healthcare organizations, regulators, and AI developers navigating the complex landscape of fairness implementation.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/abs/2410.17433
- **Zotero:** [Open in Zotero](zotero://select/items/KQU9D6DL)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='engelhardt_2025_voll_(dia)logisch__ein_werkstattbericht_√ºber_den_e'></a>

## Paper 59/266: Voll (dia)logisch? Ein Werkstattbericht √ºber den Einsatz von generativer KI in der Hochschulbildung f√ºr Soziale Arbeit ‚Äì Curriculare √úberlegungen und ver√§nderte Akteurskonstellationen

**Source file:** `Engelhardt_2025_Voll_(dia)logisch__Ein_Werkstattbericht_√ºber_den_E.md`

---
title: "Voll (dia)logisch? Ein Werkstattbericht √ºber den Einsatz von generativer KI in der Hochschulbildung f√ºr Soziale Arbeit ‚Äì Curriculare √úberlegungen und ver√§nderte Akteurskonstellationen"
zotero_key: MCLFUR45
author_year: "Engelhardt (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: bookSection
language: nan
doi: "nan"
url: "https://www.pedocs.de/volltexte/2025/33133/pdf/Engelhardt_Ley_2025_Voll_dia_logisch.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 3
rel_vulnerable: 1
rel_bias: 1
rel_praxis: 2
rel_prof: 3
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["AI Literacy", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-high", "dim-praxis-medium", "dim-prof-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Engelhardt_2025_Voll.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Voll (dia)logisch? Ein Werkstattbericht √ºber den Einsatz von generativer KI in der Hochschulbildung f√ºr Soziale Arbeit ‚Äì Curriculare √úberlegungen und ver√§nderte Akteurskonstellationen

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | AI Literacy, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Werkstattbericht zur curricularen Integration generativer KI. Positioniert Prompting als metakognitive Schl√ºsselkompetenz und diskutiert Rollenwandel von Lehr-/Lernakteuren; fordert reflektierte, ethisch eingebettete Nutzung mit Fokus auf kritische Validierung von KI-Ergebnissen.


## AI Summary

## Overview

Engelhardt and Ley's chapter addresses the critical challenge of integrating generative AI into social work higher education while maintaining academic integrity and critical thinking. Published in a 2025 edited volume on digitalization in social work education, this work is explicitly framed as a "Werkstattbericht" (workshop report)‚Äîa methodological choice emphasizing exploratory, practice-based inquiry rather than definitive conclusions. The authors position themselves as "learning teachers" actively experimenting with AI technologies in their own pedagogical contexts. The chapter's title‚Äî"Voll (dia)logisch?" (Fully Dialogical?)‚Äîsignals concern with preserving dialogical, relational dimensions of social work education amid technological transformation. The work deliberately transcends simplistic benefit-risk dichotomies to examine AI's transformative implications for curriculum design, pedagogical practice, and the reconstituted roles of educators and learners.

## Main Findings

The authors identify several critical insights grounded in three concrete application scenarios. First, **academic writing assistance** demonstrates AI's potential to support student composition while requiring explicit safeguards against plagiarism and passive dependency. Second, **methodological practice partnerships** position AI as a sparring partner for developing social work skills (interviewing, case analysis, ethical reasoning), enriching pedagogical diversity. Third, **chatbots as productive environments** enable students to engage with complex scenarios and receive immediate feedback, supporting experiential learning. Across these scenarios, the authors conclude that generative AI presents genuine pedagogical opportunities when thoughtfully implemented. However, successful integration demands explicit institutional attention to academic honesty, cultivation of critical analytical competencies, and reconceptualization of teacher-student roles. The authors argue that polarized opportunity-risk framings inadequately capture AI's transformative implications, advocating sophisticated integration strategies that acknowledge disciplinary values‚Äîparticularly social work's emphasis on relational practice, ethical judgment, and reflective practice.

## Methodology/Approach

The document employs qualitative, exploratory methodology characteristic of workshop reports. Rather than formal hypothesis testing, the authors adopt reflective, practice-based research grounded in their own pedagogical experimentation since November 2022 (coinciding with GPT-3.5's public release). This reflexive stance‚Äîpositioning researchers as active participants in iterative learning processes‚Äîlegitimizes preliminary findings while transparently acknowledging knowledge limitations. The methodology appropriately reflects the provisional nature of conclusions given rapid technological development. This approach models scholarly caution necessary when premature claims risk obsolescence, while demonstrating commitment to evidence-informed practice.

## Relevant Concepts

**Generative AI**: Artificial intelligence systems creating new content based on learned patterns; exemplified by large language models like GPT-3.5, capable of text generation, analysis, and dialogue.

**Werkstattbericht (Workshop Report)**: Methodological approach emphasizing exploratory, practice-based inquiry with provisional conclusions rather than definitive findings.

**Dialogical Pedagogy**: Educational approach emphasizing relational interaction, dialogue, and co-construction of knowledge‚Äîparticularly valued in social work education.

**Academic Integrity**: Ethical standards ensuring honest intellectual work; threatened when AI tools obscure authorship and original thinking.

**Actor Constellations**: Changing relationships and roles between educators, learners, and technological systems within educational environments.

**Curricular Transformation**: Fundamental redesign of educational programs accommodating new technologies while preserving disciplinary values and learning objectives.

**Critical Thinking**: Analytical and evaluative capacities essential to social work practice, requiring explicit protection amid AI integration.

## Significance

This work occupies strategic importance in AI-in-education discourse by providing discipline-specific, pragmatic analysis grounded in concrete pedagogical scenarios. The chapter addresses a significant gap in social work education literature while contributing to broader conversations about AI literacy and curricular transformation. For social work educators, it provides conceptual frameworks and practical application scenarios for thoughtful implementation. The authors' commitment to moving beyond polarized debates while preserving dialogical, relational dimensions of social work education establishes a scholarly model for responsible AI integration in professional disciplines. The reflexive methodology and emphasis on transformative rather than merely additive change positions this work as important guidance for educators navigating rapid technological change while maintaining disciplinary integrity.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.pedocs.de/volltexte/2025/33133/pdf/Engelhardt_Ley_2025_Voll_dia_logisch.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/MCLFUR45)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='european_commission._joint_research_centre._2017_digcomp_2.1__the_digital_competence_framework_for_'></a>

## Paper 60/266: DigComp 2.1: the digital competence framework for citizens with eight proficiency levels and examples of use.

**Source file:** `European_Commission._Joint_Research_Centre._2017_DigComp_2.1__the_digital_competence_framework_for_.md`

---
title: "DigComp 2.1: the digital competence framework for citizens with eight proficiency levels and examples of use."
zotero_key: UI47PY9E
author_year: "European Commission. Joint Research Centre. (2017)"
authors: []

# Publication
publication_year: 2017.0
item_type: book
language: en
doi: "nan"
url: "https://data.europa.eu/doi/10.2760/38842"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# DigComp 2.1: the digital competence framework for citizens with eight proficiency levels and examples of use.

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2017.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://data.europa.eu/doi/10.2760/38842
- **Zotero:** [Open in Zotero](zotero://select/items/UI47PY9E)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='european_data_protection_supervisor_2023_explainable_artificial_intelligence'></a>

## Paper 61/266: Explainable Artificial Intelligence

**Source file:** `European_Data_Protection_Supervisor_2023_Explainable_Artificial_Intelligence.md`

---
title: "Explainable Artificial Intelligence"
zotero_key: CE3C2JNF
author_year: "European Data Protection Supervisor (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: report
language: nan
doi: "nan"
url: "https://www.edps.europa.eu/system/files/2023-11/23-11-16_techdispatch_xai_en.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 2
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_European_Data_Protection_Supervisor_2023_Explainab.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Explainable Artificial Intelligence

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Emphasizes the necessity of Explainable AI (XAI) to ensure human-centered, transparent, and ethical AI systems. By increasing transparency and comprehensibility of algorithmic decisions, XAI enables individuals‚Äîincluding those from marginalized groups‚Äîto participate meaningfully in digital decision-making and challenge unjust outcomes.


## AI Summary

## Overview

The EDPS TechDispatch addresses a critical governance challenge in contemporary AI deployment: the "black box" problem. This regulatory document, issued by the European Data Protection Supervisor, examines why artificial intelligence systems' opacity poses unacceptable risks in automated decision-making contexts. The document's central concern is that despite rapid AI adoption across healthcare, finance, transportation, and manufacturing sectors, many AI systems‚Äîincluding large language models like ChatGPT and text-to-image generators like Stable Diffusion‚Äîoperate in ways that remain opaque to providers, deployers, and affected individuals alike. Crucially, the document distinguishes between general technological opacity (where users need not understand underlying mechanisms) and AI opacity in decision-making contexts (where transparency and accountability are legal imperatives). The document argues that explainability is not merely a technical preference but a mandatory legal and ethical requirement, particularly when AI systems influence consequential decisions affecting individuals' rights and opportunities.

## Main Findings

The analysis reveals several critical findings. First, AI opacity is fundamentally distinct from opacity in other technologies because AI systems make autonomous decisions affecting individuals, whereas traditional opaque technologies (like automatic transmissions) do not. Second, the document establishes that current AI systems cannot guarantee explainability even by their creators‚Äîdevelopers themselves often cannot trace how millions of interacting parameters produce specific outputs, a technical constraint inherent to machine learning and deep learning architectures. Third, opacity directly enables two distinct harms: discrimination and misplaced trust/over-reliance. The document demonstrates discrimination through concrete examples, such as hiring algorithms inadvertently discriminating against candidates from certain demographics due to biased training data. Fourth, opacity masks systemic deficiencies including bias, inaccuracies, and "hallucinations" (AI-generated false information presented as factual). Finally, the analysis concludes that transparency and accountability are legal requirements in most jurisdictions under data protection frameworks, making opacity legally unacceptable in automated decision-making contexts, particularly for public authorities.

## Methodology/Approach

The document employs a normative-analytical approach rather than empirical research methodology. It utilizes conceptual analysis to define and contextualize the "black box effect," comparative reasoning through technology analogies to distinguish AI from other opaque systems, and systematic risk-based argumentation to establish why opacity poses unacceptable dangers. The framework identifies three distinct stakeholder groups‚Äîproviders (AI developers), deployers (organizations implementing AI), and affected individuals (those subject to AI decisions)‚Äîestablishing accountability relationships across the AI ecosystem. The approach implicitly references EU data protection regulatory frameworks, grounding arguments in existing legal requirements rather than proposing new ones. The document distinguishes between "automated decision-making" (fully autonomous systems) and "decision support" (human-assisted systems), recognizing different transparency requirements for each context.

## Relevant Concepts

**Black Box Effect:** The phenomenon where AI systems' decision-making processes remain opaque to all stakeholders, including developers, even when systems produce accurate outputs, resulting from complex interactions among millions of parameters.

**Explainability:** The capacity to understand and articulate how AI systems reach specific decisions, tracing the relationship between inputs and outputs‚Äîa legal requirement distinct from mere system functionality.

**Transparency:** The disclosure of AI system logic, training data sources, and decision-making processes to enable meaningful oversight and accountability.

**Automated Decision-Making:** Systems that make consequential decisions affecting individuals with minimal or no human intervention, particularly in public authority contexts, requiring mandatory transparency.

**Algorithmic Bias:** Systematic discrimination embedded in training data that causes AI systems to produce discriminatory outcomes against protected groups, often undetectable without explainability.

**Hallucinations:** AI-generated false information or inaccurate outputs presented with confidence, representing a specific category of opacity-masked deficiency.

**Training Data:** The datasets used to develop AI models, whose quality and representativeness directly determine bias propagation and system reliability.

## Significance

This document holds substantial significance for AI governance and regulatory development. It establishes explainability as a non-negotiable prerequisite for legitimate AI deployment, particularly in public sector contexts where accountability is mandatory. By positioning explainability as a legal requirement rather than optional enhancement, the EDPS contributes to the emerging regulatory consensus reflected in the EU AI Act framework. The document bridges technical constraints (inherent opacity in ML/DL systems) with legal imperatives (accountability requirements), demonstrating that innovation cannot proceed at the expense of fundamental rights protection. Its prescriptive stance prioritizes accountability over innovation flexibility, establishing a regulatory precedent that influences global AI governance discussions, institutional policy development, and organizational compliance frameworks. The document's emphasis on distinguishing AI decision-making from other opaque technologies provides a principled basis for regulatory differentiation and targeted governance approaches.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.edps.europa.eu/system/files/2023-11/23-11-16_techdispatch_xai_en.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/CE3C2JNF)

## Related Concepts

- Black Box Effect
- Explainability
- Transparency
- Automated Decision-Making
- Algorithmic Bias
- Hallucinations
- Training Data

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='feist-ortmanns_2025_ki-basiertes_assistenzsystem_im_kinderschutzverfah'></a>

## Paper 62/266: KI-basiertes Assistenzsystem im Kinderschutzverfahren

**Source file:** `Feist-Ortmanns_2025_KI-basiertes_Assistenzsystem_im_Kinderschutzverfah.md`

---
title: "KI-basiertes Assistenzsystem im Kinderschutzverfahren"
zotero_key: 6W9NNFY2
author_year: "Feist-Ortmanns (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: bookSection
language: nan
doi: "nan"
url: "https://afet-ev.de/assets/themenplattform/KI-in-der-Kinder--und-Jugendhilfe-Naher-Grasshoff-Forum-2.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 3
rel_prof: 3
total_relevance: 11

# Categorization
relevance_category: high
top_dimensions: ["Practical Implementation", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-high", "dim-prof-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Feist-Ortmanns_2025_basiertes.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# KI-basiertes Assistenzsystem im Kinderschutzverfahren

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **11/15** (high) |
| **Top Dimensions** | Practical Implementation, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Praxisnaher Bericht zu einem KI-Assistenzsystem f√ºr Gef√§hrdungseinsch√§tzung. Betont Human-in-the-Loop, Datenschutz, Organisationsentwicklung und Schulungsbedarf; zeigt Potenziale (pr√§ventive Hinweise) und Risiken (Bias, Fehlklassifikationen) im sensiblen Kinderschutzkontext.


## AI Summary

!summary_Feist-Ortmanns_2025_basiertes.md


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://afet-ev.de/assets/themenplattform/KI-in-der-Kinder--und-Jugendhilfe-Naher-Grasshoff-Forum-2.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/6W9NNFY2)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='field_2023_examining_risks_of_racial_biases_in_nlp_tools_for_'></a>

## Paper 63/266: Examining risks of racial biases in NLP tools for child protective services

**Source file:** `Field_2023_Examining_risks_of_racial_biases_in_NLP_tools_for_.md`

---
title: "Examining risks of racial biases in NLP tools for child protective services"
zotero_key: 5UGREN98
author_year: "Field (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: conferencePaper
language: nan
doi: "10.1145/3593013.3594094"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 2
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-prof-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Examining risks of racial biases in NLP tools for child protective services

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

Empirical study examining racial bias in natural language processing tools used to analyze child protective services case notes and make risk assessments. Demonstrates language models trained on case narratives exhibit systematic biases disadvantaging families of color. Testing multiple NLP architectures on real child welfare text data, finds models consistently predict higher risk scores for cases mentioning dialects or cultural contexts associated with Black and Latinx families, even when case severity is identical. Reveals how linguistic biases in administrative data get encoded into AI systems, creating automated discrimination.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1145/3593013.3594094](https://doi.org/10.1145/3593013.3594094)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/5UGREN98)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='fraile-rojas_2025_female_perspectives_on_algorithmic_bias__implicati'></a>

## Paper 64/266: Female perspectives on algorithmic bias: Implications for AI researchers and practitioners

**Source file:** `Fraile-Rojas_2025_Female_perspectives_on_algorithmic_bias__Implicati.md`

---
title: "Female perspectives on algorithmic bias: Implications for AI researchers and practitioners"
zotero_key: HVLX8ERT
author_year: "Fraile-Rojas (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1108/MD-04-2024-0884"
url: "https://colab.ws/articles/10.1108%2Fmd-04-2024-0884"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Female perspectives on algorithmic bias: Implications for AI researchers and practitioners

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This study uses NLP and machine learning to analyze 172,041 tweets from female users discussing gender inequality in AI. It identifies prominent themes including the future of AI technologies and women's active role in ensuring gender-balanced systems. Findings show that algorithmic bias directly affects women's experiences, prompting engagement in online discourse about injustices. Women lead constructive conversations and create entrepreneurial solutions when faced with bias, demonstrating how feminist digital literacies can make AI biases visible and push for their reduction.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1108/MD-04-2024-0884](https://doi.org/10.1108/MD-04-2024-0884)
- **URL:** https://colab.ws/articles/10.1108%2Fmd-04-2024-0884
- **Zotero:** [Open in Zotero](zotero://select/items/HVLX8ERT)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='franken__gender_und_ki-anwendungen._tr√§gt_ki_zum_genderprob'></a>

## Paper 65/266: Gender und KI-Anwendungen. Tr√§gt KI zum Genderproblem oder zu seiner L√∂sung bei?

**Source file:** `Franken__Gender_und_KI-Anwendungen._Tr√§gt_KI_zum_Genderprob.md`

---
title: "Gender und KI-Anwendungen. Tr√§gt KI zum Genderproblem oder zu seiner L√∂sung bei?"
zotero_key: VBCWG8MC
author_year: "Franken ()"
authors: []

# Publication
publication_year: nan
item_type: report
language: nan
doi: "nan"
url: "https://www.fh-bielefeld.de/multimedia/Fachbereiche/Wirtschaft+und+Gesundheit/ Forschung/Denkfabrik+Digitalisierte+Arbeitswelt/geki_Abschlussbericht.pdf"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Gender und KI-Anwendungen. Tr√§gt KI zum Genderproblem oder zu seiner L√∂sung bei?

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | nan |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.fh-bielefeld.de/multimedia/Fachbereiche/Wirtschaft+und+Gesundheit/ Forschung/Denkfabrik+Digitalisierte+Arbeitswelt/geki_Abschlussbericht.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/VBCWG8MC)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='freinhofer_2025_prompten_nach_plan__das_pcrr-framework_als_p√§dagog'></a>

## Paper 66/266: Prompten nach Plan: Das PCRR-Framework als p√§dagogisches Werkzeug f√ºr den Einsatz von K√ºnstlicher Intelligenz.

**Source file:** `Freinhofer_2025_Prompten_nach_Plan__Das_PCRR-Framework_als_p√§dagog.md`

---
title: "Prompten nach Plan: Das PCRR-Framework als p√§dagogisches Werkzeug f√ºr den Einsatz von K√ºnstlicher Intelligenz."
zotero_key: ENXUUHW8
author_year: "Freinhofer (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: de
doi: "10.21243/mi-01-25-26"
url: "https://journals.univie.ac.at/index.php/mp/article/view/9274"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Prompten nach Plan: Das PCRR-Framework als p√§dagogisches Werkzeug f√ºr den Einsatz von K√ºnstlicher Intelligenz.

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Die rasante Entwicklung generativer K√ºnstlicher Intelligenz (KI) macht Prompt Engineering zu einer Schl√ºsselkompetenz f√ºr den kompetenten Umgang mit KI-Modellen. W√§hrend zahlreiche Prompting-Techniken und -Frameworks existieren, fehlt bislang eine systematische Integration in den schulischen Kontext. Diese Publikation stellt das PCRR-Framework (Plan ‚Äì Create ‚Äì Review ‚Äì Reflect) vor, das als ganzheitlicher Ansatz f√ºr den Einsatz von Prompt Engineering im Unterricht dient. Basierend auf Erfahrungen aus dem Hochschullehrgang ‚ÄûK√ºnstliche Intelligenz im IT-Unterricht der Berufsbildung‚Äú (PH Tirol und Hochschule f√ºr Agrar- und Umweltp√§dagogik) wurde das Framework iterativ weiterentwickelt und in drei Praxisbeispielen erprobt. Die Ergebnisse zeigen, dass das PCRR-Framework die Effizienz und Qualit√§t der Prompterstellung steigern kann und die Sch√ºler:innen beim Umgang mit Sprachmodellen (Large Language Models, LLMs) unterst√ºtzt. Gleichzeitig wurden Herausforderungen deutlich, insbesondere hinsichtlich der methodischen Vergleichbarkeit der Ergebnisse sowie der Akzeptanz bestimmter Prompting-Techniken. Das Paper diskutiert diese Erkenntnisse, methodischen Limitationen und Verbesserungspotenziale und bietet einen Ausblick auf zuk√ºnftige Forschungsarbeiten. Neben der Weiterentwicklung des PCRR-Frameworks wird die Notwendigkeit betont, AI Literacy systematisch in Lehrpl√§ne und Lehramtsausbildungen zu integrieren, um eine nachhaltige und verantwortungsbewusste Nutzung von KI im Bildungsbereich zu erm√∂glichen.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.21243/mi-01-25-26](https://doi.org/10.21243/mi-01-25-26)
- **URL:** https://journals.univie.ac.at/index.php/mp/article/view/9274
- **Zotero:** [Open in Zotero](zotero://select/items/ENXUUHW8)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='friedrich-ebert-stiftung_2025_the_eu_artificial_intelligence_act_through_a_gende'></a>

## Paper 67/266: The EU artificial intelligence act through a gender lens

**Source file:** `Friedrich-Ebert-Stiftung_2025_The_EU_artificial_intelligence_act_through_a_gende.md`

---
title: "The EU artificial intelligence act through a gender lens"
zotero_key: LGNJHQQL
author_year: "Friedrich-Ebert-Stiftung (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://library.fes.de/pdf-files/bueros/bruessel/21887-20250304.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Friedrich-Ebert-Stiftung_2025_artificial.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# The EU artificial intelligence act through a gender lens

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Politikanalyse des EU AI Acts mit Fokus auf Geschlechtergerechtigkeit. Identifiziert Potenziale und L√ºcken im Gesetzestext und gibt konkrete Empfehlungen zur Umsetzung.


## AI Summary

!summary_Friedrich-Ebert-Stiftung_2025_artificial.md


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://library.fes.de/pdf-files/bueros/bruessel/21887-20250304.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/LGNJHQQL)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='fujii_2024_bildungsteilhabe_-_flucht_-_digitalisierung__eine_'></a>

## Paper 68/266: Bildungsteilhabe - Flucht - Digitalisierung: Eine multilokale Ethnografie im (digitalen) Alltag junger Gefl√ºchteter

**Source file:** `Fujii_2024_Bildungsteilhabe_-_Flucht_-_Digitalisierung__Eine_.md`

---
title: "Bildungsteilhabe - Flucht - Digitalisierung: Eine multilokale Ethnografie im (digitalen) Alltag junger Gefl√ºchteter"
zotero_key: BBPB9TPN
author_year: "Fujii (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: book
language: nan
doi: "nan"
url: "https://www.beltz.de/fachmedien/sozialpaedagogik_soziale_arbeit/produkte/details/53146"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Bildungsteilhabe - Flucht - Digitalisierung: Eine multilokale Ethnografie im (digitalen) Alltag junger Gefl√ºchteter

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Presents ethnographic research on how digital media shapes educational participation for young refugees, examining ambivalent role of digital technologies: enabling connection to educational resources and transnational networks while simultaneously creating new forms of exclusion through surveillance, documentation requirements, and digital skill barriers. Reveals digital access alone does not guarantee educational participation‚Äîdigital literacy, linguistic competence, and social support remain crucial. Documents how digitalization intersects with migration status, creating specific vulnerabilities related to data collection, surveillance, and documentation requirements.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.beltz.de/fachmedien/sozialpaedagogik_soziale_arbeit/produkte/details/53146
- **Zotero:** [Open in Zotero](zotero://select/items/BBPB9TPN)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='furniturewala_2024_reasoning_towards_fairness__mitigating_bias_in_lan'></a>

## Paper 69/266: Reasoning towards fairness: Mitigating bias in language models through reasoning-guided fine-tuning

**Source file:** `Furniturewala_2024_Reasoning_towards_fairness__Mitigating_bias_in_lan.md`

---
title: "Reasoning towards fairness: Mitigating bias in language models through reasoning-guided fine-tuning"
zotero_key: FDIUT6YU
author_year: "Furniturewala (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://powerdrill.ai/discover/summary-reasoning-towards-fairness-mitigating-bias-in-cm9af0g1h7sb507pn7f5qh32q"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Reasoning towards fairness: Mitigating bias in language models through reasoning-guided fine-tuning

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://powerdrill.ai/discover/summary-reasoning-towards-fairness-mitigating-bias-in-cm9af0g1h7sb507pn7f5qh32q
- **Zotero:** [Open in Zotero](zotero://select/items/FDIUT6YU)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='gaba_2025_bias,_accuracy,_and_trust__gender-diverse_perspect'></a>

## Paper 70/266: Bias, accuracy, and trust: Gender-diverse perspectives on large language models

**Source file:** `Gaba_2025_Bias,_accuracy,_and_trust__Gender-diverse_perspect.md`

---
title: "Bias, accuracy, and trust: Gender-diverse perspectives on large language models"
zotero_key: AGNLCTFB
author_year: "Gaba (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://arxiv.org/abs/2506.21898"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Gaba_2025_Bias.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Bias, accuracy, and trust: Gender-diverse perspectives on large language models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Qualitative study with 25 interviews exploring how users of different gender identities perceive bias and trustworthiness in ChatGPT. Found perceived biases in LLM outputs undermine user trust, with non-binary participants encountering stereotyped responses that eroded confidence. Trust levels varied by group with male participants reporting higher trust. Participants recommended bias mitigation strategies including diversifying training data and implementing clarifying follow-up prompts to check biases.


## AI Summary

## Overview

This empirical study examines how gender-diverse populations perceive bias, accuracy, and trustworthiness in Large Language Models, specifically ChatGPT. Conducted by researchers across multiple institutions and submitted to ACM for publication in the CSCW/HCI field, the work addresses a critical gap in AI development by centering the experiences of non-binary, transgender, male, and female users. The research operates from the premise that LLMs, like other machine learning systems, reflect and perpetuate societal inequalities, particularly regarding gender representation. By investigating differential responses to gendered versus neutral prompts and analyzing user evaluations across gender identities, the study provides empirical evidence that gender identity materially shapes how individuals interact with and trust AI systems. This qualitative investigation contributes to broader conversations about responsible AI development and the necessity of inclusive design practices in technology development.

## Main Findings

The research reveals several significant patterns across three gender identity categories. Gendered prompts‚Äîthose explicitly referencing or implying gender identity‚Äîconsistently elicited identity-specific responses from ChatGPT, with non-binary participants experiencing particularly problematic outputs characterized by condescension and stereotyping. This vulnerability represents a critical equity concern. Perceived accuracy remained relatively consistent across gender groups, though errors concentrated in technical topics and creative tasks‚Äîsuggesting domain-specific rather than gender-specific accuracy issues. Trustworthiness demonstrated substantial gender variation: male participants reported higher overall trust levels, while non-binary participants showed elevated performance-based trust, and female participants occupied intermediate positions. Participants offered actionable recommendations including diversifying training data, ensuring equitable response depth across gender categories, and implementing clarifying questions to reduce ambiguity. These findings suggest that gender bias in LLMs operates through multiple mechanisms affecting user experience and trust formation differently across populations.

## Methodology/Approach

The study employed qualitative methodology through 25 in-depth interviews with participants stratified across three gender identity categories: non-binary/transgender, male, and female. This approach allowed researchers to capture nuanced user experiences and evaluative processes. Participants systematically evaluated LLM responses to both gendered prompts (explicitly referencing gender identity) and neutral prompts (without gender references), enabling direct comparison of how prompt framing influenced outputs. The analysis was grounded in Human-Computer Interaction (HCI) and Computer-Supported Cooperative Work (CSCW) theoretical frameworks, positioning the research within established traditions of user-centered technology evaluation. The gender-critical theoretical lens situated LLM bias within broader patterns of technological discrimination, moving beyond purely technical analyses to incorporate social and contextual dimensions.

## Relevant Concepts

**Algorithmic bias:** Systematic errors in AI systems reflecting and amplifying existing societal inequalities, particularly regarding marginalized groups.

**Gender identity:** Individual's internal sense of gender, encompassing cisgender, transgender, and non-binary identities‚Äîdistinct from biological sex.

**Gendered prompts:** User inputs explicitly referencing or implying gender identity, used to test whether LLMs produce identity-specific responses.

**Trust in AI:** Multi-dimensional construct encompassing reliability, competence, and performance-based confidence in automated systems.

**Inclusive design:** Development approach prioritizing diverse user perspectives and needs from inception rather than as afterthoughts.

**Responsible AI:** Framework emphasizing fairness, transparency, accountability, and stakeholder involvement in AI system development.

## Significance

This research advances algorithmic fairness scholarship by extending beyond demographic representation to examine user perception and trust formation across gender identities. By centering non-binary and transgender experiences‚Äîhistorically marginalized in HCI research‚Äîthe study challenges normative assumptions about gender in technology design and highlights specific vulnerabilities of non-binary users to stereotypical AI outputs. The work bridges HCI/CSCW and AI ethics literatures, providing empirical evidence supporting calls for inclusive development practices. Practically, findings inform LLM improvement strategies and broader AI governance discussions. Theoretically, the study reinforces recognition that algorithmic bias requires integrated technical and social solutions, emphasizing that meaningful progress requires incorporating diverse stakeholder perspectives throughout development cycles rather than treating inclusion as a compliance requirement. The research contributes to growing recognition within the CSCW/HCI field that gender-diverse perspectives are essential for developing trustworthy AI systems.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/abs/2506.21898
- **Zotero:** [Open in Zotero](zotero://select/items/AGNLCTFB)

## Related Concepts

- Algorithmic bias
- Gender identity
- Gendered prompts
- Trust in AI
- Inclusive design
- Responsible AI

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='gallegos_2024_bias_and_fairness_in_large_language_models__a_surv'></a>

## Paper 71/266: Bias and fairness in large language models: A survey

**Source file:** `Gallegos_2024_Bias_and_fairness_in_large_language_models__A_surv.md`

---
title: "Bias and fairness in large language models: A survey"
zotero_key: FIZ9DG6J
author_year: "Gallegos (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1162/coli_a_00524"
url: "https://doi.org/10.1162/coli_a_00524"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Bias and fairness in large language models: A survey

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Comprehensive survey consolidating notions of social bias and fairness in NLP, defining distinct facets of harm and introducing desiderata to operationalize fairness for LLMs. Proposes three taxonomies: metrics for bias evaluation, datasets for bias evaluation, and techniques for bias mitigation classified by intervention timing.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1162/coli_a_00524](https://doi.org/10.1162/coli_a_00524)
- **URL:** https://doi.org/10.1162/coli_a_00524
- **Zotero:** [Open in Zotero](zotero://select/items/FIZ9DG6J)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='garg_2019_counterfactual_fairness_in_text_classification_thr'></a>

## Paper 72/266: Counterfactual fairness in text classification through robustness

**Source file:** `Garg_2019_Counterfactual_fairness_in_text_classification_thr.md`

---
title: "Counterfactual fairness in text classification through robustness"
zotero_key: 9BUXCM4X
author_year: "Garg (2019)"
authors: []

# Publication
publication_year: 2019.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "nan"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Counterfactual fairness in text classification through robustness

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2019.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/9BUXCM4X)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='garkisch_2024_considering_a_unified_model_of_artificial_intellig'></a>

## Paper 73/266: Considering a unified model of artificial intelligence enhanced social work: A systematic review

**Source file:** `Garkisch_2024_Considering_a_unified_model_of_artificial_intellig.md`

---
title: "Considering a unified model of artificial intelligence enhanced social work: A systematic review"
zotero_key: 2C2GPB6P
author_year: "Garkisch (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1007/s41134-024-00326-y"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 1
rel_bias: 1
rel_praxis: 1
rel_prof: 3
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Professional Context", "AI Literacy"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-ai-komp-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Considering a unified model of artificial intelligence enhanced social work: A systematic review

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Professional Context, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Systematic review mapping research landscape of social work AI scholarship, analyzing 67 articles using qualitative analytic approaches to explore how social work researchers investigate AI. Identified themes consistent with Staub-Bernasconi's triple mandate covering profession level, social agencies/organizations, and clients. Emphasizes importance of enhancing computational thinking, AI literacy, and data literacy, and developing skills for evaluating automated systems. Stresses that professionals must be educated and sensitized to data and AI literacy with regular training opportunities.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1007/s41134-024-00326-y](https://doi.org/10.1007/s41134-024-00326-y)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/2C2GPB6P)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='gengler_2024_faires_ki-prompting_‚Äì_ein_leitfaden_f√ºr_unternehme'></a>

## Paper 74/266: Faires KI-Prompting ‚Äì Ein Leitfaden f√ºr Unternehmen

**Source file:** `Gengler_2024_Faires_KI-Prompting_‚Äì_Ein_Leitfaden_f√ºr_Unternehme.md`

---
title: "Faires KI-Prompting ‚Äì Ein Leitfaden f√ºr Unternehmen"
zotero_key: UWGSYB27
author_year: "Gengler (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://www.digitalzentrum-zukunftskultur.de/wp-content/uploads/2024/05/Faires-KI-Prompting-Ein-Leitfaden-fuer-Unternehmen.pdf"

# Assessment
decision: Exclude
exclusion_reason: "Wrong publication type"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Faires KI-Prompting ‚Äì Ein Leitfaden f√ºr Unternehmen

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Wrong publication type


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

This practical guide argues that feminist AI competency results in conscious prompt design to actively reduce stereotypes and discrimination in AI-generated content. It presents the "KI-FAIRNESS" framework for diversity-reflective prompting and demonstrates how specific, context-rich prompts lead to fairer and more representative results by compensating for AI's "blind spots" through targeted instructions.


## AI Summary

## Overview

"Faires KI-Prompting" is a practitioner-oriented guidance document explicitly designed for business leaders and employees in small and medium-sized enterprises (SMEs) implementing generative artificial intelligence responsibly. Published by the Mittelstand-Digital Zentrum Zukunftskultur‚Äîpart of Germany's federally-supported digital transformation network‚Äîthe guide addresses a critical implementation gap: while generative AI systems (capable of autonomously producing text, images, and multimedia) are rapidly integrating into organizational workflows, most SME practitioners lack frameworks for ethical deployment. The document positions itself as a "compass" enabling organizations not merely to navigate AI adoption but to actively shape inclusive digital futures. Deliberately non-technical and accessibility-focused, it targets leaders without AI expertise, emphasizing that fair and diverse AI outcomes are achievable through informed decision-making and deliberate design choices rather than occurring automatically.

## Main Findings

The guide establishes five core findings: (1) **Generative AI is transformative yet risky**‚Äîwhile enabling innovation, efficiency gains, and creative enhancement, these systems carry substantive ethical challenges requiring conscious management; (2) **Fairness and diversity are implementable**‚Äîfair outcomes result from deliberate prompting strategies and design choices, not technological inevitability; (3) **Responsibility is operationalizable**‚Äîbusiness leaders can implement concrete ethical practices without advanced technical expertise; (4) **Understanding precedes effective implementation**‚Äîusers must comprehend how systems function, appropriate applications, and inherent limitations; (5) **AI-generated content requires critical evaluation**‚Äîgenerative systems produce errors and biases, necessitating human oversight before deployment. The document emphasizes understanding both positive and negative impacts of generative AI, recognizing that identical technologies can either reinforce inequalities or advance inclusive outcomes depending on implementation choices.

## Methodology/Approach

The guide employs a **prescriptive-practical methodology** combining expert consultation with institutional authority and normative frameworks. Development involved collaboration between AI trainers (Kristina Bodro≈æiƒá-Brniƒá), specialized consultants (enableYou Consulting GmbH, feminist AI), and domain experts (Eva Gengler, Andreas Kraus, Lisa Krawczyk, Maren Burghard, Dr. Sabine Lang, Sibylle Riehle). Rather than conducting empirical research, authors synthesized existing knowledge within explicit normative commitments to ethics, diversity, and inclusivity. The theoretical foundation integrates **responsible AI** perspectives (emphasizing transparency, fairness, accountability) and **feminist AI** approaches (examining how systems perpetuate or challenge intersectional inequalities). Notably, the methodology prioritizes accessibility and practical applicability over academic rigor, deliberately avoiding technical jargon and framing content as a "journey" through AI concepts to maximize engagement with non-specialist audiences.

## Relevant Concepts

**Generative AI**: Self-operating systems autonomously producing original text, images, and multimedia content without explicit programming for each specific output; distinct from traditional AI systems requiring predetermined responses.

**Fair Prompting**: Strategic formulation of user instructions to generative AI systems designed to produce equitable, unbiased, and inclusive outputs; core operational practice for achieving fairness.

**Responsible AI**: Governance framework ensuring AI systems operate with transparency, fairness, and accountability while minimizing harm and enabling human oversight.

**Feminist AI**: Analytical approach examining how AI systems perpetuate, challenge, or intersect with gender-based and structural inequalities across diverse populations.

**Diversity and Inclusion by Design**: Intentional architectural and operational choices embedding fairness principles into AI applications from inception rather than treating them as post-deployment corrections.

**Prompting**: User-generated instructions or queries directing generative AI system behavior; distinct from traditional programming and requiring new literacy skills.

## Significance

This document addresses urgent practical needs as generative AI adoption accelerates across European SMEs, with particular significance in three dimensions. **First, democratization**: It distributes AI ethics literacy and implementation capacity beyond large corporations with dedicated teams, preventing two-tier systems where only well-resourced enterprises practice ethical AI. **Second, policy alignment**: The work reflects contemporary European governance approaches (particularly German frameworks) emphasizing precautionary principles, stakeholder participation, and distributed responsibility for technological outcomes. **Third, behavioral change**: By emphasizing "why" alongside "how" and "what," the guide recognizes that sustainable implementation requires understanding underlying principles, not merely technical instructions. The explicit focus on diverse and inclusive outcomes‚Äîrather than treating fairness as optional‚Äîsignals institutional commitment to ensuring AI benefits distribute equitably across society. Concrete use cases (recruitment image generation, marketing content creation) ground abstract principles in recognizable organizational contexts, enhancing practical applicability for SME leaders navigating real implementation decisions.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.digitalzentrum-zukunftskultur.de/wp-content/uploads/2024/05/Faires-KI-Prompting-Ein-Leitfaden-fuer-Unternehmen.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/UWGSYB27)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='gengler_2024_faires_kiprompting_‚Äì_ein_leitfaden_f√ºr_unternehmen'></a>

## Paper 75/266: Faires KIPrompting ‚Äì Ein Leitfaden f√ºr Unternehmen. BSP Business and Law School ‚Äì Hochschule f√ºr Management und Recht

**Source file:** `Gengler_2024_Faires_KIPrompting_‚Äì_Ein_Leitfaden_f√ºr_Unternehmen.md`

---
title: "Faires KIPrompting ‚Äì Ein Leitfaden f√ºr Unternehmen. BSP Business and Law School ‚Äì Hochschule f√ºr Management und Recht"
zotero_key: Q7SK8TJB
author_year: "Gengler (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://www.businessschool-berlin.de/files/Business-School-Berlin/Publikation/Faires-KI-Prompting-Ein-Leitfaden-fuer-Unternehmen%202024.pdf"

# Assessment
decision: Exclude
exclusion_reason: "Wrong publication type"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Faires KIPrompting ‚Äì Ein Leitfaden f√ºr Unternehmen. BSP Business and Law School ‚Äì Hochschule f√ºr Management und Recht

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Wrong publication type


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Der vorliegende Leitfaden m√∂chte Sie auf eine Reise durch die Welt der Generativen KI mitnehmen und Ihnen Werkzeuge an die Hand geben, um diese Technolo- gien verantwortungsvoll und bewusst zu nutzen. Wir m√∂chten Verst√§ndnis f√ºr die positive wie negative Wirkung von Generativer KI schaffen, zugleich aber auch den Weg f√ºr einen diversen und fairen Einsatz ebnen. Dieser Guide kann Ihr Kompass sein, um nicht nur zu navigieren, sondern die digitale Zukunft mitzugestalten


## AI Summary

## Overview

"Faires KI-Prompting" is a practitioner-oriented guidance document explicitly designed for business leaders and employees in small and medium-sized enterprises (SMEs) implementing generative artificial intelligence responsibly. Published by the Mittelstand-Digital Zentrum Zukunftskultur‚Äîpart of Germany's federally-supported digital transformation network‚Äîthe guide addresses a critical implementation gap: while generative AI systems (capable of autonomously producing text, images, and multimedia) are rapidly integrating into organizational workflows, most SME practitioners lack frameworks for ethical deployment. The document positions itself as a "compass" enabling organizations not merely to navigate AI adoption but to actively shape inclusive digital futures. Deliberately non-technical and accessibility-focused, it targets leaders without AI expertise, emphasizing that fair and diverse AI outcomes are achievable through informed decision-making and deliberate design choices rather than occurring automatically.

## Main Findings

The guide establishes five core findings: (1) **Generative AI is transformative yet risky**‚Äîwhile enabling innovation, efficiency gains, and creative enhancement, these systems carry substantive ethical challenges requiring conscious management; (2) **Fairness and diversity are implementable**‚Äîfair outcomes result from deliberate prompting strategies and design choices, not technological inevitability; (3) **Responsibility is operationalizable**‚Äîbusiness leaders can implement concrete ethical practices without advanced technical expertise; (4) **Understanding precedes effective implementation**‚Äîusers must comprehend how systems function, appropriate applications, and inherent limitations; (5) **AI-generated content requires critical evaluation**‚Äîgenerative systems produce errors and biases, necessitating human oversight before deployment. The document emphasizes understanding both positive and negative impacts of generative AI, recognizing that identical technologies can either reinforce inequalities or advance inclusive outcomes depending on implementation choices.

## Methodology/Approach

The guide employs a **prescriptive-practical methodology** combining expert consultation with institutional authority and normative frameworks. Development involved collaboration between AI trainers (Kristina Bodro≈æiƒá-Brniƒá), specialized consultants (enableYou Consulting GmbH, feminist AI), and domain experts (Eva Gengler, Andreas Kraus, Lisa Krawczyk, Maren Burghard, Dr. Sabine Lang, Sibylle Riehle). Rather than conducting empirical research, authors synthesized existing knowledge within explicit normative commitments to ethics, diversity, and inclusivity. The theoretical foundation integrates **responsible AI** perspectives (emphasizing transparency, fairness, accountability) and **feminist AI** approaches (examining how systems perpetuate or challenge intersectional inequalities). Notably, the methodology prioritizes accessibility and practical applicability over academic rigor, deliberately avoiding technical jargon and framing content as a "journey" through AI concepts to maximize engagement with non-specialist audiences.

## Relevant Concepts

**Generative AI**: Self-operating systems autonomously producing original text, images, and multimedia content without explicit programming for each specific output; distinct from traditional AI systems requiring predetermined responses.

**Fair Prompting**: Strategic formulation of user instructions to generative AI systems designed to produce equitable, unbiased, and inclusive outputs; core operational practice for achieving fairness.

**Responsible AI**: Governance framework ensuring AI systems operate with transparency, fairness, and accountability while minimizing harm and enabling human oversight.

**Feminist AI**: Analytical approach examining how AI systems perpetuate, challenge, or intersect with gender-based and structural inequalities across diverse populations.

**Diversity and Inclusion by Design**: Intentional architectural and operational choices embedding fairness principles into AI applications from inception rather than treating them as post-deployment corrections.

**Prompting**: User-generated instructions or queries directing generative AI system behavior; distinct from traditional programming and requiring new literacy skills.

## Significance

This document addresses urgent practical needs as generative AI adoption accelerates across European SMEs, with particular significance in three dimensions. **First, democratization**: It distributes AI ethics literacy and implementation capacity beyond large corporations with dedicated teams, preventing two-tier systems where only well-resourced enterprises practice ethical AI. **Second, policy alignment**: The work reflects contemporary European governance approaches (particularly German frameworks) emphasizing precautionary principles, stakeholder participation, and distributed responsibility for technological outcomes. **Third, behavioral change**: By emphasizing "why" alongside "how" and "what," the guide recognizes that sustainable implementation requires understanding underlying principles, not merely technical instructions. The explicit focus on diverse and inclusive outcomes‚Äîrather than treating fairness as optional‚Äîsignals institutional commitment to ensuring AI benefits distribute equitably across society. Concrete use cases (recruitment image generation, marketing content creation) ground abstract principles in recognizable organizational contexts, enhancing practical applicability for SME leaders navigating real implementation decisions.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.businessschool-berlin.de/files/Business-School-Berlin/Publikation/Faires-KI-Prompting-Ein-Leitfaden-fuer-Unternehmen%202024.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/Q7SK8TJB)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='ghosal_2024_an_empirical_study_of_structural_social_and_ethica'></a>

## Paper 76/266: An empirical study of structural social and ethical challenges in AI

**Source file:** `Ghosal_2024_An_empirical_study_of_structural_social_and_ethica.md`

---
title: "An empirical study of structural social and ethical challenges in AI"
zotero_key: CV9PXYWB
author_year: "Ghosal (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://link.springer.com/article/10.1007/s00146-025-02207-y"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 2
rel_prof: 2
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-medium", "dim-prof-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Ghosal_2024_empirical.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# An empirical study of structural social and ethical challenges in AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

This empirical study examines how professionals (N=32) in AI development perceive structural ethical challenges such as injustices and inequalities. The research identifies three main themes: (1) barriers to responsibility in a changing ecosystem, (2) the need for holistic consideration of AI products and their harms, and (3) structural obstacles that prevent engineers from taking personal responsibility.


## AI Summary

!summary_Ghosal_2024_empirical.md


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://link.springer.com/article/10.1007/s00146-025-02207-y
- **Zotero:** [Open in Zotero](zotero://select/items/CV9PXYWB)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='ghosal_2025_unequal_voices__how_llms_construct_constrained_que'></a>

## Paper 77/266: Unequal voices: How LLMs construct constrained queer narratives

**Source file:** `Ghosal_2025_Unequal_voices__How_LLMs_construct_constrained_que.md`

---
title: "Unequal voices: How LLMs construct constrained queer narratives"
zotero_key: RSMUFA95
author_year: "Ghosal (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/abs/2507.15585"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Ghosal_2025_Unequal.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Unequal voices: How LLMs construct constrained queer narratives

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Investigates how large language models represent queer individuals in generated narratives, uncovering tendencies toward stereotyped and narrow portrayals. Identifies phenomena including narrow topic range, discursive othering, and identity foregrounding. Shows LLMs unconsciously reinforce divide where marginalized groups are not afforded same breadth of narrative roles as others.


## AI Summary

## Overview

This paper investigates how Large Language Models systematically construct narrower, more identity-constrained narratives about LGBTQ+ individuals compared to non-queer personas in neutral contexts. The research addresses a critical gap in AI ethics by moving beyond traditional toxicity auditing to examine subtle representational biases embedded in LLM outputs. The authors argue that even when responses appear neutral or positive, they reflect and amplify real-world media disparities that marginalize LGBTQ+ people by reducing their complexity to identity-focused characteristics. The motivation is urgent: LLMs increasingly function in creative, educational, and therapeutic roles (including therapist chatbots), making their representational patterns consequential for cultural narrative formation. The paper demonstrates that problematic outputs operate through subtle prevalence patterns rather than overt toxicity, rendering them invisible to conventional bias auditing methods.

## Main Findings

The study demonstrates statistically significant differences in LLM portrayals of queer versus non-queer personas. When assuming LGBTQ+ identities, models disproportionately center responses on sexual orientation or gender identity, even when contextually irrelevant. For example, queried about employment, a queer male persona emphasizes identity-related job aspects, while a non-queer male persona focuses on career aspirations and community contributions. This reveals that LLMs default to identity-focused narratives for marginalized groups while affording non-marginalized groups full human complexity. The authors identify three distinct textual phenomena operating simultaneously: (1) **explicitly harmful representations** (overtly negative content), (2) **overly narrow representations** (identity-constrained scope), and (3) **discursive othering** (systematic marking of difference). Crucially, findings show that problematic outputs operate through cumulative prevalence patterns‚Äîthe sheer frequency of identity-focus‚Äîrather than individual toxic instances, making detection difficult through standard bias audits.

## Methodology/Approach

The research employs comparative persona-based prompting experiments using Llama-3.1-8B-Instruct. Researchers instruct the model to assume specific identities (queer and non-queer pairs) and respond to identical neutral queries without additional contextual cues. This methodology operationalizes "constrained narratives" through systematic textual comparison. The theoretical framework integrates media studies (examining how narratives shape cultural consciousness and societal perception of marginalized groups) with disability justice scholarship (particularly the "inspiration porn" concept). Replication code is forthcoming, supporting methodological transparency and reproducibility.

## Relevant Concepts

**Constrained Narratives**: Narrow, stereotyped topic ranges assigned to marginalized groups, contrasting with full complexity afforded to default groups.

**Explicitly Harmful Representations**: Overtly negative or toxic content about marginalized individuals.

**Overly Narrow Representations**: Identity-constrained portrayals that reduce individuals to single characteristics.

**Discursive Othering**: Textual practices systematically marking marginalized individuals as fundamentally different through representational choices.

**Inspiration Porn**: Problematic portrayals reducing marginalized people (especially disabled individuals) to motivational examples, denying full humanity.

**Representational Bias**: Subtle associative patterns in AI outputs mirroring real-world media biases without explicit toxicity.

## Significance

This work advances AI ethics by establishing analytical frameworks for detecting structural representational constraints beyond explicit bias. It demonstrates how neutral prompts reveal embedded biases and connects LLM outputs to real-world narrative marginalization patterns. The research has immediate implications for applications where LLMs function as creative or therapeutic agents, suggesting current bias auditing practices inadequately address how AI systems reproduce social inequalities through representation. By bridging computational linguistics with critical media studies and disability justice frameworks, the paper opens new methodological pathways for understanding AI's role in cultural narrative formation and provides evidence-based grounds for demanding more equitable LLM development practices. The distinction between explicit bias and structural representational constraints represents a significant conceptual advance in the field.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/abs/2507.15585
- **Zotero:** [Open in Zotero](zotero://select/items/RSMUFA95)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='giannoni_adielsson_2024_the_ai_act,_gender_equality_and_non-discrimination'></a>

## Paper 78/266: The AI Act, gender equality and non-discrimination: what role for the AI Office?

**Source file:** `Giannoni_Adielsson_2024_The_AI_Act,_gender_equality_and_non-discrimination.md`

---
title: "The AI Act, gender equality and non-discrimination: what role for the AI Office?"
zotero_key: ZSBFXEKJ
author_year: "Giannoni Adielsson (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1007/s12027-024-00785-w"
url: "https://doi.org/10.1007/s12027-024-00785-w"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# The AI Act, gender equality and non-discrimination: what role for the AI Office?

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Diese Analyse bewertet, ob der EU AI Act Fragen der Geschlechtergerechtigkeit und Nichtdiskriminierung ausreichend adressiert. Die substantiellen Bestimmungen des AI Acts werden durch die Linse von Gleichstellungs- und Antidiskriminierungsrecht analysiert, wobei vorgeschlagene Tools wie grundrechtliche Folgenabsch√§tzungen und Bias-Audits zur Reduzierung von Geschlechterverzerrungen und Diskriminierungsrisiken hervorgehoben werden. Die Rolle des AI Office und seine Kooperation mit nationalen, europ√§ischen und internationalen Stellen zur Durchsetzung der Geschlechtergerechtigkeit wird diskutiert.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1007/s12027-024-00785-w](https://doi.org/10.1007/s12027-024-00785-w)
- **URL:** https://doi.org/10.1007/s12027-024-00785-w
- **Zotero:** [Open in Zotero](zotero://select/items/ZSBFXEKJ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='goellner_2025_towards_responsible_ai_for_education__hybrid_human'></a>

## Paper 79/266: Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room

**Source file:** `Goellner_2025_Towards_responsible_AI_for_education__Hybrid_human.md`

---
title: "Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room"
zotero_key: 7L8SGYKI
author_year: "Goellner (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/pdf/2504.16148"

# Assessment
decision: Unclear
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 1
rel_bias: 1
rel_praxis: 2
rel_prof: 1
total_relevance: 7

# Categorization
relevance_category: medium
top_dimensions: ["AI Literacy", "Practical Implementation"]

# Tags
tags: ["paper", "unclear", "medium-relevance", "dim-ai-komp-medium", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Unclear** |
| **Total Relevance** | **7/15** (medium) |
| **Top Dimensions** | AI Literacy, Practical Implementation |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Identifies nine persistent challenges undermining responsible use of AI in education, including neglect of key learning processes, lack of stakeholder involvement, and use of unreliable XAI methods. Proposes hybrid human-AI methods, specifically neural-symbolic AI (NSAI), which integrates expert domain knowledge with data-driven approaches. This hybrid architecture allows for built-in transparency, stakeholder engagement, and modeling of complex pedagogically-grounded principles.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/pdf/2504.16148
- **Zotero:** [Open in Zotero](zotero://select/items/7L8SGYKI)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='gohar_2023_a_survey_on_intersectional_fairness_in_machine_lea'></a>

## Paper 80/266: A Survey on Intersectional Fairness in Machine Learning: Opportunities and Challenges

**Source file:** `Gohar_2023_A_Survey_on_Intersectional_Fairness_in_Machine_Lea.md`

---
title: "A Survey on Intersectional Fairness in Machine Learning: Opportunities and Challenges"
zotero_key: 8IR8NQTC
author_year: "Gohar (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: conferencePaper
language: nan
doi: "10.24963/ijcai.2023/742"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Gohar_2023_Survey.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# A Survey on Intersectional Fairness in Machine Learning: Opportunities and Challenges

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Comprehensive survey examining intersectional fairness in machine learning systems beyond traditional binary fairness approaches. Presents taxonomy of intersectional fairness concepts showing how multiple sensitive attributes interact to create unique discrimination forms. Identifies challenges including data sparsity for intersectional groups and bias mitigation complexity. Demonstrates applications in NLP systems, ranking algorithms, and image recognition. Shows traditional fairness metrics fail for intersectional identities as Black women experience different discrimination than Black people or women separately.


## AI Summary

!summary_Gohar_2023_Survey.md


## Links & Resources

- **DOI:** [10.24963/ijcai.2023/742](https://doi.org/10.24963/ijcai.2023/742)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/8IR8NQTC)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='goldkind_2023_the_end_of_the_world_as_we_know_it__chatgpt_and_so'></a>

## Paper 81/266: The End of the World as We Know It? ChatGPT and Social Work

**Source file:** `Goldkind_2023_The_End_of_the_World_as_We_Know_It__ChatGPT_and_So.md`

---
title: "The End of the World as We Know It? ChatGPT and Social Work"
zotero_key: GBF7IZM4
author_year: "Goldkind (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "10.1093/sw/swad044"
url: "https://doi.org/10.1093/sw/swad044"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 1
rel_bias: 1
rel_praxis: 1
rel_prof: 3
total_relevance: 7

# Categorization
relevance_category: medium
top_dimensions: ["Professional Context", "AI Literacy"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# The End of the World as We Know It? ChatGPT and Social Work

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **7/15** (medium) |
| **Top Dimensions** | Professional Context, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Brief commentary marking ChatGPT as a pivotal moment for social work. Encourages proactive engagement to steer AI toward just, human-centered outcomes and warns that non-engagement risks value misalignment and inequity.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1093/sw/swad044](https://doi.org/10.1093/sw/swad044)
- **URL:** https://doi.org/10.1093/sw/swad044
- **Zotero:** [Open in Zotero](zotero://select/items/GBF7IZM4)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='goldkind_2024_artificial_intelligence_in_social_work__an_epic_mo'></a>

## Paper 82/266: Artificial intelligence in social work: An EPIC model for practice

**Source file:** `Goldkind_2024_Artificial_intelligence_in_social_work__An_EPIC_mo.md`

---
title: "Artificial intelligence in social work: An EPIC model for practice"
zotero_key: 322LCA6H
author_year: "Goldkind (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1080/0312407X.2025.2488345"
url: "https://doi.org/10.1080/0312407X.2025.2488345"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 2
rel_prof: 3
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Professional Context", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Artificial intelligence in social work: An EPIC model for practice

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Professional Context, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Develops EPIC model (Ethics and justice, Policy development and advocacy, Intersectoral collaboration, Community engagement and empowerment) as structured approach for AI integration in social work. Emphasizes that AI integration must align with social work mission and values through four overlapping components. Ethics and justice aspect addresses bias mitigation and transparent use, while policy development prioritizes client protection. Warns against excessive calibration of AI systems toward trustworthiness that could impair utility, emphasizing importance of transparency in both models and underlying technologies.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1080/0312407X.2025.2488345](https://doi.org/10.1080/0312407X.2025.2488345)
- **URL:** https://doi.org/10.1080/0312407X.2025.2488345
- **Zotero:** [Open in Zotero](zotero://select/items/322LCA6H)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='goldkind_2024_the_end_of_the_world_as_we_know_it__chatgpt_and_so'></a>

## Paper 83/266: The end of the world as we know it? ChatGPT and social work

**Source file:** `Goldkind_2024_The_end_of_the_world_as_we_know_it__ChatGPT_and_so.md`

---
title: "The end of the world as we know it? ChatGPT and social work"
zotero_key: 7QB9DC7Z
author_year: "Goldkind (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1093/sw/swad044"
url: "nan"

# Assessment
decision: Exclude
exclusion_reason: "Wrong publication type"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# The end of the world as we know it? ChatGPT and social work

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Wrong publication type


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Editorial in flagship journal Social Work providing critical reflection on ChatGPT's introduction and implications for social work practice. Addresses how ChatGPT, built on natural language processing, responds to prompts and generates text responses. Notes social work's historical reluctance to embrace new technologies and positions ChatGPT as opportunity to reflect on strategies promoting just technology use. Urges social workers to join cross-disciplinary conversations about AI evolution and advocate for fair use. Calls for profession to move beyond fear and awe toward critical, reflective engagement with AI systems through thoughtful prompting practices.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1093/sw/swad044](https://doi.org/10.1093/sw/swad044)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/7QB9DC7Z)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='gravelmann_2024_k√ºnstliche_intelligenz_in_der_sozialen_arbeit_‚Äì_zw'></a>

## Paper 84/266: K√ºnstliche Intelligenz in der Sozialen Arbeit ‚Äì Zwischen Bedenken und Optionen

**Source file:** `Gravelmann_2024_K√ºnstliche_Intelligenz_in_der_Sozialen_Arbeit_‚Äì_Zw.md`

---
title: "K√ºnstliche Intelligenz in der Sozialen Arbeit ‚Äì Zwischen Bedenken und Optionen"
zotero_key: J8S8EDJ6
author_year: "Gravelmann (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://content-select.com/de/portal/media/view/66472512-3a7c-4fef-b8e6-a5d3ac1b0002"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 3
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Bias Analysis", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# K√ºnstliche Intelligenz in der Sozialen Arbeit ‚Äì Zwischen Bedenken und Optionen

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Bias Analysis, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Gravelmann analyzes the impact of AI on the social work profession, identifying both opportunities and risks. Potential benefits include AI as communication aid, documentation tool, and data analysis instrument. Critical concerns include the danger of decision delegation to AI systems, potentially reducing professionals to executors. The author warns against automated AI-based procedures that massively intervene in people's lives, especially in child protection. Ethical problems are identified in the lack of objectivity in AI data, which reflects societal power relations and reproduces discriminatory value systems.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://content-select.com/de/portal/media/view/66472512-3a7c-4fef-b8e6-a5d3ac1b0002
- **Zotero:** [Open in Zotero](zotero://select/items/J8S8EDJ6)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='guerra_2023_feminist_reflections_for_the_development_of_artifi'></a>

## Paper 85/266: Feminist reflections for the development of artificial intelligence

**Source file:** `Guerra_2023_Feminist_reflections_for_the_development_of_artifi.md`

---
title: "Feminist reflections for the development of artificial intelligence"
zotero_key: U7QYR2AA
author_year: "Guerra (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: report
language: nan
doi: "nan"
url: "https://www.derechosdigitales.org/fair-2023-en/"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-medium", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Feminist reflections for the development of artificial intelligence

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Comprehensive synthesis of Latin American women's conversations developing AI under feminist frameworks establishes methodological commitments for co-design with communities, gender perspective integration in data science projects, and strategies for women crowd workers. Key feminist AI principles include building diverse intersectional teams, establishing community collaboration agreements, choosing technology based on context rather than consumption, and protecting autonomy through strong anonymity criteria.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.derechosdigitales.org/fair-2023-en/
- **Zotero:** [Open in Zotero](zotero://select/items/U7QYR2AA)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='hall_2024_a_systematic_review_of_sophisticated_predictive_an'></a>

## Paper 86/266: A systematic review of sophisticated predictive and prescriptive analytics in child welfare: Accuracy, equity, and bias

**Source file:** `Hall_2024_A_systematic_review_of_sophisticated_predictive_an.md`

---
title: "A systematic review of sophisticated predictive and prescriptive analytics in child welfare: Accuracy, equity, and bias"
zotero_key: 36MC5GZ8
author_year: "Hall (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1007/s10560-023-00931-2"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 3
total_relevance: 12

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium", "dim-prof-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Hall_2024_systematic.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# A systematic review of sophisticated predictive and prescriptive analytics in child welfare: Accuracy, equity, and bias

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **12/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Systematic review examining how researchers address ethics, equity, bias, and model performance in predictive and prescriptive algorithms used in child welfare settings. Analyzing 67 articles published 2010-2020, reveals inconsistent approaches to measuring and mitigating algorithmic fairness in child welfare applications. Identifies that most predictive models use administrative data reflecting surveillance biases rather than true child maltreatment incidence, leading to discrimination against low-income families and communities of color. Highlights many tools fail to address how automation bias and dehumanization affect social workers' professional judgment.


## AI Summary

!summary_Hall_2024_systematic.md


## Links & Resources

- **DOI:** [10.1007/s10560-023-00931-2](https://doi.org/10.1007/s10560-023-00931-2)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/36MC5GZ8)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='hartshorne_2025_generative_ai_and_the_future_of_digital_literacy__'></a>

## Paper 87/266: Generative AI and the Future of Digital Literacy: Opportunities for Gender Inclusion

**Source file:** `Hartshorne_2025_Generative_AI_and_the_Future_of_Digital_Literacy__.md`

---
title: "Generative AI and the Future of Digital Literacy: Opportunities for Gender Inclusion"
zotero_key: GFNQY7GF
author_year: "Hartshorne (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "AI Literacy"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-medium", "dim-vulnerable-high", "dim-bias-medium", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Generative AI and the Future of Digital Literacy: Opportunities for Gender Inclusion

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Examines generative AI potential for promoting female inclusion in primary and secondary education while addressing inherent risks of reinforcing gender-specific biases. Qualitative research analyzes systemic effects of generative AI tools on teaching and learning through interviews and focus groups with students and teachers. Results show complex interactions between generative AI technologies and gender dynamics. Key factors for effective AI-supported learning environments include personalized learning experiences, bias-aware content generation, and teachers' role as mediators of AI interactions.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/GFNQY7GF)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='hauck_2025_a_framework_for_the_learning_and_teaching_of_criti'></a>

## Paper 88/266: A framework for the learning and teaching of Critical AI Literacy skills (Version 0.1)

**Source file:** `Hauck_2025_A_framework_for_the_learning_and_teaching_of_Criti.md`

---
title: "A framework for the learning and teaching of Critical AI Literacy skills (Version 0.1)"
zotero_key: B4S75S5S
author_year: "Hauck (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://www.open.ac.uk/blogs/learning-design/wp-content/uploads/2025/01/OU-Critical-AI-Literacy-framework-2025-external-sharing.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 3
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["AI Literacy", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-high", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# A framework for the learning and teaching of Critical AI Literacy skills (Version 0.1)

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | AI Literacy, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This framework defines Critical AI Literacy as expanding beyond traditional AI literacy to examine how Large Language Models contribute to ongoing epistemic injustices that can lead to significant social and personal harm. It applies equality, diversity, inclusion, and accessibility principles to AI use, emphasizing the importance of critically evaluating AI-generated outputs and engaging in equitable and inclusive prompting practices. Critical AI Literacy is conceptualized as context-specific and treats literacy as a social practice rather than a possession. At advanced levels, it examines AI's potential to shift power relationships and explores how AI contributes to inequalities while considering ways it could help redress power balances.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.open.ac.uk/blogs/learning-design/wp-content/uploads/2025/01/OU-Critical-AI-Literacy-framework-2025-external-sharing.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/B4S75S5S)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='hayati_2024_how_far_can_we_extract_diverse_perspectives_from_l'></a>

## Paper 89/266: How Far Can We Extract Diverse Perspectives from Large Language Models?

**Source file:** `Hayati_2024_How_Far_Can_We_Extract_Diverse_Perspectives_from_L.md`

---
title: "How Far Can We Extract Diverse Perspectives from Large Language Models?"
zotero_key: ZHLPQEII
author_year: "Hayati (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "https://aclanthology.org/2024.emnlp-main.306.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Hayati_2024_Extract.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# How Far Can We Extract Diverse Perspectives from Large Language Models?

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Systematically evaluates prompting strategies to extract diverse perspectives from LLMs and mitigate dominant group bias in outputs. Measuring subjective tasks such as argumentation and hate speech labeling, the study finds that diversity prompting increases perspective variety and reduces monocultural output tendencies.


## AI Summary

!summary_Hayati_2024_Extract.md


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://aclanthology.org/2024.emnlp-main.306.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/ZHLPQEII)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='he_2024_on_the_steerability_of_large_language_models'></a>

## Paper 90/266: On the steerability of large language models

**Source file:** `He_2024_On_the_steerability_of_large_language_models.md`

---
title: "On the steerability of large language models"
zotero_key: WUC94FIH
author_year: "He (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "https://aclanthology.org/2025.naacl-long.400.pdf"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# On the steerability of large language models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://aclanthology.org/2025.naacl-long.400.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/WUC94FIH)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='heinz_2025_clinical_trial_of_an_llm-based_conversational_ai_p'></a>

## Paper 91/266: Clinical trial of an LLM-based conversational AI psychotherapy

**Source file:** `Heinz_2025_Clinical_trial_of_an_LLM-based_conversational_AI_p.md`

---
title: "Clinical trial of an LLM-based conversational AI psychotherapy"
zotero_key: 7RQUVQ5I
author_year: "Heinz (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://ai.nejm.org/doi/full/10.1056/AIoa2400802"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 1
rel_praxis: 3
rel_prof: 2
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Practical Implementation", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-praxis-high", "dim-prof-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Clinical trial of an LLM-based conversational AI psychotherapy

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Practical Implementation, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

Groundbreaking study representing first randomized controlled trial of generative AI-powered therapy chatbot, Therabot. Trial included 106 participants across United States diagnosed with major depressive disorder, generalized anxiety disorder, or eating disorders. Participants interacted with Therabot via smartphone app over 4-8 weeks. Results showed clinically significant symptom improvements: 51% reduction in depression symptoms, 31% reduction in anxiety symptoms, and 19% reduction in eating disorder concerns, with outcomes comparable to traditional outpatient therapy. Participants reported therapeutic alliance levels comparable to human therapists and engaged average of 6 hours (equivalent to 8 therapy sessions).


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://ai.nejm.org/doi/full/10.1056/AIoa2400802
- **Zotero:** [Open in Zotero](zotero://select/items/7RQUVQ5I)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='himmelreich_2022_artificial_intelligence_and_structural_injustice__'></a>

## Paper 92/266: Artificial Intelligence and Structural Injustice: Foundations for Equity, Values, and Responsibility

**Source file:** `Himmelreich_2022_Artificial_Intelligence_and_Structural_Injustice__.md`

---
title: "Artificial Intelligence and Structural Injustice: Foundations for Equity, Values, and Responsibility"
zotero_key: RCD7NJM2
author_year: "Himmelreich (2022)"
authors: []

# Publication
publication_year: 2022.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://arxiv.org/abs/2205.02389"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Himmelreich_2022_Artificial.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Artificial Intelligence and Structural Injustice: Foundations for Equity, Values, and Responsibility

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2022.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This work develops a structural injustice approach to AI governance based on Iris Marion Young's theory of structural injustice. The authors argue that structural injustice is a powerful conceptual tool that enables researchers and practitioners to identify, articulate, and potentially even anticipate AI bias. The approach includes both an analytical component (structural explanations) and an evaluative component (justice theory) and provides methodological and normative foundations for diversity, equity, and inclusion values.


## AI Summary

## Overview

This academic work by Himmelreich and Lim presents a philosophical framework for understanding and addressing AI bias through structural injustice theory, derived from philosopher Iris Marion Young's work. The authors argue that AI systems perpetuate pre-existing unjust social structures rather than creating bias independently. Emerging from contemporary global awareness of systemic racism, colonialism, and inequality across the Global North (US, Europe), the work positions AI governance as inseparable from broader justice concerns. The central claim is that structural injustice theory provides superior analytical and normative foundations for AI ethics compared to conventional harm-benefit analyses or abstract value statements.

## Main Findings

The authors establish several critical findings. First, structural injustice operates through both analytical components (explaining systemic mechanisms) and evaluative components (normative justice theory), providing dual explanatory and moral frameworks. Second, AI biases emerge from interaction between well-intentioned systems and fundamentally unjust social contexts, not individual malice. Third, structural injustice functions as a diagnostic tool for identifying, articulating, and anticipating AI biases before deployment. Fourth, the framework provides rigorous philosophical grounding for Diversity, Equity, and Inclusion initiatives, moving beyond rhetorical commitments. Fifth, responsibility for addressing structural injustice is distributed across individuals and organizations, independent of causal responsibility for injustice's existence. Finally, an open theoretical question remains: whether AI itself is becoming constitutive of society's structural fabric, potentially amplifying injustice systematically.

## Methodology/Approach

The authors employ comparative interdisciplinary philosophical analysis, explicitly contrasting the structural injustice approach against alternative governance frameworks (harm-benefit analyses, value-based approaches). The methodology integrates social science structural explanations with political philosophy and applied ethics, grounding abstract theory in concrete case studies of racial bias in AI systems. The approach draws from sociology, philosophy, and policy studies, examining how institutional arrangements and systemic processes illuminate AI's role in perpetuating injustice while simultaneously applying justice theory to evaluate these structures' legitimacy.

## Relevant Concepts

**Structural Injustice:** Injustice embedded in institutional practices and social structures through both analytical mechanisms (how systems operate) and evaluative dimensions (normative justice standards); operates through systemic patterns affecting particular groups.

**Iris Marion Young's Theory:** Foundational philosophical framework distinguishing structural from individual injustice, emphasizing how institutions perpetuate inequality through normal operations.

**Dual-Component Framework:** Structural injustice analysis combining analytical explanation (social science mechanisms) with evaluative judgment (justice theory).

**Algorithmic Bias:** Systematic errors in AI systems disproportionately harming particular groups, understood as emerging from structural rather than purely technical sources.

**Distributed Responsibility:** Moral obligation to address injustice shared across individuals and organizations, distinct from causal responsibility for creating injustice.

**Status Quo Injustice:** Recognition that existing arrangements embed historical inequalities (racism, colonialism, gender/class disparities) that AI risks perpetuating.

## Significance

This work represents a paradigm shift in AI ethics discourse, moving from individualistic to systemic responsibility frameworks. It challenges narrow technical approaches by demonstrating that algorithmic "objectivity" cannot overcome structural injustice. The significance extends globally: it provides policy-makers, practitioners, and researchers with conceptual tools for recognizing how AI governance connects to broader social justice movements addressing systemic racism and colonialism. By grounding DEI initiatives in rigorous philosophical theory, the framework offers methodological rigor to equity-focused governance. The work ultimately argues that responsible AI deployment requires understanding and addressing the unjust structures into which AI is embedded, while acknowledging that the extent of AI's constitutive role in society's structure remains an open theoretical question requiring further investigation.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/abs/2205.02389
- **Zotero:** [Open in Zotero](zotero://select/items/RCD7NJM2)

## Related Concepts

- Structural Injustice
- Iris Marion Young's Theory
- Dual-Component Framework
- Algorithmic Bias
- Distributed Responsibility
- Status Quo Injustice

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='hodgson_2022_problematising_artificial_intelligence_in_social_w'></a>

## Paper 93/266: Problematising artificial intelligence in social work education: Challenges, issues and possibilities

**Source file:** `Hodgson_2022_Problematising_artificial_intelligence_in_social_w.md`

---
title: "Problematising artificial intelligence in social work education: Challenges, issues and possibilities"
zotero_key: M7UIILSI
author_year: "Hodgson (2022)"
authors: []

# Publication
publication_year: 2022.0
item_type: journalArticle
language: nan
doi: "10.1093/bjsw/bcab168"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 1
rel_prof: 3
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Professional Context", "AI Literacy"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-medium", "dim-vulnerable-medium", "dim-bias-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Problematising artificial intelligence in social work education: Challenges, issues and possibilities

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2022.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Professional Context, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Critical examination of AI's fourth industrial revolution implications for social work education, questioning what skills and knowledge should be taught to prepare students for digital working lives. Adopts problematizing approach challenging both celebratory and catastrophic narratives about AI. Argues social work education must address fundamental tensions between training for technologically-driven environments and engaging students in critical theories acknowledging unequal power distributions that technological change reinforces. Advocates for developing critical digital literacies beyond technical competence.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1093/bjsw/bcab168](https://doi.org/10.1093/bjsw/bcab168)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/M7UIILSI)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='james_2023_algorithmic_decision-making_in_social_work_practic'></a>

## Paper 94/266: Algorithmic decision-making in social work practice and pedagogy: confronting the competency/critique dilemma

**Source file:** `James_2023_Algorithmic_decision-making_in_social_work_practic.md`

---
title: "Algorithmic decision-making in social work practice and pedagogy: confronting the competency/critique dilemma"
zotero_key: XPY6WUH4
author_year: "James (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "10.1080/02615479.2023.2195425"
url: "https://www.tandfonline.com/doi/full/10.1080/02615479.2023.2195425"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 2
rel_prof: 3
total_relevance: 11

# Categorization
relevance_category: high
top_dimensions: ["Professional Context", "AI Literacy"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-medium", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Algorithmic decision-making in social work practice and pedagogy: confronting the competency/critique dilemma

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **11/15** (high) |
| **Top Dimensions** | Professional Context, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

The world is experiencing an accelerating digital transformation. One aspect of this is the implementation of...[source](https://www.google.com/search?q=https://www.aminer.cn/pub/645d0410d68f896efa94d024/algorithmic-decision-making-in-social-work-practice-and-pedagogy-confronting-the-competency)


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1080/02615479.2023.2195425](https://doi.org/10.1080/02615479.2023.2195425)
- **URL:** https://www.tandfonline.com/doi/full/10.1080/02615479.2023.2195425
- **Zotero:** [Open in Zotero](zotero://select/items/XPY6WUH4)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='james_2025_responsible_prompting_recommendation__fostering_re'></a>

## Paper 95/266: Responsible prompting recommendation: Fostering responsible AI practices in prompting-time

**Source file:** `James_2025_Responsible_prompting_recommendation__Fostering_re.md`

---
title: "Responsible prompting recommendation: Fostering responsible AI practices in prompting-time"
zotero_key: 5ED8INP4
author_year: "James (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: conferencePaper
language: nan
doi: "10.1145/3706598.3713365"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 1
rel_bias: 2
rel_praxis: 3
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Practical Implementation", "AI Literacy"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-ai-komp-medium", "dim-bias-medium", "dim-praxis-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Responsible prompting recommendation: Fostering responsible AI practices in prompting-time

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Practical Implementation, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Presents insights from interviews and user studies with IT professionals exploring prompting practices and develops open-source responsible prompting recommender system. Research reveals responsible prompt recommendations can support novice prompt engineers and raise awareness about Responsible AI in prompting-time, helping people reflect on responsible practices before LLM content generation. Demonstrates that finding right balance between adding social values to prompts and removing potentially harmful content is critical for recommendation systems. Framework highly relevant for social services as it addresses how to design systems encouraging reflective, values-based prompting practices.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1145/3706598.3713365](https://doi.org/10.1145/3706598.3713365)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/5ED8INP4)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='jarke_2024_who_cares_about_data__data_care_arrangements_in_ev'></a>

## Paper 96/266: Who cares about data? Data care arrangements in everyday organisational practice

**Source file:** `Jarke_2024_Who_cares_about_data__Data_care_arrangements_in_ev.md`

---
title: "Who cares about data? Data care arrangements in everyday organisational practice"
zotero_key: PQ78ECFH
author_year: "Jarke (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1080/1369118X.2024.2320917"
url: "nan"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Who cares about data? Data care arrangements in everyday organisational practice

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Introduces data care arrangements concept to understand mundane data work in organizations. Demonstrates through empirical research in educational and social service organizations how data care work is distributed across organizational members with different, often conflicting care obligations. Reveals data quality maintenance involves complex sociomaterial configurations of people, infrastructures, routines, and practices. Shows data care work is frequently backgrounded and assumed effortless despite being essential for datafied organizations.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1080/1369118X.2024.2320917](https://doi.org/10.1080/1369118X.2024.2320917)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/PQ78ECFH)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='jarke_2025_datafied_ageing_futures__regimes_of_anticipation_a'></a>

## Paper 97/266: Datafied ageing futures: Regimes of anticipation and participatory futuring

**Source file:** `Jarke_2025_Datafied_ageing_futures__Regimes_of_anticipation_a.md`

---
title: "Datafied ageing futures: Regimes of anticipation and participatory futuring"
zotero_key: ZKHP6UJ5
author_year: "Jarke (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1177/20539517241306363"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 1
rel_praxis: 2
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Practical Implementation"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Datafied ageing futures: Regimes of anticipation and participatory futuring

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Practical Implementation |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Challenges regimes of anticipation suggesting datafied futures are inevitable, arguing futures are actively made through sociotechnical imaginaries promoted by powerful actors. Explores how to democratize futures-making regarding aging populations, critiquing how current anticipations around data-driven systems and ageist assumptions dominate discussions without adequate participation from affected populations. Develops participatory futuring methodology enabling diverse stakeholders, particularly older adults, to imagine and design alternative futures beyond dominant techno-solutionist narratives.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1177/20539517241306363](https://doi.org/10.1177/20539517241306363)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/ZKHP6UJ5)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='jarrahi_2021_algorithmic_management_in_a_work_context'></a>

## Paper 98/266: Algorithmic management in a work context

**Source file:** `Jarrahi_2021_Algorithmic_management_in_a_work_context.md`

---
title: "Algorithmic management in a work context"
zotero_key: 9XLDABF4
author_year: "Jarrahi (2021)"
authors: []

# Publication
publication_year: 2021.0
item_type: journalArticle
language: nan
doi: "10.1177/20539517211020332"
url: "nan"

# Assessment
decision: Unclear
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 1
rel_prof: 1
total_relevance: 6

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "unclear", "medium-relevance", "dim-vulnerable-medium", "dim-bias-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Algorithmic management in a work context

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2021.0 |
| **Decision** | **Unclear** |
| **Total Relevance** | **6/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Interdisciplinary analysis examining algorithmic management as sociotechnical phenomenon shaped by organizational choices and power structures rather than technological determinism. Authors critically analyze how algorithmic systems in standard work settings redefine pre-existing power dynamics between workers and managers, demanding new competencies while fostering oppositional attitudes. Key critical insights include risk of treating workers as programmable cogs through automation, commodification and alienation in digitally-mediated work, and elimination of middle management functions. Challenges simplified narratives of algorithmic replacement, demonstrating how algorithms emerge through continuous interaction between organizational members and systems. Critical concerns include deskilling, surveillance, loss of discretion, and embedding of managerial control in opaque technical systems.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1177/20539517211020332](https://doi.org/10.1177/20539517211020332)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/9XLDABF4)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='jiang_2022_assessing_gpt's_bias_towards_stigmatized_social_gr'></a>

## Paper 99/266: Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia

**Source file:** `Jiang_2022_Assessing_GPT's_bias_towards_stigmatized_social_gr.md`

---
title: "Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia"
zotero_key: TNTENI9Y
author_year: "Jiang (2022)"
authors: []

# Publication
publication_year: 2022.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/pdf/2505.17045"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2022.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/pdf/2505.17045
- **Zotero:** [Open in Zotero](zotero://select/items/TNTENI9Y)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='jin_2025_glat__the_generative_ai_literacy_assessment_test'></a>

## Paper 100/266: GLAT: The generative AI literacy assessment test

**Source file:** `Jin_2025_GLAT__The_generative_AI_literacy_assessment_test.md`

---
title: "GLAT: The generative AI literacy assessment test"
zotero_key: B4KQS7M5
author_year: "Jin (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: en
doi: "10.1016/j.caeai.2025.100436"
url: "https://linkinghub.elsevier.com/retrieve/pii/S2666920X25000761"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# GLAT: The generative AI literacy assessment test

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1016/j.caeai.2025.100436](https://doi.org/10.1016/j.caeai.2025.100436)
- **URL:** https://linkinghub.elsevier.com/retrieve/pii/S2666920X25000761
- **Zotero:** [Open in Zotero](zotero://select/items/B4KQS7M5)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='j√§√§skel√§inen_2025_intersectional_analysis_of_visual_generative_ai__t'></a>

## Paper 101/266: Intersectional analysis of visual generative AI: The case of Stable Diffusion

**Source file:** `J√§√§skel√§inen_2025_Intersectional_analysis_of_visual_generative_AI__T.md`

---
title: "Intersectional analysis of visual generative AI: The case of Stable Diffusion"
zotero_key: 9G6AQ3AA
author_year: "J√§√§skel√§inen (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1007/s00146-025-02207-y"
url: "https://link.springer.com/article/10.1007/s00146-025-02207-y"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: true
summary_file: "summary_J√§√§skel√§inen_2025_Intersectional.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Intersectional analysis of visual generative AI: The case of Stable Diffusion

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This open-access paper provides a feminist intersectional critique of Stable Diffusion through qualitative visual analysis of 180 AI-generated images. Authors examined how power systems like racism, sexism, heteronormativity, ableism, colonialism, and capitalism are reflected in AI outputs. Found that default outputs frequently perpetuate harmful stereotypes and assume a "white, able-bodied, masculine-presenting" default subject position. Advocates for social justice-oriented approach to AI by acknowledging cultural-aesthetic biases and engaging in reparative strategies.


## AI Summary

## Overview

This paper by J√§√§skel√§inen, Sharma, Pallett, and √Ösberg presents a critical examination of Stable Diffusion (SD), a widely-adopted open-source visual generative AI tool launched in August 2022 that has generated over 12 billion images with 10 million daily active users. The research fundamentally challenges the prevailing assumption that vGenAI technologies operate as culturally neutral tools, instead arguing that these systems actively encode, reflect, and perpetuate existing societal power structures and inequalities. By analyzing 180 deliberately-prompted SD-generated images through an intersectional lens, the authors provide empirical evidence that generative AI is not neutral but functions as an active agent reproducing harmful visual politics that systematically advantage dominant groups while marginalizing others.

## Main Findings

The analysis reveals three critical and interconnected findings. First, SD-generated imagery perpetuates systemic inequalities including sexism, racism, heteronormativity, and ableism through consistent aesthetic choices and representational patterns‚Äîdemonstrating that the technology actively reproduces rather than merely reflects pre-existing biases. Second, SD exhibits a persistent "default individual" assumption, with images predominantly featuring white, able-bodied, masculine-presenting subjects, indicating embedded cultural hierarchies within training data and design architecture. Third, outputs demonstrate pronounced Euro- and North America-centric cultural representations, directly traceable to the institutional origins, development contexts, and training datasets of these tools. Critically, the authors document how SD produces harmful and violent imagery targeting marginalized groups, establishing that vGenAI cannot achieve cultural or aesthetic neutrality without deliberate intervention in training data, institutional practices, and design choices.

## Methodology/Approach

The research employs qualitative, interpretative visual analysis examining 180 SD-generated images deliberately prompted across multiple intersecting axes of privilege and disadvantage‚Äîincluding wealth/poverty distinctions, citizen/immigrant statuses, and other social positioning variables. This systematic prompting strategy enables investigation of how the technology differentially represents social groups. The theoretical framework integrates three complementary disciplinary perspectives: Feminist Science and Technology Studies (examining power dynamics in technological development and design), visual media studies (analyzing aesthetic and representational politics), and intersectional critical theory (understanding how multiple systems of oppression simultaneously operate and reinforce one another). This interdisciplinary integration moves beyond isolated bias identification toward comprehensive understanding of intersecting power systems within visual outputs.

## Relevant Concepts

**Visual Generative AI (vGenAI)**: AI systems trained on large image datasets that generate novel photorealistic images from text prompts, including Stable Diffusion, DALL-E, and Midjourney.

**Intersectionality**: Analytical framework examining how multiple systems of oppression (race, gender, class, ability, citizenship) simultaneously operate and interact within social structures and technologies.

**Algorithmic bias**: Systematic prejudices embedded within AI systems through training data selection, design choices, and institutional contexts that disadvantage specific social groups.

**Cultural-aesthetic politics**: Ideological dimensions embedded within aesthetic choices and visual representations that reflect and reinforce particular worldviews, hierarchies, and power arrangements.

**Algorithmic reparation**: Restorative justice approach acknowledging harms caused by biased AI systems and implementing material and symbolic interventions to mend injustices against affected social groups.

**Default individual assumption**: Implicit design assumption that treats particular demographic characteristics (whiteness, able-bodiedness, masculinity) as unmarked or universal, rendering other identities as marked or exceptional.

## Significance

This paper makes substantial contributions to critical AI scholarship by centering visual and intersectional analysis‚Äîdimensions frequently overlooked in technical AI audits and fairness frameworks. It bridges humanities-centered critique with technology studies, offering essential counterweight to techno-optimist narratives claiming AI neutrality. The work advances beyond problem identification toward constructive solutions through its explicit reparative justice framework, representing an emerging normative turn in critical AI studies. The research has immediate relevance for policymakers, technologists, and researchers developing generative AI systems, providing empirical evidence that algorithmic neutrality is impossible without deliberate interventions. Most significantly, by demonstrating how vGenAI actively reproduces harmful visual politics rather than passively reflecting them, the paper establishes urgent grounds for implementing algorithmic reparation strategies, restorative justice approaches, and institutional reforms in AI development and deployment. The authors' interdisciplinary positioning‚Äîspanning media technology, environmental sciences, data science, and gender studies‚Äîstrengthens the credibility of their humanities-centered critique within technology discourse.


## Links & Resources

- **DOI:** [10.1007/s00146-025-02207-y](https://doi.org/10.1007/s00146-025-02207-y)
- **URL:** https://link.springer.com/article/10.1007/s00146-025-02207-y
- **Zotero:** [Open in Zotero](zotero://select/items/9G6AQ3AA)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='j√∏rgensen_2023_data_and_rights_in_the_digital_welfare_state__the_'></a>

## Paper 102/266: Data and rights in the digital welfare state: the case of Denmark

**Source file:** `J√∏rgensen_2023_Data_and_rights_in_the_digital_welfare_state__the_.md`

---
title: "Data and rights in the digital welfare state: the case of Denmark"
zotero_key: 7V9GKPNZ
author_year: "J√∏rgensen (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "10.1080/1369118X.2021.1934069"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 1
rel_prof: 2
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-medium", "dim-prof-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Data and rights in the digital welfare state: the case of Denmark

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

Critical analysis examining how surveillance capitalism logic manifests in public sector through Denmark's automated welfare decision-making systems. Argues that unless more human-centric approaches are adopted, digital welfare states advance digital technocracy treating citizens as data points for calculation and prediction rather than individuals with agency and rights. Employs theories of surveillance capitalism, digital-era governance, and data politics to analyze automated decision support deployment by state actors. Key concerns include data maximization, profiling, surveillance, and citizen disempowerment through predictive analytics used to identify fraud and vulnerability. Demonstrates how Danish municipalities' enthusiasm for AI-driven welfare administration mirrors tech giants' commercial data exploitation.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1080/1369118X.2021.1934069](https://doi.org/10.1080/1369118X.2021.1934069)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/7V9GKPNZ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='kamruzzaman_2024_prompting_techniques_for_reducing_social_bias_in_l'></a>

## Paper 103/266: Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes

**Source file:** `Kamruzzaman_2024_Prompting_techniques_for_reducing_social_bias_in_L.md`

---
title: "Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes"
zotero_key: FMXXR4DH
author_year: "Kamruzzaman (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/html/2404.17218v1"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 3
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Bias Analysis", "Practical Implementation"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Kamruzzaman_2024_Prompting.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Prompting techniques for reducing social bias in LLMs through System 1 and System 2 cognitive processes

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Bias Analysis, Practical Implementation |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This study evaluates 12 prompt strategies across five LLMs, finding that instructing a model to adopt a System 2 (deliberative) reasoning style and a "human persona" most effectively reduces stereotypes. Combining these two strategies yielded up to a 13% reduction in stereotypical responses. Contrary to prior assumptions, Chain-of-Thought (CoT) prompting alone was not as effective, showing bias levels similar to a default prompt. The results suggest that prompts encouraging careful, human-like reasoning are key for mitigating bias.


## AI Summary

## Overview

This research paper addresses social bias mitigation in Large Language Models through psychologically-informed prompting techniques grounded in dual process theory. Authored by Kamruzzaman and Kim (University of South Florida), and accepted at RANLP-2025, the work applies Kahneman's cognitive framework‚Äîdistinguishing System 1 (fast, intuitive, bias-prone) from System 2 (slow, deliberate, analytical) reasoning‚Äîto develop practical prompting strategies. Unlike computationally expensive fine-tuning requiring model weight access, this approach offers lightweight, accessible solutions suitable for closed-source models and resource-constrained environments. The research systematically evaluates three prompting strategies across two bias datasets spanning nine social bias categories (beyond previous gender-bias-focused work), testing multiple LLM models including Llama3.3. Code is publicly available, enabling reproducibility and practitioner implementation.

## Main Findings

The research demonstrates that combining multiple interventions achieves substantial bias reduction, with maximum improvements reaching 33% decrease in stereotypical judgments. Critically, all four tested interventions independently reduce social bias: (1) human persona adoption, (2) explicit debiasing instructions, (3) System 2 reasoning prompts, and (4) zero-shot chain-of-thought prompting. However, no universal optimal strategy exists; effectiveness varies significantly by specific LLM model and bias category. Notably, human persona modeling produces independent bias-reduction effects beyond cognitive process simulation alone, indicating LLMs respond to social role-playing through mechanisms transcending pure reasoning enhancement. This context-dependent effectiveness reveals that optimal combinations differ across model-bias category pairs, requiring tailored approaches rather than standardized solutions. The findings suggest prompting interventions operate through multiple complementary mechanisms rather than single causal pathways.

## Methodology/Approach

The experimental design employs comparative analysis across three prompting strategies: zero-shot chain-of-thought (step-by-step reasoning without examples), direct debiasing instructions (explicit fairness guidance), and dual process theory-based prompting (System 2 engagement). Testing occurs on two bias datasets encompassing nine social bias categories, substantially expanding previous research scope. Researchers introduce human and machine personas as independent variables, enabling isolation of whether dual process effects operate independently or depend on explicit persona modeling. Multiple LLM models undergo testing to ensure generalizability. This methodological design allows disentanglement of confounding factors and identification of which intervention components contribute most substantially to bias reduction, with systematic comparison revealing interaction effects between strategies.

## Relevant Concepts

**Dual Process Theory:** Kahneman's psychological framework positing two cognitive systems‚ÄîSystem 1 (automatic, emotional, susceptible to biases) and System 2 (deliberate, analytical, rational)‚Äîgoverning human decision-making and reasoning.

**Chain-of-Thought (CoT) Prompting:** Technique requiring LLMs to articulate step-by-step reasoning processes, theoretically engaging System 2 analytical capabilities and improving accuracy, transparency, and fairness in outputs.

**Zero-shot CoT:** CoT prompting without providing examples, relying on the model's inherent reasoning capabilities without demonstration-based learning.

**Social Bias:** Systematic stereotypical judgments embedded in LLM outputs reflecting training data prejudices, cultural assumptions, and learned associations across nine categories (gender, race, religion, age, disability, sexual orientation, etc.).

**Persona Modeling:** Instructing LLMs to adopt specific social roles or identities (human vs. machine), influencing response generation patterns and bias manifestation.

**Direct Debiasing:** Explicit fairness instructions embedded in prompts, instructing models to avoid stereotypical reasoning.

## Significance

This work makes four critical contributions to AI ethics and cognitive science. First, it systematizes the theoretical connection between established psychological frameworks and LLM behavior, providing principled foundations for bias mitigation grounded in cognitive science. Second, it offers immediately actionable solutions for practitioners lacking model access or computational resources, democratizing bias reduction techniques beyond institutional capabilities. Third, it expands bias research beyond gender to encompass nine categories, providing comprehensive empirical evidence of prompting effectiveness across diverse bias types. Fourth, it demonstrates that accessible, lightweight interventions achieve meaningful bias reduction (up to 33%), making ethical AI deployment more feasible across diverse organizational contexts. The research acknowledges inherent limitations‚Äîprompting may have ceiling effects compared to architectural modifications‚Äîyet establishes that accessible interventions can substantially improve fairness in deployed systems.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2404.17218v1
- **Zotero:** [Open in Zotero](zotero://select/items/FMXXR4DH)

## Related Concepts

- Dual Process Theory
- Chain-of-Thought (CoT) Prompting
- Zero-shot CoT
- Social Bias
- Persona Modeling
- Direct Debiasing

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='kaneko_2024_debiasing_prompts_for_gender_bias_in_large_languag'></a>

## Paper 104/266: Debiasing prompts for gender bias in large language models

**Source file:** `Kaneko_2024_Debiasing_prompts_for_gender_bias_in_large_languag.md`

---
title: "Debiasing prompts for gender bias in large language models"
zotero_key: YJAHIZMR
author_year: "Kaneko (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/html/2404.17218v3"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Debiasing prompts for gender bias in large language models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2404.17218v3
- **Zotero:** [Open in Zotero](zotero://select/items/YJAHIZMR)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='kaneko_2024_evaluating_gender_bias_in_large_language_models_vi'></a>

## Paper 105/266: Evaluating gender bias in large language models via chain-of-thought prompting

**Source file:** `Kaneko_2024_Evaluating_gender_bias_in_large_language_models_vi.md`

---
title: "Evaluating gender bias in large language models via chain-of-thought prompting"
zotero_key: CCCM5RLR
author_year: "Kaneko (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/abs/2401.15585"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Evaluating gender bias in large language models via chain-of-thought prompting

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

This study investigates if Chain-of-Thought (CoT) prompting reduces implicit gender bias in LLMs. Using a synthetic task of counting gendered words, the authors found that without step-by-step prompting, models made biased errors. CoT prompting, which forced the model to explicitly label each word's gender before counting, significantly reduced these mistakes. This suggests that guiding the model through an explicit reasoning process makes it rely more on logic than on stereotypes, thereby mitigating bias.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/abs/2401.15585
- **Zotero:** [Open in Zotero](zotero://select/items/CCCM5RLR)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='karagianni_2025_gender_in_a_stereo-(gender)typical_eu_ai_law__a_fe'></a>

## Paper 106/266: Gender in a stereo-(gender)typical EU AI law: A feminist reading of the AI Act

**Source file:** `Karagianni_2025_Gender_in_a_stereo-(gender)typical_EU_AI_law__A_fe.md`

---
title: "Gender in a stereo-(gender)typical EU AI law: A feminist reading of the AI Act"
zotero_key: F8GURS3C
author_year: "Karagianni (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1017/cfl.2025.12"
url: "https://www.cambridge.org/core/journals/cambridge-forum-on-ai-law-and-governance/article/gender-in-astereogendertypical-eu-ai-law-a-feminist-reading-of-the-ai-act/E9DEFC1E114CBE577D737EC616610921"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Karagianni_2025_Gender.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Gender in a stereo-(gender)typical EU AI law: A feminist reading of the AI Act

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This peer-reviewed article offers a feminist critique of the European Union‚Äôs AI Act, arguing that the law‚Äôs current bias mitigation provisions only superficially address gendered risks and fail to confront deeply embedded structural biases. Using frameworks like hermeneutical injustice and feminist legal theory, Karagianni shows that AI systems can perpetuate historical power imbalances and systemic discrimination against women and marginalized groups. The AI Act‚Äôs pre-market and post-market measures are analyzed to reveal how they often reinforce, rather than challenge, algorithmic discrimination. The author concludes that effective AI governance must go beyond technical fixes, incorporating an intersectional perspective and substantive equality principles. She calls for feminist-informed revisions to the AI Act ‚Äì emphasizing gender inclusivity, intersectionality, and accountability ‚Äì to ensure AI regulation actively dismantles (instead of inadvertently upholding) existing power asymmetries.


## AI Summary

## Overview

Anastasia Karagianni's research article, published in 2025, provides a critical feminist examination of the European Union's AI Act (Regulation 2024/1689), questioning whether this landmark regulatory framework adequately protects marginalized communities from gender-based discrimination embedded in artificial intelligence systems. Conducted within the Law Science Technology and Society (LSTS) Research Group at Vrije Universiteit Brussels, the paper challenges the assumption that formal regulatory provisions automatically translate into substantive protection against algorithmic bias. By applying feminist legal theory to specific AI Act provisions‚Äîparticularly those addressing "special categories of personal data" processing‚ÄîKaragianni argues that the regulation, while well-intentioned, perpetuates structural inequalities by failing to address the systemic power imbalances that shape AI development and deployment. The work positions itself within emerging critical scholarship that demands transformative rather than incremental approaches to technology governance.

## Main Findings

The analysis reveals significant gaps between the AI Act's stated objectives and its practical capacity to protect vulnerable populations. While the regulation permits processing "special categories of personal data" (including gender identity information) to prevent algorithmic discrimination, this mechanism proves insufficient for addressing deeply embedded structural biases. The paper identifies four critical deficiencies: first, existing provisions fail to challenge underlying structural inequalities that predate and shape AI systems; second, binary gender frameworks persist throughout AI development and deployment, marginalizing transgender and non-binary individuals; third, accountability mechanisms lack enforceability for gender-based harms; and fourth, the regulation inadequately addresses gender-based violence enabled by generative AI, particularly non-consensual deepfakes. The research demonstrates that current regulatory approaches treat discrimination as isolated technical problems rather than manifestations of systemic power hierarchies. Concrete examples‚Äîincluding Amazon's biased recruitment algorithm disadvantaging women, healthcare AI systems operating within binary gender frameworks that misgendered patients, and non-consensual deepfake technology constituting gender-based violence‚Äîillustrate how AI amplifies existing societal inequalities. Karagianni concludes that intersectional perspectives remain absent from regulatory design, limiting the AI Act's capacity to address compounded discrimination experienced by individuals with multiple marginalized identities (women, LGBTQIA+ people, and other marginalized communities).

## Methodology/Approach

The paper employs sophisticated feminist legal methods grounded in four complementary theoretical frameworks. Miranda Fricker's hermeneutical injustice theory illuminates how epistemic marginalization prevents certain groups from contributing to knowledge production about AI harms. Catharine MacKinnon's feminist legal theory reveals how male dominance structures become embedded in technological systems and legal frameworks. An√≠bal Quijano's coloniality of power concept exposes how historical hierarchies persist within contemporary technology governance. Walter Mignolo's decolonial epistemology challenges whose knowledge counts in law and technology development. This multidisciplinary theoretical architecture enables systematic critical examination of specific AI Act articles, revealing how regulatory language either reinforces or fails to challenge algorithmic discrimination. The methodology combines textual analysis of regulatory provisions with theoretical critique of underlying epistemological assumptions.

## Relevant Concepts

**Hermeneutical injustice:** Epistemic marginalization preventing certain groups from contributing to collective understanding of AI harms and discrimination.

**Structural bias:** Systemic inequalities embedded within AI systems that disproportionately harm marginalized communities, rooted in biased training data and design choices.

**Intersectionality:** Framework recognizing how multiple marginalized identities (gender, sexuality, race, disability) compound discrimination experiences in AI systems.

**Coloniality of power:** Historical power hierarchies and epistemological dominance persisting within contemporary institutions, technologies, and regulatory frameworks.

**Algorithmic discrimination:** Discriminatory outcomes produced through automated decision-making systems, often reproducing historical biases at scale.

**Special categories of personal data:** EU legal concept referring to sensitive data (including gender identity) whose processing is restricted but permitted under specific conditions for non-discrimination purposes.

**Gender-based violence:** Harm targeting individuals based on gender, including non-consensual deepfakes and sexualized content enabled by generative AI.

## Significance

This research significantly contributes to critical AI governance scholarship by demonstrating that formal regulatory frameworks require feminist-informed revision to achieve substantive equity. The paper challenges technocratic optimism surrounding the AI Act, arguing that regulatory provisions alone cannot address discrimination rooted in epistemological and structural inequalities. By proposing feminist-informed revisions emphasizing gender inclusivity, intersectionality, and accountability mechanisms with enforcement capacity, Karagianni advances scholarship demanding transformative approaches to technology governance. The work proves particularly significant for policymakers, legal scholars, technology ethicists, and marginalized communities seeking to develop regulations that genuinely protect vulnerable populations rather than merely appearing to do so. It contributes to broader EU policy discourse on AI governance by identifying specific gaps requiring legislative amendment before the AI Act's full implementation.


## Links & Resources

- **DOI:** [10.1017/cfl.2025.12](https://doi.org/10.1017/cfl.2025.12)
- **URL:** https://www.cambridge.org/core/journals/cambridge-forum-on-ai-law-and-governance/article/gender-in-astereogendertypical-eu-ai-law-a-feminist-reading-of-the-ai-act/E9DEFC1E114CBE577D737EC616610921
- **Zotero:** [Open in Zotero](zotero://select/items/F8GURS3C)

## Related Concepts

- Hermeneutical injustice
- Structural bias
- Intersectionality
- Coloniality of power
- Algorithmic discrimination
- Special categories of personal data
- Gender-based violence

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='kattnig_2024_assessing_trustworthy_ai__technical_and_legal_pers'></a>

## Paper 107/266: Assessing trustworthy AI: Technical and legal perspectives of fairness in AI

**Source file:** `Kattnig_2024_Assessing_trustworthy_AI__Technical_and_legal_pers.md`

---
title: "Assessing trustworthy AI: Technical and legal perspectives of fairness in AI"
zotero_key: 9A73EE79
author_year: "Kattnig (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1016/j.clsr.2024.106053"
url: "https://graz.elsevierpure.com/ws/portalfiles/portal/89954869/1-s2.0-S0267364924001195-main.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Kattnig_2024_Assessing.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Assessing trustworthy AI: Technical and legal perspectives of fairness in AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Kattnig et al. examine the disconnect between existing AI fairness techniques and the requirements of non-discrimination law, highlighting that many algorithmic bias mitigation methods do not meet legal standards for equality. Focusing on the EU context (with particular attention to the forthcoming AI Act and established non-discrimination directives), the authors review state-of-the-art bias mitigation approaches ‚Äì from pre-processing data fixes to in-processing algorithms ‚Äì and evaluate them against legal concepts of fairness and equality. They discuss how ambiguous legal frameworks and the difficulty of defining ‚Äúfairness‚Äù pose challenges: for instance, fairness has multiple interpretations (individual vs. group fairness, formal vs. substantive equality) and is understood differently across disciplines. The paper argues for an interdisciplinary legal methodology to complement technical solutions. In practice, this means moving beyond purely quantitative parity metrics and ensuring AI systems comply with human rights and equality principles (e.g. ensuring de facto non-discrimination for all data subjects). By contrasting algorithms with legal norms, the study underlines that trustworthy AI requires more than technical robustness ‚Äì it demands alignment with justice and accountability frameworks.


## AI Summary

## Overview

This academic paper addresses a critical contemporary challenge in artificial intelligence: ensuring fairness and non-discrimination in AI systems through integrated technical and legal analysis. Published in *Computer Law & Security Review*, the work by Kattnig et al. from Graz University of Technology examines the fundamental gap between technical bias mitigation methods and legal compliance requirements, particularly within the European Union regulatory framework and the emerging AI Act. The research recognizes that as AI systems increasingly influence consequential decisions affecting human lives‚Äîfrom hiring algorithms to criminal risk assessment‚Äîthe imperative to ensure fair, unbiased decision-making has become paramount. The paper's central premise challenges the prevailing assumption that technical solutions for fairness automatically satisfy legal standards, arguing instead for an integrated approach that bridges computer science and legal scholarship to establish trustworthy AI governance.

## Main Findings

The research reveals several critical gaps between technical and legal approaches to AI fairness. Most significantly, few existing bias mitigation methods adequately meet legal requirements for non-discrimination under EU regulations and the AI Act. The paper identifies that bias‚Äîdefined as systematic and unfair behavior in AI systems‚Äîoften emerges when training data contains historical inequalities, inadvertently perpetuating discrimination against already disadvantaged groups. The COMPAS algorithm case study exemplifies this problem, demonstrating how racial bias becomes embedded in decision-making systems with serious social consequences. Additional findings indicate that bias identification remains technically challenging despite widespread AI deployment, creating operational and compliance risks. The paper reveals that fairness definitions remain contested across technical and legal disciplines, creating conceptual confusion that impedes implementation. Crucially, AI systems lack "common sense" or causal reasoning capabilities, compounding fairness challenges beyond statistical bias mitigation. The authors conclude that comprehensive legal methodology is essential for proper AI fairness assessment, and that technical bias mitigation alone proves insufficient without legal validation and alignment with regulatory requirements.

## Methodology/Approach

The paper employs a comparative analytical framework that systematically reviews state-of-the-art bias mitigation technical methods while contrasting them against legal requirements. The geographic scope is limited to the European Union, with particular emphasis on AI Act compliance and existing legal frameworks. The methodology examines both fairness definitions and measurement approaches, identifying conceptual challenges in operationalizing fairness across disciplines. The framework analyzes both group fairness (equitable treatment across demographic groups) and individual fairness (similar treatment for similar individuals). This comparative approach deliberately bridges disciplinary boundaries, recognizing that neither purely technical optimization nor purely legal analysis suffices independently. The methodology acknowledges that bias identification remains technically challenging, requiring integrated expertise from computer science, statistics, and legal scholarship.

## Relevant Concepts

**Bias**: Systematic and unfair behavior or errors in AI systems leading to discriminatory outcomes and unjust decisions.

**Group Fairness**: Ensuring equitable treatment across demographic groups or protected categories in AI decision-making.

**Individual Fairness**: Ensuring similar individuals receive similar treatment from AI systems regardless of protected characteristics.

**Trustworthy AI**: AI systems operating transparently, safely, and in compliance with legal, ethical, and regulatory standards.

**Bias Mitigation**: Technical methods (pre-processing, in-processing, post-processing) designed to reduce or eliminate discriminatory outcomes.

**Non-discrimination**: Legal principle ensuring AI decisions do not unfairly disadvantage protected groups or individuals.

**Causality**: AI system's ability to infer cause-and-effect relationships, enabling "common sense" reasoning beyond statistical patterns.

**Legal Compliance Gap**: Mismatch between technical fairness metrics and actual legal requirements under EU regulations and AI Act.

**Fairness Metrics**: Quantitative measures assessing whether AI systems meet fairness standards across different definitions.

## Significance

This work significantly advances the emerging field of trustworthy AI governance by challenging disciplinary silos and advocating for integrated expertise. It contributes to responsible AI discourse by emphasizing that legal compliance must guide technical implementation, not follow it. The paper's position as a "bridge-builder" between computer science and legal studies reflects growing recognition that AI regulation requires interdisciplinary collaboration. By examining AI Act requirements specifically, the research provides practical guidance for EU compliance while advancing theoretical understanding of fairness in AI. The work establishes that data subjects' rights to fair, non-discriminatory treatment demand systematic solutions transcending technical optimization alone. For organizations deploying AI systems, the research highlights that achieving fairness requires alignment between technical bias mitigation approaches and legal frameworks, not merely technical excellence. The paper establishes a foundation for future regulatory frameworks, technical standards development, and organizational compliance strategies in trustworthy AI implementation.


## Links & Resources

- **DOI:** [10.1016/j.clsr.2024.106053](https://doi.org/10.1016/j.clsr.2024.106053)
- **URL:** https://graz.elsevierpure.com/ws/portalfiles/portal/89954869/1-s2.0-S0267364924001195-main.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/9A73EE79)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='kawakami_2022_improving_human-ai_partnerships_in_child_welfare__'></a>

## Paper 108/266: Improving human-AI partnerships in child welfare: Understanding worker practices, challenges, and desires for algorithmic decision support

**Source file:** `Kawakami_2022_Improving_human-AI_partnerships_in_child_welfare__.md`

---
title: "Improving human-AI partnerships in child welfare: Understanding worker practices, challenges, and desires for algorithmic decision support"
zotero_key: 958TWYXZ
author_year: "Kawakami (2022)"
authors: []

# Publication
publication_year: 2022.0
item_type: conferencePaper
language: nan
doi: "10.1145/3491102.3517439"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 3
total_relevance: 12

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Improving human-AI partnerships in child welfare: Understanding worker practices, challenges, and desires for algorithmic decision support

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2022.0 |
| **Decision** | **Include** |
| **Total Relevance** | **12/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Empirical study examining how child maltreatment hotline workers interact with Allegheny Family Screening Tool, an AI-based decision support system. Through interviews and contextual inquiries, found workers' reliance on algorithmic predictions guided by four key factors: knowledge of contextual information beyond AI model capabilities, beliefs about system limitations, organizational pressures around tool use, and awareness of misalignments between algorithmic predictions and their own decision-making objectives. Reveals discrimination risks stemming from workers lacking adequate training on tool's data sources and limitations.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1145/3491102.3517439](https://doi.org/10.1145/3491102.3517439)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/958TWYXZ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='keddell_2019_algorithmic_justice_in_child_protection__statistic'></a>

## Paper 109/266: Algorithmic justice in child protection: Statistical fairness, social justice and the implications for practice

**Source file:** `Keddell_2019_Algorithmic_justice_in_child_protection__Statistic.md`

---
title: "Algorithmic justice in child protection: Statistical fairness, social justice and the implications for practice"
zotero_key: G5TXUQLW
author_year: "Keddell (2019)"
authors: []

# Publication
publication_year: 2019.0
item_type: journalArticle
language: nan
doi: "10.3390/socsci8100281"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 2
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-prof-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Algorithmic justice in child protection: Statistical fairness, social justice and the implications for practice

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2019.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

Critical analysis examining fairness and justice implications of predictive algorithms in child protection from both statistical and social justice perspectives. Identifies fundamental problems with using child protection system data: biased sample frames reflecting reporting patterns rather than actual abuse incidence, feedback loops amplifying discrimination, and spurious correlations conflating system surveillance with genuine risk. Demonstrates racial and socioeconomic disparities in child welfare data lead to algorithmic tools disproportionately identifying Indigenous, Black, and poor families as high risk even when true abuse rates are similar across groups.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.3390/socsci8100281](https://doi.org/10.3390/socsci8100281)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/G5TXUQLW)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='klein_2024_data_feminism_for_ai'></a>

## Paper 110/266: Data feminism for AI

**Source file:** `Klein_2024_Data_Feminism_for_AI.md`

---
title: "Data feminism for AI"
zotero_key: IXCQLI27
author_year: "Klein (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: conferencePaper
language: nan
doi: "10.1145/3630106.3658543"
url: "https://doi.org/10.1145/3630106.3658543"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-medium", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Klein_2024_Data.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Data feminism for AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Extends the influential "Data Feminism" framework to AI research, presenting intersectional feminist principles for conducting equitable, ethical, and sustainable AI research. Rearticulates original seven data feminism principles specifically for AI contexts and introduces two new principles addressing environmental impact and consent, providing concrete methodological guidance.


## AI Summary

## Overview

Klein and D'Ignazio's "Data Feminism for AI" extends their influential 2020 framework to address contemporary challenges in artificial intelligence development and deployment. Published at the 2024 ACM Conference on Fairness, Accountability, and Transparency, this paper argues that feminist analytical approaches are essential for identifying and mitigating structural inequalities embedded within AI systems. Motivated by a decade of evidence demonstrating how data power is wielded unequally by corporations, governments, and well-resourced institutions, the authors demonstrate that feminism‚Äîwith its analytic focus on root causes of structural inequality‚Äîprovides necessary tools for rebalancing power. The work responds to growing recognition that AI technologies, despite claims of objectivity and neutrality, systematically perpetuate extractive, exclusionary, and undemocratic practices that disproportionately harm marginalized communities.

## Main Findings

The paper demonstrates that feminist principles have achieved significant uptake across academic institutions and public sector organizations since the original Data Feminism publication, validating the framework's relevance and applicability. However, the authors identify that AI's distinctive characteristics‚Äîincluding unprecedented scale, algorithmic opacity, and far-reaching societal consequences‚Äînecessitate refinement and expansion of the original seven principles: examine power, challenge power, rethink binaries and hierarchies, elevate emotion and embodiment, consider context, embrace pluralism, and make labor visible. The paper introduces two new principles addressing environmental sustainability and informed consent, recognizing that comprehensive AI governance must account for ecological impacts and user agency. The authors establish three primary objectives: (1) accounting for unequal, undemocratic, extractive, and exclusionary forces in AI research and deployment; (2) identifying and mitigating predictable harms before unsafe, discriminatory, or oppressive systems reach deployment; and (3) inspiring creative, joyful, and collective approaches toward equitable, sustainable worlds where all can thrive.

## Methodology/Approach

Rather than conducting empirical research or technical analysis, the paper employs a normative theoretical framework grounded in intersectional feminist theory and critical data studies scholarship. The methodology integrates structural inequality analysis, power dynamics examination, and interdisciplinary perspectives spanning humanities, social sciences, and computing. The authors operationalize nine principles‚Äîseven rearticulated from Data Feminism with AI-specific applications, plus two new principles‚Äîinto concrete, actionable guidelines for researchers, practitioners, policymakers, and technologists. This approach deliberately bridges disciplinary boundaries, positioning feminist scholarship as foundational to technical AI governance rather than supplementary to it. The framework prioritizes proactive harm prevention through feminist analysis applied before system deployment.

## Relevant Concepts

**Intersectional Feminism:** Analytical framework examining how multiple systems of oppression (gender, race, class, etc.) interconnect and compound inequalities within technological systems.

**Structural Inequality:** Systemic disadvantages embedded in institutional practices and power distributions that harm marginalized communities through AI deployment.

**Data Justice:** Principle ensuring equitable access, representation, and benefit-sharing in data collection, analysis, and application processes.

**Algorithmic Opacity:** The "black box" problem wherein AI decision-making processes remain incomprehensible to users and affected communities, obscuring potential biases.

**Epistemic Pluralism:** Recognition that multiple forms of knowledge and ways of knowing are valid and necessary for comprehensive understanding.

**Environmental Impact Principle:** New principle addressing ecological consequences of AI systems, including computational resource consumption and sustainability concerns.

**Consent Principle:** New principle ensuring informed, voluntary participation and agency of individuals and communities affected by AI systems.

## Significance

This work represents a paradigm shift in AI ethics discourse, moving beyond purely technical solutions toward socio-political analysis grounded in established feminist scholarship. By publishing at FAccT '24, a premier venue for computer science conversations on fairness and accountability, the paper legitimizes humanistic perspectives within mainstream AI governance discussions. The framework's demonstrated adoption across academia and public sectors indicates growing institutional recognition that addressing AI inequities requires fundamental rethinking of power structures, not merely algorithmic adjustments. The paper's emphasis on "joyful" and "collective" approaches distinguishes it from deficit-focused AI ethics, proposing affirmative visions of equitable technological futures. Ultimately, the work positions feminism as essential‚Äînot optional‚Äîfor developing AI systems that enable collective thriving rather than perpetuating systemic oppression, while establishing concrete operational principles for practitioners across sectors.


## Links & Resources

- **DOI:** [10.1145/3630106.3658543](https://doi.org/10.1145/3630106.3658543)
- **URL:** https://doi.org/10.1145/3630106.3658543
- **Zotero:** [Open in Zotero](zotero://select/items/IXCQLI27)

## Related Concepts

- Intersectional Feminism
- Structural Inequality
- Data Justice
- Algorithmic Opacity
- Epistemic Pluralism
- Environmental Impact Principle
- Consent Principle

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='klinge_2024_a_sociolinguistic_approach_to_stereotype_assessmen'></a>

## Paper 111/266: A sociolinguistic approach to stereotype assessment in large language models

**Source file:** `Klinge_2024_A_sociolinguistic_approach_to_stereotype_assessmen.md`

---
title: "A sociolinguistic approach to stereotype assessment in large language models"
zotero_key: JTXKA8KJ
author_year: "Klinge (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "https://facctconference.org/static/docs/facct2025-206archivalpdfs/facct2025-final1618-acmpaginated.pdf"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# A sociolinguistic approach to stereotype assessment in large language models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://facctconference.org/static/docs/facct2025-206archivalpdfs/facct2025-final1618-acmpaginated.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/JTXKA8KJ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='knowles_2023_trustworthy_ai_and_the_logics_of_intersectional_re'></a>

## Paper 112/266: Trustworthy AI and the Logics of Intersectional Resistance

**Source file:** `Knowles_2023_Trustworthy_AI_and_the_Logics_of_Intersectional_Re.md`

---
title: "Trustworthy AI and the Logics of Intersectional Resistance"
zotero_key: U7DW2TX8
author_year: "Knowles (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: conferencePaper
language: nan
doi: "10.1145/3593013.3593986"
url: "https://doi.org/10.1145/3593013.3593986"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Trustworthy AI and the Logics of Intersectional Resistance

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Critically examines mainstream "Trustworthy AI" frameworks from an intersectional feminist perspective, arguing that traditional AI ethics often privilege dominant groups and fail marginalized communities. Suggests reframing trustworthy AI principles to incorporate stewardship, care, humility, and empowerment, addressing intersectional injustices through power-sharing and structural change.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1145/3593013.3593986](https://doi.org/10.1145/3593013.3593986)
- **URL:** https://doi.org/10.1145/3593013.3593986
- **Zotero:** [Open in Zotero](zotero://select/items/U7DW2TX8)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='kojima_2022_large_language_models_are_zero-shot_reasoners'></a>

## Paper 113/266: Large language models are zero-shot reasoners

**Source file:** `Kojima_2022_Large_language_models_are_zero-shot_reasoners.md`

---
title: "Large language models are zero-shot reasoners"
zotero_key: VL782GFG
author_year: "Kojima (2022)"
authors: []

# Publication
publication_year: 2022.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "nan"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Large language models are zero-shot reasoners

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2022.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/VL782GFG)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='kong_2021_evaluation_of_an_artificial_intelligence_literacy_'></a>

## Paper 114/266: Evaluation of an artificial intelligence literacy course for university students with diverse study backgrounds

**Source file:** `Kong_2021_Evaluation_of_an_artificial_intelligence_literacy_.md`

---
title: "Evaluation of an artificial intelligence literacy course for university students with diverse study backgrounds"
zotero_key: 8H4QJL4B
author_year: "Kong (2021)"
authors: []

# Publication
publication_year: 2021.0
item_type: journalArticle
language: en
doi: "10.1016/j.caeai.2021.100026"
url: "https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000205"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Evaluation of an artificial intelligence literacy course for university students with diverse study backgrounds

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2021.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1016/j.caeai.2021.100026](https://doi.org/10.1016/j.caeai.2021.100026)
- **URL:** https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000205
- **Zotero:** [Open in Zotero](zotero://select/items/8H4QJL4B)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='kong_2022_are__intersectionally_fair__ai_algorithms_really_f'></a>

## Paper 115/266: Are "Intersectionally Fair" AI Algorithms Really Fair to Women of Color? A Philosophical Analysis

**Source file:** `Kong_2022_Are__Intersectionally_Fair__AI_Algorithms_Really_F.md`

---
title: "Are "Intersectionally Fair" AI Algorithms Really Fair to Women of Color? A Philosophical Analysis"
zotero_key: WPPQNAH9
author_year: "Kong (2022)"
authors: []

# Publication
publication_year: 2022.0
item_type: conferencePaper
language: nan
doi: "10.1145/3531146.3533074"
url: "https://doi.org/10.1145/3531146.3533074"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Are "Intersectionally Fair" AI Algorithms Really Fair to Women of Color? A Philosophical Analysis

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2022.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Diese philosophische Analyse identifiziert drei fundamentale Probleme mit der dominanten Interpretation intersektionaler Fairness in der KI: Die Fokussierung auf Identit√§tskategorien statt Unterdr√ºckungsstrukturen, ein Dilemma zwischen statistischer Parit√§t und substanzieller Gerechtigkeit, sowie die Vernachl√§ssigung historischer Kontexte. Kong argumentiert, dass echte intersektionale Fairness √ºber statistische Metriken hinausgehen und strukturelle Unterdr√ºckungssysteme (Rassismus, Sexismus) direkt adressieren muss. Die Arbeit fordert eine Verschiebung von rein technischen zu kritisch-theoretischen Ans√§tzen.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1145/3531146.3533074](https://doi.org/10.1145/3531146.3533074)
- **URL:** https://doi.org/10.1145/3531146.3533074
- **Zotero:** [Open in Zotero](zotero://select/items/WPPQNAH9)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='kong_2024_developing_an_artificial_intelligence_literacy_fra'></a>

## Paper 116/266: Developing an artificial intelligence literacy framework: Evaluation of a literacy course for senior secondary students using a project-based learning approach

**Source file:** `Kong_2024_Developing_an_artificial_intelligence_literacy_fra.md`

---
title: "Developing an artificial intelligence literacy framework: Evaluation of a literacy course for senior secondary students using a project-based learning approach"
zotero_key: H3QCKDMQ
author_year: "Kong (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: en
doi: "10.1016/j.caeai.2024.100214"
url: "https://linkinghub.elsevier.com/retrieve/pii/S2666920X24000158"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Developing an artificial intelligence literacy framework: Evaluation of a literacy course for senior secondary students using a project-based learning approach

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1016/j.caeai.2024.100214](https://doi.org/10.1016/j.caeai.2024.100214)
- **URL:** https://linkinghub.elsevier.com/retrieve/pii/S2666920X24000158
- **Zotero:** [Open in Zotero](zotero://select/items/H3QCKDMQ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='kong_2025_artificial_intelligence_(ai)_literacy_‚Äì_an_argumen'></a>

## Paper 117/266: Artificial Intelligence (AI) literacy ‚Äì an argument for AI literacy in education

**Source file:** `Kong_2025_Artificial_Intelligence_(AI)_literacy_‚Äì_an_argumen.md`

---
title: "Artificial Intelligence (AI) literacy ‚Äì an argument for AI literacy in education"
zotero_key: W7UBW2XJ
author_year: "Kong (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: en
doi: "10.1080/14703297.2024.2332744"
url: "https://www.tandfonline.com/doi/full/10.1080/14703297.2024.2332744"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Artificial Intelligence (AI) literacy ‚Äì an argument for AI literacy in education

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1080/14703297.2024.2332744](https://doi.org/10.1080/14703297.2024.2332744)
- **URL:** https://www.tandfonline.com/doi/full/10.1080/14703297.2024.2332744
- **Zotero:** [Open in Zotero](zotero://select/items/W7UBW2XJ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='kubes_2024_feministische_ki_‚Äì_k√ºnstliche_intelligenz_f√ºr_alle'></a>

## Paper 118/266: Feministische KI ‚Äì K√ºnstliche Intelligenz f√ºr alle? [Feminist AI ‚Äì Artificial intelligence for everyone?]

**Source file:** `Kubes_2024_Feministische_KI_‚Äì_K√ºnstliche_Intelligenz_f√ºr_alle.md`

---
title: "Feministische KI ‚Äì K√ºnstliche Intelligenz f√ºr alle? [Feminist AI ‚Äì Artificial intelligence for everyone?]"
zotero_key: 5ZQRB5PS
author_year: "Kubes (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: webpage
language: nan
doi: "nan"
url: "https://blogs.fu-berlin.de/toolbox/2024/04/08/feministische-ki-kuenstliche-intelligenz-fuer-alle/"

# Assessment
decision: Unclear
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 3
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 2
rel_prof: 1
total_relevance: 11

# Categorization
relevance_category: high
top_dimensions: ["AI Literacy", "Vulnerable Groups"]

# Tags
tags: ["paper", "unclear", "high-relevance", "dim-ai-komp-high", "dim-vulnerable-high", "dim-bias-medium", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Feministische KI ‚Äì K√ºnstliche Intelligenz f√ºr alle? [Feminist AI ‚Äì Artificial intelligence for everyone?]

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Unclear** |
| **Total Relevance** | **11/15** (high) |
| **Top Dimensions** | AI Literacy, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Innovative interdisciplinary seminar teaches students to critically analyze everyday AI applications from sociotechnical feminist perspectives across four domains: love, robots, work, and creativity. Students analyze AI within androcentric, Eurocentric, anthropocentric, and capitalist-patriarchal structures. Curriculum combines theoretical foundations with practical application through "queerbot" design workshops that reimagine AI beyond normative dichotomies, demonstrating concrete pedagogical approaches for implementing feminist AI literacy education.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://blogs.fu-berlin.de/toolbox/2024/04/08/feministische-ki-kuenstliche-intelligenz-fuer-alle/
- **Zotero:** [Open in Zotero](zotero://select/items/5ZQRB5PS)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='kumar_2024_how_ai_hype_impacts_the_lgbtq+_community'></a>

## Paper 119/266: How AI hype impacts the LGBTQ+ community

**Source file:** `Kumar_2024_How_AI_hype_impacts_the_LGBTQ+_community.md`

---
title: "How AI hype impacts the LGBTQ+ community"
zotero_key: 56VU98F2
author_year: "Kumar (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1007/s43681-024-00423-8"
url: "https://doi.org/10.1007/s43681-024-00423-8"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# How AI hype impacts the LGBTQ+ community

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Die Studie analysiert, wie der Hype um KI heteronormative Annahmen verst√§rkt. Sie f√ºhrt Fallstudien zur Gesichtserkennung, Content-Moderation und Geschlechtsklassifikation durch und zeigt auf, wie queere Identit√§ten algorithmisch marginalisiert werden.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1007/s43681-024-00423-8](https://doi.org/10.1007/s43681-024-00423-8)
- **URL:** https://doi.org/10.1007/s43681-024-00423-8
- **Zotero:** [Open in Zotero](zotero://select/items/56VU98F2)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='kutscher_2020_handbuch_soziale_arbeit_und_digitalisierung'></a>

## Paper 120/266: Handbuch Soziale Arbeit und Digitalisierung

**Source file:** `Kutscher_2020_Handbuch_Soziale_Arbeit_und_Digitalisierung.md`

---
title: "Handbuch Soziale Arbeit und Digitalisierung"
zotero_key: C8VLMDBK
author_year: "Kutscher (2020)"
authors: []

# Publication
publication_year: 2020.0
item_type: book
language: nan
doi: "nan"
url: "https://www.beltz.de/fileadmin/beltz/leseproben/978-3-7799-5258-9.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 1
rel_praxis: 1
rel_prof: 3
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Professional Context", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-prof-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Kutscher_2020_Handbuch.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Handbuch Soziale Arbeit und Digitalisierung

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2020.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Professional Context, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Dieses umfassende Handbuch mit √ºber 50 Beitr√§gen behandelt erstmals systematisch Digitalisierung in Bezug auf Disziplin und Praxis der Sozialen Arbeit. Das 658-seitige Werk beleuchtet aus verschiedenen disziplin√§ren Perspektiven gesellschaftliche Entwicklungen, Diskurse, digitalisierte Formen der Dienstleistungserbringung, Profession, Organisation und Handlungsfelder sowie neue Herausforderungen f√ºr Forschung. Zentrale Themen umfassen Mediatisierung, Akteur-Netzwerk-Theorie, ethische Fragen, informationelle Selbstbestimmung, Datenschutz, Social Media, E-Government, digitalisierte Kinder- und Jugendhilfe, Medienp√§dagogik und Sozialwirtschaft. Das Handbuch verfolgt einen √ºber technisches Verst√§ndnis hinausgehenden Begriff von Digitalit√§t und fokussiert auf soziotechnische Arrangements sowie deren Folgen f√ºr Akteure, Formen und Rahmenbedingungen Sozialer Arbeit. Mit Perspektiven auf Organisation, Fachkr√§fte, Adressat*innen und Erbringungsformen werden M√∂glichkeiten, Risiken und offene Fragestellungen diskutiert.


## AI Summary

## Overview

This German handbook introduction addresses digitalization in social work as a critical scholarly concern requiring systematic theoretical and empirical analysis. The authors argue that while digitalization discussions have existed for decades, recent developments represent qualitative transformations demanding rigorous examination. The handbook moves beyond implementation guidance to interrogate how digital technologies reshape social work practices, organizational structures, and professional decision-making. Crucially, the authors identify that action pressure stems not only from technical developments but from the discourse of digitalization itself‚Äîframed through perceived inevitability, competitive positioning via "lighthouse projects," and economic imperatives to avoid "missing out" on developments. This pressure manifests in organizational imperatives to adopt digital tools regardless of demonstrated professional necessity.

## Main Findings

The introduction identifies several critical observations. First, organizations face mounting "Handlungsdruck" (action pressure) driven by discourse framing digitalization as inevitable and economically necessary, creating competitive pressure to adopt digital tools‚Äîparticularly social media presence‚Äîindependent of professional justification. Second, software selection carries profound professional consequences: case documentation systems literally shape what professionals document and how decisions form, making technological choices fundamentally professional choices rather than neutral infrastructure decisions. Third, long-standing debates remain unresolved; online counseling discussions spanning twenty years still lack definitive answers regarding comparative advantages, appropriate technologies, data security, anonymization, and equity concerns for vulnerable populations. Fourth, emerging phenomena‚Äîdatafication, algorithmic decision-making, and automation in organizational controlling‚Äîrepresent qualitative shifts beyond simple digitization of analog processes. These establish new sociotechnical arrangements with cascading consequences for clients, professionals, and service structures. Finally, vulnerable populations face particular risks regarding digital access, competence requirements, and data security.

## Methodology/Approach

The handbook employs a critical, reflexive theoretical framework that resists both techno-determinism and dismissive rejection. This approach integrates multiple analytical perspectives: examining power dynamics embedded in sociotechnical systems, analyzing organizational and professional implications, attending to client experiences and vulnerable populations' specific challenges, and investigating normative dimensions of technology implementation. The methodology distinguishes between "digital-making" (converting analog processes to digital formats) and genuine digitalization (establishing new sociotechnical arrangements). Rather than treating digital technologies as neutral tools, the framework recognizes them as constitutive elements shaping social work's fundamental operations. The approach combines theoretical analysis with empirical investigation across multiple domains: social media representation, case documentation software, online counseling, and organizational controlling systems.

## Relevant Concepts

**Handlungsdruck (Action Pressure)**: Compulsion to adopt digital technologies driven by discourse of inevitability, competitive positioning, and economic imperatives rather than demonstrated professional necessity.

**Sociotechnical Arrangements**: Systems integrating technology, organizational structures, professional practices, and client interactions as interconnected wholes, where technological choices cascade through entire service delivery systems with normative implications.

**Datafication**: The increasing significance of data collection, analysis, and application (from small datasets to Big Data) in social work, fundamentally transforming professional understanding and intervention.

**Digital-Making vs. Digitalization**: Distinction between converting existing analog processes to digital formats versus establishing entirely new sociotechnical systems with qualitative transformations.

**Algorithmic Decision-Making**: Automated processes determining professional decisions in organizational contexts (controlling, case assessment), raising questions about transparency, accountability, and professional autonomy.

## Significance

This handbook addresses a critical gap in social work scholarship by providing systematic, critical analysis of digitalization's implications. Its significance lies in refusing both uncritical adoption and reactionary rejection, instead demanding rigorous examination of how digital technologies reshape professional practice, organizational governance, and client experiences. The work explicitly critiques the discourse of technological inevitability while acknowledging digitalization's undeniable presence. For social work educators, practitioners, and policymakers, the handbook provides essential frameworks for understanding digitalization as contested terrain requiring careful ethical and professional deliberation rather than inevitable progress. By centering vulnerable populations' experiences, examining power dynamics within sociotechnical systems, and distinguishing between discourse-driven pressure and genuine professional necessity, the work contributes to critical digital studies while advancing social work's commitment to social justice and equity.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.beltz.de/fileadmin/beltz/leseproben/978-3-7799-5258-9.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/C8VLMDBK)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='kutscher_2023_positionings,_challenges,_and_ambivalences_in_chil'></a>

## Paper 121/266: Positionings, challenges, and ambivalences in children's and parents' perspectives in digitalized familial contexts

**Source file:** `Kutscher_2023_Positionings,_challenges,_and_ambivalences_in_chil.md`

---
title: "Positionings, challenges, and ambivalences in children's and parents' perspectives in digitalized familial contexts"
zotero_key: THB3SZPD
author_year: "Kutscher (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: bookSection
language: nan
doi: "nan"
url: "nan"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Positionings, challenges, and ambivalences in children's and parents' perspectives in digitalized familial contexts

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Examines family dynamics in digitalized contexts, analyzing tensions between children's digital participation rights and parental protection responsibilities. Presents research revealing ambivalences in both parental and children's perspectives: parents struggle between enabling children's digital competence and protecting them from risks; children experience tension between desire for autonomy and need for guidance. Addresses sharenting (parents sharing children's images/information online), examining conflicts between parental expression rights and children's privacy rights.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/THB3SZPD)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='kutscher_2024_digitalit√§t_und_digitalisierung_als_gegenstand_der'></a>

## Paper 122/266: Digitalit√§t und Digitalisierung als Gegenstand der Sozialen Arbeit

**Source file:** `Kutscher_2024_Digitalit√§t_und_Digitalisierung_als_Gegenstand_der.md`

---
title: "Digitalit√§t und Digitalisierung als Gegenstand der Sozialen Arbeit"
zotero_key: D66Q6JUR
author_year: "Kutscher (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: bookSection
language: nan
doi: "nan"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 1
rel_prof: 3
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Professional Context", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Digitalit√§t und Digitalisierung als Gegenstand der Sozialen Arbeit

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Professional Context, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Differentiates between digitalization (technical processes of making things digital) and digitality (sociotechnical transformations of social practices and relations), arguing social work must engage with both technological changes and their social implications. Demonstrates how algorithms, data-driven systems, and digital platforms reshape professional practice, client relationships, and social inequalities. Argues digitalization fundamentally transforms social contexts where social work operates rather than merely adopting new tools, requiring critical engagement examining power relations, surveillance mechanisms, and social justice implications.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/D66Q6JUR)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='lahoti_2023_improving_diversity_of_demographic_representation_'></a>

## Paper 123/266: Improving diversity of demographic representation in people entities in Large Language Models

**Source file:** `Lahoti_2023_Improving_diversity_of_demographic_representation_.md`

---
title: "Improving diversity of demographic representation in people entities in Large Language Models"
zotero_key: 9VB7N2YA
author_year: "Lahoti (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "https://aclanthology.org/2023.emnlp-main.643/"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Improving diversity of demographic representation in people entities in Large Language Models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Introduces the Collective-Critique and Self-Voting (CCSV) prompting method to systematically enhance demographic diversity in LLM outputs. The approach leverages LLMs' internal capacity for diversity reasoning and combines critique and self-voting mechanisms to iteratively improve output balance while maintaining model performance.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://aclanthology.org/2023.emnlp-main.643/
- **Zotero:** [Open in Zotero](zotero://select/items/9VB7N2YA)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='laine_2025_avoiding_catastrophe_through_intersectionality_in_'></a>

## Paper 124/266: Avoiding Catastrophe Through Intersectionality in Global AI Governance

**Source file:** `Laine_2025_Avoiding_Catastrophe_Through_Intersectionality_in_.md`

---
title: "Avoiding Catastrophe Through Intersectionality in Global AI Governance"
zotero_key: G2U5EDD3
author_year: "Laine (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://www.cigionline.org/publications/avoiding-catastrophe-through-intersectionality-in-global-ai-governance/"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Avoiding Catastrophe Through Intersectionality in Global AI Governance

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Dieses Working Paper nutzt einen feministischen Policy-Analyse-Rahmen, der auf f√ºnf thematischen Bereichen basiert: Intersektionalit√§t, Kontext, Neutralit√§t, Macht und Gerechtigkeit. Die Forschung schl√§gt einen feministischen KI-Policy-Rahmen vor, der Entscheidungstr√§ger und Stakeholder ermutigt, potenzielle KI-Sicherheitsprojekte in √úbereinstimmung mit vier Zielen zu bewerten: F√∂rderung der Intersektionalit√§t, Bereitstellung diverser Kontexte, Bek√§mpfung der Neutralit√§t und transformative Gerechtigkeit. Der Ansatz w√§chst aus Ans√§tzen zur feministischen KI-Governance, die die Notwendigkeit betonen, KI als Produkt struktureller Ungleichheiten zu sehen.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.cigionline.org/publications/avoiding-catastrophe-through-intersectionality-in-global-ai-governance/
- **Zotero:** [Open in Zotero](zotero://select/items/G2U5EDD3)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='lanzetta_2024_artificial_intelligence_competence_needs_for_youth'></a>

## Paper 125/266: Artificial Intelligence Competence Needs for Youth Workers

**Source file:** `Lanzetta_2024_Artificial_Intelligence_Competence_Needs_for_Youth.md`

---
title: "Artificial Intelligence Competence Needs for Youth Workers"
zotero_key: 9Y3CDP9V
author_year: "Lanzetta (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: en
doi: "10.5281/ZENODO.11525357"
url: "https://zenodo.org/doi/10.5281/zenodo.11525357"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 3
rel_vulnerable: 2
rel_bias: 1
rel_praxis: 2
rel_prof: 3
total_relevance: 11

# Categorization
relevance_category: high
top_dimensions: ["AI Literacy", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-high", "dim-vulnerable-medium", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Artificial Intelligence Competence Needs for Youth Workers

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **11/15** (high) |
| **Top Dimensions** | AI Literacy, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

The rapid developments in AI technology and the rise of accessible AI-powered tools are transforming the way we live, work and learn. While young people have already warmly embraced these solutions, with Gen Z being the most active users and experimenters of Generative AI (Microsoft, 2024), there is a sense of confusion and fear among youth workers about the future of how AI tools are going to be used in the youth sector, mixed with diverse emotions and viewpoints ranging from apprehension, scepticism, resistance to feelings of enthusiasm and recognition of the significance of AI's role in the field (Pawluczuk, 2023). This study aims to advance knowledge on the specific competencies required by youth workers to effectively integrate AI into their professional activities, as well as picture the current and potential use of AI for youth professionals. The publication is part of the Artificial Intelligence for Youth Work (AI4YouthWork)project, a pioneering initiative under the Erasmus+ programme, co-funded by the European Union, dedicated to enhancing the youth sector across Europe through the integration of artificial intelligence (AI). The project unites four organisations - Lasc√≤ from Italy, TEAM4Excellence from Romania, Kyttaro Enallaktikon Anazitiseon Neon from Greece, and Contextos from Portugal -, aspiring to contribute to increasing youth professionals' capacity to harness AI's potential to enhance the quality, attractiveness and effectiveness of their work, and prepare young people to thrive in AI-powered environments. Chapter 1 introduces the project, highlighting the steps and methodological approaches to achieving the main objectives and the expected results. Chapter 2, dedicated to the research methodology, outlines the approach and techniques used to conduct this study. It includes the research design, data collection methods through systematic review, focus groups and interviews, data analysis procedures, as well as limitations and criteria for ensuring the validity and reliability of the findings. Chapter 3 presents the results of the desk research conducted by the consortium partners to explore the intersections of artificial intelligence, youth and youth work. The chapter is divided into four main sections, addressing an introduction to AI, the impact of AI on youth, the role of youth workers in the AI revolution, and practical applications of AI in youth work settings. Chapter 4 outlines the needs, challenges, and tasks involved in integrating AI into youth work, presenting the results of focus group discussions which have been conducted in each partner country. Chapter 5 sets out the publication's conclusions, formulating recommendations for the development of an AI Competence Framework for Youth Workers, and enhancing the capacity of youth professionals to harness AI in their work.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.5281/ZENODO.11525357](https://doi.org/10.5281/ZENODO.11525357)
- **URL:** https://zenodo.org/doi/10.5281/zenodo.11525357
- **Zotero:** [Open in Zotero](zotero://select/items/9Y3CDP9V)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='latif_2023_ai_gender_bias,_disparities,_and_fairness__does_tr'></a>

## Paper 126/266: AI gender bias, disparities, and fairness: Does training data matter?

**Source file:** `Latif_2023_AI_gender_bias,_disparities,_and_fairness__Does_tr.md`

---
title: "AI gender bias, disparities, and fairness: Does training data matter?"
zotero_key: 5KXKHIKC
author_year: "Latif (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/html/2312.10833v2"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 7

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI gender bias, disparities, and fairness: Does training data matter?

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **7/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Empirische Analyse von Geschlechterbias in Bewertungssystemen mit BERT und GPT-3.5. Mixed-gender Trainingsdaten reduzierten Bias, aber verst√§rkten Unterschiede. Drei Bias-Metriken angewendet.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2312.10833v2
- **Zotero:** [Open in Zotero](zotero://select/items/5KXKHIKC)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='latif_2024_ai_gender_bias,_disparities,_and_fairness__does_tr'></a>

## Paper 127/266: AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?

**Source file:** `Latif_2024_AI_Gender_Bias,_Disparities,_and_Fairness__Does_Tr.md`

---
title: "AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?"
zotero_key: LAR4DQJF
author_year: "Latif (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://arxiv.org/html/2312.10833v4"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 7

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **7/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Empirical study examining gender bias in large language models through analysis of over 6000 evaluated student responses from 70 male and 70 female participants. Research uses fine-tuned BERT models and GPT-3.5 to evaluate various training data configurations: gender-specific versus mixed datasets. Three evaluation metrics applied: Scoring Accuracy Difference for bias assessment, Mean Score Gaps (MSG) for gender disparities, and Equalized Odds (EO) for fairness measurement. Results show mixed-gender trained models produce significantly better results than gender-specific models with reduced MSG and fairer predictions.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2312.10833v4
- **Zotero:** [Open in Zotero](zotero://select/items/LAR4DQJF)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='lau_2023_dipper__diversity_in_prompts_for_producing_large_l'></a>

## Paper 128/266: Dipper: Diversity in Prompts for Producing Large Language Model Outputs

**Source file:** `Lau_2023_Dipper__Diversity_in_Prompts_for_Producing_Large_L.md`

---
title: "Dipper: Diversity in Prompts for Producing Large Language Model Outputs"
zotero_key: I4EY5MX4
author_year: "Lau (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "https://www.comp.nus.edu.sg/~greglau/assets/pdf/dipper_neurips_mint.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 3
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Practical Implementation"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-medium", "dim-praxis-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Lau_2023_Dipper.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Dipper: Diversity in Prompts for Producing Large Language Model Outputs

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Practical Implementation |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Presents 'Dipper', an LLM prompting ensemble framework that systematically deploys a diverse set of prompts in parallel to improve the breadth of generated perspectives, including those of minority or marginalized groups. This training-free technique enhances demographic and perspective diversity without performance degradation.


## AI Summary

## Overview

"Dipper" addresses the challenge of improving reasoning performance in resource-constrained environments by proposing a training-free LLM ensemble framework that operates at inference time. Rather than requiring multiple distinct models or relying on limited stochastic variation from repeated identical queries, Dipper generates an optimized, diverse set of prompts executed in parallel against a single LLM model. This approach leverages recent infrastructure advances in batch inference, key-value cache management, and prompt caching to achieve ensemble benefits without training overhead or heterogeneous model requirements. The innovation fundamentally reconceptualizes ensemble diversity by shifting from model space to prompt space, making advanced reasoning capabilities accessible to users facing GPU memory limitations and computational budget constraints.

## Main Findings

Empirical validation on the MATH benchmark demonstrates substantial performance improvements: an ensemble of three small Qwen2-MATH-1.5B models with diverse prompts outperforms a single larger Qwen2-MATH-7B model. This result is significant because it achieves superior reasoning performance while maintaining lower total computational requirements. The findings validate that systematic prompt diversity‚Äîdistinct from random stochastic sampling of identical prompts‚Äîeffectively injects the variation necessary for ensemble methods to function optimally. Critically, the research demonstrates that parallel execution through batch inference maintains computational efficiency, achieving sub-linear cost scaling relative to query numbers. This contrasts sharply with sequential reasoning methods (Chain-of-Thought, Reflexion) that require multiple sequential queries and cannot leverage parallel processing infrastructure.

## Methodology/Approach

Dipper implements a three-stage framework: (1) generating an optimized, diverse set of prompts designed to invoke different reasoning pathways and problem-solving strategies, (2) executing these prompts in parallel against a single LLM model using batch inference with efficient KV-cache and prompt caching mechanisms, and (3) aggregating ensemble outputs through combination methods to produce final predictions. The approach is training-free, requiring no model fine-tuning. Prompt diversity is systematically designed rather than randomly sampled, distinguishing it from self-ensembles relying on response stochasticity. The framework exploits modern LLM infrastructure to achieve practical efficiency gains.

## Relevant Concepts

**Ensemble Methods:** Multiple constituent models combined to improve prediction accuracy and robustness through aggregation mechanisms.

**Prompt Diversity:** Systematic variation in instruction formulation, reasoning pathway invocation, and problem-solving strategy framing to generate diverse outputs from identical models.

**Stochastic Self-Ensembles:** Ensembles created by sampling identical prompts multiple times, relying on inherent LLM response randomness‚Äîlimited in diversity compared to prompt-diverse ensembles.

**Inference-time Optimization:** Performance enhancement techniques applied during deployment without retraining.

**Batch Inference:** Parallel processing of multiple queries simultaneously, reducing per-query computational overhead.

**KV-Cache Management:** Efficient storage and reuse of key-value cache memory across parallel queries.

**Prompt Caching:** Mechanism to efficiently reuse common prompt components across multiple queries, enabling sub-linear cost scaling.

**Chain-of-Thought (CoT):** Sequential reasoning technique requiring multiple sequential queries to progressively develop solutions.

## Significance

This work makes four critical contributions. First, it advances inference-time optimization beyond sequential methods, enabling parallel processing advantages for reasoning tasks. Second, it democratizes ensemble benefits by eliminating heterogeneous model requirements, making advanced reasoning accessible to resource-constrained practitioners. Third, it bridges classical ensemble theory with contemporary LLM capabilities through prompt-space diversity rather than model-space heterogeneity. Fourth, by demonstrating that smaller model ensembles outperform larger individual models, Dipper challenges computational assumptions about reasoning capability requirements, potentially reshaping organizational LLM deployment strategies. The training-free nature and compatibility with modern batch inference infrastructure make this approach immediately practical for real-world deployment.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.comp.nus.edu.sg/~greglau/assets/pdf/dipper_neurips_mint.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/I4EY5MX4)

## Related Concepts

- Ensemble Methods
- Prompt Diversity
- Stochastic Self-Ensembles
- Inference-time Optimization
- Batch Inference
- KV-Cache Management
- Prompt Caching
- Chain-of-Thought (CoT)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='laupichler_2023_development_of_the_‚Äúscale_for_the_assessment_of_no'></a>

## Paper 129/266: Development of the ‚ÄúScale for the assessment of non-experts‚Äô AI literacy‚Äù ‚Äì An exploratory factor analysis

**Source file:** `Laupichler_2023_Development_of_the_‚ÄúScale_for_the_assessment_of_no.md`

---
title: "Development of the ‚ÄúScale for the assessment of non-experts‚Äô AI literacy‚Äù ‚Äì An exploratory factor analysis"
zotero_key: ZHG8756W
author_year: "Laupichler (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: en
doi: "10.1016/j.chbr.2023.100338"
url: "https://linkinghub.elsevier.com/retrieve/pii/S2451958823000714"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Development of the ‚ÄúScale for the assessment of non-experts‚Äô AI literacy‚Äù ‚Äì An exploratory factor analysis

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1016/j.chbr.2023.100338](https://doi.org/10.1016/j.chbr.2023.100338)
- **URL:** https://linkinghub.elsevier.com/retrieve/pii/S2451958823000714
- **Zotero:** [Open in Zotero](zotero://select/items/ZHG8756W)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='li_2023_ethics_&_ai__a_systematic_review_on_ethical_concer'></a>

## Paper 130/266: Ethics & AI: A systematic review on ethical concerns and related strategies for designing with AI in healthcare

**Source file:** `Li_2023_Ethics_&_AI__A_systematic_review_on_ethical_concer.md`

---
title: "Ethics & AI: A systematic review on ethical concerns and related strategies for designing with AI in healthcare"
zotero_key: A592677H
author_year: "Li (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "10.3390/ai4010003"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 2
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium", "dim-prof-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Ethics & AI: A systematic review on ethical concerns and related strategies for designing with AI in healthcare

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

Systematic literature review (2010-2020) identifying 12 main ethical issues in AI healthcare applications with direct relevance to social services. Critical value tensions identified include justice and fairness (algorithmic bias causing discrimination), freedom and autonomy (control, respecting human autonomy, and informed consent challenges), privacy violations through data-driven systems, transparency conflicts with black-box algorithms, dignity concerns when AI reduces persons to data, and conflicts in decision-making between AI and human judgment. Review analyzed 45 documents and identified 19 ethical sub-issues, providing strategies for each. Emphasizes human-centered approach respecting fundamental rights and European values.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.3390/ai4010003](https://doi.org/10.3390/ai4010003)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/A592677H)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='lin_2022_artificial_intelligence_in_a_structurally_unjust_s'></a>

## Paper 131/266: Artificial Intelligence in a Structurally Unjust Society

**Source file:** `Lin_2022_Artificial_Intelligence_in_a_Structurally_Unjust_S.md`

---
title: "Artificial Intelligence in a Structurally Unjust Society"
zotero_key: WC6JF3VD
author_year: "Lin (2022)"
authors: []

# Publication
publication_year: 2022.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://ojs.lib.uwo.ca/index.php/fpq/article/download/14191/12139/38443"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Lin_2022_Artificial.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Artificial Intelligence in a Structurally Unjust Society

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2022.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This study argues that AI bias represents a form of structural injustice that arises when AI systems interact with other social factors to exacerbate existing social inequalities. Using the example of AI in healthcare, the authors show that the goal of AI fairness is to strive for a more just social structure through appropriate development and use of AI systems. They argue that all actors involved in the unjust social structure bear shared responsibility to join collective actions for structural reform and offer practical recommendations for various social positions.


## AI Summary

## Overview

This paper by Lin and Chen presents a philosophical critique of mainstream AI fairness approaches by reframing AI bias as **structural injustice** rather than a technical problem. The authors argue that contemporary efforts‚Äîwhich pursue statistical parity and algorithmic debiasing‚Äîfundamentally misdiagnose the problem and cannot adequately address its ethical dimensions. They propose that AI bias emerges when AI systems interact with existing social inequalities to amplify disadvantages for certain groups while conferring unearned benefits to others. The paper bridges computer science ethics with social philosophy, establishing that meaningful progress requires **collective action targeting systemic social reform** rather than isolated technical interventions. Healthcare applications serve as the primary case study demonstrating this structural dynamic.

## Main Findings

The authors establish three critical findings: (1) **AI bias is structural**, not algorithmic‚Äîit results from AI systems operating within and reinforcing unjust social structures rather than from defective code; (2) **The dominant fairness paradigm is inadequate** because it mislocates the problem within algorithms, pursues disconnected statistical metrics, and obscures the distributed responsibility necessary for reform; (3) **Shared responsibility is essential**‚Äîall participating agents (developers, deployers, policymakers, affected communities, and institutions) bear moral obligation to contribute collective action according to their social position. The paper explicitly defines the goal of AI fairness as pursuing "a more just social structure with the development and use of AI systems when appropriate." Critically, the authors distinguish between two injustice mechanisms: undeserved burdens imposed on marginalized groups and unearned benefits conferred on privileged groups. Healthcare examples illustrate how technical fixes cannot overcome systemic injustices embedded in medical institutions, data practices, and resource allocation.

## Methodology/Approach

The paper employs **philosophical conceptual analysis** grounded in social justice theory rather than empirical research. The methodology combines: (1) theoretical reconceptualization of documented AI bias cases (recruiting algorithms discriminating against women, recidivism prediction systems targeting Black defendants, search engine stereotyping of women of color), (2) normative ethical reasoning about justice and responsibility distribution, and (3) case study analysis of healthcare AI applications. This approach prioritizes conceptual clarity and theoretical coherence, reflecting the authors' position that the fundamental problem is one of **problem-framing and understanding** rather than measurement or technical optimization.

## Relevant Concepts

**Structural Injustice:** Systemic disadvantage produced through interaction of multiple social institutions and practices, not reducible to individual wrongdoing or discrete policy failures.

**AI Bias (redefined):** The reproduction and amplification of existing social inequalities through AI systems interacting with unjust social structures, creating both undeserved burdens and unearned benefits.

**Shared Responsibility:** Distributed moral obligation across all participating agents in an unjust structure to contribute to collective reform efforts, differentiated by social position.

**Social Structure (in AI context):** The interconnected systems of institutions, practices, and social factors that AI systems operate within and reinforce.

**Technical-Fix Approach (critique):** The dominant paradigm treating AI fairness as achievable through algorithmic debiasing and statistical parity measures, which the authors argue is fundamentally insufficient.

## Significance

This work significantly advances **critical AI studies** by establishing that AI fairness cannot be achieved through engineering alone‚Äîit requires structural social change. The theoretical contribution clarifies why statistical parity measures fail to produce justice: they address symptoms rather than causes. The practical significance emerges through the authors' differentiated responsibility framework, offering guidance for diverse stakeholders to contribute according to their position. By grounding AI ethics in social philosophy, the paper challenges techno-optimistic assumptions and creates space for comprehensive policy approaches. The reframing has substantial implications: organizations pursuing AI fairness must recognize that technical solutions to structural problems represent a category error. This work establishes that meaningful AI fairness requires simultaneous attention to algorithmic design, institutional reform, policy change, and collective action‚Äîmaking it foundational for emerging critical perspectives in AI ethics and governance.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://ojs.lib.uwo.ca/index.php/fpq/article/download/14191/12139/38443
- **Zotero:** [Open in Zotero](zotero://select/items/WC6JF3VD)

## Related Concepts

- Structural Injustice
- AI Bias (redefined)
- Shared Responsibility
- Social Structure (in AI context)
- Technical-Fix Approach (critique)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='lin_2024_swiftsage__a_new_dual-module_framework_for_better_'></a>

## Paper 132/266: SWIFTSAGE: A new dual-module framework for better action planning in complex interactive reasoning tasks

**Source file:** `Lin_2024_SWIFTSAGE__A_new_dual-module_framework_for_better_.md`

---
title: "SWIFTSAGE: A new dual-module framework for better action planning in complex interactive reasoning tasks"
zotero_key: E5BB97AY
author_year: "Lin (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/html/2404.17218v3"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# SWIFTSAGE: A new dual-module framework for better action planning in complex interactive reasoning tasks

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2404.17218v3
- **Zotero:** [Open in Zotero](zotero://select/items/E5BB97AY)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='linnemann_2023_bedeutung_von_k√ºnstlicher_intelligenz_in_der_sozia'></a>

## Paper 133/266: Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit

**Source file:** `Linnemann_2023_Bedeutung_von_K√ºnstlicher_Intelligenz_in_der_Sozia.md`

---
title: "Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit"
zotero_key: 678XS28X
author_year: "Linnemann (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: de
doi: "10.1007/s12592-023-00455-7"
url: "https://doi.org/10.1007/s12592-023-00455-7"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 2
rel_bias: 1
rel_praxis: 1
rel_prof: 3
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Professional Context", "AI Literacy"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-ai-komp-medium", "dim-vulnerable-medium", "dim-prof-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Linnemann_2023_Bedeutung.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Bedeutung von K√ºnstlicher Intelligenz in der Sozialen Arbeit

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Professional Context, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Die Bedeutung des Einsatzes von Verfahren, die unter dem Begriff der K√ºnstlichen Intelligenz (KI) zusammenzufassen sind, wird sowohl f√ºr gesellschaftliche Prozesse als auch den Auftrag an die Soziale Arbeit zunehmend erkannt und diskutiert. Mit diesem Artikel wird ein Beitrag zum Diskurs geleistet, indem vertieft der Bereich der Sprachverarbeitung durch KI, das Natural Language Processing (NLP), in den Blick genommen wird. Verarbeitung nat√ºrlicher Sprache ist aufgrund der hohen Bedeutung kommunikativer Prozesse f√ºr die Praxis der Sozialen Arbeit von besonderer Relevanz, zugleich wird die Profession der Sozialen Arbeit tangiert. Bezugnehmend auf Staub-Bernasconis Handlungstheorie werden Implikationen und Diskussionspunkte von NLP identifiziert und diskutiert. Zudem werden m√∂gliche Gratifikationen f√ºr Klient*innen herausgearbeitet, die sich u. a. aus der Wirkung und sozialen Interaktion ergeben. Hier wird die Media-Equation-Theorie von Nass und Reeves als Erkenntnisfolie herangezogen. Vor diesen Perspektiven ergeben sich sowohl Risiken (u. a. die Gefahr einer modularisierten Herausl√∂sung genuin sozialarbeiterischer T√§tigkeit) als auch Chancen (u. a. Teilhabe, niederschwelliger Zugang, Zugriff auf breitere Datenbasis).


## AI Summary

## Overview

This German-language academic forum article by Linnemann, L√∂he, and Rottkemper (published June 2023, accepted May 2023) addresses the growing importance of Artificial Intelligence in social work, with particular emphasis on Natural Language Processing (NLP) technologies. Published across multiple institutional affiliations (Katholische Hochschule NRW, FH M√ºnster), the work contributes to emerging scholarly discourse examining how AI applications‚Äîparticularly ChatGPT and similar NLP systems‚Äîintersect with social work practice, professional identity, and client outcomes. The authors argue that NLP deserves focused attention within social work scholarship because language processing is fundamental to communicative processes that define social work practice. Rather than adopting either techno-optimistic or pessimistic positions, the article pursues a theoretically-grounded critical analysis that acknowledges both opportunities and significant risks associated with AI integration into social services across multiple practice domains including child/youth care, elderly care, and counseling services.

## Main Findings

The article identifies a dual-sided landscape of implications for social work across specific practice domains. Regarding child/youth care, elderly care, and counseling services, NLP technologies offer substantial opportunities: they can democratize access to social services by reducing barriers for marginalized populations, enable low-threshold entry points for vulnerable clients, and facilitate decision-making through access to broader data bases. These benefits align with social work's fundamental commitment to social justice and inclusion. Conversely, the authors identify critical concerns centered on modularization‚Äîthe fragmentation and extraction of genuine social work activities (relational engagement, ethical judgment, holistic assessment) into discrete algorithmic components. This risk threatens to reduce relationship-centered practice into mechanistic processes, potentially compromising the profession's human rights orientation and ethical foundations. The analysis reveals that NLP's ultimate value depends entirely on whether implementation maintains social work's core professional values or allows technological determinism to reshape practice fundamentally. ChatGPT and similar systems exemplify this tension: while offering accessibility, they risk replacing professional judgment with algorithmic responses.

## Methodology/Approach

The article employs a conceptual-analytical methodology rather than empirical research design, systematically examining theoretical implications across practice domains. Two primary theoretical frameworks structure the analysis. Staub-Bernasconi's Action Theory (2018) provides the foundational lens for examining how NLP aligns with or diverges from social work as a science of action, ensuring that technological applications are evaluated against established professional theory and values. Media Equation Theory (Nass & Reeves) complements this framework by analyzing how clients perceive and interact with NLP systems, examining psychological gratification mechanisms and social interaction effects. This dual-theoretical approach enables systematic examination of implications across different social work fields, moving beyond abstract technological discussion toward concrete professional practice considerations.

## Relevant Concepts

**Natural Language Processing (NLP)**: AI technology enabling computers to understand, interpret, and generate human language; applicable to counseling, assessment, and service delivery in social work contexts; exemplified by ChatGPT systems.

**Staub-Bernasconi's Action Theory**: Theoretical framework conceptualizing social work as a science of action grounded in professional values; serves as evaluative standard for assessing whether technological interventions maintain professional integrity and alignment with social work's fundamental mission.

**Modularization**: The fragmentation of holistic social work practice into discrete, algorithmic components; specifically the extraction of genuine social work activities (relational engagement, ethical judgment, contextual assessment) into automated processes, risking loss of professional integrity and relational dimensions.

**Media Equation Theory**: Framework explaining how humans attribute social qualities to technology and experience gratification through technological interaction; relevant for understanding client-AI interactions in social services and potential psychological effects.

**Human Rights Profession**: Conceptualization of social work as fundamentally oriented toward human rights protection, social justice, and client dignity; serves as evaluative standard for AI integration decisions.

**Genuine Social Work Activities**: Professional practices requiring human judgment, relational engagement, ethical discernment, and contextual understanding; distinguished from algorithmic or modularized functions.

## Significance

This article's significance lies in its contribution to critical, theoretically-informed discourse on AI in social professions during a period of rapid ChatGPT and NLP adoption. By grounding analysis in established social work theory (Staub-Bernasconi) rather than technological innovation alone, the authors model responsible scholarly engagement with emerging technologies. The work addresses a crucial gap: while AI adoption accelerates across social service sectors, rigorous professional analysis of implications remains limited. The forum format positions this as discussion-initiating scholarship, inviting further disciplinary engagement with specific practice domains. The article's balanced approach‚Äîacknowledging genuine opportunities while articulating serious professional risks regarding modularization and loss of relational practice‚Äîprovides a template for responsible technology assessment in helping professions. Its timeliness (2023 publication) captures the critical moment when ChatGPT and similar systems entered social service discourse, making it essential for practitioners and scholars navigating AI integration decisions.


## Links & Resources

- **DOI:** [10.1007/s12592-023-00455-7](https://doi.org/10.1007/s12592-023-00455-7)
- **URL:** https://doi.org/10.1007/s12592-023-00455-7
- **Zotero:** [Open in Zotero](zotero://select/items/678XS28X)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='linnemann_2025_k√ºnstliche_intelligenz_in_der_sozialen_arbeit__gru'></a>

## Paper 134/266: K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen f√ºr Theorie und Praxis

**Source file:** `Linnemann_2025_K√ºnstliche_Intelligenz_in_der_Sozialen_Arbeit__Gru.md`

---
title: "K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen f√ºr Theorie und Praxis"
zotero_key: 7YWKF2UF
author_year: "Linnemann (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: book
language: nan
doi: "nan"
url: "https://doi.org/10.3262/978-3-7799-8562-4"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 3
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 2
rel_prof: 3
total_relevance: 12

# Categorization
relevance_category: high
top_dimensions: ["AI Literacy", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-high", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen f√ºr Theorie und Praxis

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **12/15** (high) |
| **Top Dimensions** | AI Literacy, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

First major German-language systematic treatment of AI in social work from multiple perspectives. Bridges technological progress and ethics, treating AI theoretically and in practice-oriented applications. Addresses technical AI basics for social work, ethical and legal frameworks, bias and discrimination in training data, automation bias risks, development of AI competencies in education and organizations, and specific application fields. Emphasizes responsible, reflective engagement with AI enriching social work without losing sight of human integrity and professional responsibility.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://doi.org/10.3262/978-3-7799-8562-4
- **Zotero:** [Open in Zotero](zotero://select/items/7YWKF2UF)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='liu_2025_more_or_less_wrong__a_benchmark_for_directional_bi'></a>

## Paper 135/266: More or less wrong: A benchmark for directional bias in LLM comparative reasoning

**Source file:** `Liu_2025_More_or_less_wrong__A_benchmark_for_directional_bi.md`

---
title: "More or less wrong: A benchmark for directional bias in LLM comparative reasoning"
zotero_key: MP5QFDGI
author_year: "Liu (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/html/2506.03923v1"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# More or less wrong: A benchmark for directional bias in LLM comparative reasoning

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2506.03923v1
- **Zotero:** [Open in Zotero](zotero://select/items/MP5QFDGI)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='long_2020_what_is_ai_literacy__competencies_and_design_consi'></a>

## Paper 136/266: What is AI Literacy? Competencies and Design Considerations

**Source file:** `Long_2020_What_is_AI_Literacy__Competencies_and_Design_Consi.md`

---
title: "What is AI Literacy? Competencies and Design Considerations"
zotero_key: BFK6D38S
author_year: "Long (2020)"
authors: []

# Publication
publication_year: 2020.0
item_type: conferencePaper
language: en
doi: "10.1145/3313831.3376727"
url: "https://dl.acm.org/doi/10.1145/3313831.3376727"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# What is AI Literacy? Competencies and Design Considerations

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2020.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1145/3313831.3376727](https://doi.org/10.1145/3313831.3376727)
- **URL:** https://dl.acm.org/doi/10.1145/3313831.3376727
- **Zotero:** [Open in Zotero](zotero://select/items/BFK6D38S)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='lund_2025_algorithms,_artificial_intelligence_and_discrimina'></a>

## Paper 137/266: Algorithms, artificial intelligence and discrimination

**Source file:** `Lund_2025_Algorithms,_artificial_intelligence_and_discrimina.md`

---
title: "Algorithms, artificial intelligence and discrimination"
zotero_key: GN5D9A7D
author_year: "Lund (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://ldo.no/content/uploads/2025/05/Algorithms-artificial-intelligence-and-discrimination-report.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Lund_2025_Algorithms.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Algorithms, artificial intelligence and discrimination

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Dieser norwegische Regierungsbericht √ºberpr√ºft Schl√ºsselelemente des norwegischen Gleichstellungs- und Antidiskriminierungsgesetzes mit prim√§rem Fokus auf algorithmische Diskriminierung. Der Bericht diskutiert die m√∂gliche Einf√ºhrung spezifischer Definitionen direkter und indirekter algorithmischer Diskriminierung und schl√§gt die Schaffung einer spezifischen Bestimmung zu rechtm√§√üiger algorithmischer Differenzialbehandlung vor. Die Komplexit√§t algorithmischer Systeme erschwert die Unterscheidung zwischen direkter und indirekter Diskriminierung, was neue rechtliche Ans√§tze erfordert.


## AI Summary

## Overview

This Norwegian legal report, authored by Professor Vibeke Blaker Strand (University of Oslo) and published by the Equality and Anti-Discrimination Ombud (LDO) in 2024, examines whether existing equality and anti-discrimination legislation adequately protects individuals from discriminatory outcomes produced by algorithmic systems and artificial intelligence. The document addresses a critical gap in contemporary legal scholarship by integrating three distinct regulatory domains: Norwegian equality law, the EU's General Data Protection Regulation (GDPR), and the emerging EU AI Act. Originally published in Norwegian, the English translation reflects the author's active involvement in ensuring accuracy and currency. The report's central premise is that algorithmic discrimination cannot be adequately addressed through isolated legal frameworks, requiring instead coordinated analysis across multiple regulatory instruments.

## Main Findings

The analysis reveals that traditional anti-discrimination frameworks possess significant but incomplete capacity to regulate algorithmic systems. Specifically: (1) Norwegian equality and anti-discrimination laws offer partial protection against algorithmic harm but contain substantial gaps in detecting, proving, and remedying algorithmic discrimination; (2) The AI Act, GDPR, and equality law create both regulatory opportunities and dangerous lacunae when applied to algorithmic decision-making; (3) These frameworks were designed for human decision-making contexts, creating interpretive challenges when applied to algorithmic systems characterized by opacity, complexity, and scale; (4) Informational privacy protection and anti-discrimination objectives are deeply interconnected, requiring integrated legal analysis; (5) Current legal instruments lack adequate mechanisms for addressing causation difficulties inherent in algorithmic discrimination cases. The report concludes that existing legal frameworks require substantial adaptation to address the unique challenges posed by AI-driven discrimination, particularly regarding burden of proof and remedial mechanisms.

## Methodology/Approach

The author employs rigorous doctrinal legal methodology, systematically examining statutory provisions and their interpretive scope across multiple jurisdictions and regulatory instruments. The approach combines comparative legal analysis, evaluating Norwegian equality law against EU regulatory frameworks (AI Act, GDPR) to identify convergences, divergences, and complementarities. The institutional perspective of the Equality and Anti-Discrimination Ombud provides authoritative grounding and practical enforcement context. Rather than treating discrimination law, data protection, and AI regulation as separate domains, the methodology integrates these fields, examining their interconnections and mutual implications. This integrated theoretical framework represents a sophisticated departure from siloed legal analysis, recognizing that algorithmic discrimination requires multidimensional legal understanding and coordinated regulatory application.

## Relevant Concepts

**Algorithmic Discrimination**: Discriminatory outcomes produced by automated decision-making systems, distinct from intentional human discrimination and presenting novel evidentiary challenges regarding causation and intent.

**Algorithmic Opacity**: The technical and interpretive difficulty in understanding how algorithmic systems reach specific decisions, creating barriers to legal proof of discrimination.

**Informational Privacy**: Protection of personal data and information autonomy, increasingly critical as AI systems process vast datasets to generate predictions and decisions affecting fundamental rights.

**Regulatory Gap**: Spaces where existing legal frameworks fail to address emerging technological harms, particularly regarding algorithmic opacity, causation difficulties, and burden of proof.

**Integrated Regulation**: Coordinated application of multiple legal instruments (equality law, GDPR, AI Act) to comprehensively address algorithmic discrimination rather than siloed regulatory approaches.

**Doctrinal Legal Methodology**: Systematic examination of statutory provisions, case law, and legal principles to determine their scope, application, and interpretive possibilities.

## Significance

This report holds substantial significance for multiple audiences. For legal scholars, it contributes to emerging discourse on law and AI governance, specifically discrimination-focused analysis within European regulatory contexts. For policymakers and enforcement bodies, it provides institutional authority and practical guidance on regulatory adequacy and necessary legal reforms. The transnational relevance‚Äîaddressing EU-level regulations (GDPR, AI Act) applicable across member states‚Äîextends its impact beyond Norway, informing broader European legal development. The work identifies critical gaps between technological reality and legal capacity, establishing that algorithmic discrimination represents a qualitatively different challenge requiring adaptive legal frameworks, enhanced evidentiary mechanisms, and coordinated regulatory approaches. By bridging legal scholarship with practical regulatory concerns, the report advances understanding of how contemporary legal systems must evolve to protect fundamental equality rights in algorithmic societies, particularly regarding burden of proof, causation standards, and remedial mechanisms.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://ldo.no/content/uploads/2025/05/Algorithms-artificial-intelligence-and-discrimination-report.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/GN5D9A7D)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='ma_2023_intersectional_stereotypes_in_large_language_model'></a>

## Paper 138/266: Intersectional Stereotypes in Large Language Models: Dataset and Analysis

**Source file:** `Ma_2023_Intersectional_Stereotypes_in_Large_Language_Model.md`

---
title: "Intersectional Stereotypes in Large Language Models: Dataset and Analysis"
zotero_key: 9YEC6SQ7
author_year: "Ma (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: conferencePaper
language: nan
doi: "10.18653/v1/2023.findings-emnlp.575"
url: "https://aclanthology.org/2023.findings-emnlp.575.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Ma_2023_Intersectional.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Intersectional Stereotypes in Large Language Models: Dataset and Analysis

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This EMNLP paper introduces a dataset for studying intersectional stereotypes and applies it to three LLMs. Results reveal emergent stereotypes not predictable from single-attribute analysis. Prompt engineering reduces but does not eliminate such patterns, highlighting persistent biases in generated narratives.


## AI Summary

## Overview

This research addresses a critical gap in AI bias scholarship by systematically investigating how Large Language Models propagate stereotypes targeting intersectional demographic groups‚Äîindividuals defined by multiple overlapping identity categories simultaneously. While extensive prior research has examined single-category biases (racial bias, gender bias) or simplistic two-attribute intersections, this study pioneers comprehensive intersectional analysis. The authors recognize that real-world stereotypes frequently target complex intersectional identities (e.g., elderly Muslim women, disabled Asian conservatives) that cannot be adequately understood through reductionist frameworks. By curating a novel dataset of intersectional stereotypes across 14 demographic features spanning 6 categories and analyzing three contemporary LLMs' responses across 16 stereotype categories, the paper establishes intersectional bias as a critical but underexplored dimension of AI ethics requiring urgent attention.

## Main Findings

The research reveals several critical insights about LLM behavior. ChatGPT demonstrates capacity to generate authentic, human-validated stereotypes for intersectional groups comprising up to four demographic attributes, suggesting sophisticated understanding of complex social stereotypes. However, performance deteriorates significantly when demographic combinations exceed four attributes, resulting in overgeneralization and reduced stereotype authenticity‚Äîrequiring post-hoc filtering to maintain dataset quality. Crucially, all three LLMs studied produce stereotypical responses across diverse intersectional groups, confirming that stereotype propagation is systematic rather than isolated. The study demonstrates that stereotypes manifest across 16 distinct categories, indicating multifaceted bias. Importantly, the research reveals that context-dependent stereotypes‚Äîcovert biases embedded in contextual narratives rather than isolated words‚Äîare prevalent, extending beyond prior word-level analyses. These findings underscore that existing debiasing efforts targeting single categories remain insufficient.

## Methodology/Approach

The research employs an innovative mixed-methods framework combining computational and human validation. Dataset construction leverages ChatGPT itself to generate intersectional stereotypes across 14 demographic features: race (3 categories: white, black, Asian), age (2: young, old), religion (3: nonreligious, Christian, Muslim), gender (2: men, women), political orientation (2: conservative, progressive), and disability status (2: with, without). This permutational approach enables systematic exploration of diverse group combinations. Critically, the authors implement rigorous dual-validation mechanisms‚Äîboth ChatGPT-assisted and human validation‚Äîwith post-hoc filtering to mitigate algorithmic overgeneralization, particularly for 5+ attribute combinations. The empirical testing phase probes three contemporary LLMs using interrogation methodology adapted from prior research, analyzing responses across 16 stereotype categories while identifying patterns, severity, and context-dependency.

## Relevant Concepts

**Intersectionality**: Framework recognizing that individuals possess multiple, interconnected identity dimensions whose combined effects cannot be understood through isolated analysis.

**Stereotype propagation**: Process by which language models reproduce and amplify societal stereotypes through training data and learned patterns.

**Context-dependent stereotypes**: Covert biases embedded within contextual narratives and discourse rather than manifesting as isolated stereotypical words or phrases.

**Reductionism in bias research**: Problematic tendency to examine demographic categories independently, obscuring how biases compound at intersections.

**Dual validation with post-hoc filtering**: Methodological strategy employing both algorithmic and human assessment, followed by iterative refinement to enhance reliability and mitigate overgeneralization.

**Performance degradation threshold**: The empirically-identified limitation where LLM stereotype generation quality declines significantly beyond four intersecting demographic attributes.

## Significance

This work substantially advances AI ethics scholarship by establishing intersectional bias as a priority research area with quantifiable parameters (4-attribute threshold, 16 stereotype categories). The reusable intersectional stereotype dataset provides infrastructure for future investigations. Methodologically, the reflexive approach‚Äîusing LLMs to study LLMs‚Äîdemonstrates innovative research design while establishing LLMs' utility for bias research itself. The identification of context-dependent stereotypes extends understanding beyond word-level analysis. Most importantly, the findings demand immediate attention from AI developers and policymakers, establishing that comprehensive debiasing requires intersectional frameworks rather than single-category approaches. This research catalyzes paradigm shift toward sophisticated, nuanced understanding of algorithmic bias with practical implementation constraints.


## Links & Resources

- **DOI:** [10.18653/v1/2023.findings-emnlp.575](https://doi.org/10.18653/v1/2023.findings-emnlp.575)
- **URL:** https://aclanthology.org/2023.findings-emnlp.575.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/9YEC6SQ7)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='marjanovic_2022_theorising_algorithmic_justice'></a>

## Paper 139/266: Theorising algorithmic justice

**Source file:** `Marjanovic_2022_Theorising_algorithmic_justice.md`

---
title: "Theorising algorithmic justice"
zotero_key: MJTUVCJ2
author_year: "Marjanovic (2022)"
authors: []

# Publication
publication_year: 2022.0
item_type: journalArticle
language: nan
doi: "10.1080/0960085X.2021.1934130"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 2
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-prof-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Theorising algorithmic justice

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2022.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

Theoretical article developing framework for understanding algorithmic justice in automated decision-making systems with specific application to social welfare services. Authors examine how AI-driven automated algorithmic decision-making threatens core social justice principles in transformative services like welfare. Framework addresses WHAT matters in algorithmic justice (fairness, equity, human rights), WHO counts as subjects (vulnerable populations disproportionately affected), and HOW algorithmic justice is performed (through transparent, accountable, participatory processes). Key ethical tensions identified include datafication reducing humans to data points violating dignity; technological inscribing embedding biases perpetuating discrimination; systemic nature of injustices compounding disadvantage for marginalized groups; and loss of human judgment and participation.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1080/0960085X.2021.1934130](https://doi.org/10.1080/0960085X.2021.1934130)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/MJTUVCJ2)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='mccrory_2024_avoiding_catastrophe_through_intersectionality_in_'></a>

## Paper 140/266: Avoiding catastrophe through intersectionality in global AI governance

**Source file:** `McCrory_2024_Avoiding_catastrophe_through_intersectionality_in_.md`

---
title: "Avoiding catastrophe through intersectionality in global AI governance"
zotero_key: NMD5P5LN
author_year: "McCrory (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://www.cigionline.org/documents/3375/DPH-paper-Laine_McCrory.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_McCrory_2024_Avoiding.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Avoiding catastrophe through intersectionality in global AI governance

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

In this working paper, McCrory argues that prevailing global AI governance efforts (especially those focused on ‚ÄúAI safety‚Äù and existential risks) lack an adequate intersectional, feminist perspective, which is needed to avert not just hypothetical future catastrophes but ongoing injustices. The paper analyzes seven prominent international AI policy initiatives (e.g. global AI accords and principles) against criteria derived from feminist theory, such as intersectionality, context, and power dynamics. McCrory finds that these high-level policies often remain techno-centric: they invoke abstract risks or neutrality, but fail to engage with how AI harms are unevenly distributed along lines of gender, race, and class. For example, current AI ‚Äúsafety‚Äù pledges seldom consider the lived experiences of marginalized communities or the way existing structural inequalities are mirrored in AI systems. The author contends that treating AI governance as a purely technical, top-down process is misguided; instead, governance should include meaningful participation from under-represented groups and incorporate feminist insights about power and oppression. The paper‚Äôs recommendations call for centering intersectionality in AI policy: explicitly addressing how AI-related risks and harms intersect with social identity and historical injustices, and ensuring that any frameworks for AI risk management or ethics actively involve those who have been marginalized by past technological developments.


## AI Summary

## Overview

This working paper from CIGI's Digital Policy Hub, supported by Mitacs partnership, addresses a critical gap in artificial intelligence governance by integrating feminist intersectional analysis into AI safety discourse. The research, authored by Laine McCrory, challenges the predominant AI safety movement‚Äîwhich focuses on existential risks and system alignment with human values‚Äîfor treating these risks as universally impactful while overlooking how marginalized communities already experience disproportionate harms from current AI systems. The paper argues that meaningful AI governance requires moving beyond technocratic approaches that claim neutrality, instead recognizing how future AI risks are fundamentally interconnected with existing structural inequalities and power dynamics. By applying feminist policy analysis frameworks organized around five thematic dimensions (intersectionality, context, neutrality, control, and power), the work bridges traditionally separate scholarly domains to propose more equitable governance pathways.

## Main Findings

The analysis reveals five systematic deficiencies in current global AI safety initiatives:

1. **Limited feminist engagement:** AI safety policies demonstrate insufficient meaningful engagement with feminist principles and accountability mechanisms.

2. **Disconnected temporality:** Governance frameworks fail to establish explicit connections between hypothetical future existential risks and observable present-day harms already experienced by marginalized communities.

3. **Unacknowledged differential impacts:** Current AI systems replicate existing social biases and power hierarchies, yet policy frameworks inadequately recognize how marginalized groups face disproportionate existential risks from these systems.

4. **Exclusionary participation:** Affected communities lack meaningful participation mechanisms in policy development processes, limiting governance legitimacy and effectiveness.

5. **False universalism:** AI safety discourse operates with implicit universalism‚Äîassuming risks affect all populations equally‚Äîthereby obscuring how current technological harms are distributed unequally across social groups.

The research demonstrates that treating future AI risks as separate from current inequities prevents governance frameworks from achieving genuine accountability or addressing root causes of technological harm.

## Methodology/Approach

The paper employs feminist policy analysis as its primary analytical framework, systematically examining AI safety governance initiatives through five thematic dimensions. **Intersectionality** interrogates how multiple overlapping social identities create compounded disadvantages. **Context** examines how policies reflect specific historical and social circumstances. **Neutrality** questions claims of objectivity in governance. **Control** analyzes who holds decision-making power. **Power** investigates how policies reinforce or challenge existing hierarchies. This multidimensional approach enables evaluation of how initiatives address structural inequalities and whose voices shape policy development. The framework moves beyond surface-level critique to interrogate underlying assumptions about risk, universality, and governance legitimacy, revealing how AI safety policies often inadvertently reinforce existing power imbalances while claiming neutrality.

## Relevant Concepts

**Intersectionality:** Framework recognizing how multiple social identities (race, gender, class, disability, etc.) interact to create distinct, compounded experiences of marginalization and technological risk.

**AI Safety:** Discipline addressing existential threats from artificial intelligence through development of systems aligned with human values, ethics, explainability, and external control mechanisms.

**Feminist Policy Analysis:** Critical approach examining how policies reflect and reinforce power dynamics, whose interests are centered, how structural inequalities are addressed or perpetuated, and what accountability mechanisms exist.

**Existential Risk:** Threats to human civilization or marginalized populations' futures from advanced AI systems operating without adequate value alignment or human oversight.

**Technocratic Governance:** Top-down policy approaches prioritizing technical expertise and universal frameworks while marginalizing affected communities' participation, knowledge, and differential needs.

**Current Harms:** Observable, present-day negative impacts of existing AI systems on marginalized groups, including algorithmic bias, discriminatory outcomes, and exclusion from decision-making.

## Significance

This work holds substantial significance for academic, policy, and affected communities. It provides concrete frameworks for integrating equity considerations into AI safety governance rather than treating them as separate concerns. The research validates AI safety's core concerns about technological risks while demonstrating how universalist framings obscure differential impacts across social groups. For policymakers, the paper offers actionable recommendations: (1) integrate accountability and participation mechanisms into research and policy development, (2) ensure meaningful participation from marginalized communities, (3) explicitly connect future AI risks to current harms, and (4) acknowledge how current biases shape future AI impacts. By bridging AI safety and critical technology studies, the work establishes intersectionality as essential to legitimate, effective governance. This contribution challenges the field to move beyond technocratic approaches toward more inclusive, equitable AI development processes that acknowledge how technological futures are inseparable from present inequalities. The timing is critical: as AI systems rapidly proliferate, governance frameworks established now will determine whether future development perpetuates or disrupts existing power hierarchies.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.cigionline.org/documents/3375/DPH-paper-Laine_McCrory.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/NMD5P5LN)

## Related Concepts

- Intersectionality
- AI Safety
- Feminist Policy Analysis
- Existential Risk
- Technocratic Governance
- Current Harms

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='mcdonald_2023_algorithmic_decision-making_in_social_work_practic'></a>

## Paper 141/266: Algorithmic decision-making in social work practice and pedagogy: Confronting the competency/critique dilemma

**Source file:** `McDonald_2023_Algorithmic_decision-making_in_social_work_practic.md`

---
title: "Algorithmic decision-making in social work practice and pedagogy: Confronting the competency/critique dilemma"
zotero_key: ANU3K3D4
author_year: "McDonald (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "10.1080/02615479.2023.2195425"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 3
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 2
rel_prof: 3
total_relevance: 12

# Categorization
relevance_category: high
top_dimensions: ["AI Literacy", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-high", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Algorithmic decision-making in social work practice and pedagogy: Confronting the competency/critique dilemma

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **12/15** (high) |
| **Top Dimensions** | AI Literacy, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Explores tensions between training social workers for technologically-driven environments and engaging students in critical theories acknowledging unequal power distributions that technological change reinforces. Introduces algorithmic literacy centered on understanding critical limitations of algorithmic decision-making systems. Argues that adding social work subjects on ADM alone proves insufficient; students need opportunities to develop algorithmic literacy enabling them to ask appropriate questions and understand what is at stake with ADM normalization.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1080/02615479.2023.2195425](https://doi.org/10.1080/02615479.2023.2195425)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/ANU3K3D4)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='mei_2023_assessing_gpt's_bias_towards_stigmatized_social_gr'></a>

## Paper 142/266: Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia

**Source file:** `Mei_2023_Assessing_GPT's_bias_towards_stigmatized_social_gr.md`

---
title: "Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia"
zotero_key: AKZCFJN2
author_year: "Mei (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/pdf/2505.17045"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Assessing GPT's bias towards stigmatized social groups: An intersectional case study on nationality prejudice and psychophobia

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/pdf/2505.17045
- **Zotero:** [Open in Zotero](zotero://select/items/AKZCFJN2)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='meilvang_2024_decision_support_and_algorithmic_support__the_cons'></a>

## Paper 143/266: Decision support and algorithmic support: The construction of algorithms and professional discretion in social work

**Source file:** `Meilvang_2024_Decision_support_and_algorithmic_support__The_cons.md`

---
title: "Decision support and algorithmic support: The construction of algorithms and professional discretion in social work"
zotero_key: KJF5FFWW
author_year: "Meilvang (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: bookSection
language: nan
doi: "nan"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 1
rel_prof: 3
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Professional Context", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Decision support and algorithmic support: The construction of algorithms and professional discretion in social work

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Professional Context, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Critical analysis examining three decision-support algorithms developed for Danish municipalities in child and family social work, analyzing how they affect professional discretion despite claims to merely support professionals. Demonstrates how algorithmic systems designed to minimize subjective judgment and promote efficiency actually embody positivist assumptions that professional discretion can and should be eliminated. Key findings reveal how political actors favor standardized, automated approaches to avoid high-profile cases, effectively negating professional judgment central to ethical social work practice. Drawing on street-level bureaucracy and digitalization literature, argues that framing algorithms as neutral decision support obscures their role in fundamentally restructuring professional autonomy, expertise, and nature of care relationships.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/KJF5FFWW)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='moreau_2024_failing_our_youngest__on_the_biases,_pitfalls,_and'></a>

## Paper 144/266: Failing our youngest: On the biases, pitfalls, and risks in a decision support algorithm used for child protection

**Source file:** `Moreau_2024_Failing_our_youngest__On_the_biases,_pitfalls,_and.md`

---
title: "Failing our youngest: On the biases, pitfalls, and risks in a decision support algorithm used for child protection"
zotero_key: EFFZR3YS
author_year: "Moreau (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: conferencePaper
language: nan
doi: "10.1145/3630106.3658906"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 2
total_relevance: 11

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium", "dim-prof-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Failing our youngest: On the biases, pitfalls, and risks in a decision support algorithm used for child protection

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **11/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

Critical empirical study examining child protection decision support algorithm deployed in Danish municipalities, analyzing its biases and implementation challenges. Using real administrative data from Denmark's child welfare system, evaluated algorithm's predictions against actual case outcomes and found significant biases including disproportionate impacts on immigrant families and systematic errors in risk assessment. Results revealed concerning patterns of false positives for marginalized communities and questioned algorithm's validity for high-stakes decision-making. Documents actual harms from deployed systems.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1145/3630106.3658906](https://doi.org/10.1145/3630106.3658906)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/EFFZR3YS)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='mosene_2023_feministische_netzpolitik_und_k√ºnstliche_intellige'></a>

## Paper 145/266: Feministische Netzpolitik und K√ºnstliche Intelligenz in der politischen Bildung [Feminist network politics and artificial intelligence in political education]

**Source file:** `Mosene_2023_Feministische_Netzpolitik_und_K√ºnstliche_Intellige.md`

---
title: "Feministische Netzpolitik und K√ºnstliche Intelligenz in der politischen Bildung [Feminist network politics and artificial intelligence in political education]"
zotero_key: SAT98SKE
author_year: "Mosene (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: webpage
language: nan
doi: "nan"
url: "https://www.politische-medienkompetenz.de/debatte/ki-und-intersektionalitaet/"

# Assessment
decision: Unclear
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 1
rel_prof: 1
total_relevance: 7

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "unclear", "medium-relevance", "dim-vulnerable-medium", "dim-bias-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Feministische Netzpolitik und K√ºnstliche Intelligenz in der politischen Bildung [Feminist network politics and artificial intelligence in political education]

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Unclear** |
| **Total Relevance** | **7/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Examines intersections of feminist network politics and AI within political education, emphasizing intersectional feminist perspectives on digital technologies. Argues feminist network politics involves supporting AI researchers and activists working to eliminate bias in development and outcomes. Discusses how traditional gender roles are reinforced through AI systems and advocates for political education helping users understand how technologies function, emerged, which societal ideas they reflect, and where critical discourse is needed.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.politische-medienkompetenz.de/debatte/ki-und-intersektionalitaet/
- **Zotero:** [Open in Zotero](zotero://select/items/SAT98SKE)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='national_association_of_social_workers_2017_nasw,_aswb,_cswe,_&_cswa_standards_for_technology_'></a>

## Paper 146/266: NASW, ASWB, CSWE, & CSWA standards for technology in social work practice

**Source file:** `National_Association_of_Social_Workers_2017_NASW,_ASWB,_CSWE,_&_CSWA_standards_for_technology_.md`

---
title: "NASW, ASWB, CSWE, & CSWA standards for technology in social work practice"
zotero_key: 5FRR2AAG
author_year: "National Association of Social Workers (2017)"
authors: []

# Publication
publication_year: 2017.0
item_type: report
language: nan
doi: "nan"
url: "https://www.socialworkers.org/Practice/NASW-Practice-Standards-Guidelines/Standards-for-Technology-in-Social-Work-Practice"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 1
rel_praxis: 2
rel_prof: 3
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Professional Context", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# NASW, ASWB, CSWE, & CSWA standards for technology in social work practice

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2017.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Professional Context, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Landmark 64-page collaborative document representing unprecedented coordination among four major U.S. social work organizations to establish comprehensive technology standards for the profession. Standards address four main areas: providing information to public, designing and delivering services, gathering/managing/storing/accessing client information, and educating and supervising social workers. Covers practitioner competence, informed consent, privacy and confidentiality, boundaries and dual relationships, records and documentation, collegial relationships, and jurisdictional boundaries. Provides foundational ethical guidance directly applicable to AI technologies.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.socialworkers.org/Practice/NASW-Practice-Standards-Guidelines/Standards-for-Technology-in-Social-Work-Practice
- **Zotero:** [Open in Zotero](zotero://select/items/5FRR2AAG)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='navigli_2023_biases_in_large_language_models__origins,_inventor'></a>

## Paper 147/266: Biases in large language models: Origins, inventory and discussion

**Source file:** `Navigli_2023_Biases_in_large_language_models__Origins,_inventor.md`

---
title: "Biases in large language models: Origins, inventory and discussion"
zotero_key: IBI7IQV2
author_year: "Navigli (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "10.1145/3597307"
url: "https://doi.org/10.1145/3597307"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 7

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Biases in large language models: Origins, inventory and discussion

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **7/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Provides an overview of various social biases manifested by large language models and discusses their root causes. Examines how training data selection leads to bias and surveys different types of biases including gender, racial/ethnic, sexual orientation, age, religious and cultural biases. Compiles an inventory of biased behaviors and discusses emerging approaches to measure and mitigate such biases.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1145/3597307](https://doi.org/10.1145/3597307)
- **URL:** https://doi.org/10.1145/3597307
- **Zotero:** [Open in Zotero](zotero://select/items/IBI7IQV2)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='ng_2021_conceptualizing_ai_literacy__an_exploratory_review'></a>

## Paper 148/266: Conceptualizing AI literacy: An exploratory review

**Source file:** `Ng_2021_Conceptualizing_AI_literacy__An_exploratory_review.md`

---
title: "Conceptualizing AI literacy: An exploratory review"
zotero_key: 8WU98XN8
author_year: "Ng (2021)"
authors: []

# Publication
publication_year: 2021.0
item_type: journalArticle
language: en
doi: "10.1016/j.caeai.2021.100041"
url: "https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000357"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Conceptualizing AI literacy: An exploratory review

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2021.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1016/j.caeai.2021.100041](https://doi.org/10.1016/j.caeai.2021.100041)
- **URL:** https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000357
- **Zotero:** [Open in Zotero](zotero://select/items/8WU98XN8)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='nuwasiima_2024_the_role_of_artificial_intelligence_(ai)_and_machi'></a>

## Paper 149/266: The role of artificial intelligence (AI) and machine learning in social work practice

**Source file:** `Nuwasiima_2024_The_role_of_artificial_intelligence_(AI)_and_machi.md`

---
title: "The role of artificial intelligence (AI) and machine learning in social work practice"
zotero_key: 63ULI2RK
author_year: "Nuwasiima (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.30574/wjarr.2024.24.1.2998"
url: "https://doi.org/10.30574/wjarr.2024.24.1.2998"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 3
total_relevance: 12

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium", "dim-prof-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Nuwasiima_2024_role.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# The role of artificial intelligence (AI) and machine learning in social work practice

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **12/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

This comprehensive review identifies algorithmic bias as a critical challenge in social work AI implementation, noting that algorithms trained on historical data may perpetuate existing inequalities by replicating racial, gender, and socio-economic disparities. The authors document specific cases where predictive analytics tools disproportionately flagged families of color for child welfare interventions despite lacking substantial evidence of higher abuse rates. The study emphasizes that addressing algorithmic bias requires multi-faceted approaches including diverse and inclusive datasets, ongoing evaluation and auditing of AI systems, and involvement of social workers and community members in AI tool development.


## AI Summary

!summary_Nuwasiima_2024_role.md


## Links & Resources

- **DOI:** [10.30574/wjarr.2024.24.1.2998](https://doi.org/10.30574/wjarr.2024.24.1.2998)
- **URL:** https://doi.org/10.30574/wjarr.2024.24.1.2998
- **Zotero:** [Open in Zotero](zotero://select/items/63ULI2RK)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='n√§scher_2025_reflectai__design_and_evaluation_of_an_ai_coach_to'></a>

## Paper 150/266: ReflectAI: Design and evaluation of an AI coach to support public servants' self-reflection

**Source file:** `N√§scher_2025_ReflectAI__Design_and_evaluation_of_an_AI_coach_to.md`

---
title: "ReflectAI: Design and evaluation of an AI coach to support public servants' self-reflection"
zotero_key: L3MJF7R8
author_year: "N√§scher (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: conferencePaper
language: nan
doi: "10.1007/978-3-032-02515-9_7"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 3
rel_prof: 2
total_relevance: 7

# Categorization
relevance_category: medium
top_dimensions: ["Practical Implementation", "AI Literacy"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-ai-komp-medium", "dim-praxis-high", "dim-prof-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# ReflectAI: Design and evaluation of an AI coach to support public servants' self-reflection

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **7/15** (medium) |
| **Top Dimensions** | Practical Implementation, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

Design science research presenting ReflectAI, an LLM-based AI coach designed to support public servants in developing self-reflection competencies‚Äîcritical skill for digital transformation in public administration. Two-week user study with seven public servants revealed three key benefits: increased awareness of self-reflection opportunities, improved thought structure, and valuable conversation documentation. Demonstrates how conversational AI can facilitate reflective practice through structured prompting and dialogue. Shows AI coaching for personality-related competencies, demonstrating how prompt-based interactions can support professional development in human services contexts closely aligned with social services.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1007/978-3-032-02515-9_7](https://doi.org/10.1007/978-3-032-02515-9_7)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/L3MJF7R8)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='oecd_2023_advancing_accountability_in_ai'></a>

## Paper 151/266: Advancing Accountability in AI

**Source file:** `OECD_2023_Advancing_Accountability_in_AI.md`

---
title: "Advancing Accountability in AI"
zotero_key: 4RTCRZXA
author_year: "OECD (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: report
language: nan
doi: "nan"
url: "https://www.oecd.org/content/dam/oecd/en/publications/reports/2023/02/advancing-accountability-in-ai_753bf8c8/2448f04b-en.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_OECD_2023_Advancing.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Advancing Accountability in AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Delivers a multi-level review of AI accountability, focusing on transparency, fairness, and privacy. Discusses trade-offs in adopting explainability and transparency measures while mitigating algorithmic bias and upholding fairness, framed within legal, social, and ethical requirements for inclusive, trustworthy AI.


## AI Summary

## Overview

The OECD Digital Economy Paper No. 349 (February 2023, reference: DSTI/CDEP/AIGO(2022)5/FINAL) presents a comprehensive framework for advancing accountability and trustworthiness in artificial intelligence systems through integrated risk management approaches spanning the entire AI lifecycle. Authored by Karine Perset and Luis Aranda under supervision of Audrey Plonk (Head, OECD Digital Economy Policy Division), this document addresses the critical implementation gap between abstract AI governance principles and institutional practice. The paper synthesizes work from approximately 200 experts across two specialized groups‚ÄîTools & Accountability (co-chaired by Nozha Boujemaa/IKEA, Andrea Renda/CEPS, Barry O'Brien/IBM) and Classification & Risk (co-chaired by Marko Grobelnik/JSI, Dewey Murdick/CSET, Sebastian Hallensleben/CEN-CENELEC)‚Äîconvened between February 2020 and December 2022. Approved by the OECD Committee on Digital Economy Policy on 23 December 2022, it contributes to the AI-WIPS programme, supported by Germany's Federal Ministry of Labour and Social Affairs (BMAS), addressing implications for work and productivity.

## Main Findings

The document establishes five critical findings regarding AI accountability and trustworthiness. First, **operationalization is essential**: the OECD AI Principles require concrete tools, processes, and mechanisms to function effectively in practice. Second, **risk-based differentiation enables proportionate governance**: the OECD's AI system classification framework allows tailored accountability strategies appropriate to specific applications and risk contexts. Third, **lifecycle accountability is fundamental**: governance must span design, development, deployment, and monitoring phases; accountability cannot be imposed at single points but requires continuous oversight. Fourth, **systemic integration is necessary**: effective accountability integrates technical safeguards, organizational structures, and governance mechanisms across multiple dimensions. Fifth, **multi-stakeholder coordination is essential**: successful implementation requires coordinated engagement among policymakers, industry, civil society (including CSISAC), and technical experts, reflecting diverse interests and expertise.

## Methodology/Approach

The document employs a **collaborative expert consensus methodology** prioritizing pragmatic policy development over traditional empirical research. The approach convened approximately 200 experts from government, industry (IBM, IKEA), civil society organizations (European Centre for Not-for-Profit Law, AI Transparency Institute), academic institutions (Jozef Stefan Institute), and international bodies (Inter-American Development Bank). Expert groups held regular virtual meetings reviewed by the OECD Working Party on Artificial Intelligence (AIGO) in May and November 2022, with additional review in April and September 2022. Rather than conducting independent empirical studies, the methodology built upon existing OECD frameworks‚Äîthe OECD AI Principles, AI system lifecycle model, and AI system classification framework‚Äîas theoretical scaffolding. This approach prioritizes institutional legitimacy, stakeholder consensus, and policy applicability over novel empirical evidence, reflecting the document's normative, prescriptive orientation toward governance implementation.

## Relevant Concepts

**Trustworthy AI**: AI systems designed and governed to be reliable, transparent, and accountable throughout their lifecycle, integrating technical and governance dimensions.

**AI Lifecycle Accountability**: Continuous governance spanning design, development, deployment, and monitoring phases, recognizing accountability as systemic rather than point-based.

**Risk Management Framework**: Systematic approaches for identifying, assessing, and mitigating AI-related risks proportionate to system classification and operational context.

**AI System Classification**: Categorization of AI systems enabling differentiated governance strategies and proportionate accountability mechanisms based on risk profiles.

**OECD AI Principles**: Foundational governance principles requiring operationalization through concrete tools and institutional mechanisms.

**Tools & Accountability Framework**: Practical instruments translating abstract principles into implementable accountability mechanisms.

**Multi-Stakeholder Governance**: Coordinated engagement among government, industry, civil society, and technical experts in developing and implementing accountability mechanisms.

## Significance

This document holds substantial significance for AI governance discourse, policy development, and work-related AI implications. It provides institutional legitimacy and practical frameworks for OECD member states implementing AI accountability mechanisms, establishing governance standards influencing national and regional regulations. The work contributes to emerging AI governance literature by bridging the principle-to-practice gap, offering concrete guidance where academic literature remains theoretical. Its influence derives from OECD institutional authority, multi-stakeholder consensus, and connection to work productivity concerns rather than novel empirical findings, positioning it as a normative reference point for policymakers. The emphasis on lifecycle accountability, risk-based differentiation, and trustworthiness has influenced subsequent regulatory frameworks, including the EU AI Act. By integrating diverse stakeholder perspectives and addressing work-related implications through the AI-WIPS programme, the document reflects negotiated consensus among government, industry, and civil society, enhancing political acceptability while potentially limiting critical examination of underlying assumptions regarding governance effectiveness and implementation feasibility.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.oecd.org/content/dam/oecd/en/publications/reports/2023/02/advancing-accountability-in-ai_753bf8c8/2448f04b-en.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/4RTCRZXA)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='ovalle_2023_factoring_the_matrix_of_domination__a_critical_rev'></a>

## Paper 152/266: Factoring the matrix of domination: A critical review and reimagination of intersectionality in AI fairness

**Source file:** `Ovalle_2023_Factoring_the_Matrix_of_Domination__A_Critical_Rev.md`

---
title: "Factoring the matrix of domination: A critical review and reimagination of intersectionality in AI fairness"
zotero_key: PXQE9GBV
author_year: "Ovalle (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: conferencePaper
language: nan
doi: "10.1145/3600211.3604705"
url: "https://dl.acm.org/doi/abs/10.1145/3600211.3604705"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Ovalle_2023_Factoring.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Factoring the matrix of domination: A critical review and reimagination of intersectionality in AI fairness

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Critical review examining how intersectionality is conceptualized and operationalized within AI fairness research, analyzing 30 papers from the field. Identifies significant gaps between intersectionality theory and technical applications, proposing six key tenets for properly applying intersectionality in AI research drawn from Patricia Hill Collins and Sirma Bilge's framework.


## AI Summary

## Overview

This paper by Ovalle, Subramonian, Gee, Gautam, and Chang presents a critical examination of how intersectionality‚Äîa foundational framework from critical scholarship‚Äîis adopted and operationalized within AI fairness research. The authors argue that contemporary AI fairness literature fundamentally misappropriates intersectionality by reducing it to a technical problem of protecting intersectional demographic subgroups, rather than engaging with it as a critical analytical framework designed to expose and challenge structural oppression. The paper challenges the epistemological foundations of AI fairness research itself, contending that the field's positivist orientation‚Äîinherited from colonial scientific traditions‚Äîis fundamentally incompatible with intersectionality's praxis-oriented, power-conscious methodology. This incompatibility produces fairness interventions that fail to address systemic inequalities at their root, particularly regarding how multiple, interlocking systems of oppression operate simultaneously.

## Main Findings

The critical literature review of 30 AI fairness papers reveals significant gaps between intersectionality's theoretical origins and its practical implementation in AI contexts. First, researchers consistently interpret intersectionality narrowly as a technical challenge: ensuring that AI systems perform equitably across intersectional demographic categories (e.g., Black women, disabled Latinx individuals) through various operationalization methods (pre/in/post-processing). However, this operationalization strips intersectionality of its critical dimension‚Äîthe examination of how multiple, interlocking systems of oppression (racism, sexism, ableism, colonialism) produce and perpetuate structural inequality, distinct from mere demographic intersection. Second, AI fairness literature conflates intersectional subgroup fairness with intersectionality itself, following Kong's observation that this myopic conceptualization has non-trivial consequences for just AI design. Third, the papers analyzed demonstrate that fairness research overlooks intersectionality's praxis component: the commitment to generating knowledge that directly informs strategies for dismantling oppressive systems and reclaiming power. Instead, the field maintains a positivist distance between knowledge production and power transformation, treating fairness as a neutral technical optimization problem rather than a justice-oriented political project rooted in affected communities' knowledge systems.

## Methodology/Approach

The authors employ a critical literature review methodology, analyzing 30 papers from AI fairness discourse through both deductive and inductive analysis. Deductively, they map how intersectionality tenets theoretically operate within AI fairness paradigms, examining various fairness conceptualizations (group, individual) and operationalizations (pre/in/post-processing). Inductively, they uncover emergent gaps between intersectionality's conceptualization in critical scholarship and its operationalization in technical AI contexts. This dual analytical approach is grounded in critical theory frameworks‚Äîparticularly feminist, antiracist, and decolonial scholarship‚Äîthat examine how knowledge systems reflect and reproduce power relations. The theoretical framework explicitly critiques colonial epistemology, distinguishing between positivist approaches (claiming neutral, quantifiable observation with "unlimited rights of access" to data) and critical approaches (acknowledging knowledge production as inherently political, power-laden, and requiring community participation).

## Relevant Concepts

**Intersectionality:** A traveling framework of critical inquiry and praxis examining interlocking mechanisms of structural oppression that produce inequality across multiple social domains; originates from feminist, antiracist, and decolonial scholarship.

**Praxis:** The unity of critical reflection and practical action aimed at transforming oppressive systems; knowledge generation inseparable from power reclamation and community participation.

**Colonial Epistemology:** Knowledge systems rooted in colonialism that impose positivist paradigms on oppressed populations, erasing Indigenous knowledge while establishing dominant systems as universal truth, preventing marginalized groups from creating and sharing their own knowledge.

**Matrix of Domination:** Multiple, interconnected systems of oppression (racism, sexism, ableism, colonialism) operating simultaneously across structures and disciplines, producing inequality through interlocking mechanisms.

**Intersectional Subgroup Fairness:** Technical operationalization ensuring AI system performance equity across demographic intersections; distinct from intersectionality as critical framework.

**Positivist Paradigm:** Approach treating knowledge as result of neutral, quantifiable observation relying strictly on measurement and reason, claiming researcher objectivity and unlimited data access.

## Significance

This paper's significance lies in its epistemological intervention within computer science. By demonstrating that AI fairness research operates within colonial epistemological frameworks fundamentally misaligned with intersectionality's critical praxis, the authors argue for comprehensive reorientation of the field. The work bridges computer science with critical theory, establishing that technical AI research requires accountability to marginalized communities' intellectual traditions and power-conscious knowledge production. Critically, the paper identifies that current fairness operationalizations‚Äîregardless of conceptualization type or processing stage‚Äîfail to interrogate structural mechanisms producing inequality because they operate within positivist frameworks that decouple knowledge from power. This challenges the field to move beyond narrow fairness metrics toward justice-oriented approaches that center affected communities' knowledge, participation, and liberation, fundamentally reconceptualizing what "fairness" means in AI development.


## Links & Resources

- **DOI:** [10.1145/3600211.3604705](https://doi.org/10.1145/3600211.3604705)
- **URL:** https://dl.acm.org/doi/abs/10.1145/3600211.3604705
- **Zotero:** [Open in Zotero](zotero://select/items/PXQE9GBV)

## Related Concepts

- Intersectionality
- Praxis
- Colonial Epistemology
- Matrix of Domination
- Intersectional Subgroup Fairness
- Positivist Paradigm

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='ovalle_2024_towards_substantive_equality_in_artificial_intelli'></a>

## Paper 153/266: Towards Substantive Equality in Artificial Intelligence: Transformative AI Policy for Gender Equality and Diversity

**Source file:** `Ovalle_2024_Towards_Substantive_Equality_in_Artificial_Intelli.md`

---
title: "Towards Substantive Equality in Artificial Intelligence: Transformative AI Policy for Gender Equality and Diversity"
zotero_key: FQVQFLTQ
author_year: "Ovalle (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://datapopalliance.org/publications/towards-real-diversity-and-gender-equality-in-ai/"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Towards Substantive Equality in Artificial Intelligence: Transformative AI Policy for Gender Equality and Diversity

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Dieser GPAI-Bericht, basierend auf Konsultationen mit √ºber 200 Teilnehmern aus mehr als 50 L√§ndern, entwickelt einen menschenrechtsbasierten Rahmen f√ºr substantielle Gleichberechtigung in der KI. Der Bericht betont, dass KI ohne Intervention das Risiko birgt, gesellschaftliche Verzerrungen zu perpetuieren und zu verst√§rken, insbesondere gegen historisch marginalisierte Gruppen. Die Empfehlungen zielen darauf ab, die strukturellen Wurzeln der Ungleichheit zu bek√§mpfen und transformative Ver√§nderungen zu f√∂rdern, die substantielle Gleichberechtigung in der KI erreichen. Solche Politiken verbessern die Effektivit√§t, Fairness und Nutzbarkeit von KI-Systemen.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://datapopalliance.org/publications/towards-real-diversity-and-gender-equality-in-ai/
- **Zotero:** [Open in Zotero](zotero://select/items/FQVQFLTQ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='pan_2025_libra__measuring_bias_of_large_language_model_from'></a>

## Paper 154/266: LIBRA: Measuring bias of large language model from a local context

**Source file:** `Pan_2025_LIBRA__Measuring_bias_of_large_language_model_from.md`

---
title: "LIBRA: Measuring bias of large language model from a local context"
zotero_key: J8E5TXGQ
author_year: "Pan (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://www.researchgate.net/publication/388686547_LIBRA_Measuring_Bias_of_Large_Language_Model_from_a_Local_Context"

# Assessment
decision: Unclear
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 0
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "unclear", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# LIBRA: Measuring bias of large language model from a local context

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Unclear** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Critiques the U.S.-centricity of existing LLM bias evaluation methods. Proposes the Local Integrated Bias Recognition and Assessment (LIBRA) framework and develops dataset of over 360,000 test cases specific to New Zealand context. Results show models like BERT and GPT-2 struggle with local context, while Llama-3 responds better to different cultural contexts despite exhibiting larger bias overall.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.researchgate.net/publication/388686547_LIBRA_Measuring_Bias_of_Large_Language_Model_from_a_Local_Context
- **Zotero:** [Open in Zotero](zotero://select/items/J8E5TXGQ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='park_2025_ai_algorithm_transparency,_pipelines_for_trust_not'></a>

## Paper 155/266: AI algorithm transparency, pipelines for trust not prisms: mitigating general negative attitudes and enhancing trust toward AI

**Source file:** `Park_2025_AI_algorithm_transparency,_pipelines_for_trust_not.md`

---
title: "AI algorithm transparency, pipelines for trust not prisms: mitigating general negative attitudes and enhancing trust toward AI"
zotero_key: IV7XJZVF
author_year: "Park (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1057/s41599-025-05116-z"
url: "https://www.nature.com/articles/s41599-025-05116-z"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 1
rel_bias: 0
rel_praxis: 2
rel_prof: 1
total_relevance: 5

# Categorization
relevance_category: medium
top_dimensions: ["Practical Implementation", "AI Literacy"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Park_2025_algorithm.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI algorithm transparency, pipelines for trust not prisms: mitigating general negative attitudes and enhancing trust toward AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **5/15** (medium) |
| **Top Dimensions** | Practical Implementation, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Peer-reviewed study examining how algorithmic transparency overcomes users' predisposed distrust in AI. 2√ó2 experiment showing transparency significantly increases trust, especially for those with initially negative AI attitudes. Making AI workings visible mitigated negative relationship between user AI-skepticism and organizational trust. Transparency acts as "signal of trustworthiness," reducing skepticism and uncertainty. Emphasizing transparency in prompts and system design can reassure wary professionals by demonstrating accountability.


## AI Summary

!summary_Park_2025_algorithm.md


## Links & Resources

- **DOI:** [10.1057/s41599-025-05116-z](https://doi.org/10.1057/s41599-025-05116-z)
- **URL:** https://www.nature.com/articles/s41599-025-05116-z
- **Zotero:** [Open in Zotero](zotero://select/items/IV7XJZVF)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='parrish_2022_bbq__a_hand-built_bias_benchmark_for_question_answ'></a>

## Paper 156/266: BBQ: A hand-built bias benchmark for question answering

**Source file:** `Parrish_2022_BBQ__A_hand-built_bias_benchmark_for_question_answ.md`

---
title: "BBQ: A hand-built bias benchmark for question answering"
zotero_key: QRJRGPDE
author_year: "Parrish (2022)"
authors: []

# Publication
publication_year: 2022.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "nan"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# BBQ: A hand-built bias benchmark for question answering

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2022.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/QRJRGPDE)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='parrish_2025_self-debiasing_large_language_models__zero-shot_re'></a>

## Paper 157/266: Self-debiasing large language models: Zero-shot recognition and reduction of stereotypes

**Source file:** `Parrish_2025_Self-debiasing_large_language_models__Zero-shot_re.md`

---
title: "Self-debiasing large language models: Zero-shot recognition and reduction of stereotypes"
zotero_key: 485SYMAZ
author_year: "Parrish (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "https://aclanthology.org/2025.naacl-short.74.pdf"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Self-debiasing large language models: Zero-shot recognition and reduction of stereotypes

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://aclanthology.org/2025.naacl-short.74.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/485SYMAZ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='patton_2023_chatgpt_for_social_work_science__ethical_challenge'></a>

## Paper 158/266: ChatGPT for Social Work Science: Ethical Challenges and Opportunities

**Source file:** `Patton_2023_ChatGPT_for_Social_Work_Science__Ethical_Challenge.md`

---
title: "ChatGPT for Social Work Science: Ethical Challenges and Opportunities"
zotero_key: F7WBGDRS
author_year: "Patton (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "10.1086/726042"
url: "https://doi.org/10.1086/726042"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 2
rel_prof: 3
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Professional Context", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# ChatGPT for Social Work Science: Ethical Challenges and Opportunities

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Professional Context, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Ethical framework for using LLMs in social work research. Recommends transparency, verification, authorship integrity, anti-plagiarism, and inclusion/social justice to counter bias. Positions LLMs as assistive tools requiring critical human oversight.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1086/726042](https://doi.org/10.1086/726042)
- **URL:** https://doi.org/10.1086/726042
- **Zotero:** [Open in Zotero](zotero://select/items/F7WBGDRS)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='peng_2022_a_literature_review_of_digital_literacy_over_two_d'></a>

## Paper 159/266: A Literature Review of Digital Literacy over Two Decades

**Source file:** `Peng_2022_A_Literature_Review_of_Digital_Literacy_over_Two_D.md`

---
title: "A Literature Review of Digital Literacy over Two Decades"
zotero_key: PBGVQHC5
author_year: "Peng (2022)"
authors: []

# Publication
publication_year: 2022.0
item_type: journalArticle
language: en
doi: "10.1155/2022/2533413"
url: "https://www.hindawi.com/journals/edri/2022/2533413/"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# A Literature Review of Digital Literacy over Two Decades

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2022.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

The COVID-19 pandemic has forced online learning to be a ‚Äúnew normal‚Äù during the past three years, which highly emphasizes students‚Äô improved digital literacy. This study aims to present a literature review of students‚Äô digital literacy. Grounded on about twenty journal articles and other related publications from the Web of Science Core Collection, this paper focused on the definition of digital literacy; the factors affecting students‚Äô digital literacy (age, gender, family socioeconomic status, and parent‚Äôs education level); the relationship between students‚Äô digital literacy and their self-control, technostress, and engagement; and the three approaches to gauge the level of students‚Äô digital literacy. The study also provided some advice for educators and policymakers. Finally, the limitations and implications were presented.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1155/2022/2533413](https://doi.org/10.1155/2022/2533413)
- **URL:** https://www.hindawi.com/journals/edri/2022/2533413/
- **Zotero:** [Open in Zotero](zotero://select/items/PBGVQHC5)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='perron_2023_recommendations_for_social_work_researchers_and_jo'></a>

## Paper 160/266: Recommendations for social work researchers and journal editors on the use of generative AI and large language models

**Source file:** `Perron_2023_Recommendations_for_social_work_researchers_and_jo.md`

---
title: "Recommendations for social work researchers and journal editors on the use of generative AI and large language models"
zotero_key: 9QTT2ZFP
author_year: "Perron (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "10.1086/726021"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 1
rel_bias: 2
rel_praxis: 2
rel_prof: 3
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Professional Context", "AI Literacy"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-medium", "dim-bias-medium", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Recommendations for social work researchers and journal editors on the use of generative AI and large language models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Professional Context, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Evidence-based recommendations for social work researchers using generative AI and LLMs, addressing prompt engineering and critical engagement with AI outputs. Examines how LLMs can improve research efficiency through facilitating literature reviews, data analysis, and writing assistance while emphasizing need for critical evaluation of AI-generated content. Discusses concerns about over-reliance on AI potentially diminishing research quality when researchers don't engage in critical thinking or rigorous data evaluation. Stresses that AI tools cannot replace human expertise and judgment.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1086/726021](https://doi.org/10.1086/726021)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/9QTT2ZFP)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='petzel_2025_prejudiced_interactions_with_large_language_models'></a>

## Paper 161/266: Prejudiced interactions with large language models (LLMs) reduce trustworthiness and behavioral intentions among members of stigmatized groups

**Source file:** `Petzel_2025_Prejudiced_interactions_with_large_language_models.md`

---
title: "Prejudiced interactions with large language models (LLMs) reduce trustworthiness and behavioral intentions among members of stigmatized groups"
zotero_key: XTQSJGIB
author_year: "Petzel (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1016/j.chb.2025.108563"
url: "https://doi.org/10.1016/j.chb.2025.108563"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Prejudiced interactions with large language models (LLMs) reduce trustworthiness and behavioral intentions among members of stigmatized groups

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Investigates how biased or prejudiced content in LLM responses affects user trust and willingness to use the system, particularly for users from marginalized communities. Through three preregistered experiments, finds that when AI responses exhibited prejudice, participants from marginalized groups reported significantly lower trust and decreased intentions to continue using the system.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1016/j.chb.2025.108563](https://doi.org/10.1016/j.chb.2025.108563)
- **URL:** https://doi.org/10.1016/j.chb.2025.108563
- **Zotero:** [Open in Zotero](zotero://select/items/XTQSJGIB)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='pinski_2023_ai_literacy_-_towards_measuring_human_competency_i'></a>

## Paper 162/266: AI Literacy - Towards Measuring Human Competency in Artificial Intelligence

**Source file:** `Pinski_2023_AI_Literacy_-_Towards_Measuring_Human_Competency_i.md`

---
title: "AI Literacy - Towards Measuring Human Competency in Artificial Intelligence"
zotero_key: IKFHPHCV
author_year: "Pinski (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: conferencePaper
language: nan
doi: "10.24251/HICSS.2023.021"
url: "http://hdl.handle.net/10125/102649"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI Literacy - Towards Measuring Human Competency in Artificial Intelligence

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.24251/HICSS.2023.021](https://doi.org/10.24251/HICSS.2023.021)
- **URL:** http://hdl.handle.net/10125/102649
- **Zotero:** [Open in Zotero](zotero://select/items/IKFHPHCV)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='pinski_2024_ai_literacy_for_users_‚Äì_a_comprehensive_review_and'></a>

## Paper 163/266: AI literacy for users ‚Äì A comprehensive review and future research directions of learning methods, components, and effects

**Source file:** `Pinski_2024_AI_literacy_for_users_‚Äì_A_comprehensive_review_and.md`

---
title: "AI literacy for users ‚Äì A comprehensive review and future research directions of learning methods, components, and effects"
zotero_key: KLJFJQBH
author_year: "Pinski (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: en
doi: "10.1016/j.chbah.2024.100062"
url: "https://linkinghub.elsevier.com/retrieve/pii/S2949882124000227"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI literacy for users ‚Äì A comprehensive review and future research directions of learning methods, components, and effects

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1016/j.chbah.2024.100062](https://doi.org/10.1016/j.chbah.2024.100062)
- **URL:** https://linkinghub.elsevier.com/retrieve/pii/S2949882124000227
- **Zotero:** [Open in Zotero](zotero://select/items/KLJFJQBH)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='prakash_2023_prompt_engineering_techniques_for_mitigating_cultu'></a>

## Paper 164/266: Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review

**Source file:** `Prakash_2023_Prompt_engineering_techniques_for_mitigating_cultu.md`

---
title: "Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review"
zotero_key: 8F9ZD4CD
author_year: "Prakash (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://www.researchgate.net/publication/392942981_Prompt_Engineering_Techniques_for_Mitigating_Cultural_Bias_Against_Arabs_and_Muslims_in_Large_Language_Models_A_Systematic_Review"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Prompt engineering techniques for mitigating cultural bias against Arabs and Muslims in large language models: A systematic review

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.researchgate.net/publication/392942981_Prompt_Engineering_Techniques_for_Mitigating_Cultural_Bias_Against_Arabs_and_Muslims_in_Large_Language_Models_A_Systematic_Review
- **Zotero:** [Open in Zotero](zotero://select/items/8F9ZD4CD)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='qiu_2025_dr.gap__mitigating_bias_in_large_language_models_u'></a>

## Paper 165/266: DR.GAP: Mitigating bias in large language models using gender-aware prompting with demonstration and reasoning

**Source file:** `Qiu_2025_DR.GAP__Mitigating_bias_in_large_language_models_u.md`

---
title: "DR.GAP: Mitigating bias in large language models using gender-aware prompting with demonstration and reasoning"
zotero_key: H28VMEA3
author_year: "Qiu (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/html/2502.11603v1"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 3
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Bias Analysis", "Practical Implementation"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Qiu_2025_Mitigating.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# DR.GAP: Mitigating bias in large language models using gender-aware prompting with demonstration and reasoning

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Bias Analysis, Practical Implementation |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

DR.GAP ist eine prompting-basierte Methode zur Bias-Reduktion in LLMs. Sie nutzt Beispielf√§lle und strukturierte Reasoning-Schritte, um gendergerechtere Antworten zu erzielen.


## AI Summary

!summary_Qiu_2025_Mitigating.md


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2502.11603v1
- **Zotero:** [Open in Zotero](zotero://select/items/H28VMEA3)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='quaid-i-azam_university_2025_gender_bias_in_artificial_intelligence__empowering'></a>

## Paper 166/266: Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy

**Source file:** `Quaid-i-Azam_University_2025_Gender_Bias_in_Artificial_Intelligence__Empowering.md`

---
title: "Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy"
zotero_key: H8TEPVZW
author_year: "Quaid-i-Azam University (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.70389/PJAI.1000088"
url: "https://premierscience.com/pjai-24-524/"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-medium", "dim-vulnerable-high", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Quaid-i-Azam_University_2025_Gender.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Purpose This narrative review investigates the interplay between gender bias in artificial intelligence (AI) systems and the potential of digital literacy to empower women in technology. By synthesising research from 2010 to 2024, the study examines how gender bias manifests in AI, its impact on women‚Äôs participation in technology, and the effectiveness of digital literacy initiatives in addressing these disparities. Methods A systematic literature search was conducted across major academic databases, including Web of Science, Scopus, IEEE Xplore, and Google Scholar. The review focused on peer-reviewed articles, reports, and case studies published between 2010 and 2024 that addressed gender bias in AI, women‚Äôs participation in technology, and digital literacy initiatives. A thematic analysis framework was employed to identify and synthesise recurring themes and patterns. Results The findings reveal systemic gender biases embedded in AI applications across diverse domains, such as recruitment, healthcare, and financial services. These biases stem from factors including the under-representation of women in AI development teams, biased training datasets, and algorithmic design choices. Digital literacy programs emerge as a promising intervention, fostering a critical awareness of AI bias, encouraging women to pursue AI careers, and catalysing growth in women-led AI projects. Conclusions Although gender bias in AI poses significant challenges, this review highlights digital literacy as a transformative tool for achieving gender equity in AI development and application. The study highlights the importance of inclusive AI design, gender-responsive education policies, and sustained research efforts to mitigate bias and promote equity.


## AI Summary

## Overview

This narrative review, published in January 2025, examines the intersection of gender bias in artificial intelligence systems and digital literacy as a mechanism for women's empowerment in technology sectors. Authored by Syed Sibghatullah Shah from Quaid-i-Azam University, the study synthesizes peer-reviewed research from 2010-2024 to understand how gender discrimination manifests within AI technologies and explores whether educational interventions through digital literacy can effectively mitigate these disparities. The research addresses a critical contemporary challenge: as AI systems increasingly influence consequential decisions across recruitment, healthcare, and financial services, embedded gender biases perpetuate systemic discrimination. The document positions digital literacy as a transformative intervention capable of fostering critical awareness of AI bias, encouraging women's participation in AI development careers, and catalyzing growth in women-led AI projects.

## Main Findings

The review identifies systemic gender biases as pervasive across multiple AI application domains, particularly recruitment, healthcare, and financial services. These biases originate from three interconnected causative factors: (1) underrepresentation of women in AI development teams creates homogeneous perspectives during system design; (2) biased training datasets‚Äîoften reflecting historical discrimination‚Äîperpetuate inequitable algorithmic outcomes; and (3) algorithmic design choices frequently fail to account for gender-specific impacts. Digital literacy programs emerge as promising interventions accomplishing multiple objectives: cultivating critical awareness of how AI systems embed and amplify gender bias; encouraging women to pursue careers in AI development; and supporting growth of women-led AI projects. The findings suggest digital literacy functions dually as a defensive mechanism (enabling women to recognize and critique biased systems) and an offensive strategy (empowering women to become creators of equitable AI solutions). However, the review acknowledges limited empirical evidence demonstrating measurable effectiveness of specific digital literacy interventions.

## Methodology/Approach

The study employs a narrative review methodology, conducting systematic literature searches across major academic databases (Web of Science, Scopus, IEEE Xplore, Google Scholar) covering 2010-2024. The analytical framework utilizes thematic analysis to identify and synthesize recurring patterns across peer-reviewed articles, reports, and case studies. The document was commissioned and underwent external peer review. However, the methodology exhibits significant limitations: it lacks explicit inclusion/exclusion criteria, quality assessment protocols, bias mitigation strategies, and quantitative synthesis methods typical of rigorous systematic reviews. These constraints limit evidence precision, may introduce selection bias, and restrict suitability for meta-analysis or automated systematic analysis.

## Relevant Concepts

**Gender Bias in AI:** Systematic discrimination embedded in algorithmic systems that disadvantages individuals based on gender, manifesting through biased training data, flawed design assumptions, or homogeneous development teams.

**Digital Literacy:** Critical understanding of how digital technologies function, their societal implications, and capacity to engage meaningfully with technological systems‚Äîextending beyond basic technical skills.

**Women's Technological Participation:** Women's representation and active engagement in AI development, deployment, and decision-making roles across technology sectors.

**Inclusive AI Design:** Development processes intentionally incorporating diverse perspectives, particularly from underrepresented groups, to identify and mitigate potential biases before deployment.

**Gender-Responsive Education Policies:** Educational frameworks explicitly designed to address gender-specific barriers and promote equitable access to technology education and careers.

**AI Workforce Diversity:** Representation of women and other underrepresented groups across all levels of AI development, from entry-level positions to leadership roles.

## Significance

This review contributes to AI ethics literature by bridging technical bias scholarship with educational intervention research. It advances policy discourse by proposing digital literacy as a multifaceted solution addressing both individual empowerment and systemic change. The work emphasizes that achieving gender equity in AI requires simultaneous interventions: inclusive AI design, gender-responsive educational policies, AI workforce diversity initiatives, and sustained research efforts. The author's position is explicitly advocacy-oriented within the scientific discourse on algorithmic fairness. However, critical limitations constrain impact: the narrative methodology provides limited empirical evidence regarding digital literacy's measurable effectiveness, lacks specific implementation frameworks, and offers no quantitative outcomes demonstrating concrete impact on women's AI participation or bias mitigation. Future research should provide longitudinal studies, measurable outcome data, and comparative analysis of different digital literacy intervention models to strengthen evidence-based policymaking in this domain.


## Links & Resources

- **DOI:** [10.70389/PJAI.1000088](https://doi.org/10.70389/PJAI.1000088)
- **URL:** https://premierscience.com/pjai-24-524/
- **Zotero:** [Open in Zotero](zotero://select/items/H8TEPVZW)

## Related Concepts

- Gender Bias in AI
- Digital Literacy
- Women's Technological Participation
- Inclusive AI Design
- Gender-Responsive Education Policies
- AI Workforce Diversity

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='raji_2024_the_algorithmic_auditing_landscape__a_social_justi'></a>

## Paper 167/266: The Algorithmic Auditing Landscape: A Social Justice Approach

**Source file:** `Raji_2024_The_Algorithmic_Auditing_Landscape__A_Social_Justi.md`

---
title: "The Algorithmic Auditing Landscape: A Social Justice Approach"
zotero_key: 8DTUS7Y2
author_year: "Raji (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: conferencePaper
language: nan
doi: "10.1145/3630659.3630671"
url: "https://dl.acm.org/doi/10.1145/3630659.3630671"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# The Algorithmic Auditing Landscape: A Social Justice Approach

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Raji and Buolamwini, pioneers of algorithmic auditing, argue for an approach rooted in social justice. They critique audits that focus solely on technical metrics, advocating instead for methods that center the lived experiences of marginalized communities. This involves a multi-stakeholder process, transparency, and a focus on real-world harms. Such an audit practice inherently makes the co-constitution of discrimination visible by investigating not just the algorithm's output, but the entire socio-technical system in which it is embedded. The paper implicitly highlights the limits of individual competence (e.g., a single auditor's skill) by emphasizing the need for collective, community-involved processes to challenge structural power.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1145/3630659.3630671](https://doi.org/10.1145/3630659.3630671)
- **URL:** https://dl.acm.org/doi/10.1145/3630659.3630671
- **Zotero:** [Open in Zotero](zotero://select/items/8DTUS7Y2)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='reamer_2023_artificial_intelligence_in_social_work__emerging_e'></a>

## Paper 168/266: Artificial intelligence in social work: Emerging ethical issues

**Source file:** `Reamer_2023_Artificial_intelligence_in_social_work__Emerging_e.md`

---
title: "Artificial intelligence in social work: Emerging ethical issues"
zotero_key: JIYZUBLR
author_year: "Reamer (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "10.55521/10-020-205"
url: "https://doi.org/10.55521/10-020-205"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 3
total_relevance: 11

# Categorization
relevance_category: high
top_dimensions: ["Bias Analysis", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium", "dim-prof-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Reamer_2023_Artificial.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Artificial intelligence in social work: Emerging ethical issues

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **11/15** (high) |
| **Top Dimensions** | Bias Analysis, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Systematic analysis of ethical challenges in AI use within social work, developing comprehensive framework for ethical AI implementation. Identifies eight central ethical areas including informed consent, data privacy, transparency, misdiagnosis, client neglect, surveillance, scientific misconduct, and algorithmic bias. Emphasizes transparency as fundamental principle requiring social workers to inform clients about AI use. Develops eight-step protocol for ethical AI implementation including ethics committees and continuous peer-review processes.


## AI Summary

## Overview

Frederic G. Reamer's article addresses a critical professional gap: while artificial intelligence applications proliferate across social work practice, systematic ethical examination remains underdeveloped. Published in the International Journal of Social Work Values and Ethics (2023), this work by a prominent ethics scholar argues that social workers increasingly employ AI for risk assessments, crisis intervention, prevention efforts, bias identification, educational delivery, and burnout prediction. However, social work literature extensively discusses AI applications while minimally addressing ethical implications. Reamer positions this analysis as foundational to developing professional standards that balance technological innovation with ethical accountability, ensuring AI serves vulnerable populations responsibly rather than perpetuating harm through uncritical adoption.

## Main Findings

The analysis identifies nine interconnected ethical domains requiring immediate professional attention. **Informed consent and autonomy** concerns emerge when clients remain unaware that algorithms influence decisions affecting their lives and welfare. **Privacy and confidentiality** risks intensify as AI systems require extensive data collection and storage, potentially exposing sensitive information. **Transparency challenges** arise from algorithmic opacity‚Äîthe "black box" phenomenon where decision-making processes remain incomprehensible to practitioners and clients. **Misdiagnosis risks** threaten vulnerable populations when AI systems generate inaccurate assessments. **Client abandonment** occurs when technology substitutes for human relationships essential to social work's human-centered practice model. **Surveillance implications** extend monitoring capabilities beyond professional necessity. **Dishonesty and fraud** manifest through misrepresenting AI capabilities to clients and organizations. **Algorithmic bias** perpetuates systemic inequalities by encoding historical discrimination into automated systems, distinct from but related to broader systemic bias in service delivery. **Evidence-based practice standards** require rigorous validation before implementation. Reamer concludes practitioners need: clear ethical guidelines for AI implementation, enhanced informed consent procedures, transparency mechanisms for algorithmic decision-making, systematic bias auditing processes, and unwavering commitment to human-centered practice that maintains professional relationships.

## Methodology/Approach

The article employs comprehensive literature review methodology, synthesizing existing scholarship across AI ethics and social work domains. Reamer applies deductive reasoning, moving from established professional ethical standards grounded in the NASW Code of Ethics to specific AI applications. This framework bridges technology studies and professional ethics, translating abstract ethical principles into concrete practice concerns. The approach prioritizes identifying gaps between technological capability and ethical accountability, establishing foundational frameworks for subsequent policy development rather than prescribing definitive solutions.

## Relevant Concepts

**Artificial Intelligence**: Computer systems combining datasets and algorithms to simulate human intelligence and enable problem-solving across clinical, administrative, advocacy, and policy contexts.

**Machine Learning**: AI subset using historical data to predict and generate new outputs, enabling pattern recognition and forecasting in social work contexts.

**Generative AI**: Technology creating images, videos, audio, text, and 3D models by learning patterns from existing data.

**Algorithmic Bias**: Systematic errors in AI systems perpetuating discrimination against specific populations, distinct from but reinforcing systemic bias in service delivery.

**Informed Consent**: Ethical requirement that clients understand and voluntarily agree to interventions, including AI involvement in decision-making processes.

**Transparency**: Principle requiring explainability of algorithmic decision-making processes to affected parties, addressing "black box" opacity.

**Human-Centered Practice**: Social work's core commitment to maintaining professional relationships and human dignity, requiring technology serve rather than replace human judgment.

**Evidence-Based AI Tools**: AI applications requiring rigorous validation and demonstrated effectiveness before professional implementation.

## Significance

This work represents essential professional scholarship addressing technology adoption through proactive ethical deliberation rather than reactive problem-solving. By foregrounding ethical concerns, Reamer challenges techno-optimistic narratives dominating social work literature. The article advocates professional agency in shaping technology integration, positioning social work as proactive rather than passive adopter. It establishes foundational frameworks for subsequent policy development, professional guidelines, and practitioner education. The work identifies critical implementation barriers‚Äîincluding the gap between literature on AI applications and ethical examination‚Äîrequiring immediate attention. Ultimately, the analysis protects vulnerable populations by demanding ethical accountability before widespread AI implementation, ensuring technological advancement serves social work's core values of dignity, justice, and human rights while maintaining the human relationships central to professional practice.


## Links & Resources

- **DOI:** [10.55521/10-020-205](https://doi.org/10.55521/10-020-205)
- **URL:** https://doi.org/10.55521/10-020-205
- **Zotero:** [Open in Zotero](zotero://select/items/JIYZUBLR)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='ricaurte_2024_how_can_feminism_inform_ai_governance_in_practice_'></a>

## Paper 169/266: How can feminism inform AI governance in practice?

**Source file:** `Ricaurte_2024_How_can_feminism_inform_AI_governance_in_practice_.md`

---
title: "How can feminism inform AI governance in practice?"
zotero_key: N3JH3CUQ
author_year: "Ricaurte (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://www.unesco.org/en/articles/how-can-feminism-inform-ai-governance-practice"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-medium", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# How can feminism inform AI governance in practice?

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Diese UNESCO-Publikation definiert feministische KI-Governance als einen aufkommenden Bereich von Politik, Forschung und Entwicklung, der darauf abzielt, KI-Systeme gerecht, gleichberechtigt und inklusiv zu gestalten. Feministische KI-Governance zielt darauf ab, Machtungleichgewichte im KI-√ñkosystem zu adressieren und strukturelle Ungleichheiten, koloniale Verm√§chtnisse und multidimensionale Sch√§den zu ber√ºcksichtigen, die √ºberproportional Gemeinschaften der globalen Mehrheit betreffen. Der Ansatz versteht Algorithmen als kontextuell und durch soziale Beziehungen geformt, nicht als rein mathematische Entit√§ten.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.unesco.org/en/articles/how-can-feminism-inform-ai-governance-practice
- **Zotero:** [Open in Zotero](zotero://select/items/N3JH3CUQ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='ricaurte_quijano_2024_towards_substantive_equality_in_artificial_intelli'></a>

## Paper 170/266: Towards substantive equality in artificial intelligence: Transformative AI policy for gender equality and diversity

**Source file:** `Ricaurte_Quijano_2024_Towards_substantive_equality_in_artificial_intelli.md`

---
title: "Towards substantive equality in artificial intelligence: Transformative AI policy for gender equality and diversity"
zotero_key: SXXJ9Y7C
author_year: "Ricaurte Quijano (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://wp.oecd.ai/app/uploads/2025/05/towards-substantive-equality-in-artificial-intelligence_Transformative-AI-policy-for-gender-equality-and-diversity.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Ricaurte_Quijano_2024_Towards.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Towards substantive equality in artificial intelligence: Transformative AI policy for gender equality and diversity

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This extensive report ‚Äì developed through the Global Partnership on AI (GPAI) with contributors from academia and policy ‚Äì sets out a vision for ‚Äúsubstantive equality‚Äù in AI as opposed to mere formal equality. It recognizes that AI systems can replicate and even amplify societal power imbalances (‚Äúalgorithmic discrimination‚Äù), thus requiring proactive governance to ensure historically marginalized groups are not left behind in the AI era. The report argues that purely technical bias mitigation is insufficient; instead, transformative policies should intervene at multiple points in the AI lifecycle to address structural inequities. Key principles advocated include: anchoring AI development in human rights and intersectional gender analysis, improving inclusive representation in data and AI design, and imposing stronger transparency and accountability obligations on AI systems to prevent harm. It provides practical policy recommendations (e.g., mandating diverse datasets and impact assessments, establishing a ‚Äúright to information‚Äù about AI algorithms, and supporting community-led AI initiatives) to achieve de facto equality of outcomes ‚Äì meaning AI should actively help reduce societal inequalities rather than reinforce them. Overall, the report positions digital equity as an extension of social justice: ensuring not only fairness within AI outputs but also equitable access, participation, and empowerment in shaping AI technology.


## AI Summary

## Overview

This November 2024 GPAI-endorsed report addresses a critical gap in AI governance by examining how to achieve genuine gender equality and diversity in artificial intelligence systems. Developed through a formal project titled "Towards Real Diversity and Gender Equality in AI: Evidence-Based Promising Practices and Recommendations," the document was declassified and made publicly available by the GPAI Responsible AI Working Group, signifying its authoritative policy status. Led by scholars at Tecnol√≥gico de Monterrey, Harvard University's Berkman Klein Center, and Mila (Quebec AI Institute), the research advocates for **transformative AI policy** grounded in substantive equality principles rather than formal compliance. The document fundamentally challenges techno-optimistic narratives by positioning gender and diversity as systemic governance imperatives requiring structural intervention, not peripheral concerns addressable through technical solutions alone.

## Main Findings

The analysis identifies that current AI development perpetuates and amplifies existing gender and diversity gaps through three interconnected mechanisms: (1) structural inequalities embedded in technical systems and algorithms; (2) organizational practices within AI development ecosystems that exclude marginalized groups; and (3) policy frameworks that prioritize innovation over equity. The document establishes that formal equality approaches‚Äîensuring equal representation or access‚Äîprove fundamentally insufficient for addressing substantive inequalities rooted in power asymmetries. Key findings demonstrate that evidence-based promising practices exist across diverse global contexts (Africa, Asia, Latin America, Europe, North America) yet remain fragmented and underutilized in policy design. The research concludes that transformative solutions require intersectional approaches acknowledging how gender inequality compounds with disability, indigenous status, geographic marginalization, and economic precarity. Critical recommendations address policy design, implementation accountability mechanisms, and stakeholder engagement frameworks, though specific policy recommendations remain detailed in the full document beyond this excerpt.

## Methodology/Approach

The project employs rigorous collaborative methodology combining participatory research with evidence synthesis across 30+ international stakeholders. The **Project Advisory Group** (30+ members) provides ongoing guidance and validation, while the **Consultation Expert Group** (6 members) contributed formative expertise during research development stages. Geographic representation spans five continents: Africa (AIMS, Research ICT Africa), Asia (Taiwan, India, Japan), Latin America (Mexico, Brazil), Europe (Nordic countries, Mediterranean), and North America (Canada, USA). Disciplinary expertise encompasses computer science, law, governance studies, feminist theory, indigenous knowledge systems, disability rights, and digital rights advocacy. The framework synthesizes **evidence-based policy development** by identifying and evaluating promising practices across contexts, grounded in **substantive equality theory** emphasizing structural transformation. The participatory design ensures recommendations reflect diverse lived experiences and contextual realities rather than imposing universal solutions.

## Relevant Concepts

**Substantive Equality**: Moving beyond formal equality (equal treatment) toward addressing systemic inequalities and their root causes, ensuring genuine opportunity and outcomes in AI development and deployment.

**Transformative Policy**: Policy interventions designed to fundamentally restructure AI systems and governance rather than implement incremental adjustments, targeting underlying power dynamics, institutional barriers, and exclusionary practices.

**Intersectionality**: Analytical framework recognizing how multiple marginalized identities (gender, race, disability, geography, economic status) interact and compound inequalities in complex, non-additive ways within AI contexts.

**Substantive Equality in AI**: Specific application of equality theory to artificial intelligence, addressing how algorithmic systems, development practices, and governance structures embed and perpetuate discrimination.

**Responsible AI Governance**: Framework for developing, deploying, and regulating AI systems accountable to public interest through stakeholder participation, equity-centered design, and structural accountability‚Äîdistinct from ethics-focused approaches that lack enforcement mechanisms.

## Significance

This work represents a watershed contribution to AI governance discourse by institutionalizing gender and diversity concerns within GPAI's authoritative policy framework and securing formal declassification for public availability. Its significance derives from bridging academic research and policy practice through rigorous evidence synthesis and deliberate global stakeholder engagement (30+ advisory members across five continents). By advancing substantive equality frameworks specifically for AI contexts, the document provides actionable guidance for policymakers, technologists, and civil society organizations. The deliberate incorporation of indigenous perspectives (Indigenous AI, Te Kotahi Research Institute), disability rights (International Disability Alliance), and Global South voices (Africa, Latin America, Asia representation) challenges dominant Western-centric AI governance narratives. The document's theoretical contribution‚Äîpositioning gender and diversity as structural governance imperatives rather than peripheral ethics concerns‚Äîfundamentally reframes AI policy discourse. Ultimately, this research establishes that achieving genuine equality in AI requires transformative policy commitment addressing systemic inequalities at their source, not technical fixes or compliance measures.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://wp.oecd.ai/app/uploads/2025/05/towards-substantive-equality-in-artificial-intelligence_Transformative-AI-policy-for-gender-equality-and-diversity.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/SXXJ9Y7C)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='rodriguez_2024_introducing_generative_artificial_intelligence_int'></a>

## Paper 171/266: Introducing Generative Artificial Intelligence into the MSW Curriculum: A Proposal for the 2029 Educational Policy and Accreditation Standards

**Source file:** `Rodriguez_2024_Introducing_Generative_Artificial_Intelligence_int.md`

---
title: "Introducing Generative Artificial Intelligence into the MSW Curriculum: A Proposal for the 2029 Educational Policy and Accreditation Standards"
zotero_key: C3QBQA6X
author_year: "Rodriguez (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1080/10437797.2024.2340931"
url: "https://doi.org/10.1080/10437797.2024.2340931"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 3
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 2
rel_prof: 3
total_relevance: 12

# Categorization
relevance_category: high
top_dimensions: ["AI Literacy", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-high", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Introducing Generative Artificial Intelligence into the MSW Curriculum: A Proposal for the 2029 Educational Policy and Accreditation Standards

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **12/15** (high) |
| **Top Dimensions** | AI Literacy, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Proposal to add an explicit AI competency to MSW accreditation. Outlines benefits and risks of generative AI, recommends curricular content on ethics, bias, transparency, and responsible use, and frames AI literacy as essential for safeguarding client dignity and equity while leveraging innovation.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1080/10437797.2024.2340931](https://doi.org/10.1080/10437797.2024.2340931)
- **URL:** https://doi.org/10.1080/10437797.2024.2340931
- **Zotero:** [Open in Zotero](zotero://select/items/C3QBQA6X)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='rodr√≠guez-mart√≠nez_2024_ethical_issues_related_to_the_use_of_technology_in'></a>

## Paper 172/266: Ethical issues related to the use of technology in social work practice: A systematic review

**Source file:** `Rodr√≠guez-Mart√≠nez_2024_Ethical_issues_related_to_the_use_of_technology_in.md`

---
title: "Ethical issues related to the use of technology in social work practice: A systematic review"
zotero_key: HCKBP3EJ
author_year: "Rodr√≠guez-Mart√≠nez (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1177/21582440241274842"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 1
rel_prof: 3
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Professional Context", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Ethical issues related to the use of technology in social work practice: A systematic review

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Professional Context, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Systematic literature review examining ethical tensions arising from technology integration in social work practice. Review identifies three main categories of ethical challenges: effects of digitization on professional practice (tensions between efficiency and human connection), education, research and engagement challenges (digital literacy requirements conflicting with traditional social work training), and ethical challenges in digital professional practice (boundary issues, dual relationships, informed consent in digital contexts). Key value conflicts include technology's potential to erode professional relationships and trust; digital divide creating social justice concerns; automated systems undermining client self-determination and participation; and privacy violations threatening human dignity.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1177/21582440241274842](https://doi.org/10.1177/21582440241274842)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/HCKBP3EJ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='ruiz_2024_ai_literacy__a_framework_to_understand,_evaluate,_'></a>

## Paper 173/266: AI Literacy: A Framework to Understand, Evaluate, and Use Emerging Technology

**Source file:** `Ruiz_2024_AI_Literacy__A_Framework_to_Understand,_Evaluate,_.md`

---
title: "AI Literacy: A Framework to Understand, Evaluate, and Use Emerging Technology"
zotero_key: BGS9X3FC
author_year: "Ruiz (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://hdl.handle.net/20.500.12265/218"

# Assessment
decision: Exclude
exclusion_reason: "Wrong publication type"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI Literacy: A Framework to Understand, Evaluate, and Use Emerging Technology

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Wrong publication type


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

To enable all who participate in educational settings to leverage AI tools for powerful learning, this paper describes a framework and strategies for educational leaders to design and implement a clear approach to AI Literacy for their specific audiences (e.g. learners, teachers, or others) that are safe and effective. The first part of the paper describes a framework that identifies essential components of AI Literacy and connects them to existing initiatives. The second part of the paper identifies strategies and illustrative examples as guidance for educational leaders to integrate AI Literacy in PK‚Äì12 education and adapt to their unique contexts.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://hdl.handle.net/20.500.12265/218
- **Zotero:** [Open in Zotero](zotero://select/items/BGS9X3FC)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='sabour_2023_a_chatbot_for_mental_health_support__exploring_the'></a>

## Paper 174/266: A chatbot for mental health support: Exploring the impact of Emohaa on reducing mental distress in China

**Source file:** `Sabour_2023_A_chatbot_for_mental_health_support__Exploring_the.md`

---
title: "A chatbot for mental health support: Exploring the impact of Emohaa on reducing mental distress in China"
zotero_key: B59U6C8V
author_year: "Sabour (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "10.3389/fdgth.2023.1133987"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 2
rel_bias: 1
rel_praxis: 3
rel_prof: 2
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Practical Implementation", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-praxis-high", "dim-prof-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# A chatbot for mental health support: Exploring the impact of Emohaa on reducing mental distress in China

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Practical Implementation, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

Three-arm randomized controlled trial examining Emohaa mental health chatbot system in China with 141 university students experiencing mental distress. Participants assigned to: CBT-Bot only (n=69), Full Emohaa with CBT-Bot and emotional support bot (n=31), or control group (n=41). 8-week intervention used AI-powered conversational agents providing cognitive-behavioral therapy techniques and emotional support. Results showed significant improvements in depression (p<.001, effect size Œ∑¬≤p=.05) and anxiety for intervention groups compared to control, with effects maintained at 3-week follow-up for Full Emohaa group. Satisfaction surveys showed 71% would recommend platform.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.3389/fdgth.2023.1133987](https://doi.org/10.3389/fdgth.2023.1133987)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/B59U6C8V)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='salecha_2025_model_explanations_for_gender_and_ethnicity_bias_m'></a>

## Paper 175/266: Model explanations for gender and ethnicity bias mitigation in AI-generated narratives

**Source file:** `Salecha_2025_Model_explanations_for_gender_and_ethnicity_bias_m.md`

---
title: "Model explanations for gender and ethnicity bias mitigation in AI-generated narratives"
zotero_key: 2DFRY422
author_year: "Salecha (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: thesis
language: nan
doi: "nan"
url: "https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=7888&context=open_access_etds"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Model explanations for gender and ethnicity bias mitigation in AI-generated narratives

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=7888&context=open_access_etds
- **Zotero:** [Open in Zotero](zotero://select/items/2DFRY422)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='salinas_2025_what‚Äôs_in_a_name__auditing_large_language_models_f'></a>

## Paper 176/266: What‚Äôs in a name? Auditing large language models for race and gender bias

**Source file:** `Salinas_2025_What‚Äôs_in_a_name__Auditing_large_language_models_f.md`

---
title: "What‚Äôs in a name? Auditing large language models for race and gender bias"
zotero_key: WIHW537P
author_year: "Salinas (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/html/2402.14875v3"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# What‚Äôs in a name? Auditing large language models for race and gender bias

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This interdisciplinary audit of GPT-4 and other LLMs reveals systematic intersectional biases based on names signaling race and gender. Prompts with names suggesting a Black woman received less favorable advice compared to those with white male names. This disparity was robust across 42 prompt templates. The study found that adding quantitative anchors (facts, numbers) to the prompt largely eliminated this bias, whereas adding qualitative descriptive details had inconsistent effects and sometimes amplified stereotypes.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2402.14875v3
- **Zotero:** [Open in Zotero](zotero://select/items/WIHW537P)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='sant_2024_the_power_of_prompts__evaluating_and_mitigating_ge'></a>

## Paper 177/266: The power of prompts: Evaluating and mitigating gender bias in MT with LLMs

**Source file:** `Sant_2024_The_power_of_prompts__Evaluating_and_mitigating_ge.md`

---
title: "The power of prompts: Evaluating and mitigating gender bias in MT with LLMs"
zotero_key: CNWL34SP
author_year: "Sant (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: conferencePaper
language: nan
doi: "10.18653/v1/2024.gebnlp-1.7"
url: "https://doi.org/10.18653/v1/2024.gebnlp-1.7"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# The power of prompts: Evaluating and mitigating gender bias in MT with LLMs

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Examines gender bias in machine translation through LLMs using four widely-used test sets. Develops specific prompting engineering techniques that reduce gender bias by up to 12% on WinoMT evaluation dataset. Identifies optimal prompt structures incorporating explicit fairness instructions and context-aware guidelines.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.18653/v1/2024.gebnlp-1.7](https://doi.org/10.18653/v1/2024.gebnlp-1.7)
- **URL:** https://doi.org/10.18653/v1/2024.gebnlp-1.7
- **Zotero:** [Open in Zotero](zotero://select/items/CNWL34SP)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='santos_2024_explainability_through_systematicity__the_hard_sys'></a>

## Paper 178/266: Explainability through systematicity: The hard systematicity challenge

**Source file:** `Santos_2024_Explainability_through_systematicity__The_hard_sys.md`

---
title: "Explainability through systematicity: The hard systematicity challenge"
zotero_key: 4VZE4BCT
author_year: "Santos (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://pmc.ncbi.nlm.nih.gov/articles/PMC12307450/"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Explainability through systematicity: The hard systematicity challenge

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

This philosophical paper argues that the pursuit of "explainability" in AI is too narrow. It proposes a richer ideal called "systematicity," which demands that an AI's reasoning be consistent, coherent, comprehensive, and principled, akin to an integrated body of human thought. The author distinguishes this "hard systematicity challenge" from the historical Fodorian debate on connectionism and explores how the demand for AI to be systematic should be regulated by different rationales.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://pmc.ncbi.nlm.nih.gov/articles/PMC12307450/
- **Zotero:** [Open in Zotero](zotero://select/items/4VZE4BCT)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='santos_2025_how_large_language_models_judge_cooperation'></a>

## Paper 179/266: How large language models judge cooperation

**Source file:** `Santos_2025_How_large_language_models_judge_cooperation.md`

---
title: "How large language models judge cooperation"
zotero_key: 6VI6NUNL
author_year: "Santos (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/pdf/2507.00088"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# How large language models judge cooperation

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

This study investigates how 21 state-of-the-art LLMs make social and moral judgments about cooperative behavior. Using an evolutionary game-theory model and a dataset of 43,200 prompts, the authors find significant variation in how different models assign reputations, particularly when judging interactions with "ill-reputed" actors. Demonstrates that LLM social norms are highly malleable and can be consistently steered by different types of prompt interventions.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/pdf/2507.00088
- **Zotero:** [Open in Zotero](zotero://select/items/6VI6NUNL)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='santy_2023_nlpositionality__characterizing_design_biases_of_d'></a>

## Paper 180/266: NLPositionality: Characterizing design biases of datasets and models

**Source file:** `Santy_2023_NLPositionality__Characterizing_design_biases_of_d.md`

---
title: "NLPositionality: Characterizing design biases of datasets and models"
zotero_key: 9HXBT8YG
author_year: "Santy (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "https://aclanthology.org/2023.acl-long.530/"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: true
summary_file: "summary_Santy_2023_NLPositionality.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# NLPositionality: Characterizing design biases of datasets and models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This study provides a theoretical and methodological framework for making bias visible in AI models and datasets. It introduces the concept of "positionality" - the assumption that developers' social, cultural, and political perspectives inevitably flow into AI artifacts. The authors develop a method to quantify design biases by analyzing which dialects, demographic groups, or social contexts are under- or over-represented in datasets.


## AI Summary

## Overview

This paper addresses a critical challenge in computational political science: predicting political ideology from text when training data is sparse and systematically biased toward extreme positions. Authored by Chen Chen (CUHK Shenzhen), Dylan Walker (Chapman University), and Venkatesh Saligrama (Boston University), the research tackles selection bias in ideology prediction‚Äîa problem where self-reported labels are sparse and skewed toward extreme vocal posters, while moderate populations remain underrepresented. The core innovation decomposes document embeddings into two independent latent vectors: a neutral context vector (ideology-independent) and a position vector (ideology-aligned). This decomposition enables accurate predictions with minimal training data and strong generalization to out-of-distribution examples, particularly moderate positions absent from training sets. The approach is motivated by documented policy failures (e.g., Kansas abortion vote) where overestimation of public support resulted from overlooking silent majorities.

## Main Findings

The proposed model demonstrates substantial empirical success across multiple dimensions. First, it achieves superior performance compared to state-of-the-art baselines while training on only 5% of available biased data, demonstrating remarkable data efficiency. Second, evaluation on two benchmark datasets shows consistent outperformance of existing methods. Third, crowdsourced validation confirms that contextual vectors genuinely capture neutral content independent of ideology, validating the core theoretical decomposition assumption. Fourth, the context-filtering approach‚Äîremoving neutral vectors during inference and retaining only position vectors‚Äîsuccessfully enables predictions on out-of-distribution examples, particularly moderate positions systematically underrepresented in training data. These results collectively demonstrate that explicit decomposition effectively mitigates selection bias and enables ideology prediction extending beyond extreme vocal groups to broader population segments, including silent majorities crucial for policy analysis.

## Methodology/Approach

The technical framework combines statistical decomposition with deep learning in a modified variational autoencoder (VAE) architecture employing bi-modal priors. The model decomposes document embeddings into linear superposition: embedding = context vector + position vector. Context vectors capture ideology-neutral topical content; position vectors encode ideology-specific framing. End-to-end training produces intermediate outputs for both components. Crucially, at deployment time, predictions use exclusively position vectors, operationalizing the paper's central insight: ideology prediction should focus on "how" arguments are framed rather than "what" topics are discussed. This contextual filtering enables robust generalization. Validation combines benchmark dataset evaluation with crowdsourcing experiments verifying contextual vector neutrality and demonstrating ideological concentration after context removal.

## Relevant Concepts

**Selection Bias:** Systematic distortion where certain groups (extreme ideological positions) are overrepresented in training data while others (moderates) are underrepresented, causing models to misrepresent population distributions and fail on underrepresented groups.

**Position Vector:** Latent representation capturing ideology-specific information‚Äîhow arguments are framed and positioned along the ideological spectrum, independent of topical content.

**Context Vector:** Latent representation capturing ideology-neutral topical and semantic content‚Äîthe "what" of documents, independent of ideological stance.

**Bi-modal Priors:** Probabilistic framework in the VAE architecture using two distinct prior distributions to separately model context and position components during training.

**Out-of-Distribution Generalization:** Model's ability to accurately predict ideology on examples substantially different from training data, particularly for underrepresented moderate positions.

**Contextual Filtering:** Removing neutral context vectors during inference to isolate ideology-specific position signals, enabling robust predictions independent of topic variation.

## Significance

This work makes three interconnected contributions. Methodologically, it advances machine learning by explicitly modeling and mitigating selection bias rather than ignoring it‚Äîa novel approach to learning from imperfect real-world data. For NLP, it contributes techniques for text classification under biased supervision with minimal labeled data (5% threshold). For political science and policy analysis, it provides practical tools for understanding genuine public opinion beyond vocal minorities, directly addressing documented failures in policy prediction. The research reflects contemporary challenges in computational social science where data scarcity and bias are endemic. By enabling accurate ideology prediction from minimal biased data while generalizing to underrepresented populations, this approach has immediate applications for legislative analysis, public opinion research, media studies, and social science research broadly. The emphasis on decomposing content from ideology advances theoretical understanding of how to learn robust representations from imperfect data, with implications extending beyond political ideology to other domains involving biased, sparse supervision.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://aclanthology.org/2023.acl-long.530/
- **Zotero:** [Open in Zotero](zotero://select/items/9HXBT8YG)

## Related Concepts

- Selection Bias
- Position Vector
- Context Vector
- Bi-modal Priors
- Out-of-Distribution Generalization
- Contextual Filtering

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='schneider_2018_der_einfluss_der_algorithmen__neue_qualit√§ten_durc'></a>

## Paper 181/266: Der Einfluss der Algorithmen: Neue Qualit√§ten durch Big Data Analytics und K√ºnstliche Intelligenz

**Source file:** `Schneider_2018_Der_Einfluss_der_Algorithmen__Neue_Qualit√§ten_durc.md`

---
title: "Der Einfluss der Algorithmen: Neue Qualit√§ten durch Big Data Analytics und K√ºnstliche Intelligenz"
zotero_key: UU6PITY6
author_year: "Schneider (2018)"
authors: []

# Publication
publication_year: 2018.0
item_type: journalArticle
language: nan
doi: "10.1007/s12054-018-0046-y"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 1
rel_bias: 3
rel_praxis: 1
rel_prof: 3
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Professional Context"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-bias-high", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Der Einfluss der Algorithmen: Neue Qualit√§ten durch Big Data Analytics und K√ºnstliche Intelligenz

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2018.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Bias Analysis, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Examines how algorithmic decision-making affects professional judgment formation and discretionary decision-making space for practitioners. Analyzes automation bias where professionals may over-rely on algorithmic recommendations without adequate critical evaluation. Stresses social work requires debate about what forms of knowledge new technologies can generate, where limits lie, and how it can meaningfully be incorporated into professional reflection and decision-making practices.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1007/s12054-018-0046-y](https://doi.org/10.1007/s12054-018-0046-y)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/UU6PITY6)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='schneider_2022_exploring_opportunities_and_risks_in_decision_supp'></a>

## Paper 182/266: Exploring opportunities and risks in decision support technologies for social workers: An empirical study in the field of disabled people's services

**Source file:** `Schneider_2022_Exploring_opportunities_and_risks_in_decision_supp.md`

---
title: "Exploring opportunities and risks in decision support technologies for social workers: An empirical study in the field of disabled people's services"
zotero_key: VJGGQZQS
author_year: "Schneider (2022)"
authors: []

# Publication
publication_year: 2022.0
item_type: journalArticle
language: nan
doi: "10.1093/bjsw/bcab262"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 2
rel_prof: 3
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Professional Context", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Exploring opportunities and risks in decision support technologies for social workers: An empirical study in the field of disabled people's services

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2022.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Professional Context, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Empirical study investigating German social workers' perspectives on decision support systems in disability services through practitioner interviews. Identifies both opportunities (consistency across cases, evidence-based practice, administrative time-saving) and significant risks (deprofessionalization, data protection concerns, reduced professional autonomy, loss of holistic assessment capabilities). Social workers express ambivalence: recognizing potential for reducing subjective bias and improving resource allocation transparency while worrying about losing relational aspects of assessment and client trust.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1093/bjsw/bcab262](https://doi.org/10.1093/bjsw/bcab262)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/VJGGQZQS)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='schneider_2024_ai_for_decision_support__what_are_possible_futures'></a>

## Paper 183/266: AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?

**Source file:** `Schneider_2024_AI_for_decision_support__What_are_possible_futures.md`

---
title: "AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?"
zotero_key: ZBAYPR6D
author_year: "Schneider (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.14512/tatup.33.1.08"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI for decision support: What are possible futures, social impacts, regulatory options, ethical conundrums and agency constellations?

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Comprehensive examination of AI-based decision support systems across healthcare, legal systems, and border control. Provides critical analysis of technical, ethical, legal, and societal challenges when machines make or support decisions previously made by humans. Reviews regulatory attempts including EU AI Act. Examines key issues: opacity of algorithmic systems creating black box problems for accountability, professional deskilling risks when practitioners defer to AI, and potential for discrimination embedded in training data and algorithmic logic.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.14512/tatup.33.1.08](https://doi.org/10.14512/tatup.33.1.08)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/ZBAYPR6D)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='schneider_2024_das_verflixte_problem_mit_klassifikationen__zum_ei'></a>

## Paper 184/266: Das verflixte Problem mit Klassifikationen: Zum Einfluss der Digitalisierung auf die Soziale Diagnostik in der Sozialen Arbeit

**Source file:** `Schneider_2024_Das_verflixte_Problem_mit_Klassifikationen__Zum_Ei.md`

---
title: "Das verflixte Problem mit Klassifikationen: Zum Einfluss der Digitalisierung auf die Soziale Diagnostik in der Sozialen Arbeit"
zotero_key: MN6E9Y5E
author_year: "Schneider (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: bookSection
language: nan
doi: "nan"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 3
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Bias Analysis", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Das verflixte Problem mit Klassifikationen: Zum Einfluss der Digitalisierung auf die Soziale Diagnostik in der Sozialen Arbeit

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Bias Analysis, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Examines how digitalization affects social diagnostics and assessment practices in social work. Analyzes fundamental problem of classification systems: tension between necessary categorization for resource allocation and profession's commitment to individualized, contextual understanding of clients. Digital systems intensify this tension by requiring standardized data inputs that may not capture social complexity or unique circumstances. Explores how algorithmic decision support systems rely on predefined categories that risk reifying social problems and overlooking contextual factors.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/MN6E9Y5E)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='schneider_2025_indecision_on_the_use_of_artificial_intelligence_i'></a>

## Paper 185/266: Indecision on the use of artificial intelligence in healthcare: A qualitative study of patient perspectives on trust, responsibility and self-determination using AI-CDSS

**Source file:** `Schneider_2025_Indecision_on_the_use_of_artificial_intelligence_i.md`

---
title: "Indecision on the use of artificial intelligence in healthcare: A qualitative study of patient perspectives on trust, responsibility and self-determination using AI-CDSS"
zotero_key: 5ZIDAC8C
author_year: "Schneider (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1186/s12910-024-01143-5"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 1
rel_praxis: 1
rel_prof: 2
total_relevance: 7

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Professional Context"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-prof-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Indecision on the use of artificial intelligence in healthcare: A qualitative study of patient perspectives on trust, responsibility and self-determination using AI-CDSS

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **7/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

Empirical qualitative study exploring patient perspectives on AI-based clinical decision support systems (AI-CDSS) in healthcare, revealing significant ambivalence about AI use in medical decision-making. Through interviews examining trust, responsibility distribution, and self-determination when AI systems assist physicians, findings show patients worry about decreased human interaction, loss of holistic care perspectives, and unclear accountability when AI makes errors. Vulnerable populations express particular concerns about algorithmic systems making decisions affecting their wellbeing.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1186/s12910-024-01143-5](https://doi.org/10.1186/s12910-024-01143-5)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/5ZIDAC8C)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='sch√∂nauer_2025_akzeptanz_von_ki_und_organisationale_rahmenbedingu'></a>

## Paper 186/266: Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen Arbeit

**Source file:** `Sch√∂nauer_2025_Akzeptanz_von_KI_und_organisationale_Rahmenbedingu.md`

---
title: "Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen Arbeit"
zotero_key: GCBMWEKM
author_year: "Sch√∂nauer (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: de
doi: "10.1007/s12054-025-00783-3"
url: "https://doi.org/10.1007/s12054-025-00783-3"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 2
rel_bias: 1
rel_praxis: 2
rel_prof: 3
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Professional Context", "AI Literacy"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-medium", "dim-vulnerable-medium", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Akzeptanz von KI und organisationale Rahmenbedingungen in der Sozialen Arbeit

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Professional Context, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Der Artikel untersucht die Akzeptanz und organisationalen Rahmenbedingungen digitaler Technologien, insbesondere K√ºnstlicher Intelligenz (KI), in der Praxis der Sozialen Arbeit aus Sicht der Fachkr√§fte. W√§hrend digitale Technologien im administrativen Bereich bereits weit verbreitet sind, zeigt sich bei der Nutzung digitaler Technologien in der direkten Arbeit mit den Klient:innen noch Zur√ºckhaltung. Durch die Entwicklungen im Bereich KI ergeben sich zunehmend neue M√∂glichkeiten digitale Technologien auch im Bereich der Interaktionsarbeit mit den Klient:innen zu nutzen. Zugleich stellt dies die Fachkr√§fte vor ethische und professionelle Herausforderungen. Die empirische Untersuchung unter Berufseinsteiger:innen zeigt, dass der Einsatz von KI im beruflichen Kontext kritisch bewertet wird. Bedenken bestehen insbesondere in Bezug auf Datenschutz und die Unersetzbarkeit menschlicher Empathie durch die KI. Die Akzeptanz von KI und digitalen Technologien h√§ngt von den digitalen Kompetenzen und den Erfahrungen der Fachkr√§fte beim Einsatz digitaler Technologien im beruflichen Kontext ab, die in diesem Bereich noch ausbauf√§hig sind. Vor dem Hintergrund einer zunehmend digitalisierten Gesellschaft fordert der Artikel eine kritische Auseinandersetzung mit den technischen Entwicklungen und betont die Notwendigkeit von Fort- und Weiterbildungen sowie einer partizipativen Technikgestaltung, um die Praxis der Sozialen Arbeit nachhaltig zu gestalten.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1007/s12054-025-00783-3](https://doi.org/10.1007/s12054-025-00783-3)
- **URL:** https://doi.org/10.1007/s12054-025-00783-3
- **Zotero:** [Open in Zotero](zotero://select/items/GCBMWEKM)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='shah_2025_gender_bias_in_artificial_intelligence__empowering'></a>

## Paper 187/266: Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy

**Source file:** `Shah_2025_Gender_bias_in_artificial_intelligence__Empowering.md`

---
title: "Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy"
zotero_key: XT6XMMWT
author_year: "Shah (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.70389/PJAI.1000088"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-medium", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Gender Bias in Artificial Intelligence: Empowering Women Through Digital Literacy

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Narrative review examining interplay between gender bias in AI systems and digital literacy potential for empowering women in technology. Synthesizes research from 2010-2024 analyzing systematic gender biases in AI applications across recruitment, healthcare, and financial services. These biases stem from women's underrepresentation in AI development teams (only 22% globally), biased training data, and algorithmic design decisions. Digital literacy programs show promise as intervention fostering critical awareness of AI bias, encouraging women toward AI careers, and catalyzing growth of women-led AI projects.


## AI Summary

## Overview

This narrative review examines the relationship between gender bias in artificial intelligence systems and digital literacy as an empowerment mechanism for women in technology. Published in January 2025 by Syed Sibghatullah Shah from Quaid-i-Azam University (Islamabad, Pakistan), the study synthesizes research from 2010-2024 to understand how gender disparities manifest within AI technologies and whether educational interventions can effectively mitigate these inequities. The document positions digital literacy as a transformative tool for fostering critical consciousness about algorithmic bias and enabling women's meaningful participation in AI development and deployment across multiple sectors.

## Main Findings

The review reveals that gender bias in AI is fundamentally systemic rather than incidental, permeating applications across three primary domains: recruitment algorithms, healthcare diagnostics, and financial services. The research identifies three interconnected causal mechanisms: (1) underrepresentation of women in AI development teams creates homogeneous perspectives; (2) biased training datasets perpetuate historical discrimination; and (3) algorithmic design choices embed normative assumptions that disadvantage women. Digital literacy programs produce three distinct measurable outcomes: fostering critical awareness of AI bias mechanisms, encouraging women to pursue AI careers, and catalyzing the emergence of women-led AI projects. The document emphasizes that addressing gender bias requires simultaneous structural interventions (inclusive team composition, representative datasets) and educational interventions (digital literacy programs), rejecting purely technological solutions as insufficient.

## Methodology/Approach

The study employs a narrative review methodology combining systematic literature search across four major academic databases (Web of Science, Scopus, IEEE Xplore, Google Scholar) with thematic analysis frameworks. The temporal scope spans 14 years (2010-2024), capturing peer-reviewed articles, reports, and case studies. This approach prioritizes comprehensive synthesis of patterns and themes across diverse sources rather than quantitative meta-analysis. Limitations include potential selection bias inherent to narrative reviews, reliance on secondary rather than primary evidence, and lack of systematic quality assessment protocols. The methodology is more suitable for policy synthesis than empirical evidence-building.

## Relevant Concepts

**Gender Bias in AI**: Systematic discrimination embedded in algorithmic systems that disadvantages women through biased training data, design choices, or homogeneous development team perspectives.

**Biased Training Datasets**: Historical data reflecting past discrimination that, when used to train AI models, perpetuates and amplifies gender inequities in algorithmic outputs.

**Algorithmic Design Choices**: Deliberate or implicit decisions in algorithm architecture, feature selection, and optimization criteria that embed normative assumptions disadvantaging specific demographic groups.

**Digital Literacy**: Critical competency encompassing technical skills, awareness of algorithmic bias mechanisms, understanding of ethical implications, and capacity for informed participation in technology development.

**Inclusive AI Design**: Deliberate architectural and developmental practices ensuring diverse team perspectives, representative datasets, equitable outcome testing, and accountability mechanisms.

**Women-Led AI Projects**: Technology initiatives conceptualized, developed, and directed by women, representing both increased representation and decision-making authority in AI development.

## Significance

This review contributes to policy discourse by establishing digital literacy as a strategic, evidence-based intervention for gender equity in AI. Significance extends across stakeholder groups: (1) educational institutions gain justification for investing in gender-responsive technology curricula; (2) technology companies receive evidence for the business case supporting diverse development teams; (3) policymakers obtain rationale for supporting women in STEM and AI sectors; (4) international development organizations can integrate findings into gender equity programming. The work bridges technical AI ethics literature with social equity frameworks, making complex algorithmic concepts accessible to non-technical audiences. The document's advocacy orientation positions it as a catalyst for institutional change rather than a definitive empirical study, making it particularly valuable for practitioners and policymakers. However, its reliance on literature synthesis rather than original empirical research limits contribution to primary evidence-building and requires validation through subsequent empirical studies.


## Links & Resources

- **DOI:** [10.70389/PJAI.1000088](https://doi.org/10.70389/PJAI.1000088)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/XT6XMMWT)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='sharma_2024_intersectional_analysis_of_visual_generative_ai__t'></a>

## Paper 188/266: Intersectional analysis of visual generative AI: the case of stable diffusion

**Source file:** `Sharma_2024_Intersectional_analysis_of_visual_generative_AI__t.md`

---
title: "Intersectional analysis of visual generative AI: the case of stable diffusion"
zotero_key: 8TVALKIV
author_year: "Sharma (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://link.springer.com/article/10.1007/s00146-025-02498-1"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Sharma_2024_Intersectional.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Intersectional analysis of visual generative AI: the case of stable diffusion

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This study conducts a critical intersectional analysis of Stable Diffusion, a widely used visual generative AI tool. The examination of 180 generated images shows how the system perpetuates prevailing power systems such as sexism, racism, heteronormativity, and ableism, assuming white, physically healthy, masculine-presenting individuals as the standard. The study identifies three levels of analysis: (1) the aesthetics of AI images (micro-level), (2) institutional contexts (meso-level), and (3) intersections between power systems (macro-level). The authors argue for a reparative, socially just approach to visual generative AI.


## AI Summary

## Overview

This theoretical paper by Overbye-Thompson and Rice addresses a critical gap in algorithmic justice research by shifting focus from technical bias documentation to user-level adaptive responses. While extensive literature demonstrates algorithmic bias across healthcare, hiring, criminal justice, and social media platforms‚Äîincluding smartwatch inaccuracy for darker skin tones, facial recognition misgendering, and healthcare algorithms favoring White patients‚Äîlittle is known about how users actually respond to and navigate these biased systems. The authors propose that users employ strategic workarounds‚Äîadaptive behaviors to circumvent or mitigate algorithmic disadvantages‚Äîand that these responses vary significantly across four epistemic scenarios: bias that exists and is perceived, exists but is unperceived, doesn't exist but is perceived, and neither exists nor is perceived. By centering human agency alongside technological constraint, the paper contributes to more nuanced understandings of digital equity and algorithmic literacy.

## Main Findings

The paper establishes theoretical propositions predicting differential user responses across four distinct bias scenarios. When bias exists and is perceived, users develop deliberate workarounds to circumvent disadvantage. When bias exists but remains unperceived, users face particular vulnerability lacking awareness to trigger adaptive responses. When bias doesn't exist but is perceived, users may develop unnecessary workarounds that paradoxically reduce system utility. When neither exists nor is perceived, users engage systems normally without adaptation. Key conclusions include: algorithmic literacy and human agency are fundamental mechanisms for mitigating bias effects at the user level; understanding adaptive user strategies complements but does not replace technical bias-mitigation approaches; and user detection and response patterns depend on perception rather than objective bias presence alone. The framework enables prediction of context-dependent workaround behaviors and highlights that effective algorithmic accountability requires understanding user navigation strategies in everyday contexts.

## Methodology/Approach

The paper employs a theoretical framework integrating three distinct literatures: critical algorithm studies, information systems research, and media effects theory. It applies the "workarounds" concept from information systems‚Äîestablished mechanisms through which users adapt to, circumvent, or resist system constraints‚Äîcombined with the Human-AI Interaction Theory of Interactive Media Effects (HAAII-TIME). HAAII-TIME explains user detection through "cue routes" (perceptual mechanisms by which users identify algorithmic outputs and potential bias signals) and strategy development through "action routes" (behavioral processes through which users implement adaptive responses). These frameworks intersect in a 2√ó2 matrix crossing actual bias presence with user perception, generating four distinct scenarios with predicted differential workaround patterns. This theoretical architecture enables systematic analysis of user responses without requiring empirical data collection, establishing propositions for future empirical validation.

## Relevant Concepts

**Algorithmic Bias**: Systematic disadvantaging of particular demographic groups through automated decision-making systems, documented across facial recognition, hiring algorithms, healthcare systems, and criminal justice applications.

**Workarounds**: User-initiated adaptive strategies to circumvent, resist, or mitigate technological constraints and system disadvantages; established concept from information systems literature.

**Algorithmic Literacy**: Users' capacity to understand, detect, and respond critically to algorithmic processes and their potential biases; essential for developing adaptive strategies.

**Cue Routes**: Perceptual mechanisms through which users detect and interpret algorithmic outputs, system behaviors, and potential bias signals in media environments.

**Action Routes**: Behavioral processes through which users develop, deliberate, and implement adaptive strategies in response to perceived algorithmic bias.

**Four-Category Epistemic Framework**: Matrix distinguishing scenarios where bias exists/doesn't exist crossed with user perception/non-perception, predicting distinct workaround behaviors in each quadrant.

## Significance

This work advances algorithmic justice scholarship by centering user agency and adaptive capacity rather than positioning users as passive victims of biased systems. By theorizing how users detect and respond to bias across different epistemic conditions, the paper provides crucial insights for developing more inclusive technologies and fostering algorithmic literacy. The framework has practical implications: technology designers should anticipate user workarounds and design systems that support rather than obstruct adaptive strategies; policymakers should recognize that algorithmic accountability requires user-level interventions alongside technical solutions; and digital equity initiatives must prioritize algorithmic literacy to enable user detection and response. Importantly, it highlights that unperceived bias represents a critical vulnerability requiring proactive disclosure mechanisms. This humanistic approach enriches ongoing debates about technological constraint and human autonomy in digital societies, positioning users as active negotiators rather than passive recipients of algorithmic mediation.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://link.springer.com/article/10.1007/s00146-025-02498-1
- **Zotero:** [Open in Zotero](zotero://select/items/8TVALKIV)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='shin_2024_can_prompt_modifiers_control_bias__a_comparative_a'></a>

## Paper 189/266: Can prompt modifiers control bias? A comparative analysis of text-to-image generative models

**Source file:** `Shin_2024_Can_prompt_modifiers_control_bias__A_comparative_a.md`

---
title: "Can prompt modifiers control bias? A comparative analysis of text-to-image generative models"
zotero_key: Q6NX3MNX
author_year: "Shin (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/abs/2406.05602"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Can prompt modifiers control bias? A comparative analysis of text-to-image generative models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This preprint investigates whether explicit prompt modifiers can reduce societal biases in text-to-image generative AI models. Authors evaluated three models (Stable Diffusion, DALL¬∑E 3, Adobe Firefly) comparing baseline versus bias-mitigating prompts. Analysis revealed notable biases across all models, with inconsistent effectiveness of prompt modifiers. While diversity-reflective prompting can expose hidden biases and sometimes nudge outputs towards inclusivity, it is not a comprehensive fix and must be combined with broader ethical AI development efforts.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/abs/2406.05602
- **Zotero:** [Open in Zotero](zotero://select/items/Q6NX3MNX)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='shin_2025_mitigating_age-related_bias_in_large_language_mode'></a>

## Paper 190/266: Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development

**Source file:** `Shin_2025_Mitigating_age-related_bias_in_large_language_mode.md`

---
title: "Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development"
zotero_key: 8SRB7NHJ
author_year: "Shin (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://pubsonline.informs.org/doi/10.1287/ijoc.2024.0645"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Mitigating age-related bias in large language models: Strategies for responsible artificial intelligence development

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://pubsonline.informs.org/doi/10.1287/ijoc.2024.0645
- **Zotero:** [Open in Zotero](zotero://select/items/8SRB7NHJ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='shukla_2025_investigating_ai_systems__examining_data_and_algor'></a>

## Paper 191/266: Investigating AI systems: examining data and algorithmic bias through hermeneutic reverse engineering

**Source file:** `Shukla_2025_Investigating_AI_systems__examining_data_and_algor.md`

---
title: "Investigating AI systems: examining data and algorithmic bias through hermeneutic reverse engineering"
zotero_key: XU4WU6E8
author_year: "Shukla (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://www.frontiersin.org/journals/communication/articles/10.3389/fcomm.2025.1380252/full"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Investigating AI systems: examining data and algorithmic bias through hermeneutic reverse engineering

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This work presents hermeneutic reverse engineering as a framework for investigating bias in AI systems. The approach considers AI systems as boundary objects and analyzes cultural meanings and assumptions embedded in techno-cultural objects. The study proposes three research perspectives: (1) comparative exploration of algorithmic bias, (2) investigation of impacts on various social groups, and (3) participatory approaches to include users in AI design.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.frontiersin.org/journals/communication/articles/10.3389/fcomm.2025.1380252/full
- **Zotero:** [Open in Zotero](zotero://select/items/XU4WU6E8)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='siapka_2023_towards_a_feminist_metaethics_of_ai'></a>

## Paper 192/266: Towards a Feminist Metaethics of AI

**Source file:** `Siapka_2023_Towards_a_Feminist_Metaethics_of_AI.md`

---
title: "Towards a Feminist Metaethics of AI"
zotero_key: DSPW44SK
author_year: "Siapka (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://arxiv.org/abs/2311.14700"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "AI Literacy"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-ai-komp-medium", "dim-vulnerable-medium", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Siapka_2023_Towards.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Towards a Feminist Metaethics of AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Bias Analysis, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This work develops a research agenda for a feminist metaethics of AI. Unlike traditional metaethics that reflects on moral judgments non-normatively, feminist metaethics expands its scope to ask not only what ethics is, but also how our approach to it should be. The author argues that a feminist metaethics of AI should investigate four areas: (1) continuity between theory and action in AI ethics, (2) real-world impacts of AI ethics, (3) the role and profile of those involved in AI ethics, and (4) AI's impacts on power relations through methods that consider context, emotions, and narrative.


## AI Summary

## Overview

Anastasia Siapka's paper identifies a fundamental paradox in contemporary AI ethics: despite overwhelming proliferation of ethical guidelines, boards, codes of conduct, and dedicated journals across multiple disciplines (law, philosophy, computer science, policy, industry), the field lacks systematic self-examination. The author argues that AI ethics scholarship has become predominantly theory-focused, analyzing competing principles and values for AI development without critically evaluating the field's own institutional practices, practitioner composition, and measurable real-world impacts. This oversight is particularly problematic given recent incidents involving AI ethicists themselves, demonstrating that theoretical sophistication alone cannot ensure ethical practice. The paper proposes developing a feminist metaethics of AI‚Äîa methodologically distinct approach that shifts from abstract principle-analysis to concrete institutional accountability, power dynamics, and implementation effectiveness.

## Main Findings

The analysis reveals critical insufficiencies in current AI ethics scholarship across multiple dimensions. First, a significant theory-practice gap exists, manifested as "ethics-washing"‚Äîorganizations adopting ethical frameworks without substantive behavioral change. Second, AI ethics exhibits an accountability asymmetry: ethicists evaluate AI systems while remaining unexamined themselves, creating institutional blind spots. Third, the field's proliferation of meta-analyses merely catalogs and compares frameworks without examining their actual effects on AI development or affected communities. Fourth, AI ethics overlooks how practitioner demographics, institutional hierarchies, and power relations determine which ethical concerns receive attention and whose voices are marginalized. Fifth, the field lacks systematic examination of who participates in AI ethics and how their background shapes the field's priorities. These findings collectively demonstrate that theory-centered approaches cannot address the field's structural and practical limitations.

## Methodology/Approach

The author employs feminist metaethics as a distinctive analytical framework that transcends traditional metaethics' non-normative stance. Traditional metaethics examines what morality is without prescribing how engagement should occur; feminist metaethics adds normative dimensions, asking not only what ethics is but also what ethical engagement should be like. The proposed framework systematically examines four interconnected dimensions: (1) continuity between theoretical commitments and practical actions within AI ethics; (2) measurable real-life effects of AI ethical frameworks on actual AI systems and affected communities; (3) the role, background, and profile of AI ethics practitioners and institutional composition; and (4) effects on power relations through methodologies emphasizing context-sensitivity, emotional dimensions, and narrative analysis. This approach integrates characteristic feminist philosophical methods‚Äîattention to particularity, relationality, lived experience, and structural power‚Äîinto metaethical inquiry, distinguishing it from conventional philosophical approaches.

## Relevant Concepts

**Feminist Metaethics:** A philosophical approach combining metaethical reflection (examining the nature of morality itself) with normative inquiry (determining how ethical engagement should proceed), incorporating feminist epistemological commitments to contextuality, power analysis, and practitioner accountability.

**Ethics-Washing:** The adoption of ethical frameworks, language, and institutional structures without corresponding substantive changes in practice, outcomes, or power distributions‚Äîanalogous to "greenwashing" in environmental discourse.

**Accountability Asymmetry:** The structural condition where AI ethics practitioners and institutions evaluate external AI systems while remaining exempt from equivalent self-scrutiny and evaluation.

**Reflexivity:** Critical self-examination of a field's own assumptions, institutional structures, practitioner composition, and effects rather than focusing exclusively on external subjects of evaluation.

**Power Relations:** Structural dynamics determining whose perspectives are valued, whose concerns are prioritized, how resources and authority are distributed, and which voices are marginalized within the AI ethics field.

**Implementation Gap:** The disconnect between theoretical ethical commitments and measurable real-world effects on AI development, deployment, and governance.

## Significance

This paper makes crucial contributions to AI ethics scholarship by initiating systematic second-order reflection on the field itself. It challenges the assumption that proliferating ethical frameworks and institutional infrastructure automatically improve AI governance. By proposing feminist metaethics as an analytical tool, Siapka provides a methodological pathway for examining institutional accountability, practitioner diversity, implementation effectiveness, and power dynamics. The work is significant for establishing that AI ethics cannot achieve legitimacy through theoretical sophistication or institutional proliferation alone; instead, the field must demonstrate reflexivity, accountability mechanisms, attention to power dynamics, and measurable real-world effects. This argument has implications for how AI ethics is institutionalized, who gains authority within the field, how practitioners are selected and evaluated, and how ethical frameworks are assessed for genuine impact rather than mere symbolic adoption. The paper contributes to emerging scholarship questioning AI ethics' legitimacy and effectiveness, establishing grounds for institutional reform.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/abs/2311.14700
- **Zotero:** [Open in Zotero](zotero://select/items/DSPW44SK)

## Related Concepts

- Feminist Metaethics
- Ethics-Washing
- Accountability Asymmetry
- Reflexivity
- Power Relations
- Implementation Gap

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='siddals_2024__it_happened_to_be_the_perfect_thing___experiences'></a>

## Paper 193/266: "It happened to be the perfect thing": Experiences of generative AI chatbots for mental health

**Source file:** `Siddals_2024__It_happened_to_be_the_perfect_thing___Experiences.md`

---
title: ""It happened to be the perfect thing": Experiences of generative AI chatbots for mental health"
zotero_key: A682HKMW
author_year: "Siddals (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1038/s44184-024-00097-4"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 1
rel_praxis: 2
rel_prof: 2
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Practical Implementation"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-praxis-medium", "dim-prof-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# "It happened to be the perfect thing": Experiences of generative AI chatbots for mental health

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Practical Implementation |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

Qualitative study using semi-structured interviews with 19 individuals from 8 countries who used generative AI chatbots (primarily Pi, ChatGPT) for mental health support in real-world settings. Participants reported high engagement and positive impacts including improved relationships, healing from trauma and loss, and improved mood. Four themes emerged: emotional sanctuary (non-judgmental, always-available support), insightful guidance particularly for relationships, joy of connection, and comparisons with human therapy. Some participants described life-changing impacts. Identified challenges including frustrating safety guardrails disrupting emotional sanctuary, limited memory capabilities, and inability to lead therapeutic process.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1038/s44184-024-00097-4](https://doi.org/10.1038/s44184-024-00097-4)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/A682HKMW)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='sinders_2017_feminist_data_set'></a>

## Paper 194/266: Feminist Data Set

**Source file:** `Sinders_2017_Feminist_Data_Set.md`

---
title: "Feminist Data Set"
zotero_key: IVTVGBKG
author_year: "Sinders (2017)"
authors: []

# Publication
publication_year: 2017.0
item_type: webpage
language: nan
doi: "nan"
url: "https://carolinesinders.com/feminist-data-set/"

# Assessment
decision: Exclude
exclusion_reason: "Wrong publication type"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Feminist Data Set

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2017.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Wrong publication type


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Multi-year art-research project directly addresses critical prompting practices by interrogating every AI development step‚Äîdata collection, labeling, training, algorithm selection, and chatbot design‚Äîthrough feminist and intersectional lenses. Conducts public workshops to collaboratively build feminist datasets. Represents concrete critical prompting practice through community-based data creation as protest against biased AI systems, demonstrating practical approaches to feminist prompting by creating alternative training data.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://carolinesinders.com/feminist-data-set/
- **Zotero:** [Open in Zotero](zotero://select/items/IVTVGBKG)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='singer_2023_ai_creates_the_message__integrating_ai_language_le'></a>

## Paper 195/266: AI Creates the Message: Integrating AI Language Learning Models into Social Work Education and Practice

**Source file:** `Singer_2023_AI_Creates_the_Message__Integrating_AI_Language_Le.md`

---
title: "AI Creates the Message: Integrating AI Language Learning Models into Social Work Education and Practice"
zotero_key: F5D34SBY
author_year: "Singer (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "10.1080/10437797.2023.2189878"
url: "https://doi.org/10.1080/10437797.2023.2189878"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 1
rel_bias: 2
rel_praxis: 1
rel_prof: 3
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Professional Context", "AI Literacy"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-ai-komp-medium", "dim-bias-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI Creates the Message: Integrating AI Language Learning Models into Social Work Education and Practice

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Professional Context, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Commentary advocating cautious integration of LLMs in teaching and practice. Describes pedagogical uses (idea generation, material tailoring) and warns of bias, factual errors, confidentiality risks, and plagiarism. Emphasizes transparency policies and that engagement with AI is ethically preferable to avoidance.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1080/10437797.2023.2189878](https://doi.org/10.1080/10437797.2023.2189878)
- **URL:** https://doi.org/10.1080/10437797.2023.2189878
- **Zotero:** [Open in Zotero](zotero://select/items/F5D34SBY)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='singer_2023_chatgpt_for_social_work_science__ethical_challenge'></a>

## Paper 196/266: ChatGPT for social work science: Ethical challenges and opportunities

**Source file:** `Singer_2023_ChatGPT_for_social_work_science__Ethical_challenge.md`

---
title: "ChatGPT for social work science: Ethical challenges and opportunities"
zotero_key: M3RB6VDA
author_year: "Singer (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "10.1086/726042"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 1
rel_bias: 2
rel_praxis: 1
rel_prof: 3
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Professional Context", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-bias-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# ChatGPT for social work science: Ethical challenges and opportunities

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Professional Context, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Invited paper exploring opportunities and ethical challenges of deploying ChatGPT and large language models specifically for social work science. Describes potential uses of ChatGPT in social work research while examining critical ethical concerns related to algorithmic bias, data quality, and risk of replacing human expertise with AI-generated content. Offers preliminary recommendations for ethical ChatGPT use in social work research, emphasizing need for researchers to critically evaluate AI outputs rather than accepting them uncritically.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1086/726042](https://doi.org/10.1086/726042)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/M3RB6VDA)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='singh_2025_a_reparative_turn_in_ai'></a>

## Paper 197/266: A reparative turn in AI

**Source file:** `Singh_2025_A_reparative_turn_in_AI.md`

---
title: "A reparative turn in AI"
zotero_key: AAJM5NZG
author_year: "Singh (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/pdf/2506.05687"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Singh_2025_reparative.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# A reparative turn in AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Argues for a "reparative turn" in AI governance, moving beyond harm prevention to focus on remedying harm after it occurs. Based on thematic analysis of 1,060 real-world AI harm incidents, proposes taxonomy of reparative actions around four goals: acknowledging harm, attributing responsibility, providing remedies, and enabling systemic change. Finds significant "accountability gap" with most corporate responses limited to symbolic acknowledgments.


## AI Summary

!summary_Singh_2025_reparative.md


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/pdf/2506.05687
- **Zotero:** [Open in Zotero](zotero://select/items/AAJM5NZG)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='skilton_2024_inclusive_prompt_engineering__a_methodology_for_ha'></a>

## Paper 198/266: Inclusive prompt engineering: A methodology for hacking biased AI image generation

**Source file:** `Skilton_2024_Inclusive_prompt_engineering__A_methodology_for_ha.md`

---
title: "Inclusive prompt engineering: A methodology for hacking biased AI image generation"
zotero_key: 5PC7KGXU
author_year: "Skilton (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: conferencePaper
language: nan
doi: "10.1145/3641237.3691655"
url: "https://www.researchgate.net/publication/385325948_Inclusive_Prompt_Engineering_A_Methodology_for_Hacking_Biased_AI_Image_Generation"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Inclusive prompt engineering: A methodology for hacking biased AI image generation

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

This conference paper introduces "inclusive prompt engineering" as a strategy to probe and mitigate biases in generative AI image systems. Authors developed methodology to systematically modify prompts and provide tools for generating more diverse outputs. User studies revealed that when participants encountered stereotypical outputs, they tried adding negative qualifiers but models often failed to obey these negations. Findings underscore need for improved prompt interfaces that actively promote inclusive representation.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1145/3641237.3691655](https://doi.org/10.1145/3641237.3691655)
- **URL:** https://www.researchgate.net/publication/385325948_Inclusive_Prompt_Engineering_A_Methodology_for_Hacking_Biased_AI_Image_Generation
- **Zotero:** [Open in Zotero](zotero://select/items/5PC7KGXU)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='slesinger_2024_training_in_co-creation_as_a_methodological_approa'></a>

## Paper 199/266: Training in Co-Creation as a Methodological Approach to Improve AI Fairness

**Source file:** `Slesinger_2024_Training_in_Co-Creation_as_a_Methodological_Approa.md`

---
title: "Training in Co-Creation as a Methodological Approach to Improve AI Fairness"
zotero_key: FL9N6Q3E
author_year: "Slesinger (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://cris.unibo.it/retrieve/707c849f-3aeb-4ca3-8d1f-2817960064bd/societies-14-00259%20(1).pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 3
rel_prof: 1
total_relevance: 12

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-medium", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Slesinger_2024_Training.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Training in Co-Creation as a Methodological Approach to Improve AI Fairness

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **12/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This study examines the integration of training components in co-creation processes with vulnerable and marginalized stakeholder groups as part of developing AI bias detection and mitigation tools. The research shows that training on AI definitions, terminology, and socio-technical impacts is necessary to enable non-technical stakeholders to clearly articulate their insights on AI fairness. The authors emphasize the importance of critical reflection on appropriate use of training in co-creation approaches and their design and implementation for a truly more inclusive approach to AI system design.


## AI Summary

## Overview

This academic paper addresses a critical gap in AI fairness governance by examining how training components integrated into co-creation (Co-C) processes can enable meaningful participation of vulnerable and marginalized groups in AI system design. Grounded in practical experience developing an AI bias mitigation developer toolkit, the research confronts a fundamental challenge: those most adversely affected by biased AI systems are typically excluded from design processes due to technical complexity barriers. The paper's central contribution is demonstrating that structured training on AI bias, when thoughtfully incorporated into participatory design frameworks, can democratize technical expertise and enable substantive stakeholder engagement. However, the authors critically caution that such approaches risk becoming performative compliance mechanisms‚Äîparticularly within the EU AI Act's regulatory landscape‚Äîwhere powerful institutions may instrumentalize Co-C exercises to demonstrate ethical credentials without achieving genuine fairness improvements or addressing underlying power asymmetries.

## Main Findings

The analysis reveals several critical insights. First, an "accessibility paradox" exists: while participatory design theoretically promotes fairness and social inclusion, AI's technical complexity creates insurmountable barriers for non-expert stakeholders, particularly vulnerable populations. Second, training serves as a viable methodological bridge for expertise democratization, but only when carefully designed with critical reflection on deployment timing, content, and pedagogical approach. Third, vulnerable groups' meaningful participation cannot be assumed automatic; it requires deliberate integration mechanisms, sustained engagement, and genuine decision-making power‚Äînot tokenistic inclusion. Fourth, the regulatory environment creates perverse incentives: institutions conduct superficial Co-C exercises primarily for EU AI Act compliance demonstration rather than substantive fairness advancement. Fifth, the paper identifies a critical distinction between procedural inclusion (having a seat at the table) and substantive equity (influencing outcomes), warning that training without power redistribution risks reinforcing existing hierarchies. Finally, the authors emphasize that Co-C's increasing instrumentalization by powerful actors threatens to co-opt participatory processes while maintaining structural inequalities.

## Methodology/Approach

The research employs a **socio-technical analytical framework** combining participatory design theory, co-creation methodology, and critical science and technology studies perspectives. Socio-technical researchers‚Äîpositioned at the intersection of technical and social analysis‚Äîground their investigation in practical experience developing an AI bias mitigation developer toolkit. This case study approach enables examination of real-world implementation challenges, power dynamics, regulatory pressures, and the gap between participatory design ideals and actual practice. The methodology integrates critical reflection on how training functions within Co-C processes, examining both enabling and constraining factors for genuine stakeholder participation.

## Relevant Concepts

**Co-creation (Co-C)**: Collaborative processes where diverse stakeholders, including those typically excluded from design, actively participate in developing technological solutions with genuine decision-making influence.

**Participatory Design (PD)**: Methodological approach ensuring marginalized stakeholder voices substantively shape technological development, not merely provide input.

**AI Bias/Fairness**: Technical and social challenge of preventing AI systems from reproducing or amplifying discrimination against vulnerable groups; requires both technical mitigation and social justice perspectives.

**Accessibility Paradox**: The contradiction between participatory design's inclusionary goals and technical barriers preventing meaningful participation by non-expert stakeholders.

**Expertise Democratization**: Process of making specialized technical knowledge accessible to non-experts, enabling informed participation in technical decision-making.

**Performative Compliance**: Superficial adherence to regulatory or ethical requirements (e.g., EU AI Act) without substantive commitment to underlying principles or outcomes.

**Power Asymmetries**: Structural inequalities in decision-making authority, resource access, and outcome influence between dominant institutions and marginalized stakeholders.

**Tokenistic Participation**: Inclusion of marginalized voices without genuine integration into decision-making processes or meaningful influence on outcomes.

## Significance

This paper makes crucial contributions to AI ethics and governance scholarship by challenging techno-optimistic assumptions about participatory approaches. It bridges applied AI fairness research with critical STS perspectives, questioning whether procedural inclusion automatically guarantees substantive equity. The work is particularly timely given regulatory developments like the EU AI Act, where Co-C exercises increasingly serve compliance functions. By highlighting risks of instrumentalization while proposing training as a methodological solution, the paper provides practical guidance for researchers and practitioners seeking genuinely inclusive AI development while maintaining critical awareness of power dynamics and regulatory capture risks. The research reveals that training alone is insufficient; meaningful participation requires redistributing decision-making power and ensuring vulnerable groups' perspectives substantively influence outcomes. This positions the work at the intersection of technical AI development, social justice concerns, and governance critique, with implications for policy frameworks, institutional practices, and future participatory AI research.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://cris.unibo.it/retrieve/707c849f-3aeb-4ca3-8d1f-2817960064bd/societies-14-00259%20(1).pdf
- **Zotero:** [Open in Zotero](zotero://select/items/FL9N6Q3E)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='small_2023_generative_ai_and_opportunities_for_feminist_class'></a>

## Paper 200/266: Generative AI and opportunities for feminist classroom assignments

**Source file:** `Small_2023_Generative_AI_and_opportunities_for_feminist_class.md`

---
title: "Generative AI and opportunities for feminist classroom assignments"
zotero_key: K5RYQDIP
author_year: "Small (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://digitalcommons.calpoly.edu/feministpedagogy/vol3/iss5/10/"

# Assessment
decision: Unclear
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 1
rel_bias: 0
rel_praxis: 2
rel_prof: 1
total_relevance: 6

# Categorization
relevance_category: medium
top_dimensions: ["AI Literacy", "Practical Implementation"]

# Tags
tags: ["paper", "unclear", "medium-relevance", "dim-ai-komp-medium", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Generative AI and opportunities for feminist classroom assignments

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Unclear** |
| **Total Relevance** | **6/15** (medium) |
| **Top Dimensions** | AI Literacy, Practical Implementation |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Addresses integration of generative AI into feminist classroom assignments to develop reflexivity and feminist epistemology. Proposes intentional feminist approaches where students collaborate with AI to develop understanding of feminist knowledge production, transforming AI from educational threat into tool for critical learning about power and epistemology.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://digitalcommons.calpoly.edu/feministpedagogy/vol3/iss5/10/
- **Zotero:** [Open in Zotero](zotero://select/items/K5RYQDIP)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='smith_2021_when_good_algorithms_go_sexist__why_and_how_to_adv'></a>

## Paper 201/266: When Good Algorithms Go Sexist: Why and How to Advance AI Gender Equity

**Source file:** `Smith_2021_When_Good_Algorithms_Go_Sexist__Why_and_How_to_Adv.md`

---
title: "When Good Algorithms Go Sexist: Why and How to Advance AI Gender Equity"
zotero_key: A484RHDC
author_year: "Smith (2021)"
authors: []

# Publication
publication_year: 2021.0
item_type: journalArticle
language: en
doi: "10.48558/A179-B138"
url: "https://ssir.org/articles/entry/when_good_algorithms_go_sexist_why_and_how_to_advance_ai_gender_equity"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# When Good Algorithms Go Sexist: Why and How to Advance AI Gender Equity

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2021.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Seven actions social change leaders and machine learning developers can take to build gender-smart artificial intelligence for a more just world.


## AI Summary

## Overview

The "Mitigating Bias in Artificial Intelligence: An Equity Fluent Leadership Playbook" is a translational research document developed by UC Berkeley's Center for Equity, Gender and Leadership that systematically bridges academic AI ethics research and industry practice. Authored by Genevieve Smith and Ishita Rustagi in July 2020, this guide targets organizational leaders across hierarchical levels‚ÄîCEOs, board members, information/data/technology officers, department heads, responsible AI leads, and project managers‚Äîseeking to understand and systematically address bias in machine learning-based AI systems. The playbook reframes bias mitigation from a purely technical challenge into a governance and leadership imperative requiring organizational accountability. It provides differentiated guidance through two complementary frameworks: a "Snapshot" offering top-line information and a "Deeper Dive" for practitioners unfamiliar with or viewing bias primarily as a technical issue. The core premise is that equitable AI outcomes demand integrated leadership engagement informed by diverse lived experiences and equity fluency.

## Main Findings

The playbook establishes several critical insights about bias in AI systems and organizational response. First, bias is systemic and multifaceted, originating from interconnected factors throughout the AI development pipeline‚Äîdata collection, algorithm design, organizational processes‚Äîrather than isolated technical failures. Second, leadership accountability extends across organizational hierarchies; responsibility for bias mitigation encompasses not only data scientists and technical teams but also executive decision-makers and board-level governance. Third, the work presents a diagnostic "Bias in AI Map" delineating how and why bias emerges in machine learning systems, providing frameworks for practitioners to understand bias origins and impacts. Fourth, it articulates seven strategic plays for bias mitigation, operationalizing abstract equity principles into concrete organizational actions. Fifth, the playbook demonstrates that bias mitigation requires simultaneous understanding of technical dimensions and organizational challenges, recognizing that responsible AI requires integrated governance structures. Finally, it concludes that equity fluency‚Äîunderstanding diverse lived experiences and using organizational power to address structural barriers‚Äîis essential for effective bias mitigation.

## Methodology/Approach

The playbook employs rigorous translational research methodology synthesizing multiple evidence sources into practitioner-oriented strategies. It incorporates expert interviews with 11 leading researchers spanning AI ethics, gender studies, computer science, and related fields from prestigious institutions (Stanford University, Oxford University, UC Berkeley). The work integrates institutional feedback from major technology companies (Google, Microsoft), consulting firms (BCG), and industry leaders (Levi Strauss & Co.), ensuring practical relevance and real-world applicability. The theoretical framework centers on Equity Fluent Leadership‚Ñ¢, positioning bias mitigation within broader equity and inclusion discourse rather than treating it as an isolated technical problem. The document employs a dual-track guidance structure: the "Snapshot" provides accessible top-line information for all leaders, while the "Deeper Dive" offers comprehensive analysis for practitioners with varying familiarity levels. This approach emphasizes systemic rather than individualistic solutions, reflecting contemporary understanding that organizational change requires integrated engagement across multiple stakeholder groups.

## Relevant Concepts

**Equity Fluent Leadership**: Understanding the value of different lived experiences and courageously using organizational power to address barriers, increase access, and drive systemic change.

**Bias in AI Systems**: Systematic errors in machine learning models disadvantaging particular groups, originating from data, algorithms, organizational processes, and governance structures.

**Translational Research**: Converting academic insights into practitioner-oriented strategies, actionable frameworks, and implementable organizational tools.

**Systemic Bias**: Bias embedded throughout AI development pipelines and organizational decision-making rather than isolated technical failures.

**Responsible AI**: AI development and deployment that unlocks value while ensuring equitable outcomes and addressing stakeholder impacts.

**Bias in AI Map**: Diagnostic framework delineating how and why bias emerges in machine learning systems.

## Significance

This playbook's significance lies in its recognition that AI bias is fundamentally a governance and leadership problem requiring organizational accountability across hierarchical levels. It addresses a critical implementation gap between established academic knowledge about algorithmic bias and actual organizational practice. By positioning equity fluency as central to bias mitigation, the work extends traditional diversity, equity, and inclusion frameworks into AI governance contexts. The dual-track guidance structure (Snapshot/Deeper Dive) acknowledges varying practitioner sophistication while ensuring accessibility for organizational leaders without technical AI expertise. The work represents contemporary discourse emphasizing that responsible AI development demands integrated leadership engagement, diverse perspectives, and systemic organizational change‚Äînot merely technical solutions. Published in July 2020, the playbook reflects growing recognition of AI ethics as a strategic organizational priority. Its significance extends to corporate governance, stakeholder accountability, regulatory compliance, and equitable technology deployment across industries, positioning bias mitigation as essential to organizational value creation and risk management.


## Links & Resources

- **DOI:** [10.48558/A179-B138](https://doi.org/10.48558/A179-B138)
- **URL:** https://ssir.org/articles/entry/when_good_algorithms_go_sexist_why_and_how_to_advance_ai_gender_equity
- **Zotero:** [Open in Zotero](zotero://select/items/A484RHDC)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='spaulding_2023_predicting_successful_placements_for_youth_in_chil'></a>

## Paper 202/266: Predicting successful placements for youth in child welfare with machine learning

**Source file:** `Spaulding_2023_Predicting_successful_placements_for_youth_in_chil.md`

---
title: "Predicting successful placements for youth in child welfare with machine learning"
zotero_key: WNXD959W
author_year: "Spaulding (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "10.1016/j.childyouth.2023.107229"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 3
rel_prof: 3
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Practical Implementation", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-high", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Predicting successful placements for youth in child welfare with machine learning

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Practical Implementation, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Study developed machine learning models to predict treatment success for youth in various child welfare placement settings using data from 4,788 youth served by Children's Hope Alliance in North Carolina. Models aimed to distinguish which youth would succeed in high-cost residential psychiatric treatment versus community-based alternatives. Using Treatment Outcome Package (TOP) assessment data and gradient boosting algorithms, models achieved AUROCs >0.70 in predicting placement success. Addresses Family First Prevention Services Act requirement for right level of care determinations. Multiple raters per case improved model accuracy. Demonstrates feasibility of using existing clinical data to inform high-stakes placement decisions.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1016/j.childyouth.2023.107229](https://doi.org/10.1016/j.childyouth.2023.107229)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/WNXD959W)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='sperling_2024_in_search_of_artificial_intelligence_(ai)_literacy'></a>

## Paper 203/266: In search of artificial intelligence (AI) literacy in teacher education: A scoping review

**Source file:** `Sperling_2024_In_search_of_artificial_intelligence_(AI)_literacy.md`

---
title: "In search of artificial intelligence (AI) literacy in teacher education: A scoping review"
zotero_key: AEQCMKBI
author_year: "Sperling (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: en
doi: "10.1016/j.caeo.2024.100169"
url: "https://linkinghub.elsevier.com/retrieve/pii/S2666557324000107"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# In search of artificial intelligence (AI) literacy in teacher education: A scoping review

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1016/j.caeo.2024.100169](https://doi.org/10.1016/j.caeo.2024.100169)
- **URL:** https://linkinghub.elsevier.com/retrieve/pii/S2666557324000107
- **Zotero:** [Open in Zotero](zotero://select/items/AEQCMKBI)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='srinivasan_2024_worst_of_both_worlds__a_comparative_analysis_of_er'></a>

## Paper 204/266: Worst of both worlds: A comparative analysis of error in language and vision-language models

**Source file:** `Srinivasan_2024_Worst_of_both_worlds__A_comparative_analysis_of_er.md`

---
title: "Worst of both worlds: A comparative analysis of error in language and vision-language models"
zotero_key: RNXP7XFT
author_year: "Srinivasan (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/html/2405.20152v1"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Worst of both worlds: A comparative analysis of error in language and vision-language models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2405.20152v1
- **Zotero:** [Open in Zotero](zotero://select/items/RNXP7XFT)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='srinivasan_2025_mitigating_trust-induced_inappropriate_reliance_on'></a>

## Paper 205/266: Mitigating trust-induced inappropriate reliance on AI assistance

**Source file:** `Srinivasan_2025_Mitigating_trust-induced_inappropriate_reliance_on.md`

---
title: "Mitigating trust-induced inappropriate reliance on AI assistance"
zotero_key: ZM97HKVW
author_year: "Srinivasan (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://arxiv.org/pdf/2502.13321.pdf"

# Assessment
decision: Unclear
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 0
rel_bias: 1
rel_praxis: 3
rel_prof: 1
total_relevance: 6

# Categorization
relevance_category: medium
top_dimensions: ["Practical Implementation", "AI Literacy"]

# Tags
tags: ["paper", "unclear", "medium-relevance", "dim-praxis-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Mitigating trust-induced inappropriate reliance on AI assistance

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Unclear** |
| **Total Relevance** | **6/15** (medium) |
| **Top Dimensions** | Practical Implementation, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Investigates trust-adaptive interventions to reduce inappropriate reliance on AI recommendations in decision-support tasks. Argues that AI assistants should adapt behavior based on user trust to mitigate both under- and over-trust. In two decision scenarios shows that providing supportive explanations for low trust and counter-explanations for high trust leads to 38% reduction in inappropriate reliance and 20% improvement in decision accuracy. Demonstrates that adaptive insertion of forced pauses to promote deliberation reduces over-trust, opening new paths for improved human-AI collaboration.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/pdf/2502.13321.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/ZM97HKVW)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='srivastava_2024_algorithmic_governance_and_the_international_polit'></a>

## Paper 206/266: Algorithmic Governance and the International Politics of Big Tech

**Source file:** `Srivastava_2024_Algorithmic_Governance_and_the_International_Polit.md`

---
title: "Algorithmic Governance and the International Politics of Big Tech"
zotero_key: 88UIN782
author_year: "Srivastava (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://www.cambridge.org/core/journals/perspectives-on-politics/article/algorithmic-governance-and-the-international-politics-of-big-tech/3C04908735A5F2EE8A70AFED647741FB"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Algorithmic Governance and the International Politics of Big Tech

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Structural analysis examines how Big Tech corporations exercise "entrepreneurial private authority" through algorithmic governance systems challenging state sovereignty. Moves beyond individual-focused approaches to analyze how technology firms exercise instrumental, structural, and discursive power globally. Critiques approaches focusing on individual user agency or technical fixes, examining instead how "Big Tech's algorithmic governance incentivizes 'information pollution'" and creates systemic power imbalances.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.cambridge.org/core/journals/perspectives-on-politics/article/algorithmic-governance-and-the-international-politics-of-big-tech/3C04908735A5F2EE8A70AFED647741FB
- **Zotero:** [Open in Zotero](zotero://select/items/88UIN782)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='steiner_2022_k√ºnstliche_intelligenz_in_der_sozialen_arbeit__gru'></a>

## Paper 207/266: K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen, Entwicklungen, Herausforderungen

**Source file:** `Steiner_2022_K√ºnstliche_Intelligenz_in_der_Sozialen_Arbeit__Gru.md`

---
title: "K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen, Entwicklungen, Herausforderungen"
zotero_key: 9T5KGGH6
author_year: "Steiner (2022)"
authors: []

# Publication
publication_year: 2022.0
item_type: journalArticle
language: nan
doi: "10.1007/s12054-022-00546-4"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 3
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Bias Analysis", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# K√ºnstliche Intelligenz in der Sozialen Arbeit: Grundlagen, Entwicklungen, Herausforderungen

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2022.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Bias Analysis, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Systematically analyzes two key AI application scenarios: Predictive Risk Modeling (PRM) and chatbots in counseling. Discusses neural networks' black box problem, dangers of case labeling through standardization, ethical questions of responsibility and liability when AI predictions diverge from professional judgment, and algorithmic bias risks perpetuating social inequalities. Uses Jonas' ethical theory of responsibility to emphasize ethical responsibility as foundational to all AI implementation decisions.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1007/s12054-022-00546-4](https://doi.org/10.1007/s12054-022-00546-4)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/9T5KGGH6)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='steyvers_2025_what_large_language_models_know_and_what_people_th'></a>

## Paper 208/266: What large language models know and what people think they know

**Source file:** `Steyvers_2025_What_large_language_models_know_and_what_people_th.md`

---
title: "What large language models know and what people think they know"
zotero_key: MEYUKKTT
author_year: "Steyvers (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1038/s42256-024-00976-7"
url: "https://www.nature.com/articles/s42256-024-00976-7"

# Assessment
decision: Unclear
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 0
rel_bias: 1
rel_praxis: 2
rel_prof: 1
total_relevance: 6

# Categorization
relevance_category: medium
top_dimensions: ["AI Literacy", "Practical Implementation"]

# Tags
tags: ["paper", "unclear", "medium-relevance", "dim-ai-komp-medium", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# What large language models know and what people think they know

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Unclear** |
| **Total Relevance** | **6/15** (medium) |
| **Top Dimensions** | AI Literacy, Practical Implementation |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

High-impact study investigating mismatch between LLMs' confidence and users' trust. Found users often over-trust LLM answers, especially with explanations. Longer, detailed explanations increased user confidence even when answers weren't more accurate. Demonstrated that prompt engineering to convey uncertainty accurately helps users calibrate trust better. Aligning explanation with model's true confidence narrowed "calibration gap," improving accuracy in judging when to trust AI.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1038/s42256-024-00976-7](https://doi.org/10.1038/s42256-024-00976-7)
- **URL:** https://www.nature.com/articles/s42256-024-00976-7
- **Zotero:** [Open in Zotero](zotero://select/items/MEYUKKTT)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='strau√ü_2024_cail_‚Äì_critical_ai_literacy__kritische_technikkomp'></a>

## Paper 209/266: CAIL ‚Äì Critical AI Literacy: Kritische Technikkompetenz f√ºr konstruktiven Umgang mit KI-basierter Technologie in Betrieben

**Source file:** `Strau√ü_2024_CAIL_‚Äì_Critical_AI_Literacy__Kritische_Technikkomp.md`

---
title: "CAIL ‚Äì Critical AI Literacy: Kritische Technikkompetenz f√ºr konstruktiven Umgang mit KI-basierter Technologie in Betrieben"
zotero_key: ZLVBN3R2
author_year: "Strau√ü (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://wien.arbeiterkammer.at/service/digifonds/gefoerderte-projekte/CAIL_Bericht-FINAL.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 3
rel_vulnerable: 1
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["AI Literacy", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-high", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# CAIL ‚Äì Critical AI Literacy: Kritische Technikkompetenz f√ºr konstruktiven Umgang mit KI-basierter Technologie in Betrieben

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | AI Literacy, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This study develops a Critical AI Literacy framework defined as critical competency for assessing practical utility and limitations of AI applications in specific contexts. It emphasizes that AI-based automation is more complex, dynamic, and volatile than classical automation forms, creating new challenges. A central finding is that critical AI competency becomes part of knowledge work, as interpretation and verification of AI results remains an essential human task. The project identifies deep automation bias as a meta-risk of AI deployment.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://wien.arbeiterkammer.at/service/digifonds/gefoerderte-projekte/CAIL_Bericht-FINAL.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/ZLVBN3R2)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='struppek_2024_homoglyph_unlearning__a_novel_approach_to_bias_mit'></a>

## Paper 210/266: Homoglyph unlearning: A novel approach to bias mitigation

**Source file:** `Struppek_2024_Homoglyph_unlearning__A_novel_approach_to_bias_mit.md`

---
title: "Homoglyph unlearning: A novel approach to bias mitigation"
zotero_key: RQEYJP43
author_year: "Struppek (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/html/2406.05602v1"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Homoglyph unlearning: A novel approach to bias mitigation

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2406.05602v1
- **Zotero:** [Open in Zotero](zotero://select/items/RQEYJP43)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='studeny_2025_digitale_werkzeuge_und_machtasymmetrien_'></a>

## Paper 211/266: Digitale Werkzeuge und Machtasymmetrien?

**Source file:** `Studeny_2025_Digitale_Werkzeuge_und_Machtasymmetrien_.md`

---
title: "Digitale Werkzeuge und Machtasymmetrien?"
zotero_key: JIQLZ5E6
author_year: "Studeny (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "https://www.ogsa.at/wp-content/uploads/2025/03/ogsaTAGUNG2025_AG-Digitalisierung_Handout-Machtasymmetrien-Susanne-Studeny.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 3
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Bias Analysis", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-prof-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Studeny_2025_Digitale.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Digitale Werkzeuge und Machtasymmetrien?

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Bias Analysis, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Studeny analyzes power asymmetries in digital social work, emphasizing that digital tools and algorithms create new, often invisible forms of power and control. AI decisions remove influence from both professionals and clients while responsibility remains unclear. Algorithms reinforce discrimination as they work with biased data. The author demands that social work critically reflects on digital technologies, demands transparency, and ensures that technology serves people.


## AI Summary

## Overview

This academic document presents a critical examination of digital tools within social work practice, specifically addressing how technology mediates power relationships between practitioners and clients. Prepared for the OGSA conference in Graz (March 2025), the work challenges the prevailing assumption that digitalization inherently improves social services. Instead, it argues that digital systems often reproduce and amplify existing institutional power asymmetries while creating novel forms of control and dependency. The document's central concern is whether technology serves emancipatory or oppressive functions within welfare systems, with particular attention to how digital infrastructure shapes client autonomy, data rights, and access to services. It contextualizes "Projekt Konrad" as a case study examining these dynamics in practice.

## Main Findings

The analysis reveals several critical tensions in digital social work implementation. First, digital tools function as mechanisms of power amplification rather than neutral instruments‚Äîthey embed institutional interests and structural inequalities into their design and deployment. Second, algorithmic decision-making systems perpetuate discrimination by encoding historical biases into automated processes, creating ostensibly objective yet fundamentally unjust outcomes. Third, surveillance technologies marketed as care mechanisms paradoxically undermine the trust essential to effective social work relationships. Fourth, digital exclusion creates stratified access to services, with vulnerable populations experiencing compounded marginalization. Fifth, **digital steering (Steuerung)** operates invisibly to constrain client autonomy without explicit coercion, representing subtle but pervasive threats to self-determination. Sixth, institutional dependencies on proprietary Big Tech systems create structural vulnerabilities requiring alternative models. Finally, **human decision-making authority must be preserved** as a non-negotiable ethical requirement, with transparency mechanisms embedded as accountability safeguards rather than afterthoughts.

## Methodology/Approach

The document employs critical social theory as its analytical foundation, integrating Foucauldian power analysis with institutional critique and rights-based frameworks. This approach examines how surveillance, data control, and algorithmic governance function within welfare bureaucracies. The methodology is explicitly reflexive, requiring social workers to examine their complicity in digital power structures rather than positioning themselves as neutral implementers. The framework combines structural inequality analysis‚Äîparticularly regarding algorithmic bias‚Äîwith participatory action research traditions emphasizing co-determination. Critically, the approach identifies **practitioner reflexivity itself as an essential finding**, not merely a methodological tool. This interdisciplinary approach bridges critical digital sociology, social work ethics, and data justice scholarship, creating a comprehensive critique that moves beyond technical solutions toward systemic transformation.

## Relevant Concepts

**Data Sovereignty**: Client control over personal information and decision-making authority regarding data usage; fundamental to dignity and autonomy.

**Algorithmic Discrimination**: Systematic bias embedded in AI systems that reproduces social inequalities through ostensibly objective automated decisions.

**Surveillance Paradox**: The contradiction between surveillance framed as protective care and its actual function as control mechanism undermining trust.

**Digital Steuerung (Steering)**: Invisible algorithmic governance that constrains autonomy through subtle nudging rather than explicit coercion; distinct from overt surveillance.

**Digital Exclusion**: Stratified access to digital services creating new forms of social marginalization alongside traditional inequalities.

**Transparenz (Transparency)**: Requirement for explicability of algorithmic decisions and data usage as justice imperative, not merely technical documentation.

**Mitbestimmung (Co-determination)**: Genuine client participation in technology governance decisions rather than passive service consumption.

**Digital Sovereignty**: Institutional and individual independence from proprietary systems, enabling autonomous decision-making and reducing Big Tech dependency.

**Vertrauen (Trust)**: Central relational element of social work undermined by surveillance systems; prerequisite for effective practice.

## Significance

This work holds substantial significance for social work practice, policy, and ethics. It provides practitioners with critical frameworks for evaluating technology adoption rather than uncritically accepting digitalization narratives. For policymakers, it demonstrates that efficiency gains cannot justify autonomy erosion or rights violations. Theoretically, it advances critical digital sociology by specifically contextualizing algorithm studies within social work's ethical obligations to vulnerable populations. The emphasis on co-determination, digital literacy, and digital sovereignty as justice imperatives reframes digitalization as fundamentally a question of power distribution rather than technical progress. Critically, the document identifies **digital education as a key to social justice**, positioning literacy not as individual skill-building but as structural empowerment. By challenging techno-optimism while avoiding technological determinism, the document enables more nuanced, ethically grounded approaches to digital transformation in welfare systems that prioritize human decision-making authority and institutional independence from proprietary systems.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.ogsa.at/wp-content/uploads/2025/03/ogsaTAGUNG2025_AG-Digitalisierung_Handout-Machtasymmetrien-Susanne-Studeny.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/JIQLZ5E6)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='s≈´na_2024_diskriminierung_durch_algorithmen_‚Äì_√ºberlegungen_z'></a>

## Paper 212/266: Diskriminierung durch Algorithmen ‚Äì √úberlegungen zur St√§rkung KI-bezogener Kompetenzen

**Source file:** `S≈´na_2024_Diskriminierung_durch_Algorithmen_‚Äì_√úberlegungen_z.md`

---
title: "Diskriminierung durch Algorithmen ‚Äì √úberlegungen zur St√§rkung KI-bezogener Kompetenzen"
zotero_key: WLY34JHH
author_year: "S≈´na (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: bookSection
language: nan
doi: "nan"
url: "https://www.gmk-net.de/wp-content/uploads/2024/12/gmk60_suna_hoffmann_mollen.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Bias Analysis", "AI Literacy"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-medium", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: true
summary_file: "summary_S_na_2024_Diskriminierung.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Diskriminierung durch Algorithmen ‚Äì √úberlegungen zur St√§rkung KI-bezogener Kompetenzen

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Bias Analysis, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Konzeptioneller Beitrag zu Ursachen und Formen algorithmischer Diskriminierung und zur F√∂rderung kritischer KI-Kompetenzen. Pl√§diert f√ºr Aufkl√§rung zu Datenbias, reflexive Nutzung und partizipative Trainings, um Benachteiligungen zu erkennen und digitale Teilhabe zu st√§rken.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.gmk-net.de/wp-content/uploads/2024/12/gmk60_suna_hoffmann_mollen.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/WLY34JHH)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='taeihagh_2025_governance_of_generative_ai__a_comprehensive_frame'></a>

## Paper 213/266: Governance of generative AI: A comprehensive framework for navigating challenges and opportunities

**Source file:** `Taeihagh_2025_Governance_of_generative_AI__A_comprehensive_frame.md`

---
title: "Governance of generative AI: A comprehensive framework for navigating challenges and opportunities"
zotero_key: QMUWKIKK
author_year: "Taeihagh (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://academic.oup.com/policyandsociety/article/44/1/1/7997395"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Governance of generative AI: A comprehensive framework for navigating challenges and opportunities

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Provides comprehensive overview of governance challenges posed by generative AI, including bias amplification, privacy violations, misinformation, and exacerbation of power imbalances. Critiques inadequacy of voluntary self-regulation and proposes comprehensive governance framework that is proactive, adaptive, and participatory. Recommends improving data governance, mandating independent audits, enhancing public engagement, and fostering international cooperation.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://academic.oup.com/policyandsociety/article/44/1/1/7997395
- **Zotero:** [Open in Zotero](zotero://select/items/QMUWKIKK)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='takaoka_2022_ai_implementation_science_for_social_issues__pitfa'></a>

## Paper 214/266: AI implementation science for social issues: Pitfalls and tips

**Source file:** `Takaoka_2022_AI_implementation_science_for_social_issues__Pitfa.md`

---
title: "AI implementation science for social issues: Pitfalls and tips"
zotero_key: NDKXZHTX
author_year: "Takaoka (2022)"
authors: []

# Publication
publication_year: 2022.0
item_type: journalArticle
language: nan
doi: "10.2188/jea.JE20210380"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 3
rel_prof: 3
total_relevance: 11

# Categorization
relevance_category: high
top_dimensions: ["Practical Implementation", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-high", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI implementation science for social issues: Pitfalls and tips

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2022.0 |
| **Decision** | **Include** |
| **Total Relevance** | **11/15** (high) |
| **Top Dimensions** | Practical Implementation, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Case study documenting four-stage social implementation of AI system (AiCAN - Assistant of Intelligence for Child Abuse and Neglect) in Japanese Child Guidance Centers from 2012-2020. System uses machine learning to predict child abuse recurrence and Bayesian networks for real-time probabilistic inference to guide temporary protection decisions. Data from over 6,000 cases (2014-2018) were used to develop gradient boosting algorithms with AUROC >0.70. Implementation involved iterative stakeholder engagement, workflow redesign, training field staff, and addressing organizational resistance. Emphasizes critical importance of building consensus with practitioners, designing for field usability, ensuring data quality through validated scales, and employing eXplainable AI for transparency.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.2188/jea.JE20210380](https://doi.org/10.2188/jea.JE20210380)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/NDKXZHTX)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='thwaites_2024_operationalizing_positive-constructive_pedagogy_to'></a>

## Paper 215/266: Operationalizing positive-constructive pedagogy to artificial intelligence: the 3E model for scaffolding AI technology adoption

**Source file:** `Thwaites_2024_Operationalizing_positive-constructive_pedagogy_to.md`

---
title: "Operationalizing positive-constructive pedagogy to artificial intelligence: the 3E model for scaffolding AI technology adoption"
zotero_key: CUBA5DBW
author_year: "Thwaites (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.3389/feduc.2024.1293235"
url: "https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2024.1293235/full"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 3
rel_vulnerable: 1
rel_bias: 2
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["AI Literacy", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-ai-komp-high", "dim-bias-medium", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Operationalizing positive-constructive pedagogy to artificial intelligence: the 3E model for scaffolding AI technology adoption

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | AI Literacy, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This article proposes the "3E model" (Expose, Explore, Exploit) as a pedagogical framework for developing critical AI literacy. The model aims to move students beyond passive use of AI to a more critical engagement. The "Expose" phase involves revealing the underlying mechanisms and biases of AI systems. "Explore" encourages students to test AI boundaries and critically question its outputs, a practice akin to critical prompting. "Exploit" focuses on using AI for creative and novel purposes. While not explicitly feminist, the framework provides a practical method for developing the critical literacies needed to identify and question the co-constitution of discrimination in AI outputs, thereby making biases visible.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.3389/feduc.2024.1293235](https://doi.org/10.3389/feduc.2024.1293235)
- **URL:** https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2024.1293235/full
- **Zotero:** [Open in Zotero](zotero://select/items/CUBA5DBW)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='tinmaz_2022_a_systematic_review_on_digital_literacy'></a>

## Paper 216/266: A systematic review on digital literacy

**Source file:** `Tinmaz_2022_A_systematic_review_on_digital_literacy.md`

---
title: "A systematic review on digital literacy"
zotero_key: CIPACAP5
author_year: "Tinmaz (2022)"
authors: []

# Publication
publication_year: 2022.0
item_type: journalArticle
language: en
doi: "10.1186/s40561-022-00204-y"
url: "https://slejournal.springeropen.com/articles/10.1186/s40561-022-00204-y"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# A systematic review on digital literacy

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2022.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Abstract The purpose of this study is to discover the main themes and categories of the research studies regarding digital literacy. To serve this purpose, the databases of WoS/Clarivate Analytics, Proquest Central, Emerald Management Journals, Jstor Business College Collections and Scopus/Elsevier were searched with four keyword-combinations and final forty-three articles were included in the dataset. The researchers applied a systematic literature review method to the dataset. The preliminary findings demonstrated that there is a growing prevalence of digital literacy articles starting from the year 2013. The dominant research methodology of the reviewed articles is qualitative. The four major themes revealed from the qualitative content analysis are: digital literacy, digital competencies, digital skills and digital thinking. Under each theme, the categories and their frequencies are analysed. Recommendations for further research and for real life implementations are generated.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1186/s40561-022-00204-y](https://doi.org/10.1186/s40561-022-00204-y)
- **URL:** https://slejournal.springeropen.com/articles/10.1186/s40561-022-00204-y
- **Zotero:** [Open in Zotero](zotero://select/items/CIPACAP5)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='tint_2025_guardrails,_not_guidance__understanding_responses_'></a>

## Paper 217/266: Guardrails, not guidance: Understanding responses to LGBTQ+ language in large language models

**Source file:** `Tint_2025_Guardrails,_not_guidance__Understanding_responses_.md`

---
title: "Guardrails, not guidance: Understanding responses to LGBTQ+ language in large language models"
zotero_key: CWBWUWYL
author_year: "Tint (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "https://aclanthology.org/2025.queerinai-main.2.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Tint_2025_Guardrails.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Guardrails, not guidance: Understanding responses to LGBTQ+ language in large language models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Examines how large language models respond to prompts involving LGBTQ+ terminology and how current safety measures handle such content. Finds disparity where LLMs invoke safety guardrails for overtly heteronormative prompts but exhibit subtle biases when handling queer slang or informal LGBTQ+ language, responding with more negative emotional tone without triggering content filters.


## AI Summary

## Overview

Joshua Tint's research examines how Large Language Models respond to LGBTQ+ linguistic expression, investigating implicit biases in emotional content of model outputs across six major systems (GPT-3.5, GPT-4o, Llama2, Llama3, Gemma, Mistral). The paper addresses a critical gap in fairness-in-AI research by distinguishing between two linguistic contexts: heteronormative language (normative expressions assuming heterosexuality as default) and LGBTQ+ slang (community-specific linguistic markers). Rather than analyzing how LLMs handle queer *topics*, this study examines how models process language *used by* queer communities. This distinction is significant because LGBTQ+ individuals disproportionately rely on online spaces mediated by LLMs for connection and support, making systemic bias particularly consequential. The research challenges the assumption that current safety mechanisms and debiasing techniques adequately protect marginalized communities, arguing that these "guardrails" primarily address overt discrimination while overlooking subtler forms of bias affecting non-standard linguistic features.

## Main Findings

The study reveals a critical paradox: heteronormative language triggers safety mechanisms producing neutral or corrective responses, while LGBTQ+ slang elicits disproportionately negative emotional labels. This counterintuitive finding indicates that safety guardrails, designed to prevent harm, may inadvertently disadvantage minority linguistic communities. Specifically, the research demonstrates that current fairness approaches fail to address systemic biases in emotional responses to queer slang‚Äîmodels assign more negative sentiment to LGBTQ+ linguistic markers than to heteronormative equivalents. The findings suggest that equitable LLM outcomes require moving beyond eliminating explicit bigotry to addressing how models emotionally respond to marginalized communities' authentic language use. Critically, the paper emphasizes that safety mechanisms function as "guardrails" (restrictive boundaries) rather than "guidance" (constructive support), limiting rather than enabling equitable outcomes for minority communities.

## Methodology/Approach

The research employs two experiments measuring emotional content in LLM responses using sentiment analysis across six major models. The methodology operationalizes "emotional content" through sentiment valence measurement‚Äîquantifying positive versus negative emotional tone in model outputs. Rather than measuring explicit discriminatory outputs or safety filter triggers, this approach captures implicit bias through emotional labeling patterns. The study compares responses to heteronormative versus non-heteronormative prompts and examines how LGBTQ+ slang influences emotional valence. This measurement innovation enables detection of systemic inequities manifesting through emotional tone rather than overt content filtering. The approach distinguishes itself by focusing on language *used by* queer people rather than queer topics, and by examining implicit bias through emotional analysis rather than surface-level fairness assessments.

## Relevant Concepts

**Heteronormative Language**: Linguistic expressions assuming heterosexuality as the default or normative orientation, often invisible as a linguistic marker.

**LGBTQ+ Slang/Argot**: Community-specific linguistic markers and non-standard language features used by queer communities for identity expression and in-group communication.

**Implicit Bias**: Unconscious prejudices reflected in system outputs that don't constitute explicit discrimination but systematically disadvantage marginalized groups through subtle mechanisms.

**Emotional Content/Sentiment Valence**: Quantifiable measurement of positive versus negative emotional tone embedded in model responses, used as an indicator of systemic bias.

**Safety Mechanisms as Guardrails**: Automated restrictions designed to prevent harmful outputs that may inadvertently neutralize or negatively respond to marginalized communities' authentic language.

**Community-Driven Evaluation**: Fairness assessment frameworks incorporating perspectives from affected communities rather than relying solely on technical metrics.

## Significance

This research advances fairness-in-AI scholarship by introducing emotional content measurement as a bias detection tool and demonstrating that existing debiasing approaches are fundamentally insufficient. The findings have direct implications for LLM developers: achieving equitable outcomes requires redesigning safety mechanisms to avoid disadvantaging minority linguistic communities. The work contributes to emerging community-driven evaluation frameworks, emphasizing that technical solutions must be complemented by structural changes addressing how language technologies process marginalized communities' authentic expression. By demonstrating that guardrails can paradoxically harm the communities they're designed to protect, this research fundamentally challenges assumptions about fairness in contemporary language technologies. The distinction between guardrails (restrictive) and guidance (constructive) suggests that future approaches must shift from preventing harm to actively supporting equitable outcomes for marginalized linguistic communities.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://aclanthology.org/2025.queerinai-main.2.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/CWBWUWYL)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='toupin_2024_shaping_feminist_artificial_intelligence'></a>

## Paper 218/266: Shaping feminist artificial intelligence

**Source file:** `Toupin_2024_Shaping_feminist_artificial_intelligence.md`

---
title: "Shaping feminist artificial intelligence"
zotero_key: AMAZDU2T
author_year: "Toupin (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1177/14614448221150776"
url: "https://journals.sagepub.com/doi/full/10.1177/14614448221150776"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["AI Literacy", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-ai-komp-medium", "dim-vulnerable-medium", "dim-bias-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Shaping feminist artificial intelligence

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | AI Literacy, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Comprehensive examination of feminist artificial intelligence through historical analysis and contemporary typology development. Provides detailed framework categorizing FAI as: model, design, policy, culture, discourse, and science. Traces FAI's evolution from foundational work to contemporary initiatives, analyzing tensions between commercialized approaches and community-oriented methods.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1177/14614448221150776](https://doi.org/10.1177/14614448221150776)
- **URL:** https://journals.sagepub.com/doi/full/10.1177/14614448221150776
- **Zotero:** [Open in Zotero](zotero://select/items/AMAZDU2T)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='tun_2025_trust_in_artificial_intelligence‚Äìbased_clinical_de'></a>

## Paper 219/266: Trust in artificial intelligence‚Äìbased clinical decision support systems among health care workers: Systematic review

**Source file:** `Tun_2025_Trust_in_artificial_intelligence‚Äìbased_clinical_de.md`

---
title: "Trust in artificial intelligence‚Äìbased clinical decision support systems among health care workers: Systematic review"
zotero_key: FFAQTTR8
author_year: "Tun (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.2196/69678"
url: "https://www.jmir.org/2025/1/e69678"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 1
rel_bias: 2
rel_praxis: 2
rel_prof: 1
total_relevance: 7

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Practical Implementation"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-bias-medium", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Tun_2025_Trust.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Trust in artificial intelligence‚Äìbased clinical decision support systems among health care workers: Systematic review

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **7/15** (medium) |
| **Top Dimensions** | Bias Analysis, Practical Implementation |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Systematic review synthesizing 27 studies on clinicians' trust in AI decision-support tools. Identifies eight key factors shaping professional trust, notably system transparency as primary enabler. Lack of transparency was recurring barrier with "black-box" algorithms undermining trust. Improving transparency, providing explainable recommendations, and addressing bias/fairness issues recommended to bolster trust. Concludes transparent, explainable AI with proper training and ethical safeguards crucial for practitioner trust.


## AI Summary

## Overview

The PRISMA 2020 Checklist is a **standardized reporting guideline**‚Äînot empirical research‚Äîthat establishes normative requirements for documenting systematic reviews and meta-analyses. Comprising 27 mandatory reporting items distributed across six sections (Title, Abstract, Introduction, Methods, Results, Discussion), PRISMA 2020 functions as a meta-methodological governance tool rather than a substantive research contribution. Developed through consensus among methodologists, researchers, and stakeholders, this framework evolved from PRISMA 2009 to address emerging systematic review methodologies including network meta-analyses and qualitative syntheses. The checklist operationalizes a fundamental principle: systematic reviews require standardized, transparent documentation to enable critical appraisal, prevent selective reporting bias, and facilitate integration into broader evidence synthesis efforts. PRISMA 2020 has achieved normative authority as the gold standard adopted by major journals, funding agencies, and research institutions globally.

## Main Findings

The PRISMA 2020 Checklist establishes comprehensive reporting standards across four integrated domains. **Design specification** (Items 1-5) requires explicit documentation of review identification, abstract completeness, rationale, objectives, and eligibility criteria, establishing foundational transparency. **Methodological transparency** (Items 6-16) mandates detailed reporting of information sources, search strategies, study selection procedures, data extraction methods, and risk assessment protocols, enabling readers to evaluate methodological rigor and identify potential bias sources. **Results presentation** (Items 17-21) standardizes communication of study characteristics, synthesis methods, certainty of evidence assessments, and reporting bias evaluation. **Interpretation and accountability** (Items 22-27) requires transparent discussion of findings' implications, study limitations, funding sources, and protocol registration. The checklist implicitly concludes that adherence to these 27 items substantially improves systematic review quality, credibility, and utility while preventing publication bias and selective outcome reporting.

## Methodology/Approach

PRISMA 2020 employs a **normative-prescriptive approach** grounded in evidence-based consensus development. The framework utilizes iterative refinement, incorporating lessons from PRISMA 2009 and evolving systematic review practices across disciplines. Its modular structure accommodates specialized applications through separate checklists addressing abstracts, protocols, and specific review types (network meta-analyses, scoping reviews, qualitative syntheses). The methodology assumes that standardized reporting mechanisms enhance research quality through enforced methodological transparency and accountability. Implementation guidance accompanying each checklist item clarifies its rationale, application context, and relationship to research integrity. This prescriptive structure functions as a quality assurance mechanism enabling journals, funders, and institutions to evaluate and enforce minimum reporting standards.

## Relevant Concepts

**Transparency Imperative**: The foundational principle that systematic reviews require standardized reporting to enable external evaluation, replication, and critical appraisal across studies and disciplines.

**Structural Completeness**: Comprehensive documentation across defined sections ensures methodological rigor, prevents selective reporting bias, and establishes accountability for methodological decisions.

**Accessibility Framework**: Standardized checklists facilitate reader comprehension and enable consistent critical appraisal by establishing predictable reporting structures and terminology.

**Meta-Methodological Governance**: PRISMA occupies a governance role within evidence synthesis, shaping how scientific knowledge is synthesized and communicated rather than generating substantive empirical findings.

**Consensus-Driven Authority**: PRISMA 2020 derives legitimacy from stakeholder consensus and institutional adoption, functioning as normative infrastructure rather than empirically validated methodology.

**Prevention of Reporting Bias**: Explicit documentation requirements prevent selective outcome reporting, publication bias, and methodological opacity that compromise evidence synthesis integrity.

## Significance

PRISMA 2020 holds transformative significance within contemporary scientific practice and evidence-based medicine. It functions as the gold standard for systematic review reporting, achieving near-universal adoption by major journals (Cochrane, JAMA, Lancet), funding agencies (NIH, Wellcome Trust), and research institutions globally. The checklist operationalizes quality assurance mechanisms that prevent inadequately documented reviews from entering the evidence base, thereby protecting research integrity. By standardizing knowledge synthesis processes, PRISMA 2020 supports the broader evidence-based medicine movement, enhancing reliability and accessibility of synthesized evidence across healthcare, social sciences, and policy domains. Its evolving framework demonstrates responsiveness to emerging methodological challenges, including network meta-analyses, living reviews, and qualitative syntheses. Critically, PRISMA 2020 prevents selective reporting bias and publication bias by mandating transparent documentation of methodological decisions, search strategies, and outcome reporting. Ultimately, PRISMA 2020 represents consensus-driven governance infrastructure that fundamentally shapes how scientific knowledge is synthesized, evaluated, and communicated, making it foundational to evidence-based practice and research integrity across multiple disciplines.


## Links & Resources

- **DOI:** [10.2196/69678](https://doi.org/10.2196/69678)
- **URL:** https://www.jmir.org/2025/1/e69678
- **Zotero:** [Open in Zotero](zotero://select/items/FFAQTTR8)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='u.s._bureau_of_labor_statistics_2023_occupational_employment_statistics'></a>

## Paper 220/266: Occupational employment statistics

**Source file:** `U.S._Bureau_of_Labor_Statistics_2023_Occupational_employment_statistics.md`

---
title: "Occupational employment statistics"
zotero_key: 4A2Q3Y4H
author_year: "U.S. Bureau of Labor Statistics (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://www.bls.gov/oes/"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Occupational employment statistics

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.bls.gov/oes/
- **Zotero:** [Open in Zotero](zotero://select/items/4A2Q3Y4H)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='ulnicane_2024_artificial_intelligence_and_intersectionality'></a>

## Paper 221/266: Artificial Intelligence and Intersectionality

**Source file:** `Ulnicane_2024_Artificial_Intelligence_and_Intersectionality.md`

---
title: "Artificial Intelligence and Intersectionality"
zotero_key: 4RNTYLA5
author_year: "Ulnicane (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://ecpr.eu/news/news/details/749"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Artificial Intelligence and Intersectionality

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Diese Analyse untersucht, wie KI-Dokumente Bedenken √ºber Bias und Ungleichheit in KI rahmen und Empfehlungen zur Bek√§mpfung formulieren. Mittels intersektionaler Linse wird die Interaktion multipler Identit√§ten (Geschlecht, Rasse, Klasse) hervorgehoben, die zu Marginalisierung und Diskriminierung bestimmter sozialer Gruppen f√ºhrt. Die Studie unterscheidet zwischen technischen und sozio-technischen Framings von KI-Bias und zeigt auf, dass technische Frames KI oft als objektiv und neutral darstellen, w√§hrend sozio-technische Ans√§tze die sozialen, politischen und historischen Dimensionen von Bias anerkennen.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://ecpr.eu/news/news/details/749
- **Zotero:** [Open in Zotero](zotero://select/items/4RNTYLA5)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='ulnicane_2024_intersectionality_in_artificial_intelligence__fram'></a>

## Paper 222/266: Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action

**Source file:** `Ulnicane_2024_Intersectionality_in_artificial_intelligence__Fram.md`

---
title: "Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action"
zotero_key: JXKVY242
author_year: "Ulnicane (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: en
doi: "10.17645/si.7543"
url: "https://www.cogitatiopress.com/socialinclusion/article/view/7543"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Intersectionality in Artificial Intelligence: Framing Concerns and Recommendations for Action

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Diese Studie analysiert die aufkommende Agenda zu Intersektionalit√§t in AI durch Untersuchung von vier hochrangigen Berichten zu diesem Thema (2019-2021). Die Forschung zeigt, wie diese Dokumente Probleme rahmen und Empfehlungen zur Adressierung von Ungleichheiten formulieren. AI-Systeme verst√§rken und versch√§rfen oft menschliche Verzerrungen und Stereotypen, was zu Diskriminierung und Marginalisierung f√ºhrt. Die Analyse deckt systematische Probleme auf: Diversit√§tskrisen in AI-Entwicklung, wo Gr√ºnder und Mitarbeiter haupts√§chlich aus homogenen Gruppen wei√üer M√§nner stammen, sowie die Verst√§rkung bestehender Machtbeziehungen durch AI-Systeme. Die Studie betont die Notwendigkeit intersektionaler Ans√§tze, die multiple Ungleichheiten ber√ºcksichtigen, die aus der Interaktion verschiedener sozialer Identit√§ten entstehen. Empfehlungen umfassen partizipative AI-Design-Prinzipien und die Einbindung diverser Stakeholder in AI-Governance-Prozesse.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.17645/si.7543](https://doi.org/10.17645/si.7543)
- **URL:** https://www.cogitatiopress.com/socialinclusion/article/view/7543
- **Zotero:** [Open in Zotero](zotero://select/items/JXKVY242)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='un_women_2024_artificial_intelligence_and_gender_equality'></a>

## Paper 223/266: Artificial Intelligence and gender equality

**Source file:** `UN_Women_2024_Artificial_Intelligence_and_gender_equality.md`

---
title: "Artificial Intelligence and gender equality"
zotero_key: A4REJRN2
author_year: "UN Women (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://www.unwomen.org/en/articles/explainer/artificial-intelligence-and-gender-equality"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Women_2024_Artificial.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Artificial Intelligence and gender equality

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Comprehensive policy brief series analyzing how AI systems perpetuate gender inequalities while highlighting pathways for more equitable development. Documents that 44% of AI systems show gender bias, with 25% exhibiting both gender and racial bias, and provides evidence-based policy recommendations for addressing systematic underrepresentation and discriminatory outcomes.


## AI Summary

## Overview

This peer-reviewed paper by researchers at Epoch AI (with affiliations at University of Aberdeen, MIT CSAIL, Centre for the Governance of AI, and University of T√ºbingen) investigates a fundamental constraint facing artificial intelligence development: the potential exhaustion of publicly available human-generated text data for training large language models. The research addresses whether current LLM scaling trajectories can be sustained given finite data resources, or whether the field will encounter a critical bottleneck within this decade. By combining quantitative forecasting with analysis of existing datasets (RefinedWeb, C4, RedPajama) and established neural scaling laws, the authors provide both a timeline for data exhaustion and potential mitigation strategies. The work is significant because it bridges theoretical AI research with practical resource limitations, informing both technical development strategies and policy discussions around AI governance.

## Main Findings

The paper's central quantitative finding projects that the effective stock of publicly indexed human text‚Äîapproximately 4√ó10¬π‚Å¥ tokens‚Äîwill be fully utilized around 2028 under baseline assumptions, with a plausible range between 2026-2032 depending on model overtraining practices. This exhaustion point corresponds to approximately 5√ó10¬≤‚Å∏ FLOP of training compute for non-overtrained models; overtraining scenarios could accelerate exhaustion by several years. The research demonstrates that current LLM development trajectories, when plotted against available data supply, intersect at this median year (2028), indicating that scaling according to established neural scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022) cannot continue indefinitely using only public human-generated text. However, the authors argue this constraint is not absolute; they identify three primary mitigation pathways: synthetic data generation (creating artificial training data through computational methods), transfer learning from data-rich specialized domains (leveraging domain-specific corpora), and improvements in data efficiency (achieving better performance with less data through algorithmic innovation). This dual finding‚Äîacknowledging real constraints while proposing solutions‚Äîpositions the work as neither apocalyptic nor dismissive of genuine challenges.

## Methodology/Approach

The research employs a sophisticated dual-sided forecasting model integrating supply and demand analysis. The demand-side analysis projects future training dataset requirements by extrapolating observed LLM development patterns and applying established neural scaling laws, which mathematically relate model performance improvements to dataset expansion. The supply-side analysis quantifies the total stock of indexed public human text (data accessible through web indexing and curated corpora) by synthesizing historical internet growth data (Coffman & Odlyzko, 1998; Reinsel et al., 2018) with contemporary large-scale datasets. The methodology incorporates scenario analysis, including overtraining scenarios (training beyond theoretical optima) that could accelerate data exhaustion. Figure 1 provides visual synthesis, plotting projected dataset sizes against available stock to identify intersection points. This approach integrates historical data growth patterns with current AI development practices, creating a coherent forecasting framework suitable for policy and research planning.

## Relevant Concepts

**Neural Scaling Laws**: Mathematical relationships describing how model performance improves with increased training data, model size, and compute‚Äîfundamental to modern LLM development strategy and efficiency optimization.

**Effective Stock of Text**: The quantifiable amount of indexed, publicly available human-generated text suitable for LLM training, estimated at approximately 4√ó10¬π‚Å¥ tokens, derived from web pages and curated corpora.

**Data Exhaustion**: The projected point (circa 2028) at which demand for training data exceeds available supply under current development paradigms and scaling law assumptions.

**Synthetic Data Generation**: Creating artificial training data through computational methods to supplement or replace human-generated text, enabling continued scaling beyond natural data limits.

**Transfer Learning from Data-Rich Domains**: Leveraging specialized, high-quality datasets from specific fields (scientific literature, technical documentation) to improve model performance with limited general-purpose data.

**Data Efficiency Improvements**: Algorithmic and architectural innovations enabling models to achieve equivalent performance using smaller training datasets.

## Significance

This work significantly impacts AI development strategy, resource allocation decisions, and policy frameworks. It provides concrete timelines for addressing data constraints, enabling proactive research into alternative scaling methods before exhaustion occurs. The paper bridges technical AI research with governance implications, suggesting that resource scarcity‚Äîwhile real‚Äîmay catalyze innovation rather than halt progress. By identifying specific, implementable mitigation strategies and quantifying uncertainty ranges (2026-2032), it reframes the data constraint as a solvable challenge requiring strategic planning. The research informs both industry development priorities and policy discussions around AI safety, resource governance, and the sustainability of current scaling paradigms.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.unwomen.org/en/articles/explainer/artificial-intelligence-and-gender-equality
- **Zotero:** [Open in Zotero](zotero://select/items/A4REJRN2)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='unesco_2020_artificial_intelligence_and_gender_equality'></a>

## Paper 224/266: ARTIFICIAL INTELLIGENCE and GENDER EQUALITY

**Source file:** `UNESCO_2020_ARTIFICIAL_INTELLIGENCE_and_GENDER_EQUALITY.md`

---
title: "ARTIFICIAL INTELLIGENCE and GENDER EQUALITY"
zotero_key: ICV9Q7XF
author_year: "UNESCO (2020)"
authors: []

# Publication
publication_year: 2020.0
item_type: report
language: nan
doi: "nan"
url: "unesdoc.unesco.org/in/rest/annotationSVC/DownloadWatermarkedAttachment/attach_import_ab07646d-c784-4a4e-96a1-3be7855b6f76?_=374174eng.pdf&to=54&from=1"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# ARTIFICIAL INTELLIGENCE and GENDER EQUALITY

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2020.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

The present report builds on UNESCO‚Äôs previous work on gender equality and AI and aims to continue the conversation on this topic with a select group of experts from key stakeholder groups. In March 2019, UNESCO published a groundbreaking report, I‚Äôd Blush if I Could: closing gender divides in digital skills through education, based on research funded by the German Federal Ministry for Economic Cooperation and Development. This report featured recommendations on actions to overcome global gender gaps in digital skills, with a special examination of the impact of gender biases coded into some of the most prevalent AI applications.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** unesdoc.unesco.org/in/rest/annotationSVC/DownloadWatermarkedAttachment/attach_import_ab07646d-c784-4a4e-96a1-3be7855b6f76?_=374174eng.pdf&to=54&from=1
- **Zotero:** [Open in Zotero](zotero://select/items/ICV9Q7XF)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='unesco_2021_recommendation_on_the_ethics_of_artificial_intelli'></a>

## Paper 225/266: Recommendation on the Ethics of Artificial Intelligence

**Source file:** `UNESCO_2021_Recommendation_on_the_Ethics_of_Artificial_Intelli.md`

---
title: "Recommendation on the Ethics of Artificial Intelligence"
zotero_key: Y7HUDGKK
author_year: "UNESCO (2021)"
authors: []

# Publication
publication_year: 2021.0
item_type: report
language: nan
doi: "nan"
url: "https://unesdoc.unesco.org/ark:/48223/pf0000380455"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Recommendation on the Ethics of Artificial Intelligence

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2021.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

First global standard on AI ethics adopted by 193 member states, establishing comprehensive policy frameworks addressing gender equality in AI development and deployment. Explicitly addresses gender stereotyping, discriminatory biases, and need for equitable participation across the AI lifecycle, with recent implementation including Women4Ethical AI platform and systematic bias studies.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://unesdoc.unesco.org/ark:/48223/pf0000380455
- **Zotero:** [Open in Zotero](zotero://select/items/Y7HUDGKK)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='unesco_2024_bias_against_women_and_girls_in_large_language_mod'></a>

## Paper 226/266: Bias against women and girls in large language models: A UNESCO study

**Source file:** `UNESCO_2024_Bias_against_women_and_girls_in_large_language_mod.md`

---
title: "Bias against women and girls in large language models: A UNESCO study"
zotero_key: MMF4WBLF
author_year: "UNESCO (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://www.unesco.org/en/articles/generative-ai-unesco-study-reveals-alarming-evidence-regressive-gender-stereotypes"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_UNESCO_2024_Bias.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Bias against women and girls in large language models: A UNESCO study

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Die Studie analysiert LLMs (z. B. GPT-3.5, Llama 2) und dokumentiert signifikante Stereotypisierungen gegen Frauen und queere Menschen in generierten Texten. Besondere Auff√§lligkeit bei Open-Source-Modellen.


## AI Summary

## Overview

UNESCO's March 2024 press release presents a significant institutional investigation into systemic gender bias, homophobic content, and racial stereotyping within large language models (LLMs), the foundational technology underlying generative AI applications. The study examines three major systems‚ÄîGPT-3.5, GPT-2, and Llama 2‚Äîto determine whether these widely-used AI tools perpetuate retrograde gender stereotypes (outdated, regressive gender role expectations) and discriminatory patterns affecting women, LGBTQ+ individuals, and racial minorities. The research emerges strategically before International Women's Day, positioning AI bias as a contemporary social justice concern requiring multi-stakeholder intervention. UNESCO Director-General Audrey Azoulay emphasizes that millions of users interact daily with these systems in professional, educational, and domestic contexts, making even subtle biases consequential for real-world inequality amplification‚Äîthe mechanism through which algorithmic discrimination scales globally and reinforces existing social hierarchies.

## Main Findings

The research reveals quantifiable disparities across all examined models. **Gender findings:** Women appear in domestic roles at four times the frequency of men, with consistent semantic associations linking women to "home," "family," and "children," while men correlate with "work," "executive positions," "salary," and "employment." Narrative analysis demonstrates qualitative differences: AI-generated stories about men employ adventurous language ("sea," "forest," "treasure," "adventure"), while female narratives emphasize romantic and domestic vocabulary ("love," "garden," "cute," "husband," "rose"). **LGBTQ+ findings:** 70% of Llama 2's completions about gay individuals expressed negative sentiments, while 60% of GPT-2's outputs similarly generated homophobic content. **Model comparison:** Open-source models (Llama 2, GPT-2) exhibited more pronounced bias than proprietary systems (GPT-3.5, GPT-4, Google Gemini), though this transparency paradoxically enables collaborative correction opportunities. **Racial stereotyping:** The study documents systematic racial bias in content generation, though specific percentages remain undisclosed in this press release.

## Methodology/Approach

The study employs mixed-methods comparative content analysis combining quantitative and qualitative approaches across three major LLM systems. Researchers conducted systematic prompt completion tasks, requesting AI systems to generate narratives about diverse demographic groups across racial, sexual orientation, and gender categories. Quantitative measurement tracked role representation frequencies and semantic associations through linguistic pattern analysis (word frequency correlations). Qualitative narrative analysis examined generated content for thematic patterns and stereotypical framings. The methodological framework assumes that training data biases become embedded in model parameters during training, subsequently influencing user perceptions through repeated exposure to biased outputs at scale.

## Relevant Concepts

**Algorithmic bias:** Systematic discrimination embedded in AI systems through biased training data and design choices, producing discriminatory outputs at scale affecting millions of users.

**Large Language Models (LLMs):** Neural networks trained on vast text corpora to predict and generate human language, serving as infrastructure for generative AI applications like ChatGPT and Gemini.

**Semantic associations:** Linguistic patterns linking concepts through statistical co-occurrence in training data, reinforcing stereotypical connections (e.g., "woman" + "domestic," "man" + "professional").

**Bias amplification:** The mechanism through which algorithmic discrimination magnifies existing social inequalities through widespread deployment, user influence, and perception-shaping at population scale.

**Retrograde stereotypes:** Outdated, regressive gender role expectations that contradict contemporary gender equality standards, perpetuating historical power imbalances.

## Significance

This research carries substantial policy implications, advocating for explicit regulatory frameworks, continuous corporate bias monitoring, and international research collaboration aligned with UNESCO's 2021 AI ethics recommendations. The study directs recommendations toward three stakeholder groups: governments (regulatory framework development), private corporations (systematic bias monitoring), and research communities (collaborative bias mitigation). By demonstrating that open-source transparency enables collaborative bias correction while closed models obscure accountability, the research rejects technological determinism while emphasizing multi-stakeholder responsibility. The findings inform emerging governance debates about AI accountability, corporate responsibility, and international regulatory harmonization, establishing UNESCO as an authoritative voice in AI ethics discourse and positioning AI bias within critical social justice frameworks rather than treating it as purely technical.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.unesco.org/en/articles/generative-ai-unesco-study-reveals-alarming-evidence-regressive-gender-stereotypes
- **Zotero:** [Open in Zotero](zotero://select/items/MMF4WBLF)

## Related Concepts

- Algorithmic bias
- Large Language Models (LLMs)
- Semantic associations
- Bias amplification
- Retrograde stereotypes

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='unesco_2024_challenging_systematic_prejudices__an_investigatio'></a>

## Paper 227/266: Challenging systematic prejudices: an Investigation into Gender Bias in Large Language Models

**Source file:** `UNESCO_2024_Challenging_systematic_prejudices__an_Investigatio.md`

---
title: "Challenging systematic prejudices: an Investigation into Gender Bias in Large Language Models"
zotero_key: WD9KQG9J
author_year: "UNESCO (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://discovery.ucl.ac.uk/id/eprint/10188772/1/Unesco_results.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 7

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_UNESCO_2024_Challenging.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Challenging systematic prejudices: an Investigation into Gender Bias in Large Language Models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **7/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This study explores biases in three significant large language models (LLMs): OpenAI‚Äôs GPT-2 and ChatGPT, along with Meta‚Äôs Llama 2, highlighting their role in both advanced decision-making systems and as user-facing conversational agents. Across multiple studies, the brief reveals how biases emerge in the text generated by LLMs, through gendered word associations, positive or negative regard for gendered subjects, or diversity in text generated by gender and culture. The research uncovers persistent social biases within these state-of-the-art language models, despite ongoing efforts to mitigate such issues. The findings underscore the critical need for continuous research and policy intervention to address the biases that exacerbate as these technologies are integrated across diverse societal and cultural landscapes. The emphasis on GPT-2 and Llama 2 being open-source foundational models is particularly noteworthy, as their widespread adoption underlines the urgent need for scalable, objective methods to assess and correct biases, ensuring fairness in AI systems globally.


## AI Summary

## Overview

This UNESCO-IRCAI study, funded by the European Union's Horizon 2020 programme and published in 2024, systematically investigates gender bias within three major large language models: OpenAI's GPT-2 and ChatGPT, and Meta's Llama 2. The research directly addresses UNESCO's Ethics of AI Recommendation, which mandates that AI actors minimize discriminatory outcomes throughout system lifecycles. The study fundamentally reframes bias not as a technical anomaly but as systematic discrimination embedded within training data, model architectures, and deployment mechanisms. Published as open-access research, the work bridges academic investigation and policy implementation, establishing gender bias as central to responsible AI governance rather than peripheral concern.

## Main Findings

The research documents a critical paradox: despite substantial industry mitigation efforts, all three examined LLMs persistently embed and amplify gender-based discrimination. Bias manifests through three primary mechanisms. First, gendered word associations reveal systematic patterns linking women and girls to stereotypical roles, occupations, and attributes within model outputs. Second, sentiment analysis demonstrates differential valuation of gendered subjects‚Äîmodels exhibit measurably different positive or negative regard depending on gender framing of identical scenarios. Third, cross-cultural analysis reveals that gender bias operates intersectionally, with text generation varying significantly across cultural contexts, indicating that bias compounds across identity dimensions. Critically, the study concludes that current mitigation strategies remain fundamentally insufficient, indicating bias is structural rather than incidental. The findings suggest that technical approaches alone cannot address discrimination embedded at foundational levels of model training and architecture.

## Methodology/Approach

The study employs a rigorous multi-dimensional empirical framework integrating quantitative and qualitative analysis across three complementary dimensions. Gendered word association analysis examines semantic relationships and co-occurrence patterns within model outputs, identifying systematic linguistic patterns. Sentiment analysis measures differential treatment through natural language processing techniques evaluating positive/negative valuation of gendered subjects. Cross-cultural diversity analysis assesses how text generation varies across gender and cultural categories, capturing intersectional bias patterns. This methodology deliberately bridges technical AI analysis with social science perspectives, enabling identification of meaningful harms beyond statistical patterns. The theoretical framework explicitly positions bias mitigation as governance imperative requiring policy intervention, regulatory frameworks, and systemic redesign‚Äînot merely technical optimization.

## Relevant Concepts

**Algorithmic bias**: Systematic discrimination embedded in AI systems through training data, model design, or deployment contexts producing disparate outcomes for protected groups.

**Systemic bias**: Discrimination foundational to AI architecture and training processes, requiring comprehensive intervention rather than incremental technical fixes.

**Multi-level harm**: Gender bias consequences operating simultaneously at individual level (personalized discrimination), collective level (group stereotyping), and societal level (reinforcing structural inequalities).

**Intersectionality**: Recognition that gender bias operates differently across cultural contexts and compounds with other identity dimensions, producing amplified discrimination.

**Persistent bias paradox**: The phenomenon where contemporary LLMs continue embedding discrimination despite ongoing mitigation efforts, indicating current approaches are fundamentally inadequate.

**Normative AI governance**: Policy frameworks establishing ethical requirements for AI development and deployment, positioning fairness as regulatory obligation rather than optional enhancement.

## Significance

This research holds substantial significance for AI governance, policy development, and corporate accountability. By documenting persistent bias in widely-deployed systems affecting millions of users, the study challenges assumptions that technical sophistication ensures fairness. The UNESCO/IRCAI collaboration provides institutional authority elevating findings into policy-relevant territory, with implications for international AI regulation. The interdisciplinary authorship spanning machine learning, ethics, social science, and development studies demonstrates that addressing algorithmic bias requires integrated expertise beyond computer science. The open-access publication model maximizes research impact across academic, policy, and practitioner communities. Most critically, the work establishes gender bias as central to responsible AI governance, providing evidence-based justification for regulatory intervention, mandatory bias auditing, and comprehensive mitigation strategies extending beyond current industry self-regulation practices. The research contributes to growing consensus that algorithmic fairness requires systemic solutions rather than incremental technical improvements.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://discovery.ucl.ac.uk/id/eprint/10188772/1/Unesco_results.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/WD9KQG9J)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='unesco_2024_women4ethical_ai__global_cooperation_for_gender-in'></a>

## Paper 228/266: Women4Ethical AI: Global cooperation for gender-inclusive AI

**Source file:** `UNESCO_2024_Women4Ethical_AI__Global_cooperation_for_gender-in.md`

---
title: "Women4Ethical AI: Global cooperation for gender-inclusive AI"
zotero_key: 2JTAQTYV
author_year: "UNESCO (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://www.unesco.org/en/articles/unescos-women4ethical-ai-urges-global-cooperation-gender-inclusive-ai"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 2
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-medium", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_UNESCO_2024_Bias.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Women4Ethical AI: Global cooperation for gender-inclusive AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

UNESCO-Initiative zur F√∂rderung genderinklusiver KI-Entwicklung. Fokus auf globale Zusammenarbeit, Menschenrechtsprinzipien und Expertinnenbeteiligung in allen Phasen.


## AI Summary

!summary_UNESCO_2024_Bias.md


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.unesco.org/en/articles/unescos-women4ethical-ai-urges-global-cooperation-gender-inclusive-ai
- **Zotero:** [Open in Zotero](zotero://select/items/2JTAQTYV)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='unknown_2024_ai_competency_framework_for_students'></a>

## Paper 229/266: AI competency framework for students

**Source file:** `Unknown_2024_AI_competency_framework_for_students.md`

---
title: "AI competency framework for students"
zotero_key: UDU8GUD2
author_year: "Unknown (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: book
language: nan
doi: "nan"
url: "https://unesdoc.unesco.org/ark:/48223/pf0000391105"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI competency framework for students

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://unesdoc.unesco.org/ark:/48223/pf0000391105
- **Zotero:** [Open in Zotero](zotero://select/items/UDU8GUD2)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='unknown_2024_research_on_the_application_risks_and_countermeasu'></a>

## Paper 230/266: Research on the application risks and countermeasures of ChatGPT generative artificial intelligence in social work

**Source file:** `Unknown_2024_Research_on_the_application_risks_and_countermeasu.md`

---
title: "Research on the application risks and countermeasures of ChatGPT generative artificial intelligence in social work"
zotero_key: S2IMUC8M
author_year: "Unknown (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.23977/jaip.2024.070222"
url: "https://www.clausiuspress.com/article/12988.html"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Research on the application risks and countermeasures of ChatGPT generative artificial intelligence in social work

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.23977/jaip.2024.070222](https://doi.org/10.23977/jaip.2024.070222)
- **URL:** https://www.clausiuspress.com/article/12988.html
- **Zotero:** [Open in Zotero](zotero://select/items/S2IMUC8M)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='unknown_2024_rethinking_social_services_with_artificial_intelli'></a>

## Paper 231/266: RETHINKING SOCIAL SERVICES WITH ARTIFICIAL INTELLIGENCE: OPPORTUNITIES, RISKS, AND FUTURE PERSPECTIVES

**Source file:** `Unknown_2024_RETHINKING_SOCIAL_SERVICES_WITH_ARTIFICIAL_INTELLI.md`

---
title: "RETHINKING SOCIAL SERVICES WITH ARTIFICIAL INTELLIGENCE: OPPORTUNITIES, RISKS, AND FUTURE PERSPECTIVES"
zotero_key: UXUSIRST
author_year: "Unknown (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: bookSection
language: nan
doi: "nan"
url: "https://www.researchgate.net/publication/393509196_RETHINKING_SOCIAL_SERVICES_WITH_ARTIFICIAL_INTELLIGENCE_OPPORTUNITIES_RISKS_AND_FUTURE_PERSPECTIVES"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 1
rel_prof: 3
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Professional Context", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# RETHINKING SOCIAL SERVICES WITH ARTIFICIAL INTELLIGENCE: OPPORTUNITIES, RISKS, AND FUTURE PERSPECTIVES

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Professional Context, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

This chapter explores the transformative potential of artificial intelligence (AI) in the field of social services. It highlights how AI‚Äîthrough data analysis, predictive modeling, and administrative automation‚Äîcan enhance the effectiveness, accessibility, and efficiency of social work practice. The chapter also presents significant ethical concerns, including risks of algorithmic bias, loss of human connection, and violations of privacy. The author emphasizes that while AI can complement social work, it cannot replace the human-centered values at the core of the profession. It concludes by urging institutions and educational programs to prepare social workers for ethical and effective use of AI and calls for multidisciplinary collaboration to develop guidelines that ensure AI integration supports social justice, equality, and human rights.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.researchgate.net/publication/393509196_RETHINKING_SOCIAL_SERVICES_WITH_ARTIFICIAL_INTELLIGENCE_OPPORTUNITIES_RISKS_AND_FUTURE_PERSPECTIVES
- **Zotero:** [Open in Zotero](zotero://select/items/UXUSIRST)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='unknown_2025_artificial_intelligence_in_social_sciences_and_soc'></a>

## Paper 232/266: Artificial Intelligence in Social Sciences and Social Work: Bridging Technology and Humanity to Revolutionize Research, Policy, and Human Services

**Source file:** `Unknown_2025_Artificial_Intelligence_in_Social_Sciences_and_Soc.md`

---
title: "Artificial Intelligence in Social Sciences and Social Work: Bridging Technology and Humanity to Revolutionize Research, Policy, and Human Services"
zotero_key: 5ML7W6AU
author_year: "Unknown (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://www.multispecialityjournal.com/uploads/archives/20250920153430_MCR-2025-5-004.1.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 1
rel_prof: 2
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-prof-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Unknown_2025_Artificial.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Artificial Intelligence in Social Sciences and Social Work: Bridging Technology and Humanity to Revolutionize Research, Policy, and Human Services

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

This review explores the transformative role of artificial intelligence (AI) in the fields of social sciences and social work, with a focus on developments from 2022 to 2025. It examines how AI technologies‚Äîsuch as machine learning, natural language processing‚Äîenhance the analysis of complex social phenomena, support real-time forecasting, and inform data-driven policymaking. Within social work and human services, AI-driven tools facilitate case management, mental health interventions, crisis response, and resource allocation. While acknowledging AI's potential to improve equity and access, the article critically engages with ethical concerns around algorithmic bias, privacy, surveillance, and the erosion of human-centered care. Drawing on recent policy frameworks like the EU AI Act and UNESCO's AI Ethics Guidelines, the review calls for interdisciplinary collaboration to ensure the ethical, inclusive, and accountable integration of AI in social contexts.


## AI Summary

## Overview

This academic review examines the transformative integration of artificial intelligence technologies into social sciences and human services during 2022-2025. The document addresses a critical contemporary question: how can AI be ethically implemented across research methodologies, policy-making, and service delivery while protecting human dignity and advancing social justice? Authored by Reji TR (PhD Scholar, Madurai Kamaraj University), the review adopts a balanced, critical-pragmatic stance that neither dismisses AI's potential nor uncritically embraces technological solutions. By synthesizing recent scholarship across computer science, social work, policy analysis, and ethics, the author positions AI integration as an inevitable but governance-dependent development requiring interdisciplinary collaboration and robust ethical frameworks.

## Main Findings

The review identifies substantial AI applications across multiple social science disciplines and human services contexts. **Research methodology applications** include machine learning and natural language processing (NLP) for analyzing complex social phenomena, AI-driven simulations and digital twin models for real-time policy impact assessment, and automated literature synthesis and intelligent peer-review systems that enhance research integrity by reducing human bias. **Policy and forecasting applications** enable real-time trend forecasting and data-driven policymaking in sociology, political science, economics, and anthropology.

**Human services applications** demonstrate concrete benefits: automated case management systems improve resource allocation efficiency, risk assessment algorithms identify vulnerable populations at scale, mental health chatbots and NLP-enabled crisis intervention tools expand therapeutic capacity, and emerging technologies (blockchain, virtual reality, AI-powered virtual assistants) expand social support infrastructure.

However, significant implementation challenges accompany these opportunities. Algorithmic bias threatens equitable service delivery, privacy violations undermine client confidentiality, surveillance capabilities create dystopian risks, and automation risks eroding human-centered care‚Äîthe foundational principle of social work practice. The review concludes that ethical integration requires interdisciplinary collaboration, adherence to policy frameworks (EU AI Act, UNESCO AI Ethics Guidelines), sustained commitment to human dignity principles, and future research prioritizing socially responsible AI applications.

## Methodology/Approach

This narrative literature review synthesizes recent scholarship (2022-2025) through an explicitly interdisciplinary framework combining: computer science and AI ethics literature, social science research methodology, social work practice literature, and policy analysis. The analytical approach deliberately balances technological optimism with critical ethical examination, adopting a "socially responsible AI lens" that prioritizes human dignity and social justice. This framework acknowledges both AI's transformative potential and legitimate concerns about uncritical adoption, positioning the review within contemporary academic consensus that AI governance requires simultaneous technological advancement and ethical caution. The review explicitly bridges technical AI discourse with social science concerns, connecting abstract policy frameworks with practice-level implementation challenges.

## Relevant Concepts

**Algorithmic Bias:** Systematic errors in AI decision-making that disproportionately disadvantage specific populations, particularly vulnerable groups in social services contexts.

**Natural Language Processing (NLP):** AI technology enabling machines to understand, interpret, and generate human language, applied to crisis intervention, literature analysis, and peer-review processes.

**Predictive Analytics:** AI-driven forecasting techniques analyzing historical data to anticipate social and economic trends, enabling proactive policy interventions and resource allocation.

**Digital Twin Models:** AI-driven simulations replicating complex social dynamics, enabling real-time assessment of policy changes before implementation.

**Human-Centered Care:** Social work's foundational principle emphasizing individual dignity, autonomy, and relational engagement‚Äîpotentially threatened by automation and algorithmic decision-making.

**Socially Responsible AI:** AI development and deployment prioritizing ethical considerations, equity, transparency, and accountability alongside technical performance metrics.

**Critical-Pragmatic Positioning:** Analytical stance acknowledging technological potential while maintaining skepticism about uncritical adoption, balancing innovation with ethical accountability.

## Significance

This review addresses a critical gap in academic discourse by systematically examining AI's role in social contexts rather than purely technical applications. Its significance lies in: (1) bridging technical AI literature with social science concerns, (2) connecting policy frameworks with practice-level implementation challenges, (3) articulating how human-centered values can guide technological advancement, and (4) establishing that technological progress and ethical accountability are interdependent rather than competing imperatives. By emphasizing social justice and human dignity, the work aligns with critical social work traditions while engaging pragmatically with technological realities. The review provides essential guidance for practitioners, policymakers, and researchers navigating AI integration, establishing future research directions aimed at fostering socially responsible AI applications. Its critical-pragmatic positioning reflects emerging academic consensus that neither technophobic rejection nor uncritical enthusiasm serves social welfare objectives.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.multispecialityjournal.com/uploads/archives/20250920153430_MCR-2025-5-004.1.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/5ML7W6AU)

## Related Concepts

- Algorithmic Bias
- Natural Language Processing (NLP)
- Predictive Analytics
- Digital Twin Models
- Human-Centered Care
- Socially Responsible AI
- Critical-Pragmatic Positioning

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='unknown_2025_artificial_intelligence_in_social_work__an_epic_mo'></a>

## Paper 233/266: Artificial Intelligence in Social Work: An EPIC Model for Practice

**Source file:** `Unknown_2025_Artificial_Intelligence_in_Social_Work__An_EPIC_Mo.md`

---
title: "Artificial Intelligence in Social Work: An EPIC Model for Practice"
zotero_key: TBLSIEUC
author_year: "Unknown (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1080/0312407X.2025.2488345"
url: "https://www.tandfonline.com/doi/full/10.1080/0312407X.2025.2488345"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 2
rel_prof: 3
total_relevance: 11

# Categorization
relevance_category: high
top_dimensions: ["Professional Context", "AI Literacy"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-medium", "dim-vulnerable-medium", "dim-bias-medium", "dim-praxis-medium", "dim-prof-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Unknown_2025_Artificial.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Artificial Intelligence in Social Work: An EPIC Model for Practice

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **11/15** (high) |
| **Top Dimensions** | Professional Context, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

As artificial intelligence (AI) permeates the workplace environments of social workers, there is a need to understand the risks and benefits posed to the mission and values of the profession. This article examines the influence of artificial intelligence on the profession, including opportunities to advance socially just outcomes and challenges that risk ethical practice. A comprehensive review of literature was conducted to examine existing research on the intersection of AI and social work. Drawing on insights from this review, an EPIC model for integrating artificial intelligence into the profession is presented, consisting of four components: (E) ethics and justice; (P) policy development and advocacy; (I) intersectoral collaboration; and (C) community engagement and empowerment. The author contends that augmenting the benefits of artificial intelligence in social work requires a proactive and ethical approach towards a more secure, safe, transparent, and socially just future.


## AI Summary

## Overview

This academic review examines the transformative integration of artificial intelligence technologies into social sciences and human services during 2022-2025. The document addresses a critical contemporary question: how can AI be ethically implemented across research methodologies, policy-making, and service delivery while protecting human dignity and advancing social justice? Authored by Reji TR (PhD Scholar, Madurai Kamaraj University), the review adopts a balanced, critical-pragmatic stance that neither dismisses AI's potential nor uncritically embraces technological solutions. By synthesizing recent scholarship across computer science, social work, policy analysis, and ethics, the author positions AI integration as an inevitable but governance-dependent development requiring interdisciplinary collaboration and robust ethical frameworks.

## Main Findings

The review identifies substantial AI applications across multiple social science disciplines and human services contexts. **Research methodology applications** include machine learning and natural language processing (NLP) for analyzing complex social phenomena, AI-driven simulations and digital twin models for real-time policy impact assessment, and automated literature synthesis and intelligent peer-review systems that enhance research integrity by reducing human bias. **Policy and forecasting applications** enable real-time trend forecasting and data-driven policymaking in sociology, political science, economics, and anthropology.

**Human services applications** demonstrate concrete benefits: automated case management systems improve resource allocation efficiency, risk assessment algorithms identify vulnerable populations at scale, mental health chatbots and NLP-enabled crisis intervention tools expand therapeutic capacity, and emerging technologies (blockchain, virtual reality, AI-powered virtual assistants) expand social support infrastructure.

However, significant implementation challenges accompany these opportunities. Algorithmic bias threatens equitable service delivery, privacy violations undermine client confidentiality, surveillance capabilities create dystopian risks, and automation risks eroding human-centered care‚Äîthe foundational principle of social work practice. The review concludes that ethical integration requires interdisciplinary collaboration, adherence to policy frameworks (EU AI Act, UNESCO AI Ethics Guidelines), sustained commitment to human dignity principles, and future research prioritizing socially responsible AI applications.

## Methodology/Approach

This narrative literature review synthesizes recent scholarship (2022-2025) through an explicitly interdisciplinary framework combining: computer science and AI ethics literature, social science research methodology, social work practice literature, and policy analysis. The analytical approach deliberately balances technological optimism with critical ethical examination, adopting a "socially responsible AI lens" that prioritizes human dignity and social justice. This framework acknowledges both AI's transformative potential and legitimate concerns about uncritical adoption, positioning the review within contemporary academic consensus that AI governance requires simultaneous technological advancement and ethical caution. The review explicitly bridges technical AI discourse with social science concerns, connecting abstract policy frameworks with practice-level implementation challenges.

## Relevant Concepts

**Algorithmic Bias:** Systematic errors in AI decision-making that disproportionately disadvantage specific populations, particularly vulnerable groups in social services contexts.

**Natural Language Processing (NLP):** AI technology enabling machines to understand, interpret, and generate human language, applied to crisis intervention, literature analysis, and peer-review processes.

**Predictive Analytics:** AI-driven forecasting techniques analyzing historical data to anticipate social and economic trends, enabling proactive policy interventions and resource allocation.

**Digital Twin Models:** AI-driven simulations replicating complex social dynamics, enabling real-time assessment of policy changes before implementation.

**Human-Centered Care:** Social work's foundational principle emphasizing individual dignity, autonomy, and relational engagement‚Äîpotentially threatened by automation and algorithmic decision-making.

**Socially Responsible AI:** AI development and deployment prioritizing ethical considerations, equity, transparency, and accountability alongside technical performance metrics.

**Critical-Pragmatic Positioning:** Analytical stance acknowledging technological potential while maintaining skepticism about uncritical adoption, balancing innovation with ethical accountability.

## Significance

This review addresses a critical gap in academic discourse by systematically examining AI's role in social contexts rather than purely technical applications. Its significance lies in: (1) bridging technical AI literature with social science concerns, (2) connecting policy frameworks with practice-level implementation challenges, (3) articulating how human-centered values can guide technological advancement, and (4) establishing that technological progress and ethical accountability are interdependent rather than competing imperatives. By emphasizing social justice and human dignity, the work aligns with critical social work traditions while engaging pragmatically with technological realities. The review provides essential guidance for practitioners, policymakers, and researchers navigating AI integration, establishing future research directions aimed at fostering socially responsible AI applications. Its critical-pragmatic positioning reflects emerging academic consensus that neither technophobic rejection nor uncritical enthusiasm serves social welfare objectives.


## Links & Resources

- **DOI:** [10.1080/0312407X.2025.2488345](https://doi.org/10.1080/0312407X.2025.2488345)
- **URL:** https://www.tandfonline.com/doi/full/10.1080/0312407X.2025.2488345
- **Zotero:** [Open in Zotero](zotero://select/items/TBLSIEUC)

## Related Concepts

- Algorithmic Bias
- Natural Language Processing (NLP)
- Predictive Analytics
- Digital Twin Models
- Human-Centered Care
- Socially Responsible AI
- Critical-Pragmatic Positioning

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='unknown__feminist_ai___academy'></a>

## Paper 234/266: feminist AI | ACADEMY

**Source file:** `Unknown__feminist_AI___ACADEMY.md`

---
title: "feminist AI | ACADEMY"
zotero_key: H2W89APG
author_year: "Unknown ()"
authors: []

# Publication
publication_year: nan
item_type: webpage
language: en
doi: "nan"
url: "https://www.feminist-ai.com/academy"

# Assessment
decision: Exclude
exclusion_reason: "Wrong publication type"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Unknown_XXXX_feminist.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# feminist AI | ACADEMY

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | nan |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Wrong publication type


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Get ready for transforming power! We enable organizations to create more equitable AI through education. We hold workshops, give training, and provide learning material to raise awareness, build knowledge, and ease your creation of equitable AI.


## AI Summary

!summary_Unknown_2025_Artificial.md


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.feminist-ai.com/academy
- **Zotero:** [Open in Zotero](zotero://select/items/H2W89APG)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='van_toorn_2024_introduction_to_the_digital_welfare_state__contest'></a>

## Paper 235/266: Introduction to the digital welfare state: Contestations, considerations and entanglements

**Source file:** `van_Toorn_2024_Introduction_to_the_digital_welfare_state__Contest.md`

---
title: "Introduction to the digital welfare state: Contestations, considerations and entanglements"
zotero_key: RRVQGP5P
author_year: "van Toorn (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1177/14407833241260890"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 2
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-prof-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Introduction to the digital welfare state: Contestations, considerations and entanglements

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

Special issue introduction providing critical sociological analysis of digital welfare state, examining how datafication and automation amplify existing trends of surveillance and control over marginalized populations. Authors argue that contrary to neutral efficiency narratives, digital welfare technologies are embedded in fiscal austerity politics and criminalization of poverty. Employs power relations and human agency frameworks to demonstrate how algorithmic systems increase scrutiny of welfare recipients, migrants, and undeserving populations while prioritizing cost-cutting over meeting social needs. Critiques framing of AI as unprecedented innovation, situating digital welfare within historical dynamics of social control. Key themes include erosion of professional discretion, surveillance assemblages, and automated decision-making reproducing structural inequalities.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1177/14407833241260890](https://doi.org/10.1177/14407833241260890)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/RRVQGP5P)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='vethman_2025_fairness_beyond_the_algorithmic_frame__actionable_'></a>

## Paper 236/266: Fairness Beyond the Algorithmic Frame: Actionable Recommendations for an Intersectional Approach

**Source file:** `Vethman_2025_Fairness_Beyond_the_Algorithmic_Frame__Actionable_.md`

---
title: "Fairness Beyond the Algorithmic Frame: Actionable Recommendations for an Intersectional Approach"
zotero_key: Q2HILDPI
author_year: "Vethman (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "https://facctconference.org/static/docs/facct2025-206archivalpdfs/facct2025-final2348-acmpaginated.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Vethman_2025_Fairness.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Fairness Beyond the Algorithmic Frame: Actionable Recommendations for an Intersectional Approach

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This study develops actionable recommendations for AI experts to implement intersectional approaches beyond purely technical solutions. The authors criticize the reduction of intersectionality to algorithmic bias measurements between subgroups and develop a framework that emphasizes interdisciplinary teams, community participation, and analysis of power structures in social context. Their approach includes six core recommendations: responsible AI expert role, multidisciplinary team building, reflection on societal positioning, participatory community engagement, power and context analysis, and data-sensitive metrics.


## AI Summary

## Overview

This 2025 FAccT conference paper addresses a critical gap in AI fairness research: the reduction of intersectionality‚Äîa theoretical framework from Black Feminist scholarship examining interconnected systems of oppression‚Äîto narrow algorithmic bias metrics. The authors argue that current AI fairness approaches focus exclusively on technical solutions (bias detection, fairness metrics) while sidelining essential intersectional dimensions: power relations, social justice, and structural inequality. Real-world cases including the 2019 Dutch childcare benefits scandal (disproportionately targeting 26,000 families with dual nationality) and France's automated welfare fraud detection system (flagging low-income and immigrant populations) demonstrate how algorithmic systems compound historical discrimination against marginalized communities. The paper proposes that AI experts, occupying central roles in system development, bear responsibility for limiting unjust outcomes through genuinely intersectional approaches that extend beyond technical optimization.

## Main Findings

The study identifies five actionable themes for implementing intersectional AI practices:

1. **Mandate interdisciplinary collaboration** incorporating expertise from social sciences, humanities, and affected communities
2. **Embed reflexivity and recognize positionality** to acknowledge researchers' social positions and inherent biases
3. **Approach communities as co-owners** in design and deployment, not passive research subjects
4. **Engage with power dynamics and social context** to understand historical discrimination and structural inequality
5. **Critically assess data framing and metric limitations** to prevent bias perpetuation through measurement systems

Significant implementation barriers emerged: tech-optimism (belief that technical solutions suffice for social problems) and experts' fear of insufficient knowledge in social justice domains. Conversely, participating AI experts responded positively, valuing recommendations as practical tools for communicating intersectionality's importance and catalyzing institutional change toward more just AI practices.

## Methodology/Approach

The authors employed participatory mixed-methods research combining systematic thematic analysis of AI fairness literature with community engagement involving AI experts. This approach grounds abstract theoretical critique in institutional practice contexts. The theoretical framework explicitly draws from Black Feminist intersectionality scholarship, emphasizing how multiple identity categories (race, gender, class, disability) interact to create compounded discrimination, rather than treating demographic categories as isolated variables. Critically, the literature analysis was evaluated through expert participation, ensuring recommendations reflected both theoretical rigor and practical feasibility. This participatory dimension distinguishes the work from purely theoretical critique.

## Relevant Concepts

**Intersectionality:** Theoretical framework examining how multiple identity categories interact to create compounded forms of discrimination and privilege; originated in Black Feminist scholarship to address interconnected systems of oppression.

**Algorithmic frame:** The dominant approach in AI fairness research that reduces intersectionality to technical problems addressable through bias metrics and algorithmic adjustments, sidelining social justice and power dynamics.

**Algorithmic bias:** Systematic errors in AI systems disproportionately harming specific demographic groups, often reflecting historical discrimination embedded in training data and design choices.

**Positionality:** Recognition that researchers and practitioners occupy specific social positions shaped by identity, experience, and power relations, influencing their perspectives, decisions, and blind spots.

**Tech-optimism:** Belief that technological solutions can resolve complex social problems without addressing underlying structural inequalities and power imbalances.

**Co-ownership:** Collaborative models where affected communities participate as equal partners in decision-making, design, and governance rather than serving as research subjects or passive stakeholders.

**Structural inequality:** Systemic disadvantages embedded in institutions, policies, and practices that perpetuate discrimination across generations and demographic groups.

## Significance

This work makes substantial theoretical and practical contributions to responsible AI discourse by fundamentally challenging the dominant technical paradigm in AI fairness research. Rather than proposing new algorithms or metrics, it advocates for epistemological and structural transformation in how fairness is conceptualized and practiced. The paper bridges computer science and social justice scholarship, demonstrating that technical expertise alone cannot address discrimination rooted in historical and structural inequality. By grounding recommendations in expert feedback and community engagement, the authors translate critical theory into actionable institutional practice‚Äîessential for meaningful implementation beyond academic discourse. The work explicitly positions itself against "algorithmic solutionism," arguing that genuine fairness requires interdisciplinary collaboration, reflexivity, power analysis, and community co-ownership. This represents a significant contribution to FAccT scholarship and broader conversations about responsible AI development, offering concrete guidance for practitioners while maintaining theoretical rigor grounded in social justice frameworks.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://facctconference.org/static/docs/facct2025-206archivalpdfs/facct2025-final2348-acmpaginated.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/Q2HILDPI)

## Related Concepts

- Intersectionality
- Algorithmic frame
- Algorithmic bias
- Positionality
- Tech-optimism
- Co-ownership
- Structural inequality

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='victor_2023_recommendations_for_social_work_researchers_and_jo'></a>

## Paper 237/266: Recommendations for social work researchers and journal editors on the use of generative AI and large language models

**Source file:** `Victor_2023_Recommendations_for_social_work_researchers_and_jo.md`

---
title: "Recommendations for social work researchers and journal editors on the use of generative AI and large language models"
zotero_key: HUDLW3NU
author_year: "Victor (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "10.1086/726021"
url: "https://doi.org/10.1086/726021"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 1
rel_bias: 3
rel_praxis: 2
rel_prof: 3
total_relevance: 11

# Categorization
relevance_category: high
top_dimensions: ["Bias Analysis", "Professional Context"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-medium", "dim-bias-high", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Recommendations for social work researchers and journal editors on the use of generative AI and large language models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **11/15** (high) |
| **Top Dimensions** | Bias Analysis, Professional Context |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

Develops "disruptive-disrupting" framework for analyzing generative AI in social work research with concrete recommendations for researchers and journal editors. Emphasizes transparency and accountability requiring researchers to fully document AI use, disclose prompts, and take responsibility for AI-generated content. Argues that continuous education about AI construction, limitations, and ethical considerations is essential for developing appropriate trust in LLM systems.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1086/726021](https://doi.org/10.1086/726021)
- **URL:** https://doi.org/10.1086/726021
- **Zotero:** [Open in Zotero](zotero://select/items/HUDLW3NU)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='voutyrakou_2025_algorithmic_governance__gender_bias_in_ai-generate'></a>

## Paper 238/266: Algorithmic Governance: Gender Bias in AI-Generated Policymaking?

**Source file:** `Voutyrakou_2025_Algorithmic_Governance__Gender_Bias_in_AI-Generate.md`

---
title: "Algorithmic Governance: Gender Bias in AI-Generated Policymaking?"
zotero_key: LI4QASVU
author_year: "Voutyrakou (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1007/s44230-025-00109-2"
url: "https://doi.org/10.1007/s44230-025-00109-2"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Algorithmic Governance: Gender Bias in AI-Generated Policymaking?

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Examines whether gender-specific needs are reflected in AI-generated policies, demonstrating through GPT-4 and Copilot experiments that AI tends to overlook female-specific needs unless explicitly prompted. Highlights androcentric biases, advocating intersectionally-informed prompting to surface hidden biases but recognizing the limits of individual prompt-based solutions in addressing structural AI biases.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1007/s44230-025-00109-2](https://doi.org/10.1007/s44230-025-00109-2)
- **URL:** https://doi.org/10.1007/s44230-025-00109-2
- **Zotero:** [Open in Zotero](zotero://select/items/LI4QASVU)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='waag_2023_rationalisierung_durch_digitalisierung_'></a>

## Paper 239/266: Rationalisierung durch Digitalisierung?

**Source file:** `Waag_2023_Rationalisierung_durch_Digitalisierung_.md`

---
title: "Rationalisierung durch Digitalisierung?"
zotero_key: 9NHF43IT
author_year: "Waag (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "10.1007/s12592-023-00472-6"
url: "nan"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Rationalisierung durch Digitalisierung?

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Contributes labor sociology and interaction sociology perspectives (particularly Luhmann's interaction theory) to digitalization analyses in social work. Examines potential advantages and disadvantages from multiple stakeholder perspectives (professionals, service users, organizations), revealing that fears and hopes regarding rationalization through digitalization are overly simplistic. Highlights irreducible complexity of professional helping relationships and fundamental limitations of applying rationalization logic to social work contexts.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1007/s12592-023-00472-6](https://doi.org/10.1007/s12592-023-00472-6)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/9NHF43IT)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='wadmann_2020_'meaningless_work'__how_the_datafication_of_health'></a>

## Paper 240/266: 'Meaningless work': How the datafication of health reconfigures knowledge about work and erodes professional judgement

**Source file:** `Wadmann_2020_'Meaningless_work'__How_the_datafication_of_health.md`

---
title: "'Meaningless work': How the datafication of health reconfigures knowledge about work and erodes professional judgement"
zotero_key: X4F73VQZ
author_year: "Wadmann (2020)"
authors: []

# Publication
publication_year: 2020.0
item_type: journalArticle
language: nan
doi: "10.1177/0950017020950021"
url: "nan"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# 'Meaningless work': How the datafication of health reconfigures knowledge about work and erodes professional judgement

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2020.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Critical ethnographic study examining how datafication and digitalization in Denmark's healthcare sector erode professional judgment and create meaningless work through surveillance and control mechanisms. Authors challenge policy narratives that more data leads to better, evidence-based healthcare decisions, revealing instead how data-intensive practices create Kafkaesque idiocy reconfiguring perceptions of work and undermining goal orientation. Key critical insights demonstrate how dual ambitions of datafication introduce detailed surveillance tools regulating what counts as knowledge, forcing professionals into documentation activities divorced from care relationships. Reveals how data work serves administrative inspection rather than professional expertise, threatening autonomy essential to professional practice.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1177/0950017020950021](https://doi.org/10.1177/0950017020950021)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/X4F73VQZ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='wajcman_2023_feminism_confronts_ai__the_gender_relations_of_dig'></a>

## Paper 241/266: Feminism Confronts AI: The Gender Relations of Digitalisation

**Source file:** `Wajcman_2023_Feminism_Confronts_AI__The_Gender_Relations_of_Dig.md`

---
title: "Feminism Confronts AI: The Gender Relations of Digitalisation"
zotero_key: JL438SYR
author_year: "Wajcman (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: bookSection
language: nan
doi: "nan"
url: "https://academic.oup.com/book/55103/chapter/423909956"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Feminism Confronts AI: The Gender Relations of Digitalisation

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Wajcman and Young provide a feminist critique of AI, arguing that the technology is not neutral but deeply embedded in existing gendered power structures. They highlight the severe underrepresentation of women in AI development as a key source of bias, leading to the creation of systems that reflect and amplify a masculine worldview. The authors contend that simply adding more women to the field is insufficient. Instead, they call for a fundamental shift in the culture of technology production, challenging the technical-social dualism and integrating feminist perspectives into the very design of AI. This requires addressing the structural power asymmetries that shape technological development and moving beyond individualistic solutions.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://academic.oup.com/book/55103/chapter/423909956
- **Zotero:** [Open in Zotero](zotero://select/items/JL438SYR)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='walgenbach_2023_intersektionalit√§t'></a>

## Paper 242/266: Intersektionalit√§t

**Source file:** `Walgenbach_2023_Intersektionalit√§t.md`

---
title: "Intersektionalit√§t"
zotero_key: YVXN8TKM
author_year: "Walgenbach (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: bookSection
language: de
doi: "nan"
url: "https://wb-erwachsenenbildung.net/intersektionalitaet/"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: true
summary_file: "summary_Ma_2023_Intersectional.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Intersektionalit√§t

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://wb-erwachsenenbildung.net/intersektionalitaet/
- **Zotero:** [Open in Zotero](zotero://select/items/YVXN8TKM)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='wang_2023_measuring_user_competence_in_using_artificial_inte'></a>

## Paper 243/266: Measuring user competence in using artificial intelligence: validity and reliability of artificial intelligence literacy scale

**Source file:** `Wang_2023_Measuring_user_competence_in_using_artificial_inte.md`

---
title: "Measuring user competence in using artificial intelligence: validity and reliability of artificial intelligence literacy scale"
zotero_key: IHB58BIG
author_year: "Wang (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: en
doi: "10.1080/0144929X.2022.2072768"
url: "https://www.tandfonline.com/doi/full/10.1080/0144929X.2022.2072768"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Measuring user competence in using artificial intelligence: validity and reliability of artificial intelligence literacy scale

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1080/0144929X.2022.2072768](https://doi.org/10.1080/0144929X.2022.2072768)
- **URL:** https://www.tandfonline.com/doi/full/10.1080/0144929X.2022.2072768
- **Zotero:** [Open in Zotero](zotero://select/items/IHB58BIG)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='wang_2024_a_survey_on_fairness_in_large_language_models'></a>

## Paper 244/266: A survey on fairness in large language models

**Source file:** `Wang_2024_A_survey_on_fairness_in_large_language_models.md`

---
title: "A survey on fairness in large language models"
zotero_key: G5GRGK8Y
author_year: "Wang (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://file.mixpaper.cn/paper_store/2023/5ddea1cd-c031-433f-a09b-14e7754f7826.pdf"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# A survey on fairness in large language models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://file.mixpaper.cn/paper_store/2023/5ddea1cd-c031-433f-a09b-14e7754f7826.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/G5GRGK8Y)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='wang_2024_algorithmic_discrimination__examining_its_types_an'></a>

## Paper 245/266: Algorithmic discrimination: examining its types and regulatory measures with emphasis on US legal practices

**Source file:** `Wang_2024_Algorithmic_discrimination__examining_its_types_an.md`

---
title: "Algorithmic discrimination: examining its types and regulatory measures with emphasis on US legal practices"
zotero_key: K4XSNE3E
author_year: "Wang (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.3389/frai.2024.1320277"
url: "https://doi.org/10.3389/frai.2024.1320277"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Algorithmic discrimination: examining its types and regulatory measures with emphasis on US legal practices

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Diese umfassende Systematik identifiziert f√ºnf prim√§re Typen algorithmischer Diskriminierung: Bias durch algorithmische Agenten, diskriminierende Merkmalsselektion, Proxy-Diskriminierung, disparate Auswirkungen und gezielte Werbung. Die Analyse der US-Rechtslandschaft offenbart einen mehrstufigen Regulierungsansatz aus prinzipieller Regulierung, pr√§ventiven Kontrollen, konsequenter Haftung und Selbstregulierung. Zentral ist die Erkenntnis, dass unbeabsichtigte Diskriminierung durch scheinbar neutrale Algorithmen besonders schwer zu erkennen und zu regulieren ist, da sie strukturelle historische Ungleichheiten perpetuiert. Die Studie betont die Notwendigkeit interdisziplin√§rer Forschung und proaktiver Politikentwicklung.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.3389/frai.2024.1320277](https://doi.org/10.3389/frai.2024.1320277)
- **URL:** https://doi.org/10.3389/frai.2024.1320277
- **Zotero:** [Open in Zotero](zotero://select/items/K4XSNE3E)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='wang_2024_multilingual_prompting_for_improving_llm_generatio'></a>

## Paper 246/266: Multilingual Prompting for Improving LLM Generation Diversity

**Source file:** `Wang_2024_Multilingual_Prompting_for_Improving_LLM_Generatio.md`

---
title: "Multilingual Prompting for Improving LLM Generation Diversity"
zotero_key: PMHP5747
author_year: "Wang (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/html/2505.15229v1"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Multilingual Prompting for Improving LLM Generation Diversity

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

This paper introduces multilingual and multicultural prompting as methods to enhance the demographic and cultural diversity of Large Language Model outputs. The authors demonstrate these approaches outperform established diversity methods across multiple LLM architectures. Results indicate that prompting in culturally and linguistically aligned languages reduces hallucinated outputs and supports more representative generation.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2505.15229v1
- **Zotero:** [Open in Zotero](zotero://select/items/PMHP5747)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='wang_2025_multilingual_prompting_for_improving_llm_generatio'></a>

## Paper 247/266: Multilingual Prompting for Improving LLM Generation Diversity

**Source file:** `Wang_2025_Multilingual_Prompting_for_Improving_LLM_Generatio.md`

---
title: "Multilingual Prompting for Improving LLM Generation Diversity"
zotero_key: N8GSM8AD
author_year: "Wang (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/html/2505.15229v1"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Multilingual Prompting for Improving LLM Generation Diversity

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

This study introduces multilingual prompting as a strategy to enhance narrative diversity in LLM outputs. By using prompts with diverse languages and cultural cues, models produced outputs with improved demographic and opinion diversity. Compared to temperature-based and persona prompting, multilingual prompting was more effective and reduced cultural hallucinations.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2505.15229v1
- **Zotero:** [Open in Zotero](zotero://select/items/N8GSM8AD)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='washington_2025_fragile_foundations__hidden_risks_of_generative_ai'></a>

## Paper 248/266: Fragile Foundations: Hidden Risks of Generative AI

**Source file:** `Washington_2025_Fragile_Foundations__Hidden_Risks_of_Generative_AI.md`

---
title: "Fragile Foundations: Hidden Risks of Generative AI"
zotero_key: BALHXHPD
author_year: "Washington (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: en
doi: "10.11586/2025078"
url: "https://www.bertelsmann-stiftung.de/doi/10.11586/2025078"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Washington_2025_Fragile.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Fragile Foundations: Hidden Risks of Generative AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Foundation models are the backbone of generative AI and thus central to applications such as ChatGPT, Gemini, or Copilot. However, their use comes with risks: from randomly compiled training data and opaque processes to profit-driven business models. The new report Fragile Foundations: Hidden Risks of Generative AI by the Bertelsmann Stiftung shows why mission-driven organizations in particular should critically question the foundations of AI. It highlights the systemic weaknesses of foundation models, the dangers they pose to vulnerable groups, and the possible alternatives. This report illustrates why it is critical to scrutinize foundation models. It thus offers a starting point and impetus for decision-makers and practitioners in mission-driven organizations, as well as anyone committed to a responsible digital future. Using generative AI meaningfully in the service of the common good requires a clear understanding of the technology‚Äôs foundations ‚Äìand of the questions these raise for mission-driven organizations.


## AI Summary

## Overview

"Fragile Foundations: Hidden Risks of Generative AI" is a critical policy analysis by Dr. Anne L. Washington (Duke University), published by Bertelsmann Stiftung in September 2025. The document systematically examines structural vulnerabilities in foundation models‚Äîlarge-scale AI systems powering ChatGPT, Gemini, and similar applications. Washington argues that risks stem not from technical limitations alone but from systemic failures in data quality, business models, and computational design. The analysis addresses a critical governance gap by demonstrating how economic incentives and social biases become embedded in generative AI systems at scale, ultimately shaping digital infrastructure affecting billions of users. The work bridges academic AI ethics with actionable policy recommendations for policymakers and practitioners.

## Main Findings

Washington identifies four interconnected risk categories. **Data quality problems** reveal that training datasets contain unaddressed biases, poor curation, and representation gaps that propagate through AI systems with amplified consequences. **Business model constraints** demonstrate that proprietary development prioritizes rapid deployment and profit over safety and harm prevention. **False certainty** describes systematic user overestimation of AI reliability despite inherent limitations. **Structural barriers**‚Äîincluding resource-intensive computing, data monocultures, and recycled historical errors‚Äîcreate systemic obstacles to responsible development. Critically, foundation models amplify existing biases and automate cultural associations at unprecedented scale while externalizing environmental and computational costs. Washington proposes four solution categories: (1) **computational alternatives** improving efficiency; (2) **participatory alternatives** enabling inclusive design; (3) **source alternatives** ensuring diverse, deliberate representation; (4) **collaboration alternatives** supporting open development. Governance should emulate public libraries: transparent, deliberately curated, continuously improved, and publicly accessible rather than proprietary and static.

## Methodology/Approach

The document employs critical policy analysis synthesizing existing literature on AI risks and governance. Washington systematically categorizes structural barriers across data, business, computational, and cultural dimensions through comparative analysis contrasting current practices with proposed alternatives. The theoretical framework draws from critical AI studies, emphasizing how technical systems embed social and economic structures. Rather than conducting original empirical research, the analysis maps problems to solutions across four distinct frameworks. This methodology prioritizes accessibility for non-academic audiences while maintaining analytical rigor, bridging scholarship with implementation guidance.

## Relevant Concepts

**Foundation models:** Large-scale AI systems trained on vast datasets, versatile across applications

**Data monocultures:** Homogeneous training datasets lacking diversity, systematically amplifying embedded biases

**Deliberate representation:** Intentional, curated inclusion of diverse perspectives rather than passive diversity

**Data recycling:** Perpetuation of historical errors through retraining on contaminated datasets

**Cultural associations:** Automated encoding of social biases and stereotypes within model outputs

**Structural barriers:** Systemic obstacles‚Äînot individual failures‚Äîpreventing responsible AI development

**False certainty:** User overconfidence in AI reliability despite inherent uncertainties and limitations

**Public-interest models:** Alternative governance frameworks prioritizing transparency, accountability, and continuous improvement over profit maximization

## Significance

This work significantly advances critical AI governance discourse by reframing foundation model risks as structural rather than technical problems requiring systemic reform. It challenges the assumption that algorithmic improvements ensure responsible AI, instead advocating regulatory frameworks and alternative development models. The library-based governance metaphor provides concrete institutional alternatives to proprietary approaches. By synthesizing fragmented concerns into coherent analysis with explicit problem-solution mapping, Washington advances emerging consensus that responsible AI demands social and economic transformation alongside technological innovation. The policy-oriented approach makes critical scholarship accessible to decision-makers, potentially influencing regulatory development and corporate governance. Its emphasis on participatory design, deliberate representation, and collaborative development reflects recognition that foundation model risks require structural change, not merely technical optimization.


## Links & Resources

- **DOI:** [10.11586/2025078](https://doi.org/10.11586/2025078)
- **URL:** https://www.bertelsmann-stiftung.de/doi/10.11586/2025078
- **Zotero:** [Open in Zotero](zotero://select/items/BALHXHPD)

## Related Concepts

- Foundation models
- Data monocultures
- Deliberate representation
- Data recycling
- Cultural associations
- Structural barriers
- False certainty
- Public-interest models

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='weber_2023_messung_von_ai_literacy_‚Äì_empirische_evidenz_und_i'></a>

## Paper 249/266: Messung von AI Literacy ‚Äì Empirische Evidenz und Implikationen

**Source file:** `Weber_2023_Messung_von_AI_Literacy_‚Äì_Empirische_Evidenz_und_I.md`

---
title: "Messung von AI Literacy ‚Äì Empirische Evidenz und Implikationen"
zotero_key: PRSE2699
author_year: "Weber (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://aisel.aisnet.org/wi2023/3"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Messung von AI Literacy ‚Äì Empirische Evidenz und Implikationen

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://aisel.aisnet.org/wi2023/3
- **Zotero:** [Open in Zotero](zotero://select/items/PRSE2699)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='west_2023_discriminating_systems__gender,_race,_and_power_in'></a>

## Paper 250/266: Discriminating Systems: Gender, Race, and Power in AI

**Source file:** `West_2023_Discriminating_Systems__Gender,_Race,_and_Power_in.md`

---
title: "Discriminating Systems: Gender, Race, and Power in AI"
zotero_key: SIXWV7D4
author_year: "West (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: report
language: nan
doi: "nan"
url: "https://ainowinstitute.org/wp-content/uploads/2023/04/discriminatingsystems.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_West_2023_Discriminating.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Discriminating Systems: Gender, Race, and Power in AI

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Diese einflussreiche Studie argumentiert, dass die Diversit√§tskrise im KI-Sektor und Bias in KI-Systemen zwei Manifestationen desselben Problems sind und gemeinsam angegangen werden m√ºssen. Die Autoren zeigen, dass rein technische Ans√§tze zur Bias-Behebung unzureichend sind, da sie die systemischen Machtverh√§ltnisse ignorieren, die sowohl Arbeitspl√§tze als auch Technologien formen. Das "Pipeline-Problem"-Narrativ wird als zu eng kritisiert, da es tieferliegende Probleme mit Arbeitsplatzkultur, Machtasymmetrien und struktureller Diskriminierung nicht adressiert. Die Studie fordert eine Verschiebung von technischer "Debiasing" zu breiterer sozialer Analyse.


## AI Summary

## Overview

"Discriminating Systems: Gender, Race, and Power in AI" is a 2019 report by West, Whittaker, and Crawford from the AI Now Institute that investigates systemic discrimination within artificial intelligence development and deployment. The document establishes a critical causal relationship between workforce underrepresentation and algorithmic bias, arguing that the homogeneous demographics of AI creators directly produce discriminatory AI systems that reflect and amplify historical discrimination patterns. Rather than treating diversity and algorithmic fairness as separate concerns, the authors position them as interconnected manifestations of structural inequality requiring simultaneous institutional transformation. The report fundamentally challenges technocratic approaches to bias mitigation, asserting that technical solutions alone cannot address problems rooted in power asymmetries, workplace cultures, and systemic exclusion.

## Main Findings

The research reveals an acute diversity crisis across the AI sector with extreme underrepresentation. Women constitute only 18% of authors at leading AI conferences and represent merely 10-15% of AI research staff at major technology companies (Google and Facebook respectively). The situation is dramatically worse for Black workers, comprising only 2.5-4% of workforces at major tech firms, while no public data exists regarding trans or non-binary workers. The document demonstrates that decades of "pipeline problem" research and intervention have failed to produce meaningful progress, indicating that recruitment-focused solutions are fundamentally inadequate. Instead, the authors identify workplace culture, power asymmetries, harassment, exclusionary hiring practices, unfair compensation, and tokenization as primary drivers of attrition and exclusion. Critically, the research highlights how AI systems are actively deployed for "classification, detection, and prediction of race and gender," practices requiring urgent re-evaluation given their discriminatory potential. Additionally, narrow "women in tech" frameworks inadvertently privilege white women while marginalizing other intersecting identities, and binary gender assumptions in AI research systematically erase non-binary and transgender experiences.

## Methodology/Approach

The document employs mixed analytical methods combining empirical data aggregation with critical institutional analysis. Authors compile diversity statistics from leading AI conferences and major technology companies, providing quantitative evidence of disparity. Simultaneously, they employ intersectional feminist theory to examine how race, gender, and power dynamics intersect within institutional contexts. The framework incorporates historical contextualization, connecting contemporary algorithmic bias to historical patterns of discrimination and "race science." Critically, the methodology rejects technocratic problem-framing, instead emphasizing structural power dynamics, institutional barriers, and the interconnection between workforce composition and system outputs as primary analytical lenses.

## Relevant Concepts

**Intersectionality**: The analytical framework recognizing how multiple marginalized identities (race, gender, sexuality, etc.) interact and compound discrimination experiences, rather than existing independently.

**Pipeline Problem**: The conventional framing attributing diversity gaps to insufficient recruitment of underrepresented groups from educational pipelines, which the authors critique as inadequate and masking deeper structural issues.

**Discrimination Feedback Loop**: The bidirectional mechanism whereby homogeneous AI development teams create systems reflecting their biases and historical discrimination patterns, which subsequently reinforce and amplify inequality in broader society through algorithmic deployment.

**Tokenization**: The practice of including minimal representation of marginalized individuals without addressing systemic barriers, power structures, or workplace culture.

**Algorithmic Bias as Social Justice Issue**: The conceptual reframing positioning AI discrimination not as technical problem requiring engineering solutions, but as structural inequality requiring institutional transformation.

## Significance

This report represents a paradigm shift in AI ethics discourse, establishing that algorithmic bias is fundamentally a social justice issue rather than merely a technical problem. By demonstrating the causal relationship between workforce demographics and system outputs, the authors provide empirical grounding for structural critiques of technology development. The work challenges industry narratives of meritocracy and incremental progress, instead advocating for profound institutional transformation addressing workplace power dynamics, hiring practices, and retention simultaneously with algorithmic fairness. The document's emphasis on intersectionality and rejection of narrow "women in tech" frameworks has substantially influenced subsequent policy discussions and corporate accountability frameworks. Published by influential scholars at a leading AI ethics institute, this work has become foundational to contemporary AI ethics scholarship and policy development.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://ainowinstitute.org/wp-content/uploads/2023/04/discriminatingsystems.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/SIXWV7D4)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='wilson_2024_ai_tools_show_biases_in_ranking_job_applicants'_na'></a>

## Paper 251/266: AI tools show biases in ranking job applicants' names according to perceived race and gender

**Source file:** `Wilson_2024_AI_tools_show_biases_in_ranking_job_applicants'_na.md`

---
title: "AI tools show biases in ranking job applicants' names according to perceived race and gender"
zotero_key: ITG64UXK
author_year: "Wilson (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: conferencePaper
language: nan
doi: "nan"
url: "https://ojs.aaai.org/index.php/AIES/article/view/31748"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI tools show biases in ranking job applicants' names according to perceived race and gender

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Large-scale empirical study using over 550 resumes and 3+ million comparisons reveals that intersectional patterns of bias in AI resume screening cannot be understood as additive combinations of single-axis discrimination. Discovered unique harm against Black men invisible when examining race or gender independently‚ÄîBlack male names were never preferred over white male names (0% selection rate). Demonstrates co-constitutive nature of multiple discrimination where intersection of Blackness and masculinity creates distinct exclusion patterns.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://ojs.aaai.org/index.php/AIES/article/view/31748
- **Zotero:** [Open in Zotero](zotero://select/items/ITG64UXK)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='wilson_2024_gender,_race,_and_intersectional_bias_in_ai_resume'></a>

## Paper 252/266: Gender, race, and intersectional bias in AI resume screening via language model retrieval

**Source file:** `Wilson_2024_Gender,_race,_and_intersectional_bias_in_AI_resume.md`

---
title: "Gender, race, and intersectional bias in AI resume screening via language model retrieval"
zotero_key: X873LLEU
author_year: "Wilson (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: conferencePaper
language: nan
doi: "10.1609/aies.v7i1.31748"
url: "https://doi.org/10.1609/aies.v7i1.31748"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-high", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Gender, race, and intersectional bias in AI resume screening via language model retrieval

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Analyzes bias in large language models used for resume screening, examining over 550 job descriptions and 550 resumes across multiple demographic combinations. Reveals significant intersectional discrimination with Black male candidates facing most severe disadvantages, validating three key hypotheses of intersectionality theory through over 40,000 comparisons.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1609/aies.v7i1.31748](https://doi.org/10.1609/aies.v7i1.31748)
- **URL:** https://doi.org/10.1609/aies.v7i1.31748
- **Zotero:** [Open in Zotero](zotero://select/items/X873LLEU)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='world_economic_forum_2024_ai_for_impact__the_prism_framework_for_responsible'></a>

## Paper 253/266: AI for impact: The PRISM framework for responsible AI in social innovation

**Source file:** `World_Economic_Forum_2024_AI_for_impact__The_PRISM_framework_for_responsible.md`

---
title: "AI for impact: The PRISM framework for responsible AI in social innovation"
zotero_key: YSJRFESD
author_year: "World Economic Forum (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://www.weforum.org/publications/ai-for-impact-the-prism-framework-for-responsible-ai-in-social-innovation/"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 2
rel_bias: 1
rel_praxis: 3
rel_prof: 2
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Practical Implementation", "AI Literacy"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-ai-komp-medium", "dim-vulnerable-medium", "dim-praxis-high", "dim-prof-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# AI for impact: The PRISM framework for responsible AI in social innovation

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Practical Implementation, AI Literacy |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

Institutional report introducing PRISM framework specifically designed for social innovators, impact enterprises, and intermediaries working in social services sectors. Building on Presidio Framework of AI Governance Alliance, PRISM provides adoption pathways through which organizations can filter their impact mission, capabilities, and risks against AI technology use. Framework includes AI-enabled readiness assessment matrix enabling organizations to evaluate current practices and develop actionable roadmaps for AI integration both internally and externally. Emphasizes ethical adoption of AI aligned with social impact missions.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.weforum.org/publications/ai-for-impact-the-prism-framework-for-responsible-ai-in-social-innovation/
- **Zotero:** [Open in Zotero](zotero://select/items/YSJRFESD)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='wu_2025_bias_in_decision-making_for_ai's_ethical_dilemmas_'></a>

## Paper 254/266: Bias in decision-making for AI's ethical dilemmas: A comparative study of ChatGPT and Claude

**Source file:** `Wu_2025_Bias_in_decision-making_for_AI's_ethical_dilemmas_.md`

---
title: "Bias in decision-making for AI's ethical dilemmas: A comparative study of ChatGPT and Claude"
zotero_key: MKQZ27QE
author_year: "Wu (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/html/2501.10484v2"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Bias in decision-making for AI's ethical dilemmas: A comparative study of ChatGPT and Claude

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2501.10484v2
- **Zotero:** [Open in Zotero](zotero://select/items/MKQZ27QE)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='wudel_2025_what_is_feminist_ai_'></a>

## Paper 255/266: What is Feminist AI?

**Source file:** `Wudel_2025_What_is_Feminist_AI_.md`

---
title: "What is Feminist AI?"
zotero_key: 6F8C5RXW
author_year: "Wudel (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: report
language: nan
doi: "nan"
url: "https://library.fes.de/pdf-files/bueros/bruessel/21888-20250304.pdf"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 10

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: true
summary_file: "summary_Wudel_2025_What.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# What is Feminist AI?

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **10/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Defines Feminist AI (FAI) as framework using intersectional feminism to address bias and inequalities in AI systems. Emphasizes interdisciplinary collaboration, systemic power analysis, and iterative theory-practice loops. Embeds feminist values of equality, freedom, and justice to transform AI development. Includes practical applications like FemAI advocacy for feminist perspectives in EU AI Act and MIRA diagnostic platform aligning AI tools with social justice goals. Distinguishes FAI from traditional "Responsible AI" approaches through focus on structural power inequalities rather than individual "bad actors."


## AI Summary

## Overview

"What is Feminist AI?" is a policy-oriented academic document published by the Friedrich-Ebert-Stiftung's Competence Centre on the Future of Work in January 2025. Authored by Alexandra Wudel and Anna Ehrenberg, the paper introduces Feminist Artificial Intelligence (FAI) as a comprehensive framework for addressing systemic biases and power inequalities in AI development. The document positions FAI as both theoretical intervention and practical policy tool, grounded in intersectional feminist methodology rather than technical fixes alone. Published by a social democratic think tank, the work targets EU governance contexts while advancing broader arguments about the necessity of feminist approaches to technological development and the inadequacy of conventional "ethical AI" frameworks.

## Main Findings

The analysis reveals five critical findings. First, FAI fundamentally departs from conventional AI paradigms by prioritizing structural inequality reduction over surface-level bias mitigation, explicitly critiquing male-dominated, Western-centric biases embedded in traditional development. Second, practical FAI implementation requires a three-part strategy: (1) embedding feminist methodology to reduce power inequalities throughout development cycles; (2) ensuring AI systems meet concrete societal demands with demonstrable implementation potential; (3) establishing continuous mechanisms for systems to evolve with changing social contexts. Third, accountability and equity must be architecturally embedded from inception rather than applied retrospectively. Fourth, existing frameworks like "ethical AI" and "responsible AI" are insufficient because they fail to interrogate underlying structural power dynamics. Fifth, FAI demands genuine interdisciplinary collaboration and iterative theory-practice cycles, illustrated through concrete interventions: FemAI's advocacy for feminist perspectives in EU AI Act regulation and the MIRA diagnostic platform aligning AI tools with social justice objectives.

## Methodology/Approach

The paper employs intersectional feminism as theoretical foundation combined with systemic power analysis. The methodology operationalizes three interconnected elements: (1) **Interdisciplinary collaboration** bridging technical AI development with social science and feminist scholarship; (2) **Iterative theory-practice cycles** continuously testing and refining feminist principles against real-world implementation contexts; (3) **Continuous societal assessment** ensuring AI systems remain responsive to evolving social needs rather than crystallizing existing inequalities. Rather than remaining purely theoretical, authors ground FAI through case studies demonstrating how feminist methodology translates into actionable policy interventions and technological design practices.

## Relevant Concepts

**Intersectional Feminism**: Analytical framework recognizing how multiple, overlapping systems of oppression (gender, race, class, nationality) compound to shape experiences and outcomes; applied to understand how AI systems perpetuate compound inequalities across intersecting dimensions.

**Systemic Power Analysis**: Examination of structural power dynamics embedded in institutions, technologies, and development processes, focusing on how systems reproduce hierarchies rather than individual bias.

**Structural Inequality Reduction**: Moving beyond diversity metrics to fundamentally alter power relationships embedded in AI architecture, governance, and deployment.

**Theory-Practice Integration**: Deliberate bridging of academic feminist theory and practical technological implementation through continuous feedback and collaborative development cycles.

**Social Sustainability**: Ensuring AI systems serve diverse populations equitably over time and adapt to changing societal contexts without perpetuating harm.

**Accountability Mechanisms**: Embedded processes ensuring responsibility throughout development, not external oversight added post-deployment.

## Significance

This work holds substantial significance for multiple constituencies. For EU policy makers, it provides a framework for integrating feminist principles into AI governance and regulation. For technologists and developers, it challenges conventional practices by demanding structural accountability from inception. For scholars, it advances critical AI studies by demonstrating how feminist theory productively interrogates technological systems. For social justice advocates, it positions feminism as essential methodology rather than supplementary consideration. Most broadly, the document argues that achieving socially sustainable, equitable AI requires fundamental reimagining of development processes‚Äîmoving beyond incremental reforms toward structural transformation of how AI systems are conceptualized, governed, and deployed across society.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://library.fes.de/pdf-files/bueros/bruessel/21888-20250304.pdf
- **Zotero:** [Open in Zotero](zotero://select/items/6F8C5RXW)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='xu_2023_transparency_enhances_positive_perceptions_of_soci'></a>

## Paper 256/266: Transparency enhances positive perceptions of social artificial intelligence

**Source file:** `Xu_2023_Transparency_enhances_positive_perceptions_of_soci.md`

---
title: "Transparency enhances positive perceptions of social artificial intelligence"
zotero_key: FLY5W3P5
author_year: "Xu (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: journalArticle
language: nan
doi: "10.1155/2023/5550418"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 2
rel_vulnerable: 1
rel_bias: 1
rel_praxis: 2
rel_prof: 1
total_relevance: 7

# Categorization
relevance_category: medium
top_dimensions: ["AI Literacy", "Practical Implementation"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-ai-komp-medium", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Transparency enhances positive perceptions of social artificial intelligence

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Include** |
| **Total Relevance** | **7/15** (medium) |
| **Top Dimensions** | AI Literacy, Practical Implementation |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 2/3 | ‚≠ê‚≠ê Medium |
| Vulnerable Groups & Digital Equity | 1/3 | ‚≠ê Low |
| Bias & Discrimination Analysis | 1/3 | ‚≠ê Low |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Experimental study with 914 participants testing how transparent explanations about chatbot workings affect user perceptions. Results show transparency modestly improved trust-related perceptions: users found chatbot "less creepy," felt more affinity, and perceived it as more socially intelligent when transparent explanation provided. Transparency's impact on perceived intelligence stronger for users with low prior AI knowledge. Suggests prompt-engineering for greater transparency can foster user comfort and trust.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1155/2023/5550418](https://doi.org/10.1155/2023/5550418)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/FLY5W3P5)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='yan_2024_promises_and_challenges_of_generative_artificial_i'></a>

## Paper 257/266: Promises and challenges of generative artificial intelligence for human learning

**Source file:** `Yan_2024_Promises_and_challenges_of_generative_artificial_i.md`

---
title: "Promises and challenges of generative artificial intelligence for human learning"
zotero_key: E2SAEKPK
author_year: "Yan (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: en
doi: "10.1038/s41562-024-02004-5"
url: "https://www.nature.com/articles/s41562-024-02004-5"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Promises and challenges of generative artificial intelligence for human learning

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1038/s41562-024-02004-5](https://doi.org/10.1038/s41562-024-02004-5)
- **URL:** https://www.nature.com/articles/s41562-024-02004-5
- **Zotero:** [Open in Zotero](zotero://select/items/E2SAEKPK)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='yu_2025_algorithmic-assisted_decision-making_tools_in_chil'></a>

## Paper 258/266: Algorithmic-Assisted Decision-Making Tools in Child Welfare Practice: A Systematic Review

**Source file:** `Yu_2025_Algorithmic-Assisted_Decision-Making_Tools_in_Chil.md`

---
title: "Algorithmic-Assisted Decision-Making Tools in Child Welfare Practice: A Systematic Review"
zotero_key: GIHHDECQ
author_year: "Yu (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1177/10497315251350933"
url: "https://doi.org/10.1177/10497315251350933"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 3
rel_bias: 3
rel_praxis: 2
rel_prof: 3
total_relevance: 12

# Categorization
relevance_category: high
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "high-relevance", "dim-vulnerable-high", "dim-bias-high", "dim-praxis-medium", "dim-prof-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Algorithmic-Assisted Decision-Making Tools in Child Welfare Practice: A Systematic Review

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **12/15** (high) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 3/3 | ‚≠ê‚≠ê‚≠ê High |


## Abstract

PRISMA-guided review of algorithmic tools in child welfare. Finds potential for consistency and early risk identification but significant concerns about bias, transparency, practitioner training, and stakeholder inclusion. Recommends audits, participatory design, and ethical guidelines; highlights evidence gaps.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1177/10497315251350933](https://doi.org/10.1177/10497315251350933)
- **URL:** https://doi.org/10.1177/10497315251350933
- **Zotero:** [Open in Zotero](zotero://select/items/GIHHDECQ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='yuan_2025_the_cultural_stereotype_and_cultural_bias_of_chatg'></a>

## Paper 259/266: The cultural stereotype and cultural bias of ChatGPT

**Source file:** `Yuan_2025_The_cultural_stereotype_and_cultural_bias_of_ChatG.md`

---
title: "The cultural stereotype and cultural bias of ChatGPT"
zotero_key: KVL2SBWV
author_year: "Yuan (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1177/18344909251355673"
url: "https://www.researchgate.net/publication/393408216_The_cultural_stereotype_and_cultural_bias_of_ChatGPT"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 2
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "dim-praxis-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# The cultural stereotype and cultural bias of ChatGPT

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 2/3 | ‚≠ê‚≠ê Medium |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This article examines cultural biases in ChatGPT-3.5 and GPT-4. Study 1 measures alignment with human cultural values. Study 2 finds clear cultural stereotypes in GPT-3.5 but fewer in GPT-4. Study 3 tests four diversity-sensitive prompts (emphasizing individuality, fairness, egalitarian futures, or multiculturalism). All four strategies eliminated cultural stereotypes in GPT-3.5's outputs. For GPT-4, bias mitigation was more nuanced, requiring task-specific prompts. This indicates that while various prompts can reduce stereotypes, newer models may need more targeted strategies.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1177/18344909251355673](https://doi.org/10.1177/18344909251355673)
- **URL:** https://www.researchgate.net/publication/393408216_The_cultural_stereotype_and_cultural_bias_of_ChatGPT
- **Zotero:** [Open in Zotero](zotero://select/items/KVL2SBWV)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='yunusov_2024_mirrorstories__reflecting_diversity_through_person'></a>

## Paper 260/266: MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models

**Source file:** `Yunusov_2024_MirrorStories__Reflecting_Diversity_through_Person.md`

---
title: "MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models"
zotero_key: JEMUIGSX
author_year: "Yunusov (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/html/2409.13935v1"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high", "has-summary"]

# Summary
has_summary: true
summary_file: "summary_Yunusov_2024_MirrorStories.md"

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

This empirical study introduces a corpus of 1,500 personalized short stories generated with LLMs, incorporating identity features like gender, ethnicity, and age. Human judges rated these stories higher in engagement, diversity, and personalness. Narrative personalization increased textual diversity without harming moral comprehension. However, biases persist, such as preferential engagement for certain identities. The paper illustrates both potential and limitations of diversity-sensitive prompting.


## AI Summary

## Overview

This seminal paper presents critical improvements to the Skip-gram model, a neural network architecture for learning distributed word representations from large-scale unstructured text. The authors address two fundamental challenges: optimizing computational efficiency and representation quality of the Skip-gram model, and extending it to capture multi-word expressions that resist compositional interpretation. The work bridges theoretical advances in distributed semantics with practical engineering solutions, enabling scalable training on massive corpora (100+ billion words daily). By tackling both algorithmic efficiency and linguistic expressiveness, the paper establishes foundational techniques that became central to modern NLP systems.

## Main Findings

The research demonstrates three major technical achievements. First, subsampling frequent words during training‚Äîremoving words with probability 1 - ‚àö(t/f) where t is a threshold and f is word frequency‚Äîproduces 2-10x speedup while improving representation quality for less-frequent words by reducing noise from high-frequency terms that carry minimal semantic information. Second, negative sampling, a simplified variant of Noise Contrastive Estimation (NCE), replaces hierarchical softmax by sampling k negative examples rather than computing full probability distributions, delivering faster training and superior vector representations for frequent words. Third, the authors demonstrate that learning high-quality representations for millions of phrases is computationally feasible through data-driven phrase identification, addressing a critical limitation where compositional approaches fail for idiomatic expressions like "Boston Globe" (a newspaper, not a composition of "Boston" and "Globe").

## Methodology/Approach

The paper employs an empirical optimization framework combining three technical innovations. **Subsampling** probabilistically discards frequent words based on occurrence probability, reducing training data while preserving linguistic information and improving signal-to-noise ratio. **Negative sampling** replaces expensive hierarchical softmax with a contrastive objective: for each training pair, the model distinguishes the true context word from k randomly sampled negative examples, reducing computational complexity from O(V) to O(k) where V is vocabulary size. **Phrase detection** uses scoring mechanisms (unspecified in excerpt but data-driven) to identify multi-word expressions in raw text, enabling phrase-level embedding learning. The evaluation methodology emphasizes practical metrics: training speed improvements (2-10x) and representation quality comparisons. The theoretical foundation assumes linguistic patterns manifest as linear transformations in vector space, validated through analogical reasoning (vec('Madrid') - vec('Spain') + vec('France') ‚âà vec('Paris')).

## Relevant Concepts

**Distributed representations:** Vector-based word encodings capturing semantic and syntactic properties through learned numerical patterns in continuous vector space.

**Skip-gram model:** Neural architecture predicting context words from target words within fixed windows, enabling efficient unsupervised learning from raw text without labeled data.

**Negative sampling:** Contrastive learning technique distinguishing true word-context pairs from randomly sampled negative examples, reducing computational complexity from full softmax normalization to k negative samples.

**Noise Contrastive Estimation (NCE):** Statistical framework treating classification as distinguishing true data from noise; negative sampling is a simplified NCE variant.

**Compositionality failure:** Principle that certain phrase meanings cannot be derived from component word meanings; idiomatic expressions require dedicated phrase-level representations.

**Hierarchical softmax:** Prior normalization technique using binary tree structures for efficient probability computation; replaced by negative sampling for superior performance.

**Subsampling probability:** Formula 1 - ‚àö(t/f) removes frequent words probabilistically, where t is threshold parameter and f is word frequency, balancing training efficiency with information preservation.

## Significance

This work fundamentally transformed NLP by democratizing access to high-quality word embeddings through computational optimization. The subsampling technique and negative sampling became standard training procedures for embedding models, enabling practitioners to train on previously intractable datasets. The explicit treatment of phrase representations established that multi-word expressions require dedicated handling rather than compositional approaches, creating a research agenda for phrase embeddings. Historically, this paper (published 2013) catalyzed the embedding revolution, making distributed representations practical for industry applications. The techniques introduced‚Äîparticularly negative sampling‚Äîbecame foundational to subsequent advances in representation learning, influencing architectures from fastText to modern transformer-based models. By balancing theoretical insight with engineering pragmatism, the paper exemplifies how algorithmic innovation enables scientific progress in machine learning and established Word2Vec as the dominant embedding approach for nearly a decade.


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/html/2409.13935v1
- **Zotero:** [Open in Zotero](zotero://select/items/JEMUIGSX)

## Related Concepts

- Distributed representations
- Skip-gram model
- Negative sampling
- Noise Contrastive Estimation (NCE)
- Compositionality failure
- Hierarchical softmax
- Subsampling probability

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='zakharova_2024_tensions_in_digital_welfare_states__three_perspect'></a>

## Paper 261/266: Tensions in digital welfare states: Three perspectives on care and control

**Source file:** `Zakharova_2024_Tensions_in_digital_welfare_states__Three_perspect.md`

---
title: "Tensions in digital welfare states: Three perspectives on care and control"
zotero_key: EDM8628L
author_year: "Zakharova (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "10.1177/14407833241226800"
url: "nan"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 2
rel_praxis: 1
rel_prof: 2
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Vulnerable Groups", "Bias Analysis"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-medium", "dim-prof-medium"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Tensions in digital welfare states: Three perspectives on care and control

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Vulnerable Groups, Bias Analysis |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 2/3 | ‚≠ê‚≠ê Medium |


## Abstract

Examines tensions between care and control in digital welfare states, analyzing how welfare services increasingly rely on digital technologies and data systems. Develops three analytical perspectives: datafied care practices, algorithmic governance, and digitalized welfare encounters. Demonstrates how digitalization reshapes welfare provision by intensifying surveillance while potentially enabling new forms of care. Reveals fundamental contradictions where care logics and control logics coexist uneasily.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1177/14407833241226800](https://doi.org/10.1177/14407833241226800)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/EDM8628L)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='zannone_2023_intersectional_fairness__a_fractal_approach'></a>

## Paper 262/266: Intersectional Fairness: A Fractal Approach

**Source file:** `Zannone_2023_Intersectional_Fairness__A_Fractal_Approach.md`

---
title: "Intersectional Fairness: A Fractal Approach"
zotero_key: 2MY2F9CI
author_year: "Zannone (2023)"
authors: []

# Publication
publication_year: 2023.0
item_type: report
language: nan
doi: "nan"
url: "https://arxiv.org/abs/2302.12683"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Intersectional Fairness: A Fractal Approach

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2023.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Diese Studie rahmt intersektionale Fairness in einem geometrischen Setting und projiziert Daten auf einen Hyperkubus. Die Autoren beweisen mathematisch, dass Fairness "nach oben" propagiert - die Sicherstellung von Fairness f√ºr alle Untergruppen auf der niedrigsten intersektionalen Ebene f√ºhrt notwendigerweise zu Fairness auf allen h√∂heren Ebenen. Sie definieren eine Familie von Metriken zur Erfassung intersektionaler Verzerrung und schlagen vor, Fairness als "fraktales" Problem zu betrachten, bei dem Muster auf der kleinsten Skala auf gr√∂√üeren Skalen wiederholt werden. Dieser Bottom-up-Ansatz f√ºhrt zur nat√ºrlichen Entstehung fairer KI.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://arxiv.org/abs/2302.12683
- **Zotero:** [Open in Zotero](zotero://select/items/2MY2F9CI)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='zayed_2024_scaling_implicit_bias_analysis_across_transformer-'></a>

## Paper 263/266: Scaling implicit bias analysis across transformer-based language models through embedding association test and prompt engineering

**Source file:** `Zayed_2024_Scaling_implicit_bias_analysis_across_transformer-.md`

---
title: "Scaling implicit bias analysis across transformer-based language models through embedding association test and prompt engineering"
zotero_key: VBREN7SI
author_year: "Zayed (2024)"
authors: []

# Publication
publication_year: 2024.0
item_type: journalArticle
language: nan
doi: "nan"
url: "https://www.mdpi.com/2076-3417/14/8/3483"

# Assessment
decision: Exclude
exclusion_reason: "No full text"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Scaling implicit bias analysis across transformer-based language models through embedding association test and prompt engineering

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2024.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

No full text


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

nan


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [nan](https://doi.org/nan)
- **URL:** https://www.mdpi.com/2076-3417/14/8/3483
- **Zotero:** [Open in Zotero](zotero://select/items/VBREN7SI)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='zeng_2025_governing_discriminatory_content_in_conversational'></a>

## Paper 264/266: Governing discriminatory content in conversational AI: A cross-system, cross-lingual, and cross-topic audit

**Source file:** `Zeng_2025_Governing_discriminatory_content_in_conversational.md`

---
title: "Governing discriminatory content in conversational AI: A cross-system, cross-lingual, and cross-topic audit"
zotero_key: FDMJU7HF
author_year: "Zeng (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: nan
doi: "10.1080/1369118X.2025.2537803"
url: "https://doi.org/10.1080/1369118X.2025.2537803"

# Assessment
decision: Include
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 1
rel_vulnerable: 2
rel_bias: 3
rel_praxis: 1
rel_prof: 1
total_relevance: 8

# Categorization
relevance_category: medium
top_dimensions: ["Bias Analysis", "Vulnerable Groups"]

# Tags
tags: ["paper", "include", "medium-relevance", "dim-vulnerable-medium", "dim-bias-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Governing discriminatory content in conversational AI: A cross-system, cross-lingual, and cross-topic audit

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Include** |
| **Total Relevance** | **8/15** (medium) |
| **Top Dimensions** | Bias Analysis, Vulnerable Groups |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 1/3 | ‚≠ê Low |
| Vulnerable Groups & Digital Equity | 2/3 | ‚≠ê‚≠ê Medium |
| Bias & Discrimination Analysis | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Practical Implementation | 1/3 | ‚≠ê Low |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Conducts mixed-method audit of how major conversational AI systems respond to and regulate discriminatory content. Analysis is cross-system, cross-lingual, and cross-topic, revealing that refusal sensitivity and answering strategies vary significantly across all three axes. Discusses value alignment process through reinforcement learning with human feedback and implementation of guardrails, highlighting tensions when tech platforms become arbiters of morality.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1080/1369118X.2025.2537803](https://doi.org/10.1080/1369118X.2025.2537803)
- **URL:** https://doi.org/10.1080/1369118X.2025.2537803
- **Zotero:** [Open in Zotero](zotero://select/items/FDMJU7HF)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='zhang_2025_learning_about_ai__a_systematic_review_of_reviews_'></a>

## Paper 265/266: Learning About AI: A Systematic Review of Reviews on AI Literacy

**Source file:** `Zhang_2025_Learning_About_AI__A_Systematic_Review_of_Reviews_.md`

---
title: "Learning About AI: A Systematic Review of Reviews on AI Literacy"
zotero_key: 2PCEEUM8
author_year: "Zhang (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: journalArticle
language: en
doi: "10.1177/07356331251342081"
url: "https://journals.sagepub.com/doi/10.1177/07356331251342081"

# Assessment
decision: Exclude
exclusion_reason: "Not relevant topic"

# Relevance Scores (0-3)
rel_ai_komp: 0
rel_vulnerable: 0
rel_bias: 0
rel_praxis: 0
rel_prof: 0
total_relevance: 0

# Categorization
relevance_category: low
top_dimensions: []

# Tags
tags: ["paper", "exclude", "low-relevance"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Learning About AI: A Systematic Review of Reviews on AI Literacy

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Exclude** |
| **Total Relevance** | **0/15** (low) |
| **Top Dimensions** | None |


## Exclusion Reason

Not relevant topic


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 0/3 | ‚Äî None |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 0/3 | ‚Äî None |
| Practical Implementation | 0/3 | ‚Äî None |
| Professional/Social Work Context | 0/3 | ‚Äî None |


## Abstract

Given the ubiquity of artificial intelligence (AI), it is essential to empower students to become creators, designers, and producers of AI technologies, rather than limiting them to the role of informed consumers. To achieve this, learners need to be equipped with AI knowledge and concepts and develop AI literacy. Paradoxically, it is largely unclear what AI literacy is and how we should learn and teach it. We address both of these questions through a systematic review of systematic reviews, also known as an umbrella review, to gain a comprehensive understanding of AI literacy. After searching the literature, we critically examine the results of 17 reviews focusing on AI literacy and the teaching and learning of AI concepts. Our analysis revealed several encouraging developments: a general consensus on the definition of AI literacy, the availability of teaching tools and materials that support AI learning without prior programming experience, and effective pedagogical approaches that have shown positive effects on students' understanding and engagement. In addition, we identified several areas needing attention in the field: an interdisciplinary pedagogical approach, integration of ethical considerations in AI education, discussions on AI policy, and standardized, content-validated, reliable assessments across educational levels and cultures.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1177/07356331251342081](https://doi.org/10.1177/07356331251342081)
- **URL:** https://journals.sagepub.com/doi/10.1177/07356331251342081
- **Zotero:** [Open in Zotero](zotero://select/items/2PCEEUM8)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

<a id='zhao_2025_thinking_like_a_scientist__can_interactive_simulat'></a>

## Paper 266/266: Thinking like a scientist: Can interactive simulations foster critical AI literacy?

**Source file:** `Zhao_2025_Thinking_like_a_scientist__Can_interactive_simulat.md`

---
title: "Thinking like a scientist: Can interactive simulations foster critical AI literacy?"
zotero_key: EM3RD6NJ
author_year: "Zhao (2025)"
authors: []

# Publication
publication_year: 2025.0
item_type: conferencePaper
language: nan
doi: "10.1007/978-3-031-98417-4_5"
url: "nan"

# Assessment
decision: Unclear
exclusion_reason: "nan"

# Relevance Scores (0-3)
rel_ai_komp: 3
rel_vulnerable: 0
rel_bias: 2
rel_praxis: 3
rel_prof: 1
total_relevance: 9

# Categorization
relevance_category: medium
top_dimensions: ["AI Literacy", "Practical Implementation"]

# Tags
tags: ["paper", "unclear", "medium-relevance", "dim-ai-komp-high", "dim-bias-medium", "dim-praxis-high"]

# Summary
has_summary: false
summary_file: ""

# Metadata
date_added: 2025-11-10
source_tool: Manual
---

# Thinking like a scientist: Can interactive simulations foster critical AI literacy?

## Quick Info

| Attribute | Value |
|-----------|-------|
| **Authors** | Unknown |
| **Year** | 2025.0 |
| **Decision** | **Unclear** |
| **Total Relevance** | **9/15** (medium) |
| **Top Dimensions** | AI Literacy, Practical Implementation |


## Relevance Profile

| Dimension | Score | Assessment |
|-----------|-------|------------|
| AI Literacy & Competencies | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Vulnerable Groups & Digital Equity | 0/3 | ‚Äî None |
| Bias & Discrimination Analysis | 2/3 | ‚≠ê‚≠ê Medium |
| Practical Implementation | 3/3 | ‚≠ê‚≠ê‚≠ê High |
| Professional/Social Work Context | 1/3 | ‚≠ê Low |


## Abstract

Empirical study with 605 participants demonstrates that interactive simulations enhance critical AI literacy by engaging learners in scientific thinking processes including hypothesis testing and direct observation of AI behavior. Reveals that critical AI literacy requires understanding of fairness, dataset representativeness, and bias mechanisms in language models beyond technical knowledge. Establishes that effective AI literacy education must move beyond static instruction toward experiential engagement that fosters deep conceptual understanding of power structures embedded in AI systems.


## AI Summary

*No AI summary available. This paper was assessed but not yet processed through the summarization pipeline.*


## Links & Resources

- **DOI:** [10.1007/978-3-031-98417-4_5](https://doi.org/10.1007/978-3-031-98417-4_5)
- **URL:** nan
- **Zotero:** [Open in Zotero](zotero://select/items/EM3RD6NJ)

## Related Papers

*Use Obsidian graph view to explore papers with similar relevance profiles*

## Notes

*Add your research notes here*

---

# Concepts (144)

<a id='accountability_asymmetry'></a>

## Concept 1/144: Accountability Asymmetry

**Source file:** `Accountability_Asymmetry.md`

---
title: "Accountability Asymmetry"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Accountability Asymmetry

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Siapka_2023_Towards_a_Feminist_Metaethics_of_AI...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='ai_accountability'></a>

## Concept 2/144: Ai Accountability

**Source file:** `Ai_Accountability.md`

---
title: "Ai Accountability"
type: concept
frequency: 2
related_papers: 2
tags: [concept, auto-generated]
---

# Ai Accountability

**Frequency:** 2 summaries mention this concept
**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- European_Data_Protection_Supervisor_2023_Explainable_Artificial_Intelligence
- OECD_2023_Advancing_Accountability_in_AI


## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='ai_act'></a>

## Concept 3/144: Ai Act

**Source file:** `Ai_Act.md`

---
title: "Ai Act"
type: concept
frequency: 2
related_papers: 2
tags: [concept, auto-generated]
---

# Ai Act

**Frequency:** 2 summaries mention this concept
**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- Karagianni_2025_Gender_in_a_stereo-(gender)typical_EU_AI_law__A_fe
- Kattnig_2024_Assessing_trustworthy_AI__Technical_and_legal_pers


## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='ai_bias_mitigation'></a>

## Concept 4/144: Ai Bias Mitigation

**Source file:** `Ai_Bias_Mitigation.md`

---
title: "Ai Bias Mitigation"
type: concept
frequency: 2
related_papers: 1
tags: [concept, auto-generated]
---

# Ai Bias Mitigation

**Frequency:** 2 summaries mention this concept
**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Slesinger_2024_Training_in_Co-Creation_as_a_Methodological_Approa


## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='ai_bias_redefined'></a>

## Concept 5/144: AI Bias (redefined)

**Source file:** `AI_Bias_redefined.md`

---
title: "AI Bias (redefined)"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# AI Bias (redefined)

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Lin_2022_Artificial_Intelligence_in_a_Structurally_Unjust_S...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='ai_ethics'></a>

## Concept 6/144: Ai Ethics

**Source file:** `Ai_Ethics.md`

---
title: "Ai Ethics"
type: concept
frequency: 6
related_papers: 5
tags: [concept, auto-generated]
---

# Ai Ethics

**Frequency:** 6 summaries mention this concept
**Related papers:** 5

## Definition

*[This section can be manually expanded]*

## Related Papers

- Klein_2024_Data_Feminism_for_AI
- Linnemann_2023_Bedeutung_von_K√ºnstlicher_Intelligenz_in_der_Sozia
- Siapka_2023_Towards_a_Feminist_Metaethics_of_AI
- UNESCO_2024_Challenging_systematic_prejudices__an_Investigatio
- Unknown_2025_Artificial_Intelligence_in_Social_Work__An_EPIC_Mo


## Usage in Research

This concept appears in 6 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='ai_governance'></a>

## Concept 7/144: Ai Governance

**Source file:** `Ai_Governance.md`

---
title: "Ai Governance"
type: concept
frequency: 3
related_papers: 3
tags: [concept, auto-generated]
---

# Ai Governance

**Frequency:** 3 summaries mention this concept
**Related papers:** 3

## Definition

*[This section can be manually expanded]*

## Related Papers

- Biegelbauer_2023_Leitfaden_Digitale_Verwaltung_und_Ethik__Praxislei
- Himmelreich_2022_Artificial_Intelligence_and_Structural_Injustice__
- Washington_2025_Fragile_Foundations__Hidden_Risks_of_Generative_AI


## Usage in Research

This concept appears in 3 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='ai_safety'></a>

## Concept 8/144: AI Safety

**Source file:** `AI_Safety.md`

---
title: "AI Safety"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# AI Safety

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- McCrory_2024_Avoiding_catastrophe_through_intersectionality_...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='ai_transparency'></a>

## Concept 9/144: Ai Transparency

**Source file:** `Ai_Transparency.md`

---
title: "Ai Transparency"
type: concept
frequency: 2
related_papers: 1
tags: [concept, auto-generated]
---

# Ai Transparency

**Frequency:** 2 summaries mention this concept
**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- European_Data_Protection_Supervisor_2023_Explainable_Artificial_Intelligence


## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='ai_workforce_diversity'></a>

## Concept 10/144: AI Workforce Diversity

**Source file:** `Ai_Workforce_Diversity.md`

---
title: "AI Workforce Diversity"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# AI Workforce Diversity

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Quaid-i-Azam_University_2025_Gender_Bias_in_Artificial_Intel...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='algorithmic_bias'></a>

## Concept 11/144: Algorithmic bias

**Source file:** `Algorithmic_Bias.md`

---
title: "Algorithmic bias"
type: concept
related_papers: 3
tags: [concept, auto-generated]
---

# Algorithmic bias

**Related papers:** 3

## Definition

*[This section can be manually expanded]*

## Related Papers

- Gaba_2025_Bias,_accuracy,_and_trust__Gender-diverse_perspect...
- UNESCO_2024_Bias_against_women_and_girls_in_large_language_m...
- Vethman_2025_Fairness_Beyond_the_Algorithmic_Frame__Actionab...

## Usage in Research

This concept appears in 3 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='algorithmic_decision-making'></a>

## Concept 12/144: Algorithmic Decision-Making

**Source file:** `Algorithmic_Decision-Making.md`

---
title: "Algorithmic Decision-Making"
type: concept
frequency: 2
related_papers: 2
tags: [concept, auto-generated]
---

# Algorithmic Decision-Making

**Frequency:** 2 summaries mention this concept
**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- Kutscher_2020_Handbuch_Soziale_Arbeit_und_Digitalisierung
- Linnemann_2023_Bedeutung_von_K√ºnstlicher_Intelligenz_in_der_Sozia


## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='algorithmic_discrimination'></a>

## Concept 13/144: Algorithmic discrimination

**Source file:** `Algorithmic_Discrimination.md`

---
title: "Algorithmic discrimination"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Algorithmic discrimination

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Karagianni_2025_Gender_in_a_stereo-(gender)typical_EU_AI_law...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='algorithmic_fairness'></a>

## Concept 14/144: Algorithmic Fairness

**Source file:** `Algorithmic_Fairness.md`

---
title: "Algorithmic Fairness"
type: concept
frequency: 14
related_papers: 10
tags: [concept, auto-generated]
---

# Algorithmic Fairness

**Frequency:** 14 summaries mention this concept
**Related papers:** 10

## Definition

*[This section can be manually expanded]*

## Related Papers

- Chisca_2024_Prompting_techniques_for_reducing_social_bias_in_L
- DIVERSIFAIR_Project_2024_AI_&_Intersectionality__A_Toolkit_For_Fairness_&_I
- Djiberou_Mahamadou_2024_Revisiting_Technical_Bias_Mitigation_Strategies
- Hall_2024_A_systematic_review_of_sophisticated_predictive_an
- Kattnig_2024_Assessing_trustworthy_AI__Technical_and_legal_pers
- Ovalle_2023_Factoring_the_Matrix_of_Domination__A_Critical_Rev
- Slesinger_2024_Training_in_Co-Creation_as_a_Methodological_Approa
- UNESCO_2024_Challenging_systematic_prejudices__an_Investigatio
- Vethman_2025_Fairness_Beyond_the_Algorithmic_Frame__Actionable_
- Walgenbach_2023_Intersektionalit√§t


## Usage in Research

This concept appears in 14 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='algorithmic_frame'></a>

## Concept 15/144: Algorithmic frame

**Source file:** `Algorithmic_frame.md`

---
title: "Algorithmic frame"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Algorithmic frame

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Vethman_2025_Fairness_Beyond_the_Algorithmic_Frame__Actionab...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='algorithmic_opacity'></a>

## Concept 16/144: Algorithmic Opacity

**Source file:** `Algorithmic_Opacity.md`

---
title: "Algorithmic Opacity"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Algorithmic Opacity

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Klein_2024_Data_Feminism_for_AI...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='algorithmic_reparation'></a>

## Concept 17/144: Algorithmic Reparation

**Source file:** `Algorithmic_Reparation.md`

---
title: "Algorithmic Reparation"
type: concept
frequency: 3
related_papers: 3
tags: [concept, auto-generated]
---

# Algorithmic Reparation

**Frequency:** 3 summaries mention this concept
**Related papers:** 3

## Definition

*[This section can be manually expanded]*

## Related Papers

- Ghosal_2024_An_empirical_study_of_structural_social_and_ethica
- J√§√§skel√§inen_2025_Intersectional_analysis_of_visual_generative_AI__T
- Walgenbach_2023_Intersektionalit√§t


## Usage in Research

This concept appears in 3 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='artificial_intelligence'></a>

## Concept 18/144: Artificial Intelligence

**Source file:** `Artificial_Intelligence.md`

---
title: "Artificial Intelligence"
type: concept
frequency: 4
related_papers: 2
tags: [concept, auto-generated]
---

# Artificial Intelligence

**Frequency:** 4 summaries mention this concept
**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- Reamer_2023_Artificial_intelligence_in_social_work__Emerging_e
- Unknown_2025_Artificial_Intelligence_in_Social_Work__An_EPIC_Mo


## Usage in Research

This concept appears in 4 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='automated_decision_making'></a>

## Concept 19/144: Automated Decision-Making

**Source file:** `Automated_Decision_Making.md`

---
title: "Automated Decision-Making"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Automated Decision-Making

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- European_Data_Protection_Supervisor_2023_Explainable_Artific...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='batch_inference'></a>

## Concept 20/144: Batch Inference

**Source file:** `Batch_Inference.md`

---
title: "Batch Inference"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Batch Inference

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Lau_2023_Dipper__Diversity_in_Prompts_for_Producing_Large_L...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='bi_modal_priors'></a>

## Concept 21/144: Bi-modal Priors

**Source file:** `Bi_modal_Priors.md`

---
title: "Bi-modal Priors"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Bi-modal Priors

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Santy_2023_NLPositionality__Characterizing_design_biases_of_...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='bias_amplification'></a>

## Concept 22/144: Bias amplification

**Source file:** `Bias_amplification.md`

---
title: "Bias amplification"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Bias amplification

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- UNESCO_2024_Bias_against_women_and_girls_in_large_language_m...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='bias_mitigation'></a>

## Concept 23/144: Bias Mitigation

**Source file:** `Bias_Mitigation.md`

---
title: "Bias Mitigation"
type: concept
frequency: 5
related_papers: 2
tags: [concept, auto-generated]
---

# Bias Mitigation

**Frequency:** 5 summaries mention this concept
**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- Djiberou_Mahamadou_2024_Revisiting_Technical_Bias_Mitigation_Strategies
- Kattnig_2024_Assessing_trustworthy_AI__Technical_and_legal_pers


## Usage in Research

This concept appears in 5 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='black_box_effect'></a>

## Concept 24/144: Black Box Effect

**Source file:** `Black_Box_Effect.md`

---
title: "Black Box Effect"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Black Box Effect

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- European_Data_Protection_Supervisor_2023_Explainable_Artific...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='chain_of_thought_cot'></a>

## Concept 25/144: Chain-of-Thought (CoT)

**Source file:** `Chain_of_Thought_CoT.md`

---
title: "Chain-of-Thought (CoT)"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Chain-of-Thought (CoT)

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Lau_2023_Dipper__Diversity_in_Prompts_for_Producing_Large_L...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='chain_of_thought_cot_prompting'></a>

## Concept 26/144: Chain-of-Thought (CoT) Prompting

**Source file:** `Chain_of_Thought_CoT_Prompting.md`

---
title: "Chain-of-Thought (CoT) Prompting"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Chain-of-Thought (CoT) Prompting

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Kamruzzaman_2024_Prompting_techniques_for_reducing_social_bi...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='client_confidentiality'></a>

## Concept 27/144: Client Confidentiality

**Source file:** `Client_Confidentiality.md`

---
title: "Client Confidentiality"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Client Confidentiality

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Chen_2025_Social_work_and_artificial_intelligence__Collabora...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='co_ownership'></a>

## Concept 28/144: Co-ownership

**Source file:** `Co_ownership.md`

---
title: "Co-ownership"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Co-ownership

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Vethman_2025_Fairness_Beyond_the_Algorithmic_Frame__Actionab...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='colonial_epistemology'></a>

## Concept 29/144: Colonial Epistemology

**Source file:** `Colonial_Epistemology.md`

---
title: "Colonial Epistemology"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Colonial Epistemology

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Ovalle_2023_Factoring_the_Matrix_of_Domination__A_Critical_R...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='coloniality_of_power'></a>

## Concept 30/144: Coloniality of power

**Source file:** `Coloniality_of_power.md`

---
title: "Coloniality of power"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Coloniality of power

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Karagianni_2025_Gender_in_a_stereo-(gender)typical_EU_AI_law...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='community_co-design'></a>

## Concept 31/144: Community Co-Design

**Source file:** `Community_Co-Design.md`

---
title: "Community Co-Design"
type: concept
frequency: 2
related_papers: 2
tags: [concept, auto-generated]
---

# Community Co-Design

**Frequency:** 2 summaries mention this concept
**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- A+_Alliance_2024_Incubating_Feminist_AI__Executive_Summary_2021-202
- A+_Alliance_2024_Incubating_Feminist_AI__Executive_Summary_2021-202


## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='compositionality_failure'></a>

## Concept 32/144: Compositionality failure

**Source file:** `Compositionality_failure.md`

---
title: "Compositionality failure"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Compositionality failure

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Yunusov_2024_MirrorStories__Reflecting_Diversity_through_Per...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='consent_principle'></a>

## Concept 33/144: Consent Principle

**Source file:** `Consent_Principle.md`

---
title: "Consent Principle"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Consent Principle

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Klein_2024_Data_Feminism_for_AI...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='context_vector'></a>

## Concept 34/144: Context Vector

**Source file:** `Context_Vector.md`

---
title: "Context Vector"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Context Vector

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Santy_2023_NLPositionality__Characterizing_design_biases_of_...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='contextual_filtering'></a>

## Concept 35/144: Contextual Filtering

**Source file:** `Contextual_Filtering.md`

---
title: "Contextual Filtering"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Contextual Filtering

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Santy_2023_NLPositionality__Characterizing_design_biases_of_...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='critical_pragmatic_positioning'></a>

## Concept 36/144: Critical-Pragmatic Positioning

**Source file:** `Critical_Pragmatic_Positioning.md`

---
title: "Critical-Pragmatic Positioning"
type: concept
related_papers: 2
tags: [concept, auto-generated]
---

# Critical-Pragmatic Positioning

**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- Unknown_2025_Artificial_Intelligence_in_Social_Sciences_and_...
- Unknown_2025_Artificial_Intelligence_in_Social_Work__An_EPIC...

## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='cultural_associations'></a>

## Concept 37/144: Cultural associations

**Source file:** `Cultural_associations.md`

---
title: "Cultural associations"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Cultural associations

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Washington_2025_Fragile_Foundations__Hidden_Risks_of_Generat...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='current_harms'></a>

## Concept 38/144: Current Harms

**Source file:** `Current_Harms.md`

---
title: "Current Harms"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Current Harms

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- McCrory_2024_Avoiding_catastrophe_through_intersectionality_...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='data_justice'></a>

## Concept 39/144: Data Justice

**Source file:** `Data_Justice.md`

---
title: "Data Justice"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Data Justice

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Klein_2024_Data_Feminism_for_AI...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='data_monocultures'></a>

## Concept 40/144: Data monocultures

**Source file:** `Data_monocultures.md`

---
title: "Data monocultures"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Data monocultures

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Washington_2025_Fragile_Foundations__Hidden_Risks_of_Generat...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='data_recycling'></a>

## Concept 41/144: Data recycling

**Source file:** `Data_recycling.md`

---
title: "Data recycling"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Data recycling

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Washington_2025_Fragile_Foundations__Hidden_Risks_of_Generat...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='decision_making_transparency'></a>

## Concept 42/144: Decision-making Transparency

**Source file:** `Decision_making_Transparency.md`

---
title: "Decision-making Transparency"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Decision-making Transparency

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Chen_2025_Social_work_and_artificial_intelligence__Collabora...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='deliberate_representation'></a>

## Concept 43/144: Deliberate representation

**Source file:** `Deliberate_representation.md`

---
title: "Deliberate representation"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Deliberate representation

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Washington_2025_Fragile_Foundations__Hidden_Risks_of_Generat...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='digital_literacy'></a>

## Concept 44/144: Digital Literacy

**Source file:** `Digital_Literacy.md`

---
title: "Digital Literacy"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Digital Literacy

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Quaid-i-Azam_University_2025_Gender_Bias_in_Artificial_Intel...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='digital_twin_models'></a>

## Concept 45/144: Digital Twin Models

**Source file:** `Digital_Twin_Models.md`

---
title: "Digital Twin Models"
type: concept
related_papers: 2
tags: [concept, auto-generated]
---

# Digital Twin Models

**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- Unknown_2025_Artificial_Intelligence_in_Social_Sciences_and_...
- Unknown_2025_Artificial_Intelligence_in_Social_Work__An_EPIC...

## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='direct_debiasing'></a>

## Concept 46/144: Direct Debiasing

**Source file:** `Direct_Debiasing.md`

---
title: "Direct Debiasing"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Direct Debiasing

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Kamruzzaman_2024_Prompting_techniques_for_reducing_social_bi...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='distributed_representations'></a>

## Concept 47/144: Distributed representations

**Source file:** `Distributed_representations.md`

---
title: "Distributed representations"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Distributed representations

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Yunusov_2024_MirrorStories__Reflecting_Diversity_through_Per...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='distributed_responsibility'></a>

## Concept 48/144: Distributed Responsibility

**Source file:** `Distributed_Responsibility.md`

---
title: "Distributed Responsibility"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Distributed Responsibility

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Himmelreich_2022_Artificial_Intelligence_and_Structural_Inju...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='diversity_in_ai'></a>

## Concept 49/144: Diversity In Ai

**Source file:** `Diversity_In_Ai.md`

---
title: "Diversity In Ai"
type: concept
frequency: 2
related_papers: 1
tags: [concept, auto-generated]
---

# Diversity In Ai

**Frequency:** 2 summaries mention this concept
**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Unknown__feminist_AI___ACADEMY


## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='dual_component_framework'></a>

## Concept 50/144: Dual-Component Framework

**Source file:** `Dual_Component_Framework.md`

---
title: "Dual-Component Framework"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Dual-Component Framework

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Himmelreich_2022_Artificial_Intelligence_and_Structural_Inju...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='dual_process_theory'></a>

## Concept 51/144: Dual Process Theory

**Source file:** `Dual_Process_Theory.md`

---
title: "Dual Process Theory"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Dual Process Theory

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Kamruzzaman_2024_Prompting_techniques_for_reducing_social_bi...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='ensemble_methods'></a>

## Concept 52/144: Ensemble Methods

**Source file:** `Ensemble_Methods.md`

---
title: "Ensemble Methods"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Ensemble Methods

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Lau_2023_Dipper__Diversity_in_Prompts_for_Producing_Large_L...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='environmental_impact_principle'></a>

## Concept 53/144: Environmental Impact Principle

**Source file:** `Environmental_Impact_Principle.md`

---
title: "Environmental Impact Principle"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Environmental Impact Principle

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Klein_2024_Data_Feminism_for_AI...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='epistemic_pluralism'></a>

## Concept 54/144: Epistemic Pluralism

**Source file:** `Epistemic_Pluralism.md`

---
title: "Epistemic Pluralism"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Epistemic Pluralism

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Klein_2024_Data_Feminism_for_AI...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='ethical_governance'></a>

## Concept 55/144: Ethical Governance

**Source file:** `Ethical_Governance.md`

---
title: "Ethical Governance"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Ethical Governance

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Chen_2025_Social_work_and_artificial_intelligence__Collabora...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='ethics_washing'></a>

## Concept 56/144: Ethics-Washing

**Source file:** `Ethics_Washing.md`

---
title: "Ethics-Washing"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Ethics-Washing

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Siapka_2023_Towards_a_Feminist_Metaethics_of_AI...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='existential_risk'></a>

## Concept 57/144: Existential Risk

**Source file:** `Existential_Risk.md`

---
title: "Existential Risk"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Existential Risk

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- McCrory_2024_Avoiding_catastrophe_through_intersectionality_...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='explainability'></a>

## Concept 58/144: Explainability

**Source file:** `Explainability.md`

---
title: "Explainability"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Explainability

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- European_Data_Protection_Supervisor_2023_Explainable_Artific...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='explainable_ai'></a>

## Concept 59/144: Explainable Ai

**Source file:** `Explainable_Ai.md`

---
title: "Explainable Ai"
type: concept
frequency: 2
related_papers: 1
tags: [concept, auto-generated]
---

# Explainable Ai

**Frequency:** 2 summaries mention this concept
**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- European_Data_Protection_Supervisor_2023_Explainable_Artificial_Intelligence


## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='explainable_ai_xai'></a>

## Concept 60/144: Explainable AI (XAI)

**Source file:** `Explainable_AI_XAI.md`

---
title: "Explainable AI (XAI)"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Explainable AI (XAI)

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Chen_2025_Social_work_and_artificial_intelligence__Collabora...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='fair_ai_prompting'></a>

## Concept 61/144: Fair Ai Prompting

**Source file:** `Fair_Ai_Prompting.md`

---
title: "Fair Ai Prompting"
type: concept
frequency: 2
related_papers: 1
tags: [concept, auto-generated]
---

# Fair Ai Prompting

**Frequency:** 2 summaries mention this concept
**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Unknown__feminist_AI___ACADEMY


## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='false_certainty'></a>

## Concept 62/144: False certainty

**Source file:** `False_certainty.md`

---
title: "False certainty"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# False certainty

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Washington_2025_Fragile_Foundations__Hidden_Risks_of_Generat...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='feminist_ai'></a>

## Concept 63/144: Feminist Ai

**Source file:** `Feminist_Ai.md`

---
title: "Feminist Ai"
type: concept
frequency: 3
related_papers: 3
tags: [concept, auto-generated]
---

# Feminist Ai

**Frequency:** 3 summaries mention this concept
**Related papers:** 3

## Definition

*[This section can be manually expanded]*

## Related Papers

- A+_Alliance_2024_Incubating_Feminist_AI__Executive_Summary_2021-202
- A+_Alliance_2024_Incubating_Feminist_AI__Executive_Summary_2021-202
- Wudel_2025_What_is_Feminist_AI_


## Usage in Research

This concept appears in 3 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='feminist_metaethics'></a>

## Concept 64/144: Feminist Metaethics

**Source file:** `Feminist_Metaethics.md`

---
title: "Feminist Metaethics"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Feminist Metaethics

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Siapka_2023_Towards_a_Feminist_Metaethics_of_AI...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='feminist_policy_analysis'></a>

## Concept 65/144: Feminist Policy Analysis

**Source file:** `Feminist_Policy_Analysis.md`

---
title: "Feminist Policy Analysis"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Feminist Policy Analysis

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- McCrory_2024_Avoiding_catastrophe_through_intersectionality_...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='foundation_models'></a>

## Concept 66/144: Foundation models

**Source file:** `Foundation_models.md`

---
title: "Foundation models"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Foundation models

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Washington_2025_Fragile_Foundations__Hidden_Risks_of_Generat...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='gender_based_violence'></a>

## Concept 67/144: Gender-based violence

**Source file:** `Gender_based_violence.md`

---
title: "Gender-based violence"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Gender-based violence

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Karagianni_2025_Gender_in_a_stereo-(gender)typical_EU_AI_law...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='gender_bias'></a>

## Concept 68/144: Gender Bias

**Source file:** `Gender_Bias.md`

---
title: "Gender Bias"
type: concept
frequency: 6
related_papers: 4
tags: [concept, auto-generated]
---

# Gender Bias

**Frequency:** 6 summaries mention this concept
**Related papers:** 4

## Definition

*[This section can be manually expanded]*

## Related Papers

- Chisca_2024_Prompting_techniques_for_reducing_social_bias_in_L
- Friedrich-Ebert-Stiftung_2025_The_EU_artificial_intelligence_act_through_a_gende
- Karagianni_2025_Gender_in_a_stereo-(gender)typical_EU_AI_law__A_fe
- UNESCO_2024_Challenging_systematic_prejudices__an_Investigatio


## Usage in Research

This concept appears in 6 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='gender_bias_in_ai'></a>

## Concept 69/144: Gender Bias in AI

**Source file:** `Gender_Bias_In_Ai.md`

---
title: "Gender Bias in AI"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Gender Bias in AI

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Quaid-i-Azam_University_2025_Gender_Bias_in_Artificial_Intel...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='gender_identity'></a>

## Concept 70/144: Gender identity

**Source file:** `Gender_identity.md`

---
title: "Gender identity"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Gender identity

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Gaba_2025_Bias,_accuracy,_and_trust__Gender-diverse_perspect...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='gender_justice'></a>

## Concept 71/144: Gender Justice

**Source file:** `Gender_Justice.md`

---
title: "Gender Justice"
type: concept
frequency: 2
related_papers: 2
tags: [concept, auto-generated]
---

# Gender Justice

**Frequency:** 2 summaries mention this concept
**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- A+_Alliance_2024_Incubating_Feminist_AI__Executive_Summary_2021-202
- A+_Alliance_2024_Incubating_Feminist_AI__Executive_Summary_2021-202


## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='gender_responsive_education_policies'></a>

## Concept 72/144: Gender-Responsive Education Policies

**Source file:** `Gender_Responsive_Education_Policies.md`

---
title: "Gender-Responsive Education Policies"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Gender-Responsive Education Policies

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Quaid-i-Azam_University_2025_Gender_Bias_in_Artificial_Intel...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='gendered_prompts'></a>

## Concept 73/144: Gendered prompts

**Source file:** `Gendered_prompts.md`

---
title: "Gendered prompts"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Gendered prompts

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Gaba_2025_Bias,_accuracy,_and_trust__Gender-diverse_perspect...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='generative_ai'></a>

## Concept 74/144: Generative Ai

**Source file:** `Generative_Ai.md`

---
title: "Generative Ai"
type: concept
frequency: 9
related_papers: 4
tags: [concept, auto-generated]
---

# Generative Ai

**Frequency:** 9 summaries mention this concept
**Related papers:** 4

## Definition

*[This section can be manually expanded]*

## Related Papers

- Ghosal_2024_An_empirical_study_of_structural_social_and_ethica
- J√§√§skel√§inen_2025_Intersectional_analysis_of_visual_generative_AI__T
- Unknown__feminist_AI___ACADEMY
- Walgenbach_2023_Intersektionalit√§t


## Usage in Research

This concept appears in 9 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='hallucinations'></a>

## Concept 75/144: Hallucinations

**Source file:** `Hallucinations.md`

---
title: "Hallucinations"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Hallucinations

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- European_Data_Protection_Supervisor_2023_Explainable_Artific...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='hermeneutical_injustice'></a>

## Concept 76/144: Hermeneutical injustice

**Source file:** `Hermeneutical_injustice.md`

---
title: "Hermeneutical injustice"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Hermeneutical injustice

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Karagianni_2025_Gender_in_a_stereo-(gender)typical_EU_AI_law...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='hierarchical_softmax'></a>

## Concept 77/144: Hierarchical softmax

**Source file:** `Hierarchical_softmax.md`

---
title: "Hierarchical softmax"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Hierarchical softmax

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Yunusov_2024_MirrorStories__Reflecting_Diversity_through_Per...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='human_centered_care'></a>

## Concept 78/144: Human-Centered Care

**Source file:** `Human_Centered_Care.md`

---
title: "Human-Centered Care"
type: concept
related_papers: 2
tags: [concept, auto-generated]
---

# Human-Centered Care

**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- Unknown_2025_Artificial_Intelligence_in_Social_Sciences_and_...
- Unknown_2025_Artificial_Intelligence_in_Social_Work__An_EPIC...

## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='human_rights'></a>

## Concept 79/144: Human Rights

**Source file:** `Human_Rights.md`

---
title: "Human Rights"
type: concept
frequency: 2
related_papers: 2
tags: [concept, auto-generated]
---

# Human Rights

**Frequency:** 2 summaries mention this concept
**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- Amnesty_International_2024_Coded_injustice__Surveillance_and_discrimination_i
- Linnemann_2023_Bedeutung_von_K√ºnstlicher_Intelligenz_in_der_Sozia


## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='humanistic_care'></a>

## Concept 80/144: Humanistic Care

**Source file:** `Humanistic_Care.md`

---
title: "Humanistic Care"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Humanistic Care

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Chen_2025_Social_work_and_artificial_intelligence__Collabora...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='implementation_gap'></a>

## Concept 81/144: Implementation Gap

**Source file:** `Implementation_Gap.md`

---
title: "Implementation Gap"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Implementation Gap

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Siapka_2023_Towards_a_Feminist_Metaethics_of_AI...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='inclusive_ai_design'></a>

## Concept 82/144: Inclusive AI Design

**Source file:** `Inclusive_Ai_Design.md`

---
title: "Inclusive AI Design"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Inclusive AI Design

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Quaid-i-Azam_University_2025_Gender_Bias_in_Artificial_Intel...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='inclusive_design'></a>

## Concept 83/144: Inclusive design

**Source file:** `Inclusive_design.md`

---
title: "Inclusive design"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Inclusive design

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Gaba_2025_Bias,_accuracy,_and_trust__Gender-diverse_perspect...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='index'></a>

## Concept 84/144: Concept Index - SozArb Research

**Source file:** `INDEX.md`

---
title: "Concept Index"
type: index
tags: [concepts, navigation]
---

# Concept Index - SozArb Research

Auto-generated index of key concepts extracted from paper summaries.

## Concepts by Frequency

- **Algorithmic Fairness** (14 papers)
- **Intersectionality** (12 papers)
- **Algorithmic Bias** (12 papers)
- **Responsible Ai** (11 papers)
- **Large Language Models** (10 papers)
- **Generative Ai** (9 papers)
- **Algorithmic Discrimination** (8 papers)
- **Gender Bias** (6 papers)
- **Ai Ethics** (6 papers)
- **Social Work** (6 papers)
- **Bias Mitigation** (5 papers)
- **Artificial Intelligence** (4 papers)
- **Sme Implementation** (3 papers)
- **Ai Governance** (3 papers)
- **Welfare Systems** (3 papers)
- **Stable Diffusion** (3 papers)
- **Visual Bias** (3 papers)
- **Algorithmic Reparation** (3 papers)
- **Feminist Ai** (3 papers)
- **Llm Scaling** (2 papers)
- **Neural Scaling Laws** (2 papers)
- **Synthetic Data** (2 papers)
- **Ai Bias Mitigation** (2 papers)
- **Ai Act** (2 papers)
- **Machine Learning** (2 papers)
- **Structural Injustice** (2 papers)
- **Gender Bias In Ai** (2 papers)
- **Digital Literacy** (2 papers)
- **Women Empowerment In Technology** (2 papers)
- **Inclusive Ai Design** (2 papers)
- **Ai Workforce Diversity** (2 papers)
- **Fair Ai Prompting** (2 papers)
- **Diversity In Ai** (2 papers)
- **Explainable Ai** (2 papers)
- **Ai Transparency** (2 papers)
- **Ai Accountability** (2 papers)
- **Prompt Engineering** (2 papers)
- **Algorithmic Decision-Making** (2 papers)
- **Social Equity** (2 papers)
- **Human Rights** (2 papers)
- **Gender Justice** (2 papers)
- **Community Co-Design** (2 papers)


---

*Auto-generated from summary keywords*
*To update: run `python analysis/extract_concepts_from_summaries.py`*

---

<a id='inference_time_optimization'></a>

## Concept 85/144: Inference-time Optimization

**Source file:** `Inference_time_Optimization.md`

---
title: "Inference-time Optimization"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Inference-time Optimization

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Lau_2023_Dipper__Diversity_in_Prompts_for_Producing_Large_L...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='intersectional_feminism'></a>

## Concept 86/144: Intersectional Feminism

**Source file:** `Intersectional_Feminism.md`

---
title: "Intersectional Feminism"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Intersectional Feminism

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Klein_2024_Data_Feminism_for_AI...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='intersectional_subgroup_fairness'></a>

## Concept 87/144: Intersectional Subgroup Fairness

**Source file:** `Intersectional_Subgroup_Fairness.md`

---
title: "Intersectional Subgroup Fairness"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Intersectional Subgroup Fairness

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Ovalle_2023_Factoring_the_Matrix_of_Domination__A_Critical_R...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='intersectionality'></a>

## Concept 88/144: Intersectionality

**Source file:** `Intersectionality.md`

---
title: "Intersectionality"
type: concept
related_papers: 4
tags: [concept, auto-generated]
---

# Intersectionality

**Related papers:** 4

## Definition

*[This section can be manually expanded]*

## Related Papers

- Karagianni_2025_Gender_in_a_stereo-(gender)typical_EU_AI_law...
- McCrory_2024_Avoiding_catastrophe_through_intersectionality_...
- Ovalle_2023_Factoring_the_Matrix_of_Domination__A_Critical_R...
- Vethman_2025_Fairness_Beyond_the_Algorithmic_Frame__Actionab...

## Usage in Research

This concept appears in 4 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='iris_marion_youngs_theory'></a>

## Concept 89/144: Iris Marion Young's Theory

**Source file:** `Iris_Marion_Youngs_Theory.md`

---
title: "Iris Marion Young's Theory"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Iris Marion Young's Theory

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Himmelreich_2022_Artificial_Intelligence_and_Structural_Inju...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='kv_cache_management'></a>

## Concept 90/144: KV-Cache Management

**Source file:** `KV_Cache_Management.md`

---
title: "KV-Cache Management"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# KV-Cache Management

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Lau_2023_Dipper__Diversity_in_Prompts_for_Producing_Large_L...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='large_language_models'></a>

## Concept 91/144: Large Language Models

**Source file:** `Large_Language_Models.md`

---
title: "Large Language Models"
type: concept
frequency: 10
related_papers: 6
tags: [concept, auto-generated]
---

# Large Language Models

**Frequency:** 10 summaries mention this concept
**Related papers:** 6

## Definition

*[This section can be manually expanded]*

## Related Papers

- Colombatto_2025_The_influence_of_mental_state_attributions_on_trus
- Kamruzzaman_2024_Prompting_techniques_for_reducing_social_bias_in_L
- Tint_2025_Guardrails,_not_guidance__Understanding_responses_
- UNESCO_2024_Challenging_systematic_prejudices__an_Investigatio
- UNESCO_2024_Challenging_systematic_prejudices__an_Investigatio
- Walgenbach_2023_Intersektionalit√§t


## Usage in Research

This concept appears in 10 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='large_language_models_llms'></a>

## Concept 92/144: Large Language Models (LLMs)

**Source file:** `Large_Language_Models_LLMs.md`

---
title: "Large Language Models (LLMs)"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Large Language Models (LLMs)

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- UNESCO_2024_Bias_against_women_and_girls_in_large_language_m...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='llm_scaling'></a>

## Concept 93/144: Llm Scaling

**Source file:** `Llm_Scaling.md`

---
title: "Llm Scaling"
type: concept
frequency: 2
related_papers: 2
tags: [concept, auto-generated]
---

# Llm Scaling

**Frequency:** 2 summaries mention this concept
**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- UN_Women_2024_Artificial_Intelligence_and_gender_equality
- UN_Women_2024_Artificial_Intelligence_and_gender_equality


## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='matrix_of_domination'></a>

## Concept 94/144: Matrix of Domination

**Source file:** `Matrix_of_Domination.md`

---
title: "Matrix of Domination"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Matrix of Domination

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Ovalle_2023_Factoring_the_Matrix_of_Domination__A_Critical_R...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='natural_language_processing_nlp'></a>

## Concept 95/144: Natural Language Processing (NLP)

**Source file:** `Natural_Language_Processing_NLP.md`

---
title: "Natural Language Processing (NLP)"
type: concept
related_papers: 2
tags: [concept, auto-generated]
---

# Natural Language Processing (NLP)

**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- Unknown_2025_Artificial_Intelligence_in_Social_Sciences_and_...
- Unknown_2025_Artificial_Intelligence_in_Social_Work__An_EPIC...

## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='negative_sampling'></a>

## Concept 96/144: Negative sampling

**Source file:** `Negative_sampling.md`

---
title: "Negative sampling"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Negative sampling

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Yunusov_2024_MirrorStories__Reflecting_Diversity_through_Per...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='neural_scaling_laws'></a>

## Concept 97/144: Neural Scaling Laws

**Source file:** `Neural_Scaling_Laws.md`

---
title: "Neural Scaling Laws"
type: concept
frequency: 2
related_papers: 2
tags: [concept, auto-generated]
---

# Neural Scaling Laws

**Frequency:** 2 summaries mention this concept
**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- UN_Women_2024_Artificial_Intelligence_and_gender_equality
- UN_Women_2024_Artificial_Intelligence_and_gender_equality


## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='noise_contrastive_estimation_nce'></a>

## Concept 98/144: Noise Contrastive Estimation (NCE)

**Source file:** `Noise_Contrastive_Estimation_NCE.md`

---
title: "Noise Contrastive Estimation (NCE)"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Noise Contrastive Estimation (NCE)

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Yunusov_2024_MirrorStories__Reflecting_Diversity_through_Per...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='out_of_distribution_generalization'></a>

## Concept 99/144: Out-of-Distribution Generalization

**Source file:** `Out_of_Distribution_Generalization.md`

---
title: "Out-of-Distribution Generalization"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Out-of-Distribution Generalization

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Santy_2023_NLPositionality__Characterizing_design_biases_of_...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='persona_modeling'></a>

## Concept 100/144: Persona Modeling

**Source file:** `Persona_Modeling.md`

---
title: "Persona Modeling"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Persona Modeling

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Kamruzzaman_2024_Prompting_techniques_for_reducing_social_bi...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='position_vector'></a>

## Concept 101/144: Position Vector

**Source file:** `Position_Vector.md`

---
title: "Position Vector"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Position Vector

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Santy_2023_NLPositionality__Characterizing_design_biases_of_...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='positionality'></a>

## Concept 102/144: Positionality

**Source file:** `Positionality.md`

---
title: "Positionality"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Positionality

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Vethman_2025_Fairness_Beyond_the_Algorithmic_Frame__Actionab...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='positivist_paradigm'></a>

## Concept 103/144: Positivist Paradigm

**Source file:** `Positivist_Paradigm.md`

---
title: "Positivist Paradigm"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Positivist Paradigm

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Ovalle_2023_Factoring_the_Matrix_of_Domination__A_Critical_R...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='power_relations'></a>

## Concept 104/144: Power Relations

**Source file:** `Power_Relations.md`

---
title: "Power Relations"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Power Relations

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Siapka_2023_Towards_a_Feminist_Metaethics_of_AI...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='praxis'></a>

## Concept 105/144: Praxis

**Source file:** `Praxis.md`

---
title: "Praxis"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Praxis

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Ovalle_2023_Factoring_the_Matrix_of_Domination__A_Critical_R...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='predictive_analytics'></a>

## Concept 106/144: Predictive Analytics

**Source file:** `Predictive_Analytics.md`

---
title: "Predictive Analytics"
type: concept
related_papers: 2
tags: [concept, auto-generated]
---

# Predictive Analytics

**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- Unknown_2025_Artificial_Intelligence_in_Social_Sciences_and_...
- Unknown_2025_Artificial_Intelligence_in_Social_Work__An_EPIC...

## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='prompt_caching'></a>

## Concept 107/144: Prompt Caching

**Source file:** `Prompt_Caching.md`

---
title: "Prompt Caching"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Prompt Caching

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Lau_2023_Dipper__Diversity_in_Prompts_for_Producing_Large_L...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='prompt_diversity'></a>

## Concept 108/144: Prompt Diversity

**Source file:** `Prompt_Diversity.md`

---
title: "Prompt Diversity"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Prompt Diversity

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Lau_2023_Dipper__Diversity_in_Prompts_for_Producing_Large_L...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='public_interest_models'></a>

## Concept 109/144: Public-interest models

**Source file:** `Public_interest_models.md`

---
title: "Public-interest models"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Public-interest models

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Washington_2025_Fragile_Foundations__Hidden_Risks_of_Generat...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='reflexivity'></a>

## Concept 110/144: Reflexivity

**Source file:** `Reflexivity.md`

---
title: "Reflexivity"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Reflexivity

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Siapka_2023_Towards_a_Feminist_Metaethics_of_AI...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='responsible_ai'></a>

## Concept 111/144: Responsible AI

**Source file:** `Responsible_Ai.md`

---
title: "Responsible AI"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Responsible AI

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Gaba_2025_Bias,_accuracy,_and_trust__Gender-diverse_perspect...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='retrograde_stereotypes'></a>

## Concept 112/144: Retrograde stereotypes

**Source file:** `Retrograde_stereotypes.md`

---
title: "Retrograde stereotypes"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Retrograde stereotypes

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- UNESCO_2024_Bias_against_women_and_girls_in_large_language_m...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='selection_bias'></a>

## Concept 113/144: Selection Bias

**Source file:** `Selection_Bias.md`

---
title: "Selection Bias"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Selection Bias

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Santy_2023_NLPositionality__Characterizing_design_biases_of_...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='semantic_associations'></a>

## Concept 114/144: Semantic associations

**Source file:** `Semantic_associations.md`

---
title: "Semantic associations"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Semantic associations

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- UNESCO_2024_Bias_against_women_and_girls_in_large_language_m...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='shared_responsibility'></a>

## Concept 115/144: Shared Responsibility

**Source file:** `Shared_Responsibility.md`

---
title: "Shared Responsibility"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Shared Responsibility

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Lin_2022_Artificial_Intelligence_in_a_Structurally_Unjust_S...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='skip_gram_model'></a>

## Concept 116/144: Skip-gram model

**Source file:** `Skip_gram_model.md`

---
title: "Skip-gram model"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Skip-gram model

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Yunusov_2024_MirrorStories__Reflecting_Diversity_through_Per...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='sme_implementation'></a>

## Concept 117/144: Sme Implementation

**Source file:** `Sme_Implementation.md`

---
title: "Sme Implementation"
type: concept
frequency: 3
related_papers: 1
tags: [concept, auto-generated]
---

# Sme Implementation

**Frequency:** 3 summaries mention this concept
**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Unknown__feminist_AI___ACADEMY


## Usage in Research

This concept appears in 3 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='social_bias'></a>

## Concept 118/144: Social Bias

**Source file:** `Social_Bias.md`

---
title: "Social Bias"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Social Bias

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Kamruzzaman_2024_Prompting_techniques_for_reducing_social_bi...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='social_equity'></a>

## Concept 119/144: Social Equity

**Source file:** `Social_Equity.md`

---
title: "Social Equity"
type: concept
frequency: 2
related_papers: 2
tags: [concept, auto-generated]
---

# Social Equity

**Frequency:** 2 summaries mention this concept
**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- A+_Alliance_2024_Incubating_Feminist_AI__Executive_Summary_2021-202
- Wudel_2025_What_is_Feminist_AI_


## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='social_structure_in_ai_context'></a>

## Concept 120/144: Social Structure (in AI context)

**Source file:** `Social_Structure_in_AI_context.md`

---
title: "Social Structure (in AI context)"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Social Structure (in AI context)

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Lin_2022_Artificial_Intelligence_in_a_Structurally_Unjust_S...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='social_work'></a>

## Concept 121/144: Social Work

**Source file:** `Social_Work.md`

---
title: "Social Work"
type: concept
frequency: 6
related_papers: 4
tags: [concept, auto-generated]
---

# Social Work

**Frequency:** 6 summaries mention this concept
**Related papers:** 4

## Definition

*[This section can be manually expanded]*

## Related Papers

- Kutscher_2020_Handbuch_Soziale_Arbeit_und_Digitalisierung
- Linnemann_2023_Bedeutung_von_K√ºnstlicher_Intelligenz_in_der_Sozia
- Studeny_2025_Digitale_Werkzeuge_und_Machtasymmetrien_
- Unknown_2025_Artificial_Intelligence_in_Social_Work__An_EPIC_Mo


## Usage in Research

This concept appears in 6 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='socially_responsible_ai'></a>

## Concept 122/144: Socially Responsible AI

**Source file:** `Socially_Responsible_AI.md`

---
title: "Socially Responsible AI"
type: concept
related_papers: 2
tags: [concept, auto-generated]
---

# Socially Responsible AI

**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- Unknown_2025_Artificial_Intelligence_in_Social_Sciences_and_...
- Unknown_2025_Artificial_Intelligence_in_Social_Work__An_EPIC...

## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='special_categories_of_personal_data'></a>

## Concept 123/144: Special categories of personal data

**Source file:** `Special_categories_of_personal_data.md`

---
title: "Special categories of personal data"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Special categories of personal data

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Karagianni_2025_Gender_in_a_stereo-(gender)typical_EU_AI_law...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='stable_diffusion'></a>

## Concept 124/144: Stable Diffusion

**Source file:** `Stable_Diffusion.md`

---
title: "Stable Diffusion"
type: concept
frequency: 3
related_papers: 3
tags: [concept, auto-generated]
---

# Stable Diffusion

**Frequency:** 3 summaries mention this concept
**Related papers:** 3

## Definition

*[This section can be manually expanded]*

## Related Papers

- Ghosal_2024_An_empirical_study_of_structural_social_and_ethica
- J√§√§skel√§inen_2025_Intersectional_analysis_of_visual_generative_AI__T
- Walgenbach_2023_Intersektionalit√§t


## Usage in Research

This concept appears in 3 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='status_quo_injustice'></a>

## Concept 125/144: Status Quo Injustice

**Source file:** `Status_Quo_Injustice.md`

---
title: "Status Quo Injustice"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Status Quo Injustice

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Himmelreich_2022_Artificial_Intelligence_and_Structural_Inju...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='stochastic_self_ensembles'></a>

## Concept 126/144: Stochastic Self-Ensembles

**Source file:** `Stochastic_Self_Ensembles.md`

---
title: "Stochastic Self-Ensembles"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Stochastic Self-Ensembles

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Lau_2023_Dipper__Diversity_in_Prompts_for_Producing_Large_L...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='structural_barriers'></a>

## Concept 127/144: Structural barriers

**Source file:** `Structural_barriers.md`

---
title: "Structural barriers"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Structural barriers

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Washington_2025_Fragile_Foundations__Hidden_Risks_of_Generat...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='structural_bias'></a>

## Concept 128/144: Structural bias

**Source file:** `Structural_bias.md`

---
title: "Structural bias"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Structural bias

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Karagianni_2025_Gender_in_a_stereo-(gender)typical_EU_AI_law...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='structural_inequality'></a>

## Concept 129/144: Structural inequality

**Source file:** `Structural_Inequality.md`

---
title: "Structural inequality"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Structural inequality

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Vethman_2025_Fairness_Beyond_the_Algorithmic_Frame__Actionab...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='structural_injustice'></a>

## Concept 130/144: Structural Injustice

**Source file:** `Structural_Injustice.md`

---
title: "Structural Injustice"
type: concept
related_papers: 2
tags: [concept, auto-generated]
---

# Structural Injustice

**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- Himmelreich_2022_Artificial_Intelligence_and_Structural_Inju...
- Lin_2022_Artificial_Intelligence_in_a_Structurally_Unjust_S...

## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='subsampling_probability'></a>

## Concept 131/144: Subsampling probability

**Source file:** `Subsampling_probability.md`

---
title: "Subsampling probability"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Subsampling probability

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Yunusov_2024_MirrorStories__Reflecting_Diversity_through_Per...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='synthetic_data'></a>

## Concept 132/144: Synthetic Data

**Source file:** `Synthetic_Data.md`

---
title: "Synthetic Data"
type: concept
frequency: 2
related_papers: 2
tags: [concept, auto-generated]
---

# Synthetic Data

**Frequency:** 2 summaries mention this concept
**Related papers:** 2

## Definition

*[This section can be manually expanded]*

## Related Papers

- UN_Women_2024_Artificial_Intelligence_and_gender_equality
- UN_Women_2024_Artificial_Intelligence_and_gender_equality


## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='tech_optimism'></a>

## Concept 133/144: Tech-optimism

**Source file:** `Tech_optimism.md`

---
title: "Tech-optimism"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Tech-optimism

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Vethman_2025_Fairness_Beyond_the_Algorithmic_Frame__Actionab...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='technical_fix_approach_critique'></a>

## Concept 134/144: Technical-Fix Approach (critique)

**Source file:** `Technical_Fix_Approach_critique.md`

---
title: "Technical-Fix Approach (critique)"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Technical-Fix Approach (critique)

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Lin_2022_Artificial_Intelligence_in_a_Structurally_Unjust_S...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='technical_literacy'></a>

## Concept 135/144: Technical Literacy

**Source file:** `Technical_Literacy.md`

---
title: "Technical Literacy"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Technical Literacy

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Chen_2025_Social_work_and_artificial_intelligence__Collabora...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='technocratic_governance'></a>

## Concept 136/144: Technocratic Governance

**Source file:** `Technocratic_Governance.md`

---
title: "Technocratic Governance"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Technocratic Governance

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- McCrory_2024_Avoiding_catastrophe_through_intersectionality_...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='training_data'></a>

## Concept 137/144: Training Data

**Source file:** `Training_Data.md`

---
title: "Training Data"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Training Data

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- European_Data_Protection_Supervisor_2023_Explainable_Artific...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='transparency'></a>

## Concept 138/144: Transparency

**Source file:** `Transparency.md`

---
title: "Transparency"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Transparency

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- European_Data_Protection_Supervisor_2023_Explainable_Artific...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='trust_in_ai'></a>

## Concept 139/144: Trust in AI

**Source file:** `Trust_in_AI.md`

---
title: "Trust in AI"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Trust in AI

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Gaba_2025_Bias,_accuracy,_and_trust__Gender-diverse_perspect...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='visual_bias'></a>

## Concept 140/144: Visual Bias

**Source file:** `Visual_Bias.md`

---
title: "Visual Bias"
type: concept
frequency: 3
related_papers: 3
tags: [concept, auto-generated]
---

# Visual Bias

**Frequency:** 3 summaries mention this concept
**Related papers:** 3

## Definition

*[This section can be manually expanded]*

## Related Papers

- Ghosal_2024_An_empirical_study_of_structural_social_and_ethica
- J√§√§skel√§inen_2025_Intersectional_analysis_of_visual_generative_AI__T
- Walgenbach_2023_Intersektionalit√§t


## Usage in Research

This concept appears in 3 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='welfare_systems'></a>

## Concept 141/144: Welfare Systems

**Source file:** `Welfare_Systems.md`

---
title: "Welfare Systems"
type: concept
frequency: 3
related_papers: 3
tags: [concept, auto-generated]
---

# Welfare Systems

**Frequency:** 3 summaries mention this concept
**Related papers:** 3

## Definition

*[This section can be manually expanded]*

## Related Papers

- Amnesty_International_2024_Coded_injustice__Surveillance_and_discrimination_i
- Amnesty_International_2024_Coded_injustice__Surveillance_and_discrimination_i
- European_Data_Protection_Supervisor_2023_Explainable_Artificial_Intelligence


## Usage in Research

This concept appears in 3 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='women_empowerment_in_technology'></a>

## Concept 142/144: Women Empowerment In Technology

**Source file:** `Women_Empowerment_In_Technology.md`

---
title: "Women Empowerment In Technology"
type: concept
frequency: 2
related_papers: 1
tags: [concept, auto-generated]
---

# Women Empowerment In Technology

**Frequency:** 2 summaries mention this concept
**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Quaid-i-Azam_University_2025_Gender_Bias_in_Artificial_Intelligence__Empowering


## Usage in Research

This concept appears in 2 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-10*

---

<a id='womens_technological_participation'></a>

## Concept 143/144: Women's Technological Participation

**Source file:** `Womens_Technological_Participation.md`

---
title: "Women's Technological Participation"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Women's Technological Participation

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Quaid-i-Azam_University_2025_Gender_Bias_in_Artificial_Intel...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

<a id='zero_shot_cot'></a>

## Concept 144/144: Zero-shot CoT

**Source file:** `Zero_shot_CoT.md`

---
title: "Zero-shot CoT"
type: concept
related_papers: 1
tags: [concept, auto-generated]
---

# Zero-shot CoT

**Related papers:** 1

## Definition

*[This section can be manually expanded]*

## Related Papers

- Kamruzzaman_2024_Prompting_techniques_for_reducing_social_bi...

## Usage in Research

This concept appears in 1 paper summaries, indicating its relevance to the SozArb research domain.

---

*Auto-generated from summary keywords*
*Last updated: 2025-11-16*

---

